file_path,api_count,code
setup.py,0,"b'\n# Imports\nfrom setuptools import setup, find_packages\n\n# Installation setup\nsetup(\n    name=\'EchoTorch\',\n    version=\'0.1.2\',\n    description=""A Python toolkit for Reservoir Computing."",\n    long_description=""A Python toolkit for Reservoir Computing, Echo State Network and Conceptor experimentation based on pyTorch."",\n    author=\'Nils Schaetti\',\n    author_email=\'nils.schaetti@unine.ch\',\n    license=\'GPLv3\',\n    packages=find_packages(),\n    install_requires=[\n      \'torch==1.3.0\',\n      \'torchvision==0.4.1\'\n    ],\n    zip_safe=False\n)\n\n'"
echotorch/__init__.py,0,"b""# -*- coding: utf-8 -*-\n#\n\n# Imports\nfrom . import datasets\nfrom . import models\nfrom . import nn\nfrom . import utils\n\n\n# All EchoTorch's modules\n__all__ = ['datasets', 'models', 'nn', 'utils']\n"""
test/__init__.py,0,b''
test/test_narma10_prediction.py,0,"b'# -*- coding: utf-8 -*-\n#\n# File : test/narma10_prediction\n# Description : NARMA-10 prediction test case.\n# Date : 26th of January, 2018\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti <nils.schaetti@unine.ch>\n\n# Imports\nimport unittest\nfrom unittest import TestCase\n\n\n# Test NARMA10 timeseries prediction.\nclass Test_NARMA10_Prediction(TestCase):\n    """"""\n    Test NARMA10 timeseries prediction\n    """"""\n\n    ##############################\n    # TESTS\n    ##############################\n\n    # Simple test\n    def test_simple(self):\n        """"""\n        Simple test\n        :return:\n        """"""\n        return True\n    # end test_simple\n\n# end Test_NARMA10_Prediction\n\n\n# Run test\nif __name__ == \'__main__\':\n    unittest.main()\n# end if\n'"
docs/source/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# EchoTorch documentation build configuration file, created by\n# sphinx-quickstart on Thu Apr  6 11:30:46 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nimport echotorch\n#import sphinx_bootstrap_theme\nsys.path.insert(0, os.path.abspath(\'../../echotorch\'))\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\'sphinx.ext.autodoc\',\n    \'sphinx.ext.todo\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.githubpages\']\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'EchoTorch\'\ncopyright = u\'2017, Nils Schaetti\'\nauthor = u\'Nils Schaetti\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = u\'0.1\'\n# The full version, including alpha/beta/rc tags.\nrelease = u\'0.1\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = []\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n#html_theme = \'bootstrap\'\n#html_theme_path = sphinx_bootstrap_theme.get_html_theme_path()\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'EchoTorchdoc\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'EchoTorch.tex\', u\'EchoTorch Documentation\',\n     u\'Nils Schaetti\', \'manual\'),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'echotorch\', u\'EchoTorch Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'EchoTorch\', u\'EchoTorch Documentation\',\n     author, \'EchoTorch\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n\n'"
echotorch/datasets/DatasetComposer.py,3,"b'# -*- coding: utf-8 -*-\n#\n\n# Imports\nimport torch\nfrom torch.utils.data.dataset import Dataset\nimport numpy as np\n\n\n# Dataset Composer\nclass DatasetComposer(Dataset):\n    """"""\n    Compose dataset\n    """"""\n\n    # Constructor\n    def __init__(self, datasets, *args, **kwargs):\n        """"""\n        Constructor\n        :param datasets:\n        """"""\n        # Super\n        super(DatasetComposer, self).__init__(*args, **kwargs)\n\n        # Properties\n        self.datasets = datasets\n        self.n_datasets = len(datasets)\n\n        # Map item to datasets items\n        self.map_items = {}\n        self.n_samples = 0\n        index = 0\n        for i, d in enumerate(datasets):\n            for j in range(len(d)):\n                self.map_items[index] = (i, j)\n                index += 1\n                self.n_samples += 1\n            # end for\n        # end for\n    # end __init__\n\n    #############################################\n    # OVERRIDE\n    #############################################\n\n    # Length\n    def __len__(self):\n        """"""\n        Length\n        :return:\n        """"""\n        return self.n_samples\n    # end __len__\n\n    # Get item\n    def __getitem__(self, idx):\n        """"""\n        Get item\n        :param idx:\n        :return:\n        """"""\n        (d, e) = self.map_items[idx]\n        sample = self.datasets[d][e]\n        # print(d, e)\n        outputs = self._create_outputs(d, sample.shape[0])\n        # print(outputs[:10])\n        return self.datasets[d][e], outputs, torch.LongTensor([d])\n    # end __getitem__\n\n    #############################################\n    # PRIVATE\n    #############################################\n\n    # Create outputs\n    def _create_outputs(self, i, time_length):\n        """"""\n        Create outputs\n        :param i:\n        :param time_length:\n        :return:\n        """"""\n        # print(""create {}"".format(i))\n        # Create tensor\n        outputs = torch.zeros(time_length, self.n_datasets)\n\n        # Put to one\n        outputs[:, i] = 1.0\n\n        return outputs\n    # end _create_outputs\n\n# end DatasetComposer\n'"
echotorch/datasets/HenonAttractor.py,6,"b'# -*- coding: utf-8 -*-\n#\n\n# Imports\nimport torch\nfrom torch.utils.data.dataset import Dataset\nfrom random import shuffle\nimport numpy as np\n\n\n# Henon Attractor\nclass HenonAttractor(Dataset):\n    """"""\n    The R\xc3\xb6ssler attractor is the attractor for the R\xc3\xb6ssler system, a system of three non-linear ordinary differential\n    equations originally studied by Otto R\xc3\xb6ssler. These differential equations define a continuous-time dynamical\n    system that exhibits chaotic dynamics associated with the fractal properties of the attractor.\n    """"""\n\n    # Constructor\n    def __init__(self, sample_len, n_samples, xy, a, b, washout=0, normalize=False, seed=None):\n        """"""\n        Constructor\n        :param sample_len: Length of the time-series in time steps.\n        :param n_samples: Number of samples to generate.\n        :param a:\n        :param b:\n        :param c:\n        """"""\n        # Properties\n        self.sample_len = sample_len\n        self.n_samples = n_samples\n        self.a = a\n        self.b = b\n        self.xy = xy\n        self.normalize = normalize\n        self.washout = washout\n\n        # Seed\n        if seed is not None:\n            torch.initial_seed(seed)\n        # end if\n\n        # Generate data set\n        self.outputs = self._generate()\n    # end __init__\n\n    #############################################\n    # OVERRIDE\n    #############################################\n\n    # Length\n    def __len__(self):\n        """"""\n        Length\n        :return:\n        """"""\n        return self.n_samples\n    # end __len__\n\n    # Get item\n    def __getitem__(self, idx):\n        """"""\n        Get item\n        :param idx:\n        :return:\n        """"""\n        return self.outputs[idx]\n    # end __getitem__\n\n    ##############################################\n    # PUBLIC\n    ##############################################\n\n    # Regenerate\n    def regenerate(self):\n        """"""\n        Regenerate\n        :return:\n        """"""\n        # Generate data set\n        self.outputs = self._generate()\n    # end regenerate\n\n    ##############################################\n    # PRIVATE\n    ##############################################\n\n    # Henon\n    def _henon(self, x, y):\n        """"""\n        Henon\n        :param x:\n        :param y:\n        :param z:\n        :return:\n        """"""\n        x_dot = 1 - (self.a * (x * x)) + y\n        y_dot = self.b * x\n        return x_dot, y_dot\n    # end _lorenz\n\n    # Generate\n    def _generate(self):\n        """"""\n        Generate dataset\n        :return:\n        """"""\n        # Sizes\n        total_size = self.sample_len\n\n        # First position\n        xy = self.xy\n\n        # Samples\n        samples = list()\n\n        # Washout\n        for t in range(self.washout):\n            xy = self._henon(xy[0], xy[1])\n        # end for\n\n        # For each sample\n        for n in range(self.n_samples):\n            # Tensor\n            sample = torch.zeros(total_size, 2)\n\n            # Timesteps\n            for t in range(total_size):\n                xy = self._henon(xy[0], xy[1])\n                sample[t] = xy\n            # end for\n\n            # Normalize\n            if self.normalize:\n                maxval = torch.max(sample, dim=0)\n                minval = torch.min(sample, dim=0)\n                sample = torch.mm(torch.inv(torch.diag(maxval - minval)), (sample - minval.repeat(total_size, 1)))\n            # end if\n\n            # Add\n            samples.append(sample)\n        # end for\n\n        # Shuffle\n        shuffle(samples)\n\n        return samples\n    # end _generate\n\n# end HenonAttractor\n'"
echotorch/datasets/LambdaDataset.py,3,"b'# -*- coding: utf-8 -*-\n#\n\n# Imports\nimport torch\nfrom torch.utils.data.dataset import Dataset\nimport numpy as np\n\n\n# Lambda dataset\nclass LambdaDataset(Dataset):\n    """"""\n    Create simple periodic signal timeseries\n    """"""\n\n    # Constructor\n    def __init__(self, sample_len, n_samples, func, start=0, dtype=torch.float32):\n        """"""\n        Constructor\n        :param sample_len: Sample\'s length\n        :param period:\n        """"""\n        # Properties\n        self.sample_len = sample_len\n        self.n_samples = n_samples\n        self.func = func\n        self.start = start\n        self.dtype = dtype\n\n        # Generate data set\n        self.outputs = self._generate()\n    # end __init__\n\n    #############################################\n    # OVERRIDE\n    #############################################\n\n    # Length\n    def __len__(self):\n        """"""\n        Length\n        :return:\n        """"""\n        return self.n_samples\n    # end __len__\n\n    # Get item\n    def __getitem__(self, idx):\n        """"""\n        Get item\n        :param idx:\n        :return:\n        """"""\n        return self.outputs[idx]\n    # end __getitem__\n\n    ##############################################\n    # PRIVATE\n    ##############################################\n\n    # Generate\n    def _generate(self):\n        """"""\n        Generate dataset\n        :return:\n        """"""\n        # List of samples\n        samples = list()\n\n        # For each sample\n        for i in range(self.n_samples):\n            # Tensor\n            sample = torch.zeros(self.sample_len, 1, dtype=self.dtype)\n\n            # Timestep\n            for t in range(self.sample_len):\n                sample[t, 0] = self.func(t + self.start)\n            # end for\n\n            # Append\n            samples.append(sample)\n        # end for\n\n        return samples\n    # end _generate\n\n# end LambdaDataset\n'"
echotorch/datasets/LogisticMapDataset.py,3,"b'# -*- coding: utf-8 -*-\n#\n\n# Imports\nimport torch\nfrom torch.utils.data.dataset import Dataset\nimport numpy as np\n\n\n# Logistic Map dataset\nclass LogisticMapDataset(Dataset):\n    """"""\n    Logistic Map dataset\n    """"""\n\n    # Constructor\n    def __init__(self, sample_len, n_samples, alpha=5, beta=11, gamma=13, c=3.6, b=0.13, seed=None):\n        """"""\n        Constructor\n        :param sample_len:\n        :param n_samples:\n        :param alpha:\n        :param beta:\n        :param gamma:\n        :param c:\n        :param b:\n        :param seed:\n        """"""\n        # Properties\n        self.sample_len = sample_len\n        self.n_samples = n_samples\n        self.alpha = alpha\n        self.beta = beta\n        self.gamma = gamma\n        self.c = c\n        self.b = b\n        self.p2 = np.pi * 2\n\n        # Init seed if needed\n        if seed is not None:\n            torch.manual_seed(seed)\n        # end if\n    # end __init__\n\n    # Length\n    def __len__(self):\n        """"""\n        Length\n        :return:\n        """"""\n        return self.n_samples\n    # end __len__\n\n    # Get item\n    def __getitem__(self, idx):\n        """"""\n        Get item\n        :param idx:\n        :return:\n        """"""\n        # Time and forces\n        t = np.linspace(0, 1, self.sample_len, endpoint=0)\n        dforce = np.sin(self.p2 * self.alpha * t) + np.sin(self.p2 * self.beta * t) + np.sin(self.p2 * self.gamma * t)\n\n        # Series\n        series = torch.zeros(self.sample_len, 1)\n        series[0] = 0.6\n\n        # Generate\n        for i in range(1, self.sample_len):\n            series[i] = self._logistic_map(series[i-1], self.c + self.b * dforce[i])\n        # end for\n\n        return series\n    # end __getitem__\n\n    #######################################\n    # Private\n    #######################################\n\n    # Logistic map\n    def _logistic_map(self, x, r):\n        """"""\n        Logistic map\n        :param x:\n        :param r:\n        :return:\n        """"""\n        return r * x * (1-x)\n    # end logistic_map\n\n# end MackeyGlassDataset\n'"
echotorch/datasets/LorenzAttractor.py,6,"b'# -*- coding: utf-8 -*-\n#\n\n# Imports\nimport torch\nfrom torch.utils.data.dataset import Dataset\nimport numpy as np\n\n\n# Lorenz Attractor\nclass LorenzAttractor(Dataset):\n    """"""\n    The R\xc3\xb6ssler attractor is the attractor for the R\xc3\xb6ssler system, a system of three non-linear ordinary differential\n    equations originally studied by Otto R\xc3\xb6ssler. These differential equations define a continuous-time dynamical\n    system that exhibits chaotic dynamics associated with the fractal properties of the attractor.\n    """"""\n\n    # Constructor\n    def __init__(self, sample_len, n_samples, xyz, sigma, b, r, dt=0.01, washout=0, normalize=False, seed=None):\n        """"""\n        Constructor\n        :param sample_len: Length of the time-series in time steps.\n        :param n_samples: Number of samples to generate.\n        :param a:\n        :param b:\n        :param c:\n        """"""\n        # Properties\n        self.sample_len = sample_len\n        self.n_samples = n_samples\n        self.xyz = xyz\n        self.dt = dt\n        self.normalize = normalize\n        self.washout = washout\n        self.sigma = sigma\n        self.b = b\n        self.r = r\n\n        # Seed\n        if seed is not None:\n            torch.initial_seed(seed)\n        # end if\n\n        # Generate data set\n        self.outputs = self._generate()\n    # end __init__\n\n    #############################################\n    # OVERRIDE\n    #############################################\n\n    # Length\n    def __len__(self):\n        """"""\n        Length\n        :return:\n        """"""\n        return self.n_samples\n    # end __len__\n\n    # Get item\n    def __getitem__(self, idx):\n        """"""\n        Get item\n        :param idx:\n        :return:\n        """"""\n        return self.outputs[idx]\n    # end __getitem__\n\n    ##############################################\n    # PUBLIC\n    ##############################################\n\n    # Regenerate\n    def regenerate(self):\n        """"""\n        Regenerate\n        :return:\n        """"""\n        # Generate data set\n        self.outputs = self._generate()\n    # end regenerate\n\n    ##############################################\n    # PRIVATE\n    ##############################################\n\n    # Lorenz\n    def _lorenz(self, x, y, z):\n        """"""\n        Lorenz\n        :param x:\n        :param y:\n        :param z:\n        :return:\n        """"""\n        x_dot = self.sigma * (y - x)\n        y_dot = self.r * x - y - x * z\n        z_dot = x * y - self.b * z\n        return x_dot, y_dot, z_dot\n    # end _lorenz\n\n    # Generate\n    def _generate(self):\n        """"""\n        Generate dataset\n        :return:\n        """"""\n        # Sizes\n        total_size = self.sample_len\n\n        # List of samples\n        samples = list()\n\n        # XYZ\n        xyz = self.xyz\n\n        # Washout\n        for t in range(self.washout):\n            # Derivatives of the X, Y, Z state\n            x_dot, y_dot, z_dot = self._lorenz(xyz[0], xyz[1], xyz[2])\n\n            # Apply changes\n            xyz[0] += self.dt * x_dot\n            xyz[1] += self.dt * y_dot\n            xyz[2] += self.dt * z_dot\n        # end for\n\n        # For each sample\n        for i in range(self.n_samples):\n            # Tensor\n            sample = torch.zeros(self.sample_len, 3)\n            for t in range(self.sample_len):\n                # Derivatives of the X, Y, Z state\n                x_dot, y_dot, z_dot = self._lorenz(xyz[0], xyz[1], xyz[2])\n\n                # Apply changes\n                xyz[0] += self.dt * x_dot\n                xyz[1] += self.dt * y_dot\n                xyz[2] += self.dt * z_dot\n\n                # Set\n                sample[t, 0] = xyz[0]\n                sample[t, 1] = xyz[1]\n                sample[t, 2] = xyz[2]\n            # end for\n\n            # Normalize\n            if self.normalize:\n                maxval = torch.max(sample, dim=0)\n                minval = torch.min(sample, dim=0)\n                sample = torch.mm(torch.inv(torch.diag(maxval - minval)), (sample - minval.repeat(total_size, 1)))\n            # end if\n\n            # Append\n            samples.append(sample)\n        # end for\n\n        return samples\n    # end _generate\n\n# end LorenzAttractor\n'"
echotorch/datasets/MackeyGlass2DDataset.py,7,"b'# -*- coding: utf-8 -*-\n#\n\n# Imports\nimport torch\nfrom torch.utils.data.dataset import Dataset\n\n\n# Mackey Glass dataset\nclass MackeyGlass2DDataset(Dataset):\n    """"""\n    Mackey Glass 2D dataset\n    """"""\n\n    # Constructor\n    def __init__(self, sample_len, n_samples, tau, subsample_rate, normalize=False, seed=None):\n        """"""\n        Constructor\n        :param sample_len: Length of the time-series in time steps.\n        :param n_samples: Number of samples to generate.\n        :param tau: Delay of the MG with commonly used value of tau=17 (mild chaos) and tau=30 is moderate chaos.\n        :param seed: Seed of random number generator.\n        """"""\n        # Properties\n        self.sample_len = sample_len\n        self.n_samples = n_samples\n        self.tau = tau\n        self.delta_t = 10\n        self.timeseries = 1.2\n        self.history_len = tau * self.delta_t\n        self.subsample_rate = subsample_rate\n        self.normalize = normalize\n\n        # Init seed if needed\n        if seed is not None:\n            torch.manual_seed(seed)\n        # end if\n    # end __init__\n\n    # Length\n    def __len__(self):\n        """"""\n        Length\n        :return:\n        """"""\n        return self.n_samples\n    # end __len__\n\n    # Get item\n    def __getitem__(self, idx):\n        """"""\n        Get item\n        :param idx:\n        :return:\n        """"""\n        # Total size\n        total_size = self.sample_len\n        oldval = 1.2\n        samples = list()\n\n        # History\n        history = 1.2 * torch.ones(self.history_len) + 0.2 * (torch.rand(self.history_len) - 0.5)\n\n        # For each sample\n        for n in range(self.n_samples):\n            # Preallocate tensor for time-serie\n            sample = torch.zeros(self.sample_len, 2)\n\n            # For each time step\n            step = 0\n            for t in range(total_size):\n                for _ in range(self.delta_t * self.subsample_rate):\n                    step = step + 1\n                    tauval = history[step % self.history_len]\n                    newval = oldval + (0.2 * tauval / (1.0 + tauval**10) - 0.1 * oldval) / self.delta_t\n                    history[step % self.history_len] = oldval\n                    oldval = newval\n                # end for\n                sample[t, 0] = newval\n                sample[t, 1] = tauval\n            # end for\n\n            # Normalize\n            if self.normalize:\n                maxval = torch.max(sample, dim=0)\n                minval = torch.min(sample, dim=0)\n                sample = torch.mm(torch.inv(torch.diag(maxval - minval)), (sample - minval.repeat(total_size, 1)))\n            # end if\n\n            # Append\n            samples.append(sample)\n        # end for\n\n        # Squash timeseries through tanh\n        return samples\n    # end __getitem__\n\n# end MackeyGlassDataset\n'"
echotorch/datasets/MackeyGlassDataset.py,5,"b'# -*- coding: utf-8 -*-\n#\n\n# Imports\nimport torch\nfrom torch.utils.data.dataset import Dataset\nimport collections\n\n\n# Mackey Glass dataset\nclass MackeyGlassDataset(Dataset):\n    """"""\n    Mackey Glass dataset\n    """"""\n\n    # Constructor\n    def __init__(self, sample_len, n_samples, tau=17, seed=None):\n        """"""\n        Constructor\n        :param sample_len: Length of the time-series in time steps.\n        :param n_samples: Number of samples to generate.\n        :param tau: Delay of the MG with commonly used value of tau=17 (mild chaos) and tau=30 is moderate chaos.\n        :param seed: Seed of random number generator.\n        """"""\n        # Properties\n        self.sample_len = sample_len\n        self.n_samples = n_samples\n        self.tau = tau\n        self.delta_t = 10\n        self.timeseries = 1.2\n        self.history_len = tau * self.delta_t\n\n        # Init seed if needed\n        if seed is not None:\n            torch.manual_seed(seed)\n        # end if\n    # end __init__\n\n    # Length\n    def __len__(self):\n        """"""\n        Length\n        :return:\n        """"""\n        return self.n_samples\n    # end __len__\n\n    # Get item\n    def __getitem__(self, idx):\n        """"""\n        Get item\n        :param idx:\n        :return:\n        """"""\n        # History\n        history = collections.deque(1.2 * torch.ones(self.history_len) + 0.2 * (torch.rand(self.history_len) - 0.5))\n\n        # Preallocate tensor for time-serie\n        inp = torch.zeros(self.sample_len, 1)\n\n        # For each time step\n        for timestep in range(self.sample_len):\n            for _ in range(self.delta_t):\n                xtau = history.popleft()\n                history.append(self.timeseries)\n                self.timeseries = history[-1] + (0.2 * xtau / (1.0 + xtau ** 10) - 0.1 * history[-1]) / self.delta_t\n            # end for\n            inp[timestep] = self.timeseries\n        # end for\n\n        # Inputs\n        inputs = torch.tan(inp - 1)\n\n        # Squash timeseries through tanh\n        return inputs[:-1], inputs[1:]\n    # end __getitem__\n\n# end MackeyGlassDataset\n'"
echotorch/datasets/MemTestDataset.py,5,"b'# -*- coding: utf-8 -*-\n#\n\n# Imports\nimport torch\nfrom torch.utils.data.dataset import Dataset\n\n\n# Generates a series of input timeseries and delayed versions as outputs.\nclass MemTestDataset(Dataset):\n    """"""\n    Generates a series of input timeseries and delayed versions as outputs.\n    Delay is given in number of timesteps. Can be used to empirically measure the\n    memory capacity of a system.\n    """"""\n\n    # Constructor\n    def __init__(self, sample_len, n_samples, n_delays=10, seed=None):\n        """"""\n        Constructor\n        :param sample_len: Length of the time-series in time steps.\n        :param n_samples: Number of samples to generate.\n        :param n_delays: Number of step to delay\n        :param seed: Seed of random number generator.\n        """"""\n        # Properties\n        self.sample_len = sample_len\n        self.n_samples = n_samples\n        self.n_delays = n_delays\n\n        # Init seed if needed\n        if seed is not None:\n            torch.manual_seed(seed)\n        # end if\n    # end __init__\n\n    # Length\n    def __len__(self):\n        """"""\n        Length\n        :return:\n        """"""\n        return self.n_samples\n    # end __len__\n\n    # Get item\n    def __getitem__(self, idx):\n        """"""\n        Get item\n        :param idx:\n        :return:\n        """"""\n        inputs = (torch.rand(self.sample_len, 1) - 0.5) * 1.6\n        outputs = torch.zeros(self.sample_len, self.n_delays)\n        for k in range(self.n_delays):\n            outputs[:, k:k+1] = torch.cat((torch.zeros(k + 1, 1), inputs[:-k - 1, :]), dim=0)\n        # end for\n        return inputs, outputs\n    # end __getitem__\n\n# end MemTestDataset\n'"
echotorch/datasets/NARMADataset.py,6,"b'# -*- coding: utf-8 -*-\n#\n\n# Imports\nimport torch\nfrom torch.utils.data.dataset import Dataset\n\n\n# 10th order NARMA task\nclass NARMADataset(Dataset):\n    """"""\n    xth order NARMA task\n    WARNING: this is an unstable dataset. There is a small chance the system becomes\n    unstable, leading to an unusable dataset. It is better to use NARMA30 which\n    where this problem happens less often.\n    """"""\n\n    # Constructor\n    def __init__(self, sample_len, n_samples, system_order=10, seed=None):\n        """"""\n        Constructor\n        :param sample_len: Length of the time-series in time steps.\n        :param n_samples: Number of samples to generate.\n        :param system_order: th order NARMA\n        :param seed: Seed of random number generator.\n        """"""\n        # Properties\n        self.sample_len = sample_len\n        self.n_samples = n_samples\n        self.system_order = system_order\n\n        # System order\n        self.parameters = torch.zeros(4)\n        if system_order == 10:\n            self.parameters[0] = 0.3\n            self.parameters[1] = 0.05\n            self.parameters[2] = 9\n            self.parameters[3] = 0.1\n        else:\n            self.parameters[0] = 0.2\n            self.parameters[1] = 0.04\n            self.parameters[2] = 29\n            self.parameters[3] = 0.001\n        # end if\n\n        # Init seed if needed\n        if seed is not None:\n            torch.manual_seed(seed)\n        # end if\n\n        # Generate data set\n        self.inputs, self.outputs = self._generate()\n    # end __init__\n\n    #############################################\n    # OVERRIDE\n    #############################################\n\n    # Length\n    def __len__(self):\n        """"""\n        Length\n        :return:\n        """"""\n        return self.n_samples\n    # end __len__\n\n    # Get item\n    def __getitem__(self, idx):\n        """"""\n        Get item\n        :param idx:\n        :return:\n        """"""\n        return self.inputs[idx], self.outputs[idx]\n    # end __getitem__\n\n    ##############################################\n    # PRIVATE\n    ##############################################\n\n    # Generate\n    def _generate(self):\n        """"""\n        Generate dataset\n        :return:\n        """"""\n        inputs = list()\n        outputs = list()\n        for i in range(self.n_samples):\n            ins = torch.rand(self.sample_len, 1) * 0.5\n            outs = torch.zeros(self.sample_len, 1)\n            for k in range(self.system_order - 1, self.sample_len - 1):\n                outs[k + 1] = self.parameters[0] * outs[k] + self.parameters[1] * outs[k] * torch.sum(\n                    outs[k - (self.system_order - 1):k + 1]) + 1.5 * ins[k - int(self.parameters[2])] * ins[k] + \\\n                                 self.parameters[3]\n            # end for\n            inputs.append(ins)\n            outputs.append(outs)\n        # end for\n\n        return inputs, outputs\n    # end _generate\n\n# end NARMADataset\n'"
echotorch/datasets/PeriodicSignalDataset.py,3,"b'# -*- coding: utf-8 -*-\n#\n\n# Imports\nimport torch\nfrom torch.utils.data.dataset import Dataset\nimport numpy as np\n\n\n# Periodic signal timeseries\nclass PeriodicSignalDataset(Dataset):\n    """"""\n    Create simple periodic signal timeseries\n    """"""\n\n    # Constructor\n    def __init__(self, sample_len, n_samples, period, start=1, dtype=torch.float64):\n        """"""\n        Constructor\n        :param sample_len: Sample\'s length\n        :param period:\n        """"""\n        # Properties\n        self.sample_len = sample_len\n        self.n_samples = n_samples\n        self.period = period\n        self.start = start\n        self.dtype = dtype\n\n        # Period\n        max_val = np.max(period)\n        min_val = np.min(period)\n        self.rp = 1.8 * (period - min_val) / (max_val - min_val) - 0.9\n        self.period_length = len(period)\n\n        # Function\n        self.func = lambda n: self.rp[(n + 1) % self.period_length]\n\n        # Generate data set\n        self.outputs = self._generate()\n    # end __init__\n\n    #############################################\n    # OVERRIDE\n    #############################################\n\n    # Length\n    def __len__(self):\n        """"""\n        Length\n        :return:\n        """"""\n        return self.n_samples\n    # end __len__\n\n    # Get item\n    def __getitem__(self, idx):\n        """"""\n        Get item\n        :param idx:\n        :return:\n        """"""\n        return self.outputs[idx]\n    # end __getitem__\n\n    ##############################################\n    # PRIVATE\n    ##############################################\n\n    # Generate\n    def _generate(self):\n        """"""\n        Generate dataset\n        :return:\n        """"""\n        # List of samples\n        samples = list()\n\n        # For each sample\n        for i in range(self.n_samples):\n            # Tensor\n            sample = torch.zeros(self.sample_len, 1, dtype=self.dtype)\n\n            # Timestep\n            for t in range(self.sample_len):\n                sample[t, 0] = self.func(i * self.sample_len + t)\n            # end for\n\n            # Append\n            samples.append(sample)\n        # end for\n\n        return samples\n    # end _generate\n\n# end PeriodicSignalDataset\n'"
echotorch/datasets/RosslerAttractor.py,5,"b'# -*- coding: utf-8 -*-\n#\n\n# Imports\nimport torch\nfrom torch.utils.data.dataset import Dataset\nimport numpy as np\n\n\n# Rossler Attractor\nclass RosslerAttractor(Dataset):\n    """"""\n    The R\xc3\xb6ssler attractor is the attractor for the R\xc3\xb6ssler system, a system of three non-linear ordinary differential\n    equations originally studied by Otto R\xc3\xb6ssler. These differential equations define a continuous-time dynamical\n    system that exhibits chaotic dynamics associated with the fractal properties of the attractor.\n    """"""\n\n    # Constructor\n    def __init__(self, sample_len, n_samples, xyz, a, b, c, dt=0.01, washout=0, normalize=False, seed=None):\n        """"""\n        Constructor\n        :param sample_len: Length of the time-series in time steps.\n        :param n_samples: Number of samples to generate.\n        :param a:\n        :param b:\n        :param c:\n        """"""\n        # Properties\n        self.sample_len = sample_len\n        self.n_samples = n_samples\n        self.a = a\n        self.b = b\n        self.c = c\n        self.dt = dt\n        self.normalize = normalize\n        self.washout = washout\n        self.xyz = xyz\n\n        # Seed\n        if seed is not None:\n            np.random.seed(seed)\n        # end if\n\n        # Generate data set\n        self.outputs = self._generate()\n    # end __init__\n\n    #############################################\n    # OVERRIDE\n    #############################################\n\n    # Length\n    def __len__(self):\n        """"""\n        Length\n        :return:\n        """"""\n        return self.n_samples\n    # end __len__\n\n    # Get item\n    def __getitem__(self, idx):\n        """"""\n        Get item\n        :param idx:\n        :return:\n        """"""\n        return self.outputs[idx]\n    # end __getitem__\n\n    ##############################################\n    # PUBLIC\n    ##############################################\n\n    # Regenerate\n    def regenerate(self):\n        """"""\n        Regenerate\n        :return:\n        """"""\n        # Generate data set\n        self.outputs = self._generate()\n    # end regenerate\n\n    ##############################################\n    # PRIVATE\n    ##############################################\n\n    # Rossler\n    def _rossler(self, x, y, z):\n        """"""\n        Lorenz\n        :param x:\n        :param y:\n        :param z:\n        :return:\n        """"""\n        x_dot = -(y + z)\n        y_dot = x + self.a * y\n        z_dot = self.b + x * z - self.c * z\n        return x_dot, y_dot, z_dot\n    # end _lorenz\n\n    # Generate\n    def _generate(self):\n        """"""\n        Generate dataset\n        :return:\n        """"""\n        # Sizes\n        total_size = self.sample_len\n\n        # List of samples\n        samples = list()\n\n        # XYZ\n        xyz = self.xyz\n\n        # Washout\n        for t in range(self.washout):\n            # Derivatives of the X, Y, Z state\n            x_dot, y_dot, z_dot = self._rossler(xyz[0], xyz[1], xyz[2])\n\n            # Apply changes\n            xyz[0] += self.dt * x_dot\n            xyz[1] += self.dt * y_dot\n            xyz[2] += self.dt * z_dot\n        # end for\n\n        # For each sample\n        for i in range(self.n_samples):\n            # Tensor\n            sample = torch.zeros(total_size, 3)\n\n            # Time steps\n            for t in range(1, self.sample_len):\n                # Derivatives of the X, Y, Z state\n                x_dot, y_dot, z_dot = self._rossler(xyz[0], xyz[1], xyz[2])\n\n                # Apply changes\n                xyz[0] += self.dt * x_dot\n                xyz[1] += self.dt * y_dot\n                xyz[2] += self.dt * z_dot\n\n                # Set\n                sample[t, 0] = xyz[0]\n                sample[t, 1] = xyz[1]\n                sample[t, 2] = xyz[2]\n            # end for\n\n            # Normalize\n            if self.normalize:\n                maxval = torch.max(sample, dim=0)\n                minval = torch.min(sample, dim=0)\n                sample = torch.mm(torch.inv(torch.diag(maxval - minval)), (sample - minval.repeat(total_size, 1)))\n            # end if\n\n            # Append\n            samples.append(sample)\n        # end for\n\n        return samples\n    # end _generate\n\n# end RosslerAttractor\n'"
echotorch/datasets/SinusoidalTimeseries.py,3,"b'# -*- coding: utf-8 -*-\n#\n\n# Imports\nimport torch\nfrom torch.utils.data.dataset import Dataset\nimport math\nimport numpy as np\n\n\n# Sinusoidal Timeseries\nclass SinusoidalTimeseries(Dataset):\n    """"""\n    Sinusoidal timeseries\n    """"""\n\n    # Constructor\n    def __init__(self, sample_len, n_samples, period, a=1.0, m=0.0, start=1, dtype=torch.float64):\n        """"""\n        Constructor\n        :param sample_len: Length of the time-series in time steps.\n        :param n_samples: Number of samples to generate.\n        :param a:\n        :param b:\n        :param c:\n        """"""\n        # Properties\n        self.sample_len = sample_len\n        self.n_samples = n_samples\n        self.dtype = dtype\n        self.func = lambda n: a * math.sin(2.0 * math.pi * (n + start) / period) + m\n\n        # Generate data set\n        self.outputs = self._generate()\n    # end __init__\n\n    #############################################\n    # OVERRIDE\n    #############################################\n\n    # Length\n    def __len__(self):\n        """"""\n        Length\n        :return:\n        """"""\n        return self.n_samples\n    # end __len__\n\n    # Get item\n    def __getitem__(self, idx):\n        """"""\n        Get item\n        :param idx:\n        :return:\n        """"""\n        return self.outputs[idx]\n    # end __getitem__\n\n    ##############################################\n    # PUBLIC\n    ##############################################\n\n    # Regenerate\n    def regenerate(self):\n        """"""\n        Regenerate\n        :return:\n        """"""\n        # Generate data set\n        self.outputs = self._generate()\n    # end regenerate\n\n    ##############################################\n    # PRIVATE\n    ##############################################\n\n    # Random initial points\n    def random_initial_points(self):\n        """"""\n        Random initial points\n        :return:\n        """"""\n        # Set\n        return np.random.random() * (math.pi * 2.0)\n    # end random_initial_points\n\n    # Generate\n    def _generate(self):\n        """"""\n        Generate dataset\n        :return:\n        """"""\n        # List of samples\n        samples = list()\n\n        # For each sample\n        for i in range(self.n_samples):\n            # Tensor\n            sample = torch.zeros(self.sample_len, 1, dtype=self.dtype)\n\n            # Time steps\n            for t in range(self.sample_len):\n                sample[t, 0] = self.func(i * self.sample_len + t)\n            # end for\n\n            # Append\n            samples.append(sample)\n        # end for\n\n        return samples\n    # end _generate\n\n# end SinusoidalTimeseries\n'"
echotorch/datasets/SwitchAttractorDataset.py,4,"b'# -*- coding: utf-8 -*-\n#\n\n# Imports\nimport torch\nfrom torch.utils.data.dataset import Dataset\nimport numpy as np\n\n\n# Switch attractor dataset\nclass SwitchAttractorDataset(Dataset):\n    """"""\n    Generate a dataset where the reservoir must switch\n    between two attractors.\n    """"""\n\n    # Constructor\n    def __init__(self, sample_len, n_samples, seed=None):\n        """"""\n        Constructor\n        :param sample_len: Length of the time-series in time steps.\n        :param n_samples: Number of samples to generate.\n        :param system_order: th order NARMA\n        :param seed: Seed of random number generator.\n        """"""\n        # Properties\n        self.sample_len = sample_len\n        self.n_samples = n_samples\n\n        # Init seed if needed\n        if seed is not None:\n            torch.manual_seed(seed)\n        # end if\n\n        # Generate data set\n        self.inputs, self.outputs = self._generate()\n    # end __init__\n\n    #############################################\n    # OVERRIDE\n    #############################################\n\n    # Length\n    def __len__(self):\n        """"""\n        Length\n        :return:\n        """"""\n        return self.n_samples\n    # end __len__\n\n    # Get item\n    def __getitem__(self, idx):\n        """"""\n        Get item\n        :param idx:\n        :return:\n        """"""\n        return self.inputs[idx], self.outputs[idx]\n    # end __getitem__\n\n    ##############################################\n    # PRIVATE\n    ##############################################\n\n    # Generate\n    def _generate(self):\n        """"""\n        Generate dataset\n        :return:\n        """"""\n        inputs = list()\n        outputs = list()\n\n        # Generate each sample\n        for i in range(self.n_samples):\n            # Start end stop\n            start = np.random.randint(0, self.sample_len)\n            stop = np.random.randint(start, start + self.sample_len / 2)\n\n            # Limits\n            if stop >= self.sample_len:\n                stop = self.sample_len - 1\n            # end if\n\n            # Sample tensor\n            inp = torch.zeros(self.sample_len, 1)\n            out = torch.zeros(self.sample_len)\n\n            # Set inputs\n            inp[start, 0] = 1.0\n            inp[stop] = 1.0\n\n            # Set outputs\n            out[start:stop] = 1.0\n\n            # Add\n            inputs.append(inp)\n            outputs.append(out)\n        # end for\n\n        return inputs, outputs\n    # end _generate\n\n# end SwitchAttractorDataset\n'"
echotorch/datasets/__init__.py,0,"b""# -*- coding: utf-8 -*-\n#\n\n# Imports\nfrom .DatasetComposer import DatasetComposer\nfrom .HenonAttractor import HenonAttractor\nfrom .LambdaDataset import LambdaDataset\nfrom .LogisticMapDataset import LogisticMapDataset\nfrom .LorenzAttractor import LorenzAttractor\nfrom .MackeyGlassDataset import MackeyGlassDataset\nfrom .MemTestDataset import MemTestDataset\nfrom .NARMADataset import NARMADataset\nfrom .RosslerAttractor import RosslerAttractor\nfrom .SinusoidalTimeseries import SinusoidalTimeseries\nfrom .PeriodicSignalDataset import PeriodicSignalDataset\n\n__all__ = [\n   'DatasetComposer', 'HenonAttractor', 'LambdaDataset', 'LogisticMapDataset', 'LorenzAttractor', 'MackeyGlassDataset',\n   'MemTestDataset', 'NARMADataset', 'RosslerAttractor', 'SinusoidalTimeseries', 'PeriodicSignalDataset'\n]\n"""
echotorch/models/HNilsNet.py,1,"b'# -*- coding: utf-8 -*-\n#\n# File : echotorch/models/NilsNet.py\n# Description : A Hierarchical NilsNet module.\n# Date : 09th of April, 2018\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti, University of Neuch\xc3\xa2tel <nils.schaetti@unine.ch>\n\n# Imports\nimport torchvision\nimport torch.nn as nn\n\n\n# A Hierarchical NilsNet\nclass HNilsNet(nn.Module):\n    """"""\n    A Hierarchical NilsNet\n    """"""\n\n    # Constructor\n    def __init__(self):\n        """"""\n        Constructor\n        """"""\n        pass\n    # end __init__\n\n    # Forward\n    def forward(self):\n        """"""\n        Forward\n        :return:\n        """"""\n        pass\n    # end forward\n\n# end HNilsNet\n'"
echotorch/models/NilsNet.py,1,"b'# -*- coding: utf-8 -*-\n#\n# File : echotorch/models/NilsNet.py\n# Description : An NilsNet module.\n# Date : 09th of April, 2018\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti, University of Neuch\xc3\xa2tel <nils.schaetti@unine.ch>\n\n# Imports\nimport torchvision\nimport torch.nn as nn\nfrom echotorch import nn as ecnn\n\n\n# A NilsNet\nclass NilsNet(nn.Module):\n    """"""\n    A NilsNet\n    """"""\n\n    # Constructor\n    def __init__(self, reservoir_dim, sfa_dim, ica_dim, pretrained=False, feature_selector=\'resnet18\'):\n        """"""\n        Constructor\n        """"""\n        # Upper class\n        super(NilsNet, self).__init__()\n\n        # ResNet\n        if feature_selector == \'resnet18\':\n            self.feature_selector = torchvision.models.resnet18(pretrained=True)\n        elif feature_selector == \'resnet34\':\n            self.feature_selector = torchvision.models.resnet34(pretrained=True)\n        elif feature_selector == \'resnet50\':\n            self.feature_selector = torchvision.models.resnet50(pretrained=True)\n        elif feature_selector == \'alexnet\':\n            self.feature_selector = torchvision.models.alexnet(pretrained=True)\n        # end if\n\n        # Skip last layer\n        self.reservoir_input_dim = self.feature_selector.fc.in_features\n        self.feature_selector.fc = ecnn.Identity()\n\n        # Echo State Network\n        # self.esn = ecnn.ESNCell(input_dim=self.reservoir_input_dim, output_dim=reservoir_dim)\n\n        # Slow feature analysis layer\n        # self.sfa = ecnn.SFACell(input_dim=reservoir_dim, output_dim=sfa_dim)\n\n        # Independent Feature Analysis layer\n        # self.ica = ecnn.ICACell(input_dim=sfa_dim, output_dim=ica_dim)\n    # end __init__\n\n    # Forward\n    def forward(self, x):\n        """"""\n        Forward\n        :return:\n        """"""\n        # ResNet\n        return self.feature_selector(x)\n    # end forward\n\n# end NilsNet\n'"
echotorch/models/TNilsNet.py,0,b''
echotorch/models/__init__.py,0,"b'# -*- coding: utf-8 -*-\n#\n# File : echotorch/models/__init__.py\n# Description : Models init.\n# Date : 09th of April, 2018\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti, University of Neuch\xc3\xa2tel <nils.schaetti@unine.ch>\n\n# Imports\nfrom .HNilsNet import HNilsNet\nfrom .NilsNet import NilsNet\n'"
echotorch/nn/BDESN.py,4,"b'# -*- coding: utf-8 -*-\n#\n# File : echotorch/nn/ESN.py\n# Description : An Echo State Network module.\n# Date : 26th of January, 2018\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti, University of Neuch\xc3\xa2tel <nils.schaetti@unine.ch>\n\n""""""\nCreated on 26 January 2018\n@author: Nils Schaetti\n""""""\n\n# Imports\nimport torch.sparse\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom .BDESNCell import BDESNCell\nfrom .RRCell import RRCell\n\n\n# Bi-directional Echo State Network module\nclass BDESN(nn.Module):\n    """"""\n    Bi-directional Echo State Network module\n    """"""\n\n    # Constructor\n    def __init__(self, input_dim, hidden_dim, output_dim, leaky_rate=1.0, spectral_radius=0.9, bias_scaling=0,\n                 input_scaling=1.0, w=None, w_in=None, w_bias=None, sparsity=None, input_set=[1.0, -1.0],\n                 w_sparsity=None, nonlin_func=torch.tanh, learning_algo=\'inv\', ridge_param=0.0, create_cell=True):\n        """"""\n        Constructor\n        :param input_dim: Inputs dimension.\n        :param hidden_dim: Hidden layer dimension\n        :param output_dim: Reservoir size\n        :param spectral_radius: Reservoir\'s spectral radius\n        :param bias_scaling: Scaling of the bias, a constant input to each neuron (default: 0, no bias)\n        :param input_scaling: Scaling of the input weight matrix, default 1.\n        :param w: Internal weights matrix\n        :param w_in: Input-reservoir weights matrix\n        :param w_bias: Bias weights matrix\n        :param sparsity:\n        :param input_set:\n        :param w_sparsity:\n        :param nonlin_func: Reservoir\'s activation function (tanh, sig, relu)\n        :param learning_algo: Which learning algorithm to use (inv, LU, grad)\n        """"""\n        super(BDESN, self).__init__()\n\n        # Properties\n        self.output_dim = output_dim\n\n        # Recurrent layer\n        if create_cell:\n            self.esn_cell = BDESNCell(\n                input_dim=input_dim, hidden_dim=hidden_dim, spectral_radius=spectral_radius, bias_scaling=bias_scaling,\n                input_scaling=input_scaling, w=w, w_in=w_in, w_bias=w_bias, sparsity=sparsity, input_set=input_set,\n                w_sparsity=w_sparsity, nonlin_func=nonlin_func, leaky_rate=leaky_rate, create_cell=create_cell\n            )\n        # end if\n\n        # Ouput layer\n        self.output = RRCell(\n            input_dim=hidden_dim * 2, output_dim=output_dim, ridge_param=ridge_param, learning_algo=learning_algo\n        )\n    # end __init__\n\n    ###############################################\n    # PROPERTIES\n    ###############################################\n\n    # Hidden layer\n    @property\n    def hidden(self):\n        """"""\n        Hidden layer\n        :return:\n        """"""\n        return self.esn_cell.hidden\n    # end hidden\n\n    # Hidden weight matrix\n    @property\n    def w(self):\n        """"""\n        Hidden weight matrix\n        :return:\n        """"""\n        return self.esn_cell.w\n    # end w\n\n    # Input matrix\n    @property\n    def w_in(self):\n        """"""\n        Input matrix\n        :return:\n        """"""\n        return self.esn_cell.w_in\n    # end w_in\n\n    ###############################################\n    # PUBLIC\n    ###############################################\n\n    # Reset learning\n    def reset(self):\n        """"""\n        Reset learning\n        :return:\n        """"""\n        # Reset output layer\n        self.output.reset()\n\n        # Training mode again\n        self.train(True)\n    # end reset\n\n    # Output matrix\n    def get_w_out(self):\n        """"""\n        Output matrix\n        :return:\n        """"""\n        return self.output.w_out\n    # end get_w_out\n\n    # Set W\n    def set_w(self, w):\n        """"""\n        Set W\n        :param w:\n        :return:\n        """"""\n        self.esn_cell.w = w\n    # end set_w\n\n    # Forward\n    def forward(self, u, y=None):\n        """"""\n        Forward\n        :param u: Input signal.\n        :return: Output or hidden states\n        """"""\n        # Compute hidden states\n        hidden_states = self.esn_cell(u)\n\n        # Learning algorithm\n        return self.output(hidden_states, y)\n    # end forward\n\n    # Finish training\n    def finalize(self):\n        """"""\n        Finalize training with LU factorization\n        """"""\n        # Finalize output training\n        self.output.finalize()\n\n        # Not in training mode anymore\n        self.train(False)\n    # end finalize\n\n    # Reset hidden layer\n    def reset_hidden(self):\n        """"""\n        Reset hidden layer\n        :return:\n        """"""\n        self.esn_cell.reset_hidden()\n    # end reset_hidden\n\n    # Get W\'s spectral radius\n    def get_spectral_radius(self):\n        """"""\n        Get W\'s spectral radius\n        :return: W\'s spectral radius\n        """"""\n        return self.esn_cell.get_spectral_raduis()\n    # end spectral_radius\n\n# end BDESN\n'"
echotorch/nn/BDESNCell.py,7,"b'# -*- coding: utf-8 -*-\n#\n# File : echotorch/nn/ESN.py\n# Description : An Echo State Network module.\n# Date : 26th of January, 2018\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti, University of Neuch\xc3\xa2tel <nils.schaetti@unine.ch>\n\n""""""\nCreated on 26 January 2018\n@author: Nils Schaetti\n""""""\n\n# Imports\nimport torch.sparse\nimport torch\nimport torch.nn as nn\nfrom .LiESNCell import LiESNCell\nimport numpy as np\nfrom torch.autograd import Variable\n\n\n# Bi-directional Echo State Network module\nclass BDESNCell(nn.Module):\n    """"""\n    Bi-directional Echo State Network module\n    """"""\n\n    # Constructor\n    def __init__(self, input_dim, hidden_dim, spectral_radius=0.9, bias_scaling=0, input_scaling=1.0,\n                 w=None, w_in=None, w_bias=None, sparsity=None, input_set=[1.0, -1.0], w_sparsity=None,\n                 nonlin_func=torch.tanh,  leaky_rate=1.0, create_cell=True):\n        """"""\n        Constructor\n        :param input_dim: Inputs dimension.\n        :param hidden_dim: Hidden layer dimension\n        :param spectral_radius: Reservoir\'s spectral radius\n        :param bias_scaling: Scaling of the bias, a constant input to each neuron (default: 0, no bias)\n        :param input_scaling: Scaling of the input weight matrix, default 1.\n        :param w: Internal weights matrix\n        :param w_in: Input-reservoir weights matrix\n        :param w_bias: Bias weights matrix\n        :param sparsity:\n        :param input_set:\n        :param w_sparsity:\n        :param nonlin_func: Reservoir\'s activation function (tanh, sig, relu)\n        """"""\n        super(BDESNCell, self).__init__()\n\n        # Recurrent layer\n        if create_cell:\n            self.esn_cell = LiESNCell(leaky_rate, False, input_dim, hidden_dim, spectral_radius, bias_scaling,\n                                      input_scaling, w, w_in, w_bias, None, sparsity, input_set, w_sparsity,\n                                      nonlin_func)\n        # end if\n    # end __init__\n\n    ###############################################\n    # PROPERTIES\n    ###############################################\n\n    # Hidden weight matrix\n    @property\n    def w(self):\n        """"""\n        Hidden weight matrix\n        :return:\n        """"""\n        return self.esn_cell.w\n    # end w\n\n    # Input matrix\n    @property\n    def w_in(self):\n        """"""\n        Input matrix\n        :return:\n        """"""\n        return self.esn_cell.w_in\n    # end w_in\n\n    ###############################################\n    # PUBLIC\n    ###############################################\n\n    # Reset learning\n    def reset(self):\n        """"""\n        Reset learning\n        :return:\n        """"""\n        # Reset output layer\n        self.output.reset()\n\n        # Training mode again\n        self.train(True)\n    # end reset\n\n    # Output matrix\n    def get_w_out(self):\n        """"""\n        Output matrix\n        :return:\n        """"""\n        return self.output.w_out\n    # end get_w_out\n\n    # Set W\n    def set_w(self, w):\n        """"""\n        Set W\n        :param w:\n        :return:\n        """"""\n        self.esn_cell.w = w\n    # end set_w\n\n    # Forward\n    def forward(self, u, y=None):\n        """"""\n        Forward\n        :param u: Input signal.\n        :param y: Target outputs\n        :return: Output or hidden states\n        """"""\n        # Forward compute hidden states\n        forward_hidden_states = self.esn_cell(u)\n\n        # Backward compute hidden states\n        backward_hidden_states = self.esn_cell(Variable(torch.from_numpy(np.flip(u.data.numpy(), 1).copy())))\n        backward_hidden_states = Variable(torch.from_numpy(np.flip(backward_hidden_states.data.numpy(), 1).copy()))\n\n        return torch.cat((forward_hidden_states, backward_hidden_states), dim=2)\n    # end forward\n\n    # Finish training\n    def finalize(self):\n        """"""\n        Finalize training with LU factorization\n        """"""\n        # Finalize output training\n        self.output.finalize()\n\n        # Not in training mode anymore\n        self.train(False)\n    # end finalize\n\n    # Reset hidden layer\n    def reset_hidden(self):\n        """"""\n        Reset hidden layer\n        :return:\n        """"""\n        self.esn_cell.reset_hidden()\n    # end reset_hidden\n\n    # Get W\'s spectral radius\n    def get_spectral_radius(self):\n        """"""\n        Get W\'s spectral radius\n        :return: W\'s spectral radius\n        """"""\n        return self.esn_cell.get_spectral_raduis()\n    # end spectral_radius\n\n# end BDESNCell\n'"
echotorch/nn/BDESNPCA.py,6,"b'# -*- coding: utf-8 -*-\n#\n# File : echotorch/nn/ESN.py\n# Description : An Echo State Network module.\n# Date : 26th of January, 2018\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti, University of Neuch\xc3\xa2tel <nils.schaetti@unine.ch>\n\n""""""\nCreated on 26 January 2018\n@author: Nils Schaetti\n""""""\n\n# Imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .BDESNCell import BDESNCell\nfrom sklearn.decomposition import IncrementalPCA\nimport matplotlib.pyplot as plt\nfrom torch.autograd import Variable\n\n\n# Bi-directional Echo State Network module with PCA reduction\nclass BDESNPCA(nn.Module):\n    """"""\n    Bi-directional Echo State Network module with PCA reduction\n    """"""\n\n    # Constructor\n    def __init__(self, input_dim, hidden_dim, output_dim, pca_dim, linear_dim, leaky_rate=1.0, spectral_radius=0.9, bias_scaling=0,\n                 input_scaling=1.0, w=None, w_in=None, w_bias=None, sparsity=None, input_set=[1.0, -1.0],\n                 w_sparsity=None, nonlin_func=torch.tanh, learning_algo=\'inv\', ridge_param=0.0, create_cell=True,\n                 pca_batch_size=10):\n        """"""\n        Constructor\n        :param input_dim: Inputs dimension.\n        :param hidden_dim: Hidden layer dimension\n        :param output_dim: Reservoir size\n        :param spectral_radius: Reservoir\'s spectral radius\n        :param bias_scaling: Scaling of the bias, a constant input to each neuron (default: 0, no bias)\n        :param input_scaling: Scaling of the input weight matrix, default 1.\n        :param w: Internal weights matrix\n        :param w_in: Input-reservoir weights matrix\n        :param w_bias: Bias weights matrix\n        :param sparsity:\n        :param input_set:\n        :param w_sparsity:\n        :param nonlin_func: Reservoir\'s activation function (tanh, sig, relu)\n        :param learning_algo: Which learning algorithm to use (inv, LU, grad)\n        """"""\n        super(BDESNPCA, self).__init__()\n\n        # Properties\n        self.output_dim = output_dim\n        self.pca_dim = pca_dim\n\n        # Recurrent layer\n        if create_cell:\n            self.esn_cell = BDESNCell(\n                input_dim=input_dim, hidden_dim=hidden_dim, spectral_radius=spectral_radius, bias_scaling=bias_scaling,\n                input_scaling=input_scaling, w=w, w_in=w_in, w_bias=w_bias, sparsity=sparsity, input_set=input_set,\n                w_sparsity=w_sparsity, nonlin_func=nonlin_func, leaky_rate=leaky_rate, create_cell=create_cell\n            )\n        # end if\n\n        # PCA\n        self.ipca = IncrementalPCA(n_components=pca_dim, batch_size=pca_batch_size)\n\n        # FFNN output\n        self.linear1 = nn.Linear(pca_dim, linear_dim)\n        self.linear2 = nn.Linear(linear_dim, output_dim)\n    # end __init__\n\n    ###############################################\n    # PROPERTIES\n    ###############################################\n\n    # Hidden layer\n    @property\n    def hidden(self):\n        """"""\n        Hidden layer\n        :return:\n        """"""\n        return self.esn_cell.hidden\n    # end hidden\n\n    # Hidden weight matrix\n    @property\n    def w(self):\n        """"""\n        Hidden weight matrix\n        :return:\n        """"""\n        return self.esn_cell.w\n    # end w\n\n    # Input matrix\n    @property\n    def w_in(self):\n        """"""\n        Input matrix\n        :return:\n        """"""\n        return self.esn_cell.w_in\n    # end w_in\n\n    ###############################################\n    # PUBLIC\n    ###############################################\n\n    # Reset learning\n    def reset(self):\n        """"""\n        Reset learning\n        :return:\n        """"""\n        # Reset output layer\n        self.output.reset()\n\n        # Training mode again\n        self.train(True)\n    # end reset\n\n    # Output matrix\n    def get_w_out(self):\n        """"""\n        Output matrix\n        :return:\n        """"""\n        return self.output.w_out\n    # end get_w_out\n\n    # Set W\n    def set_w(self, w):\n        """"""\n        Set W\n        :param w:\n        :return:\n        """"""\n        self.esn_cell.w = w\n    # end set_w\n\n    # Forward\n    def forward(self, u, y=None):\n        """"""\n        Forward\n        :param u: Input signal.\n        :return: Output or hidden states\n        """"""\n        # Compute hidden states\n        hidden_states = self.esn_cell(u)\n\n        # Resulting reduced stated\n        pca_states = torch.zeros(1, hidden_states.size(1), self.pca_dim)\n\n        # For each batch\n        pca_states[0] = torch.from_numpy(self.ipca.fit_transform(hidden_states.data[0].numpy()).copy())\n        pca_states = Variable(pca_states)\n\n        # FFNN output\n        return F.relu(self.linear2(F.relu(self.linear1(pca_states))))\n    # end forward\n\n    # Finish training\n    def finalize(self):\n        """"""\n        Finalize training with LU factorization\n        """"""\n        # Finalize output training\n        self.output.finalize()\n\n        # Not in training mode anymore\n        self.train(False)\n    # end finalize\n\n    # Reset hidden layer\n    def reset_hidden(self):\n        """"""\n        Reset hidden layer\n        :return:\n        """"""\n        self.esn_cell.reset_hidden()\n    # end reset_hidden\n\n    # Get W\'s spectral radius\n    def get_spectral_radius(self):\n        """"""\n        Get W\'s spectral radius\n        :return: W\'s spectral radius\n        """"""\n        return self.esn_cell.get_spectral_raduis()\n    # end spectral_radius\n\n# end BDESNPCA\n'"
echotorch/nn/Conceptor.py,48,"b'# -*- coding: utf-8 -*-\n#\n# File : echotorch/nn/ESN.py\n# Description : An Echo State Network module.\n# Date : 26th of January, 2018\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti <nils.schaetti@unine.ch>\n\n""""""\nCreated on 26 January 2018\n@author: Nils Schaetti\n""""""\n\n# Imports\nimport torch.sparse\nimport torch\nfrom .RRCell import RRCell\nimport math\nfrom echotorch.utils import generalized_squared_cosine\nimport math as m\nfrom torch.autograd import Variable\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n# Conceptor\nclass Conceptor(RRCell):\n    """"""\n    Conceptor\n    """"""\n\n    # Constructor\n    def __init__(self, conceptor_dim, aperture=0.0, learning_algo=\'inv\', name="""", conceptor_matrix=None, dtype=torch.float32):\n        """"""\n        Constructor\n        :param input_dim: Inputs dimension.\n        :param output_dim: Reservoir size\n        """"""\n        super(Conceptor, self).__init__(conceptor_dim, conceptor_dim, ridge_param=aperture, feedbacks=False, with_bias=False, learning_algo=learning_algo, softmax_output=False, dtype=dtype)\n\n        # Properties\n        self.conceptor_dim = conceptor_dim\n        self.aperture = aperture\n        self.name = name\n        self.n_samples = 0.0\n        self.attenuation = 0.0\n\n        # Set it as buffer\n        self.register_buffer(\'R\', Variable(torch.zeros(self.x_size, self.x_size, dtype=self.dtype), requires_grad=False))\n        self.register_buffer(\'C\', Variable(torch.zeros(1, conceptor_dim, dtype=self.dtype), requires_grad=False))\n\n        # Set conceptor\n        if conceptor_matrix is not None:\n            self.C = conceptor_matrix\n            self.train(False)\n        # end if\n    # end __init__\n\n    ###############################################\n    # PROPERTIES\n    ###############################################\n\n    # Compute quota\n    @property\n    def quota(self):\n        """"""\n        Compute quota\n        :return:\n        """"""\n        # Conceptor matrix\n        conceptor_matrix = self.get_C()\n\n        # Compute sum of singular values devided by number of neurons\n        return float(torch.sum(conceptor_matrix.mm(torch.eye(self.conceptor_dim, dtype=self.dtype))) / self.conceptor_dim)\n    # end quota\n\n    ###############################################\n    # PUBLIC\n    ###############################################\n\n    # Plot 2D ellipse\n    def plot(self, colorstring, linewidth=3, resolution=200, dim=\'2d\'):\n        """"""\n        Plot 2D ellipse\n        :return:\n        """"""\n        # 2D or 3D ?\n        if dim == \'2d\':\n            Conceptor.plot_ellipse_2D(self.get_C(), colorstring, linewidth, resolution)\n        else:\n            pass\n        # end if\n    # end plot\n\n    # Clone\n    def clone(self):\n        """"""\n        Clone\n        :return:\n        """"""\n        return Conceptor(conceptor_dim=self.conceptor_dim, aperture=self.aperture, name=self.name, conceptor_matrix=self.C, dtype=self.dtype)\n    # end clone\n\n    # Show the conceptor matrix\n    def show(self):\n        """"""\n        Show the conceptor matrix\n        :return:\n        """"""\n        plt.imshow(self.C, cmap=\'Greys\')\n        plt.show()\n    # end show\n\n    # Change aperture\n    def set_aperture(self, new_a):\n        """"""\n        Change aperture\n        :param new_a:\n        :return:\n        """"""\n        self.C = Conceptor.phi_function(self.C, new_a / self.aperture)\n        self.aperture = new_a\n    # end set_aperture\n\n    # Multiply aperture\n    def multiply_aperture(self, factor):\n        """"""\n        Multiply aperture\n        :param factor:\n        :return:\n        """"""\n        self.C = Conceptor.phi_function(self.C, factor)\n        self.aperture *= factor\n    # end multiply_aperture\n\n    # Plot delta measure\n    def plot_delta_measure(self, start, end, steps=50):\n        """"""\n        Plot delta measure\n        :param start:\n        :param end:\n        :return:\n        """"""\n        # Gamma values\n        gamma_values = torch.logspace(start=start, end=end, steps=steps)\n\n        # Log10 of gamma values\n        gamma_log_values = torch.log10(gamma_values)\n\n        # Delta measures\n        C_norms = torch.zeros(steps)\n        delta_scores = torch.zeros(steps)\n\n        # For each gamma measure\n        for i, gamma in enumerate(gamma_values):\n            delta_scores[i], C_norms[i] = self.delta_measure(float(gamma), epsilon=0.1)\n        # end for\n\n        # Plot\n        plt.plot(gamma_log_values.numpy(), delta_scores.numpy())\n        plt.plot(gamma_log_values.numpy(), C_norms.numpy())\n        plt.show()\n    # end plot_delta_measure\n\n    # Compute Delta measure\n    def delta_measure(self, gamma, epsilon=0.01):\n        """"""\n        Compute Delta measure\n        :param gamma:\n        :param epsilon:\n        :return:\n        """"""\n        # Conceptor matrix for both sides\n        A = Conceptor.phi_function(self.C, gamma - epsilon)\n        B = Conceptor.phi_function(self.C, gamma + epsilon)\n\n        # Gradient in Frobenius norm of matrix\n        A_norm = math.pow(torch.norm(A, p=2), 2)\n        B_norm = math.pow(torch.norm(B, p=2), 2)\n        d_C_norm = B_norm - A_norm\n\n        # Change in log(gamma)\n        d_log_gamma = np.log(gamma + epsilon) - np.log(gamma - epsilon)\n        """"""if d_C_norm / d_log_gamma > 50.0:\n            print(A)\n            print(B)\n            print(torch.norm(A, p=2))\n            print(torch.norm(B, p=2))\n            print(d_C_norm)\n            print(gamma)\n            print(d_C_norm / d_log_gamma)\n            exit()\n        # end if""""""\n        return d_C_norm / d_log_gamma, d_C_norm\n    # end delta_measure\n\n    # Output matrix\n    def get_C(self):\n        """"""\n        Output matrix\n        :return:\n        """"""\n        return self.C\n    # end get_w_out\n\n    # Forward\n    def forward(self, x, y=None):\n        """"""\n        Forward\n        :param x: Input signal.\n        :param y: Target outputs\n        :return: Output or hidden states\n        """"""\n        # Batch size\n        batch_size = x.size(0)\n\n        # Time length\n        time_length = x.size(1)\n\n        # Learning algo\n        if self.training:\n            for b in range(batch_size):\n                Rj = x[b].t().mm(x[b]) / time_length\n                self.R.data.add_(Rj.data)\n                self.n_samples += 1.0\n            # end for\n            return x\n        elif not self.training:\n            # Outputs\n            outputs = Variable(torch.zeros(batch_size, time_length, self.output_dim, dtype=self.dtype), requires_grad=False)\n            outputs = outputs.cuda() if self.C.is_cuda else outputs\n\n            # For each batch\n            for b in range(batch_size):\n                outputs[b] = torch.mm(x[b], self.C)\n            # end for\n\n            # Compute attenuation\n            self.attenuation = torch.mean(torch.pow(torch.abs(x - outputs), 2)) / torch.mean(torch.pow(torch.abs(x), 2))\n\n            return outputs\n        # end if\n    # end forward\n\n    # Finish training\n    def finalize(self):\n        """"""\n        Finalize training with LU factorization or Pseudo-inverse\n        """"""\n        # Average\n        self.R = self.R / self.n_samples\n\n        # SVF\n        (U, S, V) = torch.svd(self.R)\n\n        # Compute new singular values\n        Snew = torch.mm(torch.diag(S), torch.inverse(torch.diag(S) + math.pow(self.aperture, -2) * torch.eye(self.input_dim, dtype=self.dtype)))\n\n        # Apply new SVs to get the conceptor\n        self.C.data = torch.mm(torch.mm(U, Snew), U.t()).data\n\n        # Not in training mode anymore\n        self.train(False)\n    # end finalize\n\n    # Set conceptor\n    def set_conceptor(self, c):\n        """"""\n        Set conceptor\n        :param c:\n        :return:\n        """"""\n        # Set matrix\n        self.w_out.data = c\n    # end set_conceptor\n\n    # Singular values\n    def singular_values(self):\n        """"""\n        Singular values\n        :return:\n        """"""\n        # Compute SVD\n        (Ua, Sa, Va) = torch.svd(self.get_C())\n        return Ua, torch.diag(Sa), Va\n    # end singular_values\n\n    # Some of singular values\n    def get_quota(self):\n        """"""\n        Sum of singular values\n        :return:\n        """"""\n        return float(torch.sum(self.singular_values()))\n    # end get_quota\n\n    ###############################################\n    # STATIC\n    ###############################################\n\n    # Plot 2D ellipse\n    @staticmethod\n    def plot_ellipse_2D(A, colorstring, linewidth=3, resolution=200):\n        """"""\n        Plot 2D ellipse\n        :return:\n        """"""\n        # Plot cross and circle\n        plt.plot([-1, 1], [0, 0], \'--\', color=\'black\', linewidth=1)\n        plt.plot([0, 0], [-1, 1], \'--\', color=\'black\', linewidth=1)\n        plt.plot(\n            np.cos(2.0 * math.pi * np.arange(200) / 200.0),\n            np.sin(2.0 * math.pi * np.arange(200) / 200),\n            \'-\',\n            color=\'black\',\n            linewidth=1\n        )\n\n        # Compute the ellipse representing the correlation matrix\n        circ_points = torch.from_numpy(np.array([\n            np.cos(2.0 * math.pi * np.arange(0, resolution) / resolution),\n            np.sin(2.0 * math.pi * np.arange(0, resolution) / resolution)\n        ]))\n\n        # Transform the circle\n        E1 = torch.mm(A, circ_points)\n\n        # SVD on A\n        (U, S, Ut) = torch.svd(A)\n\n        # Plot singular value 1\n        plt.plot(\n            S[0].item() * np.array([0., U[0, 0]]),\n            S[0].item() * np.array([0., U[1, 0]]),\n            linewidth=linewidth,\n            color=colorstring\n        )\n\n        # Plot singular value 2\n        plt.plot(\n            S[1].item() * np.array([0., U[0, 1]]),\n            S[1].item() * np.array([0., U[1, 1]]),\n            linewidth=linewidth,\n            color=colorstring\n        )\n\n        # Plot ellipse\n        plt.plot(E1[0, :].numpy(), E1[1, :].numpy(), linewidth=linewidth, color=colorstring)\n    # end plot_ellipse_2D\n\n    # Multiply aperture matrix\n    @staticmethod\n    def phi_function(C, gamma):\n        """"""\n        Multiply aperture matrix\n        :param c:\n        :param gamma:\n        :return:\n        """"""\n        # Conceptor matrix\n        c = C.clone()\n        conceptor_dim = c.shape[0]\n        dtype = c.dtype\n\n        # New tensor\n        return c.mm(torch.inverse(c + m.pow(gamma, -2) * (torch.eye(conceptor_dim, dtype=dtype) - c)))\n    # end phi_function\n\n    # Morphing patterns\n    @staticmethod\n    def morphing(conceptor_list, mu):\n        """"""\n        Morphing pattern\n        :param conceptor_list:\n        :return:\n        """"""\n        # For each conceptors\n        for i, c in enumerate(conceptor_list):\n            if i == 0:\n                M = c.mul(mu[i])\n            else:\n                M += c.mul(mu[i])\n            # end if\n        # end for\n        return M\n    # end for\n\n    # Similarity between two conceptors\n    @staticmethod\n    def similarity(C1, C2):\n        """"""\n        Similarity between two conceptors\n        :param C1:\n        :param C2:\n        :return:\n        """"""\n        # Compute singular values\n        Ua, Sa, _ = torch.svd(C1.get_C())\n        Ub, Sb, _ = torch.svd(C2.get_C())\n\n        # Measure\n        return generalized_squared_cosine(Sa, Ua, Sb, Ub)\n    # end similarity\n\n    ###############################################\n    # OPERATORS\n    ###############################################\n\n    # Similarity with another conceptor\n    def sim(self, cb, measure=\'gsc\'):\n        """"""\n        Similarity with another conceptor\n        :param cb:\n        :return:\n        """"""\n        # Compute singular values\n        Ua, Sa, _ = torch.svd(self.C)\n        Ub, Sb, _ = torch.svd(cb.get_C())\n\n        # Measure\n        if measure == \'gsc\':\n            return generalized_squared_cosine(Sa, Ua, Sb, Ub)\n        # end if\n    # end sim\n\n    # Positive evidence\n    def E_plus(self, x):\n        """"""\n        Positive evidence\n        :param x: states (x)\n        :return:\n        """"""\n        return x.mm(self.w_out).mm(x.t())\n    # end E_plus\n\n    # Evidence against\n    def E_neg(self, x, conceptor_list):\n        """"""\n        Evidence against\n        :param x:\n        :param conceptor_list:\n        :return:\n        """"""\n        # For each conceptor in the list\n        for i, c in enumerate(conceptor_list):\n            if i == 0:\n                new_c = c\n            else:\n                new_c = new_c.logical_or(c)\n            # end if\n        # end for\n\n        # Take the not\n        N = new_c.logical_not()\n\n        return x.t().mm(N.w_out).mm(x)\n    # end E_neg\n\n    # Evidence\n    def E(self, x, conceptor_list):\n        """"""\n        Evidence\n        :param x:\n        :param conceptor_list:\n        :return:\n        """"""\n        return self.E_plus(x) + self.E_neg(x, conceptor_list)\n    # end E\n\n    # OR\n    def logical_or(self, c):\n        """"""\n        Logical OR\n        :param c:\n        :return:\n        """"""\n        # Matrices\n        C = self.C\n        B = c.get_C()\n        I = torch.eye(self.conceptor_dim, dtype=self.dtype)\n\n        # Compute C1 \\/ C2\n        conceptor_matrix = torch.inverse(I + torch.inverse(C.mm(torch.inverse(I - C)) + B.mm(torch.inverse(I - B))))\n\n        # Set conceptor\n        new_c = Conceptor(\n            conceptor_dim=self.conceptor_dim,\n            conceptor_matrix=conceptor_matrix,\n            name=""({} OR {})"".format(self.name, c.name),\n            aperture=math.sqrt(math.pow(self.aperture, 2) + math.pow(c.aperture, 2)),\n            dtype=self.dtype\n        )\n\n        return new_c\n    # end logical_or\n\n    # OR\n    def __or__(self, other):\n        """"""\n        OR\n        :param other:\n        :return:\n        """"""\n        return self.logical_or(other)\n    # end __or__\n\n    # NOT\n    def logical_not(self):\n        """"""\n        Logical NOT\n        :param c:\n        :return:\n        """"""\n        # Matrices\n        C = self.C\n\n        # Compute not C\n        conceptor_matrix = torch.eye(self.conceptor_dim, dtype=self.dtype) - C\n\n        # Set conceptor\n        new_c = Conceptor(\n            conceptor_dim=self.conceptor_dim,\n            conceptor_matrix=conceptor_matrix,\n            name=""NOT {}"".format(self.name),\n            aperture=1.0 / self.aperture,\n            dtype=self.dtype\n        )\n\n        return new_c\n    # end logical_not\n\n    # Not\n    def __invert__(self):\n        """"""\n        NOT\n        :return:\n        """"""\n        return self.logical_not()\n    # end __invert__\n\n    # AND\n    def logical_and(self, C2):\n        """"""\n        Logical AND\n        :param c:\n        :return:\n        """"""\n        # Get conceptor matrix\n        A = self.get_C()\n        B = C2.get_C()\n\n        # Dimension\n        dim = A.shape[0]\n        tol = 1e-14\n\n        # SV on both conceptor\n        (UC, SC, UtC) = torch.svd(A)\n        (UB, SB, UtB) = torch.svd(B)\n\n        # Get singular values\n        dSC = SC\n        dSB = SB\n\n        # How many non-zero singular values\n        numRankC = int(torch.sum(1.0 * (dSC > tol)))\n        numRankB = int(torch.sum(1.0 * (dSB > tol)))\n\n        if numRankC < dim and numRankB < dim:\n            # Select zero singular vector\n            UC0 = UC[:, numRankC:]\n            UB0 = UB[:, numRankB:]\n\n            # SVD on UC0 + UB0\n            (W, Sigma, Wt) = torch.svd(torch.mm(UC0, UC0.t()) + torch.mm(UB0, UB0.t()))\n\n            # Number of non-zero SV\n            numRankSigma = int(torch.sum(1.0 * (Sigma > tol)))\n\n            # Select zero singular vector\n            Wgk = W[:, numRankSigma:]\n\n            # C and B\n            # Wgk * (Wgk^T * (C^-1 + B^-1 - I) * Wgk)^-1 * Wgk^T\n            CandB = np.dot(\n                np.dot(\n                    Wgk,\n                    torch.inverse(\n                        np.dot(\n                            np.dot(\n                                Wgk.T,\n                                (torch.pinverse(A, tol) + torch.pinverse(B, tol) - np.eye(dim))\n                            ),\n                            Wgk\n                        )\n                    )\n                ),\n                Wgk.T\n            )\n        else:\n            # C and B\n            # Wgk * (Wgk^T * (C^-1 + B^-1 - I) * Wgk)^-1 * Wgk^T\n            CandB = torch.pinverse(A, tol) + torch.pinverse(B, tol) - torch.eye(dim, dtype=self.dtype)\n        # end if\n\n        # Set conceptor\n        new_c = Conceptor(\n            conceptor_dim=self.conceptor_dim,\n            conceptor_matrix=CandB,\n            name=""({} AND {})"".format(self.name, C2.name),\n            aperture=math.pow(math.pow(self.aperture, -2) + math.pow(C2.aperture, -2), -0.5),\n            dtype=self.dtype\n        )\n\n        return new_c\n    # end logical_and\n\n    # AND\n    def __and__(self, other):\n        """"""\n        AND\n        :param other:\n        :return:\n        """"""\n        return self.logical_and(other)\n    # end __and__\n\n    # Multiply\n    def mul(self, other):\n        """"""\n        Multiply\n        :param other:\n        :return:\n        """"""\n        # Multiply matrix\n        if type(other) is Conceptor:\n            new_c = self.get_C() * other.get_C()\n        else:\n            new_c = self.get_C() * other\n        # end if\n\n        # New conceptor\n        return Conceptor(self.conceptor_dim, self.aperture, conceptor_matrix=new_c, dtype=self.dtype)\n    # end mul\n\n    # Multiply\n    def __mul__(self, other):\n        """"""\n        Multiply\n        :param other:\n        :return:\n        """"""\n        # Multiply matrix\n        if type(other) is Conceptor:\n            new_c = self.get_C() * other.get_C()\n        else:\n            new_c = self.get_C() * other\n        # end if\n\n        # New conceptor\n        return Conceptor(self.conceptor_dim, self.aperture, conceptor_matrix=new_c, dtype=self.dtype)\n    # end __mul__\n\n    # Multiply\n    def __rmul__(self, other):\n        """"""\n        Multiply\n        :param other:\n        :return:\n        """"""\n        # Multiply matrix\n        if type(other) is Conceptor:\n            new_c = self.get_C() * other.get_C()\n        else:\n            new_c = self.get_C() * other\n        # end if\n\n        # New conceptor\n        return Conceptor(self.conceptor_dim, self.aperture, conceptor_matrix=new_c, dtype=self.dtype)\n    # end __mul__\n\n    # Override *=\n    def __imul__(self, other):\n        """"""\n        *=\n        :param other:\n        :return:\n        """"""\n        # Multiply matrix\n        if type(other) is Conceptor:\n            new_c = self.get_C() * other.get_C()\n        else:\n            new_c = self.get_C() * other\n        # end if\n\n        # New conceptor\n        return Conceptor(self.conceptor_dim, self.aperture, conceptor_matrix=new_c, dtype=self.dtype)\n    # end __imul__\n\n    # Add\n    def __add__(self, other):\n        """"""\n        Add\n        :param other:\n        :return:\n        """"""\n        # Add matrix\n        if type(other) is Conceptor:\n            new_c = self.get_C() + other.get_C()\n        else:\n            new_c = self.get_C() + other\n        # end if\n\n        # New conceptor\n        return Conceptor(self.conceptor_dim, self.aperture, conceptor_matrix=new_c, dtype=self.dtype)\n    # end __add__\n\n    # Add\n    def __radd__(self, other):\n        """"""\n        Add\n        :param other:\n        :return:\n        """"""\n        # Add matrix\n        if type(other) is Conceptor:\n            new_c = self.get_C() + other.get_C()\n        else:\n            new_c = self.get_C() + other\n        # end if\n\n        # New conceptor\n        return Conceptor(self.conceptor_dim, self.aperture, conceptor_matrix=new_c, dtype=self.dtype)\n    # end __radd__\n\n    # +=\n    def __iadd__(self, other):\n        """"""\n        +=\n        :param other:\n        :return:\n        """"""\n        # Add matrix\n        if type(other) is Conceptor:\n            new_c = self.get_C() + other.get_C()\n        else:\n            new_c = self.get_C() + other\n        # end if\n\n        # New conceptor\n        return Conceptor(self.conceptor_dim, self.aperture, conceptor_matrix=new_c, dtype=self.dtype)\n    # end __iadd__\n\n    # Greater or equal\n    def __ge__(self, other):\n        """"""\n        Greater or equal\n        :param other:\n        :return:\n        """"""\n        # Compute eigenvalues of a - b\n        eig_v = torch.eig(other.get_C() - self.w_out, eigenvectors=False)\n        return float(torch.max(eig_v)) >= 0.0\n    # end __ge__\n\n    # Greater\n    def __gt__(self, other):\n        """"""\n        Greater\n        :param other:\n        :return:\n        """"""\n        # Compute eigenvalues of a - b\n        eig_v = torch.eig(other.get_C() - self.w_out, eigenvectors=False)\n        return float(torch.max(eig_v)) > 0.0\n    # end __gt__\n\n    # Less\n    def __lt__(self, other):\n        """"""\n        Less than\n        :param other:\n        :return:\n        """"""\n        return not self >= other\n    # end __lt__\n\n    # Less or equal\n    def __le__(self, other):\n        """"""\n        Less or equal\n        :param other:\n        :return:\n        """"""\n        return not self > other\n    # end __le__\n\n# end RRCell\n'"
echotorch/nn/ConceptorNet.py,4,"b'# -*- coding: utf-8 -*-\n#\n# File : echotorch/nn/ConceptorNet.py\n# Description : A ESN-based Conceptor Network.\n# Date : 17th of January, 2019\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti, University of Neuch\xc3\xa2tel <nils.schaetti@unine.ch>\n\n""""""\nCreated on 17 January 2019\n@author: Nils Schaetti\n""""""\n\n# Imports\nimport torch\nimport torch.nn as nn\nfrom .ConceptorNetCell import ConceptorNetCell\nfrom .RRCell import RRCell\nimport matplotlib.pyplot as plt\n\n\n# ESN-based ConceptorNet\nclass ConceptorNet(nn.Module):\n    """"""\n    ESN-based ConceptorNet\n    """"""\n\n    # Constructor\n    def __init__(self, input_dim, hidden_dim, output_dim=None, spectral_radius=0.9, bias_scaling=0, input_scaling=1.0,\n                 w=None, w_in=None, w_bias=None, sparsity=None, input_set=[1.0, -1.0], w_sparsity=None,\n                 leaky_rate=1.0, nonlin_func=torch.tanh, learning_algo=\'inv\', ridge_param=0.0,\n                 with_bias=True, seed=None, washout=1, w_distrib=\'uniform\', win_distrib=\'uniform\',\n                 wbias_distrib=\'uniform\', win_normal=(0.0, 1.0), w_normal=(0.0, 1.0), wbias_normal=(0.0, 1.0),\n                 w_ridge_param=0.0, dtype=torch.float32):\n        """"""\n        Constructor\n        :param input_dim: Inputs dimension.\n        :param hidden_dim: Hidden layer dimension\n        :param spectral_radius: Reservoir\'s spectral radius\n        :param bias_scaling: Scaling of the bias, a constant input to each neuron (default: 0, no bias)\n        :param input_scaling: Scaling of the input weight matrix, default 1.\n        :param w: Internation weights matrix\n        :param w_in: Input-reservoir weights matrix\n        :param w_bias: Bias weights matrix\n        :param w_fdb: Feedback weights matrix\n        :param sparsity:\n        :param input_set:\n        :param w_sparsity:\n        :param nonlin_func: Reservoir\'s activation function (tanh, sig, relu)\n        :param learning_algo: Which learning algorithm to use (inv, LU, grad)\n        """"""\n        super(ConceptorNet, self).__init__()\n\n        # Properties\n        self.with_bias = with_bias\n        self.washout = washout\n        self.hidden_dim = hidden_dim\n\n        # Recurrent layer\n        self.esn_cell = ConceptorNetCell(leaky_rate, False, input_dim, hidden_dim, spectral_radius=spectral_radius,\n                                         bias_scaling=bias_scaling, input_scaling=input_scaling,\n                                         w=w, w_in=w_in, w_bias=w_bias, sparsity=sparsity, input_set=input_set,\n                                         w_sparsity=w_sparsity, nonlin_func=nonlin_func, feedbacks=False,\n                                         feedbacks_dim=input_dim, wfdb_sparsity=None,\n                                         normalize_feedbacks=False, seed=seed, w_distrib=w_distrib,\n                                         win_distrib=win_distrib, wbias_distrib=wbias_distrib, win_normal=win_normal,\n                                         w_normal=w_normal, wbias_normal=wbias_normal, dtype=dtype)\n        # end if\n\n        # Input recreation weights layer (Ridge regression)\n        self.input_recreation = RRCell(\n            hidden_dim,\n            hidden_dim,\n            w_ridge_param,\n            None,\n            with_bias=False,\n            learning_algo=learning_algo,\n            softmax_output=False,\n            averaged=True,\n            dtype=dtype\n        )\n\n        # Output (state observer)\n        if output_dim is not None:\n            self.output = RRCell(\n                hidden_dim,\n                output_dim,\n                ridge_param,\n                None,\n                with_bias=False,\n                learning_algo=learning_algo,\n                softmax_output=False,\n                averaged=True,\n                dtype=dtype\n            )\n        else:\n            self.output = None\n        # end if\n    # end __init__\n\n    ###############################################\n    # PROPERTIES\n    ###############################################\n\n    # Hidden layer\n    @property\n    def hidden(self):\n        """"""\n        Hidden layer\n        :return:\n        """"""\n        return self.esn_cell.hidden\n    # end hidden\n\n    # Hidden weight matrix\n    @property\n    def w(self):\n        """"""\n        Hidden weight matrix\n        :return:\n        """"""\n        return self.esn_cell.w\n    # end w\n\n    # Input matrix\n    @property\n    def w_in(self):\n        """"""\n        Input matrix\n        :return:\n        """"""\n        return self.esn_cell.w_in\n    # end w_in\n\n    # Input recreation matrix\n    @property\n    def input_recreation_matrix(self):\n        """"""\n        Input recreation matrix\n        :return:\n        """"""\n        return self.input_recreation.get_w_out()\n    # end input_recreation_matrix\n\n    ###############################################\n    # PRIVATE\n    ###############################################\n\n    # Arctanh\n    def arctanh(self, x):\n        """"""\n        Inverse tanh\n        :param x:\n        :return:\n        """"""\n        return 0.5 * torch.log((1 + x) / (1 - x))\n    # end arctanh\n\n    ###############################################\n    # PUBLIC\n    ###############################################\n\n    # Mode\n    def set_train(self):\n        """"""\n        Mode\n        :return:\n        """"""\n        self.train(True)\n        self.input_recreation.train(True)\n        if self.output is not None:\n            self.output.train(True)\n        # end if\n    # end set_train\n\n    # Reset learning\n    def reset(self):\n        """"""\n        Reset learning\n        :return:\n        """"""\n        # Reset output layer\n        self.output.reset()\n\n        # Training mode again\n        self.train(True)\n    # end reset\n\n    # Output matrix\n    def get_w_out(self):\n        """"""\n        Output matrix\n        :return:\n        """"""\n        return self.output.w_out\n    # end get_w_out\n\n    # Set W\n    def set_w(self, w):\n        """"""\n        Set W\n        :param w:\n        :return:\n        """"""\n        self.esn_cell.w = w\n    # end set_w\n\n    # Forward\n    def forward(self, u=None, y=None, c=None, reset_state=True, length=None, mu=None, return_states=False, x0=None):\n        """"""\n        Forward\n        :param u: Input signal.\n        :param y: Target outputs\n        :return: Output or hidden states\n        """"""\n        # Compute hidden states\n        if self.training:\n            hidden_states = self.esn_cell(\n                u,\n                reset_state=reset_state,\n                x0=x0\n            )\n\n            # Batch size and time length\n            batch_size = hidden_states.shape[0]\n\n            # X and Washout\n            x = hidden_states[:, self.washout:]\n            time_length = x.shape[1]\n\n            # Past hidden states\n            x_tilda = hidden_states[:, self.washout-1:-1]\n\n            # Bias\n            bias = self.esn_cell.w_bias[0].expand(batch_size, time_length, self.hidden_dim)\n\n            # Learning input recreation\n            self.input_recreation(x_tilda, self.arctanh(x) - bias)\n\n            # Learning state observer\n            if self.output is not None and y is not None:\n                self.output(x, y[:, self.washout:])\n            # end if\n\n            # Learning conceptor\n            return c(x)\n        elif c is None:\n            hidden_states = self.esn_cell(\n                u,\n                reset_state=reset_state,\n                x0=x0\n            )\n\n            # Return outputs or states\n            if self.output is not None and not return_states:\n                return self.output(hidden_states)\n            else:\n                return hidden_states\n            # end if\n        else:\n            hidden_states = self.esn_cell(\n                u=u,\n                reset_state=reset_state,\n                input_recreation=self.input_recreation,\n                conceptor=c,\n                length=length,\n                mu=mu,\n                x0=x0\n            )\n\n            # Return outputs of states\n            if self.output is not None:\n                return self.output(hidden_states)\n            else:\n                return self.input_recreation(hidden_states)\n            # end if\n        # end if\n    # end forward\n\n    # Finish training\n    def finalize(self, train=False):\n        """"""\n        Finalize training with LU factorization\n        """"""\n        # Finalize input recreation training\n        self.input_recreation.finalize()\n\n        # Finalize output\n        if self.output is not None:\n            self.output.finalize()\n        # end if\n\n        # Not in training mode anymore\n        self.train(train)\n    # end finalize\n\n    # Reset hidden layer\n    def reset_hidden(self):\n        """"""\n        Reset hidden layer\n        :return:\n        """"""\n        self.esn_cell.reset_hidden()\n    # end reset_hidden\n\n    # Get W\'s spectral radius\n    def get_spectral_radius(self):\n        """"""\n        Get W\'s spectral radius\n        :return: W\'s spectral radius\n        """"""\n        return self.esn_cell.get_spectral_raduis()\n    # end spectral_radius\n\n# end ESNCell\n'"
echotorch/nn/ConceptorNetCell.py,3,"b'# -*- coding: utf-8 -*-\n#\n# File : echotorch/nn/LiESNCell.py\n# Description : An Leaky-Integrated Echo State Network layer.\n# Date : 26th of January, 2018\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti <nils.schaetti@unine.ch>\n\n""""""\nCreated on 26 January 2018\n@author: Nils Schaetti\n""""""\n\nimport torch\nimport torch.sparse\nfrom torch.autograd import Variable\nfrom .LiESNCell import LiESNCell\nfrom .Conceptor import Conceptor\nfrom .ConceptorPool import ConceptorPool\n\n\n# Special reservoir layer for Conceptors\nclass ConceptorNetCell(LiESNCell):\n    """"""\n    Special reservoir layer for Conceptors\n    """"""\n\n    # Constructor\n    def __init__(self, *args, **kwargs):\n        """"""\n        Constructor\n        :param leaky_rate: Reservoir\'s leaky rate (default 1.0, normal ESN)\n        :param train_leaky_rate: Train leaky rate as parameter? (default: False)\n        """"""\n        super(ConceptorNetCell, self).__init__(*args, **kwargs)\n    # end __init__\n\n    ###############################################\n    # PUBLIC\n    ###############################################\n\n    # Forward\n    def forward(self, u=None, y=None, w_out=None, reset_state=True, input_recreation=None, conceptor=None, length=None, mu=None, x0=None):\n        """"""\n        Forward execution\n        :param u:\n        :param y:\n        :param w_out:\n        :param reset_state:\n        :param generative_mode:\n        :return:\n        """"""\n        # Time length\n        if u is not None:\n            time_length = int(u.size()[1])\n        else:\n            time_length = length\n        # end if\n\n        # Number of batches\n        if u is not None:\n            n_batches = int(u.size()[0])\n        else:\n            n_batches = 1\n        # end if\n\n        # Outputs\n        outputs = Variable(torch.zeros(n_batches, time_length, self.output_dim, dtype=self.dtype))\n        outputs = outputs.cuda() if self.hidden.is_cuda else outputs\n\n        # For each batch\n        for b in range(n_batches):\n            # Reset hidden layer\n            if x0 is not None:\n                self.set_hidden(x0)\n            elif reset_state:\n                self.reset_hidden()\n            # end if\n\n            # For each steps\n            for t in range(time_length):\n                # Generative mode ?\n                if self.training:\n                    # Current input\n                    ut = u[b, t]\n\n                    # Compute input layer\n                    u_win = self.w_in.mv(ut)\n\n                    # Apply W to x\n                    x_w = self.w.mv(self.hidden)\n\n                    # Add everything\n                    x = u_win + x_w + self.w_bias\n\n                    # Apply activation function\n                    x = self.nonlin_func(x)\n\n                    # Add to outputs\n                    self.hidden.data = x.view(-1).data\n\n                    # New last state\n                    outputs[b, t] = self.hidden\n                else:\n                    if u is not None:\n                        # Current input\n                        ut = u[b, t]\n\n                        # Compute input layer\n                        u_win = self.w_in.mv(ut)\n\n                        # Apply W to x\n                        x_w = self.w.mv(self.hidden)\n\n                        # Add everything\n                        x = u_win + x_w + self.w_bias\n                    else:\n                        # Apply W to x\n                        x_w = input_recreation(self.hidden.view(1, 1, -1))\n\n                        # Add everything\n                        x = x_w + self.w_bias\n                    # end if\n\n                    # Apply activation function\n                    x = self.nonlin_func(x)\n\n                    # Apply conceptor\n                    if type(conceptor) is Conceptor:\n                        xc = conceptor(x.view(1, 1, -1)).view(-1)\n                    elif type(conceptor) is ConceptorPool:\n                        # Apply morphing\n                        M = conceptor.morphing(mu[b, t])\n                        xc = M(x.view(1, 1, -1)).view(-1)\n                    else:\n                        xc = x.view(-1)\n                    # end if\n\n                    # New hidden\n                    self.hidden.data = xc.data\n\n                    # Add to outputs\n                    outputs[b, t] = self.hidden\n                # end if\n            # end for\n        # end for\n\n        return outputs\n    # end forward\n\n    ###############################################\n    # PRIVATE\n    ###############################################\n\n# end LiESNCell\n'"
echotorch/nn/ConceptorPool.py,16,"b'# -*- coding: utf-8 -*-\n#\n# File : echotorch/nn/ConceptorPool.py\n# Description : A pool of conceptor.\n# Date : 26th of January, 2018\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti <nils.schaetti@unine.ch>\n\n""""""\nCreated on 26 January 2018\n@author: Nils Schaetti\n""""""\n\n# Imports\nimport torch.sparse\nimport torch\nimport math\nfrom echotorch.utils import generalized_squared_cosine\nimport math as m\nfrom torch.autograd import Variable\nfrom .Conceptor import Conceptor\n\n\n# Conceptor\nclass ConceptorPool(object):\n    """"""\n    ConceptorPool\n    """"""\n\n    # Contructor\n    def __init__(self, conceptor_dim, conceptors=list(), esn=None, dtype=torch.float32):\n        """"""\n        Constructor\n        :param conceptors:\n        """"""\n        # Properties\n        self.conceptor_dim = conceptor_dim\n        self.conceptors = conceptors\n        self.name_to_conceptor = {}\n        self.esn = esn\n        self.dtype = dtype\n    # end __init__\n\n    ###############################################\n    # PROPERTIES\n    ###############################################\n\n    # Singular values of A\n    @property\n    def A_SV(self):\n        """"""\n        Singular values of A\n        :return:\n        """"""\n        return ConceptorPool.compute_A_SV(self.conceptors)\n    # end A_SV\n\n    # A (OR of all conceptors)\n    @property\n    def A(self):\n        return ConceptorPool.compute_A(self.conceptors)\n    # end A\n\n    # Quota\n    @property\n    def quota(self):\n        """"""\n        Quota\n        :return:\n        """"""\n        return ConceptorPool.compute_quota(self.conceptors)\n    # end quota\n\n    ###############################################\n    # PUBLIC\n    ###############################################\n\n    # Get similarity matrix\n    def similarity_matrix(self):\n        """"""\n        Get similarity matrix\n        :return:\n        """"""\n        return ConceptorPool.compute_similarity_matrix(self.conceptors)\n    # end similarity_matrix\n\n    # Finalize conceptor\n    def finalize_conceptor(self, i):\n        """"""\n        Finalize conceptor\n        :param i:\n        :return:\n        """"""\n        # Finalize\n        self.conceptors[i].finalize()\n    # end finalize_conceptor\n\n    # Finalize all conceptors\n    def finalize(self):\n        """"""\n        Finalize all conceptors\n        :return:\n        """"""\n        for c in self.conceptors:\n            c.finalize()\n        # end for\n    # end finalize\n\n    # Positive evidence\n    def E_plus(self, p):\n        """"""\n        Positive evidence\n        :param x: states (x)\n        :return:\n        """"""\n        return ConceptorPool.compute_E_plus(self.conceptors, self.esn, p)\n    # end E_plus\n\n    # Evidence for other\n    def E_other(self, p):\n        """"""\n        Evidence for other\n        :param p:\n        :return:\n        """"""\n        # Sizes\n        batch_size = p.shape[0]\n        time_length = p.shape[1]\n\n        # List of evidences\n        evidences = torch.zeros(batch_size, time_length)\n\n        # Compute hidden states\n        x = self.esn(u=p, return_states=True)\n\n        # All conceptors\n        A = self.A\n\n        # Not A\n        N = A.logical_not()\n\n        # For each batch\n        for b in range(batch_size):\n            # Time\n            for t in range(time_length):\n                evidences[b, t] = torch.mm(x[b, t].view(1, -1), N.get_C()).mm(x[b, t].view(-1, 1))\n            # end for\n        # end for\n        return torch.mean(evidences, dim=1)\n    # end E_other\n\n    # Negative evidence\n    def E_neg(self, p):\n        """"""\n        Negative evidence\n        :param p:\n        :return:\n        """"""\n        # Sizes\n        batch_size = p.shape[0]\n        n_conceptors = len(self.conceptors)\n        time_length = p.shape[1]\n\n        # List of evidences\n        evidences = torch.zeros(batch_size, time_length, n_conceptors)\n\n        # Compute hidden states\n        x = self.esn(u=p, return_states=True)\n\n        # For each batch\n        for b in range(batch_size):\n            # Time\n            for i, c in enumerate(self.conceptors):\n                # List of all conceptor without c\n                other_c = list(self.conceptors)\n                other_c.remove(c)\n\n                # Compute A\n                A = ConceptorPool.compute_A(other_c)\n\n                # Not A\n                N = A.logical_not()\n\n                # For each conceptors\n                for t in range(time_length):\n                    evidences[b, t, i] = torch.mm(x[b, t].view(1, -1), N.get_C()).mm(\n                        x[b, t].view(-1, 1))\n                # end for\n            # end for\n        # end for\n        return torch.mean(evidences, dim=1)\n    # end E_neg\n\n    # Evidence for each conceptor\n    def E(self, p):\n        """"""\n        Evidence for each conceptor\n        :return:\n        """"""\n        return (self.E_plus(p) + self.E_neg(p)) / 2.0\n    # end E\n\n    # New conceptor\n    def add(self, aperture, name):\n        """"""\n        New conceptor\n        :param aperture: Aperture\n        :param name: Conceptor\'s name\n        :return: New conceptor\n        """"""\n        new_conceptor = Conceptor(self.conceptor_dim, aperture=aperture, name=name, dtype=self.dtype)\n        self.conceptors.append(new_conceptor)\n        self.name_to_conceptor[name] = new_conceptor\n        return new_conceptor\n    # end add\n\n    # Add an OR between conceptors\n    def add_or(self, i, j):\n        """"""\n        Add an OR between conceptors\n        :param i:\n        :param j:\n        :return:\n        """"""\n        self.append(self.conceptors[i].logical_or(self.conceptors[j]))\n    # end for\n\n    # Add an AND between conceptors\n    def add_and(self, i, j):\n        """"""\n        Add an AND between conceptors\n        :param i:\n        :param j:\n        :return:\n        """"""\n        self.append(self.conceptors[i].logical_and(self.conceptors[j]))\n    # end for\n\n    # Add a NOT of a conceptor\n    def add_not(self, i):\n        """"""\n        Add an OR between conceptors\n        :param i:\n        :return:\n        """"""\n        self.append(self.conceptors[i].logical_not())\n    # end for\n\n    # Add (C1 OR ... OR CN)\n    def add_A(self):\n        """"""\n        Add an OR between conceptors\n        :param i:\n        :return:\n        """"""\n        # Compute A\n        A = ConceptorPool.compute_A(self.conceptors)\n\n        # Append\n        self.append(A)\n    # end for\n\n    # Add NOT (C1 OR ... OR CN)\n    def add_Not_A(self):\n        """"""\n        Add an OR between conceptors\n        :param i:\n        :return:\n        """"""\n        # Compute A\n        A = ConceptorPool.compute_A(self.conceptors)\n\n        # Not A\n        N = A.logical_not()\n\n        # Append\n        self.append(N)\n    # end for\n\n    # Append a conceptor\n    def append(self, c):\n        """"""\n        Append a conceptor\n        :param c:\n        :return:\n        """"""\n        self.conceptors.append(c)\n        self.name_to_conceptor[c.name] = c\n    # end append\n\n    # Morphing patterns\n    def morphing(self, mu):\n        """"""\n        Morphing pattern\n        :param conceptor_list:\n        :return:\n        """"""\n        # For each conceptors\n        for i, c in enumerate(self.conceptors):\n            if i == 0:\n                M = c.mul(mu[i])\n            else:\n                M += c.mul(mu[i])\n                # end if\n        # end for\n        return M\n    # end for\n\n    ###############################################\n    # PRIVATE\n    ###############################################\n\n    # Get item\n    def __getitem__(self, item):\n        """"""\n        Get item\n        :param item:\n        :return:\n        """"""\n        if type(item) is int:\n            return self.conceptors[item]\n        elif type(item) is str:\n            return self.name_to_conceptor[item]\n        # end if\n    # end __getitem__\n\n    # Set item\n    def __setitem__(self, key, value):\n        """"""\n        Set item\n        :param key:\n        :param value:\n        :return:\n        """"""\n        self.conceptors[key] = value\n    # end __setitem__\n\n    # Length\n    def __len__(self):\n        """"""\n        Length\n        :return:\n        """"""\n        return len(self.conceptors)\n    # end __len__\n\n    ###############################################\n    # STATIC\n    ###############################################\n\n    # Get similarity matrix\n    @staticmethod\n    def compute_similarity_matrix(conceptors):\n        """"""\n        Get similarity matrix\n        :return:\n        """"""\n        # Similarity matrix\n        sim_matrix = torch.zeros(len(conceptors), len(conceptors))\n        for i, ca in enumerate(conceptors):\n            for j, cb in enumerate(conceptors):\n                sim_matrix[i, j] = ca.sim(cb)\n            # end for\n        # end for\n        return sim_matrix\n    # end similarity_matrix\n\n    # Positive evidence\n    @staticmethod\n    def compute_E_plus(conceptors, esn, p):\n        """"""\n        Positive evidence\n        :param x: states (x)\n        :return:\n        """"""\n        # Sizes\n        batch_size = p.shape[0]\n        n_conceptors = len(conceptors)\n        time_length = p.shape[1]\n\n        # List of evidences\n        evidences = torch.zeros(batch_size, time_length, n_conceptors)\n\n        # Compute hidden states\n        x = esn(u=p, return_states=True)\n\n        # For each batch\n        for b in range(batch_size):\n            # Time\n            for t in range(time_length):\n                # For each conceptors\n                for i, c in enumerate(conceptors):\n                    evidences[b, t, i] = torch.mm(x[b, t].view(1, -1), c.get_C()).mm(x[b, t].view(-1, 1))\n                # end for\n            # end for\n        # end for\n        return torch.mean(evidences, dim=1)\n    # end E_plus\n\n    # Get singular values of A\n    @staticmethod\n    def compute_A_SV(conceptors):\n        """"""\n        Get singular values of A\n        :param conceptors:\n        :return:\n        """"""\n        # A (OR of all conceptors)\n        A = ConceptorPool.compute_A(conceptors)\n\n        # Compute SVD\n        _, S, _ = torch.svd(A.get_C())\n\n        return S\n    # end compute_A_SV\n\n    # Compute A (OR of all conceptors\n    @staticmethod\n    def compute_A(conceptors):\n        """"""\n        Compute A (OR of all conceptors)\n        :param conceptors:\n        :return:\n        """"""\n        # OR for all conceptor\n        for i, c in enumerate(conceptors):\n            if i == 0:\n                A = c\n            else:\n                A = c.logical_or(A)\n            # end if\n        # end for\n        A.name = ""A""\n\n        return A\n    # end compute_A\n\n    # Compute quota\n    @staticmethod\n    def compute_quota(conceptors):\n        """"""\n        Compute quota\n        :param conceptors:\n        :return:\n        """"""\n        # Singular values of A\n        S = ConceptorPool.compute_A_SV(conceptors)\n\n        # Quota is the sum of SV\n        return float(\n            torch.mean(S)\n        )\n    # end compute_quota\n\n# end __ConceptorPool__\n'"
echotorch/nn/EESN.py,3,"b'# -*- coding: utf-8 -*-\n#\n# File : echotorch/nn/EESN.py\n# Description : An ESN with an embedding layer at the beginning.\n# Date : 22 March, 2018\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti <nils.schaetti@unine.ch>\n\nimport torch\nimport torch.sparse\nimport torch.nn as nn\nfrom .LiESN import LiESN\n\n\n# An ESN with an embedding layer\nclass EESN(object):\n    """"""\n    An ESN with an embedding layer\n    """"""\n\n    # Constructor\n    def __init__(self, voc_size, embedding_dim, hidden_dim, output_dim, spectral_radius=0.9,\n                 bias_scaling=0, input_scaling=1.0, w=None, w_in=None, w_bias=None, sparsity=None,\n                 input_set=[1.0, -1.0], w_sparsity=None, nonlin_func=torch.tanh, learning_algo=\'inv\', ridge_param=0.0,\n                 leaky_rate=1.0, train_leaky_rate=False, feedbacks=False, wfdb_sparsity=None,\n                 normalize_feedbacks=False):\n        # Embedding layer\n        self.embedding = nn.Embedding(voc_size, embedding_dim)\n\n        # Li-ESN\n        self.esn = LiESN(embedding_dim, hidden_dim, output_dim, spectral_radius, bias_scaling, input_scaling,\n                         w, w_in, w_bias, sparsity, input_set, w_sparsity, nonlin_func, learning_algo, ridge_param,\n                         leaky_rate, train_leaky_rate, feedbacks, wfdb_sparsity, normalize_feedbacks)\n    # end __init__\n\n    ###############################################\n    # PROPERTIES\n    ###############################################\n\n    # Hidden layer\n    @property\n    def hidden(self):\n        """"""\n        Hidden layer\n        :return:\n        """"""\n        return self.esn.hidden\n\n    # end hidden\n\n    # Hidden weight matrix\n    @property\n    def w(self):\n        """"""\n        Hidden weight matrix\n        :return:\n        """"""\n        return self.esn.w\n\n    # end w\n\n    # Input matrix\n    @property\n    def w_in(self):\n        """"""\n        Input matrix\n        :return:\n        """"""\n        return self.esn.w_in\n    # end w_in\n\n    # Embedding weights\n    @property\n    def weights(self):\n        """"""\n        Embedding weights\n        :return:\n        """"""\n        return self.embedding.weight\n    # end weights\n\n    ###############################################\n    # PUBLIC\n    ###############################################\n\n    # Forward\n    def forward(self, u, y=None):\n        """"""\n        Forward\n        :param x:\n        :return:\n        """"""\n        # Embedding layer\n        emb = self.embedding(u)\n\n        # ESN\n        return self.esn(emb, y)\n    # end forward\n\n# end EESN\n'"
echotorch/nn/ESN.py,3,"b'# -*- coding: utf-8 -*-\n#\n# File : echotorch/nn/ESN.py\n# Description : An Echo State Network module.\n# Date : 26th of January, 2018\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti, University of Neuch\xc3\xa2tel <nils.schaetti@unine.ch>\n\n""""""\nCreated on 26 January 2018\n@author: Nils Schaetti\n""""""\n\n# Imports\nimport torch\nimport torch.nn as nn\nfrom . import ESNCell\nfrom .RRCell import RRCell\n\n\n# Echo State Network module\nclass ESN(nn.Module):\n    """"""\n    Echo State Network module\n    """"""\n\n    # Constructor\n    def __init__(self, input_dim, hidden_dim, output_dim, spectral_radius=0.9, bias_scaling=0, input_scaling=1.0,\n                 w=None, w_in=None, w_bias=None, w_fdb=None, sparsity=None, input_set=[1.0, -1.0], w_sparsity=None,\n                 nonlin_func=torch.tanh, learning_algo=\'inv\', ridge_param=0.0, create_cell=True,\n                 feedbacks=False, with_bias=True, wfdb_sparsity=None, normalize_feedbacks=False,\n                 softmax_output=False, seed=None, washout=0, w_distrib=\'uniform\', win_distrib=\'uniform\',\n                 wbias_distrib=\'uniform\', win_normal=(0.0, 1.0), w_normal=(0.0, 1.0), wbias_normal=(0.0, 1.0),\n                 dtype=torch.float32):\n        """"""\n        Constructor\n        :param input_dim: Inputs dimension.\n        :param hidden_dim: Hidden layer dimension\n        :param output_dim: Reservoir size\n        :param spectral_radius: Reservoir\'s spectral radius\n        :param bias_scaling: Scaling of the bias, a constant input to each neuron (default: 0, no bias)\n        :param input_scaling: Scaling of the input weight matrix, default 1.\n        :param w: Internation weights matrix\n        :param w_in: Input-reservoir weights matrix\n        :param w_bias: Bias weights matrix\n        :param w_fdb: Feedback weights matrix\n        :param sparsity:\n        :param input_set:\n        :param w_sparsity:\n        :param nonlin_func: Reservoir\'s activation function (tanh, sig, relu)\n        :param learning_algo: Which learning algorithm to use (inv, LU, grad)\n        """"""\n        super(ESN, self).__init__()\n\n        # Properties\n        self.output_dim = output_dim\n        self.feedbacks = feedbacks\n        self.with_bias = with_bias\n        self.normalize_feedbacks = normalize_feedbacks\n        self.washout = washout\n        self.dtype = dtype\n\n        # Recurrent layer\n        if create_cell:\n            self.esn_cell = ESNCell(input_dim, hidden_dim, spectral_radius, bias_scaling, input_scaling, w, w_in,\n                                    w_bias, w_fdb, sparsity, input_set, w_sparsity, nonlin_func, feedbacks, output_dim,\n                                    wfdb_sparsity, normalize_feedbacks, seed, w_distrib, win_distrib, wbias_distrib,\n                                    win_normal, w_normal, wbias_normal, dtype)\n        # end if\n\n        # Ouput layer\n        self.output = RRCell(hidden_dim, output_dim, ridge_param, feedbacks, with_bias, learning_algo, softmax_output, dtype)\n    # end __init__\n\n    ###############################################\n    # PROPERTIES\n    ###############################################\n\n    # Hidden layer\n    @property\n    def hidden(self):\n        """"""\n        Hidden layer\n        :return:\n        """"""\n        return self.esn_cell.hidden\n    # end hidden\n\n    # Hidden weight matrix\n    @property\n    def w(self):\n        """"""\n        Hidden weight matrix\n        :return:\n        """"""\n        return self.esn_cell.w\n    # end w\n\n    # Input matrix\n    @property\n    def w_in(self):\n        """"""\n        Input matrix\n        :return:\n        """"""\n        return self.esn_cell.w_in\n    # end w_in\n\n    ###############################################\n    # PUBLIC\n    ###############################################\n\n    # Reset learning\n    def reset(self):\n        """"""\n        Reset learning\n        :return:\n        """"""\n        # Reset output layer\n        self.output.reset()\n\n        # Training mode again\n        self.train(True)\n    # end reset\n\n    # Output matrix\n    def get_w_out(self):\n        """"""\n        Output matrix\n        :return:\n        """"""\n        return self.output.w_out\n    # end get_w_out\n\n    # Set W\n    def set_w(self, w):\n        """"""\n        Set W\n        :param w:\n        :return:\n        """"""\n        self.esn_cell.w = w\n    # end set_w\n\n    # Forward\n    def forward(self, u, y=None, reset_state=True):\n        """"""\n        Forward\n        :param u: Input signal.\n        :param y: Target outputs\n        :return: Output or hidden states\n        """"""\n        # Compute hidden states\n        if self.feedbacks and self.training:\n            hidden_states = self.esn_cell(u, y, reset_state=reset_state)\n        elif self.feedbacks and not self.training:\n            hidden_states = self.esn_cell(u, w_out=self.output.w_out, reset_state=reset_state)\n        else:\n            hidden_states = self.esn_cell(u, reset_state=reset_state)\n        # end if\n\n        # Learning algo\n        if y is not None:\n            return self.output(hidden_states[:, self.washout:], y[:, self.washout:])\n        else:\n            return self.output(hidden_states[:, self.washout:], y)\n        # end if\n    # end forward\n\n    # Finish training\n    def finalize(self):\n        """"""\n        Finalize training with LU factorization\n        """"""\n        # Finalize output training\n        self.output.finalize()\n\n        # Not in training mode anymore\n        self.train(False)\n    # end finalize\n\n    # Reset hidden layer\n    def reset_hidden(self):\n        """"""\n        Reset hidden layer\n        :return:\n        """"""\n        self.esn_cell.reset_hidden()\n    # end reset_hidden\n\n    # Get W\'s spectral radius\n    def get_spectral_radius(self):\n        """"""\n        Get W\'s spectral radius\n        :return: W\'s spectral radius\n        """"""\n        return self.esn_cell.get_spectral_raduis()\n    # end spectral_radius\n\n# end ESNCell\n'"
echotorch/nn/ESN2d.py,0,b''
echotorch/nn/ESNCell.py,43,"b'# -*- coding: utf-8 -*-\n#\n# File : echotorch/nn/ESNCell.py\n# Description : An Echo State Network layer.\n# Date : 26th of January, 2018\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti <nils.schaetti@unine.ch>\n\n""""""\nCreated on 26 January 2018\n@author: Nils Schaetti\n""""""\n\nimport torch\nimport torch.sparse\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport echotorch.utils\nimport numpy as np\n\n\n# Echo State Network layer\nclass ESNCell(nn.Module):\n    """"""\n    Echo State Network layer\n    """"""\n\n    # Constructor\n    def __init__(self, input_dim, output_dim, spectral_radius=0.9, bias_scaling=0, input_scaling=1.0, w=None, w_in=None,\n                 w_bias=None, w_fdb=None, sparsity=None, input_set=[1.0, -1.0], w_sparsity=None,\n                 nonlin_func=torch.tanh, feedbacks=False, feedbacks_dim=None, wfdb_sparsity=None,\n                 normalize_feedbacks=False, seed=None, w_distrib=\'uniform\', win_distrib=\'uniform\',\n                 wbias_distrib=\'uniform\', win_normal=(0.0, 1.0), w_normal=(0.0, 1.0), wbias_normal=(0.0, 1.0),\n                 dtype=torch.float32):\n        """"""\n        Constructor\n        :param input_dim: Inputs dimension.\n        :param output_dim: Reservoir size\n        :param spectral_radius: Reservoir\'s spectral radius\n        :param bias_scaling: Scaling of the bias, a constant input to each neuron (default: 0, no bias)\n        :param input_scaling: Scaling of the input weight matrix, default 1.\n        :param w: Internation weights matrix\n        :param w_in: Input-reservoir weights matrix\n        :param w_bias: Bias weights matrix\n        :param sparsity:\n        :param input_set:\n        :param w_sparsity:\n        :param nonlin_func: Reservoir\'s activation function (tanh, sig, relu)\n        """"""\n        super(ESNCell, self).__init__()\n\n        # Params\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.spectral_radius = spectral_radius\n        self.bias_scaling = bias_scaling\n        self.input_scaling = input_scaling\n        self.sparsity = sparsity\n        self.input_set = input_set\n        self.w_sparsity = w_sparsity\n        self.nonlin_func = nonlin_func\n        self.feedbacks = feedbacks\n        self.feedbacks_dim = feedbacks_dim\n        self.wfdb_sparsity = wfdb_sparsity\n        self.normalize_feedbacks = normalize_feedbacks\n        self.w_distrib = w_distrib\n        self.win_distrib = win_distrib\n        self.wbias_distrib = wbias_distrib\n        self.win_normal = win_normal\n        self.w_normal = w_normal\n        self.wbias_normal = wbias_normal\n        self.dtype = dtype\n\n        # Init hidden state\n        self.register_buffer(\'hidden\', self.init_hidden())\n\n        # Initialize input weights\n        self.register_buffer(\'w_in\', self._generate_win(w_in, seed=seed))\n\n        # Initialize reservoir weights randomly\n        self.register_buffer(\'w\', self._generate_w(w, seed=seed))\n\n        # Initialize bias\n        self.register_buffer(\'w_bias\', self._generate_wbias(w_bias, seed=seed))\n\n        # Initialize feedbacks weights randomly\n        if feedbacks:\n            self.register_buffer(\'w_fdb\', self._generate_wfdb(w_fdb, seed=seed))\n        # end if\n    # end __init__\n\n    ###############################################\n    # PUBLIC\n    ###############################################\n\n    # Forward\n    def forward(self, u, y=None, w_out=None, reset_state=True):\n        """"""\n        Forward\n        :param u: Input signal\n        :param y: Target output signal for teacher forcing\n        :param w_out: Output weights for teacher forcing\n        :return: Resulting hidden states\n        """"""\n        # Time length\n        time_length = int(u.size()[1])\n\n        # Number of batches\n        n_batches = int(u.size()[0])\n\n        # Outputs\n        outputs = Variable(torch.zeros(n_batches, time_length, self.output_dim, dtype=self.dtype))\n        outputs = outputs.cuda() if self.hidden.is_cuda else outputs\n\n        # For each batch\n        for b in range(n_batches):\n            # Reset hidden layer\n            if reset_state:\n                self.reset_hidden()\n            # end if\n\n            # For each steps\n            for t in range(time_length):\n                # Current input\n                ut = u[b, t]\n\n                # Compute input layer\n                u_win = self.w_in.mv(ut)\n\n                # Apply W to x\n                x_w = self.w.mv(self.hidden)\n\n                # Feedback or not\n                if self.feedbacks and self.training and y is not None:\n                    # Current target\n                    yt = y[b, t]\n\n                    # Compute feedback layer\n                    y_wfdb = self.w_fdb.mv(yt)\n\n                    # Add everything\n                    x = u_win + x_w + y_wfdb + self.w_bias\n                elif self.feedbacks and not self.training and w_out is not None:\n                    # Add bias\n                    bias_hidden = torch.cat((Variable(torch.ones(1)), self.hidden), dim=0)\n\n                    # Compute past output\n                    yt = w_out.t().mv(bias_hidden)\n\n                    # Normalize\n                    if self.normalize_feedbacks:\n                        yt -= torch.min(yt)\n                        yt /= torch.max(yt) - torch.min(yt)\n                        yt /= torch.sum(yt)\n                    # end if\n\n                    # Compute feedback layer\n                    y_wfdb = self.w_fdb.mv(yt)\n\n                    # Add everything\n                    x = u_win + x_w + y_wfdb + self.w_bias\n                else:\n                    # Add everything\n                    x = u_win + x_w + self.w_bias\n                # end if\n\n                # Apply activation function\n                x = self.nonlin_func(x)\n\n                # Add to outputs\n                self.hidden.data = x.view(self.output_dim).data\n\n                # New last state\n                outputs[b, t] = self.hidden\n            # end for\n        # end for\n\n        return outputs\n    # end forward\n\n    # Init hidden layer\n    def init_hidden(self):\n        """"""\n        Init hidden layer\n        :return: Initiated hidden layer\n        """"""\n        return Variable(torch.zeros(self.output_dim, dtype=self.dtype), requires_grad=False)\n        # return torch.zeros(self.output_dim)\n    # end init_hidden\n\n    # Reset hidden layer\n    def reset_hidden(self):\n        """"""\n        Reset hidden layer\n        :return:\n        """"""\n        self.hidden.fill_(0.0)\n    # end reset_hidden\n\n    # Set hidden layer\n    def set_hidden(self, x):\n        """"""\n        Set hidden layer\n        :param x:\n        :return:\n        """"""\n        self.hidden.data = x.data\n    # end set_hidden\n\n    # Get W\'s spectral radius\n    def get_spectral_radius(self):\n        """"""\n        Get W\'s spectral radius\n        :return: W\'s spectral radius\n        """"""\n        return echotorch.utils.spectral_radius(self.w)\n    # end spectral_radius\n\n    ###############################################\n    # PRIVATE\n    ###############################################\n\n    # Generate W matrix\n    def _generate_w(self, w, seed=None):\n        """"""\n        Generate W matrix\n        :return:\n        """"""\n        # Initialize reservoir weight matrix\n        if w is None:\n            w = self.generate_w(output_dim=self.output_dim, w_distrib=self.w_distrib, w_sparsity=self.w_sparsity, mean=self.w_normal[0], std=self.w_normal[1], seed=seed, dtype=self.dtype)\n        else:\n            if callable(w):\n                w = w(self.output_dim)\n            # end if\n        # end if\n\n        # Scale it to spectral radius\n        w *= self.spectral_radius / echotorch.utils.spectral_radius(w)\n\n        return Variable(w, requires_grad=False)\n    # end generate_W\n\n    # Generate Win matrix\n    def _generate_win(self, w_in, seed=None):\n        """"""\n        Generate Win matrix\n        :return:\n        """"""\n        # Manual seed\n        if seed is not None:\n            np.random.seed(seed)\n            torch.random.manual_seed(seed)\n        # end if\n\n        # Initialize input weight matrix\n        if w_in is None:\n            # Distribution\n            if self.win_distrib == \'uniform\':\n                w_in = self.generate_uniform_matrix(size=(self.output_dim, self.input_dim), sparsity=self.sparsity, input_set=self.input_set)\n                if self.dtype == torch.float32:\n                    w_in = torch.from_numpy(w_in.astype(np.float32))\n                else:\n                    w_in = torch.from_numpy(w_in.astype(np.float64))\n                # end if\n            else:\n                w_in = self.generate_gaussian_matrix(size=(self.output_dim, self.input_dim), sparsity=self.sparsity, mean=self.win_normal[0], std=self.win_normal[1], dtype=self.dtype)\n            # end if\n            w_in *= self.input_scaling\n        else:\n            if callable(w_in):\n                w_in = w_in(self.output_dim, self.input_dim)\n            # end if\n        # end if\n\n        return Variable(w_in, requires_grad=False)\n    # end _generate_win\n\n    # Generate Wbias matrix\n    def _generate_wbias(self, w_bias, seed=None):\n        """"""\n        Generate Wbias matrix\n        :return:\n        """"""\n        # Manual seed\n        if seed is not None:\n            torch.manual_seed(seed)\n        # end if\n\n        # Initialize bias matrix\n        if w_bias is None:\n            # Distribution\n            if self.w_distrib == \'uniform\':\n                w_bias = self.generate_uniform_matrix(size=(1, self.output_dim), sparsity=1.0, input_set=[-1.0, 1.0])\n                if self.dtype == torch.float32:\n                    w_bias = torch.from_numpy(w_bias.astype(np.float32))\n                else:\n                    w_bias = torch.from_numpy(w_bias.astype(np.float64))\n                # end if\n            else:\n                w_bias = self.generate_gaussian_matrix(size=(1, self.output_dim), sparsity=1.0, mean=self.wbias_normal[0], std=self.wbias_normal[1], dtype=self.dtype)\n            # end if\n            w_bias *= self.bias_scaling\n        else:\n            if callable((w_bias)):\n                w_bias = w_bias(self.output_dim)\n            # end if\n        # end if\n\n        return Variable(w_bias, requires_grad=False)\n    # end _generate_wbias\n\n    # Generate Wfdb matrix\n    def _generate_wfdb(self, w_fdb, seed=None):\n        """"""\n        Generate Wfdb matrix\n        :return:\n        """"""\n        # Manual seed\n        if seed is not None:\n            torch.manual_seed(seed)\n        # end if\n\n        # Initialize feedbacks weight matrix\n        if w_fdb is None:\n            if self.wfdb_sparsity is None:\n                w_fdb = self.input_scaling * (\n                        np.random.randint(0, 2, (self.output_dim, self.feedbacks_dim)) * 2.0 - 1.0)\n                w_fdb = torch.from_numpy(w_fdb.astype(np.float32))\n            else:\n                w_fdb = self.input_scaling * np.random.choice(np.append([0], self.input_set),\n                                                             (self.output_dim, self.feedbacks_dim),\n                                                             p=np.append([1.0 - self.wfdb_sparsity],\n                                                                         [self.wfdb_sparsity / len(\n                                                                             self.input_set)] * len(\n                                                                             self.input_set)))\n                if self.dtype == torch.float32:\n                    w_fdb = torch.from_numpy(w_fdb.astype(np.float32))\n                else:\n                    w_fdb = torch.from_numpy(w_fdb.astype(np.float64))\n                # end if\n            # end if\n        else:\n            if callable(w_fdb):\n                w_fdb = w_fdb(self.output_dim, self.feedbacks_dim)\n            # end if\n        # end if\n\n        return Variable(w_fdb, requires_grad=False)\n    # end _generate_wfdb\n\n    ############################################\n    # STATIC\n    ############################################\n\n    # Generate uniform matrix\n    @staticmethod\n    def generate_uniform_matrix(size, sparsity, input_set):\n        """"""\n        Generate uniform Win matrix\n        :param w_in:\n        :param seed:\n        :return:\n        """"""\n        if sparsity is None:\n            w = (np.random.randint(0, 2, size) * 2.0 - 1.0)\n        else:\n            w = np.random.choice(np.append([0], input_set), size,\n                                 p=np.append([1.0 - sparsity], [sparsity / len(input_set)] * len(input_set)))\n        # end if\n        return w\n\n    # end _generate_uniform_win\n\n    # Generate gaussian matrix\n    @staticmethod\n    def generate_gaussian_matrix(size, sparsity, mean=0.0, std=1.0, dtype=torch.float32):\n        """"""\n        Generate gaussian Win matrix\n        :return:\n        """"""\n        if sparsity is None:\n            w = torch.zeros(size, dtype=dtype)\n            w = w.normal_(mean=mean, std=std)\n        else:\n            w = torch.zeros(size, dtype=dtype)\n            w = w.normal_(mean=mean, std=std)\n            mask = torch.zeros(size, dtype=dtype)\n            mask.bernoulli_(p=sparsity)\n            w *= mask\n        # end if\n        return w\n    # end if\n\n    # Generate W matrix\n    @staticmethod\n    def generate_w(output_dim, w_distrib=\'uniform\', w_sparsity=None, mean=0.0, std=1.0, seed=None, dtype=torch.float32):\n        """"""\n        Generate W matrix\n        :param output_dim:\n        :param w_sparsity:\n        :return:\n        """"""\n        # Manual seed\n        if seed is not None:\n            torch.manual_seed(seed)\n            np.random.seed(seed)\n        # end if\n\n        # Distribution\n        if w_distrib == \'uniform\':\n            w = ESNCell.generate_uniform_matrix(size=(output_dim, output_dim), sparsity=w_sparsity, input_set=[-1.0, 1.0])\n            w = torch.from_numpy(w.astype(np.float32))\n        else:\n            w = ESNCell.generate_gaussian_matrix(size=(output_dim, output_dim), sparsity=w_sparsity, mean=mean, std=std, dtype=dtype)\n        # end if\n\n        return w\n    # end generate_w\n\n    # To sparse matrix\n    @staticmethod\n    def to_sparse(m):\n        """"""\n        To sparse matrix\n        :param m:\n        :return:\n        """"""\n        # Rows, columns and values\n        rows = torch.LongTensor()\n        columns = torch.LongTensor()\n        values = torch.FloatTensor()\n\n        # For each row\n        for i in range(m.shape[0]):\n            # For each column\n            for j in range(m.shape[1]):\n                if m[i, j] != 0.0:\n                    rows = torch.cat((rows, torch.LongTensor([i])), dim=0)\n                    columns = torch.cat((columns, torch.LongTensor([j])), dim=0)\n                    values = torch.cat((values, torch.FloatTensor([m[i, j]])), dim=0)\n                # end if\n            # end for\n        # end for\n\n        # Indices\n        indices = torch.cat((rows.unsqueeze(0), columns.unsqueeze(0)), dim=0)\n\n        # To sparse\n        return torch.sparse.FloatTensor(indices, values)\n    # end to_sparse\n\n# end ESNCell\n'"
echotorch/nn/GatedESN.py,10,"b'# -*- coding: utf-8 -*-\n#\n# File : echotorch/nn/ESN.py\n# Description : An Echo State Network module.\n# Date : 26th of January, 2018\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti, University of Neuch\xc3\xa2tel <nils.schaetti@unine.ch>\n\n""""""\nCreated on 26 January 2018\n@author: Nils Schaetti\n""""""\n\n# Imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .LiESNCell import LiESNCell\nfrom sklearn.decomposition import IncrementalPCA\nfrom .PCACell import PCACell\nimport matplotlib.pyplot as plt\nfrom torch.autograd import Variable\n\n\n# Gated Echo State Network\nclass GatedESN(nn.Module):\n    """"""\n    Gated Echo State Network\n    """"""\n\n    # Constructor\n    def __init__(self, input_dim, reservoir_dim, pca_dim, hidden_dim, leaky_rate=1.0, spectral_radius=0.9,\n                 bias_scaling=0, input_scaling=1.0, w=None, w_in=None, w_bias=None, sparsity=None,\n                 input_set=[1.0, -1.0], w_sparsity=None, nonlin_func=torch.tanh,\n                 create_cell=True):\n        """"""\n        Constructor\n        :param input_dim: Inputs dimension.\n        :param hidden_dim: Hidden layer dimension\n        :param reservoir_dim: Reservoir size\n        :param spectral_radius: Reservoir\'s spectral radius\n        :param bias_scaling: Scaling of the bias, a constant input to each neuron (default: 0, no bias)\n        :param input_scaling: Scaling of the input weight matrix, default 1.\n        :param w: Internal weights matrix\n        :param w_in: Input-reservoir weights matrix\n        :param w_bias: Bias weights matrix\n        :param sparsity:\n        :param input_set:\n        :param w_sparsity:\n        :param nonlin_func: Reservoir\'s activation function (tanh, sig, relu)\n        :param learning_algo: Which learning algorithm to use (inv, LU, grad)\n        """"""\n        super(GatedESN, self).__init__()\n\n        # Properties\n        self.reservoir_dim = reservoir_dim\n        self.pca_dim = pca_dim\n        self.hidden_dim = hidden_dim\n        self.finalized = False\n\n        # Recurrent layer\n        if create_cell:\n            self.esn_cell = LiESNCell(\n                input_dim=input_dim, output_dim=reservoir_dim, spectral_radius=spectral_radius, bias_scaling=bias_scaling,\n                input_scaling=input_scaling, w=w, w_in=w_in, w_bias=w_bias, sparsity=sparsity, input_set=input_set,\n                w_sparsity=w_sparsity, nonlin_func=nonlin_func, leaky_rate=leaky_rate\n            )\n        # end if\n\n        # PCA\n        if self.pca_dim > 0:\n            self.pca_cell = PCACell(input_dim=reservoir_dim, output_dim=pca_dim)\n        # end if\n\n        # Initialize input update weights\n        self.register_parameter(\'wzp\', nn.Parameter(self.init_wzp()))\n\n        # Initialize hidden update weights\n        self.register_parameter(\'wzh\', nn.Parameter(self.init_wzh()))\n\n        # Initialize update bias\n        self.register_parameter(\'bz\', nn.Parameter(self.init_bz()))\n    # end __init__\n\n    ###############################################\n    # PROPERTIES\n    ###############################################\n\n    # Hidden layer\n    @property\n    def hidden(self):\n        """"""\n        Hidden layer\n        :return:\n        """"""\n        return self.esn_cell.hidden\n    # end hidden\n\n    # Hidden weight matrix\n    @property\n    def w(self):\n        """"""\n        Hidden weight matrix\n        :return:\n        """"""\n        return self.esn_cell.w\n    # end w\n\n    # Input matrix\n    @property\n    def w_in(self):\n        """"""\n        Input matrix\n        :return:\n        """"""\n        return self.esn_cell.w_in\n    # end w_in\n\n    ###############################################\n    # PUBLIC\n    ###############################################\n\n    # Init hidden vector\n    def init_hidden(self):\n        """"""\n        Init hidden layer\n        :return: Initiated hidden layer\n        """"""\n        return Variable(torch.zeros(self.hidden_dim), requires_grad=False)\n    # end init_hidden\n\n    # Init update vector\n    def init_update(self):\n        """"""\n        Init hidden layer\n        :return: Initiated hidden layer\n        """"""\n        return self.init_hidden()\n    # end init_hidden\n\n    # Init update-reduced matrix\n    def init_wzp(self):\n        """"""\n        Init update-reduced matrix\n        :return: Initiated update-reduced matrix\n        """"""\n        return torch.rand(self.pca_dim, self.hidden_dim)\n    # end init_hidden\n\n    # Init update-hidden matrix\n    def init_wzh(self):\n        """"""\n        Init update-hidden matrix\n        :return: Initiated update-hidden matrix\n        """"""\n        return torch.rand(self.pca_dim, self.hidden_dim)\n    # end init_hidden\n\n    # Init update bias\n    def init_bz(self):\n        """"""\n        Init update bias\n        :return:\n        """"""\n        return torch.rand(self.hidden_dim)\n    # end init_bz\n\n    # Reset learning\n    def reset(self):\n        """"""\n        Reset learning\n        :return:\n        """"""\n        # Reset PCA layer\n        self.pca_cell.reset()\n\n        # Reset reservoir\n        self.reset_reservoir()\n\n        # Training mode again\n        self.train(True)\n    # end reset\n\n    # Forward\n    def forward(self, u, y=None):\n        """"""\n        Forward\n        :param u: Input signal.\n        :return: Output or hidden states\n        """"""\n        # Time length\n        time_length = int(u.size()[1])\n\n        # Number of batches\n        n_batches = int(u.size()[0])\n\n        # Compute reservoir states\n        reservoir_states = self.esn_cell(u)\n        reservoir_states.required_grad = False\n\n        # Reduce\n        if self.pca_dim > 0:\n            # Reduce states\n            pca_states = self.pca_cell(reservoir_states)\n            pca_states.required_grad = False\n\n            # Stop here if we learn PCA\n            if self.finalized:\n                return\n            # end if\n\n            # Hidden states\n            hidden_states = Variable(torch.zeros(n_batches, time_length, self.hidden_dim))\n            hidden_states = hidden_states.cuda() if pca_states.is_cuda else hidden_states\n        else:\n            # Hidden states\n            hidden_states = Variable(torch.zeros(n_batches, time_length, self.hidden_dim))\n            hidden_states = hidden_states.cuda() if reservoir_states.is_cuda else hidden_states\n        # end if\n\n        # For each batch\n        for b in range(n_batches):\n            # Reset hidden layer\n            hidden = self.init_hidden()\n\n            # TO CUDA\n            if u.is_cuda:\n                hidden = hidden.cuda()\n            # end if\n\n            # For each steps\n            for t in range(time_length):\n                # Current reduced state\n                if self.pca_dim > 0:\n                    pt = pca_states[b, t]\n                else:\n                    pt = reservoir_states[b, t]\n                # end if\n\n                # Compute update vector\n                zt = F.sigmoid(self.wzp.mv(pt) + self.wzh.mv(hidden) + self.bz)\n\n                # Compute hidden state\n                ht = (1.0 - zt) * hidden + zt * pt\n\n                # Add to outputs\n                hidden = ht.view(self.hidden_dim)\n\n                # New last state\n                hidden_states[b, t] = hidden\n            # end for\n        # end for\n\n        # Return hidden states\n        return hidden_states\n    # end forward\n\n    # Finish training\n    def finalize(self):\n        """"""\n        Finalize training with LU factorization\n        """"""\n        # Finalize output training\n        self.pca_cell.finalize()\n\n        # Finalized\n        self.finalized = True\n    # end finalize\n\n    # Reset reservoir layer\n    def reset_reservoir(self):\n        """"""\n        Reset hidden layer\n        :return:\n        """"""\n        self.esn_cell.reset_hidden()\n    # end reset_reservoir\n\n    # Reset hidden layer\n    def reset_hidden(self):\n        """"""\n        Reset hidden layer\n        :return:\n        """"""\n        self.hidden.fill_(0.0)\n    # end reset_hidden\n\n# end GatedESN\n'"
echotorch/nn/HESN.py,3,"b'# -*- coding: utf-8 -*-\n#\n# File : echotorch/nn/HESN.py\n# Description : ESN with input pre-trained and used with transfer learning.\n# Date : 22 March, 2018\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti <nils.schaetti@unine.ch>\n\nimport torch\nimport torch.sparse\nimport torch.nn as nn\nfrom .LiESN import LiESN\n\n\n# ESN with input pre-trained and used with transfer learning\nclass HESN(object):\n    """"""\n    ESN with input pre-trained and used with transfer learning\n    """"""\n\n    # Constructor\n    def __init__(self, model, input_dim, hidden_dim, output_dim, spectral_radius=0.9,\n                 bias_scaling=0, input_scaling=1.0, w=None, w_in=None, w_bias=None, sparsity=None,\n                 input_set=[1.0, -1.0], w_sparsity=None, nonlin_func=torch.tanh, learning_algo=\'inv\', ridge_param=0.0,\n                 leaky_rate=1.0, train_leaky_rate=False, feedbacks=False, wfdb_sparsity=None,\n                 normalize_feedbacks=False):\n        # Embedding layer\n        self.mode = model\n\n        # Li-ESN\n        self.esn = LiESN(input_dim, hidden_dim, output_dim, spectral_radius, bias_scaling, input_scaling,\n                         w, w_in, w_bias, sparsity, input_set, w_sparsity, nonlin_func, learning_algo, ridge_param,\n                         leaky_rate, train_leaky_rate, feedbacks, wfdb_sparsity, normalize_feedbacks)\n    # end __init__\n\n    ###############################################\n    # PROPERTIES\n    ###############################################\n\n    # Hidden layer\n    @property\n    def hidden(self):\n        """"""\n        Hidden layer\n        :return:\n        """"""\n        return self.esn.hidden\n\n    # end hidden\n\n    # Hidden weight matrix\n    @property\n    def w(self):\n        """"""\n        Hidden weight matrix\n        :return:\n        """"""\n        return self.esn.w\n\n    # end w\n\n    # Input matrix\n    @property\n    def w_in(self):\n        """"""\n        Input matrix\n        :return:\n        """"""\n        return self.esn.w_in\n    # end w_in\n\n    ###############################################\n    # PUBLIC\n    ###############################################\n\n    # Forward\n    def forward(self, u, y=None):\n        """"""\n        Forward\n        :param x:\n        :return:\n        """"""\n        # Selected features\n        selected_features = self.model(u)\n\n        # ESN\n        return self.esn(selected_features, y)\n    # end forward\n\n# end HESN\n'"
echotorch/nn/ICACell.py,5,"b'# -*- coding: utf-8 -*-\n#\n# File : echotorch/nn/ESN.py\n# Description : An Echo State Network module.\n# Date : 26th of January, 2018\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti <nils.schaetti@unine.ch>\n\n""""""\nCreated on 26 January 2018\n@author: Nils Schaetti\n""""""\n\n# Imports\nimport torch.sparse\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\n\n# Independent Component Analysis layer\nclass ICACell(nn.Module):\n    """"""\n    Principal Component Analysis layer. It can be used to handle different batch-mode algorithm for ICA.\n    """"""\n\n    # Constructor\n    def __init__(self, input_dim, output_dim):\n        """"""\n        Constructor\n        :param input_dim: Inputs dimension.\n        :param output_dim: Reservoir size\n        """"""\n        super(ICACell, self).__init__()\n        pass\n    # end __init__\n\n    ###############################################\n    # PROPERTIES\n    ###############################################\n\n    ###############################################\n    # PUBLIC\n    ###############################################\n\n    # Reset learning\n    def reset(self):\n        """"""\n        Reset learning\n        :return:\n        """"""\n        # Training mode again\n        self.train(True)\n    # end reset\n\n    # Forward\n    def forward(self, x, y=None):\n        """"""\n        Forward\n        :param x: Input signal.\n        :param y: Target outputs\n        :return: Output or hidden states\n        """"""\n        # Batch size\n        batch_size = x.size()[0]\n\n        # Time length\n        time_length = x.size()[1]\n\n        # Add bias\n        if self.with_bias:\n            x = self._add_constant(x)\n        # end if\n    # end forward\n\n    # Finish training\n    def finalize(self):\n        """"""\n        Finalize training with LU factorization or Pseudo-inverse\n        """"""\n        pass\n    # end finalize\n\n    ###############################################\n    # PRIVATE\n    ###############################################\n\n    # Add constant\n    def _add_constant(self, x):\n        """"""\n        Add constant\n        :param x:\n        :return:\n        """"""\n        bias = Variable(torch.ones((x.size()[0], x.size()[1], 1)), requires_grad=False)\n        return torch.cat((bias, x), dim=2)\n    # end _add_constant\n\n# end ICACell\n'"
echotorch/nn/Identity.py,2,"b'# -*- coding: utf-8 -*-\n#\n# File : echotorch/nn/Identity.py\n# Description : An Leaky-Integrated Echo State Network layer.\n# Date : 09th of April, 2018\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti <nils.schaetti@unine.ch>\n\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\n\n# Identity layer\nclass Identity(nn.Module):\n    """"""\n    Identity layer\n    """"""\n\n    # Forward\n    def forward(self, x):\n        """"""\n        Forward\n        :return:\n        """"""\n        return x\n    # end forward\n\n# end Identity\n'"
echotorch/nn/IncSFACell.py,0,b''
echotorch/nn/LiESN.py,4,"b'# -*- coding: utf-8 -*-\n#\n# File : echotorch/nn/ESN.py\n# Description : An Echo State Network module.\n# Date : 26th of January, 2018\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti <nils.schaetti@unine.ch>\n\n""""""\nCreated on 26 January 2018\n@author: Nils Schaetti\n""""""\n\nimport torch\nfrom .LiESNCell import LiESNCell\nfrom .ESN import ESN\n\n\n# Leaky-Integrated Echo State Network module\nclass LiESN(ESN):\n    """"""\n    Leaky-Integrated Echo State Network module\n    """"""\n\n    # Constructor\n    def __init__(self, input_dim, hidden_dim, output_dim, spectral_radius=0.9,\n                 bias_scaling=0, input_scaling=1.0, w=None, w_in=None, w_bias=None, sparsity=None,\n                 input_set=[1.0, -1.0], w_sparsity=None, nonlin_func=torch.tanh, learning_algo=\'inv\', ridge_param=0.0,\n                 leaky_rate=1.0, train_leaky_rate=False, feedbacks=False, wfdb_sparsity=None,\n                 normalize_feedbacks=False, softmax_output=False, seed=None, washout=0, w_distrib=\'uniform\',\n                 win_distrib=\'uniform\', wbias_distrib=\'uniform\', win_normal=(0.0, 1.0), w_normal=(0.0, 1.0),\n                 wbias_normal=(0.0, 1.0), dtype=torch.float32):\n        """"""\n        Constructor\n        :param input_dim:\n        :param hidden_dim:\n        :param output_dim:\n        :param spectral_radius:\n        :param bias_scaling:\n        :param input_scaling:\n        :param w:\n        :param w_in:\n        :param w_bias:\n        :param sparsity:\n        :param input_set:\n        :param w_sparsity:\n        :param nonlin_func:\n        :param learning_algo:\n        :param ridge_param:\n        :param leaky_rate:\n        :param train_leaky_rate:\n        :param feedbacks:\n        """"""\n        super(LiESN, self).__init__(input_dim, hidden_dim, output_dim, spectral_radius=spectral_radius,\n                                    bias_scaling=bias_scaling, input_scaling=input_scaling,\n                                    w=w, w_in=w_in, w_bias=w_bias, sparsity=sparsity, input_set=input_set,\n                                    w_sparsity=w_sparsity, nonlin_func=nonlin_func, learning_algo=learning_algo,\n                                    ridge_param=ridge_param, create_cell=False, feedbacks=feedbacks,\n                                    wfdb_sparsity=wfdb_sparsity, normalize_feedbacks=normalize_feedbacks,\n                                    softmax_output=softmax_output, seed=seed, washout=washout, w_distrib=w_distrib,\n                                    win_distrib=win_distrib, wbias_distrib=wbias_distrib, win_normal=win_normal,\n                                    w_normal=w_normal, wbias_normal=wbias_normal, dtype=torch.float32)\n\n        # Recurrent layer\n        self.esn_cell = LiESNCell(leaky_rate, train_leaky_rate, input_dim, hidden_dim, spectral_radius=spectral_radius,\n                                  bias_scaling=bias_scaling, input_scaling=input_scaling,\n                                  w=w, w_in=w_in, w_bias=w_bias, sparsity=sparsity, input_set=input_set,\n                                  w_sparsity=w_sparsity, nonlin_func=nonlin_func, feedbacks=feedbacks,\n                                  feedbacks_dim=output_dim, wfdb_sparsity=wfdb_sparsity,\n                                  normalize_feedbacks=normalize_feedbacks, seed=seed, w_distrib=w_distrib,\n                                  win_distrib=win_distrib, wbias_distrib=wbias_distrib, win_normal=win_normal,\n                                  w_normal=w_normal, wbias_normal=wbias_normal, dtype=torch.float32)\n    # end __init__\n\n    ###############################################\n    # PROPERTIES\n    ###############################################\n\n    ###############################################\n    # PUBLIC\n    ###############################################\n\n    ###############################################\n    # PRIVATE\n    ###############################################\n\n# end ESNCell\n'"
echotorch/nn/LiESNCell.py,11,"b'# -*- coding: utf-8 -*-\n#\n# File : echotorch/nn/LiESNCell.py\n# Description : An Leaky-Integrated Echo State Network layer.\n# Date : 26th of January, 2018\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti <nils.schaetti@unine.ch>\n\n""""""\nCreated on 26 January 2018\n@author: Nils Schaetti\n""""""\n\nimport torch\nimport torch.sparse\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom .ESNCell import ESNCell\nimport matplotlib.pyplot as plt\n\n\n# Leak-Integrated Echo State Network layer\nclass LiESNCell(ESNCell):\n    """"""\n    Leaky-Integrated Echo State Network layer\n    """"""\n\n    # Constructor\n    def __init__(self, leaky_rate=1.0, train_leaky_rate=False, *args, **kwargs):\n        """"""\n        Constructor\n        :param leaky_rate: Reservoir\'s leaky rate (default 1.0, normal ESN)\n        :param train_leaky_rate: Train leaky rate as parameter? (default: False)\n        """"""\n        super(LiESNCell, self).__init__(*args, **kwargs)\n\n        # Type\n        if self.dtype == torch.float32:\n            tensor_type = torch.FloatTensor\n        else:\n            tensor_type = torch.DoubleTensor\n        # end if\n\n        # Params\n        if train_leaky_rate:\n            self.leaky_rate = nn.Parameter(tensor_type(1).fill_(leaky_rate), requires_grad=True)\n        else:\n            # Initialize bias\n            self.register_buffer(\'leaky_rate\', Variable(tensor_type(1).fill_(leaky_rate), requires_grad=False))\n        # end if\n    # end __init__\n\n    ###############################################\n    # PUBLIC\n    ###############################################\n\n    # Forward\n    def forward(self, u, y=None, w_out=None, reset_state=True):\n        """"""\n        Forward\n        :param u: Input signal.\n        :return: Resulting hidden states.\n        """"""\n        # Time length\n        time_length = int(u.size()[1])\n\n        # Number of batches\n        n_batches = int(u.size()[0])\n\n        # Outputs\n        outputs = Variable(torch.zeros(n_batches, time_length, self.output_dim, dtype=self.dtype))\n        outputs = outputs.cuda() if self.hidden.is_cuda else outputs\n\n        # For each batch\n        for b in range(n_batches):\n            # Reset hidden layer\n            if reset_state:\n                self.reset_hidden()\n            # end if\n\n            # For each steps\n            for t in range(time_length):\n                # Current input\n                ut = u[b, t]\n\n                # Compute input layer\n                u_win = self.w_in.mv(ut)\n\n                # Apply W to x\n                x_w = self.w.mv(self.hidden)\n\n                # Feedback or not\n                if self.feedbacks and self.training and y is not None:\n                    # Current target\n                    yt = y[b, t]\n\n                    # Compute feedback layer\n                    y_wfdb = self.w_fdb.mv(yt)\n\n                    # Add everything\n                    x = u_win + x_w + y_wfdb + self.w_bias\n                elif self.feedbacks and not self.training and w_out is not None:\n                    # Add bias\n                    bias_hidden = torch.cat((Variable(torch.ones(1)), self.hidden), dim=0)\n\n                    # Compute past output\n                    yt = w_out.t().mv(bias_hidden)\n\n                    # Normalize\n                    if self.normalize_feedbacks:\n                        yt -= torch.min(yt)\n                        yt /= torch.max(yt) - torch.min(yt)\n                        yt /= torch.sum(yt)\n                    # end if\n\n                    # Compute feedback layer\n                    y_wfdb = self.w_fdb.mv(yt)\n\n                    # Add everything\n                    x = u_win + x_w + y_wfdb + self.w_bias\n                else:\n                    # Add everything\n                    x = u_win + x_w + self.w_bias\n                # end if\n\n                # Apply activation function\n                x = self.nonlin_func(x)\n\n                # Add to outputs\n                self.hidden.data = (self.hidden.mul(1.0 - self.leaky_rate) + x.view(self.output_dim).mul(self.leaky_rate)).data\n\n                # New last state\n                outputs[b, t] = self.hidden\n            # end for\n        # end for\n\n        return outputs\n    # end forward\n\n    ###############################################\n    # PRIVATE\n    ###############################################\n\n# end LiESNCell\n'"
echotorch/nn/OnlinePCACell.py,9,"b'# -*- coding: utf-8 -*-\n#\n# File : echotorch/nn/ESN.py\n# Description : An Echo State Network module.\n# Date : 26th of January, 2018\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti <nils.schaetti@unine.ch>\n\n""""""\nCreated on 26 January 2018\n@author: Nils Schaetti\n""""""\n\n# Imports\nimport torch.sparse\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\n\n# Online PCA cell\n# We extract the principal components from the input data incrementally.\nclass OnlinePCACell(nn.Module):\n    """"""\n    Online PCA cell\n    We extract the principal components from the input data incrementally.\n    Weng J., Zhang Y. and Hwang W.,\n    Candid covariance-free incremental principal component analysis,\n    IEEE Trans. Pattern Analysis and Machine Intelligence,\n    vol. 25, 1034--1040, 2003.\n    """"""\n\n    # Constructor\n    def __init__(self, input_dim, output_dim, amn_params=(20, 200, 2000, 3), init_eigen_vectors=None, var_rel=1, numx_rng=None):\n        """"""\n        Constructor\n        :param input_dim:\n        :param output_dim:\n        :param amn_params:\n        :param init_eigen_vectors:\n        :param var_rel:\n        :param numx_rng:\n        """"""\n        # Super call\n        super(OnlinePCACell, self).__init__()\n\n        # Properties\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.amn_params = amn_params\n        self._init_v = init_eigen_vectors\n        self.var_rel = var_rel\n        self._train_iteration = 0\n        self._training_type = None\n\n        # (Internal) eigenvectors\n        self._v = None\n        self.v = None\n        self.d = None\n\n        # Total and reduced\n        self._var_tot = 1.0\n        self._reduced_dims = self.output_dim\n    # end __init__\n\n    ###############################################\n    # PROPERTIES\n    ###############################################\n\n    # Initial eigen vectors\n    @property\n    def init_eigen_vectors(self):\n        """"""\n        Initial eigen vectors\n        :return:\n        """"""\n        return self._init_v\n    # end init_eigen_vectors\n\n    # Set initial eigen vectors\n    @init_eigen_vectors.setter\n    def init_eigen_vectors(self, init_eigen_vectors=None):\n        """"""\n        Set initial eigen vectors\n        :param init_eigen_vectors:\n        :return:\n        """"""\n        self._init_v = init_eigen_vectors\n\n        # Set input dim\n        if self._input_dim is None:\n            self._input_dim = self._init_v.shape[0]\n        else:\n            # Check input dim\n            assert(\n                self.input_dim == self._init_v.shape[0]), \\\n                Exception(u""Dimension mismatch. init_eigen_vectors shape[0] must be {}, given {}"".format(\n                    self.input_dim,\n                    self._init_v.shape[0]\n                )\n            )\n        # end if\n\n        # Set output dim\n        if self._output_dim is None:\n            self._output_dim = self._init_v.shape[1]\n        else:\n            # Check output dim\n            assert(\n                self.output_dim == self._init_v.shape[1],\n                Exception(u""Dimension mismatch, init_eigen_vectors shape[1] must be {}, given {}"".format(\n                    self.output_dim,\n                    self._init_v.shape[1])\n                )\n            )\n        # end if\n\n        # Set V\n        if self.v is None:\n            self._v = self._init_v.copy()\n            self.d = torch.norm(self._v, p=2, dim=0)\n            self.v = self._v / self.d\n        # end if\n    # end init_eigen_vectors\n\n    ###############################################\n    # PUBLIC\n    ###############################################\n\n    # Get variance explained by PCA\n    def get_var_tot(self):\n        """"""\n        Get variance explained by PCA\n        :return:\n        """"""\n        return self._var_tot\n    # end get_var_tot\n\n    # Get reducible dimensionality based on the set thresholds\n    def get_reduced_dimensionality(self):\n        """"""\n        Return reducible dimensionality based on the set thresholds.\n        :return:\n        """"""\n        return self._reduced_dims\n    # end get_reduced_dimensionality\n\n    # Get projection matrix\n    def get_projmatrix(self, transposed=1):\n        """"""\n        Get projection matrix\n        :param transposed:\n        :return:\n        """"""\n        if transposed:\n            return self.v\n        # end if\n        return self.v.t()\n    # end get_projmatrix\n\n    # Get back-projection matrix (reconstruction matrix)\n    def get_recmatrix(self, transposed=1):\n        """"""\n        Get reconstruction matrix\n        :param transposed:\n        :return:\n        """"""\n        if transposed:\n            return self.v.t()\n        # end if\n        return self.v\n    # end get_recmatrix\n\n    # Reset learning\n    def reset(self):\n        """"""\n        Reset learning\n        :return:\n        """"""\n        # Training mode again\n        self.train(True)\n    # end reset\n\n    # Forward\n    def forward(self, x, y=None):\n        """"""\n        Forward\n        :param x: Input signal.\n        :param y: Target outputs\n        :return: Output or hidden states\n        """"""\n        # Update components\n        self._update_pca(x)\n\n        # Execute\n        return self._execute(x)\n    # end forward\n\n    ###############################################\n    # PRIVATE\n    ###############################################\n\n    # Project the input on the first \'n\' components\n    def _execute(self, x, n=None):\n        """"""\n        Project the input on the first \'n\' components\n        :param x:\n        :param n:\n        :return:\n        """"""\n        if n is not None:\n            return x.mm(self.v[:, :n])\n        # end if\n        return x.mm(self.v)\n    # end _execute\n\n    # Update the principal components.\n    def _update_pca(self, x):\n        """"""\n        Update the principal components\n        :param x:\n        :return:\n        """"""\n        # Params\n        [w1, w2] = self._amnesic(self.get_current_train_iteration() + 1)\n        red_j = self.output_dim\n        red_j_flag = False\n        explained_var = 0.0\n\n        # For each output\n        r = x\n        for j in range(self.output_dim):\n            v = self._v[:, j:j + 1]\n            d = self.d[j]\n\n            v = w1 * v + w2 * r.mv(v) / d * r.t()\n            d = torch.norm(v)\n            vn = v / d\n            r = r - r.mv(vn) * vn.t()\n            explained_var += d\n\n            # Red flag\n            if not red_j_flag:\n                ratio = explained_var / self._var_tot\n                if ratio > self.var_rel:\n                    red_j = j\n                    red_j_flag = True\n                # end if\n            # end if\n\n            self._v[:, j:j + 1] = v\n            self.v[:, j:j + 1] = vn\n            self.d[j] = d\n        # end for\n\n        self._var_tot = explained_var\n        self._reduced_dims = red_j\n    # end update_pca\n\n    # Initialize parameters\n    def _check_params(self, *args):\n        """"""\n        Initialize parameters\n        :param args:\n        :return:\n        """"""\n        if self._init_v is None:\n            if self.output_dim is not None:\n                self.init_eigen_vectors = 0.1 * torch.randn(self.input_dim, self.output_dim)\n            else:\n                self.init_eigen_vectors = 0.1 * torch.randn(self.input_dim, self.input_dim)\n            # end if\n        # end if\n    # end _check_params\n\n    # Return amnesic weights\n    def _amnesic(self, n):\n        """"""\n        Return amnesic weights\n        :param n:\n        :return:\n        """"""\n        _i = float(n + 1)\n        n1, n2, m, c = self.amn_params\n        if _i < n1:\n            l = 0\n        elif (_i >= n1) and (_i < n2):\n            l = c * (_i - n1) / (n2 - n1)\n        else:\n            l = c + (_i - n2) / m\n        # end if\n        _world = float(_i - 1 - l) / _i\n        _wnew = float(1 + l) / _i\n        return [_world, _wnew]\n    # end _amnesic\n\n    # Add constant\n    def _add_constant(self, x):\n        """"""\n        Add constant\n        :param x:\n        :return:\n        """"""\n        bias = Variable(torch.ones((x.size()[0], x.size()[1], 1)), requires_grad=False)\n        return torch.cat((bias, x), dim=2)\n    # end _add_constant\n\n# end PCACell\n'"
echotorch/nn/PCACell.py,16,"b'# -*- coding: utf-8 -*-\n#\n# File : echotorch/nn/ESN.py\n# Description : An Echo State Network module.\n# Date : 26th of January, 2018\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti <nils.schaetti@unine.ch>\n\n""""""\nCreated on 26 January 2018\n@author: Nils Schaetti\n""""""\n\n# Imports\nimport torch.sparse\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\n\n# Filter the input data through the most significatives principal components.\nclass PCACell(nn.Module):\n    """"""\n    Filter the input data through the most significatives principal components\n    """"""\n\n    # Constructor\n    def __init__(self, input_dim, output_dim, svd=False, reduce=False, var_rel=1E-12, var_abs=1E-15, var_part=None):\n        """"""\n        Constructor\n        :param input_dim:\n        :param output_dim:\n        :param svd: If True use Singular Value Decomposition instead of the standard eigenvalue problem solver. Use it when PCANode complains about singular covariance matrices.\n        :param reduce: Keep only those principal components which have a variance larger than \'var_abs\'\n        :param val_rel: Variance relative to first principal component threshold. Default is 1E-12.\n        :param var_abs: Absolute variance threshold. Default is 1E-15.\n        :param var_part: Variance relative to total variance threshold. Default is None.\n        """"""\n        # Super\n        super(PCACell, self).__init__()\n\n        # Properties\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.svd = svd\n        self.var_abs = var_abs\n        self.var_rel = var_rel\n        self.var_part = var_part\n        self.reduce = reduce\n\n        # Set it as buffer\n        self.register_buffer(\'xTx\', Variable(torch.zeros(input_dim, input_dim), requires_grad=False))\n        self.register_buffer(\'xTx_avg\', Variable(torch.zeros(input_dim), requires_grad=False))\n\n        # Eigen values\n        self.d = None\n\n        # Eigen vectors, first index for coordinates\n        self.v = None\n\n        # Total variance\n        self.total_variance = None\n\n        # Len, average and explained variance\n        self.tlen = 0\n        self.avg = None\n        self.explained_variance = None\n    # end __init__\n\n    ###############################################\n    # PROPERTIES\n    ###############################################\n\n    ###############################################\n    # PUBLIC\n    ###############################################\n\n    # Reset learning\n    def reset(self):\n        """"""\n        Reset learning\n        :return:\n        """"""\n        # Initialize the covariance matrix one for\n        # the input data.\n        self._init_internals()\n\n        # Training mode again\n        self.train(True)\n    # end reset\n\n    # Forward\n    def forward(self, x, y=None):\n        """"""\n        Forward\n        :param x: Input signal.\n        :param y: Target outputs\n        :return: Output or hidden states\n        """"""\n        # Number of batches\n        n_batches = int(x.size()[0])\n\n        # Time length\n        time_length = x.size()[1]\n\n        # Outputs\n        outputs = Variable(torch.zeros(n_batches, time_length, self.output_dim))\n        outputs = outputs.cuda() if x.is_cuda else outputs\n\n        # For each batch\n        for b in range(n_batches):\n            # Sample\n            s = x[b]\n\n            # Train or execute\n            if self.training:\n                self._update_cov_matrix(s)\n            else:\n                outputs[b] = self._execute_pca(s)\n            # end if\n        # end for\n\n        return outputs\n    # end forward\n\n    # Finish training\n    def finalize(self):\n        """"""\n        Finalize training with LU factorization or Pseudo-inverse\n        """"""\n        # Reshape average\n        xTx, avg, tlen = self._fix(self.xTx, self.xTx_avg, self.tlen)\n\n        # Reshape\n        self.avg = avg.unsqueeze(0)\n\n        # We need more observations than variables\n        if self.tlen < self.input_dim:\n            raise Exception(u""The number of observations ({}) is larger than  the number of input variables ({})"".format(self.tlen, self.input_dim))\n        # end if\n\n        # Total variance\n        total_var = torch.diag(xTx).sum()\n\n        # Compute and sort eigenvalues\n        d, v = torch.symeig(xTx, eigenvectors=True)\n\n        # Check for negative eigenvalues\n        if float(d.min()) < 0:\n            # raise Exception(u""Got negative eigenvalues ({}). You may either set output_dim to be smaller"".format(d))\n            pass\n        # end if\n\n        # Indexes\n        indexes = range(d.size(0)-1, -1, -1)\n\n        # Sort by descending order\n        d = torch.take(d, Variable(torch.LongTensor(indexes)))\n        v = v[:, indexes]\n\n        # Explained covariance\n        self.explained_variance = torch.sum(d) / total_var\n\n        # Store eigenvalues\n        self.d = d[:self.output_dim]\n\n        # Store eigenvectors\n        self.v = v[:, :self.output_dim]\n\n        # Total variance\n        self.total_variance = total_var\n\n        # Stop training\n        self.train(False)\n    # end finalize\n\n    # Get explained variance\n    def get_explained_variance(self):\n        """"""\n        The explained variance is the fraction of the original variance that can be explained by the\n        principal components.\n        :return:\n        """"""\n        return self.explained_variance\n    # end get_explained_variance\n\n    # Get the projection matrix\n    def get_proj_matrix(self, tranposed=True):\n        """"""\n        Get the projection matrix\n        :param tranposed:\n        :return:\n        """"""\n        # Stop training\n        self.train(False)\n\n        # Transposed\n        if tranposed:\n            return self.v\n        # end if\n        return self.v.t()\n    # end get_proj_matrix\n\n    # Get the reconstruction matrix\n    def get_rec_matrix(self, tranposed=1):\n        """"""\n        Returns the reconstruction matrix\n        :param tranposed:\n        :return:\n        """"""\n        # Stop training\n        self.train(False)\n\n        # Transposed\n        if tranposed:\n            return self.v.t()\n        # end if\n        return self.v\n    # end get_rec_matrix\n\n    ###############################################\n    # PRIVATE\n    ###############################################\n\n    # Project the input on the first \'n\' principal components\n    def _execute_pca(self, x, n=None):\n        """"""\n        Project the input on the first \'n\' principal components\n        :param x:\n        :param n:\n        :return:\n        """"""\n        if n is not None:\n            return (x - self.avg).mm(self.v[:, :n])\n        # end if\n        return (x - self.avg).mm(self.v)\n    # end _execute\n\n    # Project data from the output to the input space using the first \'n\' components.\n    def _inverse(self, y, n=None):\n        """"""\n        Project data from the output to the input space using the first \'n\' components.\n        :param y:\n        :param n:\n        :return:\n        """"""\n        if n is None:\n            n = y.shape[1]\n        # end if\n\n        if n > self.output_dim:\n            raise Exception(u""y has dimension {} but should but at most {}"".format(n, self.output_dim))\n        # end if\n\n        # Get reconstruction matrix\n        v = self.get_rec_matrix()\n\n        # Reconstruct\n        if n is not None:\n            return y.mm(v[:n, :]) + self.avg\n        else:\n            return y.mm(v) + self.avg\n        # end if\n    # end _inverse\n\n    # Adjust output dim\n    def _adjust_output_dim(self):\n        """"""\n        If the output dimensions is small than the input dimension\n        :return:\n        """"""\n        # If the number of PC is not specified, keep all\n        if self.desired_variance is None and self.ouput_dim is None:\n            self.output_dim = self.input_dim\n            return None\n        # end if\n\n        # Define the range of eigenvalues to compute if the number of PC to keep\n        # has been specified directly.\n        if self.output_dim is not None and self.output_dim >= 1:\n            return (self.input_dim - self.output_dim + 1, self.input_dim)\n        else:\n            return None\n        # end if\n    # end _adjust_output_dim\n\n    # Fix covariance matrix\n    def _fix(self, mtx, avg, tlen, center=True):\n        """"""\n        Returns a triple containing the covariance matrix, the average and\n        the number of observations.\n        :param mtx:\n        :param center:\n        :return:\n        """"""\n        mtx /= tlen - 1\n\n        # Substract the mean\n        if center:\n            avg_mtx = torch.ger(avg, avg)\n            avg_mtx /= tlen * (tlen - 1)\n            mtx -= avg_mtx\n        # end if\n\n        # Fix the average\n        avg /= tlen\n\n        return mtx, avg, tlen\n    # end fix\n\n    # Update covariance matrix\n    def _update_cov_matrix(self, x):\n        """"""\n        Update covariance matrix\n        :param x:\n        :return:\n        """"""\n        # Init\n        if self.xTx is None:\n            self._init_internals()\n        # end if\n\n        # Update\n        self.xTx.data.add_(x.t().mm(x).data)\n        self.xTx_avg.add_(torch.sum(x, dim=0))\n        self.tlen += x.size(0)\n    # end _update_cov_matrix\n\n    # Initialize covariance\n    def _init_cov_matrix(self):\n        """"""\n        Initialize covariance matrix\n        :return:\n        """"""\n        self.xTx.data = torch.zeros(self.input_dim, self.input_dim)\n        self.xTx_avg.data = torch.zeros(self.input_dim)\n    # end _init_cov_matrix\n\n    # Initialize internals\n    def _init_internals(self):\n        """"""\n        Initialize internals\n        :param x:\n        :return:\n        """"""\n        # Init covariance matrix\n        self._init_cov_matrix()\n    # end _init_internals\n\n    # Add constant\n    def _add_constant(self, x):\n        """"""\n        Add constant\n        :param x:\n        :return:\n        """"""\n        bias = Variable(torch.ones((x.size()[0], x.size()[1], 1)), requires_grad=False)\n        return torch.cat((bias, x), dim=2)\n    # end _add_constant\n\n# end PCACell\n'"
echotorch/nn/RRCell.py,21,"b'# -*- coding: utf-8 -*-\n#\n# File : echotorch/nn/ESN.py\n# Description : An Echo State Network module.\n# Date : 26th of January, 2018\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti <nils.schaetti@unine.ch>\n\n""""""\nCreated on 26 January 2018\n@author: Nils Schaetti\n""""""\n\n# Imports\nimport torch.sparse\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport math\n\n\n# Ridge Regression cell\nclass RRCell(nn.Module):\n    """"""\n    Ridge Regression cell\n    """"""\n\n    # Constructor\n    def __init__(self, input_dim, output_dim, ridge_param=0.0, feedbacks=False, with_bias=True, learning_algo=\'inv\', softmax_output=False, averaged=False, dtype=torch.float32):\n        """"""\n        Constructor\n        :param input_dim: Inputs dimension.\n        :param output_dim: Reservoir size\n        """"""\n        super(RRCell, self).__init__()\n\n        # Properties\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.ridge_param = ridge_param\n        self.feedbacks = feedbacks\n        self.with_bias = with_bias\n        self.learning_algo = learning_algo\n        self.softmax_output = softmax_output\n        self.softmax = torch.nn.Softmax(dim=2)\n        self.averaged = averaged\n        self.n_samples = 0\n        self.dtype = dtype\n\n        # Size\n        if self.with_bias:\n            self.x_size = input_dim + 1\n        else:\n            self.x_size = input_dim\n        # end if\n\n        # Set it as buffer\n        self.register_buffer(\'xTx\', Variable(torch.zeros(self.x_size, self.x_size, dtype=dtype), requires_grad=False))\n        self.register_buffer(\'xTy\', Variable(torch.zeros(self.x_size, output_dim, dtype=dtype), requires_grad=False))\n        self.register_buffer(\'w_out\', Variable(torch.zeros(1, input_dim, dtype=dtype), requires_grad=False))\n    # end __init__\n\n    ###############################################\n    # PROPERTIES\n    ###############################################\n\n    ###############################################\n    # PUBLIC\n    ###############################################\n\n    # Reset learning\n    def reset(self):\n        """"""\n        Reset learning\n        :return:\n        """"""\n        """"""self.xTx.data = torch.zeros(self.x_size, self.x_size)\n        self.xTy.data = torch.zeros(self.x_size, self.output_dim)\n        self.w_out.data = torch.zeros(1, self.input_dim)""""""\n        self.xTx.data.fill_(0.0)\n        self.xTy.data.fill_(0.0)\n        self.w_out.data.fill_(0.0)\n\n        # Training mode again\n        self.train(True)\n    # end reset\n\n    # Output matrix\n    def get_w_out(self):\n        """"""\n        Output matrix\n        :return:\n        """"""\n        return self.w_out\n    # end get_w_out\n\n    # Forward\n    def forward(self, x, y=None):\n        """"""\n        Forward\n        :param x: Input signal.\n        :param y: Target outputs\n        :return: Output or hidden states\n        """"""\n        # Batch size\n        batch_size = x.size()[0]\n\n        # Time length\n        time_length = x.size()[1]\n\n        # Add bias\n        if self.with_bias:\n            x = self._add_constant(x)\n        # end if\n\n        # Learning algo\n        if self.training:\n            for b in range(batch_size):\n                if not self.averaged:\n                    self.xTx.data.add_(x[b].t().mm(x[b]).data)\n                    self.xTy.data.add_(x[b].t().mm(y[b]).data)\n                else:\n                    self.xTx.data.add_((x[b].t().mm(x[b]) / time_length).data)\n                    self.xTy.data.add_((x[b].t().mm(y[b]) / time_length).data)\n                    self.n_samples += 1.0\n            # end for\n\n            # Bias or not\n            if self.with_bias:\n                return x[:, :, 1:]\n            else:\n                return x\n            # end if\n        elif not self.training:\n            # Outputs\n            outputs = Variable(torch.zeros(batch_size, time_length, self.output_dim, dtype=self.dtype), requires_grad=False)\n            outputs = outputs.cuda() if self.w_out.is_cuda else outputs\n\n            # For each batch\n            for b in range(batch_size):\n                outputs[b] = torch.mm(x[b], self.w_out)\n            # end for\n\n            if self.softmax_output:\n                return self.softmax(outputs)\n            else:\n                return outputs\n        # end if\n    # end forward\n\n    # Finish training\n    def finalize(self, train=False):\n        """"""\n        Finalize training with LU factorization or Pseudo-inverse\n        """"""\n        if self.learning_algo == \'inv\':\n            if not self.averaged:\n                ridge_xTx = self.xTx + self.ridge_param * torch.eye(self.input_dim + self.with_bias, dtype=self.dtype)\n                inv_xTx = ridge_xTx.inverse()\n                self.w_out.data = torch.mm(inv_xTx, self.xTy).data\n            else:\n                # Average\n                self.xTx = self.xTx / self.n_samples\n                self.xTy = self.xTy / self.n_samples\n\n                # Algo\n                ridge_xTx = self.xTx + self.ridge_param * torch.eye(self.input_dim + self.with_bias, dtype=self.dtype)\n                inv_xTx = ridge_xTx.inverse()\n                self.w_out.data = torch.mm(inv_xTx, self.xTy).data\n            # end if\n        else:\n            self.w_out.data = torch.gesv(self.xTy, self.xTx + torch.eye(self.esn_cell.output_dim).mul(self.ridge_param)).data\n        # end if\n\n        # Not in training mode anymore\n        self.train(train)\n    # end finalize\n\n    ###############################################\n    # PRIVATE\n    ###############################################\n\n    # Add constant\n    def _add_constant(self, x):\n        """"""\n        Add constant\n        :param x:\n        :return:\n        """"""\n        if x.is_cuda:\n            bias = Variable(torch.ones((x.size()[0], x.size()[1], 1), dtype=self.dtype).cuda(), requires_grad=False)\n        else:\n            bias = Variable(torch.ones((x.size()[0], x.size()[1], 1), dtype=self.dtype), requires_grad=False)\n        # end if\n        return torch.cat((bias, x), dim=2)\n    # end _add_constant\n\n# end RRCell\n'"
echotorch/nn/SFACell.py,11,"b'# -*- coding: utf-8 -*-\n#\n# File : echotorch/nn/ESN.py\n# Description : An Echo State Network module.\n# Date : 26th of January, 2018\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti <nils.schaetti@unine.ch>\n\n""""""\nCreated on 26 January 2018\n@author: Nils Schaetti\n""""""\n\n# Imports\nimport torch.sparse\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom past.utils import old_div\n\n\n# Slow Feature Analysis layer\nclass SFACell(nn.Module):\n    """"""\n    Extract the slowly varying components from input data.\n    """"""\n\n    # Type keys\n    _type_keys = [\'f\', \'d\', \'F\', \'D\']\n\n    # Type conv\n    _type_conv = {(\'f\', \'d\'): \'d\', (\'f\', \'F\'): \'F\', (\'f\', \'D\'): \'D\',\n                  (\'d\', \'F\'): \'D\', (\'d\', \'D\'): \'D\',\n                  (\'F\', \'d\'): \'D\', (\'F\', \'D\'): \'D\'}\n\n    # Constructor\n    def __init__(self, input_dim, output_dim, include_last_sample=True, rank_deficit_method=\'none\', use_bias=True):\n        """"""\n        Constructor\n        :param input_dim: Input dimension\n        :param output_dim: Number of slow feature\n        :param include_last_sample: If set to False, the training method discards the last sample in every chunk during training when calculating the matrix.\n        :param rank_deficit_method: \'none\', \'reg\', \'pca\', \'svd\', \'auto\'.\n        """"""\n        super(SFACell, self).__init__()\n        self.include_last_sample = include_last_sample\n        self.use_bias = use_bias\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n\n        # Initialie the two covariance matrices one for\n        # the input data, and the other for the derivatives.\n        self.xTx = torch.zeros(input_dim, input_dim)\n        self.xTx_avg = torch.zeros(input_dim)\n        self.dxTdx = torch.zeros(input_dim, input_dim)\n        self.dxTdx_avg = torch.zeros(input_dim)\n\n        # Set routine for eigenproblem\n        self.set_rank_deficit_method(rank_deficit_method)\n        self.rank_threshold = 1e-12\n        self.rank_deficit = 0\n\n        # Will be set after training\n        self.d = None\n        self.sf = None\n        self.avg = None\n        self.bias = None\n        self.tlen = None\n    # end __init__\n\n    ###############################################\n    # PROPERTIES\n    ###############################################\n\n    ###############################################\n    # PUBLIC\n    ###############################################\n\n    # Time derivative\n    def time_derivative(self, x):\n        """"""\n        Compute the approximation of time derivative\n        :param x:\n        :return:\n        """"""\n        return x[1:, :] - x[:-1, :]\n    # end time_derivative\n\n    # Reset learning\n    def reset(self):\n        """"""\n        Reset learning\n        :return:\n        """"""\n        # Training mode again\n        self.train(True)\n    # end reset\n\n    # Forward\n    def forward(self, x):\n        """"""\n        Forward\n        :param x: Input signal.\n        :return: Output or hidden states\n        """"""\n        # For each batch\n        for b in np.arange(0, x.size(0)):\n            # If training or execution\n            if self.training:\n                # Last sample\n                last_sample_index = None if self.include_last_sample else -1\n\n                # Sample and derivative\n                xs = x[b, :last_sample_index, :]\n                xd = self.time_derivative(x[b])\n\n                # Update covariance matrix\n                self.xTx.data.add(xs.t().mm(xs))\n                self.dxTdx.data.add(xd.t().mm(xd))\n\n                # Update average\n                self.xTx_avg += torch.sum(xs, axis=1)\n                self.dxTdx_avg += torch.sum(xd, axis=1)\n\n                # Length\n                self.tlen += x.size(0)\n            else:\n                x[b].mv(self.sf) - self.bias\n            # end if\n        # end if\n        return x\n    # end forward\n\n    # Finish training\n    def finalize(self):\n        """"""\n        Finalize training with LU factorization or Pseudo-inverse\n        """"""\n        # Covariance\n        self.xTx, self.xTx_avg, self.tlen = self._fix(self.xtX, self.xTx_avg, self.tlen, center=True)\n        self.dxTdx, self.dxTdx_avg, self.tlen = self._fix(self.dxTdx, self.dxTdx_avg, self.tlen, center=False)\n\n        # Range\n        rng = (1, self.output_dim)\n\n        # Resolve system\n        self.d, self.sf = self._symeig(\n            self.dxTdx, self.xTx, rng\n        )\n        d = self.d\n\n        # We want only positive values\n        if torch.min(d) < 0:\n            raise Exception(u""Got negative values in {}"".format(d))\n        # end if\n\n        # Delete covariance matrix\n        del self.xTx\n        del self.dxTdx\n\n        # Store bias\n        self.bias = self.xTx_avg * self.sf\n    # end finalize\n\n    ###############################################\n    # PRIVATE\n    ###############################################\n\n    # Solve standard and generalized eigenvalue problem for symmetric (hermitian) definite positive matrices\n    def _symeig(self, A, B, range, eigenvectors=True):\n        """"""\n        Solve standard and generalized eigenvalue problem for symmetric (hermitian) definite positive matrices.\n        :param A: An N x N matrix\n        :param B: An N x N matrix\n        :param range: (lo, hi), the indexes of smallest and largest eigenvalues to be returned.\n        :param eigenvectors: Return eigenvalues and eigenvector or only engeivalues\n        :return: w, the eigenvalues and Z the eigenvectors\n        """"""\n        # To numpy\n        A = A.numpy()\n        B = B.numpy()\n\n        # Type\n        dtype = np.dtype()\n\n        # Make B the identity matrix\n        wB, ZB = np.linalg.eigh(B)\n\n        # Check eigenvalues\n        self._assert_eigenvalues_real(wB)\n\n        # No negative values\n        if wB.real.min() < 0:\n            raise Exception(u""Got negative eigenvalues: {}"".format(wB))\n        # end if\n\n        # Old division\n        ZB = old_div(ZB.real, np.sqrt(wB.real))\n\n        # A = ZB^T * A * ZB\n        A = np.matmul(np.matmul(ZB.T, A), ZB)\n\n        # Diagonalize A\n        w, ZA = np.linalg.eigh(A)\n        Z = np.matmul(ZB, ZA)\n\n        # Check eigenvalues\n        self._assert_eigenvalues_real(w, dtype)\n\n        # Read\n        w = w.real\n        Z = Z.real\n\n        # Sort\n        idx = w.argsort()\n        w = w.take(idx)\n        Z = Z.take(idx, axis=1)\n\n        # Sanitize range\n        n = A.shape[0]\n        lo, hi = range\n        if lo < 1:\n            lo = 1\n        # end if\n        if lo > n:\n            lo = n\n        # end if\n        if hi > n:\n            hi = n\n        # end if\n        if lo > hi:\n            lo, hi = hi, lo\n        # end if\n\n        # Get values\n        Z = Z[:, lo-1:hi]\n        w = w[lo-1:hi]\n\n        # Cast\n        w = self.refcast(w, dtype)\n        Z = self.refcast(Z, dtype)\n\n        # Eigenvectors\n        if eigenvectors:\n            return torch.FloatTensor(w), torch.FloatTensor(Z)\n        else:\n            return torch.FloatTensor(w)\n        # end if\n    # end _symeig\n\n    # Ref cast\n    def refcast(self, array, dtype):\n        """"""\n        Cast the array to dtype only if necessary, otherwise return a reference.\n        """"""\n        dtype = np.dtype(dtype)\n        if array.dtype == dtype:\n            return array\n        return array.astype(dtype)\n    # end refcast\n\n    # Check eigenvalues\n    def _assert_eigenvalues_real(self, w, dtype):\n        """"""\n        Check eigenvalues\n        :param w:\n        :param dtype:\n        :return:\n        """"""\n        tol = np.finfo(dtype.type).eps * 100\n        if abs(w.imag).max() > tol:\n            err = ""Some eigenvalues have significant imaginary part: %s "" % str(w)\n            raise Exception(err)\n        # end if\n    # end _assert_eigenvalues_real\n\n    # Greatest common type\n    def _greatest_common_dtype(self, alist):\n        """"""\n        Apply conversion rules to find the common conversion type\n        dtype \'d\' is default for \'i\' or unknown types\n        (known types: \'f\',\'d\',\'F\',\'D\').\n        """"""\n        dtype = \'f\'\n        for array in alist:\n            if array is None:\n                continue\n            tc = array.dtype.char\n            if tc not in self._type_keys:\n                tc = \'d\'\n            transition = (dtype, tc)\n            if transition in self._type_conv:\n                dtype = self._type_conv[transition]\n        return dtype\n    # end _greatest_common_dtype\n\n    # Fix covariance matrix\n    def _fix(self, mtx, avg, tlen, center=True):\n        """"""\n        Returns a triple containing the covariance matrix, the average and\n        the number of observations.\n        :param mtx:\n        :param center:\n        :return:\n        """"""\n        if self.use_bias:\n            mtx /= tlen\n        else:\n            mtx /= tlen - 1\n        # end if\n\n        # Substract the mean\n        if center:\n            avg_mtx = np.outer(avg, avg)\n            if self.use_bias:\n                avg_mtx /= tlen * tlen\n            else:\n                avg_mtx /= tlen * (tlen - 1)\n            # end if\n            mtx -= avg_mtx\n        # end if\n\n        # Fix the average\n        avg /= tlen\n\n        return mtx, avg, tlen\n    # end fix\n\n# end SFACell\n'"
echotorch/nn/StackedESN.py,13,"b'# -*- coding: utf-8 -*-\n#\n# File : echotorch/nn/ESN.py\n# Description : An Echo State Network module.\n# Date : 26th of January, 2018\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti, University of Neuch\xc3\xa2tel <nils.schaetti@unine.ch>\n\n""""""\nCreated on 26 January 2018\n@author: Nils Schaetti\n""""""\n\n# Imports\nimport torch.sparse\nimport torch\nimport torch.nn as nn\nimport echotorch.utils\nfrom torch.autograd import Variable\nfrom . import LiESNCell\nfrom .RRCell import RRCell\nfrom .ESNCell import ESNCell\nimport numpy as np\n\n\n# Stacked Echo State Network module\nclass StackedESN(nn.Module):\n    """"""\n    Stacked Echo State Network module\n    """"""\n\n    # Constructor\n    def __init__(self, input_dim, hidden_dim, output_dim, leaky_rate=1.0, spectral_radius=0.9, bias_scaling=0,\n                 input_scaling=1.0, w=None, w_in=None, w_bias=None, sparsity=None, input_set=(1.0, -1.0),\n                 w_sparsity=None, nonlin_func=torch.tanh, learning_algo=\'inv\', ridge_param=0.0, with_bias=True):\n        """"""\n        Constructor\n\n        Arguments:\n            :param input_dim: Inputs dimension.\n            :param hidden_dim: Hidden layer dimension\n            :param output_dim: Reservoir size\n            :param spectral_radius: Reservoir\'s spectral radius\n            :param bias_scaling: Scaling of the bias, a constant input to each neuron (default: 0, no bias)\n            :param input_scaling: Scaling of the input weight matrix, default 1.\n            :param w: Internation weights matrix\n            :param w_in: Input-reservoir weights matrix\n            :param w_bias: Bias weights matrix\n            :param w_fdb: Feedback weights matrix\n            :param sparsity:\n            :param input_set:\n            :param w_sparsity:\n            :param nonlin_func: Reservoir\'s activation function (tanh, sig, relu)\n            :param learning_algo: Which learning algorithm to use (inv, LU, grad)\n        """"""\n        super(StackedESN, self).__init__()\n\n        # Properties\n        self.n_layers = len(hidden_dim)\n        self.esn_layers = list()\n\n        # Number of features\n        self.n_features = 0\n\n        # Recurrent layer\n        for n in range(self.n_layers):\n            # Input dim\n            layer_input_dim = input_dim if n == 0 else hidden_dim[n-1]\n\n            # Final state size\n            self.n_features += hidden_dim[n]\n\n            # Parameters\n            layer_leaky_rate = leaky_rate[n] if type(leaky_rate) is list or type(leaky_rate) is np.ndarray else leaky_rate\n            layer_spectral_radius = spectral_radius[n] if type(spectral_radius) is list or type(spectral_radius) is np.ndarray else spectral_radius\n            layer_bias_scaling = bias_scaling[n] if type(bias_scaling) is list or type(bias_scaling) is np.ndarray else bias_scaling\n            layer_input_scaling = input_scaling[n] if type(input_scaling) is list or type(input_scaling) is np.ndarray else input_scaling\n\n            # W\n            if type(w) is torch.Tensor and w.dim() == 3:\n                layer_w = w[n]\n            elif type(w) is torch.Tensor:\n                layer_w = w\n            else:\n                layer_w = None\n            # end if\n\n            # W in\n            if type(w_in) is torch.Tensor and w_in.dim() == 3:\n                layer_w_in = w_in[n]\n            elif type(w_in) is torch.Tensor:\n                layer_w_in = w_in\n            else:\n                layer_w_in = None\n            # end if\n\n            # W bias\n            if type(w_bias) is torch.Tensor and w_bias.dim() == 2:\n                layer_w_bias = w_bias[n]\n            elif type(w_bias) is torch.Tensor:\n                layer_w_bias = w_bias\n            else:\n                layer_w_bias = None\n            # end if\n\n            # Parameters\n            layer_sparsity = sparsity[n] if type(sparsity) is list or type(sparsity) is np.ndarray else sparsity\n            layer_input_set = input_set[n] if type(input_set) is list or type(input_set) is np.ndarray else input_set\n            layer_w_sparsity = w_sparsity[n] if type(w_sparsity) is list or type(w_sparsity) is np.ndarray else w_sparsity\n            layer_nonlin_func = nonlin_func[n] if type(nonlin_func) is list or type(nonlin_func) is np.ndarray else nonlin_func\n\n            # Create LiESN cell\n            self.esn_layers.append(LiESNCell(\n                layer_leaky_rate, False, layer_input_dim, hidden_dim[n], layer_spectral_radius, layer_bias_scaling,\n                layer_input_scaling, layer_w, layer_w_in, layer_w_bias, None, layer_sparsity, layer_input_set,\n                layer_w_sparsity, layer_nonlin_func\n            ))\n        # end for\n\n        # Output layer\n        self.output = RRCell(self.n_features, output_dim, ridge_param, False, with_bias, learning_algo)\n    # end __init__\n\n    ###############################################\n    # PROPERTIES\n    ###############################################\n\n    # Hidden layer\n    @property\n    def hidden(self):\n        """"""\n        Hidden layer\n        :return:\n        """"""\n        # Hidden states\n        hidden_states = list()\n\n        # For each ESN\n        for esn_cell in self.esn_layers:\n            hidden_states.append(esn_cell.hidden)\n        # end for\n\n        return hidden_states\n    # end hidden\n\n    # Hidden weight matrix\n    @property\n    def w(self):\n        """"""\n        Hidden weight matrix\n        :return:\n        """"""\n        # W\n        w_mtx = list()\n\n        # For each ESN\n        for esn_cell in self.esn_layers:\n            w_mtx.append(esn_cell.w)\n        # end for\n\n        return w_mtx\n    # end w\n\n    # Input matrix\n    @property\n    def w_in(self):\n        """"""\n        Input matrix\n        :return:\n        """"""\n        # W in\n        win_mtx = list()\n\n        # For each ESN\n        for esn_cell in self.esn_layers:\n            win_mtx.append(esn_cell.w_in)\n        # end for\n\n        return win_mtx\n    # end w_in\n\n    ###############################################\n    # PUBLIC\n    ###############################################\n\n    # Reset learning\n    def reset(self):\n        """"""\n        Reset learning\n        :return:\n        """"""\n        self.output.reset()\n\n        # Training mode again\n        self.train(True)\n    # end reset\n\n    # Output matrix\n    def get_w_out(self):\n        """"""\n        Output matrix\n        :return:\n        """"""\n        return self.output.w_out\n    # end get_w_out\n\n    # Forward\n    def forward(self, u, y=None):\n        """"""\n        Forward\n        :param u: Input signal.\n        :param y: Target outputs\n        :return: Output or hidden states\n        """"""\n        # Hidden states\n        hidden_states = Variable(torch.zeros(u.size(0), u.size(1), self.n_features))\n\n        # Compute hidden states\n        pos = 0\n        for index, esn_cell in enumerate(self.esn_layers):\n            layer_dim = esn_cell.output_dim\n            if index == 0:\n                last_hidden_states = esn_cell(u)\n            else:\n                last_hidden_states = esn_cell(last_hidden_states)\n            # end if\n\n            # Update\n            hidden_states[:, :, pos:pos + layer_dim] = last_hidden_states\n\n            # Next position\n            pos += layer_dim\n        # end for\n\n        # Learning algo\n        return self.output(hidden_states, y)\n    # end forward\n\n    # Finish training\n    def finalize(self):\n        """"""\n        Finalize training with LU factorization\n        """"""\n        # Finalize output training\n        self.output.finalize()\n\n        # Not in training mode anymore\n        self.train(False)\n    # end finalize\n\n    # Reset hidden layer\n    def reset_hidden(self):\n        """"""\n        Reset hidden layer\n        :return:\n        """"""\n        self.esn_cell.reset_hidden()\n    # end reset_hidden\n\n    # Get W\'s spectral radius\n    def get_spectral_radius(self):\n        """"""\n        Get W\'s spectral radius\n        :return: W\'s spectral radius\n        """"""\n        return self.esn_cell.get_spectral_raduis()\n    # end spectral_radius\n\n    ############################################\n    # STATIC\n    ############################################\n\n    # Generate W matrices for a stacked ESN\n    @staticmethod\n    def generate_ws(n_layers, reservoir_size, w_sparsity):\n        """"""\n        Generate W matrices for a stacked ESN\n        :param n_layers:\n        :param reservoir_size:\n        :param w_sparsity:\n        :return:\n        """"""\n        ws = torch.FloatTensor(n_layers, reservoir_size, reservoir_size)\n        for i in range(n_layers):\n            ws[i] = ESNCell.generate_w(reservoir_size, w_sparsity)\n        # end for\n        return ws\n    # end for\n\n# end ESNCell\n'"
echotorch/nn/UMESN.py,0,b''
echotorch/nn/__init__.py,0,"b""# -*- coding: utf-8 -*-\n#\n\n# Imports\nfrom .BDESN import BDESN\nfrom .BDESNPCA import BDESNPCA\nfrom .BDESNCell import BDESNCell\nfrom .Conceptor import Conceptor\nfrom .ConceptorNet import ConceptorNet\nfrom .ConceptorNetCell import ConceptorNetCell\nfrom .ConceptorPool import ConceptorPool\nfrom .ESNCell import ESNCell\nfrom .ESN import ESN\nfrom .LiESNCell import LiESNCell\nfrom .LiESN import LiESN\nfrom .GatedESN import GatedESN\nfrom .ICACell import ICACell\nfrom .Identity import Identity\nfrom .PCACell import PCACell\nfrom .RRCell import RRCell\nfrom .SFACell import SFACell\nfrom .StackedESN import StackedESN\n\n__all__ = [\n    'BDESN', 'BDESNPCA', 'BDESNCell', 'Conceptor', 'ConceptorNet', 'ConceptorNetCell', 'ConceptorPool', 'ESNCell',\n    'ESN', 'LiESNCell', 'LiESN', 'GatedESN', 'ICACell', 'Identity', 'PCACell', 'RRCell', 'SFACell', 'StackedESN'\n]\n"""
echotorch/transforms/__init__.py,2,"b""# -*- coding: utf-8 -*-\n#\n\n# Imports\nimport echotorch.transforms.images\nimport echotorch.transforms.text\n\n__all__ = [\n    'text', 'images'\n]\n"""
echotorch/utils/__init__.py,0,"b""# -*- coding: utf-8 -*-\n#\n\n# Imports\nfrom .error_measures import nrmse, nmse, rmse, mse, perplexity, cumperplexity, generalized_squared_cosine\nfrom .utility_functions import align_pattern, compute_correlation_matrix, spectral_radius, deep_spectral_radius, normalize, average_prob, max_average_through_time, compute_singular_values, compute_similarity_matrix, find_phase_shift\nfrom .visualisation import show_3d_timeseries, show_2d_timeseries, show_1d_timeseries, neurons_activities_1d, neurons_activities_2d, neurons_activities_3d, plot_singular_values, show_similarity_matrix, show_conceptors_similarity_matrix, show_sv_for_increasing_aperture\n\n__all__ = [\n    'align_pattern', 'compute_correlation_matrix', 'nrmse', 'nmse', 'rmse', 'mse', 'perplexity', 'cumperplexity', 'spectral_radius', 'deep_spectral_radius',\n    'normalize', 'average_prob', 'max_average_through_time', 'show_3d_timeseries', 'show_2d_timeseries',\n    'show_1d_timeseries', 'neurons_activities_1d', 'neurons_activities_2d', 'neurons_activities_3d',\n    'plot_singular_values', 'compute_singular_values', 'generalized_squared_cosine', 'compute_similarity_matrix',\n    'show_similarity_matrix', 'show_conceptors_similarity_matrix', 'show_sv_for_increasing_aperture',\n    'find_phase_shift'\n]\n"""
echotorch/utils/error_measures.py,18,"b'# -*- coding: utf-8 -*-\n#\n\n# Imports\nimport torch\nimport math\nfrom decimal import Decimal\nimport numpy as np\n\n\n# Normalized root-mean-square error\ndef nrmse(outputs, targets):\n    """"""\n    Normalized root-mean square error\n    :param outputs: Module\'s outputs\n    :param targets: Target signal to be learned\n    :return: Normalized root-mean square deviation\n    """"""\n    # Flatten tensors\n    outputs = outputs.view(outputs.nelement())\n    targets = targets.view(targets.nelement())\n\n    # Check dim\n    if outputs.size() != targets.size():\n        raise ValueError(u""Ouputs and targets tensors don have the same number of elements"")\n    # end if\n\n    # Normalization with N-1\n    var = torch.std(targets) ** 2\n\n    # Error\n    error = (targets - outputs) ** 2\n\n    # Return\n    return float(math.sqrt(torch.mean(error) / var))\n# end nrmse\n\n\n# Root-mean square error\ndef rmse(outputs, targets):\n    """"""\n    Root-mean square error\n    :param outputs: Module\'s outputs\n    :param targets: Target signal to be learned\n    :return: Root-mean square deviation\n    """"""\n    # Flatten tensors\n    outputs = outputs.view(outputs.nelement())\n    targets = targets.view(targets.nelement())\n\n    # Check dim\n    if outputs.size() != targets.size():\n        raise ValueError(u""Ouputs and targets tensors don have the same number of elements"")\n    # end if\n\n    # Error\n    error = (targets - outputs) ** 2\n\n    # Return\n    return float(math.sqrt(torch.mean(error)))\n# end rmsd\n\n\n# Mean square error\ndef mse(outputs, targets):\n    """"""\n    Mean square error\n    :param outputs: Module\'s outputs\n    :param targets: Target signal to be learned\n    :return: Mean square deviation\n    """"""\n    # Flatten tensors\n    outputs = outputs.view(outputs.nelement())\n    targets = targets.view(targets.nelement())\n\n    # Check dim\n    if outputs.size() != targets.size():\n        raise ValueError(u""Ouputs and targets tensors don have the same number of elements"")\n    # end if\n\n    # Error\n    error = (targets - outputs) ** 2\n\n    # Return\n    return float(torch.mean(error))\n# end mse\n\n\n# Normalized mean square error\ndef nmse(outputs, targets):\n    """"""\n    Normalized mean square error\n    :param outputs: Module\'s output\n    :param targets: Target signal to be learned\n    :return: Normalized mean square deviation\n    """"""\n    # Flatten tensors\n    outputs = outputs.view(outputs.nelement())\n    targets = targets.view(targets.nelement())\n\n    # Check dim\n    if outputs.size() != targets.size():\n        raise ValueError(u""Ouputs and targets tensors don have the same number of elements"")\n    # end if\n\n    # Normalization with N-1\n    var = torch.std(targets) ** 2\n\n    # Error\n    error = (targets - outputs) ** 2\n\n    # Return\n    return float(torch.mean(error) / var)\n# end nmse\n\n\n# Perplexity\ndef perplexity(output_probs, targets, log=False):\n    """"""\n    Perplexity\n    :param output_probs: Output probabilities for each word/tokens (length x n_tokens)\n    :param targets: Real word index\n    :return: Perplexity\n    """"""\n    pp = Decimal(1.0)\n    e_vec = torch.FloatTensor(output_probs.size(0), output_probs.size(1)).fill_(np.e)\n    if log:\n        set_p = 1.0 / torch.gather(torch.pow(e_vec, exponent=output_probs.data.cpu()), 1,\n                                   targets.data.cpu().unsqueeze(1))\n    else:\n        set_p = 1.0 / torch.gather(output_probs.data.cpu(), 1, targets.data.cpu().unsqueeze(1))\n    # end if\n    for j in range(set_p.size(0)):\n        pp *= Decimal(set_p[j][0])\n    # end for\n    return pp\n# end perplexity\n\n\n# Cumulative perplexity\ndef cumperplexity(output_probs, targets, log=False):\n    """"""\n    Cumulative perplexity\n    :param output_probs:\n    :param targets:\n    :param log:\n    :return:\n    """"""\n    # Get prob of test events\n    set_p = torch.gather(output_probs, 1, targets.unsqueeze(1))\n\n    # Make sure it\'s log\n    if not log:\n        set_p = torch.log(set_p)\n    # end if\n\n    # Log2\n    set_log = set_p / np.log(2)\n\n    # sum log\n    sum_log = torch.sum(set_log)\n\n    # Return\n    return sum_log\n# end cumperplexity\n\n\n# Generalized squared cosine\ndef generalized_squared_cosine(Sa, Ua, Sb, Ub):\n    """"""\n    Generalized square cosine\n    :param Sa:\n    :param Ua:\n    :param Sb:\n    :param Ub:\n    :return:\n    """"""\n    # Diag\n    Sa = torch.diag(Sa)\n    Sb = torch.diag(Sb)\n\n    # Compute Va and Vb\n    Va = torch.sqrt(Sa).mm(Ua.t())\n    Vb = Ub.mm(torch.sqrt(Sb))\n\n    # Vab\n    Vab = Va.mm(Vb)\n\n    # Num\n    num = torch.pow(torch.norm(Vab), 2)\n\n    # Den\n    den = torch.norm(torch.diag(Sa), p=2) * torch.norm(torch.diag(Sb), p=2)\n\n    return num / den\n# end generalized_squared_cosine\n'"
echotorch/utils/utility_functions.py,13,"b'# -*- coding: utf-8 -*-\n#\n\n# Imports\nimport torch\nimport numpy as np\nfrom .error_measures import nrmse, generalized_squared_cosine\nfrom scipy.interpolate import interp1d\nimport numpy.linalg as lin\n\n\n# Compute correlation matrix\ndef compute_correlation_matrix(states):\n    """"""\n    Compute correlation matrix\n    :param states:\n    :return:\n    """"""\n    return states.t().mm(states) / float(states.size(0))\n# end compute_correlation_matrix\n\n\n# Align pattern\ndef align_pattern(interpolation_rate, truth_pattern, generated_pattern):\n    """"""\n    Align pattern\n    :param interpolation_rate:\n    :param truth_pattern:\n    :param generated_pattern:\n    :return:\n    """"""\n    # Length\n    truth_length = truth_pattern.size(0)\n    generated_length = generated_pattern.size(0)\n\n    # Remove useless dimension\n    truth_pattern = truth_pattern.view(-1)\n    generated_pattern = generated_pattern.view(-1)\n\n    # Quadratic interpolation functions\n    truth_pattern_func = interp1d(np.arange(truth_length), truth_pattern.numpy(), kind=\'quadratic\')\n    generated_pattern_func = interp1d(np.arange(generated_length), generated_pattern.numpy(), kind=\'quadratic\')\n\n    # Get interpolated patterns\n    truth_pattern_int = truth_pattern_func(np.arange(0, truth_length - 1.0, 1.0 / interpolation_rate))\n    generated_pattern_int = generated_pattern_func(np.arange(0, generated_length - 1.0, 1.0 / interpolation_rate))\n\n    # Generated interpolated pattern length\n    L = generated_pattern_int.shape[0]\n\n    # Truth interpolated pattern length\n    M = truth_pattern_int.shape[0]\n\n    # Save L2 distance for each phase shift\n    phase_matches = np.zeros(L - M)\n\n    # For each phase shift\n    for phases_hift in range(L - M):\n        phase_matches[phases_hift] = lin.norm(truth_pattern_int - generated_pattern_int[phases_hift:phases_hift + M])\n    # end for\n\n    # Best match\n    max_ind = int(np.argmax(-phase_matches))\n\n    # Get the position in the original signal\n    coarse_max_ind = int(np.ceil(max_ind / interpolation_rate))\n\n    # Get the generated output matching the original signal\n    generated_aligned = generated_pattern_int[\n        np.arange(max_ind, max_ind + interpolation_rate * truth_length, interpolation_rate)\n    ]\n\n    return max_ind, coarse_max_ind, torch.from_numpy(generated_aligned).view(-1, 1)\n# end align_pattern\n\n\n# Find phase shift\ndef find_phase_shift(p, y, interpolation_rate, error_measure=nrmse):\n    """"""\n    Find phase shift\n    :param s1:\n    :param s2:\n    :param window_size:\n    :return:\n    """"""\n    # Size\n    p_length = p.size(0)\n    y_length = y.size(0)\n\n    # 1D\n    p = p.view(-1)\n    y = y.view(-1)\n\n    # Interpolate p and y\n    p_int = torch.from_numpy(np.interp(np.arange(0, p_length, 1.0 / interpolation_rate), np.arange(p_length), p.numpy()))\n    y_int = torch.from_numpy(np.interp(np.arange(0, y_length, 1.0 / interpolation_rate), np.arange(y_length), y.numpy()))\n\n    # New shape\n    L = y_int.shape[0]\n    M = p_int.shape[0]\n\n    # Find best phase\n    phasematches = torch.zeros(L - M)\n    for phaseshift in range(L - M):\n        phasematches[phaseshift] = torch.norm(p_int - y_int[phaseshift:phaseshift + M], p=2)\n    # end for\n\n    # Best phase\n    max_index = torch.argmax(-phasematches)\n    # Matching phase\n    y_aligned = y_int[np.arange(max_index, max_index + interpolation_rate * p_length, interpolation_rate)]\n\n    # Original phase\n    original_phase = np.ceil(max_index / interpolation_rate)\n\n    # Error after alignment\n    error_aligned = error_measure(y_aligned.reshape(1, -1), p.reshape(1, -1))\n\n    return p, y_aligned, original_phase, error_aligned\n# end find_phase_shift\n\n\n# Compute similarity matrix\ndef compute_similarity_matrix(svd_list):\n    """"""\n    Compute similarity matrix\n    :param svd_list:\n    :return:\n    """"""\n    # N samples\n    n_samples = len(svd_list)\n\n    # Similarity matrix\n    sim_matrix = torch.zeros(n_samples, n_samples)\n\n    # For each combinasion\n    for i, (Sa, Ua) in enumerate(svd_list):\n        for j, (Sb, Ub) in enumerate(svd_list):\n            sim_matrix[i, j] = generalized_squared_cosine(Sa, Ua, Sb, Ub)\n        # end for\n    # end for\n\n    return sim_matrix\n# end compute_similarity_matrix\n\n\n# Compute singular values\ndef compute_singular_values(stats):\n    """"""\n    Compute singular values\n    :param states:\n    :return:\n    """"""\n    # Compute R (correlation matrix)\n    R = stats.t().mm(stats) / stats.shape[0]\n\n    # Compute singular values\n    return torch.svd(R)\n# end compute_singular_values\n\n\n# Compute spectral radius of a square 2-D tensor\ndef spectral_radius(m):\n    """"""\n    Compute spectral radius of a square 2-D tensor\n    :param m: squared 2D tensor\n    :return:\n    """"""\n    return torch.max(torch.abs(torch.eig(m)[0])).item()\n# end spectral_radius\n\n\n# Compute spectral radius of a square 2-D tensor for stacked-ESN\ndef deep_spectral_radius(m, leaky_rate):\n    """"""\n    Compute spectral radius of a square 2-D tensor for stacked-ESN\n    :param m: squared 2D tensor\n    :param leaky_rate: Layer\'s leaky rate\n    :return:\n    """"""\n    return spectral_radius((1.0 - leaky_rate) * torch.eye(m.size(0), m.size(0)) + leaky_rate * m)\n# end spectral_radius\n\n\n# Normalize a tensor on a single dimension\ndef normalize(tensor, dim=1):\n    """"""\n    Normalize a tensor on a single dimension\n    :param t:\n    :return:\n    """"""\n    pass\n# end normalize\n\n\n# Average probabilties through time\ndef average_prob(tensor, dim=0):\n    """"""\n    Average probabilities through time\n    :param tensor:\n    :param dim:\n    :return:\n    """"""\n    return torch.mean(tensor, dim=dim)\n# end average_prob\n\n\n# Max average through time\ndef max_average_through_time(tensor, dim=0):\n    """"""\n    Max average through time\n    :param tensor:\n    :param dim: Time dimension\n    :return:\n    """"""\n    average = torch.mean(tensor, dim=dim)\n    return torch.max(average, dim=dim)[1]\n# end max_average_through_time\n'"
echotorch/utils/visualisation.py,4,"b'# -*- coding: utf-8 -*-\n#\n\n# Imports\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom echotorch.nn.PCACell import PCACell\nfrom sklearn.decomposition import PCA\n\n\n# Show singular values increasing aperture\ndef show_sv_for_increasing_aperture(conceptor, factor, title):\n    """"""\n    Show singular values for increasing aperture\n    :param conceptors:\n    :param factor:\n    :param title:\n    :return:\n    """"""\n    # Fig\n    fig = plt.figure()\n    ax = fig.gca()\n    ax.set_xlim(0, 100)\n    ax.set_ylim(0, 1.5)\n    ax.grid(True)\n\n    # For each aperture multiplication\n    for i in range(5):\n        # Compute SVD\n        _, S, _ = torch.svd(conceptor.get_C())\n\n        # Plot\n        ax.plot(S.numpy(), \'--\')\n\n        # Multiply all conceptor\'s aperture by 10\n        conceptor.multiply_aperture(factor)\n    # end for\n\n    # Show\n    ax.set_xlabel(u""Singular values"")\n    ax.set_title(title)\n    plt.show()\n    plt.close()\n# end show_sv_for_increasing_aperture\n\n\n# Show conceptors similarity matrix\ndef show_conceptors_similarity_matrix(conceptors, title):\n    """"""\n    Show conceptors similarity matrix\n    :param conceptors:\n    :param title:\n    :return:\n    """"""\n    # Labels\n    labels = list()\n\n    # Similarity matrix\n    sim_matrix = torch.zeros(len(conceptors), len(conceptors))\n    for i, ca in enumerate(conceptors):\n        labels.append(ca.name)\n        for j, cb in enumerate(conceptors):\n            sim_matrix[i, j] = ca.sim(cb)\n        # end for\n    # end for\n    show_similarity_matrix(sim_matrix, title, labels, labels)\n# end conceptors_similarity_matrix\n\n\n# Show similarity matrix\ndef show_similarity_matrix(sim_matrix, title, column_labels=None, row_labels=None):\n    """"""\n    Show similarity matrix\n    :param sim_matrix:\n    :return:\n    """"""\n    # Get cmap\n    cmap = plt.cm.get_cmap(\'Greens\')\n    fig = plt.figure()\n    plt.title(title)\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(sim_matrix, interpolation=\'nearest\', cmap=cmap)\n    fig.colorbar(cax)\n    ax.set_xticks(np.arange(len(row_labels)))\n    ax.set_yticks(np.arange(len(column_labels)))\n    ax.set_xticklabels(row_labels, rotation=90)\n    ax.set_yticklabels(column_labels)\n    plt.show()\n# end show_similarity_matrix\n\n\n# Plot singular values\ndef plot_singular_values(stats, title, xmin, xmax, ymin, ymax, log=False):\n    """"""\n    Plot singular values\n    :param stats:\n    :param title:\n    :param timestep:\n    :param start:\n    :return:\n    """"""\n    # Compute R (correlation matrix)\n    R = stats.t().mm(stats) / stats.shape[0]\n\n    # Compute singular values\n    U, S, V = torch.svd(R)\n    singular_values = S\n\n    # Compute singular values\n    if log:\n        singular_values = np.log10(singular_values)\n    # end if\n\n    # Fig\n    fig = plt.figure()\n    ax = fig.gca()\n    ax.set_xlim(xmin, xmax)\n    ax.set_ylim(ymin, ymax)\n    ax.grid(True)\n\n    # For each plot\n    ax.plot(singular_values.numpy(), \'--o\')\n\n    ax.set_xlabel(""Timesteps"")\n    ax.set_title(title)\n    plt.show()\n    plt.close()\n\n    return singular_values, U\n# end plot_singular_values\n\n\n# Display neurons activities on a 3D plot\ndef neurons_activities_3d(stats, neurons, title, timesteps=-1, start=0):\n    """"""\n    Display neurons activities on a 3D plot\n    :param stats:\n    :param neurons:\n    :param title:\n    :param timesteps:\n    :param start:\n    :return:\n    """"""\n    # Fig\n    ax = plt.axes(projection=\'3d\')\n\n    # Two by two\n    n_neurons = neurons.shape[0]\n    stats = stats[:, neurons].view(-1, n_neurons // 3, 3)\n\n    # Plot\n    if timesteps == -1:\n        time_length = stats.shape[0]\n        ax.plot3D(stats[:, :, 0].view(time_length).numpy(), stats[:, :, 1].view(time_length).numpy(), stats[:, :, 2].view(time_length).numpy(), \'o\')\n    else:\n        ax.plot3D(stats[start:start + timesteps, :, 0].numpy(), stats[start:start + timesteps, :, 1].numpy(), stats[start:start + timesteps, :, 2].numpy(), \'o\', lw=0.5)\n    # end if\n    ax.set_xlabel(""X Axis"")\n    ax.set_ylabel(""Y Axis"")\n    ax.set_zlabel(""Z Axis"")\n    ax.set_title(title)\n    plt.show()\n    plt.close()\n# end neurons_activities_3d\n\n\n# Display neurons activities on a 2D plot\ndef neurons_activities_2d(stats, neurons, title, colors, timesteps=-1, start=0):\n    """"""\n    Display neurons activities on a 2D plot\n    :param stats:\n    :param neurons:\n    :param title:\n    :param timesteps:\n    :param start:\n    :return:\n    """"""\n    # Fig\n    fig = plt.figure()\n    ax = fig.gca()\n\n    # Two by two\n    n_neurons = neurons.shape[0]\n\n    # For each plot\n    for i, stat in enumerate(stats):\n        # Stats\n        stat = stat[:, neurons].view(-1, n_neurons // 2, 2)\n\n        # Plot\n        if timesteps == -1:\n            ax.plot(stat[:, :, 0].numpy(), stat[:, :, 1].numpy(), colors[i])\n        else:\n            ax.plot(stat[start:start + timesteps, :, 0].numpy(), stat[start:start + timesteps, :, 1].numpy(), colors[i])\n        # end if\n    # end for\n    ax.set_xlabel(""X Axis"")\n    ax.set_ylabel(""Y Axis"")\n    ax.set_title(title)\n    plt.show()\n    plt.close()\n# end neurons_activities_2d\n\n\n# Display neurons activities\ndef neurons_activities_1d(stats, neurons, title, colors, xmin, xmax, ymin, ymax, timesteps=-1, start=0):\n    """"""\n    Display neurons activities\n    :param stats:\n    :param neurons:\n    :return:\n    """"""\n    # Fig\n    fig = plt.figure()\n    ax = fig.gca()\n    ax.set_xlim(xmin, xmax)\n    ax.set_ylim(ymin, ymax)\n    ax.grid(True)\n\n    # For each neurons\n    for i, n in enumerate(neurons):\n        if timesteps == -1:\n            ax.plot(stats[:, n].numpy(), colors[i])\n        else:\n            ax.plot(stats[start:start + timesteps, n].numpy(), colors[i])\n        # end if\n    # end for\n\n    ax.set_xlabel(""Timesteps"")\n    ax.set_title(title)\n    plt.show()\n    plt.close()\n# end neurons_activities_1d\n\n\n# Show 3D time series\ndef show_3d_timeseries(ts, title):\n    """"""\n    Show 3D timeseries\n    :param axis:\n    :param title:\n    :return:\n    """"""\n    # Fig\n    fig = plt.figure()\n    ax = fig.gca(projection=\'3d\')\n\n    ax.plot(ts[:, 0].numpy(), ts[:, 1].numpy(), ts[:, 2].numpy(), lw=0.5)\n    ax.set_xlabel(""X Axis"")\n    ax.set_ylabel(""Y Axis"")\n    ax.set_zlabel(""Z Axis"")\n    ax.set_title(title)\n    plt.show()\n    plt.close()\n# end show_3d_timeseries\n\n\n# Show 2D time series\ndef show_2d_timeseries(ts, title):\n    """"""\n    Show 2D timeseries\n    :param ts:\n    :param title:\n    :return:\n    """"""\n    # Fig\n    fig = plt.figure()\n    ax = fig.gca()\n\n    ax.plot(ts[:, 0].numpy(), ts[:, 1].numpy(), lw=0.5)\n    ax.set_xlabel(""X Axis"")\n    ax.set_ylabel(""Y Axis"")\n    ax.set_title(title)\n    plt.show()\n    plt.close()\n# end show_2d_timeseries\n\n\n# Show 1D time series\ndef show_1d_timeseries(ts, title, xmin, xmax, ymin, ymax, start=0, timesteps=-1):\n    """"""\n    Show 1D time series\n    :param ts:\n    :param title:\n    :return:\n    """"""\n    # Fig\n    fig = plt.figure()\n    ax = fig.gca()\n    ax.set_xlim(xmin, xmax)\n    ax.set_ylim(ymin, ymax)\n    ax.grid(True)\n\n    if timesteps == -1:\n        ax.plot(ts[:, 0].numpy())\n    else:\n        ax.plot(ts[start:start+timesteps, 0].numpy())\n    # end if\n    ax.set_xlabel(""X Axis"")\n    ax.set_title(title)\n    plt.show()\n    plt.close()\n# end show_1d_timeseries\n'"
examples/MNIST/convert_images.py,1,"b'# -*- coding: utf-8 -*-\n#\n# File : examples/MNIST/convert_images.py\n# Description : Convert images to time series.\n# Date : 6th of April, 2017\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti <nils.schaetti@unine.ch>\n\n""""""\nCreated on 6 April 2017\n@author: Nils Schaetti\n""""""\n\nimport sys\nimport os\nsys.path.insert(0, os.path.abspath(\'./../..\'))\nimport echotorch\n\n\nif __name__ == ""__main__"":\n\n    converter = echotorch.dataset.ImageConverter()\n\n# end if\n'"
examples/conceptors/conceptors_4_patterns_generation.py,22,"b'# -*- coding: utf-8 -*-\n#\n# File : rnn_control_with_conceptor_4_patterns.py\n# Description : Section 3.2 of the paper ""Controlling RNN with Conceptors""\n# Date : 17th of January, 2019\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti <nils.schaetti@unine.ch>\n\n# Imports\nfrom echotorch.datasets import PeriodicSignalDataset\nfrom echotorch.datasets import SinusoidalTimeseries\nfrom echotorch.datasets import DatasetComposer\nfrom echotorch.nn import ConceptorNet, Conceptor\nimport echotorch.utils\nimport torch\nfrom torch.utils.data.dataloader import DataLoader\nimport matplotlib.pyplot as plt\n\n# Precision\ntorch.set_printoptions(precision=10)\n\n# Dataset params\nwashout_period = 500\ntrain_sample_length = 1000\ntest_sample_length = 1000\nn_train_samples = 1\nn_test_samples = 1\nbatch_size = 1\n\n# Params\nreservoir_size = 100\nspectral_radius = 1.5\ninput_scaling = 1.5\nbias_scaling = 0.2\ninput_sparsity = 1.0\nleak_rate = 1.0\nw_sparsity = 0.1\nwin_mean = 0.0\nwin_std = 1.0\nw_mean = 0.0\nw_std = 1.0\nwbias_mean = 0.0\nwbias_std = 1.0\nridge_param = 0.01\nw_ridge_param = 0.0001\naperture = 1\nfactor = 10.0\n\n# First pattern\npattern1_training = SinusoidalTimeseries(\n    sample_len=washout_period + train_sample_length,\n    n_samples=n_train_samples,\n    a=1,\n    w=0.711572515,\n    g=11\n)\n\n# Second pattern\npattern2_training = SinusoidalTimeseries(\n    sample_len=washout_period + train_sample_length,\n    n_samples=n_train_samples,\n    a=1,\n    w=0.63918467,\n    g=0\n)\n\n# Third pattern\npattern3_training = PeriodicSignalDataset(\n    sample_len=washout_period + train_sample_length,\n    n_samples=n_train_samples,\n    period=[-1.0, -1.0, -0.1, 0.2, 1.0]\n)\n\n# Fourth pattern\npattern4_training = PeriodicSignalDataset(\n    sample_len=washout_period + train_sample_length,\n    n_samples=n_train_samples,\n    period=[-1.0, -0.8, -0.1, 0.2, 1.0]\n)\n\n# Composer\ndataset_training = DatasetComposer([pattern1_training, pattern2_training, pattern3_training, pattern4_training])\n\n# Data loader\npatterns_loader = DataLoader(dataset_training, batch_size=batch_size, shuffle=False, num_workers=1)\n\n# Echo State Network\nesn = ConceptorNet(\n    input_dim=1,\n    hidden_dim=int(reservoir_size),\n    output_dim=1,\n    spectral_radius=spectral_radius,\n    sparsity=input_sparsity,\n    input_scaling=input_scaling,\n    w_sparsity=w_sparsity,\n    leaky_rate=leak_rate,\n    bias_scaling=bias_scaling,\n    win_distrib=\'normal\',\n    win_normal=(win_mean, win_std),\n    w_distrib=\'normal\',\n    w_normal=(w_mean, w_std),\n    wbias_distrib=\'normal\',\n    wbias_normal=(wbias_mean, wbias_std),\n    washout=washout_period,\n    ridge_param=ridge_param,\n    w_ridge_param=w_ridge_param,\n    seed=1,\n    dtype=torch.float64\n)\n\n# The four conceptors\npattern1_conceptor = Conceptor(reservoir_size, aperture=aperture, name=""pattern1"", dtype=torch.float64)\npattern2_conceptor = Conceptor(reservoir_size, aperture=aperture, name=""pattern2"", dtype=torch.float64)\npattern3_conceptor = Conceptor(reservoir_size, aperture=aperture, name=""pattern3"", dtype=torch.float64)\npattern4_conceptor = Conceptor(reservoir_size, aperture=aperture, name=""pattern4"", dtype=torch.float64)\npattern_conceptors = [pattern1_conceptor, pattern2_conceptor, pattern3_conceptor, pattern4_conceptor]\n\n# Total states\ntotal_states = torch.FloatTensor()\n\n# List of SVD\nsvd_matrices = list()\n\n# List of patterns\npatterns = list()\n\n# Go through dataset\nfor i, data in enumerate(patterns_loader):\n    # Inputs and labels\n    inputs, outputs, labels = data\n\n    # Show timeseries\n    echotorch.utils.show_1d_timeseries(ts=inputs[0], title=u""p{}"".format(i+1), xmin=0, xmax=20, ymin=-1, ymax=1, timesteps=21)\n\n    # Compute hidden states\n    hidden_states = esn(u=inputs.view(1, -1, 1).double(), y=inputs.view(1, -1, 1).double(), c=pattern_conceptors[i])\n\n    # Save patterns\n    patterns.append(inputs)\n\n    # Show neurons activities\n    echotorch.utils.neurons_activities_1d(\n        stats=hidden_states[0],\n        neurons=torch.LongTensor([0, 50, 99]),\n        title=u""Random neurons activities of p{}"".format(i+1),\n        colors=[\'r\', \'g\', \'b\'],\n        xmin=0,\n        xmax=20,\n        ymin=-1,\n        ymax=1,\n        timesteps=21\n    )\n\n    # Show log10 PC energy\n    echotorch.utils.plot_singular_values(stats=hidden_states[0], title=u""Log10 PC energy (p{})"".format(i+1), log=True, xmin=0, xmax=100, ymin=-20, ymax=10)\n\n    # Show leading PC energy\n    S, U = echotorch.utils.plot_singular_values(stats=hidden_states[0], title=u""Leading PC energy (p{})"".format(i+1), xmin=0, xmax=10, ymin=0, ymax=40)\n\n    # Save SVD\n    svd_matrices.append((S, U))\n# end for\n\n# Compute similarity matrix\nsim_matrix = echotorch.utils.compute_similarity_matrix(svd_matrices)\nechotorch.utils.show_similarity_matrix(sim_matrix, u""R-based similarities"")\n\n# Close the conceptor net\nesn.finalize()\n\n# Close conceptors\nfor i in range(4):\n    pattern_conceptors[i].finalize()\n# end for\n\n# Show similarities between conceptors\nechotorch.utils.show_conceptors_similarity_matrix(pattern_conceptors, u""C-based similarities (a = {})"".format(aperture))\n\n# For each patterns\nfor i, p in enumerate(patterns):\n    # Plot original pattern\n    plt.plot(p[0, washout_period:washout_period + 40].numpy())\n\n    # Legends\n    legends = list()\n    legends.append(u""Pattern {}"".format(i+1))\n\n    # Aperture\n    a = aperture\n\n    # Regenerate pattern 1 for different aperture\n    for j in range(5):\n        # Generate timeseries\n        y_hat = esn(c=pattern_conceptors[i],\n                    reset_state=True,\n                    length=washout_period + train_sample_length\n        )\n\n        # Phase shift\n        phase_shift, nrmse = echotorch.utils.find_phase_shift(y_hat[0, washout_period:], p[0, washout_period:].double(), window_size=40)\n\n        # Plot\n        plt.plot(y_hat[0, washout_period+phase_shift:washout_period+phase_shift + 40].numpy())\n\n        # Legend\n        legends.append(u""a = {} ({})"".format(a, nrmse))\n\n        # Multiply aperture\n        pattern_conceptors[i].multiply_aperture(factor)\n        a *= factor\n    # end for\n\n    plt.legend(legends, loc=\'upper right\')\n    plt.title(u""Regeneration of pattern {}"".format(i+1))\n    plt.show()\n    plt.close()\n# end for\n'"
examples/datasets/logistic_map.py,3,"b'# -*- coding: utf-8 -*-\n#\n\n# Imports\nimport echotorch.datasets\nfrom torch.utils.data.dataloader import DataLoader\n\n\n# Logistoc map dataset\nlog_map = echotorch.datasets.LogisticMapDataset(10000, 10)\n\n# Dataset\nlog_map_dataset = DataLoader(log_map, batch_size=10, shuffle=True)\n\n# For each sample\nfor data in log_map_dataset:\n    print(data[0])\n# end for\n'"
examples/generation/narma10_esn_feedbacks.py,10,"b'# -*- coding: utf-8 -*-\n#\n# File : examples/timeserie_prediction/switch_attractor_esn\n# Description : NARMA 30 prediction with ESN.\n# Date : 26th of January, 2018\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti <nils.schaetti@unine.ch>\n\n\n# Imports\nimport torch\nfrom echotorch.datasets.NARMADataset import NARMADataset\nimport echotorch.nn as etnn\nimport echotorch.utils\nfrom torch.autograd import Variable\nfrom torch.utils.data.dataloader import DataLoader\nimport numpy as np\nimport mdp\n\n# Dataset params\ntrain_sample_length = 5000\ntest_sample_length = 1000\nn_train_samples = 1\nn_test_samples = 1\nbatch_size = 1\nspectral_radius = 0.9\nleaky_rate = 1.0\ninput_dim = 1\nn_hidden = 100\n\n# Use CUDA?\nuse_cuda = False\nuse_cuda = torch.cuda.is_available() if use_cuda else False\n\n# Manual seed\nmdp.numx.random.seed(1)\nnp.random.seed(2)\ntorch.manual_seed(1)\n\n# NARMA30 dataset\nnarma10_train_dataset = NARMADataset(train_sample_length, n_train_samples, system_order=10, seed=1)\nnarma10_test_dataset = NARMADataset(test_sample_length, n_test_samples, system_order=10, seed=10)\n\n# Data loader\ntrainloader = DataLoader(narma10_train_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\ntestloader = DataLoader(narma10_test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n# ESN cell\nesn = etnn.ESN(\n    input_dim=input_dim,\n    hidden_dim=n_hidden,\n    output_dim=1,\n    spectral_radius=spectral_radius,\n    learning_algo=\'inv\',\n    # leaky_rate=leaky_rate,\n    feedbacks=True\n)\nif use_cuda:\n    esn.cuda()\n# end if\n\n# For each batch\nfor data in trainloader:\n    # Inputs and outputs\n    inputs, targets = data\n\n    # To variable\n    inputs, targets = Variable(inputs), Variable(targets)\n    if use_cuda: inputs, targets = inputs.cuda(), targets.cuda()\n\n    # Accumulate xTx and xTy\n    esn(inputs, targets)\n# end for\n\n# Finalize training\nesn.finalize()\n\n# Test MSE\ndataiter = iter(testloader)\ntest_u, test_y = dataiter.next()\ntest_u, test_y = Variable(test_u), Variable(test_y)\ngen_u = Variable(torch.zeros(batch_size, test_sample_length, input_dim))\nif use_cuda: test_u, test_y, gen_u = test_u.cuda(), test_y.cuda(), gen_u.cuda()\ny_predicted = esn(test_u)\nprint(u""Test MSE: {}"".format(echotorch.utils.mse(y_predicted.data, test_y.data)))\nprint(u""Test NRMSE: {}"".format(echotorch.utils.nrmse(y_predicted.data, test_y.data)))\nprint(u"""")\n\ny_generated = esn(gen_u)\nprint(y_generated)\n'"
examples/memory/memtest.py,3,"b""# -*- coding: utf-8 -*-\n#\n# File : examples/SwitchAttractor/switch_attractor_esn\n# Description : Attractor switching task with ESN.\n# Date : 26th of January, 2018\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti <nils.schaetti@unine.ch>\n\n\n# Imports\nimport torch\nfrom EchoTorch.datasets.MemTestDataset import MemTestDataset\nimport EchoTorch.nn as etnn\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data.dataloader import DataLoader\nimport matplotlib.pyplot as plt\n\n# Dataset params\nsample_length = 20\nn_samples = 2\nbatch_size = 5\n\n# MemTest dataset\nmemtest_dataset = MemTestDataset(sample_length, n_samples, seed=1)\n\n# Data loader\ndataloader = DataLoader(memtest_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n# ESN properties\ninput_dim = 1\nn_hidden = 20\n\n# ESN cell\nesn = etnn.ESNCell(input_dim, n_hidden)\n\n# Linear layer\nlinear = nn.Linear(n_hidden, 1)\n\n# Objective function\ncriterion = nn.MSELoss()\n\n# Learning rate\nlearning_rate = 0.0001\n\n# Number of iterations\nn_iterations = 10\n\nfor data in dataloader:\n    # For each sample\n    for i_sample in range(data[0].size()[0]):\n        # Inputs and outputs\n        inputs, outputs = data[0][i_sample], data[1][i_sample]\n        inputs, outputs = Variable(inputs), Variable(outputs)\n\n        # Show the graph\n        plt.plot(inputs.data.numpy(), c='b')\n        plt.plot(outputs.data[:, 9].numpy(), c='r')\n        plt.show()\n    # end for\n# end for"""
examples/models/NilsNet_example.py,4,"b'# -*- coding: utf-8 -*-\n#\n# File : examples/timeserie_prediction/switch_attractor_esn\n# Description : NARMA 30 prediction with ESN.\n# Date : 26th of January, 2018\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti <nils.schaetti@unine.ch>\n\n\n# Imports\nimport torch\nimport echotorch.utils\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom torch.autograd import Variable\n\n\ndef imshow(inp, title=None):\n    """"""Imshow for Tensor.""""""\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.show()\n# end imshow\n\n# Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    \'train\': transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    \'val\': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndata_dir = \'hymenoptera_data\'\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in [\'train\', \'val\']}\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=4)\n              for x in [\'train\', \'val\']}\ndataset_sizes = {x: len(image_datasets[x]) for x in [\'train\', \'val\']}\nclass_names = image_datasets[\'train\'].classes\n\n# Create a NilsNet\nnilsnet = echotorch.models.NilsNet(reservoir_dim=1000, sfa_dim=100, ica_dim=100)\n\n# Get a batch of training data\ninputs, classes = next(iter(dataloaders[\'train\']))\nprint(inputs.size())\nprint(classes.size())\n\ninputs = Variable(inputs)\nclasses = Variable(classes)\n\n# Make a grid from batch\n# out = torchvision.utils.make_grid(inputs)\n\n# imshow(out, title=[class_names[x] for x in classes])\n\noutputs = nilsnet(inputs)\n\nprint(outputs)\nprint(outputs.size())'"
examples/nodes/pca_tests.py,4,"b'# -*- coding: utf-8 -*-\n#\n# File : examples/timeserie_prediction/switch_attractor_esn\n# Description : NARMA 30 prediction with ESN.\n# Date : 26th of January, 2018\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti <nils.schaetti@unine.ch>\n\n\n# Imports\nimport torch\nimport echotorch.nn as etnn\nfrom torch.autograd import Variable\nimport mdp\n\n\n# Settings\ninput_dim = 10\noutput_dim = 3\ntlen = 500\n\n# Generate\ntraining_samples = torch.randn(1, tlen, input_dim)\ntest_samples = torch.randn(1, tlen, input_dim)\n\n# Generate\ntraining_samples_np = training_samples[0].numpy()\ntest_samples_np = test_samples[0].numpy()\n\n# Show\nprint(u""Training samples : {}"".format(training_samples_np))\nprint(u""Test samples : {}"".format(test_samples_np))\n\n# PCA node\nmdp_pca_node = mdp.Flow([mdp.nodes.PCANode(input_dim=input_dim, output_dim=output_dim)])\nmdp_pca_node.train(training_samples_np)\npca_reduced = mdp_pca_node(test_samples_np)\n\n# Show\nprint(u""PCA reduced : {}"".format(pca_reduced))\n\n# EchoTorch PCA node\net_pca_node = etnn.PCACell(input_dim=input_dim, output_dim=output_dim)\net_pca_node(Variable(training_samples))\net_pca_node.finalize()\net_reduced = et_pca_node(Variable(test_samples))\n\n# Show\nprint(u""Reduced with EchoTorch/PCA :"")\nprint(et_reduced)\n'"
examples/switch_attractor/switch_attractor_esn.py,7,"b""# -*- coding: utf-8 -*-\n#\n# File : examples/timeserie_prediction/switch_attractor_esn\n# Description : NARMA 30 prediction with ESN.\n# Date : 26th of January, 2018\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti <nils.schaetti@unine.ch>\n\n\n# Imports\nimport torch\nfrom echotorch.datasets.SwitchAttractorDataset import SwitchAttractorDataset\nimport echotorch.nn as etnn\nimport echotorch.utils\nfrom torch.autograd import Variable\nfrom torch.utils.data.dataloader import DataLoader\nimport numpy as np\nimport mdp\nimport matplotlib.pyplot as plt\n\n# Dataset params\ntrain_sample_length = 1000\ntest_sample_length = 1000\nn_train_samples = 40\nn_test_samples = 10\nbatch_size = 1\nspectral_radius = 0.9\nleaky_rate = 1.0\ninput_dim = 1\nn_hidden = 100\n\n# Use CUDA?\nuse_cuda = False\nuse_cuda = torch.cuda.is_available() if use_cuda else False\n\n# Manual seed\nmdp.numx.random.seed(1)\nnp.random.seed(2)\ntorch.manual_seed(1)\n\n# Switch attractor dataset\nswitch_train_dataset = SwitchAttractorDataset(train_sample_length, n_train_samples, seed=1)\nswitch_test_dataset = SwitchAttractorDataset(test_sample_length, n_test_samples, seed=10)\n\n# Data loader\ntrainloader = DataLoader(switch_train_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\ntestloader = DataLoader(switch_test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n# ESN cell\nesn = etnn.LiESN(input_dim=input_dim, hidden_dim=n_hidden, output_dim=1, spectral_radius=spectral_radius,\n                 learning_algo='inv', leaky_rate=leaky_rate, feedbacks=True)\nif use_cuda:\n    esn.cuda()\n# end if\n\n# For each batch\nfor data in trainloader:\n    # Inputs and outputs\n    inputs, targets = data\n\n    # To variable\n    inputs, targets = Variable(inputs), Variable(targets)\n    if use_cuda: inputs, targets = inputs.cuda(), targets.cuda()\n    # plt.plot(targets.data[0].numpy(), c='b')\n    # plt.plot(y_predicted.data[0, :, 0].numpy(), c='r')\n    # plt.show()\n    # Accumulate xTx and xTy\n    esn(inputs, targets)\n# end for\n\n# Finalize training\nesn.finalize()\n\n# For each batch\nfor data in testloader:\n    # Test MSE\n    test_u, test_y = data\n    test_u, test_y = Variable(test_u), Variable(test_y)\n    if use_cuda: test_u, test_y = test_u.cuda(), test_y.cuda()\n    y_predicted = esn(test_u)\n    plt.ylim(ymax=10)\n    plt.plot(test_y.data[0].numpy(), c='b')\n    plt.plot(y_predicted.data[0, :, 0].numpy(), c='r')\n    plt.show()\n# end for\n"""
examples/timeserie_prediction/mackey_glass_esn.py,10,"b'# -*- coding: utf-8 -*-\n#\n# File : examples/SwitchAttractor/switch_attractor_esn\n# Description : Attractor switching task with ESN.\n# Date : 26th of January, 2018\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti <nils.schaetti@unine.ch>\n\n\n# Imports\nimport torch\nfrom echotorch.datasets.MackeyGlassDataset import MackeyGlassDataset\nimport echotorch.nn as etnn\nimport echotorch.utils\nfrom torch.autograd import Variable\nfrom torch.utils.data.dataloader import DataLoader\n\n# Dataset params\ntrain_sample_length = 5000\ntest_sample_length = 1000\nn_train_samples = 1\nn_test_samples = 1\nspectral_radius = 0.9\nleaky_rate = 1.0\ninput_dim = 1\nn_hidden = 100\n\n# Use CUDA?\nuse_cuda = False\nuse_cuda = torch.cuda.is_available() if use_cuda else False\n\n# Mackey glass dataset\nmackey_glass_train_dataset = MackeyGlassDataset(train_sample_length, n_train_samples, tau=30)\nmackey_glass_test_dataset = MackeyGlassDataset(test_sample_length, n_test_samples, tau=30)\n\n# Data loader\ntrainloader = DataLoader(mackey_glass_train_dataset, batch_size=1, shuffle=False, num_workers=2)\ntestloader = DataLoader(mackey_glass_test_dataset, batch_size=1, shuffle=False, num_workers=2)\n\n# ESN cell\nesn = etnn.LiESN(\n    input_dim=input_dim,\n    hidden_dim=n_hidden,\n    output_dim=1,\n    spectral_radius=spectral_radius,\n    learning_algo=\'inv\',\n    leaky_rate=leaky_rate\n)\nif use_cuda:\n    esn.cuda()\n# end if\n\n# For each batch\nfor data in trainloader:\n    # Inputs and outputs\n    inputs, targets = data\n\n    # To variable\n    inputs, targets = Variable(inputs), Variable(targets)\n    if use_cuda: inputs, targets = inputs.cuda(), targets.cuda()\n\n    # Accumulate xTx and xTy\n    esn(inputs, targets)\n# end for\n\n# Finalize training\nesn.finalize()\n\n# Train MSE\ndataiter = iter(trainloader)\ntrain_u, train_y = dataiter.next()\ntrain_u, train_y = Variable(train_u), Variable(train_y)\nif use_cuda: train_u, train_y = train_u.cuda(), train_y.cuda()\ny_predicted = esn(train_u)\nprint(u""Train MSE: {}"".format(echotorch.utils.mse(y_predicted.data, train_y.data)))\nprint(u""Test NRMSE: {}"".format(echotorch.utils.nrmse(y_predicted.data, train_y.data)))\nprint(u"""")\n\n# Test MSE\ndataiter = iter(testloader)\ntest_u, test_y = dataiter.next()\ntest_u, test_y = Variable(test_u), Variable(test_y)\nif use_cuda: test_u, test_y = test_u.cuda(), test_y.cuda()\ny_predicted = esn(test_u)\nprint(u""Test MSE: {}"".format(echotorch.utils.mse(y_predicted.data, test_y.data)))\nprint(u""Test NRMSE: {}"".format(echotorch.utils.nrmse(y_predicted.data, test_y.data)))\nprint(u"""")\n'"
examples/timeserie_prediction/narma10_esn.py,11,"b'# -*- coding: utf-8 -*-\n#\n# File : examples/timeserie_prediction/switch_attractor_esn\n# Description : NARMA 30 prediction with ESN.\n# Date : 26th of January, 2018\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti <nils.schaetti@unine.ch>\n\n\n# Imports\nimport torch\nfrom echotorch.datasets.NARMADataset import NARMADataset\nimport echotorch.nn as etnn\nimport echotorch.utils\nfrom torch.autograd import Variable\nfrom torch.utils.data.dataloader import DataLoader\nimport numpy as np\nimport mdp\n\n# Dataset params\ntrain_sample_length = 5000\ntest_sample_length = 1000\nn_train_samples = 1\nn_test_samples = 1\nbatch_size = 1\nspectral_radius = 0.9\nleaky_rate = 1.0\ninput_dim = 1\nn_hidden = 100\n\n# Use CUDA?\nuse_cuda = False\nuse_cuda = torch.cuda.is_available() if use_cuda else False\n\n# Manual seed\nmdp.numx.random.seed(1)\nnp.random.seed(2)\ntorch.manual_seed(1)\n\n# NARMA30 dataset\nnarma10_train_dataset = NARMADataset(train_sample_length, n_train_samples, system_order=10, seed=1)\nnarma10_test_dataset = NARMADataset(test_sample_length, n_test_samples, system_order=10, seed=10)\n\n# Data loader\ntrainloader = DataLoader(narma10_train_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\ntestloader = DataLoader(narma10_test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n# ESN cell\nesn = etnn.LiESN(input_dim=input_dim, hidden_dim=n_hidden, output_dim=1, spectral_radius=spectral_radius, learning_algo=\'inv\', leaky_rate=leaky_rate)\nif use_cuda:\n    esn.cuda()\n# end if\n\n# For each batch\nfor data in trainloader:\n    # Inputs and outputs\n    inputs, targets = data\n\n    # To variable\n    inputs, targets = Variable(inputs), Variable(targets)\n    if use_cuda: inputs, targets = inputs.cuda(), targets.cuda()\n\n    # Accumulate xTx and xTy\n    esn(inputs, targets)\n# end for\n\n# Finalize training\nesn.finalize()\n\n# Train MSE\ndataiter = iter(trainloader)\ntrain_u, train_y = dataiter.next()\ntrain_u, train_y = Variable(train_u), Variable(train_y)\nif use_cuda: train_u, train_y = train_u.cuda(), train_y.cuda()\ny_predicted = esn(train_u)\nprint(u""Train MSE: {}"".format(echotorch.utils.mse(y_predicted.data, train_y.data)))\nprint(u""Test NRMSE: {}"".format(echotorch.utils.nrmse(y_predicted.data, train_y.data)))\nprint(u"""")\n\n# Test MSE\ndataiter = iter(testloader)\ntest_u, test_y = dataiter.next()\ntest_u, test_y = Variable(test_u), Variable(test_y)\nif use_cuda: test_u, test_y = test_u.cuda(), test_y.cuda()\ny_predicted = esn(test_u)\nprint(u""Test MSE: {}"".format(echotorch.utils.mse(y_predicted.data, test_y.data)))\nprint(u""Test NRMSE: {}"".format(echotorch.utils.nrmse(y_predicted.data, test_y.data)))\nprint(u"""")\n'"
examples/timeserie_prediction/narma10_esn_sgd.py,12,"b'# -*- coding: utf-8 -*-\n#\n# File : examples/timeserie_prediction/switch_attractor_esn\n# Description : NARMA 30 prediction with ESN.\n# Date : 26th of January, 2018\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti <nils.schaetti@unine.ch>\n\n\n# Imports\nimport torch\nimport torch.optim as optim\nfrom echotorch.datasets.NARMADataset import NARMADataset\nimport echotorch.nn as etnn\nimport echotorch.utils\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data.dataloader import DataLoader\nimport numpy as np\nimport mdp\nimport matplotlib.pyplot as plt\n\n# Parameters\nspectral_radius = 0.9\nleaky_rate = 1.0\nlearning_rate = 0.04\ninput_dim = 1\nn_hidden = 100\nn_iterations = 2000\ntrain_sample_length = 5000\ntest_sample_length = 1000\nn_train_samples = 1\nn_test_samples = 1\nbatch_size = 1\nmomentum = 0.95\nweight_decay = 0\n\n# Use CUDA?\nuse_cuda = True\nuse_cuda = torch.cuda.is_available() if use_cuda else False\n\n# Manual seed\nmdp.numx.random.seed(1)\nnp.random.seed(2)\ntorch.manual_seed(1)\n\n# NARMA30 dataset\nnarma10_train_dataset = NARMADataset(train_sample_length, n_train_samples, system_order=10, seed=1)\nnarma10_test_dataset = NARMADataset(test_sample_length, n_test_samples, system_order=10, seed=10)\n\n# Data loader\ntrainloader = DataLoader(narma10_train_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\ntestloader = DataLoader(narma10_test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n# ESN cell\nesn = etnn.ESN(input_dim=input_dim, hidden_dim=n_hidden, output_dim=1, spectral_radius=spectral_radius, learning_algo=\'grad\')\nif use_cuda:\n    esn.cuda()\n# end if\n\n# Objective function\ncriterion = nn.MSELoss()\n\n# Stochastic Gradient Descent\noptimizer = optim.SGD(esn.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n\n# For each iteration\nfor epoch in range(n_iterations):\n    # Iterate over batches\n    for data in trainloader:\n        # Inputs and outputs\n        inputs, targets = data\n        inputs, targets = Variable(inputs), Variable(targets)\n        if use_cuda: inputs, targets = inputs.cuda(), targets.cuda()\n\n        # Gradients to zero\n        optimizer.zero_grad()\n\n        # Forward\n        out = esn(inputs)\n        loss = criterion(out, targets)\n\n        # Backward pass\n        loss.backward()\n\n        # Optimize\n        optimizer.step()\n\n        # Print error measures\n        print(u""Train MSE: {}"".format(float(loss.data)))\n        print(u""Train NRMSE: {}"".format(echotorch.utils.nrmse(out.data, targets.data)))\n    # end for\n\n    # Test reservoir\n    dataiter = iter(testloader)\n    test_u, test_y = dataiter.next()\n    test_u, test_y = Variable(test_u), Variable(test_y)\n    if use_cuda: test_u, test_y = test_u.cuda(), test_y.cuda()\n    y_predicted = esn(test_u)\n\n    # Print error measures\n    print(u""Test MSE: {}"".format(echotorch.utils.mse(y_predicted.data, test_y.data)))\n    print(u""Test NRMSE: {}"".format(echotorch.utils.nrmse(y_predicted.data, test_y.data)))\n    print(u"""")\n# end for\n'"
examples/timeserie_prediction/narma10_gated_esn.py,12,"b'# -*- coding: utf-8 -*-\n#\n# File : examples/timeserie_prediction/switch_attractor_esn\n# Description : NARMA 30 prediction with ESN.\n# Date : 26th of January, 2018\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti <nils.schaetti@unine.ch>\n\n\n# Imports\nimport torch\nimport torch.optim as optim\nfrom echotorch.datasets.NARMADataset import NARMADataset\nimport echotorch.nn as etnn\nimport echotorch.utils\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data.dataloader import DataLoader\nimport numpy as np\nimport mdp\nimport matplotlib.pyplot as plt\n\n# Parameters\nspectral_radius = 0.9\nleaky_rate = 1.0\nlearning_rate = 0.04\nreservoir_dim = 100\nhidden_dim = 20\ninput_dim = 1\nn_hidden = 100\nn_iterations = 2000\ntrain_sample_length = 5000\ntest_sample_length = 1000\nn_train_samples = 1\nn_test_samples = 1\nbatch_size = 1\nmomentum = 0.95\nweight_decay = 0\n\n# Use CUDA?\nuse_cuda = True\nuse_cuda = torch.cuda.is_available() if use_cuda else False\n\n# Manual seed\nmdp.numx.random.seed(1)\nnp.random.seed(2)\ntorch.manual_seed(1)\n\n# NARMA30 dataset\nnarma10_train_dataset = NARMADataset(train_sample_length, n_train_samples, system_order=10, seed=1)\nnarma10_test_dataset = NARMADataset(test_sample_length, n_test_samples, system_order=10, seed=10)\n\n# Data loader\ntrainloader = DataLoader(narma10_train_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\ntestloader = DataLoader(narma10_test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n# Linear output\nlinear = nn.Linear(in_features=hidden_dim, out_features=1)\n\n# ESN cell\ngated_esn = etnn.GatedESN(\n    input_dim=input_dim,\n    reservoir_dim=input_dim,\n    pca_dim=hidden_dim,\n    hidden_dim=hidden_dim,\n    leaky_rate=leaky_rate,\n    spectral_radius=spectral_radius\n)\nif use_cuda:\n    gated_esn.cuda()\n    linear.cuda()\n# end if\n\n# Objective function\ncriterion = nn.MSELoss()\n\n# Stochastic Gradient Descent\noptimizer = optim.SGD(gated_esn.parameters(), lr=learning_rate, momentum=momentum)\n\n# For each iteration\nfor epoch in range(n_iterations):\n    # Iterate over batches\n    for data in trainloader:\n        # Inputs and outputs\n        inputs, targets = data\n        inputs, targets = Variable(inputs), Variable(targets)\n        if use_cuda: inputs, targets = inputs.cuda(), targets.cuda()\n        print(inputs)\n        print(targets)\n        # Gradients to zero\n        optimizer.zero_grad()\n\n        # Forward\n        out = linear(gated_esn(inputs))\n        print(out)\n        exit()\n        loss = criterion(out, targets)\n\n        # Backward pass\n        loss.backward()\n\n        # Optimize\n        optimizer.step()\n\n        # Print error measures\n        print(u""Train MSE: {}"".format(float(loss.data)))\n        print(u""Train NRMSE: {}"".format(echotorch.utils.nrmse(out.data, targets.data)))\n    # end for\n\n    # Test reservoir\n    dataiter = iter(testloader)\n    test_u, test_y = dataiter.next()\n    test_u, test_y = Variable(test_u), Variable(test_y)\n    if use_cuda: test_u, test_y = test_u.cuda(), test_y.cuda()\n    y_predicted = esn(test_u)\n\n    # Print error measures\n    print(u""Test MSE: {}"".format(echotorch.utils.mse(y_predicted.data, test_y.data)))\n    print(u""Test NRMSE: {}"".format(echotorch.utils.nrmse(y_predicted.data, test_y.data)))\n    print(u"""")\n# end for\n'"
examples/timeserie_prediction/narma10_stacked_esn.py,7,"b""# -*- coding: utf-8 -*-\n#\n# File : examples/timeserie_prediction/switch_attractor_esn\n# Description : NARMA 30 prediction with ESN.\n# Date : 26th of January, 2018\n#\n# This file is part of EchoTorch.  EchoTorch is free software: you can\n# redistribute it and/or modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation, version 2.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc., 51\n# Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n# Copyright Nils Schaetti <nils.schaetti@unine.ch>\n\n\n# Imports\nimport torch\nfrom echotorch.datasets.NARMADataset import NARMADataset\nimport echotorch.nn as etnn\nimport echotorch.utils\nfrom torch.autograd import Variable\nfrom torch.utils.data.dataloader import DataLoader\nimport numpy as np\nimport mdp\n\n# Dataset params\ntrain_sample_length = 5000\ntest_sample_length = 1000\nn_train_samples = 1\nn_test_samples = 1\nbatch_size = 1\nspectral_radius = 0.9\nleaky_rates = [1.0, 0.5, 0.1]\ninput_dim = 1\nn_hidden = [100, 100, 100]\n\n# Use CUDA?\nuse_cuda = False\nuse_cuda = torch.cuda.is_available() if use_cuda else False\n\n# Manual seed\nmdp.numx.random.seed(1)\nnp.random.seed(2)\ntorch.manual_seed(1)\n\n# NARMA30 dataset\nnarma10_train_dataset = NARMADataset(train_sample_length, n_train_samples, system_order=10, seed=1)\nnarma10_test_dataset = NARMADataset(test_sample_length, n_test_samples, system_order=10, seed=10)\n\n# Data loader\ntrainloader = DataLoader(narma10_train_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\ntestloader = DataLoader(narma10_test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n# ESN cell\nesn = etnn.StackedESN(input_dim=input_dim, hidden_dim=n_hidden, output_dim=1, spectral_radius=spectral_radius, learning_algo='inv', leaky_rate=leaky_rates)\n\n# For each batch\nfor data in trainloader:\n    # Inputs and outputs\n    inputs, targets = data\n\n    # To variable\n    inputs, targets = Variable(inputs), Variable(targets)\n    if use_cuda: inputs, targets = inputs.cuda(), targets.cuda()\n\n    # Accumulate xTx and xTy\n    hidden_states = esn(inputs, targets)\n    for i in range(10):\n        print(hidden_states[0, i])\n    # end if\n# end for"""
examples/unsupervised_learning/object_recognition.py,0,b''
examples/unsupervised_learning/sfa_logmap.py,0,"b'# -*- coding: utf-8 -*-\n#\n\n# Imports\nimport mdp\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Init. random\nnp.random.seed(0)\n\n# Parameters\nn = 10000\np2 = np.pi * 2\nt = np.linspace(0, 1, n, endpoint=0)\ndforce = np.sin(p2*5*t) + np.sin(p2*11*t) + np.sin(p2*13*t)\n\n\ndef logistic_map(x, r):\n    return r*x*(1-x)\n# end logistic_map\n\n# Series\nseries = np.zeros((n, 1), \'d\')\nseries[0] = 0.6\n\n# Create series\nfor i in range(1, n):\n    series[i] = logistic_map(series[i-1], 3.6+0.13*dforce[i])\n# end for\n\n# MDP flow\nflow = (mdp.nodes.EtaComputerNode() +\n        mdp.nodes.TimeFramesNode(10) +\n        mdp.nodes.PolynomialExpansionNode(3) +\n        mdp.nodes.SFA2Node(output_dim=1) +\n        mdp.nodes.EtaComputerNode())\n\n# Train\nflow.train(series)\n\n# Slow\nslow = flow(series)\n\nresc_dforce = (dforce - np.mean(dforce, 0)) / np.std(dforce, 0)\n\nprint(u""{}"".format(mdp.utils.cov2(resc_dforce[:-9], slow)))\nprint(u""Eta value (time serie) : {}"".format(flow[0].get_eta(t=10000)))\nprint(u""Eta value (slow feature) : {}"".format(flow[-1].get_eta(t=9996)))\n'"
echotorch/transforms/images/__init__.py,0,b''
echotorch/transforms/text/Character.py,2,"b'# -*- coding: utf-8 -*-\n#\n\n# Imports\nimport torch\nfrom .Transformer import Transformer\n\n\n# Transform text to character vectors\nclass Character(Transformer):\n    """"""\n    Transform text to character vectors\n    """"""\n\n    # Constructor\n    def __init__(self, uppercase=False, gram_to_ix=None, start_ix=0, fixed_length=-1):\n        """"""\n        Constructor\n        """"""\n        # Gram to ix\n        if gram_to_ix is not None:\n            self.gram_count = len(gram_to_ix.keys())\n            self.gram_to_ix = gram_to_ix\n        else:\n            self.gram_count = start_ix\n            self.gram_to_ix = dict()\n        # end if\n\n        # Ix to gram\n        self.ix_to_gram = dict()\n        if gram_to_ix is not None:\n            for gram in gram_to_ix.keys():\n                self.ix_to_gram[gram_to_ix[gram]] = gram\n            # end for\n        # end if\n\n        # Properties\n        self.uppercase = uppercase\n        self.fixed_length = fixed_length\n\n        # Super constructor\n        super(Character, self).__init__()\n    # end __init__\n\n    ##############################################\n    # Public\n    ##############################################\n\n    ##############################################\n    # Properties\n    ##############################################\n\n    # Get the number of inputs\n    @property\n    def input_dim(self):\n        """"""\n        Get the number of inputs.\n        :return: The input size.\n        """"""\n        return 1\n    # end input_dim\n\n    # Vocabulary size\n    @property\n    def voc_size(self):\n        """"""\n        Vocabulary size\n        :return:\n        """"""\n        return self.gram_count\n    # end voc_size\n\n    ##############################################\n    # Private\n    ##############################################\n\n    # To upper\n    def to_upper(self, gram):\n        """"""\n        To upper\n        :param gram:\n        :return:\n        """"""\n        if not self.uppercase:\n            return gram.lower()\n        # end if\n        return gram\n    # end to_upper\n\n    ##############################################\n    # Override\n    ##############################################\n\n    # Convert a string\n    def __call__(self, text):\n        """"""\n        Convert a string to a ESN input\n        :param text: Text to convert\n        :return: Tensor of word vectors\n        """"""\n        # Add to voc\n        for i in range(len(text)):\n            gram = self.to_upper(text[i])\n            if gram not in self.gram_to_ix.keys():\n                self.gram_to_ix[gram] = self.gram_count\n                self.ix_to_gram[self.gram_count] = gram\n                self.gram_count += 1\n            # end if\n        # end for\n\n        # List of character to 2grams\n        text_idxs = [self.gram_to_ix[self.to_upper(text[i])] for i in range(len(text))]\n\n        # To long tensor\n        text_idxs = torch.LongTensor(text_idxs)\n\n        # Check length\n        if self.fixed_length != -1:\n            if text_idxs.size(0) > self.fixed_length:\n                text_idxs = text_idxs[:self.fixed_length]\n            elif text_idxs.size(0) < self.fixed_length:\n                zero_idxs = torch.LongTensor(self.fixed_length).fill_(0)\n                zero_idxs[:text_idxs.size(0)] = text_idxs\n                text_idxs = zero_idxs\n            # end if\n        # end if\n\n        return text_idxs, text_idxs.size(0)\n    # end convert\n\n# end FunctionWord\n'"
echotorch/transforms/text/Character2Gram.py,2,"b'# -*- coding: utf-8 -*-\n#\n\n# Imports\nimport torch\nfrom .Transformer import Transformer\nimport numpy as np\n\n\n# Transform text to character 2-gram\nclass Character2Gram(Transformer):\n    """"""\n    Transform text to character 2-grams\n    """"""\n\n    # Constructor\n    def __init__(self, uppercase=False, gram_to_ix=None, start_ix=0, fixed_length=-1, overlapse=True):\n        """"""\n        Constructor\n        """"""\n        # Gram to ix\n        if gram_to_ix is not None:\n            self.gram_count = len(gram_to_ix.keys())\n            self.gram_to_ix = gram_to_ix\n        else:\n            self.gram_count = start_ix\n            self.gram_to_ix = dict()\n        # end if\n\n        # Ix to gram\n        self.ix_to_gram = dict()\n        if gram_to_ix is not None:\n            for gram in gram_to_ix.keys():\n                self.ix_to_gram[gram_to_ix[gram]] = gram\n            # end for\n        # end if\n\n        # Properties\n        self.uppercase = uppercase\n        self.fixed_length = fixed_length\n        self.overlapse = overlapse\n\n        # Super constructor\n        super(Character2Gram, self).__init__()\n    # end __init__\n\n    ##############################################\n    # Public\n    ##############################################\n\n    ##############################################\n    # Properties\n    ##############################################\n\n    # Get the number of inputs\n    @property\n    def input_dim(self):\n        """"""\n        Get the number of inputs.\n        :return: The input size.\n        """"""\n        return 1\n    # end input_dim\n\n    # Vocabulary size\n    @property\n    def voc_size(self):\n        """"""\n        Vocabulary size\n        :return:\n        """"""\n        return self.gram_count\n    # end voc_size\n\n    ##############################################\n    # Private\n    ##############################################\n\n    # To upper\n    def to_upper(self, gram):\n        """"""\n        To upper\n        :param gram:\n        :return:\n        """"""\n        if not self.uppercase:\n            return gram.lower()\n        # end if\n        return gram\n    # end to_upper\n\n    ##############################################\n    # Override\n    ##############################################\n\n    # Convert a string\n    def __call__(self, text):\n        """"""\n        Convert a string to a ESN input\n        :param text: Text to convert\n        :return: Tensor of word vectors\n        """"""\n        # Step\n        if self.overlapse:\n            step = 1\n        else:\n            step = 2\n        #  end if\n\n        # Add to voc\n        for i in np.arange(0, len(text) - 1, step):\n            gram = self.to_upper(text[i] + text[i+1])\n            if gram not in self.gram_to_ix.keys():\n                self.gram_to_ix[gram] = self.gram_count\n                self.ix_to_gram[self.gram_count] = gram\n                self.gram_count += 1\n            # end if\n        # end for\n\n        # List of character to 2grams\n        text_idxs = [self.gram_to_ix[self.to_upper(text[i] + text[i+1])] for i in range(len(text)-1)]\n\n        # To long tensor\n        text_idxs = torch.LongTensor(text_idxs)\n\n        # Check length\n        if self.fixed_length != -1:\n            if text_idxs.size(0) > self.fixed_length:\n                text_idxs = text_idxs[:self.fixed_length]\n            elif text_idxs.size(0) < self.fixed_length:\n                zero_idxs = torch.LongTensor(self.fixed_length).fill_(0)\n                zero_idxs[:text_idxs.size(0)] = text_idxs\n                text_idxs = zero_idxs\n            # end if\n        # end if\n\n        return text_idxs, text_idxs.size(0)\n    # end convert\n\n# end Character2Gram\n'"
echotorch/transforms/text/Character3Gram.py,2,"b'# -*- coding: utf-8 -*-\n#\n\n# Imports\nimport torch\nfrom .Transformer import Transformer\nimport numpy as np\n\n\n# Transform text to character 3-gram\nclass Character3Gram(Transformer):\n    """"""\n    Transform text to character 3-grams\n    """"""\n\n    # Constructor\n    def __init__(self, uppercase=False, gram_to_ix=None, start_ix=0, fixed_length=-1, overlapse=True):\n        """"""\n        Constructor\n        """"""\n        # Gram to ix\n        if gram_to_ix is not None:\n            self.gram_count = len(gram_to_ix.keys())\n            self.gram_to_ix = gram_to_ix\n        else:\n            self.gram_count = start_ix\n            self.gram_to_ix = dict()\n        # end if\n\n        # Ix to gram\n        self.ix_to_gram = dict()\n        if gram_to_ix is not None:\n            for gram in gram_to_ix.keys():\n                self.ix_to_gram[gram_to_ix[gram]] = gram\n            # end for\n        # end if\n\n        # Properties\n        self.uppercase = uppercase\n        self.fixed_length = fixed_length\n        self.overlapse = overlapse\n\n        # Super constructor\n        super(Character3Gram, self).__init__()\n    # end __init__\n\n    ##############################################\n    # Public\n    ##############################################\n\n    ##############################################\n    # Properties\n    ##############################################\n\n    # Get the number of inputs\n    @property\n    def input_dim(self):\n        """"""\n        Get the number of inputs.\n        :return: The input size.\n        """"""\n        return 1\n    # end input_dim\n\n    # Vocabulary size\n    @property\n    def voc_size(self):\n        """"""\n        Vocabulary size\n        :return:\n        """"""\n        return self.gram_count\n    # end voc_size\n\n    ##############################################\n    # Private\n    ##############################################\n\n    # To upper\n    def to_upper(self, gram):\n        """"""\n        To upper\n        :param gram:\n        :return:\n        """"""\n        if not self.uppercase:\n            return gram.lower()\n        # end if\n        return gram\n    # end to_upper\n\n    ##############################################\n    # Override\n    ##############################################\n\n    # Convert a string\n    def __call__(self, text):\n        """"""\n        Convert a string to a ESN input\n        :param text: Text to convert\n        :return: Tensor of word vectors\n        """"""\n        # Step\n        if self.overlapse:\n            step = 1\n        else:\n            step = 3\n        #  end if\n\n        # Add to voc\n        for i in np.arange(0, len(text) - 2, step):\n            gram = self.to_upper(text[i] + text[i+1] + text[i+2])\n            if gram not in self.gram_to_ix.keys():\n                self.gram_to_ix[gram] = self.gram_count\n                self.ix_to_gram[self.gram_count] = gram\n                self.gram_count += 1\n            # end if\n        # end for\n\n        # List of character to 3 grams\n        text_idxs = [self.gram_to_ix[self.to_upper(text[i] + text[i+1] + text[i+2])] for i in range(len(text)-2)]\n\n        # To long tensor\n        text_idxs = torch.LongTensor(text_idxs)\n\n        # Check length\n        if self.fixed_length != -1:\n            if text_idxs.size(0) > self.fixed_length:\n                text_idxs = text_idxs[:self.fixed_length]\n            elif text_idxs.size(0) < self.fixed_length:\n                zero_idxs = torch.LongTensor(self.fixed_length).fill_(0)\n                zero_idxs[:text_idxs.size(0)] = text_idxs\n                text_idxs = zero_idxs\n            # end if\n        # end if\n\n        return text_idxs, text_idxs.size(0)\n    # end convert\n\n# end Character3Gram\n'"
echotorch/transforms/text/Compose.py,0,"b'# -*- coding: utf-8 -*-\n#\n\n# Imports\nfrom .Transformer import Transformer\n\n\n# Compose multiple transformations\nclass Compose(Transformer):\n    """"""\n    Compose multiple transformations\n    """"""\n\n    # Constructor\n    def __init__(self, transforms):\n        """"""\n        Constructor\n        """"""\n        # Properties\n        self.transforms = transforms\n\n        # Super constructor\n        super(Compose, self).__init__()\n    # end __init__\n\n    ##############################################\n    # Public\n    ##############################################\n\n    ##############################################\n    # Properties\n    ##############################################\n\n    # Get the number of inputs\n    @property\n    def input_dim(self):\n        """"""\n        Get the number of inputs.\n        :return: The input size.\n        """"""\n        return self.transforms[-1].input_dim\n    # end input_dim\n\n    ##############################################\n    # Override\n    ##############################################\n\n    # Convert a string\n    def __call__(self, text):\n        """"""\n        Convert a string to a ESN input\n        :param text: Text to convert\n        :return: Tensor of word vectors\n        """"""\n        # For each transform\n        for index, transform in enumerate(self.transforms):\n            # Transform\n            if index == 0:\n                outputs, size = transform(text)\n            else:\n                outputs, size = transform(outputs)\n            # end if\n        # end for\n\n        return outputs, size\n    # end convert\n\n# end Compose\n'"
echotorch/transforms/text/Embedding.py,5,"b'# -*- coding: utf-8 -*-\n#\n\n# Imports\nimport gensim\nfrom gensim.utils import tokenize\nimport torch\nimport numpy as np\n\n\n# Transform text to vectors with embedding\nclass Embedding(object):\n    """"""\n    Transform text to vectors with embedding\n    """"""\n\n    # Constructor\n    def __init__(self, weights):\n        """"""\n        Constructor\n        :param weights: Embedding weight matrix\n        """"""\n        # Properties\n        self.weights = weights\n        self.voc_size = weights.size(0)\n        self.embedding_dim = weights.size(1)\n    # end __init__\n\n    ##############################################\n    # Properties\n    ##############################################\n\n    # Get the number of inputs\n    @property\n    def input_dim(self):\n        """"""\n        Get the number of inputs\n        :return:\n        """"""\n        return self.embedding_dim\n    # end input_dim\n\n    ##############################################\n    # Override\n    ##############################################\n\n    # Convert a string\n    def __call__(self, idxs):\n        """"""\n        Convert a strng\n        :param text:\n        :return:\n        """"""\n        # Inputs as tensor\n        inputs = torch.FloatTensor(1, self.embedding_dim)\n\n        # Start\n        start = True\n        count = 0.0\n\n        # OOV\n        zero = 0.0\n        self.oov = 0.0\n\n        # For each inputs\n        for i in range(idxs.size(0)):\n            # Get token ix\n            ix = idxs[i]\n\n            # Get vector\n            if ix < self.voc_size:\n                embedding_vector = self.weights[ix]\n            else:\n                embedding_vector = torch.zeros(self.embedding_dim)\n            # end if\n\n            # Test zero\n            if torch.sum(embedding_vector) == 0.0:\n                zero += 1.0\n                embedding_vector = np.zeros(self.input_dim)\n            # end if\n\n            # Start/continue\n            if not start:\n                inputs = torch.cat((inputs, torch.FloatTensor(embedding_vector).unsqueeze_(0)), dim=0)\n            else:\n                inputs = torch.FloatTensor(embedding_vector).unsqueeze_(0)\n                start = False\n            # end if\n            count += 1\n        # end for\n\n        # OOV\n        self.oov = zero / count * 100.0\n\n        return inputs, inputs.size()[0]\n    # end convert\n\n    ##############################################\n    # Static\n    ##############################################\n\n\n# end Embedding\n'"
echotorch/transforms/text/FunctionWord.py,3,"b'# -*- coding: utf-8 -*-\n#\n\n# Imports\nimport torch\nimport spacy\nfrom .Transformer import Transformer\n\n\n# Transform text to a function word vectors\nclass FunctionWord(Transformer):\n    """"""\n    Transform text to character vectors\n    """"""\n\n    # Constructor\n    def __init__(self, model=""en_core_web_lg""):\n        """"""\n        Constructor\n        :param model: Spacy\'s model to load.\n        """"""\n        # Super constructor\n        super(FunctionWord, self).__init__()\n\n        # Properties\n        self.model = model\n        self.nlp = spacy.load(model)\n    # end __init__\n\n    ##############################################\n    # Public\n    ##############################################\n\n    # Get tags\n    def get_tags(self):\n        """"""\n        Get tags.\n        :return: A tag list.\n        """"""\n        return [u""a"", u""about"", u""above"", u""after"", u""after"", u""again"", u""against"", u""ago"", u""ahead"",\n                u""all"",\n                u""almost"", u""along"", u""already"", u""also"", u""although"", u""always"", u""am"", u""among"", u""an"",\n                u""and"", u""any"", u""are"", u""aren\'t"", u""around"", u""as"", u""at"", u""away"", u""backward"",\n                u""backwards"", u""be"", u""because"", u""before"", u""behind"", u""below"", u""beneath"", u""beside"",\n                u""between"", u""both"", u""but"", u""by"", u""can"", u""cannot"", u""can\'t"", u""cause"", u""\'cos"",\n                u""could"",\n                u""couldn\'t"", u""\'d"", u""despite"", u""did"", u""didn\'t"", u""do"", u""does"", u""doesn\'t"", u""don\'t"",\n                u""down"", u""during"", u""each"", u""either"", u""even"", u""ever"", u""every"", u""except"", u""for"",\n                u""forward"", u""from"", u""had"", u""hadn\'t"", u""has"", u""hasn\'t"", u""have"", u""haven\'t"", u""he"",\n                u""her"", u""here"", u""hers"", u""herself"", u""him"", u""himself"", u""his"", u""how"", u""however"",\n                u""I"",\n                u""if"", u""in"", u""inside"", u""inspite"", u""instead"", u""into"", u""is"", u""isn\'t"", u""it"", u""its"",\n                u""itself"", u""just"", u""\'ll"", u""least"", u""less"", u""like"", u""\'m"", u""many"", u""may"",\n                u""mayn\'t"",\n                u""me"", u""might"", u""mightn\'t"", u""mine"", u""more"", u""most"", u""much"", u""must"", u""mustn\'t"",\n                u""my"", u""myself"", u""near"", u""need"", u""needn\'t"", u""needs"", u""neither"", u""never"", u""no"",\n                u""none"", u""nor"", u""not"", u""now"", u""of"", u""off"", u""often"", u""on"", u""once"", u""only"",\n                u""onto"",\n                u""or"", u""ought"", u""oughtn\'t"", u""our"", u""ours"", u""ourselves"", u""out"", u""outside"", u""over"",\n                u""past"", u""perhaps"", u""quite"", u""\'re"", u""rather"", u""\'s"", u""seldom"", u""several"", u""shall"",\n                u""shan\'t"", u""she"", u""should"", u""shouldn\'t"", u""since"", u""so"", u""some"", u""sometimes"",\n                u""soon"",\n                u""than"", u""that"", u""the"", u""their"", u""theirs"", u""them"", u""themselves"", u""then"", u""there"",\n                u""therefore"", u""these"", u""they"", u""this"", u""those"", u""though"", u""through"", u""thus"",\n                u""till"",\n                u""to"", u""together"", u""too"", u""towards"", u""under"", u""unless"", u""until"", u""up"", u""upon"",\n                u""us"", u""used"", u""usedn\'t"", u""usen\'t"", u""usually"", u""\'ve"", u""very"", u""was"", u""wasn\'t"",\n                u""we"", u""well"", u""were"", u""weren\'t"", u""what"", u""when"", u""where"", u""whether"", u""which"",\n                u""while"", u""who"", u""whom"", u""whose"", u""why"", u""will"", u""with"", u""without"", u""won\'t"",\n                u""would"", u""wouldn\'t"", u""yet"", u""you"", u""your"", u""yours"", u""yourself"", u""yourselves"", u""X""]\n    # end get_tags\n\n    ##############################################\n    # Override\n    ##############################################\n\n    # Convert a string\n    def __call__(self, text):\n        """"""\n        Convert a string to a ESN input\n        :param text: Text to convert\n        :return: Tensor of word vectors\n        """"""\n        # Inputs as tensor\n        inputs = torch.FloatTensor(1, self.input_dim)\n\n        # Null symbol\n        null_symbol = torch.zeros(1, self.input_dim)\n        null_symbol[0, -1] = 1.0\n\n        # Start\n        start = True\n\n        # For each tokens\n        for token in self.nlp(text):\n            # Replace if not function word\n            if token.text not in self.symbols:\n                token_fw = u""X""\n            else:\n                token_fw = token.text\n            # end if\n\n            # Get tag\n            fw = self.tag_to_symbol(token_fw)\n\n            # Add\n            if not start:\n                inputs = torch.cat((inputs, fw), dim=0)\n            else:\n                inputs = fw\n                start = False\n            # end if\n        # end for\n\n        return inputs, inputs.size()[0]\n    # end convert\n\n# end FunctionWord\n'"
echotorch/transforms/text/GensimModel.py,3,"b'# -*- coding: utf-8 -*-\n#\n\n# Imports\nimport gensim\nfrom gensim.utils import tokenize\nimport torch\nimport numpy as np\n\n\n# Transform text to vectors with a Gensim model\nclass GensimModel(object):\n    """"""\n    Transform text to vectors with a Gensim model\n    """"""\n\n    # Constructor\n    def __init__(self, model_path):\n        """"""\n        Constructor\n        :param model_path: Model\'s path.\n        """"""\n        # Properties\n        self.model_path = model_path\n\n        # Format\n        binary = False if model_path[-4:] == "".vec"" else True\n\n        # Load\n        self.model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=binary, unicode_errors=\'ignore\')\n\n        # OOV\n        self.oov = 0.0\n    # end __init__\n\n    ##############################################\n    # Properties\n    ##############################################\n\n    # Get the number of inputs\n    @property\n    def input_dim(self):\n        """"""\n        Get the number of inputs.\n        :return: The input size.\n        """"""\n        return 300\n    # end input_dim\n\n    ##############################################\n    # Override\n    ##############################################\n\n    # Convert a string\n    def __call__(self, text):\n        """"""\n        Convert a string to a ESN input\n        :param text: Text to convert\n        :return: Tensor of word vectors\n        """"""\n        # Inputs as tensor\n        inputs = torch.FloatTensor(1, self.input_dim)\n\n        # Start\n        start = True\n        count = 0.0\n\n        # OOV\n        zero = 0.0\n        self.oov = 0.0\n\n        # For each tokens\n        for token in tokenize(text):\n            found = False\n            # Try normal\n            try:\n                word_vector = self.model[token]\n                found = True\n            except KeyError:\n                pass\n            # end try\n\n            # Try lower\n            if not found:\n                try:\n                    word_vector = self.model[token.lower()]\n                except KeyError:\n                    zero += 1.0\n                    word_vector = np.zeros(self.input_dim)\n                # end try\n            # end if\n\n            # Start/continue\n            if not start:\n                inputs = torch.cat((inputs, torch.FloatTensor(word_vector).unsqueeze_(0)), dim=0)\n            else:\n                inputs = torch.FloatTensor(word_vector).unsqueeze_(0)\n                start = False\n            # end if\n            count += 1\n        # end for\n\n        # OOV\n        self.oov = zero / count * 100.0\n\n        return inputs, inputs.size()[0]\n    # end convert\n\n    ##############################################\n    # Static\n    #########################################'"
echotorch/transforms/text/GloveVector.py,3,"b'# -*- coding: utf-8 -*-\n#\n\n# Imports\nimport torch\nimport spacy\nimport numpy as np\nfrom datetime import datetime\n\n\n# Transform text to word vectors\nclass GloveVector(object):\n    """"""\n    Transform text to word vectors\n    """"""\n\n    # Constructor\n    def __init__(self, model=""en_vectors_web_lg""):\n        """"""\n        Constructor\n        :param model: Spacy\'s model to load.\n        """"""\n        # Properties\n        self.model = model\n        self.nlp = spacy.load(model)\n        self.oov = 0.0\n    # end __init__\n\n    ##############################################\n    # Properties\n    ##############################################\n\n    # Get the number of inputs\n    @property\n    def input_dim(self):\n        """"""\n        Get the number of inputs.\n        :return: The input size.\n        """"""\n        return 300\n    # end input_dim\n\n    ##############################################\n    # Override\n    ##############################################\n\n    # Convert a string\n    def __call__(self, text):\n        """"""\n        Convert a string to a ESN input\n        :param text: Text to convert\n        :return: Tensor of word vectors\n        """"""\n        # Inputs as tensor\n        inputs = torch.FloatTensor(1, self.input_dim)\n\n        # Start\n        start = True\n        count = 0.0\n\n        # Zero count\n        zero = 0.0\n        self.oov = 0.0\n\n        # For each tokens\n        for token in self.nlp(text):\n            if np.sum(token.vector) == 0:\n                zero += 1.0\n            # end if\n            if not start:\n                inputs = torch.cat((inputs, torch.FloatTensor(token.vector).unsqueeze_(0)), dim=0)\n            else:\n                inputs = torch.FloatTensor(token.vector).unsqueeze_(0)\n                start = False\n            # end if\n            count += 1.0\n        # end for\n\n        # OOV\n        self.oov = zero / count * 100.0\n\n        return inputs, inputs.size()[0]\n    # end convert\n\n    ##############################################\n    # Static\n    ##############################################\n\n# end GloveVector\n'"
echotorch/transforms/text/PartOfSpeech.py,2,"b'# -*- coding: utf-8 -*-\n#\n\n# Imports\nimport torch\nimport spacy\nfrom .Transformer import Transformer\n\n\n# Transform text to part-of-speech vectors\nclass PartOfSpeech(Transformer):\n    """"""\n    Transform text to part-of-speech vectors\n    """"""\n\n    # Constructor\n    def __init__(self, model=""en_core_web_lg""):\n        """"""\n        Constructor\n        :param model: Spacy\'s model to load.\n        """"""\n        # Super constructor\n        super(PartOfSpeech, self).__init__()\n\n        # Properties\n        self.model = model\n        self.nlp = spacy.load(model)\n    # end __init__\n\n    ##############################################\n    # Public\n    ##############################################\n\n    # Get tags\n    def get_tags(self):\n        """"""\n        Get tags.\n        :return: A list of tags.\n        """"""\n        return [u""ADJ"", u""ADP"", u""ADV"", u""CCONJ"", u""DET"", u""INTJ"", u""NOUN"", u""NUM"", u""PART"", u""PRON"", u""PROPN"",\n                u""PUNCT"", u""SYM"", u""VERB"", u""SPACE"", u""X""]\n    # end get_tags\n\n    ##############################################\n    # Override\n    ##############################################\n\n    # Convert a string\n    def __call__(self, text):\n        """"""\n        Convert a string to a ESN input\n        :param text: Text to convert\n        :return: Tensor of word vectors\n        """"""\n        # Inputs as tensor\n        inputs = torch.FloatTensor(1, self.input_dim)\n\n        # Start\n        start = True\n\n        # For each tokens\n        for token in self.nlp(text):\n            pos = self.tag_to_symbol(token.pos_)\n\n            if not start:\n                inputs = torch.cat((inputs, pos), dim=0)\n            else:\n                inputs = pos\n                start = False\n            # end if\n        # end for\n\n        return inputs, inputs.size()[0]\n    # end convert\n\n# end PartOfSpeech\n'"
echotorch/transforms/text/Tag.py,3,"b'# -*- coding: utf-8 -*-\n#\n\n# Imports\nimport torch\nimport spacy\nfrom .Transformer import Transformer\n\n\n# Transform text to tag vectors\nclass Tag(Transformer):\n    """"""\n    Transform text to tag vectors\n    """"""\n\n    # Constructor\n    def __init__(self, model=""en_core_web_lg""):\n        """"""\n        Constructor\n        :param model: Spacy\'s model to load.\n        """"""\n        # Super constructor\n        super(Tag, self).__init__()\n\n        # Properties\n        self.model = model\n        self.nlp = spacy.load(model)\n    # end __init__\n\n    ##############################################\n    # Public\n    ##############################################\n\n    # Get tags\n    def get_tags(self):\n        """"""\n        Get all tags.\n        :return: A list of tags.\n        """"""\n        return [u""\'\'"", u"","", u"":"", u""."", u""``"", u""-LRB-"", u""-RRB-"", u""AFX"", u""CC"", u""CD"", u""DT"", u""EX"", u""FW"",\n                u""IN"", u""JJ"", u""JJR"", u""JJS"", u""LS"", u""MD"", u""NN"", u""NNS"", u""NNP"", u""NNPS"", u""PDT"", u""POS"", u""PRP"",\n                u""PRP$"", u""RB"", u""RBR"", u""RBS"", u""RP"", u""SYM"", u""TO"", u""UH"", u""VB"", u""VBZ"", u""VBP"", u""VBD"", u""VBN"",\n                u""VBG"", u""WDT"", u""WP"", u""WP$"", u""WRB"", u""X""]\n    # end get_tags\n\n    ##############################################\n    # Override\n    ##############################################\n\n    # Convert a string\n    def __call__(self, text):\n        """"""\n        Convert a string to a ESN input\n        :param text: Text to convert\n        :return: Tensor of word vectors\n        """"""\n        # Inputs as tensor\n        inputs = torch.FloatTensor(1, self.input_dim)\n\n        # Null symbol\n        null_symbol = torch.zeros(1, self.input_dim)\n        null_symbol[0, -1] = 1.0\n\n        # Start\n        start = True\n\n        # For each tokens\n        for token in self.nlp(text):\n            # Replace if not function word\n            if token.tag_ not in self.symbols:\n                token_tag = u""X""\n            else:\n                token_tag = token.tag_\n            # end if\n\n            # Get tag\n            tag = self.tag_to_symbol(token_tag)\n\n            # Add\n            if not start:\n                inputs = torch.cat((inputs, tag), dim=0)\n            else:\n                inputs = tag\n                start = False\n            # end if\n        # end for\n\n        return inputs, inputs.size()[0]\n    # end convert\n\n# end FunctionWord\n'"
echotorch/transforms/text/Token.py,0,"b'# -*- coding: utf-8 -*-\n#\n\n# Imports\nimport spacy\n\n\n# Transform text to a list of tokens\nclass Token(object):\n    """"""\n    Transform text to a list of tokens\n    """"""\n\n    # Constructor\n    def __init__(self, model=""en_core_web_lg""):\n        """"""\n        Constructor\n        :param model: Spacy\'s model to load.\n        """"""\n        # Properties\n        self.model = model\n        self.nlp = spacy.load(model)\n    # end __init__\n\n    ##############################################\n    # Properties\n    ##############################################\n\n    # Get the number of inputs\n    @property\n    def input_dim(self):\n        """"""\n        Get the number of inputs.\n        :return: The input size.\n        """"""\n        return 1\n    # end input_dim\n\n    ##############################################\n    # Override\n    ##############################################\n\n    # Convert a string\n    def __call__(self, text):\n        """"""\n        Convert a string to a ESN input\n        :param text: Text to convert\n        :return: Tensor of word vectors\n        """"""\n        # Inputs as a list\n        tokens = list()\n\n        # For each tokens\n        for token in self.nlp(text):\n            tokens.append(unicode(token.text))\n        # end for\n\n        return tokens, len(tokens)\n    # end convert\n\n    ##############################################\n    # Private\n    ##############################################\n\n    # Get inputs size\n    def _get_inputs_size(self):\n        """"""\n        Get inputs size.\n        :return:\n        """"""\n        return 1\n    # end if\n\n    ##############################################\n    # Static\n    ##############################################\n\n# end Token\n'"
echotorch/transforms/text/Transformer.py,1,"b'# -*- coding: utf-8 -*-\n#\n\n# Imports\nimport torch\n\n\n# Base class for text transformers\nclass Transformer(object):\n    """"""\n    Base class for text transformers\n    """"""\n\n    # Constructor\n    def __init__(self):\n        """"""\n        Constructor\n        """"""\n        # Properties\n        self.symbols = self.generate_symbols()\n    # end __init__\n\n    ##############################################\n    # Properties\n    ##############################################\n\n    # Get the number of inputs\n    @property\n    def input_dim(self):\n        """"""\n        Get the number of inputs.\n        :return: The input size.\n        """"""\n        return len(self.get_tags())\n    # end input_dim\n\n    ##############################################\n    # Public\n    ##############################################\n\n    # Get tags\n    def get_tags(self):\n        """"""\n        Get tags.\n        :return: A list of tags.\n        """"""\n        return []\n    # end get_tags\n\n    # Get symbol from tag\n    def tag_to_symbol(self, tag):\n        """"""\n        Get symbol from tag.\n        :param tag: Tag.\n        :return: The corresponding symbols.\n        """"""\n        if tag in self.symbols.keys():\n            return self.symbols[tag]\n        return None\n    # end word_to_symbol\n\n    # Generate symbols\n    def generate_symbols(self):\n        """"""\n        Generate word symbols.\n        :return: Dictionary of tag to symbols.\n        """"""\n        result = dict()\n        for index, p in enumerate(self.get_tags()):\n            result[p] = torch.zeros(1, self.input_dim)\n            result[p][0, index] = 1.0\n        # end for\n        return result\n    # end generate_symbols\n\n    ##############################################\n    # Override\n    ##############################################\n\n    # Convert a string\n    def __call__(self, tokens):\n        """"""\n        Convert a string to a ESN input\n        :param tokens: Text to convert\n        :return: A list of symbols\n        """"""\n        pass\n    # end convert\n\n    ##############################################\n    # Static\n    ##############################################\n\n# end TextTransformer\n'"
echotorch/transforms/text/__init__.py,0,"b""# -*- coding: utf-8 -*-\n#\n\n# Imports\nfrom .Character import Character\nfrom .Character2Gram import Character2Gram\nfrom .Character3Gram import Character3Gram\nfrom .Compose import Compose\nfrom .Embedding import Embedding\nfrom .FunctionWord import FunctionWord\nfrom .GensimModel import GensimModel\nfrom .GloveVector import GloveVector\nfrom .PartOfSpeech import PartOfSpeech\nfrom .Tag import Tag\nfrom .Token import Token\nfrom .Transformer import Transformer\n\n__all__ = [\n    'Character', 'Character2Gram', 'Character3Gram', 'Compose', 'Embedding', 'FunctionWord', 'GensimModel', 'Transformer', 'GloveVector',\n    'PartOfSpeech', 'Tag', 'Token'\n]\n"""
