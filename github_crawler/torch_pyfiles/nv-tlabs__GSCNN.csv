file_path,api_count,code
config.py,4,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n\n# Code adapted from:\n# https://github.com/facebookresearch/Detectron/blob/master/detectron/core/config.py\n\nSource License\n# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n#\n# Based on:\n# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport copy\nimport six\nimport os.path as osp\n\nfrom ast import literal_eval\nimport numpy as np\nimport yaml\nimport torch\nimport torch.nn as nn\nfrom torch.nn import init\n\n\nfrom utils.AttrDict import AttrDict\n\n\n__C = AttrDict()\n# Consumers can get config by:\n# from fast_rcnn_config import cfg\ncfg = __C\n__C.EPOCH = 0\n__C.CLASS_UNIFORM_PCT=0.0\n__C.BATCH_WEIGHTING=False\n__C.BORDER_WINDOW=1\n__C.REDUCE_BORDER_EPOCH= -1\n__C.STRICTBORDERCLASS= None\n\n__C.DATASET =AttrDict()\n__C.DATASET.CITYSCAPES_DIR=\'/home/username/data/cityscapes\'\n__C.DATASET.CV_SPLITS=3\n\n__C.MODEL = AttrDict()\n__C.MODEL.BN = \'regularnorm\'\n__C.MODEL.BNFUNC = torch.nn.BatchNorm2d\n__C.MODEL.BIGMEMORY = False\n\ndef assert_and_infer_cfg(args, make_immutable=True):\n    """"""Call this function in your script after you have finished setting all cfg\n    values that are necessary (e.g., merging a config from a file, merging\n    command line config options, etc.). By default, this function will also\n    mark the global cfg as immutable to prevent changing the global cfg settings\n    during script execution (which can lead to hard to debug errors or code\n    that\'s harder to understand than is necessary).\n    """"""\n\n    if args.batch_weighting:\n        __C.BATCH_WEIGHTING=True\n\n    if args.syncbn:\n        import encoding\n        __C.MODEL.BN = \'syncnorm\'\n        __C.MODEL.BNFUNC = encoding.nn.BatchNorm2d\n    else:\n        __C.MODEL.BNFUNC = torch.nn.BatchNorm2d\n        print(\'Using regular batch norm\')\n\n    if make_immutable:\n        cfg.immutable(True)\n'"
loss.py,8,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport logging\nimport numpy as np\nfrom config import cfg\nfrom my_functionals.DualTaskLoss import DualTaskLoss\n\ndef get_loss(args):\n    \'\'\'\n    Get the criterion based on the loss function\n    args: \n    return: criterion\n    \'\'\'\n    \n    if args.img_wt_loss:\n        criterion = ImageBasedCrossEntropyLoss2d(\n            classes=args.dataset_cls.num_classes, size_average=True,\n            ignore_index=args.dataset_cls.ignore_label, \n            upper_bound=args.wt_bound).cuda()\n    elif args.joint_edgeseg_loss:\n        criterion = JointEdgeSegLoss(classes=args.dataset_cls.num_classes,\n           ignore_index=args.dataset_cls.ignore_label, upper_bound=args.wt_bound,\n           edge_weight=args.edge_weight, seg_weight=args.seg_weight, att_weight=args.att_weight, dual_weight=args.dual_weight).cuda()\n\n    else:\n        criterion = CrossEntropyLoss2d(size_average=True,\n                                       ignore_index=args.dataset_cls.ignore_label).cuda()\n\n    criterion_val = JointEdgeSegLoss(classes=args.dataset_cls.num_classes, mode=\'val\',\n       ignore_index=args.dataset_cls.ignore_label, upper_bound=args.wt_bound,\n       edge_weight=args.edge_weight, seg_weight=args.seg_weight).cuda()\n                               \n    return criterion, criterion_val\n\nclass JointEdgeSegLoss(nn.Module):\n    def __init__(self, classes, weight=None, reduction=\'mean\', ignore_index=255,\n                 norm=False, upper_bound=1.0, mode=\'train\', \n                 edge_weight=1, seg_weight=1, att_weight=1, dual_weight=1, edge=\'none\'):\n        super(JointEdgeSegLoss, self).__init__()\n        self.num_classes = classes\n        if mode == \'train\':\n            self.seg_loss = ImageBasedCrossEntropyLoss2d(\n                    classes=classes, ignore_index=ignore_index, upper_bound=upper_bound).cuda()\n        elif mode == \'val\':\n            self.seg_loss = CrossEntropyLoss2d(size_average=True,\n                                               ignore_index=ignore_index).cuda()\n\n        self.edge_weight = edge_weight\n        self.seg_weight = seg_weight\n        self.att_weight = att_weight\n        self.dual_weight = dual_weight\n\n        self.dual_task = DualTaskLoss()\n\n    def bce2d(self, input, target):\n        n, c, h, w = input.size()\n    \n        log_p = input.transpose(1, 2).transpose(2, 3).contiguous().view(1, -1)\n        target_t = target.transpose(1, 2).transpose(2, 3).contiguous().view(1, -1)\n        target_trans = target_t.clone()\n\n        pos_index = (target_t ==1)\n        neg_index = (target_t ==0)\n        ignore_index=(target_t >1)\n\n        target_trans[pos_index] = 1\n        target_trans[neg_index] = 0\n\n        pos_index = pos_index.data.cpu().numpy().astype(bool)\n        neg_index = neg_index.data.cpu().numpy().astype(bool)\n        ignore_index=ignore_index.data.cpu().numpy().astype(bool)\n\n        weight = torch.Tensor(log_p.size()).fill_(0)\n        weight = weight.numpy()\n        pos_num = pos_index.sum()\n        neg_num = neg_index.sum()\n        sum_num = pos_num + neg_num\n        weight[pos_index] = neg_num*1.0 / sum_num\n        weight[neg_index] = pos_num*1.0 / sum_num\n\n        weight[ignore_index] = 0\n\n        weight = torch.from_numpy(weight)\n        weight = weight.cuda()\n        loss = F.binary_cross_entropy_with_logits(log_p, target_t, weight, size_average=True)\n        return loss\n\n    def edge_attention(self, input, target, edge):\n        n, c, h, w = input.size()\n        filler = torch.ones_like(target) * 255\n        return self.seg_loss(input, \n                             torch.where(edge.max(1)[0] > 0.8, target, filler))\n\n    def forward(self, inputs, targets):\n        segin, edgein = inputs\n        segmask, edgemask = targets\n\n        losses = {}\n\n        losses[\'seg_loss\'] = self.seg_weight * self.seg_loss(segin, segmask)\n        losses[\'edge_loss\'] = self.edge_weight * 20 * self.bce2d(edgein, edgemask)\n        losses[\'att_loss\'] = self.att_weight * self.edge_attention(segin, segmask, edgein)\n        losses[\'dual_loss\'] = self.dual_weight * self.dual_task(segin, segmask)\n              \n        return losses\n\n#Img Weighted Loss\nclass ImageBasedCrossEntropyLoss2d(nn.Module):\n\n    def __init__(self, classes, weight=None, size_average=True, ignore_index=255,\n                 norm=False, upper_bound=1.0):\n        super(ImageBasedCrossEntropyLoss2d, self).__init__()\n        logging.info(""Using Per Image based weighted loss"")\n        self.num_classes = classes\n        self.nll_loss = nn.NLLLoss2d(weight, size_average, ignore_index)\n        self.norm = norm\n        self.upper_bound = upper_bound\n        self.batch_weights = cfg.BATCH_WEIGHTING\n\n    def calculateWeights(self, target):\n        hist = np.histogram(target.flatten(), range(\n            self.num_classes + 1), normed=True)[0]\n        if self.norm:\n            hist = ((hist != 0) * self.upper_bound * (1 / hist)) + 1\n        else:\n            hist = ((hist != 0) * self.upper_bound * (1 - hist)) + 1\n        return hist\n\n    def forward(self, inputs, targets):\n        target_cpu = targets.data.cpu().numpy()\n        if self.batch_weights:\n            weights = self.calculateWeights(target_cpu)\n            self.nll_loss.weight = torch.Tensor(weights).cuda()\n\n        loss = 0.0\n        for i in range(0, inputs.shape[0]):\n            if not self.batch_weights:\n                weights = self.calculateWeights(target_cpu[i])\n                self.nll_loss.weight = torch.Tensor(weights).cuda()\n            \n            loss += self.nll_loss(F.log_softmax(inputs[i].unsqueeze(0)),\n                                          targets[i].unsqueeze(0))\n        return loss\n\n\n#Cross Entroply NLL Loss\nclass CrossEntropyLoss2d(nn.Module):\n    def __init__(self, weight=None, size_average=True, ignore_index=255):\n        super(CrossEntropyLoss2d, self).__init__()\n        logging.info(""Using Cross Entropy Loss"")\n        self.nll_loss = nn.NLLLoss2d(weight, size_average, ignore_index)\n\n    def forward(self, inputs, targets):\n        return self.nll_loss(F.log_softmax(inputs), targets)\n\n'"
optimizer.py,1,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport torch\nfrom torch import optim\nimport math\nimport logging\nfrom config import cfg\n\ndef get_optimizer(args, net):\n\n    param_groups = net.parameters()\n\n    if args.sgd:\n        optimizer = optim.SGD(param_groups,\n                              lr=args.lr,\n                              weight_decay=args.weight_decay,\n                              momentum=args.momentum,\n                              nesterov=False)\n    elif args.adam:\n        amsgrad=False\n        if args.amsgrad:\n            amsgrad=True\n        optimizer = optim.Adam(param_groups,\n                               lr=args.lr,\n                               weight_decay=args.weight_decay,\n                               amsgrad=amsgrad\n                               )\n    else:\n        raise (\'Not a valid optimizer\')\n\n    if args.lr_schedule == \'poly\':\n        lambda1 = lambda epoch: math.pow(1 - epoch / args.max_epoch, args.poly_exp)\n        scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n    else:\n        raise ValueError(\'unknown lr schedule {}\'.format(args.lr_schedule))\n\n    if args.snapshot:\n        logging.info(\'Loading weights from model {}\'.format(args.snapshot))\n        net, optimizer = restore_snapshot(args, net, optimizer, args.snapshot)\n    else:\n        logging.info(\'Loaded weights from IMGNET classifier\')\n\n    return optimizer, scheduler\n\ndef restore_snapshot(args, net, optimizer, snapshot):\n    checkpoint = torch.load(snapshot, map_location=torch.device(\'cpu\'))\n    logging.info(""Load Compelete"")\n    if args.sgd_finetuned:\n     print(\'skipping load optimizer\')\n    else:\n        if \'optimizer\' in checkpoint and args.restore_optimizer:\n                optimizer.load_state_dict(checkpoint[\'optimizer\'])\n\n    if \'state_dict\' in checkpoint:\n        net = forgiving_state_restore(net, checkpoint[\'state_dict\'])\n    else:\n        net = forgiving_state_restore(net, checkpoint)\n\n    return net, optimizer\n\ndef forgiving_state_restore(net, loaded_dict):\n    # Handle partial loading when some tensors don\'t match up in size.\n    # Because we want to use models that were trained off a different\n    # number of classes.\n    net_state_dict = net.state_dict()\n    new_loaded_dict = {}\n    for k in net_state_dict:\n        if k in loaded_dict and net_state_dict[k].size() == loaded_dict[k].size():\n            new_loaded_dict[k] = loaded_dict[k]\n        else:\n            logging.info(\'Skipped loading parameter {}\'.format(k))\n    net_state_dict.update(new_loaded_dict)\n    net.load_state_dict(net_state_dict)\n    return net\n\n'"
train.py,5,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nimport argparse\nfrom functools import partial\nfrom config import cfg, assert_and_infer_cfg\nimport logging\nimport math\nimport os\nimport sys\n\nimport torch\nimport numpy as np\n\nfrom utils.misc import AverageMeter, prep_experiment, evaluate_eval, fast_hist\nfrom utils.f_boundary import eval_mask_boundary\nimport datasets\nimport loss\nimport network\nimport optimizer\n\n# Argument Parser\nparser = argparse.ArgumentParser(description=\'GSCNN\')\nparser.add_argument(\'--lr\', type=float, default=0.01)\nparser.add_argument(\'--arch\', type=str, default=\'network.gscnn.GSCNN\')\nparser.add_argument(\'--dataset\', type=str, default=\'cityscapes\')\nparser.add_argument(\'--cv\', type=int, default=0,\n                    help=\'cross validation split\')\nparser.add_argument(\'--joint_edgeseg_loss\', action=\'store_true\', default=True,\n                    help=\'joint loss\')\nparser.add_argument(\'--img_wt_loss\', action=\'store_true\', default=False,\n                    help=\'per-image class-weighted loss\')\nparser.add_argument(\'--batch_weighting\', action=\'store_true\', default=False,\n                    help=\'Batch weighting for class\')\nparser.add_argument(\'--eval_thresholds\', type=str, default=\'0.0005,0.001875,0.00375,0.005\',\n                    help=\'Thresholds for boundary evaluation\')\nparser.add_argument(\'--rescale\', type=float, default=1.0,\n                    help=\'Rescaled LR Rate\')\nparser.add_argument(\'--repoly\', type=float, default=1.5,\n                    help=\'Rescaled Poly\')\n\nparser.add_argument(\'--edge_weight\', type=float, default=1.0,\n                    help=\'Edge loss weight for joint loss\')\nparser.add_argument(\'--seg_weight\', type=float, default=1.0,\n                    help=\'Segmentation loss weight for joint loss\')\nparser.add_argument(\'--att_weight\', type=float, default=1.0,\n                    help=\'Attention loss weight for joint loss\')\nparser.add_argument(\'--dual_weight\', type=float, default=1.0,\n                    help=\'Dual loss weight for joint loss\')\n\nparser.add_argument(\'--evaluate\', action=\'store_true\', default=False)\n\nparser.add_argument(""--local_rank"", default=0, type=int)\n\nparser.add_argument(\'--sgd\', action=\'store_true\', default=True)\nparser.add_argument(\'--sgd_finetuned\',action=\'store_true\',default=False)\nparser.add_argument(\'--adam\', action=\'store_true\', default=False)\nparser.add_argument(\'--amsgrad\', action=\'store_true\', default=False)\n\nparser.add_argument(\'--trunk\', type=str, default=\'resnet101\',\n                    help=\'trunk model, can be: resnet101 (default), resnet50\')\nparser.add_argument(\'--max_epoch\', type=int, default=175)\nparser.add_argument(\'--start_epoch\', type=int, default=0)\nparser.add_argument(\'--color_aug\', type=float,\n                    default=0.25, help=\'level of color augmentation\')\nparser.add_argument(\'--rotate\', type=float,\n                    default=0, help=\'rotation\')\nparser.add_argument(\'--gblur\', action=\'store_true\', default=True)\nparser.add_argument(\'--bblur\', action=\'store_true\', default=False) \nparser.add_argument(\'--lr_schedule\', type=str, default=\'poly\',\n                    help=\'name of lr schedule: poly\')\nparser.add_argument(\'--poly_exp\', type=float, default=1.0,\n                    help=\'polynomial LR exponent\')\nparser.add_argument(\'--bs_mult\', type=int, default=1)\nparser.add_argument(\'--bs_mult_val\', type=int, default=2)\nparser.add_argument(\'--crop_size\', type=int, default=720,\n                    help=\'training crop size\')\nparser.add_argument(\'--pre_size\', type=int, default=None,\n                    help=\'resize image shorter edge to this before augmentation\')\nparser.add_argument(\'--scale_min\', type=float, default=0.5,\n                    help=\'dynamically scale training images down to this size\')\nparser.add_argument(\'--scale_max\', type=float, default=2.0,\n                    help=\'dynamically scale training images up to this size\')\nparser.add_argument(\'--weight_decay\', type=float, default=1e-4)\nparser.add_argument(\'--momentum\', type=float, default=0.9)\nparser.add_argument(\'--snapshot\', type=str, default=None)\nparser.add_argument(\'--restore_optimizer\', action=\'store_true\', default=False)\nparser.add_argument(\'--exp\', type=str, default=\'default\',\n                    help=\'experiment directory name\')\nparser.add_argument(\'--tb_tag\', type=str, default=\'\',\n                    help=\'add tag to tb dir\')\nparser.add_argument(\'--ckpt\', type=str, default=\'logs/ckpt\')\nparser.add_argument(\'--tb_path\', type=str, default=\'logs/tb\')\nparser.add_argument(\'--syncbn\', action=\'store_true\', default=True,\n                    help=\'Synchronized BN\')\nparser.add_argument(\'--dump_augmentation_images\', action=\'store_true\', default=False,\n                    help=\'Synchronized BN\')\nparser.add_argument(\'--test_mode\', action=\'store_true\', default=False,\n                    help=\'minimum testing (1 epoch run ) to verify nothing failed\')\nparser.add_argument(\'-wb\', \'--wt_bound\', type=float, default=1.0)\nparser.add_argument(\'--maxSkip\', type=int, default=0)\nargs = parser.parse_args()\nargs.best_record = {\'epoch\': -1, \'iter\': 0, \'val_loss\': 1e10, \'acc\': 0,\n                        \'acc_cls\': 0, \'mean_iu\': 0, \'fwavacc\': 0}\n\n#Enable CUDNN Benchmarking optimization\ntorch.backends.cudnn.benchmark = True\nargs.world_size = 1\n#Test Mode run two epochs with a few iterations of training and val\nif args.test_mode:\n    args.max_epoch = 2\n\nif \'WORLD_SIZE\' in os.environ:\n    args.world_size = int(os.environ[\'WORLD_SIZE\'])\n    print(""Total world size: "", int(os.environ[\'WORLD_SIZE\']))\n\ndef main():\n    \'\'\'\n    Main Function\n\n    \'\'\'\n\n    #Set up the Arguments, Tensorboard Writer, Dataloader, Loss Fn, Optimizer\n    assert_and_infer_cfg(args)\n    writer = prep_experiment(args,parser)\n    train_loader, val_loader, train_obj = datasets.setup_loaders(args)\n    criterion, criterion_val = loss.get_loss(args)\n    net = network.get_net(args, criterion)\n    optim, scheduler = optimizer.get_optimizer(args, net)\n\n    torch.cuda.empty_cache()\n\n    if args.evaluate:\n        # Early evaluation for benchmarking\n        default_eval_epoch = 1\n        validate(val_loader, net, criterion_val,\n                 optim, default_eval_epoch, writer)\n        evaluate(val_loader, net)\n        return\n\n    #Main Loop\n    for epoch in range(args.start_epoch, args.max_epoch):\n\t# Update EPOCH CTR\n        cfg.immutable(False)\n        cfg.EPOCH  = epoch\n        cfg.immutable(True)\n\n        scheduler.step()\n\n        train(train_loader, net, criterion, optim, epoch, writer)\n        validate(val_loader, net, criterion_val,\n                 optim, epoch, writer)\n\n\ndef train(train_loader, net, criterion, optimizer, curr_epoch, writer):\n    \'\'\'\n    Runs the training loop per epoch\n    train_loader: Data loader for train\n    net: thet network\n    criterion: loss fn\n    optimizer: optimizer\n    curr_epoch: current epoch \n    writer: tensorboard writer\n    return: val_avg for step function if required\n    \'\'\'\n    net.train()\n\n    train_main_loss = AverageMeter()\n    train_edge_loss = AverageMeter()\n    train_seg_loss = AverageMeter()\n    train_att_loss = AverageMeter()\n    train_dual_loss = AverageMeter()\n    curr_iter = curr_epoch * len(train_loader)\n\n    for i, data in enumerate(train_loader):\n        if i==0:\n            print(\'running....\')\n\n        inputs, mask, edge, _img_name = data\n\n        if torch.sum(torch.isnan(inputs)) > 0:\n            import pdb; pdb.set_trace()\n\n        batch_pixel_size = inputs.size(0) * inputs.size(2) * inputs.size(3)\n\n        inputs, mask, edge = inputs.cuda(), mask.cuda(), edge.cuda()\n\n        if i==0:\n            print(\'forward done\')\n\n        optimizer.zero_grad()\n\n        main_loss = None\n        loss_dict = None\n\n        if args.joint_edgeseg_loss:\n            loss_dict = net(inputs, gts=(mask, edge))\n            \n            if args.seg_weight > 0:\n                log_seg_loss = loss_dict[\'seg_loss\'].mean().clone().detach_()\n                train_seg_loss.update(log_seg_loss.item(), batch_pixel_size)\n                main_loss = loss_dict[\'seg_loss\']\n\n            if args.edge_weight > 0:\n                log_edge_loss = loss_dict[\'edge_loss\'].mean().clone().detach_()\n                train_edge_loss.update(log_edge_loss.item(), batch_pixel_size)\n                if main_loss is not None:\n                    main_loss += loss_dict[\'edge_loss\']\n                else:\n                    main_loss = loss_dict[\'edge_loss\']\n            \n            if args.att_weight > 0:\n                log_att_loss = loss_dict[\'att_loss\'].mean().clone().detach_()\n                train_att_loss.update(log_att_loss.item(), batch_pixel_size)\n                if main_loss is not None:\n                    main_loss += loss_dict[\'att_loss\']\n                else:\n                    main_loss = loss_dict[\'att_loss\']\n\n            if args.dual_weight > 0:\n                log_dual_loss = loss_dict[\'dual_loss\'].mean().clone().detach_()\n                train_dual_loss.update(log_dual_loss.item(), batch_pixel_size)\n                if main_loss is not None:\n                    main_loss += loss_dict[\'dual_loss\']\n                else:\n                    main_loss = loss_dict[\'dual_loss\']\n\n        else:\n            main_loss = net(inputs, gts=mask)\n\n        main_loss = main_loss.mean()\n        log_main_loss = main_loss.clone().detach_()\n\n        train_main_loss.update(log_main_loss.item(), batch_pixel_size)\n\n        main_loss.backward()\n\n        optimizer.step()\n\n        if i==0:\n            print(\'step 1 done\')\n\n        curr_iter += 1\n\n        if args.local_rank == 0:\n            msg = \'[epoch {}], [iter {} / {}], [train main loss {:0.6f}], [seg loss {:0.6f}], [edge loss {:0.6f}], [lr {:0.6f}]\'.format(\n            curr_epoch, i + 1, len(train_loader), train_main_loss.avg, train_seg_loss.avg, train_edge_loss.avg, optimizer.param_groups[-1][\'lr\'] )\n\n            logging.info(msg)\n\n            # Log tensorboard metrics for each iteration of the training phase\n            writer.add_scalar(\'training/loss\', (train_main_loss.val),\n                              curr_iter)\n            writer.add_scalar(\'training/lr\', optimizer.param_groups[-1][\'lr\'],\n                              curr_iter)\n            if args.joint_edgeseg_loss:\n\n                writer.add_scalar(\'training/seg_loss\', (train_seg_loss.val),\n                                  curr_iter)\n                writer.add_scalar(\'training/edge_loss\', (train_edge_loss.val),\n                                  curr_iter)\n                writer.add_scalar(\'training/att_loss\', (train_att_loss.val),\n                                  curr_iter)\n                writer.add_scalar(\'training/dual_loss\', (train_dual_loss.val),\n                                  curr_iter)\n        if i > 5 and args.test_mode:\n            return\n\ndef validate(val_loader, net, criterion, optimizer, curr_epoch, writer):\n    \'\'\'\n    Runs the validation loop after each training epoch\n    val_loader: Data loader for validation\n    net: thet network\n    criterion: loss fn\n    optimizer: optimizer\n    curr_epoch: current epoch \n    writer: tensorboard writer\n    return: \n    \'\'\'\n    net.eval()\n    val_loss = AverageMeter()\n    mf_score = AverageMeter()\n    IOU_acc = 0\n    dump_images = []\n    heatmap_images = []\n    for vi, data in enumerate(val_loader):\n        input, mask, edge, img_names = data\n        assert len(input.size()) == 4 and len(mask.size()) == 3\n        assert input.size()[2:] == mask.size()[1:]\n        h, w = mask.size()[1:]\n\n        batch_pixel_size = input.size(0) * input.size(2) * input.size(3)\n        input, mask_cuda, edge_cuda = input.cuda(), mask.cuda(), edge.cuda()\n\n        with torch.no_grad():\n            seg_out, edge_out = net(input)    # output = (1, 19, 713, 713)\n\n        if args.joint_edgeseg_loss:\n            loss_dict = criterion((seg_out, edge_out), (mask_cuda, edge_cuda))\n            val_loss.update(sum(loss_dict.values()).item(), batch_pixel_size)\n        else:\n            val_loss.update(criterion(seg_out, mask_cuda).item(), batch_pixel_size)\n\n        # Collect data from different GPU to a single GPU since\n        # encoding.parallel.criterionparallel function calculates distributed loss\n        # functions\n\n        seg_predictions = seg_out.data.max(1)[1].cpu()\n        edge_predictions = edge_out.max(1)[0].cpu()\n\n        #Logging\n        if vi % 20 == 0:\n            if args.local_rank == 0:\n                logging.info(\'validating: %d / %d\' % (vi + 1, len(val_loader)))\n        if vi > 10 and args.test_mode:\n            break\n        _edge = edge.max(1)[0]\n\n        #Image Dumps\n        if vi < 10:\n            dump_images.append([mask, seg_predictions, img_names])\n            heatmap_images.append([_edge, edge_predictions, img_names])\n\n        IOU_acc += fast_hist(seg_predictions.numpy().flatten(), mask.numpy().flatten(),\n                                   args.dataset_cls.num_classes)\n\n        del seg_out, edge_out, vi, data\n\n    if args.local_rank == 0:\n        evaluate_eval(args, net, optimizer, val_loss, mf_score, IOU_acc, dump_images, heatmap_images,\n                writer, curr_epoch, args.dataset_cls)\n\n    return val_loss.avg\n\ndef evaluate(val_loader, net):\n    \'\'\'\n    Runs the evaluation loop and prints F score\n    val_loader: Data loader for validation\n    net: thet network\n    return: \n    \'\'\'\n    net.eval()\n    for thresh in args.eval_thresholds.split(\',\'):\n        mf_score1 = AverageMeter()\n        mf_pc_score1 = AverageMeter()\n        ap_score1 = AverageMeter()\n        ap_pc_score1 = AverageMeter()\n        Fpc = np.zeros((args.dataset_cls.num_classes))\n        Fc = np.zeros((args.dataset_cls.num_classes))\n        for vi, data in enumerate(val_loader):\n            input, mask, edge, img_names = data\n            assert len(input.size()) == 4 and len(mask.size()) == 3\n            assert input.size()[2:] == mask.size()[1:]\n            h, w = mask.size()[1:]\n\n            batch_pixel_size = input.size(0) * input.size(2) * input.size(3)\n            input, mask_cuda, edge_cuda = input.cuda(), mask.cuda(), edge.cuda()\n\n            with torch.no_grad():\n                seg_out, edge_out = net(input)\n\n            seg_predictions = seg_out.data.max(1)[1].cpu()\n            edge_predictions = edge_out.max(1)[0].cpu()\n\n            logging.info(\'evaluating: %d / %d\' % (vi + 1, len(val_loader)))\n            _Fpc, _Fc = eval_mask_boundary(seg_predictions.numpy(), mask.numpy(), args.dataset_cls.num_classes, bound_th=float(thresh))\n            Fc += _Fc\n            Fpc += _Fpc\n\n            del seg_out, edge_out, vi, data\n\n        logging.info(\'Threshold: \' + thresh)\n        logging.info(\'F_Score: \' + str(np.sum(Fpc/Fc)/args.dataset_cls.num_classes))\n        logging.info(\'F_Score (Classwise): \' + str(Fpc/Fc))\n\nif __name__ == \'__main__\':\n    main()\n\n\n\n\n'"
datasets/__init__.py,1,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nfrom datasets import cityscapes\nimport torchvision.transforms as standard_transforms\nimport torchvision.utils as vutils\nimport transforms.joint_transforms as joint_transforms\nimport transforms.transforms as extended_transforms\nfrom torch.utils.data import DataLoader\n\ndef setup_loaders(args):\n    \'\'\'\n    input: argument passed by the user\n    return:  training data loader, validation data loader loader,  train_set\n    \'\'\'\n\n    if args.dataset == \'cityscapes\':\n        args.dataset_cls = cityscapes\n        args.train_batch_size = args.bs_mult * args.ngpu\n        if args.bs_mult_val > 0:\n            args.val_batch_size = args.bs_mult_val * args.ngpu\n        else:\n            args.val_batch_size = args.bs_mult * args.ngpu\n    else:\n        raise\n\n    args.num_workers = 4 * args.ngpu\n    if args.test_mode:\n        args.num_workers = 0 #1\n\n    mean_std = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n    # Geometric image transformations\n    train_joint_transform_list = [\n        joint_transforms.RandomSizeAndCrop(args.crop_size,\n                                           False,\n                                           pre_size=args.pre_size,\n                                           scale_min=args.scale_min,\n                                           scale_max=args.scale_max,\n                                           ignore_index=args.dataset_cls.ignore_label),\n        joint_transforms.Resize(args.crop_size),\n        joint_transforms.RandomHorizontallyFlip()]\n \n    #if args.rotate:\n    #    train_joint_transform_list += [joint_transforms.RandomRotate(args.rotate)]\n\n    train_joint_transform = joint_transforms.Compose(train_joint_transform_list)\n\n    # Image appearance transformations\n    train_input_transform = []\n    if args.color_aug:\n        train_input_transform += [extended_transforms.ColorJitter(\n            brightness=args.color_aug,\n            contrast=args.color_aug,\n            saturation=args.color_aug,\n            hue=args.color_aug)]\n\n    if args.bblur:\n        train_input_transform += [extended_transforms.RandomBilateralBlur()]\n    elif args.gblur:\n        train_input_transform += [extended_transforms.RandomGaussianBlur()]\n    else:\n        pass\n\n    train_input_transform += [standard_transforms.ToTensor(),\n                              standard_transforms.Normalize(*mean_std)]\n    train_input_transform = standard_transforms.Compose(train_input_transform)\n\n    val_input_transform = standard_transforms.Compose([\n        standard_transforms.ToTensor(),\n        standard_transforms.Normalize(*mean_std)\n    ])\n\n    target_transform = extended_transforms.MaskToTensor()\n    \n    target_train_transform = extended_transforms.MaskToTensor()\n\n    if args.dataset == \'cityscapes\':\n        city_mode = \'train\' ## Can be trainval\n        city_quality = \'fine\'\n        train_set = args.dataset_cls.CityScapes(\n            city_quality, city_mode, 0, \n            joint_transform=train_joint_transform,\n            transform=train_input_transform,\n            target_transform=target_train_transform,\n            dump_images=args.dump_augmentation_images,\n            cv_split=args.cv)\n        val_set = args.dataset_cls.CityScapes(\'fine\', \'val\', 0, \n                                              transform=val_input_transform,\n                                              target_transform=target_transform,\n                                              cv_split=args.cv)\n    else:\n        raise\n    \n    train_sampler = None\n    val_sampler = None\n\n    train_loader = DataLoader(train_set, batch_size=args.train_batch_size,\n                              num_workers=args.num_workers, shuffle=(train_sampler is None), drop_last=True, sampler = train_sampler)\n    val_loader = DataLoader(val_set, batch_size=args.val_batch_size,\n                            num_workers=args.num_workers // 2 , shuffle=False, drop_last=False, sampler = val_sampler)\n\n    return train_loader, val_loader,  train_set\n\n'"
datasets/cityscapes.py,2,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport os\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom torch.utils import data\nfrom collections import defaultdict\nimport math\nimport logging\nimport datasets.cityscapes_labels as cityscapes_labels\nimport json\nfrom config import cfg\nimport torchvision.transforms as transforms\nimport datasets.edge_utils as edge_utils\n\ntrainid_to_name = cityscapes_labels.trainId2name\nid_to_trainid = cityscapes_labels.label2trainid\nnum_classes = 19\nignore_label = 255\nroot = cfg.DATASET.CITYSCAPES_DIR\n\npalette = [128, 64, 128, 244, 35, 232, 70, 70, 70, 102, 102, 156, 190, 153, 153,\n           153, 153, 153, 250, 170, 30,\n           220, 220, 0, 107, 142, 35, 152, 251, 152, 70, 130, 180, 220, 20, 60,\n           255, 0, 0, 0, 0, 142, 0, 0, 70,\n           0, 60, 100, 0, 80, 100, 0, 0, 230, 119, 11, 32]\nzero_pad = 256 * 3 - len(palette)\nfor i in range(zero_pad):\n    palette.append(0)\n\n\ndef colorize_mask(mask):\n    # mask: numpy array of the mask\n    new_mask = Image.fromarray(mask.astype(np.uint8)).convert(\'P\')\n    new_mask.putpalette(palette)\n    return new_mask\n\n\ndef add_items(items, aug_items, cities, img_path, mask_path, mask_postfix, mode, maxSkip):\n\n    for c in cities:\n        c_items = [name.split(\'_leftImg8bit.png\')[0] for name in\n                   os.listdir(os.path.join(img_path, c))]\n        for it in c_items:\n            item = (os.path.join(img_path, c, it + \'_leftImg8bit.png\'),\n                        os.path.join(mask_path, c, it + mask_postfix))\n            items.append(item)\n\ndef make_cv_splits(img_dir_name):\n    \'\'\'\n    Create splits of train/val data.\n    A split is a lists of cities.\n    split0 is aligned with the default Cityscapes train/val.\n    \'\'\'\n    trn_path = os.path.join(root, img_dir_name, \'leftImg8bit\', \'train\')\n    val_path = os.path.join(root, img_dir_name, \'leftImg8bit\', \'val\')\n\n    trn_cities = [\'train/\' + c for c in os.listdir(trn_path)]\n    val_cities = [\'val/\' + c for c in os.listdir(val_path)]\n\n    # want reproducible randomly shuffled\n    trn_cities = sorted(trn_cities)\n\n    all_cities = val_cities + trn_cities\n    num_val_cities = len(val_cities)\n    num_cities = len(all_cities)\n\n    cv_splits = []\n    for split_idx in range(cfg.DATASET.CV_SPLITS):\n        split = {}\n        split[\'train\'] = []\n        split[\'val\'] = []\n        offset = split_idx * num_cities // cfg.DATASET.CV_SPLITS\n        for j in range(num_cities):\n            if j >= offset and j < (offset + num_val_cities):\n                split[\'val\'].append(all_cities[j])\n            else:\n                split[\'train\'].append(all_cities[j])\n        cv_splits.append(split)\n\n    return cv_splits\n\n\ndef make_split_coarse(img_path):\n    \'\'\'\n    Create a train/val split for coarse\n    return: city split in train\n    \'\'\'\n    all_cities = os.listdir(img_path)\n    all_cities = sorted(all_cities)  # needs to always be the same\n    val_cities = [] # Can manually set cities to not be included into train split\n\n    split = {}\n    split[\'val\'] = val_cities\n    split[\'train\'] = [c for c in all_cities if c not in val_cities]\n    return split\n\ndef make_test_split(img_dir_name):\n    test_path = os.path.join(root, img_dir_name, \'leftImg8bit\', \'test\')\n    test_cities = [\'test/\' + c for c in os.listdir(test_path)]\n\n    return test_cities\n\n\ndef make_dataset(quality, mode, maxSkip=0, fine_coarse_mult=6, cv_split=0):\n    \'\'\'\n    Assemble list of images + mask files\n\n    fine -   modes: train/val/test/trainval    cv:0,1,2\n    coarse - modes: train/val                  cv:na\n\n    path examples:\n    leftImg8bit_trainextra/leftImg8bit/train_extra/augsburg\n    gtCoarse/gtCoarse/train_extra/augsburg\n    \'\'\'\n    items = []\n    aug_items = []\n\n    if quality == \'fine\':\n        assert mode in [\'train\', \'val\', \'test\', \'trainval\']\n        img_dir_name = \'leftImg8bit_trainvaltest\'\n        img_path = os.path.join(root, img_dir_name, \'leftImg8bit\')\n        mask_path = os.path.join(root, \'gtFine_trainvaltest\', \'gtFine\')\n        mask_postfix = \'_gtFine_labelIds.png\'\n        cv_splits = make_cv_splits(img_dir_name)\n        if mode == \'trainval\':\n            modes = [\'train\', \'val\']\n        else:\n            modes = [mode]\n        for mode in modes:\n            if mode == \'test\':\n                cv_splits = make_test_split(img_dir_name)\n                add_items(items, cv_splits, img_path, mask_path,\n                      mask_postfix)\n            else:\n                logging.info(\'{} fine cities: \'.format(mode) + str(cv_splits[cv_split][mode]))\n\n                add_items(items, aug_items, cv_splits[cv_split][mode], img_path, mask_path,\n                      mask_postfix, mode, maxSkip)\n    else:\n        raise \'unknown cityscapes quality {}\'.format(quality)\n    logging.info(\'Cityscapes-{}: {} images\'.format(mode, len(items)+len(aug_items)))\n    return items, aug_items\n\n\nclass CityScapes(data.Dataset):\n\n    def __init__(self, quality, mode, maxSkip=0, joint_transform=None, sliding_crop=None,\n                 transform=None, target_transform=None, dump_images=False,\n                 cv_split=None, eval_mode=False, \n                 eval_scales=None, eval_flip=False):\n        self.quality = quality\n        self.mode = mode\n        self.maxSkip = maxSkip\n        self.joint_transform = joint_transform\n        self.sliding_crop = sliding_crop\n        self.transform = transform\n        self.target_transform = target_transform\n        self.dump_images = dump_images\n        self.eval_mode = eval_mode\n        self.eval_flip = eval_flip\n        self.eval_scales = None\n        if eval_scales != None:\n            self.eval_scales = [float(scale) for scale in eval_scales.split("","")]\n\n        if cv_split:\n            self.cv_split = cv_split\n            assert cv_split < cfg.DATASET.CV_SPLITS, \\\n                \'expected cv_split {} to be < CV_SPLITS {}\'.format(\n                    cv_split, cfg.DATASET.CV_SPLITS)\n        else:\n            self.cv_split = 0\n        self.imgs, _ = make_dataset(quality, mode, self.maxSkip, cv_split=self.cv_split)\n        if len(self.imgs) == 0:\n            raise RuntimeError(\'Found 0 images, please check the data set\')\n\n        self.mean_std = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n    def _eval_get_item(self, img, mask, scales, flip_bool):\n        return_imgs = []\n        for flip in range(int(flip_bool)+1):\n            imgs = []\n            if flip :\n                img = img.transpose(Image.FLIP_LEFT_RIGHT)\n            for scale in scales:\n                w,h = img.size\n                target_w, target_h = int(w * scale), int(h * scale) \n                resize_img =img.resize((target_w, target_h))\n                tensor_img = transforms.ToTensor()(resize_img)\n                final_tensor = transforms.Normalize(*self.mean_std)(tensor_img)\n                imgs.append(tensor_img)\n            return_imgs.append(imgs)\n        return return_imgs, mask\n        \n\n\n    def __getitem__(self, index):\n\n        img_path, mask_path = self.imgs[index]\n\n        img, mask = Image.open(img_path).convert(\'RGB\'), Image.open(mask_path)\n        img_name = os.path.splitext(os.path.basename(img_path))[0]\n\n        mask = np.array(mask)\n        mask_copy = mask.copy()\n        for k, v in id_to_trainid.items():\n            mask_copy[mask == k] = v\n\n        if self.eval_mode:\n            return self._eval_get_item(img, mask_copy, self.eval_scales, self.eval_flip), img_name\n\n        mask = Image.fromarray(mask_copy.astype(np.uint8))\n\n        # Image Transformations\n        if self.joint_transform is not None:\n            img, mask = self.joint_transform(img, mask)\n        if self.transform is not None:\n            img = self.transform(img)\n        if self.target_transform is not None:\n            mask = self.target_transform(mask)\n\n        _edgemap = mask.numpy()\n        _edgemap = edge_utils.mask_to_onehot(_edgemap, num_classes)\n\n        _edgemap = edge_utils.onehot_to_binary_edges(_edgemap, 2, num_classes)\n\n        edgemap = torch.from_numpy(_edgemap).float()\n        \n\t# Debug\n        if self.dump_images:\n            outdir = \'../../dump_imgs_{}\'.format(self.mode)\n            os.makedirs(outdir, exist_ok=True)\n            out_img_fn = os.path.join(outdir, img_name + \'.png\')\n            out_msk_fn = os.path.join(outdir, img_name + \'_mask.png\')\n            mask_img = colorize_mask(np.array(mask))\n            img.save(out_img_fn)\n            mask_img.save(out_msk_fn)\n\n        return img, mask, edgemap, img_name\n\n    def __len__(self):\n        return len(self.imgs)\n\n\ndef make_dataset_video():\n    img_dir_name = \'leftImg8bit_demoVideo\'\n    img_path = os.path.join(root, img_dir_name, \'leftImg8bit/demoVideo\')\n    items = []\n    categories = os.listdir(img_path)\n    for c in categories[1:]:\n        c_items = [name.split(\'_leftImg8bit.png\')[0] for name in\n                   os.listdir(os.path.join(img_path, c))]\n        for it in c_items:\n            item = os.path.join(img_path, c, it + \'_leftImg8bit.png\')\n            items.append(item)\n    return items\n\n\nclass CityScapesVideo(data.Dataset):\n\n    def __init__(self, transform=None):\n        self.imgs = make_dataset_video()\n        if len(self.imgs) == 0:\n            raise RuntimeError(\'Found 0 images, please check the data set\')\n        self.transform = transform\n\n    def __getitem__(self, index):\n        img_path = self.imgs[index]\n        img = Image.open(img_path).convert(\'RGB\')\n        img_name = os.path.splitext(os.path.basename(img_path))[0]\n\n        if self.transform is not None:\n            img = self.transform(img)\n        return img, img_name\n\n    def __len__(self):\n        return len(self.imgs)\n\n'"
datasets/cityscapes_labels.py,0,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n\n# File taken from https://github.com/mcordts/cityscapesScripts/\n# License File Available at:\n# https://github.com/mcordts/cityscapesScripts/blob/master/license.txt\n\n# ----------------------\n# The Cityscapes Dataset\n# ----------------------\n#\n#\n# License agreement\n# -----------------\n#\n# This dataset is made freely available to academic and non-academic entities for non-commercial purposes such as academic research, teaching, scientific publications, or personal experimentation. Permission is granted to use the data given that you agree:\n#\n# 1. That the dataset comes ""AS IS"", without express or implied warranty. Although every effort has been made to ensure accuracy, we (Daimler AG, MPI Informatics, TU Darmstadt) do not accept any responsibility for errors or omissions.\n# 2. That you include a reference to the Cityscapes Dataset in any work that makes use of the dataset. For research papers, cite our preferred publication as listed on our website; for other media cite our preferred publication as listed on our website or link to the Cityscapes website.\n# 3. That you do not distribute this dataset or modified versions. It is permissible to distribute derivative works in as far as they are abstract representations of this dataset (such as models trained on it or additional annotations that do not directly include any of our data) and do not allow to recover the dataset or something similar in character.\n# 4. That you may not use the dataset or any derivative work for commercial purposes as, for example, licensing or selling the data, or using the data with a purpose to procure a commercial gain.\n# 5. That all rights not expressly granted to you are reserved by us (Daimler AG, MPI Informatics, TU Darmstadt).\n#\n#\n# Contact\n# -------\n#\n# Marius Cordts, Mohamed Omran\n# www.cityscapes-dataset.net\n\n""""""\n\nfrom collections import namedtuple\n\n\n#--------------------------------------------------------------------------------\n# Definitions\n#--------------------------------------------------------------------------------\n\n# a label and all meta information\nLabel = namedtuple( \'Label\' , [\n\n    \'name\'        , # The identifier of this label, e.g. \'car\', \'person\', ... .\n                    # We use them to uniquely name a class\n\n    \'id\'          , # An integer ID that is associated with this label.\n                    # The IDs are used to represent the label in ground truth images\n                    # An ID of -1 means that this label does not have an ID and thus\n                    # is ignored when creating ground truth images (e.g. license plate).\n                    # Do not modify these IDs, since exactly these IDs are expected by the\n                    # evaluation server.\n\n    \'trainId\'     , # Feel free to modify these IDs as suitable for your method. Then create\n                    # ground truth images with train IDs, using the tools provided in the\n                    # \'preparation\' folder. However, make sure to validate or submit results\n                    # to our evaluation server using the regular IDs above!\n                    # For trainIds, multiple labels might have the same ID. Then, these labels\n                    # are mapped to the same class in the ground truth images. For the inverse\n                    # mapping, we use the label that is defined first in the list below.\n                    # For example, mapping all void-type classes to the same ID in training,\n                    # might make sense for some approaches.\n                    # Max value is 255!\n\n    \'category\'    , # The name of the category that this label belongs to\n\n    \'categoryId\'  , # The ID of this category. Used to create ground truth images\n                    # on category level.\n\n    \'hasInstances\', # Whether this label distinguishes between single instances or not\n\n    \'ignoreInEval\', # Whether pixels having this class as ground truth label are ignored\n                    # during evaluations or not\n\n    \'color\'       , # The color of this label\n    ] )\n\n\n#--------------------------------------------------------------------------------\n# A list of all labels\n#--------------------------------------------------------------------------------\n\n# Please adapt the train IDs as appropriate for you approach.\n# Note that you might want to ignore labels with ID 255 during training.\n# Further note that the current train IDs are only a suggestion. You can use whatever you like.\n# Make sure to provide your results using the original IDs and not the training IDs.\n# Note that many IDs are ignored in evaluation and thus you never need to predict these!\n\nlabels = [\n    #       name                     id    trainId   category            catId     hasInstances   ignoreInEval   color\n    Label(  \'unlabeled\'            ,  0 ,      255 , \'void\'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  \'ego vehicle\'          ,  1 ,      255 , \'void\'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  \'rectification border\' ,  2 ,      255 , \'void\'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  \'out of roi\'           ,  3 ,      255 , \'void\'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  \'static\'               ,  4 ,      255 , \'void\'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  \'dynamic\'              ,  5 ,      255 , \'void\'            , 0       , False        , True         , (111, 74,  0) ),\n    Label(  \'ground\'               ,  6 ,      255 , \'void\'            , 0       , False        , True         , ( 81,  0, 81) ),\n    Label(  \'road\'                 ,  7 ,        0 , \'flat\'            , 1       , False        , False        , (128, 64,128) ),\n    Label(  \'sidewalk\'             ,  8 ,        1 , \'flat\'            , 1       , False        , False        , (244, 35,232) ),\n    Label(  \'parking\'              ,  9 ,      255 , \'flat\'            , 1       , False        , True         , (250,170,160) ),\n    Label(  \'rail track\'           , 10 ,      255 , \'flat\'            , 1       , False        , True         , (230,150,140) ),\n    Label(  \'building\'             , 11 ,        2 , \'construction\'    , 2       , False        , False        , ( 70, 70, 70) ),\n    Label(  \'wall\'                 , 12 ,        3 , \'construction\'    , 2       , False        , False        , (102,102,156) ),\n    Label(  \'fence\'                , 13 ,        4 , \'construction\'    , 2       , False        , False        , (190,153,153) ),\n    Label(  \'guard rail\'           , 14 ,      255 , \'construction\'    , 2       , False        , True         , (180,165,180) ),\n    Label(  \'bridge\'               , 15 ,      255 , \'construction\'    , 2       , False        , True         , (150,100,100) ),\n    Label(  \'tunnel\'               , 16 ,      255 , \'construction\'    , 2       , False        , True         , (150,120, 90) ),\n    Label(  \'pole\'                 , 17 ,        5 , \'object\'          , 3       , False        , False        , (153,153,153) ),\n    Label(  \'polegroup\'            , 18 ,      255 , \'object\'          , 3       , False        , True         , (153,153,153) ),\n    Label(  \'traffic light\'        , 19 ,        6 , \'object\'          , 3       , False        , False        , (250,170, 30) ),\n    Label(  \'traffic sign\'         , 20 ,        7 , \'object\'          , 3       , False        , False        , (220,220,  0) ),\n    Label(  \'vegetation\'           , 21 ,        8 , \'nature\'          , 4       , False        , False        , (107,142, 35) ),\n    Label(  \'terrain\'              , 22 ,        9 , \'nature\'          , 4       , False        , False        , (152,251,152) ),\n    Label(  \'sky\'                  , 23 ,       10 , \'sky\'             , 5       , False        , False        , ( 70,130,180) ),\n    Label(  \'person\'               , 24 ,       11 , \'human\'           , 6       , True         , False        , (220, 20, 60) ),\n    Label(  \'rider\'                , 25 ,       12 , \'human\'           , 6       , True         , False        , (255,  0,  0) ),\n    Label(  \'car\'                  , 26 ,       13 , \'vehicle\'         , 7       , True         , False        , (  0,  0,142) ),\n    Label(  \'truck\'                , 27 ,       14 , \'vehicle\'         , 7       , True         , False        , (  0,  0, 70) ),\n    Label(  \'bus\'                  , 28 ,       15 , \'vehicle\'         , 7       , True         , False        , (  0, 60,100) ),\n    Label(  \'caravan\'              , 29 ,      255 , \'vehicle\'         , 7       , True         , True         , (  0,  0, 90) ),\n    Label(  \'trailer\'              , 30 ,      255 , \'vehicle\'         , 7       , True         , True         , (  0,  0,110) ),\n    Label(  \'train\'                , 31 ,       16 , \'vehicle\'         , 7       , True         , False        , (  0, 80,100) ),\n    Label(  \'motorcycle\'           , 32 ,       17 , \'vehicle\'         , 7       , True         , False        , (  0,  0,230) ),\n    Label(  \'bicycle\'              , 33 ,       18 , \'vehicle\'         , 7       , True         , False        , (119, 11, 32) ),\n    Label(  \'license plate\'        , -1 ,       -1 , \'vehicle\'         , 7       , False        , True         , (  0,  0,142) ),\n    Label(  \'license plate\'        , 34 ,       255 , \'vehicle\'         , 7       , False        , True         , (  0,  0,142) ),\n]\n\n\n#--------------------------------------------------------------------------------\n# Create dictionaries for a fast lookup\n#--------------------------------------------------------------------------------\n\n# Please refer to the main method below for example usages!\n\n# name to label object\nname2label      = { label.name    : label for label in labels           }\n# id to label object\nid2label        = { label.id      : label for label in labels           }\n# trainId to label object\ntrainId2label   = { label.trainId : label for label in reversed(labels) }\n# label2trainid\nlabel2trainid   = { label.id      : label.trainId for label in labels   }\n# trainId to label object\ntrainId2name   = { label.trainId : label.name for label in labels   }\ntrainId2color  = { label.trainId : label.color for label in labels      }\n# category to list of label objects\ncategory2labels = {}\nfor label in labels:\n    category = label.category\n    if category in category2labels:\n        category2labels[category].append(label)\n    else:\n        category2labels[category] = [label]\n\n#--------------------------------------------------------------------------------\n# Assure single instance name\n#--------------------------------------------------------------------------------\n\n# returns the label name that describes a single instance (if possible)\n# e.g.     input     |   output\n#        ----------------------\n#          car       |   car\n#          cargroup  |   car\n#          foo       |   None\n#          foogroup  |   None\n#          skygroup  |   None\ndef assureSingleInstanceName( name ):\n    # if the name is known, it is not a group\n    if name in name2label:\n        return name\n    # test if the name actually denotes a group\n    if not name.endswith(""group""):\n        return None\n    # remove group\n    name = name[:-len(""group"")]\n    # test if the new name exists\n    if not name in name2label:\n        return None\n    # test if the new name denotes a label that actually has instances\n    if not name2label[name].hasInstances:\n        return None\n    # all good then\n    return name\n\n#--------------------------------------------------------------------------------\n# Main for testing\n#--------------------------------------------------------------------------------\n\n# just a dummy main\nif __name__ == ""__main__"":\n    # Print all the labels\n    print(""List of cityscapes labels:"")\n    print("""")\n    print((""    {:>21} | {:>3} | {:>7} | {:>14} | {:>10} | {:>12} | {:>12}"".format( \'name\', \'id\', \'trainId\', \'category\', \'categoryId\', \'hasInstances\', \'ignoreInEval\' )))\n    print((""    "" + (\'-\' * 98)))\n    for label in labels:\n        print((""    {:>21} | {:>3} | {:>7} | {:>14} | {:>10} | {:>12} | {:>12}"".format( label.name, label.id, label.trainId, label.category, label.categoryId, label.hasInstances, label.ignoreInEval )))\n    print("""")\n\n    print(""Example usages:"")\n\n    # Map from name to label\n    name = \'car\'\n    id   = name2label[name].id\n    print((""ID of label \'{name}\': {id}"".format( name=name, id=id )))\n\n    # Map from ID to label\n    category = id2label[id].category\n    print((""Category of label with ID \'{id}\': {category}"".format( id=id, category=category )))\n\n    # Map from trainID to label\n    trainId = 0\n    name = trainId2label[trainId].name\n    print((""Name of label with trainID \'{id}\': {name}"".format( id=trainId, name=name )))\n'"
datasets/edge_utils.py,0,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport os\nimport numpy as np\nfrom PIL import Image\nfrom scipy.ndimage.morphology import distance_transform_edt\n\ndef mask_to_onehot(mask, num_classes):\n    """"""\n    Converts a segmentation mask (H,W) to (K,H,W) where the last dim is a one\n    hot encoding vector\n\n    """"""\n    _mask = [mask == (i + 1) for i in range(num_classes)]\n    return np.array(_mask).astype(np.uint8)\n\ndef onehot_to_mask(mask):\n    """"""\n    Converts a mask (K,H,W) to (H,W)\n    """"""\n    _mask = np.argmax(mask, axis=0)\n    _mask[_mask != 0] += 1\n    return _mask\n\ndef onehot_to_multiclass_edges(mask, radius, num_classes):\n    """"""\n    Converts a segmentation mask (K,H,W) to an edgemap (K,H,W)\n\n    """"""\n    if radius < 0:\n        return mask\n    \n    # We need to pad the borders for boundary conditions\n    mask_pad = np.pad(mask, ((0, 0), (1, 1), (1, 1)), mode=\'constant\', constant_values=0)\n    \n    channels = []\n    for i in range(num_classes):\n        dist = distance_transform_edt(mask_pad[i, :])+distance_transform_edt(1.0-mask_pad[i, :])\n        dist = dist[1:-1, 1:-1]\n        dist[dist > radius] = 0\n        dist = (dist > 0).astype(np.uint8)\n        channels.append(dist)\n        \n    return np.array(channels)\n\ndef onehot_to_binary_edges(mask, radius, num_classes):\n    """"""\n    Converts a segmentation mask (K,H,W) to a binary edgemap (H,W)\n\n    """"""\n    \n    if radius < 0:\n        return mask\n    \n    # We need to pad the borders for boundary conditions\n    mask_pad = np.pad(mask, ((0, 0), (1, 1), (1, 1)), mode=\'constant\', constant_values=0)\n    \n    edgemap = np.zeros(mask.shape[1:])\n\n    for i in range(num_classes):\n        dist = distance_transform_edt(mask_pad[i, :])+distance_transform_edt(1.0-mask_pad[i, :])\n        dist = dist[1:-1, 1:-1]\n        dist[dist > radius] = 0\n        edgemap += dist\n    edgemap = np.expand_dims(edgemap, axis=0)    \n    edgemap = (edgemap > 0).astype(np.uint8)\n    return edgemap\n\n'"
my_functionals/DualTaskLoss.py,10,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n\n# Code adapted from:\n# https://github.com/ericjang/gumbel-softmax/blob/3c8584924603869e90ca74ac20a6a03d99a91ef9/Categorical%20VAE.ipynb\n#\n# MIT License\n#\n# Copyright (c) 2016 Eric Jang\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport numpy as np\nfrom my_functionals.custom_functional import compute_grad_mag\n\ndef perturbate_input_(input, n_elements=200):\n    N, C, H, W = input.shape\n    assert N == 1\n    c_ = np.random.random_integers(0, C - 1, n_elements)\n    h_ = np.random.random_integers(0, H - 1, n_elements)\n    w_ = np.random.random_integers(0, W - 1, n_elements)\n    for c_idx in c_:\n        for h_idx in h_:\n            for w_idx in w_:\n                input[0, c_idx, h_idx, w_idx] = 1\n    return input\n\ndef _sample_gumbel(shape, eps=1e-10):\n    """"""\n    Sample from Gumbel(0, 1)\n\n    based on\n    https://github.com/ericjang/gumbel-softmax/blob/3c8584924603869e90ca74ac20a6a03d99a91ef9/Categorical%20VAE.ipynb ,\n    (MIT license)\n    """"""\n    U = torch.rand(shape).cuda()\n    return - torch.log(eps - torch.log(U + eps))\n\n\ndef _gumbel_softmax_sample(logits, tau=1, eps=1e-10):\n    """"""\n    Draw a sample from the Gumbel-Softmax distribution\n\n    based on\n    https://github.com/ericjang/gumbel-softmax/blob/3c8584924603869e90ca74ac20a6a03d99a91ef9/Categorical%20VAE.ipynb\n    (MIT license)\n    """"""\n    assert logits.dim() == 3\n    gumbel_noise = _sample_gumbel(logits.size(), eps=eps)\n    y = logits + gumbel_noise\n    return F.softmax(y / tau, 1)\n\n\ndef _one_hot_embedding(labels, num_classes):\n    """"""Embedding labels to one-hot form.\n\n    Args:\n      labels: (LongTensor) class labels, sized [N,].\n      num_classes: (int) number of classes.\n\n    Returns:\n      (tensor) encoded labels, sized [N, #classes].\n    """"""\n\n    y = torch.eye(num_classes).cuda()\n    return y[labels].permute(0,3,1,2)\n\nclass DualTaskLoss(nn.Module):\n    def __init__(self, cuda=False):\n        super(DualTaskLoss, self).__init__()\n        self._cuda = cuda\n        return\n\n    def forward(self, input_logits, gts, ignore_pixel=255):\n        """"""\n        :param input_logits: NxCxHxW\n        :param gt_semantic_masks: NxCxHxW\n        :return: final loss\n        """"""\n        N, C, H, W = input_logits.shape\n        th = 1e-8  # 1e-10\n        eps = 1e-10\n        ignore_mask = (gts == ignore_pixel).detach()\n        input_logits = torch.where(ignore_mask.view(N, 1, H, W).expand(N, 19, H, W),\n                                   torch.zeros(N,C,H,W).cuda(),\n                                   input_logits)\n        gt_semantic_masks = gts.detach()\n        gt_semantic_masks = torch.where(ignore_mask, torch.zeros(N,H,W).long().cuda(), gt_semantic_masks)\n        gt_semantic_masks = _one_hot_embedding(gt_semantic_masks, 19).detach()\n\n        g = _gumbel_softmax_sample(input_logits.view(N, C, -1), tau=0.5)\n        g = g.reshape((N, C, H, W))\n        g = compute_grad_mag(g, cuda=self._cuda)\n \n        g_hat = compute_grad_mag(gt_semantic_masks, cuda=self._cuda)\n\n        g = g.view(N, -1)\n        g_hat = g_hat.view(N, -1)\n        loss_ewise = F.l1_loss(g, g_hat, reduction=\'none\', reduce=False)\n\n        p_plus_g_mask = (g >= th).detach().float()\n        loss_p_plus_g = torch.sum(loss_ewise * p_plus_g_mask) / (torch.sum(p_plus_g_mask) + eps)\n\n        p_plus_g_hat_mask = (g_hat >= th).detach().float()\n        loss_p_plus_g_hat = torch.sum(loss_ewise * p_plus_g_hat_mask) / (torch.sum(p_plus_g_hat_mask) + eps)\n\n        total_loss = 0.5 * loss_p_plus_g + 0.5 * loss_p_plus_g_hat\n\n        return total_loss\n\n'"
my_functionals/GatedSpatialConv.py,14,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.modules.conv import _ConvNd\nfrom torch.nn.modules.utils import _pair\nimport numpy as np\nimport math\nimport network.mynn as mynn\nimport my_functionals.custom_functional as myF\nclass GatedSpatialConv2d(_ConvNd):\n    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,\n                 padding=0, dilation=1, groups=1, bias=False):\n        """"""\n\n        :param in_channels:\n        :param out_channels:\n        :param kernel_size:\n        :param stride:\n        :param padding:\n        :param dilation:\n        :param groups:\n        :param bias:\n        """"""\n\n        kernel_size = _pair(kernel_size)\n        stride = _pair(stride)\n        padding = _pair(padding)\n        dilation = _pair(dilation)\n        super(GatedSpatialConv2d, self).__init__(\n            in_channels, out_channels, kernel_size, stride, padding, dilation,\n            False, _pair(0), groups, bias, \'zeros\')\n\n        self._gate_conv = nn.Sequential(\n            mynn.Norm2d(in_channels+1),\n            nn.Conv2d(in_channels+1, in_channels+1, 1),\n            nn.ReLU(), \n            nn.Conv2d(in_channels+1, 1, 1),\n            mynn.Norm2d(1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input_features, gating_features):\n        """"""\n\n        :param input_features:  [NxCxHxW]  featuers comming from the shape branch (canny branch).\n        :param gating_features: [Nx1xHxW] features comming from the texture branch (resnet). Only one channel feature map.\n        :return:\n        """"""\n        alphas = self._gate_conv(torch.cat([input_features, gating_features], dim=1))\n\n        input_features = (input_features * (alphas + 1)) \n        return F.conv2d(input_features, self.weight, self.bias, self.stride,\n                        self.padding, self.dilation, self.groups)\n  \n    def reset_parameters(self):\n        nn.init.xavier_normal_(self.weight)\n        if self.bias is not None:\n            nn.init.zeros_(self.bias)\n\n\nclass Conv2dPad(nn.Conv2d):\n    def forward(self, input):\n        return myF.conv2d_same(input,self.weight,self.groups)\n\nclass HighFrequencyGatedSpatialConv2d(_ConvNd):\n    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,\n                 padding=0, dilation=1, groups=1, bias=False):\n        """"""\n\n        :param in_channels:\n        :param out_channels:\n        :param kernel_size:\n        :param stride:\n        :param padding:\n        :param dilation:\n        :param groups:\n        :param bias:\n        """"""\n\n        kernel_size = _pair(kernel_size)\n        stride = _pair(stride)\n        padding = _pair(padding)\n        dilation = _pair(dilation)\n        super(HighFrequencyGatedSpatialConv2d, self).__init__(\n            in_channels, out_channels, kernel_size, stride, padding, dilation,\n            False, _pair(0), groups, bias)\n\n        self._gate_conv = nn.Sequential(\n            mynn.Norm2d(in_channels+1),\n            nn.Conv2d(in_channels+1, in_channels+1, 1),\n            nn.ReLU(), \n            nn.Conv2d(in_channels+1, 1, 1),\n            mynn.Norm2d(1),\n            nn.Sigmoid()\n        )\n\n        kernel_size = 7\n        sigma = 3\n\n        x_cord = torch.arange(kernel_size).float()\n        x_grid = x_cord.repeat(kernel_size).view(kernel_size, kernel_size).float()\n        y_grid = x_grid.t().float()\n        xy_grid = torch.stack([x_grid, y_grid], dim=-1).float()\n\n        mean = (kernel_size - 1)/2.\n        variance = sigma**2.\n        gaussian_kernel = (1./(2.*math.pi*variance)) *\\\n                          torch.exp(\n                              -torch.sum((xy_grid - mean)**2., dim=-1) /\\\n                              (2*variance)\n                          )\n\n        gaussian_kernel = gaussian_kernel / torch.sum(gaussian_kernel)\n\n        gaussian_kernel = gaussian_kernel.view(1, 1, kernel_size, kernel_size)\n        gaussian_kernel = gaussian_kernel.repeat(in_channels, 1, 1, 1)\n\n        self.gaussian_filter = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, padding=3,\n                                         kernel_size=kernel_size, groups=in_channels, bias=False)\n\n        self.gaussian_filter.weight.data = gaussian_kernel\n        self.gaussian_filter.weight.requires_grad = False\n\n        self.cw = nn.Conv2d(in_channels * 2, in_channels, 1)\n \n        self.procdog = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels, 1),\n            mynn.Norm2d(in_channels),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input_features, gating_features):\n        """"""\n\n        :param input_features:  [NxCxHxW]  featuers comming from the shape branch (canny branch).\n        :param gating_features: [Nx1xHxW] features comming from the texture branch (resnet). Only one channel feature map.\n        :return:\n        """"""\n        n, c, h, w = input_features.size()\n        smooth_features = self.gaussian_filter(input_features)\n        dog_features = input_features - smooth_features\n        dog_features = self.cw(torch.cat((dog_features, input_features), dim=1))\n        \n        alphas = self._gate_conv(torch.cat([input_features, gating_features], dim=1))\n\n        dog_features = dog_features * (alphas + 1)\n\n        return F.conv2d(dog_features, self.weight, self.bias, self.stride,\n                        self.padding, self.dilation, self.groups)\n\n    def reset_parameters(self):\n        nn.init.xavier_normal_(self.weight)\n        if self.bias is not None:\n            nn.init.zeros_(self.bias)\n\ndef t():\n    import matplotlib.pyplot as plt\n\n    canny_map_filters_in = 8\n    canny_map = np.random.normal(size=(1, canny_map_filters_in, 10, 10))  # NxCxHxW\n    resnet_map = np.random.normal(size=(1, 1, 10, 10))  # NxCxHxW\n    plt.imshow(canny_map[0, 0])\n    plt.show()\n\n    canny_map = torch.from_numpy(canny_map).float()\n    resnet_map = torch.from_numpy(resnet_map).float()\n\n    gconv = GatedSpatialConv2d(canny_map_filters_in, canny_map_filters_in,\n                               kernel_size=3, stride=1, padding=1)\n    output_map = gconv(canny_map, resnet_map)\n    print(\'done\')\n\n\nif __name__ == ""__main__"":\n    t()\n\n'"
my_functionals/__init__.py,0,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n'"
my_functionals/custom_functional.py,16,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport torch\nimport torch.nn.functional as F\nfrom torchvision.transforms.functional import pad\nimport numpy as np\n\n\ndef calc_pad_same(in_siz, out_siz, stride, ksize):\n    """"""Calculate same padding width.\n    Args:\n    ksize: kernel size [I, J].\n    Returns:\n    pad_: Actual padding width.\n    """"""\n    return (out_siz - 1) * stride + ksize - in_siz\n\n\ndef conv2d_same(input, kernel, groups,bias=None,stride=1,padding=0,dilation=1):\n    n, c, h, w = input.shape\n    kout, ki_c_g, kh, kw = kernel.shape\n    pw = calc_pad_same(w, w, 1, kw)\n    ph = calc_pad_same(h, h, 1, kh)\n    pw_l = pw // 2\n    pw_r = pw - pw_l\n    ph_t = ph // 2\n    ph_b = ph - ph_t\n\n    input_ = F.pad(input, (pw_l, pw_r, ph_t, ph_b))\n    result = F.conv2d(input_, kernel, bias=bias, stride=stride, padding=padding, dilation=dilation, groups=groups)\n    assert result.shape == input.shape\n    return result\n\n\ndef gradient_central_diff(input, cuda):\n    return input, input\n    kernel = [[1, 0, -1]]\n    kernel_t = 0.5 * torch.Tensor(kernel) * -1.  # pytorch implements correlation instead of conv\n    if type(cuda) is int:\n        if cuda != -1:\n            kernel_t = kernel_t.cuda(device=cuda)\n    else:\n        if cuda is True:\n            kernel_t = kernel_t.cuda()\n    n, c, h, w = input.shape\n\n    x = conv2d_same(input, kernel_t.unsqueeze(0).unsqueeze(0).repeat([c, 1, 1, 1]), c)\n    y = conv2d_same(input, kernel_t.t().unsqueeze(0).unsqueeze(0).repeat([c, 1, 1, 1]), c)\n    return x, y\n\n\ndef compute_single_sided_diferences(o_x, o_y, input):\n    # n,c,h,w\n    #input = input.clone()\n    o_y[:, :, 0, :] = input[:, :, 1, :].clone() - input[:, :, 0, :].clone()\n    o_x[:, :, :, 0] = input[:, :, :, 1].clone() - input[:, :, :, 0].clone()\n    # --\n    o_y[:, :, -1, :] = input[:, :, -1, :].clone() - input[:, :, -2, :].clone()\n    o_x[:, :, :, -1] = input[:, :, :, -1].clone() - input[:, :, :, -2].clone()\n    return o_x, o_y\n\n\ndef numerical_gradients_2d(input, cuda=False):\n    """"""\n    numerical gradients implementation over batches using torch group conv operator.\n    the single sided differences are re-computed later.\n    it matches np.gradient(image) with the difference than here output=x,y for an image while there output=y,x\n    :param input: N,C,H,W\n    :param cuda: whether or not use cuda\n    :return: X,Y\n    """"""\n    n, c, h, w = input.shape\n    assert h > 1 and w > 1\n    x, y = gradient_central_diff(input, cuda)\n    return x, y\n\n\ndef convTri(input, r, cuda=False):\n    """"""\n    Convolves an image by a 2D triangle filter (the 1D triangle filter f is\n    [1:r r+1 r:-1:1]/(r+1)^2, the 2D version is simply conv2(f,f\'))\n    :param input:\n    :param r: integer filter radius\n    :param cuda: move the kernel to gpu\n    :return:\n    """"""\n    if (r <= 1):\n        raise ValueError()\n    n, c, h, w = input.shape\n    return input\n    f = list(range(1, r + 1)) + [r + 1] + list(reversed(range(1, r + 1)))\n    kernel = torch.Tensor([f]) / (r + 1) ** 2\n    if type(cuda) is int:\n        if cuda != -1:\n            kernel = kernel.cuda(device=cuda)\n    else:\n        if cuda is True:\n            kernel = kernel.cuda()\n\n    # padding w\n    input_ = F.pad(input, (1, 1, 0, 0), mode=\'replicate\')\n    input_ = F.pad(input_, (r, r, 0, 0), mode=\'reflect\')\n    input_ = [input_[:, :, :, :r], input, input_[:, :, :, -r:]]\n    input_ = torch.cat(input_, 3)\n    t = input_\n\n    # padding h\n    input_ = F.pad(input_, (0, 0, 1, 1), mode=\'replicate\')\n    input_ = F.pad(input_, (0, 0, r, r), mode=\'reflect\')\n    input_ = [input_[:, :, :r, :], t, input_[:, :, -r:, :]]\n    input_ = torch.cat(input_, 2)\n\n    output = F.conv2d(input_,\n                      kernel.unsqueeze(0).unsqueeze(0).repeat([c, 1, 1, 1]),\n                      padding=0, groups=c)\n    output = F.conv2d(output,\n                      kernel.t().unsqueeze(0).unsqueeze(0).repeat([c, 1, 1, 1]),\n                      padding=0, groups=c)\n    return output\n\n\ndef compute_normal(E, cuda=False):\n    if torch.sum(torch.isnan(E)) != 0:\n        print(\'nans found here\')\n        import ipdb;\n        ipdb.set_trace()\n    E_ = convTri(E, 4, cuda)\n    Ox, Oy = numerical_gradients_2d(E_, cuda)\n    Oxx, _ = numerical_gradients_2d(Ox, cuda)\n    Oxy, Oyy = numerical_gradients_2d(Oy, cuda)\n\n    aa = Oyy * torch.sign(-(Oxy + 1e-5)) / (Oxx + 1e-5)\n    t = torch.atan(aa)\n    O = torch.remainder(t, np.pi)\n\n    if torch.sum(torch.isnan(O)) != 0:\n        print(\'nans found here\')\n        import ipdb;\n        ipdb.set_trace()\n\n    return O\n\n\ndef compute_normal_2(E, cuda=False):\n    if torch.sum(torch.isnan(E)) != 0:\n        print(\'nans found here\')\n        import ipdb;\n        ipdb.set_trace()\n    E_ = convTri(E, 4, cuda)\n    Ox, Oy = numerical_gradients_2d(E_, cuda)\n    Oxx, _ = numerical_gradients_2d(Ox, cuda)\n    Oxy, Oyy = numerical_gradients_2d(Oy, cuda)\n\n    aa = Oyy * torch.sign(-(Oxy + 1e-5)) / (Oxx + 1e-5)\n    t = torch.atan(aa)\n    O = torch.remainder(t, np.pi)\n\n    if torch.sum(torch.isnan(O)) != 0:\n        print(\'nans found here\')\n        import ipdb;\n        ipdb.set_trace()\n\n    return O, (Oyy, Oxx)\n\n\ndef compute_grad_mag(E, cuda=False):\n    E_ = convTri(E, 4, cuda)\n    Ox, Oy = numerical_gradients_2d(E_, cuda)\n    mag = torch.sqrt(torch.mul(Ox,Ox) + torch.mul(Oy,Oy) + 1e-6)\n    mag = mag / mag.max();\n\n    return mag\n'"
network/Resnet.py,7,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n\n# Code Adapted from:\n# https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n#\n# BSD 3-Clause License\n#\n# Copyright (c) 2017,\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n# * Redistributions of source code must retain the above copyright notice, this\n#   list of conditions and the following disclaimer.\n#\n# * Redistributions in binary form must reproduce the above copyright notice,\n#   this list of conditions and the following disclaimer in the documentation\n#   and/or other materials provided with the distribution.\n#\n# * Neither the name of the copyright holder nor the names of its\n#   contributors may be used to endorse or promote products derived from\n#   this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n""""""\n\nimport torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\nimport  network.mynn as  mynn\n\n__all__ = [\'ResNet\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\',\n           \'resnet152\']\n\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = mynn.Norm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = mynn.Norm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = mynn.Norm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = mynn.Norm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = mynn.Norm2d(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = mynn.Norm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AvgPool2d(7, stride=1)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                mynn.Norm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef resnet18(pretrained=True, **kwargs):\n    """"""Constructs a ResNet-18 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet18\']))\n    return model\n\n\ndef resnet34(pretrained=True, **kwargs):\n    """"""Constructs a ResNet-34 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet34\']))\n    return model\n\n\ndef resnet50(pretrained=True, **kwargs):\n    """"""Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet50\']))\n    return model\n\n\ndef resnet101(pretrained=True, **kwargs):\n    """"""Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet101\']))\n    return model\n\n\ndef resnet152(pretrained=True, **kwargs):\n    """"""Constructs a ResNet-152 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet152\']))\n    return model\n'"
network/SEresnext.py,2,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n\n# Code adapted from:\n# https://github.com/Cadene/pretrained-models.pytorch\n#\n# BSD 3-Clause License\n#\n# Copyright (c) 2017, Remi Cadene\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n# * Redistributions of source code must retain the above copyright notice, this\n#   list of conditions and the following disclaimer.\n#\n# * Redistributions in binary form must reproduce the above copyright notice,\n#   this list of conditions and the following disclaimer in the documentation\n#   and/or other materials provided with the distribution.\n#\n# * Neither the name of the copyright holder nor the names of its\n#   contributors may be used to endorse or promote products derived from\n#   this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n""""""\n\n\nfrom collections import OrderedDict\nimport math\nimport  network.mynn as  mynn\nimport torch.nn as nn\nfrom torch.utils import model_zoo\n\n__all__ = [\'SENet\', \'senet154\', \'se_resnet50\', \'se_resnet101\', \'se_resnet152\',\n           \'se_resnext50_32x4d\', \'se_resnext101_32x4d\']\n\npretrained_settings = {\n    \'se_resnext50_32x4d\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext50_32x4d-a260b3a4.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnext101_32x4d\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext101_32x4d-3b2fe3d8.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n}\n\n\nclass SEModule(nn.Module):\n\n    def __init__(self, channels, reduction):\n        super(SEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1,\n                             padding=0)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1,\n                             padding=0)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        module_input = x\n        x = self.avg_pool(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return module_input * x\n\n\nclass Bottleneck(nn.Module):\n    """"""\n    Base class for bottlenecks that implements `forward()` method.\n    """"""\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = self.se_module(out) + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SEBottleneck(Bottleneck):\n    """"""\n    Bottleneck for SENet154.\n    """"""\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n        self.bn1 = mynn.Norm2d(planes * 2)\n        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3,\n                               stride=stride, padding=1, groups=groups,\n                               bias=False)\n        self.bn2 = mynn.Norm2d(planes * 4)\n        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1,\n                               bias=False)\n        self.bn3 = mynn.Norm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNetBottleneck(Bottleneck):\n    """"""\n    ResNet bottleneck with a Squeeze-and-Excitation module. It follows Caffe\n    implementation and uses `stride=stride` in `conv1` and not in `conv2`\n    (the latter is used in the torchvision implementation of ResNet).\n    """"""\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEResNetBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False,\n                               stride=stride)\n        self.bn1 = mynn.Norm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1,\n                               groups=groups, bias=False)\n        self.bn2 = mynn.Norm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = mynn.Norm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNeXtBottleneck(Bottleneck):\n    """"""\n    ResNeXt bottleneck type C with a Squeeze-and-Excitation module.\n    """"""\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None, base_width=4):\n        super(SEResNeXtBottleneck, self).__init__()\n        width = math.floor(planes * (base_width / 64)) * groups\n        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False,\n                               stride=1)\n        self.bn1 = mynn.Norm2d(width)\n        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n                               padding=1, groups=groups, bias=False)\n        self.bn2 = mynn.Norm2d(width)\n        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = mynn.Norm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SENet(nn.Module):\n\n    def __init__(self, block, layers, groups, reduction, dropout_p=0.2,\n                 inplanes=128, input_3x3=True, downsample_kernel_size=3,\n                 downsample_padding=1, num_classes=1000):\n        """"""\n        Parameters\n        ----------\n        block (nn.Module): Bottleneck class.\n            - For SENet154: SEBottleneck\n            - For SE-ResNet models: SEResNetBottleneck\n            - For SE-ResNeXt models:  SEResNeXtBottleneck\n        layers (list of ints): Number of residual blocks for 4 layers of the\n            network (layer1...layer4).\n        groups (int): Number of groups for the 3x3 convolution in each\n            bottleneck block.\n            - For SENet154: 64\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models:  32\n        reduction (int): Reduction ratio for Squeeze-and-Excitation modules.\n            - For all models: 16\n        dropout_p (float or None): Drop probability for the Dropout layer.\n            If `None` the Dropout layer is not used.\n            - For SENet154: 0.2\n            - For SE-ResNet models: None\n            - For SE-ResNeXt models: None\n        inplanes (int):  Number of input channels for layer1.\n            - For SENet154: 128\n            - For SE-ResNet models: 64\n            - For SE-ResNeXt models: 64\n        input_3x3 (bool): If `True`, use three 3x3 convolutions instead of\n            a single 7x7 convolution in layer0.\n            - For SENet154: True\n            - For SE-ResNet models: False\n            - For SE-ResNeXt models: False\n        downsample_kernel_size (int): Kernel size for downsampling convolutions\n            in layer2, layer3 and layer4.\n            - For SENet154: 3\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models: 1\n        downsample_padding (int): Padding for downsampling convolutions in\n            layer2, layer3 and layer4.\n            - For SENet154: 1\n            - For SE-ResNet models: 0\n            - For SE-ResNeXt models: 0\n        num_classes (int): Number of outputs in `last_linear` layer.\n            - For all models: 1000\n        """"""\n        super(SENet, self).__init__()\n        self.inplanes = inplanes\n        \n        if input_3x3:\n            layer0_modules = [\n                (\'conv1\', nn.Conv2d(3, 64, 3, stride=2, padding=1,\n                                    bias=False)),\n                (\'bn1\', mynn.Norm2d(64)),\n                (\'relu1\', nn.ReLU(inplace=True)),\n                (\'conv2\', nn.Conv2d(64, 64, 3, stride=1, padding=1,\n                                    bias=False)),\n                (\'bn2\', mynn.Norm2d(64)),\n                (\'relu2\', nn.ReLU(inplace=True)),\n                (\'conv3\', nn.Conv2d(64, inplanes, 3, stride=1, padding=1,\n                                    bias=False)),\n                (\'bn3\', mynn.Norm2d(inplanes)),\n                (\'relu3\', nn.ReLU(inplace=True)),\n            ]\n        else:\n            layer0_modules = [\n                (\'conv1\', nn.Conv2d(3, inplanes, kernel_size=7, stride=2,\n                                    padding=3, bias=False)),\n                (\'bn1\', mynn.Norm2d(inplanes)),\n                (\'relu1\', nn.ReLU(inplace=True)),\n            ]\n        # To preserve compatibility with Caffe weights `ceil_mode=True`\n        # is used instead of `padding=1`.\n        layer0_modules.append((\'pool\', nn.MaxPool2d(3, stride=2,\n                                                    ceil_mode=True)))\n        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n        self.layer1 = self._make_layer(\n            block,\n            planes=64,\n            blocks=layers[0],\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=1,\n            downsample_padding=0\n        )\n        self.layer2 = self._make_layer(\n            block,\n            planes=128,\n            blocks=layers[1],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer3 = self._make_layer(\n            block,\n            planes=256,\n            blocks=layers[2],\n            stride=1,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer4 = self._make_layer(\n            block,\n            planes=512,\n            blocks=layers[3],\n            stride=1,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.avg_pool = nn.AvgPool2d(7, stride=1)\n        self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n                    downsample_kernel_size=1, downsample_padding=0):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=downsample_kernel_size, stride=stride,\n                          padding=downsample_padding, bias=False),\n                mynn.Norm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, groups, reduction, stride,\n                            downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups, reduction))\n\n        return nn.Sequential(*layers)\n\n    def features(self, x):\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def logits(self, x):\n        x = self.avg_pool(x)\n        if self.dropout is not None:\n            x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.logits(x)\n        return x\n\n\ndef initialize_pretrained_model(model, num_classes, settings):\n    assert num_classes == settings[\'num_classes\'], \\\n        \'num_classes should be {}, but is {}\'.format(\n            settings[\'num_classes\'], num_classes)\n    weights = model_zoo.load_url(settings[\'url\'])\n    model.load_state_dict(weights)\n    model.input_space = settings[\'input_space\']\n    model.input_size = settings[\'input_size\']\n    model.input_range = settings[\'input_range\']\n    model.mean = settings[\'mean\']\n    model.std = settings[\'std\']\n\n\n\ndef se_resnext50_32x4d(num_classes=1000):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 6, 3], groups=32, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    settings = pretrained_settings[\'se_resnext50_32x4d\'][\'imagenet\']\n    initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnext101_32x4d(num_classes=1000):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 23, 3], groups=32, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    settings = pretrained_settings[\'se_resnext101_32x4d\'][\'imagenet\']\n    initialize_pretrained_model(model, num_classes, settings)\n    return model\n'"
network/__init__.py,1,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n"""""" \n\nimport importlib\nimport torch\nimport logging\n\ndef get_net(args, criterion):\n    net = get_model(network=args.arch, num_classes=args.dataset_cls.num_classes,\n                    criterion=criterion, trunk=args.trunk)\n    num_params = sum([param.nelement() for param in net.parameters()])\n    logging.info(\'Model params = {:2.1f}M\'.format(num_params / 1000000))\n\n    net = net.cuda()\n    net = torch.nn.DataParallel(net)\n    return net\n\n\ndef get_model(network, num_classes, criterion, trunk):\n    \n    module = network[:network.rfind(\'.\')]\n    model = network[network.rfind(\'.\')+1:]\n    mod = importlib.import_module(module)\n    net_func = getattr(mod, model)\n    net = net_func(num_classes=num_classes, trunk=trunk, criterion=criterion)\n    return net\n\n\n'"
network/gscnn.py,11,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n\n# Code Adapted from:\n# https://github.com/sthalles/deeplab_v3\n#\n# MIT License\n#\n# Copyright (c) 2018 Thalles Santos Silva\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n""""""\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom network import SEresnext\nfrom network import Resnet\nfrom network.wider_resnet import wider_resnet38_a2\nfrom config import cfg\nfrom network.mynn import initialize_weights, Norm2d\nfrom torch.autograd import Variable\n\nfrom my_functionals import GatedSpatialConv as gsc\n\nimport cv2\nimport numpy as np\n\nclass Crop(nn.Module):\n    def __init__(self, axis, offset):\n        super(Crop, self).__init__()\n        self.axis = axis\n        self.offset = offset\n\n    def forward(self, x, ref):\n        """"""\n\n        :param x: input layer\n        :param ref: reference usually data in\n        :return:\n        """"""\n        for axis in range(self.axis, x.dim()):\n            ref_size = ref.size(axis)\n            indices = torch.arange(self.offset, self.offset + ref_size).long()\n            indices = x.data.new().resize_(indices.size()).copy_(indices).long()\n            x = x.index_select(axis, Variable(indices))\n        return x\n\n\nclass MyIdentity(nn.Module):\n    def __init__(self, axis, offset):\n        super(MyIdentity, self).__init__()\n        self.axis = axis\n        self.offset = offset\n\n    def forward(self, x, ref):\n        """"""\n\n        :param x: input layer\n        :param ref: reference usually data in\n        :return:\n        """"""\n        return x\n\nclass SideOutputCrop(nn.Module):\n    """"""\n    This is the original implementation ConvTranspose2d (fixed) and crops\n    """"""\n\n    def __init__(self, num_output, kernel_sz=None, stride=None, upconv_pad=0, do_crops=True):\n        super(SideOutputCrop, self).__init__()\n        self._do_crops = do_crops\n        self.conv = nn.Conv2d(num_output, out_channels=1, kernel_size=1, stride=1, padding=0, bias=True)\n\n        if kernel_sz is not None:\n            self.upsample = True\n            self.upsampled = nn.ConvTranspose2d(1, out_channels=1, kernel_size=kernel_sz, stride=stride,\n                                                padding=upconv_pad,\n                                                bias=False)\n            ##doing crops\n            if self._do_crops:\n                self.crops = Crop(2, offset=kernel_sz // 4)\n            else:\n                self.crops = MyIdentity(None, None)\n        else:\n            self.upsample = False\n\n    def forward(self, res, reference=None):\n        side_output = self.conv(res)\n        if self.upsample:\n            side_output = self.upsampled(side_output)\n            side_output = self.crops(side_output, reference)\n\n        return side_output\n\n\nclass _AtrousSpatialPyramidPoolingModule(nn.Module):\n    \'\'\'\n    operations performed:\n      1x1 x depth\n      3x3 x depth dilation 6\n      3x3 x depth dilation 12\n      3x3 x depth dilation 18\n      image pooling\n      concatenate all together\n      Final 1x1 conv\n    \'\'\'\n\n    def __init__(self, in_dim, reduction_dim=256, output_stride=16, rates=[6, 12, 18]):\n        super(_AtrousSpatialPyramidPoolingModule, self).__init__()\n\n        # Check if we are using distributed BN and use the nn from encoding.nn\n        # library rather than using standard pytorch.nn\n\n        if output_stride == 8:\n            rates = [2 * r for r in rates]\n        elif output_stride == 16:\n            pass\n        else:\n            raise \'output stride of {} not supported\'.format(output_stride)\n\n        self.features = []\n        # 1x1\n        self.features.append(\n            nn.Sequential(nn.Conv2d(in_dim, reduction_dim, kernel_size=1, bias=False),\n                          Norm2d(reduction_dim), nn.ReLU(inplace=True)))\n        # other rates\n        for r in rates:\n            self.features.append(nn.Sequential(\n                nn.Conv2d(in_dim, reduction_dim, kernel_size=3,\n                          dilation=r, padding=r, bias=False),\n                Norm2d(reduction_dim),\n                nn.ReLU(inplace=True)\n            ))\n        self.features = torch.nn.ModuleList(self.features)\n\n        # img level features\n        self.img_pooling = nn.AdaptiveAvgPool2d(1)\n        self.img_conv = nn.Sequential(\n            nn.Conv2d(in_dim, reduction_dim, kernel_size=1, bias=False),\n            Norm2d(reduction_dim), nn.ReLU(inplace=True))\n        self.edge_conv = nn.Sequential(\n            nn.Conv2d(1, reduction_dim, kernel_size=1, bias=False),\n            Norm2d(reduction_dim), nn.ReLU(inplace=True))\n         \n\n    def forward(self, x, edge):\n        x_size = x.size()\n\n        img_features = self.img_pooling(x)\n        img_features = self.img_conv(img_features)\n        img_features = F.interpolate(img_features, x_size[2:],\n                                     mode=\'bilinear\',align_corners=True)\n        out = img_features\n\n        edge_features = F.interpolate(edge, x_size[2:],\n                                      mode=\'bilinear\',align_corners=True)\n        edge_features = self.edge_conv(edge_features)\n        out = torch.cat((out, edge_features), 1)\n\n        for f in self.features:\n            y = f(x)\n            out = torch.cat((out, y), 1)\n        return out\n\nclass GSCNN(nn.Module):\n    \'\'\'\n    Wide_resnet version of DeepLabV3\n    mod1\n    pool2\n    mod2 str2\n    pool3\n    mod3-7\n\n      structure: [3, 3, 6, 3, 1, 1]\n      channels = [(128, 128), (256, 256), (512, 512), (512, 1024), (512, 1024, 2048),\n                  (1024, 2048, 4096)]\n    \'\'\'\n\n    def __init__(self, num_classes, trunk=None, criterion=None):\n        \n        super(GSCNN, self).__init__()\n        self.criterion = criterion\n        self.num_classes = num_classes\n\n        wide_resnet = wider_resnet38_a2(classes=1000, dilation=True)\n        wide_resnet = torch.nn.DataParallel(wide_resnet)\n        \n        wide_resnet = wide_resnet.module\n        self.mod1 = wide_resnet.mod1\n        self.mod2 = wide_resnet.mod2\n        self.mod3 = wide_resnet.mod3\n        self.mod4 = wide_resnet.mod4\n        self.mod5 = wide_resnet.mod5\n        self.mod6 = wide_resnet.mod6\n        self.mod7 = wide_resnet.mod7\n        self.pool2 = wide_resnet.pool2\n        self.pool3 = wide_resnet.pool3\n        self.interpolate = F.interpolate\n        del wide_resnet\n\n        self.dsn1 = nn.Conv2d(64, 1, 1)\n        self.dsn3 = nn.Conv2d(256, 1, 1)\n        self.dsn4 = nn.Conv2d(512, 1, 1)\n        self.dsn7 = nn.Conv2d(4096, 1, 1)\n\n        self.res1 = Resnet.BasicBlock(64, 64, stride=1, downsample=None)\n        self.d1 = nn.Conv2d(64, 32, 1)\n        self.res2 = Resnet.BasicBlock(32, 32, stride=1, downsample=None)\n        self.d2 = nn.Conv2d(32, 16, 1)\n        self.res3 = Resnet.BasicBlock(16, 16, stride=1, downsample=None)\n        self.d3 = nn.Conv2d(16, 8, 1)\n        self.fuse = nn.Conv2d(8, 1, kernel_size=1, padding=0, bias=False)\n\n        self.cw = nn.Conv2d(2, 1, kernel_size=1, padding=0, bias=False)\n\n        self.gate1 = gsc.GatedSpatialConv2d(32, 32)\n        self.gate2 = gsc.GatedSpatialConv2d(16, 16)\n        self.gate3 = gsc.GatedSpatialConv2d(8, 8)\n         \n        self.aspp = _AtrousSpatialPyramidPoolingModule(4096, 256,\n                                                       output_stride=8)\n\n        self.bot_fine = nn.Conv2d(128, 48, kernel_size=1, bias=False)\n        self.bot_aspp = nn.Conv2d(1280 + 256, 256, kernel_size=1, bias=False)\n\n        self.final_seg = nn.Sequential(\n            nn.Conv2d(256 + 48, 256, kernel_size=3, padding=1, bias=False),\n            Norm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),\n            Norm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, num_classes, kernel_size=1, bias=False))\n\n        self.sigmoid = nn.Sigmoid()\n        initialize_weights(self.final_seg)\n\n    def forward(self, inp, gts=None):\n\n        x_size = inp.size() \n\n        # res 1\n        m1 = self.mod1(inp)\n\n        # res 2\n        m2 = self.mod2(self.pool2(m1))\n\n        # res 3\n        m3 = self.mod3(self.pool3(m2))\n\n        # res 4-7\n        m4 = self.mod4(m3)\n        m5 = self.mod5(m4)\n        m6 = self.mod6(m5)\n        m7 = self.mod7(m6) \n\n        s3 = F.interpolate(self.dsn3(m3), x_size[2:],\n                            mode=\'bilinear\', align_corners=True)\n        s4 = F.interpolate(self.dsn4(m4), x_size[2:],\n                            mode=\'bilinear\', align_corners=True)\n        s7 = F.interpolate(self.dsn7(m7), x_size[2:],\n                            mode=\'bilinear\', align_corners=True)\n        \n        m1f = F.interpolate(m1, x_size[2:], mode=\'bilinear\', align_corners=True)\n\n        im_arr = inp.cpu().numpy().transpose((0,2,3,1)).astype(np.uint8)\n        canny = np.zeros((x_size[0], 1, x_size[2], x_size[3]))\n        for i in range(x_size[0]):\n            canny[i] = cv2.Canny(im_arr[i],10,100)\n        canny = torch.from_numpy(canny).cuda().float()\n\n        cs = self.res1(m1f)\n        cs = F.interpolate(cs, x_size[2:],\n                           mode=\'bilinear\', align_corners=True)\n        cs = self.d1(cs)\n        cs = self.gate1(cs, s3)\n        cs = self.res2(cs)\n        cs = F.interpolate(cs, x_size[2:],\n                           mode=\'bilinear\', align_corners=True)\n        cs = self.d2(cs)\n        cs = self.gate2(cs, s4)\n        cs = self.res3(cs)\n        cs = F.interpolate(cs, x_size[2:],\n                           mode=\'bilinear\', align_corners=True)\n        cs = self.d3(cs)\n        cs = self.gate3(cs, s7)\n        cs = self.fuse(cs)\n        cs = F.interpolate(cs, x_size[2:],\n                           mode=\'bilinear\', align_corners=True)\n        edge_out = self.sigmoid(cs)\n        cat = torch.cat((edge_out, canny), dim=1)\n        acts = self.cw(cat)\n        acts = self.sigmoid(acts)\n\n        # aspp\n        x = self.aspp(m7, acts)\n        dec0_up = self.bot_aspp(x)\n\n        dec0_fine = self.bot_fine(m2)\n        dec0_up = self.interpolate(dec0_up, m2.size()[2:], mode=\'bilinear\',align_corners=True)\n        dec0 = [dec0_fine, dec0_up]\n        dec0 = torch.cat(dec0, 1)\n\n        dec1 = self.final_seg(dec0)  \n        seg_out = self.interpolate(dec1, x_size[2:], mode=\'bilinear\')            \n       \n        if self.training:\n            return self.criterion((seg_out, edge_out), gts)              \n        else:\n            return seg_out, edge_out\n\n'"
network/mynn.py,4,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nfrom config import cfg\nimport torch.nn as nn\nfrom math import sqrt\nimport torch\nfrom torch.autograd.function import InplaceFunction\nfrom itertools import repeat\nfrom torch.nn.modules import Module\nfrom  torch.utils.checkpoint  import checkpoint\n\n\ndef Norm2d(in_channels):\n    """"""\n    Custom Norm Function to allow flexible switching\n    """"""\n    layer = getattr(cfg.MODEL,\'BNFUNC\')\n    normalizationLayer = layer(in_channels)\n    return normalizationLayer\n\n\ndef initialize_weights(*models):\n   for model in models:\n        for module in model.modules():\n            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n                nn.init.kaiming_normal(module.weight)\n                if module.bias is not None:\n                    module.bias.data.zero_()\n            elif isinstance(module, nn.BatchNorm2d):\n                module.weight.data.fill_(1)\n                module.bias.data.zero_()\n'"
network/wider_resnet.py,4,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n\n# Code adapted from:\n# https://github.com/mapillary/inplace_abn/\n#\n# BSD 3-Clause License\n#\n# Copyright (c) 2017, mapillary\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n# * Redistributions of source code must retain the above copyright notice, this\n#   list of conditions and the following disclaimer.\n#\n# * Redistributions in binary form must reproduce the above copyright notice,\n#   this list of conditions and the following disclaimer in the documentation\n#   and/or other materials provided with the distribution.\n#\n# * Neither the name of the copyright holder nor the names of its\n#   contributors may be used to endorse or promote products derived from\n#   this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n""""""\n\nimport sys\nfrom collections import OrderedDict\nfrom functools import partial\nimport torch.nn as nn\nimport torch\nimport network.mynn as mynn\n\ndef bnrelu(channels):\n    return nn.Sequential(mynn.Norm2d(channels),\n                         nn.ReLU(inplace=True))\n\nclass GlobalAvgPool2d(nn.Module):\n\n    def __init__(self):\n        """"""Global average pooling over the input\'s spatial dimensions""""""\n        super(GlobalAvgPool2d, self).__init__()\n\n    def forward(self, inputs):\n        in_size = inputs.size()\n        return inputs.view((in_size[0], in_size[1], -1)).mean(dim=2)\n\n\nclass IdentityResidualBlock(nn.Module):\n\n    def __init__(self,\n                 in_channels,\n                 channels,\n                 stride=1,\n                 dilation=1,\n                 groups=1,\n                 norm_act=bnrelu,\n                 dropout=None,\n                 dist_bn=False\n                 ):\n        """"""Configurable identity-mapping residual block\n\n        Parameters\n        ----------\n        in_channels : int\n            Number of input channels.\n        channels : list of int\n            Number of channels in the internal feature maps.\n            Can either have two or three elements: if three construct\n            a residual block with two `3 x 3` convolutions,\n            otherwise construct a bottleneck block with `1 x 1`, then\n            `3 x 3` then `1 x 1` convolutions.\n        stride : int\n            Stride of the first `3 x 3` convolution\n        dilation : int\n            Dilation to apply to the `3 x 3` convolutions.\n        groups : int\n            Number of convolution groups.\n            This is used to create ResNeXt-style blocks and is only compatible with\n            bottleneck blocks.\n        norm_act : callable\n            Function to create normalization / activation Module.\n        dropout: callable\n            Function to create Dropout Module.\n        dist_bn: Boolean\n            A variable to enable or disable use of distributed BN\n        """"""\n        super(IdentityResidualBlock, self).__init__()\n        self.dist_bn = dist_bn\n\n        # Check if we are using distributed BN and use the nn from encoding.nn\n        # library rather than using standard pytorch.nn\n\n\n        # Check parameters for inconsistencies\n        if len(channels) != 2 and len(channels) != 3:\n            raise ValueError(""channels must contain either two or three values"")\n        if len(channels) == 2 and groups != 1:\n            raise ValueError(""groups > 1 are only valid if len(channels) == 3"")\n\n        is_bottleneck = len(channels) == 3\n        need_proj_conv = stride != 1 or in_channels != channels[-1]\n\n        self.bn1 = norm_act(in_channels)\n        if not is_bottleneck:\n            layers = [\n                (""conv1"", nn.Conv2d(in_channels,\n                                    channels[0],\n                                    3,\n                                    stride=stride,\n                                    padding=dilation,\n                                    bias=False,\n                                    dilation=dilation)),\n                (""bn2"", norm_act(channels[0])),\n                (""conv2"", nn.Conv2d(channels[0], channels[1],\n                                    3,\n                                    stride=1,\n                                    padding=dilation,\n                                    bias=False,\n                                    dilation=dilation))\n            ]\n            if dropout is not None:\n                layers = layers[0:2] + [(""dropout"", dropout())] + layers[2:]\n        else:\n            layers = [\n                (""conv1"",\n                 nn.Conv2d(in_channels,\n                           channels[0],\n                           1,\n                           stride=stride,\n                           padding=0,\n                           bias=False)),\n                (""bn2"", norm_act(channels[0])),\n                (""conv2"", nn.Conv2d(channels[0],\n                                    channels[1],\n                                    3, stride=1,\n                                    padding=dilation, bias=False,\n                                    groups=groups,\n                                    dilation=dilation)),\n                (""bn3"", norm_act(channels[1])),\n                (""conv3"", nn.Conv2d(channels[1], channels[2],\n                                    1, stride=1, padding=0, bias=False))\n            ]\n            if dropout is not None:\n                layers = layers[0:4] + [(""dropout"", dropout())] + layers[4:]\n        self.convs = nn.Sequential(OrderedDict(layers))\n\n        if need_proj_conv:\n            self.proj_conv = nn.Conv2d(\n                in_channels, channels[-1], 1, stride=stride, padding=0, bias=False)\n\n    def forward(self, x):\n        """"""\n        This is the standard forward function for non-distributed batch norm\n        """"""\n        if hasattr(self, ""proj_conv""):\n            bn1 = self.bn1(x)\n            shortcut = self.proj_conv(bn1)\n        else:\n            shortcut = x.clone()\n            bn1 = self.bn1(x)\n\n        out = self.convs(bn1)\n        out.add_(shortcut)\n        return out\n\n\n\n\nclass WiderResNet(nn.Module):\n\n    def __init__(self,\n                 structure,\n                 norm_act=bnrelu,\n                 classes=0\n                 ):\n        """"""Wider ResNet with pre-activation (identity mapping) blocks\n\n        Parameters\n        ----------\n        structure : list of int\n            Number of residual blocks in each of the six modules of the network.\n        norm_act : callable\n            Function to create normalization / activation Module.\n        classes : int\n            If not `0` also include global average pooling and \\\n            a fully-connected layer with `classes` outputs at the end\n            of the network.\n        """"""\n        super(WiderResNet, self).__init__()\n        self.structure = structure\n\n        if len(structure) != 6:\n            raise ValueError(""Expected a structure with six values"")\n\n        # Initial layers\n        self.mod1 = nn.Sequential(OrderedDict([\n            (""conv1"", nn.Conv2d(3, 64, 3, stride=1, padding=1, bias=False))\n        ]))\n\n        # Groups of residual blocks\n        in_channels = 64\n        channels = [(128, 128), (256, 256), (512, 512), (512, 1024),\n                    (512, 1024, 2048), (1024, 2048, 4096)]\n        for mod_id, num in enumerate(structure):\n            # Create blocks for module\n            blocks = []\n            for block_id in range(num):\n                blocks.append((\n                    ""block%d"" % (block_id + 1),\n                    IdentityResidualBlock(in_channels, channels[mod_id],\n                                          norm_act=norm_act)\n                ))\n\n                # Update channels and p_keep\n                in_channels = channels[mod_id][-1]\n\n            # Create module\n            if mod_id <= 4:\n                self.add_module(""pool%d"" %\n                                (mod_id + 2), nn.MaxPool2d(3, stride=2, padding=1))\n            self.add_module(""mod%d"" % (mod_id + 2), nn.Sequential(OrderedDict(blocks)))\n\n        # Pooling and predictor\n        self.bn_out = norm_act(in_channels)\n        if classes != 0:\n            self.classifier = nn.Sequential(OrderedDict([\n                (""avg_pool"", GlobalAvgPool2d()),\n                (""fc"", nn.Linear(in_channels, classes))\n            ]))\n\n    def forward(self, img):\n        out = self.mod1(img)\n        out = self.mod2(self.pool2(out))\n        out = self.mod3(self.pool3(out))\n        out = self.mod4(self.pool4(out))\n        out = self.mod5(self.pool5(out))\n        out = self.mod6(self.pool6(out))\n        out = self.mod7(out)\n        out = self.bn_out(out)\n\n        if hasattr(self, ""classifier""):\n            out = self.classifier(out)\n\n        return out\n\n\nclass WiderResNetA2(nn.Module):\n\n    def __init__(self,\n                 structure,\n                 norm_act=bnrelu,\n                 classes=0,\n                 dilation=False,\n                 dist_bn=False\n                 ):\n        """"""Wider ResNet with pre-activation (identity mapping) blocks\n\n        This variant uses down-sampling by max-pooling in the first two blocks and \\\n         by strided convolution in the others.\n\n        Parameters\n        ----------\n        structure : list of int\n            Number of residual blocks in each of the six modules of the network.\n        norm_act : callable\n            Function to create normalization / activation Module.\n        classes : int\n            If not `0` also include global average pooling and a fully-connected layer\n            \\with `classes` outputs at the end\n            of the network.\n        dilation : bool\n            If `True` apply dilation to the last three modules and change the\n            \\down-sampling factor from 32 to 8.\n        """"""\n        super(WiderResNetA2, self).__init__()\n        self.dist_bn = dist_bn\n\n        # If using distributed batch norm, use the encoding.nn as oppose to torch.nn\n\n\n        nn.Dropout = nn.Dropout2d\n        norm_act = bnrelu\n        self.structure = structure\n        self.dilation = dilation\n\n        if len(structure) != 6:\n            raise ValueError(""Expected a structure with six values"")\n\n        # Initial layers\n        self.mod1 = torch.nn.Sequential(OrderedDict([\n            (""conv1"", nn.Conv2d(3, 64, 3, stride=1, padding=1, bias=False))\n        ]))\n\n        # Groups of residual blocks\n        in_channels = 64\n        channels = [(128, 128), (256, 256), (512, 512), (512, 1024), (512, 1024, 2048),\n                    (1024, 2048, 4096)]\n        for mod_id, num in enumerate(structure):\n            # Create blocks for module\n            blocks = []\n            for block_id in range(num):\n                if not dilation:\n                    dil = 1\n                    stride = 2 if block_id == 0 and 2 <= mod_id <= 4 else 1\n                else:\n                    if mod_id == 3:\n                        dil = 2\n                    elif mod_id > 3:\n                        dil = 4\n                    else:\n                        dil = 1\n                    stride = 2 if block_id == 0 and mod_id == 2 else 1\n\n                if mod_id == 4:\n                    drop = partial(nn.Dropout, p=0.3)\n                elif mod_id == 5:\n                    drop = partial(nn.Dropout, p=0.5)\n                else:\n                    drop = None\n\n                blocks.append((\n                    ""block%d"" % (block_id + 1),\n                    IdentityResidualBlock(in_channels,\n                                          channels[mod_id], norm_act=norm_act,\n                                          stride=stride, dilation=dil,\n                                          dropout=drop, dist_bn=self.dist_bn)\n                ))\n\n                # Update channels and p_keep\n                in_channels = channels[mod_id][-1]\n\n            # Create module\n            if mod_id < 2:\n                self.add_module(""pool%d"" %\n                                (mod_id + 2), nn.MaxPool2d(3, stride=2, padding=1))\n            self.add_module(""mod%d"" % (mod_id + 2), nn.Sequential(OrderedDict(blocks)))\n\n        # Pooling and predictor\n        self.bn_out = norm_act(in_channels)\n        if classes != 0:\n            self.classifier = nn.Sequential(OrderedDict([\n                (""avg_pool"", GlobalAvgPool2d()),\n                (""fc"", nn.Linear(in_channels, classes))\n            ]))\n\n    def forward(self, img):\n        out = self.mod1(img)\n        out = self.mod2(self.pool2(out))\n        out = self.mod3(self.pool3(out))\n        out = self.mod4(out)\n        out = self.mod5(out)\n        out = self.mod6(out)\n        out = self.mod7(out)\n        out = self.bn_out(out)\n\n        if hasattr(self, ""classifier""):\n            return self.classifier(out)\n        else:\n            return out\n\n\n_NETS = {\n    ""16"": {""structure"": [1, 1, 1, 1, 1, 1]},\n    ""20"": {""structure"": [1, 1, 1, 3, 1, 1]},\n    ""38"": {""structure"": [3, 3, 6, 3, 1, 1]},\n}\n\n__all__ = []\nfor name, params in _NETS.items():\n    net_name = ""wider_resnet"" + name\n    setattr(sys.modules[__name__], net_name, partial(WiderResNet, **params))\n    __all__.append(net_name)\nfor name, params in _NETS.items():\n    net_name = ""wider_resnet"" + name + ""_a2""\n    setattr(sys.modules[__name__], net_name, partial(WiderResNetA2, **params))\n    __all__.append(net_name)\n'"
transforms/joint_transforms.py,0,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n\n# Code borrowded from:\n# https://github.com/zijundeng/pytorch-semantic-segmentation/blob/master/utils/joint_transforms.py\n#\n#\n# MIT License\n#\n# Copyright (c) 2017 ZijunDeng\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n""""""\n\nimport math\nimport numbers\nimport random\nfrom PIL import Image, ImageOps\nimport numpy as np\nimport random\n\nclass Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n        for t in self.transforms:\n            img, mask = t(img, mask)\n        return img, mask\n\n\nclass RandomCrop(object):\n    \'\'\'\n    Take a random crop from the image.\n\n    First the image or crop size may need to be adjusted if the incoming image\n    is too small...\n\n    If the image is smaller than the crop, then:\n         the image is padded up to the size of the crop\n         unless \'nopad\', in which case the crop size is shrunk to fit the image\n\n    A random crop is taken such that the crop fits within the image.\n    If a centroid is passed in, the crop must intersect the centroid.\n    \'\'\'\n    def __init__(self, size, ignore_index=0, nopad=True):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n        self.ignore_index = ignore_index\n        self.nopad = nopad\n        self.pad_color = (0, 0, 0)\n\n    def __call__(self, img, mask, centroid=None):\n        assert img.size == mask.size\n        w, h = img.size\n        # ASSUME H, W\n        th, tw = self.size\n        if w == tw and h == th:\n            return img, mask\n\n        if self.nopad:\n            if th > h or tw > w:\n                # Instead of padding, adjust crop size to the shorter edge of image.\n                shorter_side = min(w, h)\n                th, tw = shorter_side, shorter_side\n        else:\n            # Check if we need to pad img to fit for crop_size.\n            if th > h:\n                pad_h = (th - h) // 2 + 1\n            else:\n                pad_h = 0\n            if tw > w:\n                pad_w = (tw - w) // 2 + 1\n            else:\n                pad_w = 0\n            border = (pad_w, pad_h, pad_w, pad_h)\n            if pad_h or pad_w:\n                img = ImageOps.expand(img, border=border, fill=self.pad_color)\n                mask = ImageOps.expand(mask, border=border, fill=self.ignore_index)\n                w, h = img.size\n\n        if centroid is not None:\n            # Need to insure that centroid is covered by crop and that crop\n            # sits fully within the image\n            c_x, c_y = centroid\n            max_x = w - tw\n            max_y = h - th\n            x1 = random.randint(c_x - tw, c_x)\n            x1 = min(max_x, max(0, x1))\n            y1 = random.randint(c_y - th, c_y)\n            y1 = min(max_y, max(0, y1))\n        else:\n            if w == tw:\n                x1 = 0\n            else:\n                x1 = random.randint(0, w - tw)\n            if h == th:\n                y1 = 0\n            else:\n                y1 = random.randint(0, h - th)\n        return img.crop((x1, y1, x1 + tw, y1 + th)), mask.crop((x1, y1, x1 + tw, y1 + th))\n\n\nclass ResizeHeight(object):\n    def __init__(self, size, interpolation=Image.BICUBIC):\n        self.target_h = size\n        self.interpolation = interpolation\n\n    def __call__(self, img, mask):\n        w, h = img.size\n        target_w = int(w / h * self.target_h)\n        return (img.resize((target_w, self.target_h), self.interpolation),\n                mask.resize((target_w, self.target_h), Image.NEAREST))\n\n\nclass CenterCrop(object):\n    def __init__(self, size):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n        w, h = img.size\n        th, tw = self.size\n        x1 = int(round((w - tw) / 2.))\n        y1 = int(round((h - th) / 2.))\n        return img.crop((x1, y1, x1 + tw, y1 + th)), mask.crop((x1, y1, x1 + tw, y1 + th))\n\n\nclass CenterCropPad(object):\n    def __init__(self, size, ignore_index=0):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n        self.ignore_index = ignore_index\n\n    def __call__(self, img, mask):\n        \n        assert img.size == mask.size\n        w, h = img.size\n        if isinstance(self.size, tuple):\n                tw, th = self.size[0], self.size[1]\n        else:\n                th, tw = self.size, self.size\n\t\n\n        if w < tw:\n            pad_x = tw - w\n        else:\n            pad_x = 0\n        if h < th:\n            pad_y = th - h\n        else:\n            pad_y = 0\n\n        if pad_x or pad_y:\n            # left, top, right, bottom\n            img = ImageOps.expand(img, border=(pad_x, pad_y, pad_x, pad_y), fill=0)\n            mask = ImageOps.expand(mask, border=(pad_x, pad_y, pad_x, pad_y),\n                                   fill=self.ignore_index)\n\n        x1 = int(round((w - tw) / 2.))\n        y1 = int(round((h - th) / 2.))\n        return img.crop((x1, y1, x1 + tw, y1 + th)), mask.crop((x1, y1, x1 + tw, y1 + th))\n\n\n\nclass PadImage(object):\n    def __init__(self, size, ignore_index):\n        self.size = size\n        self.ignore_index = ignore_index\n\n        \n    def __call__(self, img, mask):\n        assert img.size == mask.size\n        th, tw = self.size, self.size\n\n        \n        w, h = img.size\n        \n        if w > tw or h > th :\n            wpercent = (tw/float(w))    \n            target_h = int((float(img.size[1])*float(wpercent)))\n            img, mask = img.resize((tw, target_h), Image.BICUBIC), mask.resize((tw, target_h), Image.NEAREST)\n\n        w, h = img.size\n        ##Pad\n        img = ImageOps.expand(img, border=(0,0,tw-w, th-h), fill=0)\n        mask = ImageOps.expand(mask, border=(0,0,tw-w, th-h), fill=self.ignore_index)\n        \n        return img, mask\n\nclass RandomHorizontallyFlip(object):\n    def __call__(self, img, mask):\n        if random.random() < 0.5:\n            return img.transpose(Image.FLIP_LEFT_RIGHT), mask.transpose(\n                Image.FLIP_LEFT_RIGHT)\n        return img, mask\n\n\nclass FreeScale(object):\n    def __init__(self, size):\n        self.size = tuple(reversed(size))  # size: (h, w)\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n        return img.resize(self.size, Image.BICUBIC), mask.resize(self.size, Image.NEAREST)\n\n\nclass Scale(object):\n    \'\'\'\n    Scale image such that longer side is == size\n    \'\'\'\n\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n        w, h = img.size\n        if (w >= h and w == self.size) or (h >= w and h == self.size):\n            return img, mask\n        if w > h:\n            ow = self.size\n            oh = int(self.size * h / w)\n            return img.resize((ow, oh), Image.BICUBIC), mask.resize(\n                (ow, oh), Image.NEAREST)\n        else:\n            oh = self.size\n            ow = int(self.size * w / h)\n            return img.resize((ow, oh), Image.BICUBIC), mask.resize(\n                (ow, oh), Image.NEAREST)\n\n\nclass ScaleMin(object):\n    \'\'\'\n    Scale image such that shorter side is == size\n    \'\'\'\n\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n        w, h = img.size\n        if (w <= h and w == self.size) or (h <= w and h == self.size):\n            return img, mask\n        if w < h:\n            ow = self.size\n            oh = int(self.size * h / w)\n            return img.resize((ow, oh), Image.BICUBIC), mask.resize(\n                (ow, oh), Image.NEAREST)\n        else:\n            oh = self.size\n            ow = int(self.size * w / h)\n            return img.resize((ow, oh), Image.BICUBIC), mask.resize(\n                (ow, oh), Image.NEAREST)\n\n\nclass Resize(object):\n    \'\'\'\n    Resize image to exact size of crop\n    \'\'\'\n\n    def __init__(self, size):\n        self.size = (size, size)\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n        w, h = img.size\n        if (w == h and w == self.size):\n            return img, mask\n        return (img.resize(self.size, Image.BICUBIC),\n                mask.resize(self.size, Image.NEAREST))\n\n\nclass RandomSizedCrop(object):\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n        for attempt in range(10):\n            area = img.size[0] * img.size[1]\n            target_area = random.uniform(0.45, 1.0) * area\n            aspect_ratio = random.uniform(0.5, 2)\n\n            w = int(round(math.sqrt(target_area * aspect_ratio)))\n            h = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if random.random() < 0.5:\n                w, h = h, w\n\n            if w <= img.size[0] and h <= img.size[1]:\n                x1 = random.randint(0, img.size[0] - w)\n                y1 = random.randint(0, img.size[1] - h)\n\n                img = img.crop((x1, y1, x1 + w, y1 + h))\n                mask = mask.crop((x1, y1, x1 + w, y1 + h))\n                assert (img.size == (w, h))\n\n                return img.resize((self.size, self.size), Image.BICUBIC),\\\n                    mask.resize((self.size, self.size), Image.NEAREST)\n\n        # Fallback\n        scale = Scale(self.size)\n        crop = CenterCrop(self.size)\n        return crop(*scale(img, mask))\n\n\nclass RandomRotate(object):\n    def __init__(self, degree):\n        self.degree = degree\n\n    def __call__(self, img, mask):\n        rotate_degree = random.random() * 2 * self.degree - self.degree\n        return img.rotate(rotate_degree, Image.BICUBIC), mask.rotate(\n            rotate_degree, Image.NEAREST)\n\n\nclass RandomSizeAndCrop(object):\n    def __init__(self, size, crop_nopad,\n                 scale_min=0.5, scale_max=2.0, ignore_index=0, pre_size=None):\n        self.size = size\n        self.crop = RandomCrop(self.size, ignore_index=ignore_index, nopad=crop_nopad)\n        self.scale_min = scale_min\n        self.scale_max = scale_max\n        self.pre_size = pre_size\n\n    def __call__(self, img, mask, centroid=None):\n        assert img.size == mask.size\n\n        # first, resize such that shorter edge is pre_size\n        if self.pre_size is None:\n            scale_amt = 1.\n        elif img.size[1] < img.size[0]:\n            scale_amt = self.pre_size / img.size[1]\n        else:\n            scale_amt = self.pre_size / img.size[0]\n        scale_amt *= random.uniform(self.scale_min, self.scale_max)\n        w, h = [int(i * scale_amt) for i in img.size]\n\n        if centroid is not None:\n            centroid = [int(c * scale_amt) for c in centroid]\n\n        img, mask = img.resize((w, h), Image.BICUBIC), mask.resize((w, h), Image.NEAREST)\n\n        return self.crop(img, mask, centroid)\n\n\nclass SlidingCropOld(object):\n    def __init__(self, crop_size, stride_rate, ignore_label):\n        self.crop_size = crop_size\n        self.stride_rate = stride_rate\n        self.ignore_label = ignore_label\n\n    def _pad(self, img, mask):\n        h, w = img.shape[: 2]\n        pad_h = max(self.crop_size - h, 0)\n        pad_w = max(self.crop_size - w, 0)\n        img = np.pad(img, ((0, pad_h), (0, pad_w), (0, 0)), \'constant\')\n        mask = np.pad(mask, ((0, pad_h), (0, pad_w)), \'constant\',\n                      constant_values=self.ignore_label)\n        return img, mask\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n\n        w, h = img.size\n        long_size = max(h, w)\n\n        img = np.array(img)\n        mask = np.array(mask)\n\n        if long_size > self.crop_size:\n            stride = int(math.ceil(self.crop_size * self.stride_rate))\n            h_step_num = int(math.ceil((h - self.crop_size) / float(stride))) + 1\n            w_step_num = int(math.ceil((w - self.crop_size) / float(stride))) + 1\n            img_sublist, mask_sublist = [], []\n            for yy in range(h_step_num):\n                for xx in range(w_step_num):\n                    sy, sx = yy * stride, xx * stride\n                    ey, ex = sy + self.crop_size, sx + self.crop_size\n                    img_sub = img[sy: ey, sx: ex, :]\n                    mask_sub = mask[sy: ey, sx: ex]\n                    img_sub, mask_sub = self._pad(img_sub, mask_sub)\n                    img_sublist.append(\n                        Image.fromarray(\n                            img_sub.astype(\n                                np.uint8)).convert(\'RGB\'))\n                    mask_sublist.append(\n                        Image.fromarray(\n                            mask_sub.astype(\n                                np.uint8)).convert(\'P\'))\n            return img_sublist, mask_sublist\n        else:\n            img, mask = self._pad(img, mask)\n            img = Image.fromarray(img.astype(np.uint8)).convert(\'RGB\')\n            mask = Image.fromarray(mask.astype(np.uint8)).convert(\'P\')\n            return img, mask\n\n\nclass SlidingCrop(object):\n    def __init__(self, crop_size, stride_rate, ignore_label):\n        self.crop_size = crop_size\n        self.stride_rate = stride_rate\n        self.ignore_label = ignore_label\n\n    def _pad(self, img, mask):\n        h, w = img.shape[: 2]\n        pad_h = max(self.crop_size - h, 0)\n        pad_w = max(self.crop_size - w, 0)\n        img = np.pad(img, ((0, pad_h), (0, pad_w), (0, 0)), \'constant\')\n        mask = np.pad(mask, ((0, pad_h), (0, pad_w)), \'constant\',\n                      constant_values=self.ignore_label)\n        return img, mask, h, w\n\n    def __call__(self, img, mask):\n        assert img.size == mask.size\n\n        w, h = img.size\n        long_size = max(h, w)\n\n        img = np.array(img)\n        mask = np.array(mask)\n\n        if long_size > self.crop_size:\n            stride = int(math.ceil(self.crop_size * self.stride_rate))\n            h_step_num = int(math.ceil((h - self.crop_size) / float(stride))) + 1\n            w_step_num = int(math.ceil((w - self.crop_size) / float(stride))) + 1\n            img_slices, mask_slices, slices_info = [], [], []\n            for yy in range(h_step_num):\n                for xx in range(w_step_num):\n                    sy, sx = yy * stride, xx * stride\n                    ey, ex = sy + self.crop_size, sx + self.crop_size\n                    img_sub = img[sy: ey, sx: ex, :]\n                    mask_sub = mask[sy: ey, sx: ex]\n                    img_sub, mask_sub, sub_h, sub_w = self._pad(img_sub, mask_sub)\n                    img_slices.append(\n                        Image.fromarray(\n                            img_sub.astype(\n                                np.uint8)).convert(\'RGB\'))\n                    mask_slices.append(\n                        Image.fromarray(\n                            mask_sub.astype(\n                                np.uint8)).convert(\'P\'))\n                    slices_info.append([sy, ey, sx, ex, sub_h, sub_w])\n            return img_slices, mask_slices, slices_info\n        else:\n            img, mask, sub_h, sub_w = self._pad(img, mask)\n            img = Image.fromarray(img.astype(np.uint8)).convert(\'RGB\')\n            mask = Image.fromarray(mask.astype(np.uint8)).convert(\'P\')\n            return [img], [mask], [[0, sub_h, 0, sub_w, sub_h, sub_w]]\n\n\nclass ClassUniform(object):\n    def __init__(self, size, crop_nopad, scale_min=0.5, scale_max=2.0, ignore_index=0,\n                 class_list=[16, 15, 14]):\n        """"""\n        This is the initialization for class uniform sampling\n        :param size: crop size (int)\n        :param crop_nopad: Padding or no padding (bool)\n        :param scale_min: Minimum Scale (float)\n        :param scale_max: Maximum Scale (float)\n        :param ignore_index: The index value to ignore in the GT images (unsigned int)\n        :param class_list: A list of class to sample around, by default Truck, train, bus\n        """"""\n        self.size = size\n        self.crop = RandomCrop(self.size, ignore_index=ignore_index, nopad=crop_nopad)\n\n        self.class_list = class_list.replace("" "", """").split("","")\n\n        self.scale_min = scale_min\n        self.scale_max = scale_max\n\n    def detect_peaks(self, image):\n        """"""\n        Takes an image and detect the peaks usingthe local maximum filter.\n        Returns a boolean mask of the peaks (i.e. 1 when\n        the pixel\'s value is the neighborhood maximum, 0 otherwise)\n\n        :param image: An 2d input images\n        :return: Binary output images of the same size as input with pixel value equal\n        to 1 indicating that there is peak at that point\n        """"""\n\n        # define an 8-connected neighborhood\n        neighborhood = generate_binary_structure(2, 2)\n\n        # apply the local maximum filter; all pixel of maximal value\n        # in their neighborhood are set to 1\n        local_max = maximum_filter(image, footprint=neighborhood) == image\n        # local_max is a mask that contains the peaks we are\n        # looking for, but also the background.\n        # In order to isolate the peaks we must remove the background from the mask.\n\n        # we create the mask of the background\n        background = (image == 0)\n\n        # a little technicality: we must erode the background in order to\n        # successfully subtract it form local_max, otherwise a line will\n        # appear along the background border (artifact of the local maximum filter)\n        eroded_background = binary_erosion(background, structure=neighborhood,\n                                           border_value=1)\n\n        # we obtain the final mask, containing only peaks,\n        # by removing the background from the local_max mask (xor operation)\n        detected_peaks = local_max ^ eroded_background\n\n        return detected_peaks\n\n    def __call__(self, img, mask):\n        """"""\n        :param img: PIL Input Image\n        :param mask: PIL Input Mask\n        :return: PIL output PIL (mask, crop) of self.crop_size\n        """"""\n        assert img.size == mask.size\n\n        scale_amt = random.uniform(self.scale_min, self.scale_max)\n        w = int(scale_amt * img.size[0])\n        h = int(scale_amt * img.size[1])\n\n        if scale_amt < 1.0:\n            img, mask = img.resize((w, h), Image.BICUBIC), mask.resize((w, h),\n                                                                       Image.NEAREST)\n            return self.crop(img, mask)\n        else:\n            # Smart Crop ( Class Uniform\'s ABN)\n            origw, origh = mask.size\n            img_new, mask_new = \\\n                img.resize((w, h), Image.BICUBIC), mask.resize((w, h), Image.NEAREST)\n            interested_class = self.class_list  # [16, 15, 14]  # Train, Truck, Bus\n            data = np.array(mask)\n            arr = np.zeros((1024, 2048))\n            for class_of_interest in interested_class:\n                # hist = np.histogram(data==class_of_interest)\n                map = np.where(data == class_of_interest, data, 0)\n                map = map.astype(\'float64\') / map.sum() / class_of_interest\n                map[np.isnan(map)] = 0\n                arr = arr + map\n\n            origarr = arr\n            window_size = 250\n\n            # Given a list of classes of interest find the points on the image that are\n            # of interest to crop from\n            sum_arr = np.zeros((1024, 2048)).astype(\'float32\')\n            tmp = np.zeros((1024, 2048)).astype(\'float32\')\n            for x in range(0, arr.shape[0] - window_size, window_size):\n                for y in range(0, arr.shape[1] - window_size, window_size):\n                    sum_arr[int(x + window_size / 2), int(y + window_size / 2)] = origarr[\n                        x:x + window_size,\n                        y:y + window_size].sum()\n                    tmp[x:x + window_size, y:y + window_size] = \\\n                        origarr[x:x + window_size, y:y + window_size].sum()\n\n            # Scaling Ratios in X and Y for non-uniform images\n            ratio = (float(origw) / w, float(origh) / h)\n            output = self.detect_peaks(sum_arr)\n            coord = (np.column_stack(np.where(output))).tolist()\n\n            # Check if there are any peaks in the images to crop from if not do standard\n            # cropping behaviour\n            if len(coord) == 0:\n                return self.crop(img_new, mask_new)\n            else:\n                # If peaks are detected, random peak selection followed by peak\n                # coordinate scaling to new scaled image and then random\n                # cropping around the peak point in the scaled image\n                randompick = np.random.randint(len(coord))\n                y, x = coord[randompick]\n                y, x = int(y * ratio[0]), int(x * ratio[1])\n                window_size = window_size * ratio[0]\n                cropx = random.uniform(\n                    max(0, (x - window_size / 2) - (self.size - window_size)),\n                    max((x - window_size / 2), (x - window_size / 2) - (\n                        (w - window_size) - x + window_size / 2)))\n\n                cropy = random.uniform(\n                    max(0, (y - window_size / 2) - (self.size - window_size)),\n                    max((y - window_size / 2), (y - window_size / 2) - (\n                        (h - window_size) - y + window_size / 2)))\n\n                return_img = img_new.crop(\n                    (cropx, cropy, cropx + self.size, cropy + self.size))\n                return_mask = mask_new.crop(\n                    (cropx, cropy, cropx + self.size, cropy + self.size))\n                return (return_img, return_mask)\n'"
transforms/transforms.py,3,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n\n# Code borrowded from:\n# https://github.com/zijundeng/pytorch-semantic-segmentation/blob/master/utils/transforms.py\n#\n#\n# MIT License\n#\n# Copyright (c) 2017 ZijunDeng\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n""""""\n\nimport random\nimport numpy as np\nfrom skimage.filters import gaussian\nfrom skimage.restoration import denoise_bilateral\nimport torch\nfrom PIL import Image, ImageFilter, ImageEnhance\nimport torchvision.transforms as torch_tr\nfrom scipy import ndimage\nfrom config import cfg\nfrom scipy.ndimage.interpolation import shift\nfrom scipy.misc import imsave\nfrom skimage.segmentation import find_boundaries\nclass RandomVerticalFlip(object):\n    def __call__(self, img):\n        if random.random() < 0.5:\n            return img.transpose(Image.FLIP_TOP_BOTTOM)\n        return img\n\n\nclass DeNormalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        for t, m, s in zip(tensor, self.mean, self.std):\n            t.mul_(s).add_(m)\n        return tensor\n\n\nclass MaskToTensor(object):\n    def __call__(self, img):\n        return torch.from_numpy(np.array(img, dtype=np.int32)).long()\n\nclass RelaxedBoundaryLossToTensor(object):\n    def __init__(self,ignore_id, num_classes):\n        self.ignore_id=ignore_id\n        self.num_classes= num_classes\n\n\n    def new_one_hot_converter(self,a):\n        ncols = self.num_classes+1\n        out = np.zeros( (a.size,ncols), dtype=np.uint8)\n        out[np.arange(a.size),a.ravel()] = 1\n        out.shape = a.shape + (ncols,)\n        return out\n\n    def __call__(self,img):\n        \n\n        \n        img_arr = np.array(img)\n        import scipy;scipy.misc.imsave(\'orig.png\',img_arr)\n\n        img_arr[img_arr==self.ignore_id]=self.num_classes       \n        \n        if cfg.STRICTBORDERCLASS != None:\n            one_hot_orig = self.new_one_hot_converter(img_arr)\n            mask = np.zeros((img_arr.shape[0],img_arr.shape[1]))\n            for cls in cfg.STRICTBORDERCLASS:\n                mask = np.logical_or(mask,(img_arr == cls))\n        one_hot = 0\n\n        #print(cfg.EPOCH, ""Non Reduced"", cfg.TRAIN.REDUCE_RELAXEDITERATIONCOUNT)\n        border = cfg.BORDER_WINDOW\n        if (cfg.REDUCE_BORDER_EPOCH !=-1 and cfg.EPOCH > cfg.REDUCE_BORDER_EPOCH):\n            border = border // 2\n            border_prediction = find_boundaries(img_arr, mode=\'thick\').astype(np.uint8)\n            print(cfg.EPOCH, ""Reduced"")\n        \n        for i in range(-border,border+1):\n            for j in range(-border, border+1):\n                shifted= shift(img_arr,(i,j), cval=self.num_classes)\n                one_hot += self.new_one_hot_converter(shifted)       \n        \n        one_hot[one_hot>1] = 1\n        \n        if cfg.STRICTBORDERCLASS != None:\n            one_hot = np.where(np.expand_dims(mask,2), one_hot_orig, one_hot)\n    \n        one_hot = np.moveaxis(one_hot,-1,0)\n    \n\n        if (cfg.REDUCE_BORDER_EPOCH !=-1 and cfg.EPOCH > cfg.REDUCE_BORDER_EPOCH):\n                one_hot = np.where(border_prediction,2*one_hot,1*one_hot)\n                print(one_hot.shape)\n        return torch.from_numpy(one_hot).byte()\n        #return torch.from_numpy(one_hot).float()\n        exit(0)\n\nclass ResizeHeight(object):\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        self.target_h = size\n        self.interpolation = interpolation\n\n    def __call__(self, img):\n        w, h = img.size\n        target_w = int(w / h * self.target_h)\n        return img.resize((target_w, self.target_h), self.interpolation)\n\n\nclass FreeScale(object):\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        self.size = tuple(reversed(size))  # size: (h, w)\n        self.interpolation = interpolation\n\n    def __call__(self, img):\n        return img.resize(self.size, self.interpolation)\n\n\nclass FlipChannels(object):\n    def __call__(self, img):\n        img = np.array(img)[:, :, ::-1]\n        return Image.fromarray(img.astype(np.uint8))\n\nclass RandomGaussianBlur(object):\n    def __call__(self, img):\n        sigma = 0.15 + random.random() * 1.15\n        blurred_img = gaussian(np.array(img), sigma=sigma, multichannel=True)\n        blurred_img *= 255\n        return Image.fromarray(blurred_img.astype(np.uint8))\n\n\nclass RandomBilateralBlur(object):\n    def __call__(self, img):\n        sigma = random.uniform(0.05,0.75)\n        blurred_img = denoise_bilateral(np.array(img), sigma_spatial=sigma, multichannel=True)\n        blurred_img *= 255\n        return Image.fromarray(blurred_img.astype(np.uint8))\n\ntry:\n    import accimage\nexcept ImportError:\n    accimage = None\n\n\ndef _is_pil_image(img):\n    if accimage is not None:\n        return isinstance(img, (Image.Image, accimage.Image))\n    else:\n        return isinstance(img, Image.Image)\n\n\ndef adjust_brightness(img, brightness_factor):\n    """"""Adjust brightness of an Image.\n\n    Args:\n        img (PIL Image): PIL Image to be adjusted.\n        brightness_factor (float):  How much to adjust the brightness. Can be\n            any non negative number. 0 gives a black image, 1 gives the\n            original image while 2 increases the brightness by a factor of 2.\n\n    Returns:\n        PIL Image: Brightness adjusted image.\n    """"""\n    if not _is_pil_image(img):\n        raise TypeError(\'img should be PIL Image. Got {}\'.format(type(img)))\n\n    enhancer = ImageEnhance.Brightness(img)\n    img = enhancer.enhance(brightness_factor)\n    return img\n\n\ndef adjust_contrast(img, contrast_factor):\n    """"""Adjust contrast of an Image.\n\n    Args:\n        img (PIL Image): PIL Image to be adjusted.\n        contrast_factor (float): How much to adjust the contrast. Can be any\n            non negative number. 0 gives a solid gray image, 1 gives the\n            original image while 2 increases the contrast by a factor of 2.\n\n    Returns:\n        PIL Image: Contrast adjusted image.\n    """"""\n    if not _is_pil_image(img):\n        raise TypeError(\'img should be PIL Image. Got {}\'.format(type(img)))\n\n    enhancer = ImageEnhance.Contrast(img)\n    img = enhancer.enhance(contrast_factor)\n    return img\n\n\ndef adjust_saturation(img, saturation_factor):\n    """"""Adjust color saturation of an image.\n\n    Args:\n        img (PIL Image): PIL Image to be adjusted.\n        saturation_factor (float):  How much to adjust the saturation. 0 will\n            give a black and white image, 1 will give the original image while\n            2 will enhance the saturation by a factor of 2.\n\n    Returns:\n        PIL Image: Saturation adjusted image.\n    """"""\n    if not _is_pil_image(img):\n        raise TypeError(\'img should be PIL Image. Got {}\'.format(type(img)))\n\n    enhancer = ImageEnhance.Color(img)\n    img = enhancer.enhance(saturation_factor)\n    return img\n\n\ndef adjust_hue(img, hue_factor):\n    """"""Adjust hue of an image.\n\n    The image hue is adjusted by converting the image to HSV and\n    cyclically shifting the intensities in the hue channel (H).\n    The image is then converted back to original image mode.\n\n    `hue_factor` is the amount of shift in H channel and must be in the\n    interval `[-0.5, 0.5]`.\n\n    See https://en.wikipedia.org/wiki/Hue for more details on Hue.\n\n    Args:\n        img (PIL Image): PIL Image to be adjusted.\n        hue_factor (float):  How much to shift the hue channel. Should be in\n            [-0.5, 0.5]. 0.5 and -0.5 give complete reversal of hue channel in\n            HSV space in positive and negative direction respectively.\n            0 means no shift. Therefore, both -0.5 and 0.5 will give an image\n            with complementary colors while 0 gives the original image.\n\n    Returns:\n        PIL Image: Hue adjusted image.\n    """"""\n    if not(-0.5 <= hue_factor <= 0.5):\n        raise ValueError(\'hue_factor is not in [-0.5, 0.5].\'.format(hue_factor))\n\n    if not _is_pil_image(img):\n        raise TypeError(\'img should be PIL Image. Got {}\'.format(type(img)))\n\n    input_mode = img.mode\n    if input_mode in {\'L\', \'1\', \'I\', \'F\'}:\n        return img\n\n    h, s, v = img.convert(\'HSV\').split()\n\n    np_h = np.array(h, dtype=np.uint8)\n    # uint8 addition take cares of rotation across boundaries\n    with np.errstate(over=\'ignore\'):\n        np_h += np.uint8(hue_factor * 255)\n    h = Image.fromarray(np_h, \'L\')\n\n    img = Image.merge(\'HSV\', (h, s, v)).convert(input_mode)\n    return img\n\n\nclass ColorJitter(object):\n    """"""Randomly change the brightness, contrast and saturation of an image.\n\n    Args:\n        brightness (float): How much to jitter brightness. brightness_factor\n            is chosen uniformly from [max(0, 1 - brightness), 1 + brightness].\n        contrast (float): How much to jitter contrast. contrast_factor\n            is chosen uniformly from [max(0, 1 - contrast), 1 + contrast].\n        saturation (float): How much to jitter saturation. saturation_factor\n            is chosen uniformly from [max(0, 1 - saturation), 1 + saturation].\n        hue(float): How much to jitter hue. hue_factor is chosen uniformly from\n            [-hue, hue]. Should be >=0 and <= 0.5.\n    """"""\n    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n        self.brightness = brightness\n        self.contrast = contrast\n        self.saturation = saturation\n        self.hue = hue\n\n    @staticmethod\n    def get_params(brightness, contrast, saturation, hue):\n        """"""Get a randomized transform to be applied on image.\n\n        Arguments are same as that of __init__.\n\n        Returns:\n            Transform which randomly adjusts brightness, contrast and\n            saturation in a random order.\n        """"""\n        transforms = []\n        if brightness > 0:\n            brightness_factor = np.random.uniform(max(0, 1 - brightness), 1 + brightness)\n            transforms.append(\n                torch_tr.Lambda(lambda img: adjust_brightness(img, brightness_factor)))\n\n        if contrast > 0:\n            contrast_factor = np.random.uniform(max(0, 1 - contrast), 1 + contrast)\n            transforms.append(\n                torch_tr.Lambda(lambda img: adjust_contrast(img, contrast_factor)))\n\n        if saturation > 0:\n            saturation_factor = np.random.uniform(max(0, 1 - saturation), 1 + saturation)\n            transforms.append(\n                torch_tr.Lambda(lambda img: adjust_saturation(img, saturation_factor)))\n\n        if hue > 0:\n            hue_factor = np.random.uniform(-hue, hue)\n            transforms.append(\n                torch_tr.Lambda(lambda img: adjust_hue(img, hue_factor)))\n\n        np.random.shuffle(transforms)\n        transform = torch_tr.Compose(transforms)\n\n        return transform\n\n    def __call__(self, img):\n        """"""\n        Args:\n            img (PIL Image): Input image.\n\n        Returns:\n            PIL Image: Color jittered image.\n        """"""\n        transform = self.get_params(self.brightness, self.contrast,\n                                    self.saturation, self.hue)\n        return transform(img)\n'"
utils/AttrDict.py,0,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n\n# Code adapted from:\n# https://github.com/facebookresearch/Detectron/blob/master/detectron/utils/collections.py\n\nSource License\n# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n#\n# Based on:\n# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n""""""\n\n\nclass AttrDict(dict):\n\n    IMMUTABLE = \'__immutable__\'\n\n    def __init__(self, *args, **kwargs):\n        super(AttrDict, self).__init__(*args, **kwargs)\n        self.__dict__[AttrDict.IMMUTABLE] = False\n\n    def __getattr__(self, name):\n        if name in self.__dict__:\n            return self.__dict__[name]\n        elif name in self:\n            return self[name]\n        else:\n            raise AttributeError(name)\n\n    def __setattr__(self, name, value):\n        if not self.__dict__[AttrDict.IMMUTABLE]:\n            if name in self.__dict__:\n                self.__dict__[name] = value\n            else:\n                self[name] = value\n        else:\n            raise AttributeError(\n                \'Attempted to set ""{}"" to ""{}"", but AttrDict is immutable\'.\n                format(name, value)\n            )\n\n    def immutable(self, is_immutable):\n        """"""Set immutability to is_immutable and recursively apply the setting\n        to all nested AttrDicts.\n        """"""\n        self.__dict__[AttrDict.IMMUTABLE] = is_immutable\n        # Recursively set immutable state\n        for v in self.__dict__.values():\n            if isinstance(v, AttrDict):\n                v.immutable(is_immutable)\n        for v in self.values():\n            if isinstance(v, AttrDict):\n                v.immutable(is_immutable)\n\n    def is_immutable(self):\n        return self.__dict__[AttrDict.IMMUTABLE]\n'"
utils/f_boundary.py,0,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n\n# Code adapted from:\n# https://github.com/fperazzi/davis/blob/master/python/lib/davis/measures/f_boundary.py\n#\n# Source License\n#\n# BSD 3-Clause License\n#\n# Copyright (c) 2017,\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n# * Redistributions of source code must retain the above copyright notice, this\n#   list of conditions and the following disclaimer.\n#\n# * Redistributions in binary form must reproduce the above copyright notice,\n#   this list of conditions and the following disclaimer in the documentation\n#   and/or other materials provided with the distribution.\n#\n# * Neither the name of the copyright holder nor the names of its\n#   contributors may be used to endorse or promote products derived from\n#   this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.s\n##############################################################################\n#\n# Based on:\n# ----------------------------------------------------------------------------\n# A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation\n# Copyright (c) 2016 Federico Perazzi\n# Licensed under the BSD License [see LICENSE for details]\n# Written by Federico Perazzi\n# ----------------------------------------------------------------------------\n""""""\n\n\n\n\nimport numpy as np\nfrom multiprocessing import Pool\nfrom tqdm import tqdm\n\n"""""" Utilities for computing, reading and saving benchmark evaluation.""""""\n\ndef eval_mask_boundary(seg_mask,gt_mask,num_classes,num_proc=10,bound_th=0.008):\n    """"""\n    Compute F score for a segmentation mask\n\n    Arguments:\n        seg_mask (ndarray): segmentation mask prediction\n        gt_mask (ndarray): segmentation mask ground truth\n        num_classes (int): number of classes\n\n    Returns:\n        F (float): mean F score across all classes\n        Fpc (listof float): F score per class\n    """"""\n    p = Pool(processes=num_proc)\n    batch_size = seg_mask.shape[0]\n    \n    Fpc = np.zeros(num_classes)\n    Fc = np.zeros(num_classes)\n    for class_id in tqdm(range(num_classes)):\n        args = [((seg_mask[i] == class_id).astype(np.uint8), \n                 (gt_mask[i] == class_id).astype(np.uint8),\n                 gt_mask[i] == 255,\n                 bound_th) \n                 for i in range(batch_size)]\n        temp = p.map(db_eval_boundary_wrapper, args)\n        temp = np.array(temp)\n        Fs = temp[:,0]\n        _valid = ~np.isnan(Fs)\n        Fc[class_id] = np.sum(_valid)\n        Fs[np.isnan(Fs)] = 0\n        Fpc[class_id] = sum(Fs)\n    return Fpc, Fc\n\n\n#def db_eval_boundary_wrapper_wrapper(args):\n#    seg_mask, gt_mask, class_id, batch_size, Fpc = args\n#    print(""class_id:"" + str(class_id))\n#    p = Pool(processes=10)\n#    args = [((seg_mask[i] == class_id).astype(np.uint8), \n#             (gt_mask[i] == class_id).astype(np.uint8)) \n#             for i in range(batch_size)]\n#    Fs = p.map(db_eval_boundary_wrapper, args)\n#    Fpc[class_id] = sum(Fs)\n#    return\n\ndef db_eval_boundary_wrapper(args):\n    foreground_mask, gt_mask, ignore, bound_th = args\n    return db_eval_boundary(foreground_mask, gt_mask,ignore, bound_th)\n\ndef db_eval_boundary(foreground_mask,gt_mask, ignore_mask,bound_th=0.008):\n\t""""""\n\tCompute mean,recall and decay from per-frame evaluation.\n\tCalculates precision/recall for boundaries between foreground_mask and\n\tgt_mask using morphological operators to speed it up.\n\n\tArguments:\n\t\tforeground_mask (ndarray): binary segmentation image.\n\t\tgt_mask         (ndarray): binary annotated image.\n\n\tReturns:\n\t\tF (float): boundaries F-measure\n\t\tP (float): boundaries precision\n\t\tR (float): boundaries recall\n\t""""""\n\tassert np.atleast_3d(foreground_mask).shape[2] == 1\n\n\tbound_pix = bound_th if bound_th >= 1 else \\\n\t\t\tnp.ceil(bound_th*np.linalg.norm(foreground_mask.shape))\n\n\t#print(bound_pix)\n\t#print(gt.shape)\n\t#print(np.unique(gt))\n\tforeground_mask[ignore_mask] = 0\n\tgt_mask[ignore_mask] = 0\n\n\t# Get the pixel boundaries of both masks\n\tfg_boundary = seg2bmap(foreground_mask);\n\tgt_boundary = seg2bmap(gt_mask);\n\n\tfrom skimage.morphology import binary_dilation,disk\n\n\tfg_dil = binary_dilation(fg_boundary,disk(bound_pix))\n\tgt_dil = binary_dilation(gt_boundary,disk(bound_pix))\n\n\t# Get the intersection\n\tgt_match = gt_boundary * fg_dil\n\tfg_match = fg_boundary * gt_dil\n\n\t# Area of the intersection\n\tn_fg     = np.sum(fg_boundary)\n\tn_gt     = np.sum(gt_boundary)\n\n\t#% Compute precision and recall\n\tif n_fg == 0 and  n_gt > 0:\n\t\tprecision = 1\n\t\trecall = 0\n\telif n_fg > 0 and n_gt == 0:\n\t\tprecision = 0\n\t\trecall = 1\n\telif n_fg == 0  and n_gt == 0:\n\t\tprecision = 1\n\t\trecall = 1\n\telse:\n\t\tprecision = np.sum(fg_match)/float(n_fg)\n\t\trecall    = np.sum(gt_match)/float(n_gt)\n\n\t# Compute F measure\n\tif precision + recall == 0:\n\t\tF = 0\n\telse:\n\t\tF = 2*precision*recall/(precision+recall);\n\n\treturn F, precision\n\ndef seg2bmap(seg,width=None,height=None):\n\t""""""\n\tFrom a segmentation, compute a binary boundary map with 1 pixel wide\n\tboundaries.  The boundary pixels are offset by 1/2 pixel towards the\n\torigin from the actual segment boundary.\n\n\tArguments:\n\t\tseg     : Segments labeled from 1..k.\n\t\twidth\t  :\tWidth of desired bmap  <= seg.shape[1]\n\t\theight  :\tHeight of desired bmap <= seg.shape[0]\n\n\tReturns:\n\t\tbmap (ndarray):\tBinary boundary map.\n\n\t David Martin <dmartin@eecs.berkeley.edu>\n\t January 2003\n """"""\n\n\tseg = seg.astype(np.bool)\n\tseg[seg>0] = 1\n\n\tassert np.atleast_3d(seg).shape[2] == 1\n\n\twidth  = seg.shape[1] if width  is None else width\n\theight = seg.shape[0] if height is None else height\n\n\th,w = seg.shape[:2]\n\n\tar1 = float(width) / float(height)\n\tar2 = float(w) / float(h)\n\n\tassert not (width>w | height>h | abs(ar1-ar2)>0.01),\\\n\t\t\t\'Can\'\'t convert %dx%d seg to %dx%d bmap.\'%(w,h,width,height)\n\n\te  = np.zeros_like(seg)\n\ts  = np.zeros_like(seg)\n\tse = np.zeros_like(seg)\n\n\te[:,:-1]    = seg[:,1:]\n\ts[:-1,:]    = seg[1:,:]\n\tse[:-1,:-1] = seg[1:,1:]\n\n\tb        = seg^e | seg^s | seg^se\n\tb[-1,:]  = seg[-1,:]^e[-1,:]\n\tb[:,-1]  = seg[:,-1]^s[:,-1]\n\tb[-1,-1] = 0\n\n\tif w == width and h == height:\n\t\tbmap = b\n\telse:\n\t\tbmap = np.zeros((height,width))\n\t\tfor x in range(w):\n\t\t\tfor y in range(h):\n\t\t\t\tif b[y,x]:\n\t\t\t\t\tj = 1+floor((y-1)+height / h)\n\t\t\t\t\ti = 1+floor((x-1)+width  / h)\n\t\t\t\t\tbmap[j,i] = 1;\n\n\treturn bmap\n'"
utils/image_page.py,0,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport glob\nimport os\n\nclass ImagePage(object):\n    \'\'\'\n    This creates an HTML page of embedded images, useful for showing evaluation results.\n\n    Usage:\n    ip = ImagePage(html_fn)\n\n    # Add a table with N images ...\n    ip.add_table((img, descr), (img, descr), ...)\n\n    # Generate html page\n    ip.write_page()\n    \'\'\'\n    def __init__(self, experiment_name, html_filename):\n        self.experiment_name = experiment_name\n        self.html_filename = html_filename\n        self.outfile = open(self.html_filename, \'w\')\n        self.items = []\n\n    def _print_header(self):\n        header = \'\'\'<!DOCTYPE html>\n<html>\n  <head>\n    <title>Experiment = {}</title>\n  </head>\n  <body>\'\'\'.format(self.experiment_name)\n        self.outfile.write(header)\n\n    def _print_footer(self):\n        self.outfile.write(\'\'\'  </body>\n</html>\'\'\')\n\n    def _print_table_header(self, table_name):\n        table_hdr = \'\'\'    <h3>{}</h3>\n    <table border=""1"" style=""table-layout: fixed;"">\n      <tr>\'\'\'.format(table_name)\n        self.outfile.write(table_hdr)\n\n    def _print_table_footer(self):\n        table_ftr = \'\'\'      </tr>\n    </table>\'\'\'\n        self.outfile.write(table_ftr)\n\n    def _print_table_guts(self, img_fn, descr):\n        table = \'\'\'        <td halign=""center"" style=""word-wrap: break-word;"" valign=""top"">\n          <p>\n            <a href=""{img_fn}"">\n              <img src=""{img_fn}"" style=""width:768px"">\n            </a><br>\n            <p>{descr}</p>\n          </p>\n        </td>\'\'\'.format(img_fn=img_fn, descr=descr)\n        self.outfile.write(table)\n\n    def add_table(self, img_label_pairs):\n        self.items.append(img_label_pairs)\n\n    def _write_table(self, table):\n        img, _descr = table[0]\n        self._print_table_header(os.path.basename(img))\n        for img, descr in table:\n            self._print_table_guts(img, descr)\n        self._print_table_footer()\n\n    def write_page(self):\n        self._print_header()\n\n        for table in self.items:\n            self._write_table(table)\n\n        self._print_footer()\n\n\ndef main():\n    images = glob.glob(\'dump_imgs_train/*.png\')\n    images = [i for i in images if \'mask\' not in i]\n\n    ip = ImagePage(\'test page\', \'dd.html\')\n    for img in images:\n        basename = os.path.splitext(img)[0]\n        mask_img = basename + \'_mask.png\'\n        ip.add_table(((img, \'image\'), (mask_img, \'mask\')))\n    ip.write_page()\n'"
utils/misc.py,4,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport sys\nimport re\nimport os\nimport shutil\nimport torch\nfrom datetime import datetime\nimport logging\nfrom subprocess import call\nimport shlex\nfrom tensorboardX import SummaryWriter\nimport numpy as np\nfrom utils.image_page import ImagePage\nimport torchvision.transforms as standard_transforms\nimport torchvision.utils as vutils\nfrom PIL import Image\n\n# Create unique output dir name based on non-default command line args\ndef make_exp_name(args, parser):\n    exp_name = \'{}-{}\'.format(args.dataset[:3], args.arch[:])\n    dict_args = vars(args)\n\n    # sort so that we get a consistent directory name\n    argnames = sorted(dict_args)\n\n    # build experiment name with non-default args\n    for argname in argnames:\n        if dict_args[argname] != parser.get_default(argname):\n            if argname == \'exp\' or argname == \'arch\' or argname == \'prev_best_filepath\':\n                continue\n            if argname == \'snapshot\':\n                arg_str = \'-PT\'\n            elif argname == \'nosave\':\n                arg_str = \'\'\n                argname=\'\'\n            elif argname == \'freeze_trunk\':\n                argname = \'\'\n                arg_str = \'-fr\'\n            elif argname == \'syncbn\':\n                argname = \'\'\n                arg_str = \'-sbn\'\n            elif argname == \'relaxedloss\':\n                argname = \'\'\n                arg_str = \'re-loss\'\n            elif isinstance(dict_args[argname], bool):\n                arg_str = \'T\' if dict_args[argname] else \'F\'\n            else:\n                arg_str = str(dict_args[argname])[:6]\n            exp_name += \'-{}_{}\'.format(str(argname), arg_str)\n    # clean special chars out\n    exp_name = re.sub(r\'[^A-Za-z0-9_\\-]+\', \'\', exp_name)\n    exp_name = \'testing\'\n    return exp_name\n\n\ndef save_log(prefix, output_dir, date_str):\n    fmt = \'%(asctime)s.%(msecs)03d %(message)s\'\n    date_fmt = \'%m-%d %H:%M:%S\'\n    filename = os.path.join(output_dir, prefix + \'_\' + date_str + \'.log\')\n    logging.basicConfig(level=logging.INFO, format=fmt, datefmt=date_fmt,\n                        filename=filename, filemode=\'w\')\n    console = logging.StreamHandler()\n    console.setLevel(logging.INFO)\n    formatter = logging.Formatter(fmt=fmt, datefmt=date_fmt)\n    console.setFormatter(formatter)\n    logging.getLogger(\'\').addHandler(console)\n    #logging.basicConfig(level=logging.INFO, format=\'%(asctime)s.%(msecs)03d %(levelname)s:\\t%(message)s\', datefmt=\'%Y-%m-%d %H:%M:%S\')\n\n\ndef save_code(exp_path, date_str):\n    code_root = \'.\'  # FIXME!\n    zip_outfile = os.path.join(exp_path, \'code_{}.tgz\'.format(date_str))\n    print(\'Saving code to {}\'.format(zip_outfile))\n    cmd = \'tar -czvf {zip_outfile} --exclude=\\\'*.pyc\\\' --exclude=\\\'*.png\\\' \' +\\\n        \'--exclude=\\\'*tfevents*\\\' {root}/train.py \' + \\\n        \' {root}/utils {root}/datasets {root}/models\'\n    cmd = cmd.format(zip_outfile=zip_outfile, root=code_root)\n    call(shlex.split(cmd), stdout=open(os.devnull, \'wb\'))\n\n\ndef prep_experiment(args, parser):\n    \'\'\'\n    Make output directories, setup logging, Tensorboard, snapshot code.\n    \'\'\'\n    ckpt_path = args.ckpt\n    tb_path = args.tb_path\n    exp_name = make_exp_name(args, parser)\n    args.exp_path = os.path.join(ckpt_path, args.exp, exp_name)\n    args.tb_exp_path = os.path.join(tb_path, args.exp, exp_name)\n    args.ngpu = torch.cuda.device_count()\n    args.date_str = str(datetime.now().strftime(\'%Y_%m_%d_%H_%M_%S\'))\n    args.best_record = {\'epoch\': -1, \'iter\': 0, \'val_loss\': 1e10, \'acc\': 0,\n                        \'acc_cls\': 0, \'mean_iu\': 0, \'fwavacc\': 0}\n    args.last_record = {}\n    os.makedirs(args.exp_path, exist_ok=True)\n    os.makedirs(args.tb_exp_path, exist_ok=True)\n    save_log(\'log\', args.exp_path, args.date_str)\n    #save_code(args.exp_path, args.date_str)\n    open(os.path.join(args.exp_path, args.date_str + \'.txt\'), \'w\').write(\n        str(args) + \'\\n\\n\')\n\n    writer = SummaryWriter(logdir=args.tb_exp_path, comment=args.tb_tag)\n    return writer\n\nclass AverageMeter(object):\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\n\ndef evaluate_eval(args, net, optimizer, val_loss, mf_score, hist, dump_images, heatmap_images, writer, epoch=0, dataset=None, ):\n    \'\'\'\n    Modified IOU mechanism for on-the-fly IOU calculations ( prevents memory overflow for\n    large dataset) Only applies to eval/eval.py\n    \'\'\'\n    # axis 0: gt, axis 1: prediction\n    acc = np.diag(hist).sum() / hist.sum()\n    acc_cls = np.diag(hist) / hist.sum(axis=1)\n    acc_cls = np.nanmean(acc_cls)\n    iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n\n    print_evaluate_results(hist, iu, writer, epoch, dataset)\n    freq = hist.sum(axis=1) / hist.sum()\n    mean_iu = np.nanmean(iu)\n    logging.info(\'mean {}\'.format(mean_iu))\n    fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n    #return acc, acc_cls, mean_iu, fwavacc\n\n    # update latest snapshot\n    if \'mean_iu\' in args.last_record:\n        last_snapshot = \'last_epoch_{}_mean-iu_{:.5f}.pth\'.format(\n            args.last_record[\'epoch\'], args.last_record[\'mean_iu\'])\n        last_snapshot = os.path.join(args.exp_path, last_snapshot)\n        try:\n            os.remove(last_snapshot)\n        except OSError:\n            pass\n    last_snapshot = \'last_epoch_{}_mean-iu_{:.5f}.pth\'.format(epoch, mean_iu)\n    last_snapshot = os.path.join(args.exp_path, last_snapshot)\n    args.last_record[\'mean_iu\'] = mean_iu\n    args.last_record[\'epoch\'] = epoch\n    \n    torch.cuda.synchronize()\n    \n    torch.save({\n        \'state_dict\': net.state_dict(),\n        \'optimizer\': optimizer.state_dict(),\n        \'epoch\': epoch,\n        \'mean_iu\': mean_iu,\n        \'command\': \' \'.join(sys.argv[1:])\n    }, last_snapshot)\n\n    # update best snapshot\n    if mean_iu > args.best_record[\'mean_iu\'] :\n        # remove old best snapshot\n        if args.best_record[\'epoch\'] != -1:\n            best_snapshot = \'best_epoch_{}_mean-iu_{:.5f}.pth\'.format(\n                args.best_record[\'epoch\'], args.best_record[\'mean_iu\'])\n            best_snapshot = os.path.join(args.exp_path, best_snapshot)\n            assert os.path.exists(best_snapshot), \\\n                \'cant find old snapshot {}\'.format(best_snapshot)\n            os.remove(best_snapshot)\n\n        \n        # save new best\n        args.best_record[\'val_loss\'] = val_loss.avg\n        args.best_record[\'mask_f1_score\'] = mf_score.avg\n        args.best_record[\'epoch\'] = epoch\n        args.best_record[\'acc\'] = acc\n        args.best_record[\'acc_cls\'] = acc_cls\n        args.best_record[\'mean_iu\'] = mean_iu\n        args.best_record[\'fwavacc\'] = fwavacc\n\n        best_snapshot = \'best_epoch_{}_mean-iu_{:.5f}.pth\'.format(\n            args.best_record[\'epoch\'], args.best_record[\'mean_iu\'])\n        best_snapshot = os.path.join(args.exp_path, best_snapshot)\n        shutil.copyfile(last_snapshot, best_snapshot)\n        \n    \n        to_save_dir = os.path.join(args.exp_path, \'best_images\')\n        os.makedirs(to_save_dir, exist_ok=True)\n        ip = ImagePage(epoch, \'{}/index.html\'.format(to_save_dir))\n\n        val_visual = []\n        \n        idx = 0\n        \n        visualize = standard_transforms.Compose([\n            standard_transforms.Scale(384),\n            standard_transforms.ToTensor()\n        ])\n        for bs_idx, bs_data in enumerate(dump_images):\n            for local_idx, data in enumerate(zip(bs_data[0], bs_data[1],bs_data[2])):\n                gt_pil = args.dataset_cls.colorize_mask(data[0].cpu().numpy())\n                pred = data[1].cpu().numpy()\n                predictions_pil = args.dataset_cls.colorize_mask(pred)\n                img_name = data[2]\n                \n                prediction_fn = \'{}_prediction.png\'.format(img_name)\n                predictions_pil.save(os.path.join(to_save_dir, prediction_fn))\n                gt_fn = \'{}_gt.png\'.format(img_name)\n                gt_pil.save(os.path.join(to_save_dir, gt_fn))\n                ip.add_table([(gt_fn, \'gt\'), (prediction_fn, \'prediction\')])\n                val_visual.extend([visualize(gt_pil.convert(\'RGB\')),\n                                   visualize(predictions_pil.convert(\'RGB\'))])\n                idx = idx+1\n                if idx >= 9:\n                    ip.write_page()\n                    break\n        for bs_idx, bs_data in enumerate(heatmap_images):\n            for local_idx, data in enumerate(zip(bs_data[0], bs_data[1],bs_data[2])):\n            \n                gt_pil = args.dataset_cls.colorize_mask(data[0].cpu().numpy())\n                \n                predictions_pil = data[1].cpu().numpy()\n                predictions_pil = (predictions_pil / predictions_pil.max()) * 255\n                predictions_pil = Image.fromarray(predictions_pil.astype(np.uint8))\n                img_name = data[2]\n                \n                prediction_fn = \'{}_prediction.png\'.format(img_name)\n                predictions_pil.save(os.path.join(to_save_dir, prediction_fn))\n                gt_fn = \'{}_gt.png\'.format(img_name)\n                gt_pil.save(os.path.join(to_save_dir, gt_fn))\n                ip.add_table([(gt_fn, \'gt\'), (prediction_fn, \'prediction\')])\n                val_visual.extend([visualize(gt_pil.convert(\'RGB\')),\n                                   visualize(predictions_pil.convert(\'RGB\'))])\n                idx = idx+1\n                if idx >= 9:\n                    ip.write_page()\n                    break\n\n        val_visual = torch.stack(val_visual, 0)\n        val_visual = vutils.make_grid(val_visual, nrow=10, padding=5)\n        writer.add_image(last_snapshot, val_visual)\n\n    logging.info(\'-\' * 107)\n    fmt_str = \'[epoch %d], [val loss %.5f], [mask f1 %.5f], [acc %.5f], [acc_cls %.5f], \' +\\\n              \'[mean_iu %.5f], [fwavacc %.5f]\'\n    logging.info(fmt_str % (epoch, val_loss.avg, mf_score.avg, acc, acc_cls, mean_iu, fwavacc))\n    fmt_str = \'best record: [val loss %.5f], [mask f1 %.5f], [acc %.5f], [acc_cls %.5f], \' +\\\n              \'[mean_iu %.5f], [fwavacc %.5f], [epoch %d], \'\n    logging.info(fmt_str % (args.best_record[\'val_loss\'], args.best_record[\'mask_f1_score\'],\n                            args.best_record[\'acc\'],\n                            args.best_record[\'acc_cls\'], args.best_record[\'mean_iu\'],\n                            args.best_record[\'fwavacc\'], args.best_record[\'epoch\']))\n    logging.info(\'-\' * 107)\n\n    # tensorboard logging of validation phase metrics\n\n    writer.add_scalar(\'training/acc\', acc, epoch)\n    writer.add_scalar(\'training/acc_cls\', acc_cls, epoch)\n    writer.add_scalar(\'training/mean_iu\', mean_iu, epoch)\n    writer.add_scalar(\'training/val_loss\', val_loss.avg, epoch)\n    writer.add_scalar(\'training/mask_f1_score\', mf_score.avg, epoch)\n\n\ndef fast_hist(label_pred, label_true, num_classes):\n    mask = (label_true >= 0) & (label_true < num_classes)\n    hist = np.bincount(\n        num_classes * label_true[mask].astype(int) +\n        label_pred[mask], minlength=num_classes ** 2).reshape(num_classes, num_classes)\n    return hist\n\n\n\ndef print_evaluate_results(hist, iu, writer=None, epoch=0, dataset=None):\n    try:\n        id2cat = dataset.id2cat\n    except:\n        id2cat = {i: i for i in range(dataset.num_classes)}\n    iu_false_positive = hist.sum(axis=1) - np.diag(hist)\n    iu_false_negative = hist.sum(axis=0) - np.diag(hist)\n    iu_true_positive = np.diag(hist)\n\n    logging.info(\'IoU:\')\n    logging.info(\'label_id      label    iU    Precision Recall TP     FP    FN\')\n    for idx, i in enumerate(iu):\n        idx_string = ""{:2d}"".format(idx)\n        class_name = ""{:>13}"".format(id2cat[idx]) if idx in id2cat else \'\'\n        iu_string = \'{:5.2f}\'.format(i * 100)\n        total_pixels = hist.sum()\n        tp = \'{:5.2f}\'.format(100 * iu_true_positive[idx] / total_pixels)\n        fp = \'{:5.2f}\'.format(\n            iu_false_positive[idx] / iu_true_positive[idx])\n        fn = \'{:5.2f}\'.format(iu_false_negative[idx] / iu_true_positive[idx])\n        precision = \'{:5.2f}\'.format(\n            iu_true_positive[idx] / (iu_true_positive[idx] + iu_false_positive[idx]))\n        recall = \'{:5.2f}\'.format(\n            iu_true_positive[idx] / (iu_true_positive[idx] + iu_false_negative[idx]))\n        logging.info(\'{}    {}   {}  {}     {}  {}   {}   {}\'.format(\n            idx_string, class_name, iu_string, precision, recall, tp, fp, fn))\n\n        writer.add_scalar(\'val_class_iu/{}\'.format(id2cat[idx]), i * 100, epoch)\n        writer.add_scalar(\'val_class_precision/{}\'.format(id2cat[idx]),\n                          iu_true_positive[idx] / (iu_true_positive[idx] +\n                                                   iu_false_positive[idx]), epoch)\n        writer.add_scalar(\'val_class_recall/{}\'.format(id2cat[idx]),\n                          iu_true_positive[idx] / (iu_true_positive[idx] +\n                                                   iu_false_negative[idx]), epoch)\n'"
