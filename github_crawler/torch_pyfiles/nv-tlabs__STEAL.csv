file_path,api_count,code
inference_cityscapes.py,5,"b""# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#  * Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n#  * Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#  * Neither the name of NVIDIA CORPORATION nor the names of its\n#    contributors may be used to endorse or promote products derived\n#    from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nimport torch\nimport numpy as np\nimport torch.utils.data\nfrom models.casenet import casenet101 as CaseNet101\n\nfrom utils.dataloader import default_flist_reader\nimport os\nimport cv2\nimport tqdm\n\nimport argparse\n\n#For fair comparison, this is inspired by the way CASENET inference procedure works\ndef do_test(net, output_folder, test_lst, n_classes=19, image_h=1024, image_w=2048, patch_h=512, patch_w=512,\n            step_size_y=256, step_size_x=256, pad=16):\n    num_cls = n_classes\n    # image_h = 1024  # Need to pre-determine test image size\n    # image_w = 2048  # Need to pre-determine test image size\n\n    net.eval()\n    if output_folder is not None:\n        output_images_dir_iter = os.path.join(output_folder, 'val_images')\n        if not os.path.isdir(output_images_dir_iter):\n            os.makedirs(output_images_dir_iter)\n\n    if ((2 * pad) % 8) != 0:\n        raise ValueError('Pad number must be able to be divided by 8!')\n    step_num_y = (image_h - patch_h + 0.0) / step_size_y\n    step_num_x = (image_w - patch_w + 0.0) / step_size_x\n\n    if round(step_num_y) != step_num_y:\n        raise ValueError('Vertical sliding size can not be divided by step size!')\n\n    if round(step_num_x) != step_num_x:\n        raise ValueError('Horizontal sliding size can not be divided by step size!')\n\n    step_num_y = int(step_num_y)\n    step_num_x = int(step_num_x)\n    mean_value = (104.008, 116.669, 122.675)  # BGR\n\n    pred_set = []  # only used if output_folder is none.\n    for idx_img in tqdm.tqdm(range(len(test_lst))):\n        in_ = cv2.imread(test_lst[idx_img]).astype(np.float32)\n        width, height, chn = in_.shape[1], in_.shape[0], in_.shape[2]\n        im_array = cv2.copyMakeBorder(in_, pad, pad, pad, pad, cv2.BORDER_REFLECT)\n\n        if (height != image_h) or (width != image_w):\n            raise ValueError('Input image size must be' + str(image_h) + 'x' + str(image_w) + '!')\n\n        # Perform patch-by-patch testing\n        score_pred = np.zeros((height, width, num_cls))\n        mat_count = np.zeros((height, width, 1))\n        for i in range(0, step_num_y + 1):\n            offset_y = i * step_size_y\n            for j in range(0, step_num_x + 1):\n                offset_x = j * step_size_x\n\n                # crop overlapped regions from the image\n                in_ = np.array(\n                    im_array[offset_y:offset_y + patch_h + 2 * pad, offset_x:offset_x + patch_w + 2 * pad, :])\n                in_ -= np.array(mean_value)\n                in_ = in_.transpose((2, 0, 1))  # HxWx3 -> 3xHxW\n                # ---\n                in_ = torch.from_numpy(in_).cuda()\n                in_ = in_.unsqueeze(dim=0)  # 1x3xHxW\n\n                out_masks = net(in_)  #\n                prediction = torch.sigmoid(out_masks[0]).data.cpu().numpy()[0]\n                # add the prediction to score_pred and increase count by 1\n                score_pred[offset_y:offset_y + patch_h, offset_x:offset_x + patch_w, :] += \\\n                    np.transpose(prediction, (1, 2, 0))[pad:-pad, pad:-pad, :]\n                mat_count[offset_y:offset_y + patch_h, offset_x:offset_x + patch_w, 0] += 1.0\n\n        score_pred = np.divide(score_pred, mat_count)\n\n        img_base_name = os.path.basename(test_lst[idx_img])\n        img_result_name = os.path.splitext(img_base_name)[0] + '.png'\n        if output_folder is None:\n            pred_set.append(score_pred)\n            continue\n\n        for idx_cls in range(num_cls):\n            im = (score_pred[:, :, idx_cls] * 255).astype(np.uint8)\n            result_root = os.path.join(output_images_dir_iter, 'class_' + str(idx_cls + 1))\n            if not os.path.exists(result_root):\n                os.makedirs(result_root)\n            cv2.imwrite(\n                os.path.join(result_root, img_result_name),\n                im)\n\n            # scaling 50% resolution for fast evaluation.\n            im_small = cv2.resize(im, (0, 0), fx=0.5, fy=0.5, interpolation=cv2.INTER_NEAREST)\n\n            result_root = os.path.join(output_images_dir_iter + '_scaled_0_5', 'class_' + str(idx_cls + 1))\n            if not os.path.exists(result_root):\n                os.makedirs(result_root)\n\n            cv2.imwrite(\n                os.path.join(result_root, img_result_name),\n                im_small)\n\n    return pred_set  # empty if output_folder exits\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--root_dir_val', type=str,\n                        default='./data/cityscapes-preprocess/data_proc')\n\n    parser.add_argument('--flist_val', type=str,\n                        default='./data_proc/val.txt')\n\n    # --\n    parser.add_argument('--ckpt', type=str, default='./checkpoints/cityscapes/model_checkpoint.pt')\n\n    parser.add_argument('--output_folder', type=str, default='./output/cityscapes')\n\n    args = parser.parse_args()\n    print('****')\n    print(args)\n    print('****')\n\n    output_folder = args.output_folder\n    root_dir_val = args.root_dir_val\n    flist_val = args.flist_val\n    ckpt = args.ckpt\n\n    # ---\n\n    net = CaseNet101(nclasses=19)\n    net = torch.nn.DataParallel(net.cuda())\n\n    print('loading ckpt %s...' % ckpt)\n    net.load_state_dict(torch.load(ckpt), strict=True)\n\n    imlist = default_flist_reader(None, flist_val)\n    test_lst = [os.path.join(root_dir_val, im_path) for (im_path, _) in imlist]\n\n    do_test(net, output_folder, test_lst, n_classes=19)\n\n\nif __name__ == '__main__':\n    main()\n"""
inference_sbd.py,5,"b""# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#  * Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n#  * Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#  * Neither the name of NVIDIA CORPORATION nor the names of its\n#    contributors may be used to endorse or promote products derived\n#    from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nimport torch\nimport numpy as np\nimport torch.utils.data\nfrom utils import dataloader\nfrom models.casenet import casenet101 as CaseNet101\nimport os\nimport cv2\nimport tqdm\n\nimport argparse\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument('--root_dir_val', type=str, default='./data/sbd/data_aug/')\nparser.add_argument('--flist_val', type=str, default='./data/sbd/data_aug/val_list.txt')\n\nparser.add_argument('--ckpt',\n                    type=str,\n                    default='./checkpoints/sbd/model_checkpoint.pt')\n\nparser.add_argument('--output_folder', type=str,\n                    default='./output/sbd/')\nparser.add_argument('--dataset', type=str, default='sbd')\n\nparser.add_argument('--n_classes', type=int, default=20)\n\n\ndef save_pred(im_info, predictions, num_cls, output_dir):\n    org_height, org_width = im_info['orig_size']\n    filename = os.path.basename(im_info['impath'][0])\n    img_result_name = os.path.splitext(filename)[0] + '.png'\n    for idx_cls in range(num_cls):\n        score_pred = predictions.data.cpu().numpy()[0][idx_cls, 0:org_height, 0:org_width]\n        im = (score_pred * 255).astype(np.uint8)\n        result_root = os.path.join(output_dir, 'class_' + str(idx_cls + 1))\n        if not os.path.exists(result_root):\n            os.makedirs(result_root)\n        cv2.imwrite(\n            os.path.join(result_root, img_result_name),\n            im)\n\n\ndef do_test_sbd(net_, val_data_loader_, cuda, output_folder, n_classes):\n    print('Running Inference....')\n    net_.eval()\n    output_images_dir = os.path.join(output_folder, 'val_images')\n\n    if not os.path.isdir(output_images_dir):\n        os.makedirs(output_images_dir)\n\n    for i_batch, (im_info, input_img, input_gt) in tqdm.tqdm(enumerate(val_data_loader_), total=len(val_data_loader_)):\n        if cuda:\n            im = input_img.cuda(async=True)\n        else:\n            im = input_img\n\n        out_masks = net_(im)\n\n        prediction = torch.sigmoid(out_masks[0])\n        save_pred(im_info, prediction, n_classes, output_images_dir)\n\n    return 0\n\n\ndef main():\n    args = parser.parse_args()\n    print('****')\n    print(args)\n    print('****')\n\n    output_folder = args.output_folder\n    root_dir_val = args.root_dir_val\n    flist_val = args.flist_val\n    ckpt = args.ckpt\n\n    # --\n    n_classes = args.n_classes\n    crop_size_val = 512\n\n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n\n    net = CaseNet101()\n    net = torch.nn.DataParallel(net.cuda())\n\n    # val set\n    val_dataset = dataloader.ValidationDataset(root_dir_val, flist_val, n_classes=n_classes, crop_size=crop_size_val)\n    val_data_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n                                                  batch_size=1,\n                                                  shuffle=False)\n\n    print('loading ckpt :%s' % ckpt)\n    net.load_state_dict(torch.load(ckpt), strict=True)\n\n    do_test_sbd(net, val_data_loader, cuda=True, n_classes=n_classes, output_folder=output_folder)\n\n\nif __name__ == '__main__':\n    main()\n"""
coarse_to_fine/VisualizerBox.py,0,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#  * Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n#  * Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#  * Neither the name of NVIDIA CORPORATION nor the names of its\n#    contributors may be used to endorse or promote products derived\n#    from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS\'\' AND ANY\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nimport numpy as np\nimport os\nimport skimage.measure as measure\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport matplotlib.colors as mcolors\n\ndebug_remote = False\nif debug_remote is None:\n    pass\nelif debug_remote:\n    plt.switch_backend(\'Qt5Agg\')\nelse:\n    plt.switch_backend(\'Agg\')\n\n\nclass VisualizerBox:\n    #\n    def city_pallete(self):\n        CITYSCAPE_PALLETE = np.asarray([\n            [128, 64, 128],\n            [244, 35, 232],\n            [70, 70, 70],\n            [102, 102, 156],\n            [190, 153, 153],\n            [153, 153, 153],\n            [250, 170, 30],\n            [220, 220, 0],\n            [107, 142, 35],\n            [152, 251, 152],\n            [70, 130, 180],\n            [220, 20, 60],\n            [255, 0, 0],\n            [0, 0, 142],\n            [0, 0, 70],\n            [0, 60, 100],\n            [0, 80, 100],\n            [0, 0, 230],\n            [119, 11, 32],\n            [0, 0, 0]], dtype=np.uint8)\n        self.colors_are_a_list = False\n        return CITYSCAPE_PALLETE\n\n    #\n    def sbd_pallete(self):\n        SBD_PALLETE = np.asarray([\n            [128, 0, 0],\n            [0, 128, 0],\n            [128, 128, 0],\n            [0, 0, 128],\n            [128, 0, 128],\n            [0, 128, 128],\n            [128, 128, 128],\n            [64, 0, 0],\n            [192, 0, 0],\n            [64, 128, 0],\n            [192, 128, 0],\n            [64, 0, 128],\n            [192, 0, 128],\n            [64, 128, 128],\n            [192, 128, 128],\n            [0, 64, 0],\n            [128, 64, 0],\n            [0, 192, 0],\n            [128, 192, 0],\n            [0, 64, 128],\n            [0, 0, 0]], dtype=np.uint8)\n        self.colors_are_a_list = False\n        return SBD_PALLETE\n\n    def css4_colors_pallete(self):\n        self.colors_are_a_list = True\n        return list(mcolors.CSS4_COLORS.keys())\n\n    def css4_fushia(self):\n        self.colors_are_a_list = True\n        rrr = list(mcolors.CSS4_COLORS.keys())\n        rrr[13] = \'fuchsia\'  # forcing to this color, remove if need it.\n        return rrr\n\n    def __init__(self, dataset_color, plt_backend=None, fig_size=(8, 12), only_contour=False, postfix_as_name=True):\n        self.colors_are_a_list = False\n        if dataset_color == \'cityscapes\':\n            print(\'dataset color: cityscapes\')\n            self._mycolors = self.city_pallete()\n        elif dataset_color == \'sbd\':\n            print(\'dataset color: sbd\')\n            self._mycolors = self.sbd_pallete()\n        elif dataset_color == \'css4\':\n            print(\'dataset color: css4\')\n            self._mycolors = self.css4_colors_pallete()\n        elif dataset_color == \'css4_fushia\':\n            print(\'dataset color: css4_fushia\')\n            self._mycolors = self.css4_fushia()\n        else:\n            raise ValueError()\n\n        self._output_f = None\n        self._fig_size = fig_size\n        self._only_contour = only_contour\n        self._postfix_as_name = postfix_as_name\n\n        if plt_backend is not None:\n            print(\'Switching backend to %s\' % plt_backend)\n            plt.switch_backend(plt_backend)\n\n    def plot_multichannel_mask(self, ax, masks, remapping_dict=None, ref_contour=None):\n\n        for i in range(masks.shape[0]):\n            mask = masks[i]\n\n            if not np.any(mask):\n                continue\n\n            contours = measure.find_contours(mask, 0)\n            # TODO this is a copying and pasting hack, it can be done properly.\n            if ref_contour is not None and self._only_contour is True:\n                ref_contour_i = measure.find_contours(ref_contour[i], 0)\n                for contour in ref_contour_i:\n                    contour = np.fliplr(contour)\n                    ax.plot(contour[:, 0], contour[:, 1], linewidth=3, color=\'red\')  #\n\n            for contour in contours:\n                contour = np.fliplr(contour)\n                if remapping_dict is None:\n                    c_id = i\n                else:\n                    c_id = remapping_dict[i]\n                if self._only_contour is False:\n                    if self.colors_are_a_list:\n                        color = self._mycolors[c_id]\n                    else:\n                        color = self._mycolors[c_id] / 255.0\n                    p = patches.Polygon(contour, facecolor=color, edgecolor=\'white\', linewidth=0,\n                                        alpha=0.5)\n                    ax.add_patch(p)\n                    ax.plot(contour[:, 0], contour[:, 1], linewidth=2,\n                            color=\'orange\')  # #\n\n                else:\n                    # #\n                    simple_color = \'greenyellow\'\n                    ax.plot(contour[:, 0], contour[:, 1], linewidth=2,\n                            color=simple_color, alpha=1)  #\n        return ax\n\n    def add_vis_list(self, images_dict, background=None, remapping_dict=None, grid=True, merge_channels=True,\n                     exec_fn=None, ref_contour=None):\n        if merge_channels is False:\n            return NotImplementedError()\n\n        if grid is True:\n            f, ax = plt.subplots(len(images_dict.keys()), 1, figsize=self._fig_size)\n        else:\n            f, ax = plt.subplots(1, 1, figsize=self._fig_size)\n\n        if background is None:\n            h, w = images_dict.items()[0].shape[1:3]  # C, H,W\n            background = np.ones((h, w)) * 255\n\n        for i, (title, image_array) in enumerate(images_dict.items()):\n\n            # just in case it was lazy loaded (eg.PIL)\n            image_array = np.array(image_array)\n            if not (type(ax) is np.ndarray):\n                curr_ax = ax\n            elif len(ax) > 1:\n                curr_ax = ax[i]\n            else:\n                curr_ax = ax\n            curr_ax.set_axis_off()\n            curr_ax.set_title(title)\n\n            if image_array.shape[0] == 1:  #\n                curr_ax.imshow(image_array[0])\n                if exec_fn is not None and grid is False:\n                    exec_fn(f, curr_ax, title)\n\n                continue\n\n            # setting the background, usually input image...\n            curr_ax.imshow(np.array(background), alpha=1)\n\n            self.plot_multichannel_mask(curr_ax, image_array, remapping_dict, ref_contour)\n\n            if exec_fn is not None and grid is False:\n                exec_fn(f, curr_ax, title)\n\n        if exec_fn is not None and grid is True:\n            exec_fn(f, ax, \'grid\')\n\n        return f, ax\n\n    def set_output_folder(self, output_f):\n        self._output_f = output_f\n        if self._output_f is not None:\n            if not os.path.isdir(self._output_f):\n                os.makedirs(self._output_f)\n        print(\'Vis Output Dir:\', self._output_f)\n\n    def save_vis(self, image_name, images_dict, background=None, remapping_dict=None, grid=True, auto_show=False,\n                 ref_contour=None):\n        def exec_callback(f, ax, title, **kwargs):\n            assert self._output_f is not None\n            title = title.replace("" "", """").lower()\n            if self._postfix_as_name is True:\n                fname = os.path.join(self._output_f, image_name + \'_\' + title + \'.jpg\')\n            else:\n                fname = os.path.join(self._output_f, image_name + \'.jpg\')\n            f.tight_layout()\n            f.savefig(fname)\n            ax.cla()\n\n        self.add_vis_list(images_dict, background, remapping_dict, grid, exec_fn=exec_callback, ref_contour=ref_contour)\n        if auto_show:\n            plt.show()\n\n        plt.close()\n\n    def visualize(self, images_dict, background=None, remapping_dict=None, grid=True, merge_channels=True):\n        f, ax = self.add_vis_list(images_dict, background, remapping_dict, grid, merge_channels)\n        plt.show()\n'"
coarse_to_fine/input_reader.py,0,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#  * Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n#  * Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#  * Neither the name of NVIDIA CORPORATION nor the names of its\n#    contributors may be used to endorse or promote products derived\n#    from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS\'\' AND ANY\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nimport os\nimport numpy as np\nfrom PIL import Image\nimport scipy.io\nimport random\nimport scipy.sparse\n\nimport scipy.ndimage.morphology as morp\nimport cv2\nfrom skimage.measure import regionprops\nfrom skimage import measure\nfrom collections import OrderedDict\n\n\nclass InputReaderBase:\n    def __init__(self, in_path, file_list, n_classes):\n        """"""\n\n        :param path:\n        :param file_list:\n        :param n_classes: tuple (start,end)\n        """"""\n        self._in_path = in_path\n        self._file_list = file_list\n        if file_list is not None:\n            self._read_list = open(file_list, \'r\').read().splitlines()\n        else:\n            self._read_list = None\n\n        self.n_classes = n_classes\n        self._seed = 123\n        self._classes_to_keep = []\n        self._remapping = OrderedDict()\n\n    def set_classes_to_keep(self, classes_to_keep):\n        self._classes_to_keep = classes_to_keep\n        self._remapping = OrderedDict()\n\n    def set_external_list(self, ext_list):\n        self._read_list = ext_list\n\n    def randompick(self, max_number, seed=123):\n        self._seed = seed\n        random.seed(seed)\n        random.shuffle(self._read_list)\n        self._file_list = \'external\'\n        self._read_list = self._read_list[:max_number]\n\n    def _ignore_classes(self, masks):\n        if len(self._classes_to_keep) == 0:\n            return masks\n        idx = 0\n        result = []\n        for i in range(len(masks)):\n            if i in self._classes_to_keep:\n                mask = masks[i]\n                result.append(mask)\n                self._remapping[idx] = i\n                idx = idx + 1\n        return np.array(result)\n\n    def __getitem__(self, item):\n        raise NotImplementedError()\n\n    def __len__(self):\n        return len(self._read_list)\n\n\nclass InputReader(InputReaderBase):\n    def __getitem__(self, item):\n        stack = []\n        filename = self._read_list[item]\n        for idx_cls in range(self.n_classes[0], self.n_classes[1] + self.n_classes[0]):\n            img_path = os.path.join(self._in_path, \'class_\' + str(idx_cls), filename + \'.png\')\n            img = np.array(Image.open(img_path)) / 255.0\n            stack.append(img)\n        assert len(stack) == self.n_classes[1], len(stack)\n        stack = self._ignore_classes(stack)\n\n        return filename, stack\n\n\nclass InputReaderBaseName(InputReaderBase):\n    def __getitem__(self, item):\n        stack = []\n        filename = self._read_list[item]\n        filename = os.path.basename(filename).split(\'.png\')[0]\n        for idx_cls in range(self.n_classes[0], self.n_classes[1] + self.n_classes[0]):\n            img_path = os.path.join(self._in_path, \'class_\' + str(idx_cls), filename + \'.png\')\n            img = np.array(Image.open(img_path)) / 255.0\n            stack.append(img)\n        assert len(stack) == self.n_classes[1], len(stack)\n        stack = self._ignore_classes(stack)\n\n        return filename, stack\n\n\nclass InputReaderDummy(InputReaderBase):\n    def __getitem__(self, item):\n        filename = self._read_list[item]\n        return filename, [None for _ in range(self.n_classes[1])]\n\n\nclass InputReaderRGBImage(InputReaderBase):\n    def __getitem__(self, item):\n        filename = self._read_list[item]\n        img_path = os.path.join(self._in_path, \'img\', filename + \'.png\')\n\n        if not os.path.isfile(img_path):\n            img_path = os.path.join(self._in_path, \'img\', filename + \'.jpg\')\n\n        img = Image.open(img_path)\n        return filename, img\n\n\nclass InputReaderSemMat(InputReaderBase):\n    @staticmethod\n    def map2Kchannels(maps, klasses):\n        masks = [maps == (i + 1) for i in range(klasses)]\n        return np.stack(masks, axis=0).astype(np.uint8)\n\n    def __getitem__(self, item):\n        filename = self._read_list[item]\n        mat_path = os.path.join(self._in_path, filename + \'.mat\')\n        matlab = scipy.io.loadmat(mat_path)\n        segmap = matlab[\'GTcls\'][\'Segmentation\'][0][0]\n        masks = self.map2Kchannels(segmap, self.n_classes[1])\n        assert len(masks) == self.n_classes[1], len(masks)\n        masks = self._ignore_classes(masks)\n\n        return filename, masks\n\n\nclass InputReaderSemMat2(InputReaderSemMat):\n    def __getitem__(self, item):\n        filename = self._read_list[item]\n        mat_path = os.path.join(self._in_path, \'cls\', filename + \'.mat\')\n        matlab = scipy.io.loadmat(mat_path)\n        segmap = matlab[\'GTcls\'][\'Segmentation\'][0][0]\n        masks = self.map2Kchannels(segmap, self.n_classes[1])\n        assert len(masks) == self.n_classes[1], len(masks)\n        masks = self._ignore_classes(masks)\n        return filename, masks\n\n\nclass InputReaderSemMatDemo(InputReaderSemMat):\n    def __getitem__(self, item):\n        mat_path = self._read_list[item]\n        matlab = scipy.io.loadmat(mat_path)\n        segmap = matlab[\'GTcls\'][\'Segmentation\'][0][0]\n        masks = self.map2Kchannels(segmap, self.n_classes[1])\n        assert len(masks) == self.n_classes[1], len(masks)\n        masks = self._ignore_classes(masks)\n        return os.path.basename(mat_path), masks\n\n\nclass InputReaderSemMat2BaseName(InputReaderSemMat):\n    def __getitem__(self, item):\n        filename = self._read_list[item]\n        filename = os.path.basename(filename).split(\'.png\')[0]\n\n        mat_path = os.path.join(self._in_path, \'cls\', filename + \'.mat\')\n        matlab = scipy.io.loadmat(mat_path)\n        segmap = matlab[\'GTcls\'][\'Segmentation\'][0][0]\n        masks = self.map2Kchannels(segmap, self.n_classes[1])\n        assert len(masks) == self.n_classes[1], len(masks)\n        masks = self._ignore_classes(masks)\n        return filename, masks\n\n\nclass InputReaderSemMatCoarse(InputReaderBase):\n    def __init__(self, in_path, file_list, n_classes, delta):\n        super(InputReaderSemMatCoarse, self).__init__(in_path, file_list, n_classes)\n        self._delta = delta\n\n    def map2Kchannels(self, maps, klasses):\n        masks = []\n        idx = 0\n        for i in range(klasses):\n            if (len(self._classes_to_keep) == 0) or (i in self._classes_to_keep):\n                mask = (maps == (i + 1))\n                coarse_mask = self._simulate_coarse_label(mask, self._delta)\n                masks.append(coarse_mask)\n                self._remapping[idx] = i\n                idx = idx + 1\n\n        return np.stack(masks, axis=0).astype(np.uint8)\n\n    def _simulate_coarse_label(self, mask, delta, return_polys=False):\n        all_zeros = not np.any(mask)\n        if all_zeros:\n            return mask\n\n        erosion_iter = delta // 2\n        eroded_mask = morp.binary_erosion(mask, iterations=erosion_iter)\n\n        if not np.any(eroded_mask):\n            eroded_mask = eroded_mask.astype(np.uint8)\n            # if the objects are too small and the delta (erosion) is to big, the objects may disapear\n            # if all disappear\n            # so let\'s draw a circle\n            properties = regionprops(mask.astype(np.int32))\n            c_y, c_x = properties[0].centroid\n            c_y = int(c_y)\n            c_x = int(c_x)\n            # --\n            cv2.circle(eroded_mask, (c_x, c_y), 3, 1, -1)\n            return eroded_mask\n\n        polys = measure.find_contours(eroded_mask, 0)\n\n        if len(polys) == 0:\n            print(\'error getting poly..returning empty mask\')\n            return np.zeros_like(mask)\n\n        final_mask = np.zeros_like(mask).astype(np.uint8).T\n        final_mask = np.ascontiguousarray(final_mask)\n        for poly in polys:\n            result = cv2.approxPolyDP(poly.astype(np.int32), erosion_iter, True)[:, 0, :]\n            cv2.fillPoly(final_mask, [result], 1)\n\n        final_mask = final_mask.T\n\n        if not np.any(final_mask):\n            print(\'this mask ended up being empty\')\n\n        return final_mask\n\n    def __getitem__(self, item):\n        filename = self._read_list[item]\n        mat_path = os.path.join(self._in_path, \'cls\', filename + \'.mat\')\n        matlab = scipy.io.loadmat(mat_path)\n        segmap = matlab[\'GTcls\'][\'Segmentation\'][0][0]\n\n        # this function inside does ignore classes... doing this way for speed\n        masks = self.map2Kchannels(segmap, self.n_classes[1])\n\n        return filename, masks\n\n\nclass InputReaderSemMatCoarsePerComponent(InputReaderSemMatCoarse):\n\n    def getSim_fn(self):\n        return None\n\n    def _simulate_coarse_label(self, mask, delta):\n        all_zeros = not np.any(mask)\n        if all_zeros:\n            return mask\n\n        blobs_labels, n_blobs = measure.label(mask, background=0, return_num=True)\n\n        simp_output = np.zeros_like(mask)\n\n        fn_callback = self.getSim_fn()\n        for blob_id in range(1, n_blobs + 1):\n            mask_blob = (blobs_labels == blob_id)\n            mask_blob = np.ascontiguousarray(mask_blob, np.uint8)\n\n            if fn_callback is None:\n                sim_blob = super(InputReaderSemMatCoarsePerComponent, self)._simulate_coarse_label(mask_blob, delta)\n            else:\n                sim_blob = fn_callback(mask_blob, delta)\n\n            simp_output = simp_output + sim_blob\n\n        #\n\n        return simp_output\n\n\nclass InputReaderSemMatClickSim(InputReaderSemMatCoarsePerComponent):\n    def _simulate_click(self, mask, delta):\n        all_zeros = not np.any(mask)\n        if all_zeros:\n            return mask\n        radius = 6\n        sim_mask = np.zeros_like(mask)\n        properties = regionprops(mask.astype(np.int32))\n        c_y, c_x = properties[0].centroid\n\n        c_y = c_y + delta * np.random.normal()\n        c_x = c_x + delta * np.random.normal()\n\n        c_y = max(c_y, 0)\n        c_x = max(c_x, 0)\n\n        c_y = int(c_y)\n        c_x = int(c_x)\n        # --\n        cv2.circle(sim_mask, (c_x, c_y), delta, 1, -1)\n        return sim_mask\n\n    def getSim_fn(self):\n        return self._simulate_click\n\n    def _simulate_coarse_label(self, mask, delta):\n        return super(InputReaderSemMatClickSim, self)._simulate_coarse_label(mask, delta)\n\n\nclass InputReaderBdryMat(InputReaderBase):\n    def __getitem__(self, item):\n        filename = self._read_list[item]\n        mat_path = os.path.join(self._in_path, filename + \'.mat\')\n        matlab = scipy.io.loadmat(mat_path)\n        boundaries = matlab[\'GTcls\'][\'Boundaries\']\n        masks = [scipy.sparse.csr_matrix.todense(boundaries[0][0][c_id][0]) for c_id in range(0, 20)]\n\n        assert len(masks) == self.n_classes[1], len(masks)\n\n        return filename, masks\n'"
coarse_to_fine/refine_cityscapes_coarse.py,0,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#  * Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n#  * Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#  * Neither the name of NVIDIA CORPORATION nor the names of its\n#    contributors may be used to endorse or promote products derived\n#    from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS\'\' AND ANY\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nimport tqdm\nfrom coarse_to_fine.input_reader import InputReader, InputReaderBaseName, InputReaderSemMat2BaseName, InputReaderSemMat2\nfrom contours import ContourBox\nimport argparse\nimport ast\nimport numpy as np\nimport os\nfrom PIL import Image\n\n\nclass GenerateGT_PNGMask:\n    def __init__(self, classes_to_keep, output_dir):\n        self.classes_to_keep = classes_to_keep  # Object classes: [11, 12, 13, 14, 15, 16, 17, 18]\n        self.output_dir = output_dir\n\n    def _save_fname(self, filename):\n        city_name = filename.split(\'_\')[0]\n        gt_name = filename.split(\'_leftImg8bit\')[0] + \'gtCoarseR_labelIds.png\'\n\n    def generate_save(self, gt, improved, filename):\n        only_objects_updated, fully_improved = self._generate_single_mask(gt, improved)\n\n        city_name = filename.split(\'_\')[0]\n        gt_name_objects = filename.split(\'_leftImg8bit\')[0] + \'_gtCoarseRefObj_labelIds.png\'\n        gt_name_alls = filename.split(\'_leftImg8bit\')[0] + \'_gtCoarseRefAll_labelIds.png\'\n        output_dir = os.path.join(self.output_dir, city_name)\n        if not os.path.isdir(output_dir):\n            os.makedirs(output_dir)\n\n        gt_name_alls = os.path.join(output_dir, gt_name_alls)\n\n        fully_improved = Image.fromarray(fully_improved)\n        fully_improved.save(gt_name_alls, \'png\')\n\n        gt_name_objects = os.path.join(output_dir, gt_name_objects)\n\n        only_objects_updated = Image.fromarray(only_objects_updated)\n        only_objects_updated.save(gt_name_objects, \'png\')\n\n    def _generate_single_mask(self, gt, improved):\n        final_canvas = np.zeros(gt.shape[1:]).astype(np.uint8)\n        all_updated_canvas = np.zeros(gt.shape[1:]).astype(np.uint8)\n\n        for k, (gt_k, improved_k) in enumerate(zip(gt, improved), start=0):\n\n            if k not in self.classes_to_keep:\n                if np.any(gt_k):\n                    final_canvas[gt_k != 0] = k\n            else:\n                if np.any(improved_k) and np.any(gt_k):\n                    final_canvas[improved_k != 0] = k\n\n            all_updated_canvas[improved_k != 0] = k\n\n        #\n\n        return final_canvas, all_updated_canvas\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--coarse_dir\', type=str,\n                        default=\'./cityscapes-preprocess/gt_eval_coarse/gt_thin\')\n\n    parser.add_argument(\'--in_dir\', type=str,\n                        default=\'./prediction_scaled_0_5\')\n\n    parser.add_argument(\'--val_file_list\', type=str,\n                        default=\'./Cityscapes/benchmark/datadir/val.txt\')\n\n    parser.add_argument(\'--n_classes\', type=int, default=19)\n    parser.add_argument(\'--n_classes_start\', type=int, default=1)\n    parser.add_argument(\'--level_set_method\', type=str, default=\'MLS\')\n    parser.add_argument(\'--level_set_config_dict\', type=dict, default={})\n    # ---\n    parser.add_argument(\'--n_workers\', type=int, default=8)\n    parser.add_argument(\'--smooth_lsteps\', type=int, default=1)\n    parser.add_argument(\'--lambda_\', type=float, default=0.0)\n    parser.add_argument(\'--alpha\', type=float, default=1.0)\n    parser.add_argument(\'--step_ckpts\', type=str, default=""[0,60]"")\n    parser.add_argument(\'--exp_name\', type=str, default=\'test\')\n    parser.add_argument(\'--output_dir\', type=str, default=\'./refined_data_test\')\n    parser.add_argument(\'--classes_to_keep\', type=list, default=[])\n    parser.add_argument(\'--balloon\', type=float, default=1)\n    parser.add_argument(\'--threshold\', type=float, default=0.99)\n    parser.add_argument(\'--merge_weight\', type=float, default=0.5)\n\n    args = parser.parse_args()\n\n    level_set_config_dict = {\n        \'lambda_\': args.lambda_,\n        \'alpha\': args.alpha,\n        \'smoothing\': args.smooth_lsteps,\n        \'render_radius\': -1,\n        \'is_gt_semantic\': True,\n        \'method\': args.level_set_method,\n        \'balloon\': args.balloon,\n        \'threshold\': args.threshold,\n        \'merge_weight\': args.merge_weight,\n        \'step_ckpts\': ast.literal_eval(args.step_ckpts)\n    }\n\n    args.level_set_config_dict = level_set_config_dict\n\n    return args\n\n\ndef do_it(args):\n    in_dir = args.in_dir\n    val_file_list = args.val_file_list\n    coarse_dir = args.coarse_dir\n    n_classes_interval = (args.n_classes_start, args.n_classes)\n    level_set_config_dict = args.level_set_config_dict\n\n    classes_to_keep = [11, 12, 13, 14, 15, 16, 17, 18]  # args.classes_to_keep\n\n    ireader = InputReaderBaseName(in_dir, val_file_list, n_classes_interval)\n    #\n\n    ireader_coarse = InputReaderSemMat2BaseName(coarse_dir, val_file_list, n_classes_interval)\n    #\n    ireader_coarse.set_external_list(ireader._read_list)\n\n    cbox = ContourBox.LevelSetAlignment(n_workers=1,\n                                        fn_post_process_callback=None,\n                                        config=level_set_config_dict)\n\n    mask_generator = GenerateGT_PNGMask(classes_to_keep, args.output_dir)\n\n    for (im_filename, pred_ch), (seg_fname, seg_coarse) in tqdm.tqdm(\n            zip(ireader, ireader_coarse), total=len(ireader)):\n        assert len(pred_ch) == len(seg_coarse), \'num ch should match\'\n\n        output, _ = cbox({\'seg\': np.expand_dims(seg_coarse, 0), \'bdry\': None},\n                         np.expand_dims(np.stack(pred_ch), 0))\n        # assuming the last ckpts is the one we are going to use\n        improved_mask = output[0, :, -1, :, :]\n\n        seg_coarse = np.stack(seg_coarse)\n        mask_generator.generate_save(seg_coarse, improved_mask, seg_fname)\n\n\nif __name__ == ""__main__"":\n    args = parse_args()\n    do_it(args)\n'"
coarse_to_fine/refine_segmentation.py,0,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#  * Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n#  * Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#  * Neither the name of NVIDIA CORPORATION nor the names of its\n#    contributors may be used to endorse or promote products derived\n#    from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS\'\' AND ANY\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nimport numpy as np\n\nfrom coarse_to_fine.input_reader import InputReader, InputReaderSemMat2, InputReaderRGBImage\nimport os\nimport tqdm\nimport argparse\nfrom contours import ContourBox\nimport ast\nfrom coarse_to_fine.VisualizerBox import VisualizerBox\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--val_dir\', type=str,\n                        default=\'./sbd_reannotation\')\n    parser.add_argument(\'--coarse_dir\', type=str,\n                        default=\'\')\n\n    parser.add_argument(\'--in_dir\', type=str,\n                        default=\'./edge_predictions/val_images/\')\n\n    parser.add_argument(\'--image_dir\', type=str,\n                        default=\'./sbd/data_orig/benchmark_RELEASE/dataset\')\n\n    parser.add_argument(\'--val_file_list\', type=str, default=\'\')\n    parser.add_argument(\'--n_classes\', type=int, default=20)\n    parser.add_argument(\'--n_classes_start\', type=int, default=1)\n    parser.add_argument(\'--alignment\', action=\'store_false\')\n    parser.add_argument(\'--level_set_method\', type=str, default=\'MLS\')\n    parser.add_argument(\'--level_set_config_dict\', type=dict, default={})\n    parser.add_argument(\'--eval_config\', type=dict, default={})\n    parser.add_argument(\'--random_pick\', type=int, default=10)\n    # ---\n    parser.add_argument(\'--n_workers\', type=int, default=8)\n    parser.add_argument(\'--max_lsteps\', type=int, default=10)\n    parser.add_argument(\'--smooth_lsteps\', type=int, default=1)\n    parser.add_argument(\'--lambda_\', type=float, default=1.0)\n    parser.add_argument(\'--alpha\', type=float, default=100.0)\n    parser.add_argument(\'--step_ckpts\', type=str, default=\'\')\n    parser.add_argument(\'--middle_step\', type=int, default=-1)\n    parser.add_argument(\'--sim_coarse\', action=\'store_true\')\n    parser.add_argument(\'--per_component_sim\', action=\'store_true\')\n    parser.add_argument(\'--delta_coarse\', type=int, default=4)\n    parser.add_argument(\'--exp_name\', type=str, default=\'test\')\n    parser.add_argument(\'--output_dir\', type=str, default=\'./output/refinement\')\n    parser.add_argument(\'--vis_steps\', type=str, default="""")\n    parser.add_argument(\'--classes_to_keep\', type=list, default=[])\n    parser.add_argument(\'--dataset\', type=str, default="""")\n    parser.add_argument(\'--balloon\', type=float, default=0)\n    parser.add_argument(\'--threshold\', type=float, default=0.95)\n    parser.add_argument(\'--auto_show\', action=\'store_true\')\n    parser.add_argument(\'--merge_weight\', type=float, default=0)\n    parser.add_argument(\'--per_component_click_sim\', action=\'store_true\')\n\n    args = parser.parse_args()\n\n    if args.alignment is True:\n        level_set_config_dict = {\n            \'step_ckpts\': [0, args.max_lsteps // 2, args.max_lsteps],  # , 25),\n            \'lambda_\': args.lambda_,\n            \'alpha\': args.alpha,\n            \'smoothing\': args.smooth_lsteps,\n            \'render_radius\': -1,\n            \'is_gt_semantic\': True,\n            \'method\': args.level_set_method,\n            \'balloon\': args.balloon,\n            \'threshold\': args.threshold,\n            \'merge_weight\': args.merge_weight\n        }\n\n        if args.middle_step > 0:\n            level_set_config_dict[\'step_ckpts\'][1] = args.middle_step\n\n        if args.step_ckpts != \'\':\n            level_set_config_dict[\'step_ckpts\'] = ast.literal_eval(args.step_ckpts)\n\n        args.level_set_config_dict = level_set_config_dict\n    if args.dataset == \'cityscapes\':\n        # only objects\n        args.classes_to_keep = [11, 12, 13, 14, 15, 16, 17, 18]\n    else:\n        args.classes_to_keep = []\n    return args\n\n\ndef prepair_contour_box(args):\n    level_set_config_dict = args.level_set_config_dict\n    method = args.level_set_config_dict[\'method\']\n    if method == \'MLS\':\n        raise ValueError()\n\n    cbox = ContourBox.LevelSetAlignment(n_workers=1,\n                                        fn_post_process_callback=None,\n                                        config=level_set_config_dict)\n    return cbox\n\n\ndef main(args):\n    val_dir = args.val_dir\n    in_dir = args.in_dir\n    n_classes_interval = (args.n_classes_start, args.n_classes)\n    vis_steps = args.vis_steps\n\n    if vis_steps == """":\n        vis_steps = []\n        vis_box = None\n    else:\n        print(\'Creating VisualizerBOX\')\n        vis_steps = ast.literal_eval(vis_steps)\n        vis_box = VisualizerBox(dataset_color=\'css4_fushia\', plt_backend=\'Qt5Agg\' if args.auto_show else None,\n                                fig_size=(20, 10))\n\n        vis_box.set_output_folder(os.path.join(args.output_dir, args.exp_name, \'vis\'))\n\n    if args.val_file_list == \'\':\n        val_file_list = os.path.join(val_dir, \'val.txt\')\n    else:\n        val_file_list = args.val_file_list\n\n    ireader = InputReader(in_dir, val_file_list, n_classes_interval)\n    ireader.set_classes_to_keep(args.classes_to_keep)\n\n    if args.random_pick > 0:\n        ireader.randompick(args.random_pick)\n\n    # getting the reader for seg from matlab\n    if args.sim_coarse is False:\n        print(\'Using Real Coarse Data from :\', args.coarse_dir)\n        if not os.path.isdir(args.coarse_dir):\n            raise ValueError(\'not dir found\')\n        #\n        ireader_coarse_sim = InputReaderSemMat2(args.coarse_dir, val_file_list, n_classes_interval)\n        ireader_coarse_sim.set_external_list(ireader._read_list)\n        ireader_coarse_sim.set_classes_to_keep(args.classes_to_keep)\n    else:\n        raise ValueError()\n\n    irader_semantic_init = InputReaderSemMat2(val_dir, val_file_list, n_classes_interval)\n    irader_semantic_init.set_external_list(ireader._read_list)\n    irader_semantic_init.set_classes_to_keep(args.classes_to_keep)\n\n    ireader_rgb_img = InputReaderRGBImage(args.image_dir, val_file_list, n_classes_interval)\n    ireader_rgb_img.set_external_list(ireader._read_list)\n    ireader_rgb_img.set_classes_to_keep(args.classes_to_keep)\n\n    # getting the reader for bdry from matlab\n    cbox = prepair_contour_box(args)\n\n    debug_output_dict = {}\n    debug_ = False\n    for (im_filename, pred_ch), (seg_fname, seg_coarse), (rgb_name, rgb_image), (\n            seg_init_name, seg_init_ch) in tqdm.tqdm(\n        zip(ireader, ireader_coarse_sim, ireader_rgb_img, irader_semantic_init), total=len(ireader)):\n\n        assert len(pred_ch) == len(seg_coarse) == len(seg_init_ch), \'num ch should match\'\n        assert seg_fname == im_filename == rgb_name == seg_init_name, \'this should match\'\n\n        if len(pred_ch) == 0:\n            print(\'skipping image: \', im_filename)\n            continue\n\n        ##checking the input images are not full resolution on cityscapes (for speed purposes)...\n        w_, h_ = rgb_image.size\n        if w_ == 2048:\n            rgb_image = rgb_image.resize((int(w_ * 0.5), int(h_ * 0.5)))\n\n        assert pred_ch[0].shape == seg_coarse[0].shape == tuple(reversed(rgb_image.size)) == seg_init_ch[\n            0].shape, \'spatial dim should match\'\n\n        #\n        output, _ = cbox({\'seg\': np.expand_dims(seg_coarse, 0), \'bdry\': None},\n                         np.expand_dims(np.stack(pred_ch), 0))\n\n        # let\'s cast this, we may get a torch tensor\n        output = np.array(output)\n        if debug_ is True:\n            debug_output_dict[im_filename] = np.copy(output)\n\n        if vis_box is not None:\n            plot_pairs = {\'Fine Label\': seg_init_ch,\n                          \'Semantic Edges\': np.max(pred_ch, axis=0, keepdims=True),\n                          \'Real Coarse Label\': seg_coarse}\n\n            for vis_step in vis_steps:\n                masks_step = output[0, :, vis_step, :, :]\n                vis_step = args.level_set_config_dict[\'step_ckpts\'][vis_step]\n                plot_pairs[\'Refinement (Step:%i)\' % vis_step] = masks_step\n\n            vis_box.save_vis(im_filename, plot_pairs, background=rgb_image,\n                             remapping_dict=ireader_coarse_sim._remapping,\n                             auto_show=args.auto_show, grid=False)\n\n    #\n    if not os.path.isdir(args.output_dir):\n        os.makedirs(args.output_dir)\n\n    name = os.path.join(args.output_dir, \'output.npz\')\n\n    np.savez_compressed(name,\n                        {\'_remapping\': ireader_coarse_sim._remapping})\n\n    print(\'----\')\n\n\nif __name__ == ""__main__"":\n    args = parse_args()\n    main(args)\n'"
contours/ContourBox.py,0,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#  * Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n#  * Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#  * Neither the name of NVIDIA CORPORATION nor the names of its\n#    contributors may be used to endorse or promote products derived\n#    from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS\'\' AND ANY\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nimport contours.cutils as cutils\n\n\nclass LevelSetAlignmentBase:\n    def __init__(self, fn_post_process_callback=None, n_workers=1, fn_debug=None, config=None):\n        """"""\n\n        :param fn_post_process_callback: function signature fn(evolution, pixel_wise_evol), does postprocessing inside the thread\n        :param n_workers: number of worker.\n        :param fn_debug: usually a function fn(image,str)... maybe a wrapper to plot.imshow(image,title)\n        """"""\n        self.fn_post_process_callback = fn_post_process_callback\n        self.ignore_labels = (255,)\n        self.n_workers = n_workers\n\n        if config is None:\n            self.options_dict = {\n                \'step_ckpts\': (0, 25, 50),\n                \'lambda_\': 0.2,\n                \'alpha\': 1.0,\n                \'smoothing\': 2,\n                \'render_radius\': 2,\n                \'is_gt_semantic\': True,\n                \'h_callback\': cutils.compute_h_additive,\n                \'method\': \'MLS\'\n            }\n        else:\n            self.options_dict = config\n        self.fn_debug = fn_debug\n        self.history = None\n        print(\' LevelSetAlignment config: \', self.options_dict)\n\n    def _compute_h(self, gt_K, pK_Image, lambda_, alpha):\n        if ((\'h_callback\' in self.options_dict) == True) and (self.options_dict[\'h_callback\'] is not None):\n            _fn = self.options_dict[\'h_callback\']\n        else:\n            _fn = cutils.compute_h_additive  # ...it should raise value error leaving like this to avoid breaking old experiment\n\n        return _fn(gt_K, pK_Image, lambda_, alpha)\n\n    def __call__(self, gt, pk):\n        raise NotImplementedError()\n\n\ndef LevelSetAlignment(fn_post_process_callback=None, n_workers=1, fn_debug=None, config=None, method=None):\n    import contours.ContourBox_MLS\n    _LevelSets = {\n        \'MLS\': contours.ContourBox_MLS.MLS,\n    }\n\n    if method is not None:\n        clss_cllback = _LevelSets[method]\n    elif config is None:\n        clss_cllback = _LevelSets[\'MLS\']\n    else:\n        clss_cllback = _LevelSets[config[\'method\']]\n    print(\'LevelSet Alignment n_workers: \', n_workers)\n    return clss_cllback(fn_post_process_callback, n_workers, fn_debug, config)\n'"
contours/ContourBox_MLS.py,2,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#  * Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n#  * Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#  * Neither the name of NVIDIA CORPORATION nor the names of its\n#    contributors may be used to endorse or promote products derived\n#    from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS\'\' AND ANY\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nfrom contours.morph_snakes import morphological_geodesic_active_contour\nfrom scipy.ndimage.morphology import binary_fill_holes\nimport numpy as np\nimport multiprocessing as mp\nfrom contours.cutils import seg2edges, update_callback_in_image, compute_h_additive\nfrom contours.ContourBox import LevelSetAlignmentBase\nimport torch\nimport warnings\n\n# this is a workaround to avoid creating/copying the two big arrays to a shared memory area for multi-cpu.\n# it only works on POSIX-compliant OS (linux,OSX) and it assumes there is no need for a lock.\n# this data is read only.\nshared_mem_data = None\n\n\nclass MLS(LevelSetAlignmentBase):\n    def _fill_inside(self, bdry, method=\'fill_holes\'):\n\n        if method == \'fill_holes\':\n            seg = binary_fill_holes(bdry)\n            return seg\n        else:\n            raise ValueError(\'_fill_inside wrong method:%s\' % method)\n\n    def _eval_singleK(self, gt_K, pK_Image, step_ckpts, lambda_, alpha, smoothing,\n                      render_radius, is_gt_semantic, weight_canvas=None, **kwargs):\n\n        def store_evolution_in(lst):\n            """"""Returns a callback function to store the evolution of the level sets in\n            the given list.\n            """"""\n\n            def _store(x, i):\n                if i in step_ckpts:\n                    lst.append(np.copy(x))\n\n            return _store\n\n        # This way of checking mostly cares about speed. as I am assuming the whole GT is very sparse.\n\n        all_zeros = not np.any(gt_K)\n        # nothing to do\n        if all_zeros:\n            out = [gt_K for _ in range(len(step_ckpts))]\n            return [out, out]\n\n        # let\'s remove for now the ignore areas\n        ignore_dict = {}\n        #\n        for ignore_id in self.ignore_labels:\n            idxs = np.nonzero(gt_K == ignore_id)\n            gt_K[idxs] = 0.\n            ignore_dict[ignore_id] = idxs\n\n        # let\'s check again for zeros.\n        all_zeros = not np.any(gt_K)  #\n\n        # nothing to do\n        if all_zeros:\n            # ignore areas back\n            for k, v in ignore_dict.items():\n                gt_K[v] = k\n            out = [gt_K for _ in range(len(step_ckpts))]\n            zero_pp = np.zeros_like(gt_K)\n            out2 = [zero_pp for _ in range(len(step_ckpts))]\n            return [out, out2]\n\n        if is_gt_semantic is False:\n            # filling inside_ to represent the curve\n            # this may have problem with boundaries that are not closed(corners of the image dimension)\n            # be careful!!\n            init_ls = self._fill_inside(gt_K)\n        else:\n            init_ls = gt_K\n            gt_K = seg2edges(gt_K, radius=render_radius, label_ignores=self.ignore_labels)\n\n        if self.fn_debug is not None:\n            self.fn_debug(init_ls, \'init_ls\')\n\n        # List with intermediate results to save the evolution\n        evolution = []\n        callback = store_evolution_in(evolution)\n\n        if weight_canvas is not None:\n            if \'merge_weight\' in self.options_dict:\n                if self.options_dict[\'merge_weight\'] > 0:\n                    pK_Image = pK_Image + weight_canvas\n\n        h = self._compute_h(gt_K, pK_Image, lambda_, alpha)\n\n        if self.fn_debug is not None:\n            self.fn_debug(h, \'h\')\n\n        #\n        n_iterations = step_ckpts[-1]  # equal to my last checkpoint\n\n        if \'balloon\' in kwargs:\n            balloon = kwargs[\'balloon\']\n        else:\n            balloon = 0\n\n        if \'threshold\' in kwargs:\n            threshold = kwargs[\'threshold\']\n        else:\n            threshold = 0\n\n        morphological_geodesic_active_contour(h, n_iterations, init_ls,\n                                              smoothing=smoothing, balloon=balloon,\n                                              threshold=threshold,\n                                              iter_callback=callback)\n\n        pixel_wise_evol = [seg2edges(evol, radius=render_radius, label_ignores=self.ignore_labels) for evol in\n                           evolution]\n\n        if 0 in step_ckpts:\n            # appending the original gt_K\n            # pixel_wise_evol.append(gt_K)\n            # evolution.append(init_ls)\n            # this can be appended it at the end as above... saving the shifting of the array.\n            # but I want to visualize\n            pixel_wise_evol.insert(0, gt_K)\n            evolution.insert(0, init_ls)\n        else:\n            pass\n            # ...\n\n        ##bringing back the ignored areas back\n        for k, v in ignore_dict.items():\n            for j in range(len(pixel_wise_evol)):\n                pixel_wise_evol[j][v] = k\n\n        if self.fn_post_process_callback is None:\n            return [pixel_wise_evol, pixel_wise_evol]\n        else:\n            return [pixel_wise_evol, self.fn_post_process_callback(evolution, pixel_wise_evol)]\n\n    #\n    def process_batch_fn(self, args):\n        i, K, gt, pk, weight_canvas = args\n        gt = gt[i]\n        pk = pk[i]\n        weightk_canvas = weight_canvas[i]\n\n        gt_hat = []\n        gt_hat_pp = []\n\n        for j in range(K):\n            gtk = gt[j]\n            gt_hat_k, gt_hat_pp_k = self._eval_singleK(gtk, pk[j], weight_canvas=weightk_canvas[j], **self.options_dict)\n            gt_hat.append(gt_hat_k)\n\n            if self.fn_post_process_callback is None:\n                gt_hat_pp.append(None)\n            else:\n                gt_hat_pp.append(gt_hat_pp_k)\n\n        return i, [np.stack(gt_hat, axis=0), np.stack(gt_hat_pp, axis=0)]  # KxLStepsxHxW\n\n    def process_batch_hack_multicpu(self, args):\n        i, K, mem_id_gt, mem_id_pk, mem_id_weight_canvas = args\n\n        if shared_mem_data is None:\n            raise ValueError()\n\n        gt, pk, weight_canvas = shared_mem_data[0], shared_mem_data[1], shared_mem_data[2]\n\n        if mem_id_gt != id(gt):\n            raise ValueError(\'error seems the memory id are not the same, not shared array... is this linux?\')\n\n        if mem_id_pk != id(pk):\n            raise ValueError(\'error seems the memory id are not the same, not shared array... is this linux?\')\n\n        if mem_id_weight_canvas != id(weight_canvas):\n            raise ValueError(\'error seems the memory id are not the same, not shared array... is this linux?\')\n\n        # seems all is fine...let\'s do it.\n        args2 = (i, K, gt, pk, weight_canvas)\n        return self.process_batch_fn(args2)\n\n    def _multi_cpu_call(self, gt, pk):\n        assert gt.shape == pk.shape\n        global shared_mem_data\n        shared_mem_data = (gt, pk)\n        N, K, H, W = pk.shape\n        #\n        pool = mp.Pool(min(N, self.n_workers))\n\n        output_ = pool.map(self.process_batch_hack_multicpu, [(i, K, id(gt), id(pk)) for i in range(N)])\n\n        pool.close()\n        pool.join()\n\n        return output_\n\n    def _multi_cpu_call_2(self, gt, pk, weight_canvas=None):\n        """"""\n        -->...similar to above but the workers can be reused in N*K elements instead of just K\n        :param gt:\n        :param pk:\n        :return:\n        """"""\n\n        assert gt.shape == pk.shape\n        N, K, H, W = pk.shape\n\n        gt = np.reshape(gt, [N * K, 1, H, W])\n        pk = np.reshape(pk, [N * K, 1, H, W])\n\n        if weight_canvas is not None:\n            weight_canvas = np.reshape(weight_canvas, [N * K, 1, H, W])\n        else:\n            weight_canvas = np.zeros_like(pk)\n\n        global shared_mem_data\n        shared_mem_data = (gt, pk, weight_canvas)\n\n        #\n        pool = mp.Pool(min(N * K, self.n_workers))\n\n        output_ = pool.map(self.process_batch_hack_multicpu,\n                           [(i, 1, id(gt), id(pk), id(weight_canvas)) for i in range(N * K)])\n\n        pool.close()\n        pool.join()\n        # we need to reorder the array to make it compatible with the rest of the api.\n\n        return output_\n\n    def merge_everyone_but_me(self, pks_, me):\n        canvas = np.zeros_like(pks_[0])\n        for k in range(pks_.shape[0]):\n            if k == me:\n                continue\n            canvas += pks_[k]\n\n        canvas = canvas / (np.max(canvas) + 1e-6)\n        return canvas\n\n    def weigthing_merge(self, weight, pks):\n        N, K, H, W = pks.shape\n        output = np.zeros_like(pks)\n        for n in range(N):\n            item = pks[n]\n            for k in range(K):\n                # TODO this is quadratic for now in k :((, do properly.\n                canvas = self.merge_everyone_but_me(item, k)\n                output[n, k] = weight * canvas\n        return output\n\n    def __call__(self, gt_dict, pk):\n\n        if not isinstance(gt_dict, dict):\n            raise ValueError(\'wrong param\')\n\n        gt = gt_dict[\'seg\']\n        if isinstance(gt, torch.Tensor):\n            gt = gt.byte().cpu().float().numpy()\n\n        if isinstance(pk, torch.Tensor):\n            pk = pk.cpu().numpy()\n\n        assert gt.shape == pk.shape\n        N, K, H, W = pk.shape\n\n        weight_canvas = None\n        if \'merge_weight\' in self.options_dict:\n            weights = self.options_dict[\'merge_weight\']\n            if weights >= 0:\n                weight_canvas = self.weigthing_merge(weights, pk)\n            elif weights == 0:\n                print(\'skipping...\')\n            else:\n                raise ValueError(\'negative ?, this shouldnt happen\')\n\n        if N * K > 1 and self.n_workers > 1:\n            output_ = self._multi_cpu_call_2(gt, pk, weight_canvas)\n        else:\n            output_ = []\n            for i in range(N):\n                idx, gt_hat = self.process_batch_fn((i, K, gt, pk, weight_canvas))\n                output_.append((idx, gt_hat))\n\n        # checking batch order ... it shouldnt be needed it, but  dont wanna have nasty surprises.\n        # maybe remove in the future, remove idx from the output in process_batch_fn and this is not needed.\n\n        verified_output = []\n        verified_output_pp = []\n        for i in range(len(output_)):\n            assert output_[i][0] == i\n\n            output_i1 = output_[i][1][0]\n            verified_output.append(output_i1)\n\n            if self.fn_post_process_callback is not None:\n                verified_output_pp.append(output_[i][1][1])\n        #\n\n        if self.fn_post_process_callback is not None:\n            return np.stack(verified_output, axis=0).reshape(N, K, -1, H, W), np.stack(verified_output_pp,\n                                                                                       axis=0).reshape(N, K, -1, H, W)\n        else:\n\n            return np.stack(verified_output, axis=0).reshape(N, K, -1, H, W), None  # NxKxLStepsxHxW\n'"
contours/cutils.py,4,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#  * Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n#  * Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#  * Neither the name of NVIDIA CORPORATION nor the names of its\n#    contributors may be used to endorse or promote products derived\n#    from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS\'\' AND ANY\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nimport numpy as np\nfrom scipy.ndimage.morphology import distance_transform_edt\nimport torch\n\n\ndef update_callback_in_image(image):\n    # this is for debugging so i am importing here to keep it clean\n    import matplotlib.pyplot as plt\n    def fn_post_process_callback(evol, pxwise):\n        colors = (\'r\', \'y\', \'g\', \'b\', \'w\')\n        if image is not None:\n            plt.imshow(image)\n        ctrs_l = []\n        ctrs_labels = []\n        for i in range(len(evol)):\n            cntr = plt.contour(evol[i], [0.5], colors=colors[i], lw=2)\n            h1, _ = cntr.legend_elements()\n            ctrs_l.append(h1[0])\n            ctrs_labels.append(\'contour: %i\' % i)\n\n        plt.legend(ctrs_l, ctrs_labels)\n        plt.show()\n        return pxwise\n\n    return fn_post_process_callback\n\n\ndef seg2edges(image, radius, label_ignores=(255,)):\n    """"""\n    :param image: semantic map should be HxWx1 with values 0,1,label_ignores\n    :param radius: radius size\n    :param label_ignores: values to mask.\n    :return: edgemap with boundary computed based on radius\n    """"""\n    if radius < 0:\n        return image\n\n    ignore_dict = {}\n\n    for ignore_id in label_ignores:\n        idxs = np.nonzero(image == ignore_id)\n        image[idxs] = 0.\n        ignore_dict[ignore_id] = idxs\n\n    # we need to pad the borders, to solve problems with dt around the boundaries of the image.\n    image_pad = np.pad(image, ((1, 1), (1, 1)), mode=\'constant\', constant_values=0)\n    dist1 = distance_transform_edt(image_pad)\n    dist2 = distance_transform_edt(1.0 - image_pad)\n    dist = dist1 + dist2\n\n    # removing padding, it shouldnt affect result other than if the image is seg to the boundary.\n    dist = dist[1:-1, 1:-1]\n    assert dist.shape == image.shape\n\n    dist[dist > radius] = 0\n\n    dist = (dist > 0).astype(np.uint8)  # just 0 or 1\n\n    ##bringing back the ignored areas back\n    for k, v in ignore_dict.items():\n        dist[v] = k\n\n    return dist\n\n\ndef seg2edges_2d(image, radius):\n    """"""\n    :param image: semantic map should be CxHxW with values 0,1,label_ignores\n    :param radius: radius size\n    :param label_ignores: values to mask.\n    :return: edgemap with boundary computed based on radius\n    """"""\n\n    # we need to pad the borders, to solve problems with dt around the boundaries of the image.\n    image_pad = np.pad(image, ((0, 0), (1, 1), (1, 1)), mode=\'constant\', constant_values=0)\n    dist1 = distance_transform_edt(image_pad)\n    dist2 = distance_transform_edt(1.0 - image_pad)\n    dist = dist1 + dist2\n\n    # removing padding, it shouldnt affect result other than if the image is seg to the boundary.\n    dist = dist[:, 1:-1, 1:-1]\n    assert dist.shape == image.shape, \'dist_shape: %s ; image_shape:%s \' % (dist.shape, image.shape)\n\n    dist[dist > radius] = 0\n\n    dist = (dist > 0).astype(np.uint8)  # just 0 or 1\n\n    return dist\n\n\ndef compute_h_additive(gt_K, pK_Image, lambda_, alpha):\n    # normalizing pK_image so that\'s [0..1]\n    pK_Image = pK_Image / (np.max(pK_Image) + 1e-5)\n\n    gPimage = 1.0 / np.sqrt(1.0 + alpha * pK_Image)\n    gpGT = 1.0 / np.sqrt(1.0 + alpha * gt_K)\n    gTotal = gPimage + lambda_ * gpGT\n    return gTotal\n\n\ndef compute_h_additive_torch(gt_K, pK_Image, lambda_, alpha):\n    # normalizing pK_image so that\'s [0..1]\n    pK_Image = pK_Image / (torch.max(pK_Image) + 1e-5)\n\n    gPimage = 1.0 / torch.sqrt(1.0 + alpha * pK_Image)\n    gpGT = 1.0 / torch.sqrt(1.0 + alpha * gt_K)\n    gTotal = gPimage + lambda_ * gpGT\n    return gTotal\n\n\ndef compute_h_caselles_torch(gt_K, pK_Image, lambda_, alpha):\n    # normalizing pK_image so that\'s [0..1]\n    pK_Image = pK_Image / (torch.max(pK_Image) + 1e-5)\n\n    gPimage = 1.0 / (1.0 + alpha * pK_Image)\n    gpGT = 1.0 / (1.0 + alpha * gt_K)\n    gTotal = gPimage + lambda_ * gpGT\n    return gTotal\n'"
contours/morph_snakes.py,0,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#  * Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n#  * Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#  * Neither the name of NVIDIA CORPORATION nor the names of its\n#    contributors may be used to endorse or promote products derived\n#    from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS\'\' AND ANY\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\n"""""" Copyright (C) 2019, the scikit-image team\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n\n 1. Redistributions of source code must retain the above copyright\n    notice, this list of conditions and the following disclaimer.\n 2. Redistributions in binary form must reproduce the above copyright\n    notice, this list of conditions and the following disclaimer in\n    the documentation and/or other materials provided with the\n    distribution.\n 3. Neither the name of skimage nor the names of its contributors may be\n    used to endorse or promote products derived from this software without\n    specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS\'\' AND ANY EXPRESS OR\nIMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT,\nINDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\nHOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,\nSTRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING\nIN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\nPOSSIBILITY OF SUCH DAMAGE.\n""""""\n\n# based on skimage.segmentation implementation....\n# skimage.segmentation.morphsnakes\n# -*- coding: utf-8 -*-\n\n\nfrom itertools import cycle\n\nimport numpy as np\nfrom scipy import ndimage as ndi\n\n__all__ = [\n    \'morphological_geodesic_active_contour\',\n]\n\n\ndef assert_nD(array, ndim, arg_name=\'image\'):\n    """"""\n    Verify an array meets the desired ndims and array isn\'t empty.\n\n    Parameters\n    ----------\n    array : array-like\n        Input array to be validated\n    ndim : int or iterable of ints\n        Allowable ndim or ndims for the array.\n    arg_name : str, optional\n        The name of the array in the original function.\n\n    """"""\n    array = np.asanyarray(array)\n    msg_incorrect_dim = ""The parameter `%s` must be a %s-dimensional array""\n    msg_empty_array = ""The parameter `%s` cannot be an empty array""\n    if isinstance(ndim, int):\n        ndim = [ndim]\n    if array.size == 0:\n        raise ValueError(msg_empty_array % (arg_name))\n    if not array.ndim in ndim:\n        raise ValueError(msg_incorrect_dim % (arg_name, \'-or-\'.join([str(n) for n in ndim])))\n\n\nclass _fcycle(object):\n\n    def __init__(self, iterable):\n        """"""Call functions from the iterable each time it is called.""""""\n        self.funcs = cycle(iterable)\n\n    def __call__(self, *args, **kwargs):\n        f = next(self.funcs)\n        return f(*args, **kwargs)\n\n\n# SI and IS operators for 2D and 3D.\n_P2 = [np.eye(3),\n       np.array([[0, 1, 0]] * 3),\n       np.flipud(np.eye(3)),\n       np.rot90([[0, 1, 0]] * 3)]\n_P3 = [np.zeros((3, 3, 3)) for i in range(9)]\n\n_P3[0][:, :, 1] = 1\n_P3[1][:, 1, :] = 1\n_P3[2][1, :, :] = 1\n_P3[3][:, [0, 1, 2], [0, 1, 2]] = 1\n_P3[4][:, [0, 1, 2], [2, 1, 0]] = 1\n_P3[5][[0, 1, 2], :, [0, 1, 2]] = 1\n_P3[6][[0, 1, 2], :, [2, 1, 0]] = 1\n_P3[7][[0, 1, 2], [0, 1, 2], :] = 1\n_P3[8][[0, 1, 2], [2, 1, 0], :] = 1\n\n\ndef sup_inf(u):\n    """"""SI operator.""""""\n\n    if np.ndim(u) == 2:\n        P = _P2\n    elif np.ndim(u) == 3:\n        P = _P3\n    else:\n        raise ValueError(""u has an invalid number of dimensions ""\n                         ""(should be 2 or 3)"")\n\n    erosions = []\n    for P_i in P:\n        erosions.append(ndi.binary_erosion(u, P_i))\n\n    return np.array(erosions, dtype=np.int8).max(0)\n\n\ndef inf_sup(u):\n    """"""IS operator.""""""\n\n    if np.ndim(u) == 2:\n        P = _P2\n    elif np.ndim(u) == 3:\n        P = _P3\n    else:\n        raise ValueError(""u has an invalid number of dimensions ""\n                         ""(should be 2 or 3)"")\n\n    dilations = []\n    for P_i in P:\n        dilations.append(ndi.binary_dilation(u, P_i))\n\n    return np.array(dilations, dtype=np.int8).min(0)\n\n\n_curvop = _fcycle([lambda u: sup_inf(inf_sup(u)),  # SIoIS\n                   lambda u: inf_sup(sup_inf(u))])  # ISoSI\n\n\ndef _check_input(image, init_level_set):\n    """"""Check that shapes of `image` and `init_level_set` match.""""""\n    assert_nD(image, [2, 3])\n\n    if len(image.shape) != len(init_level_set.shape):\n        raise ValueError(""The dimensions of the initial level set do not ""\n                         ""match the dimensions of the image."")\n\n\ndef _init_level_set(init_level_set, image_shape):\n    if isinstance(init_level_set, str):\n        raise ValueError("""")\n    else:\n        res = init_level_set\n\n    return res\n\n\ndef morphological_geodesic_active_contour(gimage, iterations,\n                                          init_level_set=\'circle\', smoothing=1,\n                                          threshold=0.95, balloon=0,\n                                          iter_callback=lambda x: None):\n    """"""Morphological Geodesic Active Contours (MorphGAC)\n    This is based on on skimage.segmentation implementation.\n\n    Geodesic active contours implemented with morphological operators. It can\n    be used to segment objects with visible but noisy, cluttered, broken\n    borders.\n\n    Parameters\n    ----------\n    gimage : (M, N) or (L, M, N) array\n        Preprocessed image or volume to be segmented. This is very rarely the\n        original image. Instead, this is usually a preprocessed version of the\n        original image that enhances and highlights the borders (or other\n        structures) of the object to segment.\n        `morphological_geodesic_active_contour` will try to stop the contour\n        evolution in areas where `gimage` is small. See\n        `morphsnakes.inverse_gaussian_gradient` as an example function to\n        perform this preprocessing. Note that the quality of\n        `morphological_geodesic_active_contour` might greatly depend on this\n        preprocessing.\n    iterations : uint\n        Number of iterations to run.\n    init_level_set : str, (M, N) array, or (L, M, N) array\n        Initial level set. If an array is given, it will be binarized and used\n        as the initial level set. If a string is given, it defines the method\n        to generate a reasonable initial level set with the shape of the\n        `image`. Accepted values are \'checkerboard\' and \'circle\'. See the\n        documentation of `checkerboard_level_set` and `circle_level_set`\n        respectively for details about how these level sets are created.\n    smoothing : uint, optional\n        Number of times the smoothing operator is applied per iteration.\n        Reasonable values are around 1-4. Larger values lead to smoother\n        segmentations.\n    threshold : float, optional\n        Areas of the image with a value smaller than this threshold will be\n        considered borders. The evolution of the contour will stop in this\n        areas.\n    balloon : float, optional\n        Balloon force to guide the contour in non-informative areas of the\n        image, i.e., areas where the gradient of the image is too small to push\n        the contour towards a border. A negative value will shrink the contour,\n        while a positive value will expand the contour in these areas. Setting\n        this to zero will disable the balloon force.\n    iter_callback : function, optional\n        If given, this function is called once per iteration with the current\n        level set as the only argument. This is useful for debugging or for\n        plotting intermediate results during the evolution.\n\n    Returns\n    -------\n    out : (M, N) or (L, M, N) array\n        Final segmentation (i.e., the final level set)\n\n    See also\n    --------\n    inverse_gaussian_gradient, circle_level_set, checkerboard_level_set\n\n    Notes\n    -----\n\n    This is a version of the Geodesic Active Contours (GAC) algorithm that uses\n    morphological operators instead of solving partial differential equations\n    (PDEs) for the evolution of the contour. The set of morphological operators\n    used in this algorithm are proved to be infinitesimally equivalent to the\n    GAC PDEs (see [1]_). However, morphological operators are do not suffer\n    from the numerical stability issues typically found in PDEs (e.g., it is\n    not necessary to find the right time step for the evolution), and are\n    computationally faster.\n\n    The algorithm and its theoretical derivation are described in [1]_.\n\n    References\n    ----------\n    .. [1] A Morphological Approach to Curvature-based Evolution of Curves and\n           Surfaces, Pablo M\xc3\xa1rquez-Neila, Luis Baumela, Luis \xc3\x81lvarez. In IEEE\n           Transactions on Pattern Analysis and Machine Intelligence (PAMI),\n           2014, DOI 10.1109/TPAMI.2013.106\n    """"""\n\n    image = gimage\n    init_level_set = _init_level_set(init_level_set, image.shape)\n\n    _check_input(image, init_level_set)\n\n    structure = np.ones((3,) * len(image.shape), dtype=np.int8)\n    dimage = np.gradient(image)\n\n    if threshold == 0 and balloon != 0:\n        raise ValueError(\'threshold 0 with balloon.\')\n\n    if balloon != 0:\n        threshold_mask_balloon = image > threshold\n\n    u = np.int8(init_level_set > 0)\n\n    iter_callback(u, -1)\n\n    for i in range(1, iterations + 1):\n\n        # Balloon\n        if balloon > 0:\n            aux = ndi.binary_dilation(u, structure)\n        elif balloon < 0:\n            aux = ndi.binary_erosion(u, structure)\n        if balloon != 0:\n            u[threshold_mask_balloon] = aux[threshold_mask_balloon]\n\n        aux = np.zeros_like(image)\n        # Image attachment\n        aux = aux.astype(np.float64)\n\n        du = np.gradient(u)\n\n        for el1, el2 in zip(dimage, du):\n            aux += el1 * el2\n        u[aux > 0] = 1\n        u[aux < 0] = 0\n\n        # Smoothing\n        for _ in range(smoothing):\n            u = _curvop(u)\n\n        iter_callback(u, i)\n\n    return u\n'"
models/casenet.py,5,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#  * Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n#  * Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#  * Neither the name of NVIDIA CORPORATION nor the names of its\n#    contributors may be used to endorse or promote products derived\n#    from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS\'\' AND ANY\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\n""""""\nBSD 3-Clause License\nCopyright (c) Soumith Chintala 2016,\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n* Neither the name of the copyright holder nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n""""""\n\n# ResNet implementation kindly borrow from pytorch-vision and modified to match the original casenet caffe implementation.\n\nimport torch.nn as nn\nimport torch\nimport math\nfrom torch.autograd import Variable\nimport numpy as np\n\n__all__ = [\'casenet101\']\n\nBatchNorm = nn.BatchNorm2d\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, block_no, stride=1, downsample=None):  # add dilation factor\n        super(Bottleneck, self).__init__()\n        if block_no < 5:\n            dilation = 2\n            padding = 2\n        else:\n            dilation = 4\n            padding = 4\n\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False, stride=stride)\n        self.bn1 = BatchNorm(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1,\n                               padding=padding, bias=False, dilation=dilation)\n        self.bn2 = BatchNorm(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = BatchNorm(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Crop(nn.Module):\n    def __init__(self, axis, offset):\n        super(Crop, self).__init__()\n        self.axis = axis\n        self.offset = offset\n\n    def forward(self, x, ref):\n        """"""\n\n        :param x: input layer\n        :param ref: reference usually data in\n        :return:\n        """"""\n        for axis in range(self.axis, x.dim()):\n            ref_size = ref.size(axis)\n            indices = torch.arange(self.offset, self.offset + ref_size).long()\n            indices = x.data.new().resize_(indices.size()).copy_(indices).long()\n            x = x.index_select(axis, Variable(indices))\n        return x\n\n\nclass MyIdentity(nn.Module):\n    def __init__(self, axis, offset):\n        super(MyIdentity, self).__init__()\n        self.axis = axis\n        self.offset = offset\n\n    def forward(self, x, ref):\n        """"""\n\n        :param x: input layer\n        :param ref: reference usually data in\n        :return:\n        """"""\n        return x\n\n\nclass SideOutputCrop(nn.Module):\n    """"""\n    This is the original implementation ConvTranspose2d (fixed) and crops\n    """"""\n\n    def __init__(self, num_output, kernel_sz=None, stride=None, upconv_pad=0, do_crops=True):\n        super(SideOutputCrop, self).__init__()\n        self._do_crops = do_crops\n        self.conv = nn.Conv2d(num_output, out_channels=1, kernel_size=1, stride=1, padding=0, bias=True)\n\n        if kernel_sz is not None:\n            self.upsample = True\n            self.upsampled = nn.ConvTranspose2d(1, out_channels=1, kernel_size=kernel_sz, stride=stride,\n                                                padding=upconv_pad,\n                                                bias=False)\n            ##doing crops\n            if self._do_crops:\n                self.crops = Crop(2, offset=kernel_sz // 4)\n            else:\n                self.crops = MyIdentity(None, None)\n        else:\n            self.upsample = False\n\n    def forward(self, res, reference=None):\n        side_output = self.conv(res)\n        if self.upsample:\n            side_output = self.upsampled(side_output)\n            side_output = self.crops(side_output, reference)\n\n        return side_output\n\n\nclass Res5OutputCrop(nn.Module):\n\n    def __init__(self, in_channels=2048, kernel_sz=16, stride=8, nclasses=20, upconv_pad=0, do_crops=True):\n        super(Res5OutputCrop, self).__init__()\n        self._do_crops = do_crops\n        self.conv = nn.Conv2d(in_channels, nclasses, kernel_size=1, stride=1, padding=0, bias=True)\n        self.upsampled = nn.ConvTranspose2d(nclasses, out_channels=nclasses, kernel_size=kernel_sz, stride=stride,\n                                            padding=upconv_pad,\n                                            bias=False, groups=nclasses)\n        if self._do_crops is True:\n            self.crops = Crop(2, offset=kernel_sz // 4)\n        else:\n            self.crops = MyIdentity(None, None)\n\n    def forward(self, res, reference):\n        res = self.conv(res)\n        res = self.upsampled(res)\n        res = self.crops(res, reference)\n        return res\n\n\ndef get_upsample_filter(size):\n    """"""Make a 2D bilinear kernel suitable for upsampling""""""\n    factor = (size + 1) // 2\n    if size % 2 == 1:\n        center = factor - 1\n    else:\n        center = factor - 0.5\n    og = np.ogrid[:size, :size]\n    filter = (1 - abs(og[0] - center) / factor) * \\\n             (1 - abs(og[1] - center) / factor)\n    return torch.from_numpy(filter).float()\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, nclasses=20):\n\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self._nclasses = nclasses\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=1, padding=3,\n                               bias=False)\n\n        self.bn1 = BatchNorm(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=0, ceil_mode=True)  # define ceil mode\n\n        self.layer1 = self._make_layer(block, 64, layers[0], 2)  # res2\n        self.layer2 = self._make_layer(block, 128, layers[1], 3, stride=2)  # res3\n\n        self.layer3 = self._make_layer(block, 256, layers[2], 4, stride=2)  # res4\n        self.layer4 = self._make_layer(block, 512, layers[3], 5, stride=1)  # res5\n\n        self.normals = None\n\n        ####let\'s make pointers to keep compatibility for now\n\n        SideOutput_fn = SideOutputCrop\n        Res5Output_fn = Res5OutputCrop\n\n        self.use_pytorch_upsample = False\n\n        # The original casenet implementation has padding when upsampling cityscapes that are not used for SBD.\n        # Leaving like this for now such that it is clear and it matches the original implementation (for fair comparison)\n\n        if nclasses == 19:\n            print(\'Assuming Cityscapes CASENET\')\n            self.score_edge_side1 = SideOutput_fn(64)\n            self.score_edge_side2 = SideOutput_fn(256, kernel_sz=4, stride=2, upconv_pad=1, do_crops=False)\n            self.score_edge_side3 = SideOutput_fn(512, kernel_sz=8, stride=4, upconv_pad=2, do_crops=False)\n            self.score_cls_side5 = Res5Output_fn(kernel_sz=16, stride=8, nclasses=self._nclasses, upconv_pad=4,\n                                                 do_crops=False)\n        else:\n            print(\'Assuming Classical SBD CASENET\')\n            self.score_edge_side1 = SideOutput_fn(64)\n            self.score_edge_side2 = SideOutput_fn(256, kernel_sz=4, stride=2)\n            self.score_edge_side3 = SideOutput_fn(512, kernel_sz=8, stride=4)\n            self.score_cls_side5 = Res5Output_fn(kernel_sz=16, stride=8, nclasses=self._nclasses)\n\n        num_classes = self._nclasses\n        self.ce_fusion = nn.Conv2d(4 * num_classes, num_classes, groups=num_classes, kernel_size=1, stride=1, padding=0,\n                                   bias=True)\n\n        self.sigmoid = nn.Sigmoid()\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, BatchNorm):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.ConvTranspose2d):\n                c1, c2, h, w = m.weight.data.size()\n                weight = get_upsample_filter(h)\n                m.weight.data = weight.view(1, 1, h, w).repeat(c1, c2, 1, 1)\n                if m.bias is not None:\n                    m.bias.data.zero_()\n\n        # manually initializing the new layers.\n        self.score_edge_side1.conv.weight.data.normal_(0, 0.01)\n        self.score_edge_side1.conv.bias.data.zero_()\n        # -\n        self.score_edge_side2.conv.weight.data.normal_(0, 0.01)\n        self.score_edge_side2.conv.bias.data.zero_()\n        # -\n        self.score_edge_side3.conv.weight.data.normal_(0, 0.01)\n        self.score_edge_side3.conv.bias.data.zero_()\n        # -\n        self.ce_fusion.weight.data.fill_(0.25)\n        self.ce_fusion.bias.data.zero_()\n        # ---\n\n    def _make_layer(self, block, planes, blocks, block_no, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                BatchNorm(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, block_no, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, block_no))\n\n        return nn.Sequential(*layers)\n\n    def _sliced_concat(self, res1, res2, res3, res5, num_classes):\n        out_dim = num_classes * 4\n        out_tensor = Variable(torch.FloatTensor(res1.size(0), out_dim, res1.size(2), res1.size(3))).cuda()\n        class_num = 0\n        for i in range(0, out_dim, 4):\n            out_tensor[:, i, :, :] = res5[:, class_num, :, :]\n            out_tensor[:, i + 1, :, :] = res1[:, 0, :, :]  # it needs this trick for multibatch\n            out_tensor[:, i + 2, :, :] = res2[:, 0, :, :]\n            out_tensor[:, i + 3, :, :] = res3[:, 0, :, :]\n\n            class_num += 1\n\n        return out_tensor\n\n    def forward(self, x, normals_mask=None):\n        assert x.shape[1] == 3, \'N,3,H,W BGR Image?\'\n        input_data = x\n\n        # res1\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        side_1 = self.score_edge_side1(x, input_data)\n\n        # res2\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        side_2 = self.score_edge_side2(x, input_data)\n\n        # res3\n        x = self.layer2(x)\n        side_3 = self.score_edge_side3(x, input_data)\n\n        # res4\n        x = self.layer3(x)\n\n        # res5\n        x = self.layer4(x)\n\n        side_5 = self.score_cls_side5(x, input_data)\n\n        # combine outputs and classify\n        sliced_cat = self._sliced_concat(side_1, side_2, side_3, side_5, self._nclasses)\n        acts = self.ce_fusion(sliced_cat)\n\n        normals = None\n\n        return [acts, side_5, normals, (side_1, side_2, side_3)]  # sigmoid can be taken later\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = BatchNorm(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = BatchNorm(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\ndef casenet101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        raise NotImplementedError()\n    return model\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n'"
utils/VisualizerBox.py,0,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#  * Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n#  * Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#  * Neither the name of NVIDIA CORPORATION nor the names of its\n#    contributors may be used to endorse or promote products derived\n#    from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS\'\' AND ANY\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nimport numpy as np\nimport os\nimport skimage.measure as measure\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport matplotlib.colors as mcolors\n\n\nclass VisualizerBox:\n    def city_pallete(self):\n        CITYSCAPE_PALLETE = np.asarray([\n            [128, 64, 128],\n            [244, 35, 232],\n            [70, 70, 70],\n            [102, 102, 156],\n            [190, 153, 153],\n            [153, 153, 153],\n            [250, 170, 30],\n            [220, 220, 0],\n            [107, 142, 35],\n            [152, 251, 152],\n            [70, 130, 180],\n            [220, 20, 60],\n            [255, 0, 0],\n            [0, 0, 142],\n            [0, 0, 70],\n            [0, 60, 100],\n            [0, 80, 100],\n            [0, 0, 230],\n            [119, 11, 32],\n            [0, 0, 0]], dtype=np.uint8)\n        self.colors_are_a_list = False\n        return CITYSCAPE_PALLETE\n\n    def sbd_pallete(self):\n        SBD_PALLETE = np.asarray([\n            [128, 0, 0],\n            [0, 128, 0],\n            [128, 128, 0],\n            [0, 0, 128],\n            [128, 0, 128],\n            [0, 128, 128],\n            [128, 128, 128],\n            [64, 0, 0],\n            [192, 0, 0],\n            [64, 128, 0],\n            [192, 128, 0],\n            [64, 0, 128],\n            [192, 0, 128],\n            [64, 128, 128],\n            [192, 128, 128],\n            [0, 64, 0],\n            [128, 64, 0],\n            [0, 192, 0],\n            [128, 192, 0],\n            [0, 64, 128],\n            [0, 0, 0]], dtype=np.uint8)\n        self.colors_are_a_list = False\n        return SBD_PALLETE\n\n    def css4_colors_pallete(self):\n        self.colors_are_a_list = True\n        return list(mcolors.CSS4_COLORS.keys())\n\n    def css4_fushia(self):\n        self.colors_are_a_list = True\n        rrr = list(mcolors.CSS4_COLORS.keys())\n        rrr[13] = \'fuchsia\'\n        return rrr\n\n    def __init__(self, dataset_color, plt_backend=None, fig_size=(8, 12), only_contour=False, postfix_as_name=True):\n        self.colors_are_a_list = False\n        if dataset_color == \'cityscapes\':\n            print(\'dataset color: cityscapes\')\n            self._mycolors = self.city_pallete()\n        elif dataset_color == \'sbd\':\n            print(\'dataset color: sbd\')\n            self._mycolors = self.sbd_pallete()\n        elif dataset_color == \'css4\':\n            print(\'dataset color: css4\')\n            self._mycolors = self.css4_colors_pallete()\n        elif dataset_color == \'css4_fushia\':\n            print(\'dataset color: css4_fushia\')\n            self._mycolors = self.css4_fushia()\n        else:\n            raise ValueError()\n\n        self._output_f = None\n        self._fig_size = fig_size\n        self._only_contour = only_contour\n        self._postfix_as_name = postfix_as_name\n\n        if plt_backend is not None:\n            print(\'Switching backend to %s\' % plt_backend)\n            plt.switch_backend(plt_backend)\n\n    def plot_multichannel_mask(self, ax, masks, remapping_dict=None, ref_contour=None):\n\n        for i in range(masks.shape[0]):\n            mask = masks[i]\n\n            if not np.any(mask):\n                continue\n\n            contours = measure.find_contours(mask, 0)\n            # TODO this is a copying, do this properly\n            if ref_contour is not None and self._only_contour is True:\n                ref_contour_i = measure.find_contours(ref_contour[i], 0)\n                for contour in ref_contour_i:\n                    contour = np.fliplr(contour)\n                    ax.plot(contour[:, 0], contour[:, 1], linewidth=3, color=\'red\')  #\n\n            for contour in contours:\n                contour = np.fliplr(contour)\n                if remapping_dict is None:\n                    c_id = i\n                else:\n                    c_id = remapping_dict[i]\n                if self._only_contour is False:\n                    if self.colors_are_a_list:\n                        color = self._mycolors[c_id]\n                    else:\n                        color = self._mycolors[c_id] / 255.0\n                    p = patches.Polygon(contour, facecolor=color, edgecolor=\'white\', linewidth=0,\n                                        alpha=0.5)\n                    ax.add_patch(p)\n                    ax.plot(contour[:, 0], contour[:, 1], linewidth=2,\n                            color=\'orange\')\n\n                else:\n\n                    simple_color = \'greenyellow\'\n                    ax.plot(contour[:, 0], contour[:, 1], linewidth=2,\n                            color=simple_color, alpha=1)\n        return ax\n\n    def add_vis_list(self, images_dict, background=None, remapping_dict=None, grid=True, merge_channels=True,\n                     exec_fn=None, ref_contour=None):\n        if merge_channels is False:\n            return NotImplementedError()\n\n        if grid is True:\n            f, ax = plt.subplots(len(images_dict.keys()), 1, figsize=self._fig_size)\n        else:\n            f, ax = plt.subplots(1, 1, figsize=self._fig_size)\n\n        if background is None:\n            h, w = images_dict.items()[0].shape[1:3]  # C, H,W\n            background = np.ones((h, w)) * 255\n\n        for i, (title, image_array) in enumerate(images_dict.items()):\n\n            # just in case it was lazy loaded (eg.PIL)\n            image_array = np.array(image_array)\n            # if not isinstance(ax, list):\n            if not (type(ax) is np.ndarray):\n                curr_ax = ax\n            elif len(ax) > 1:\n                curr_ax = ax[i]\n            else:\n                curr_ax = ax\n\n            curr_ax.set_axis_off()\n            curr_ax.set_title(title)\n\n            if image_array.shape[0] == 1:  #\n                curr_ax.imshow(image_array[0])\n                if exec_fn is not None and grid is False:\n                    exec_fn(f, curr_ax, title)\n\n                continue\n\n            # setting the background, usually input image...\n            curr_ax.imshow(np.array(background), alpha=1)\n\n            self.plot_multichannel_mask(curr_ax, image_array, remapping_dict, ref_contour)\n\n            if exec_fn is not None and grid is False:\n                exec_fn(f, curr_ax, title)\n\n        if exec_fn is not None and grid is True:\n            exec_fn(f, ax, \'grid\')\n\n        return f, ax\n\n    def set_output_folder(self, output_f):\n        self._output_f = output_f\n        if self._output_f is not None:\n            if not os.path.isdir(self._output_f):\n                os.makedirs(self._output_f)\n        print(\'Vis Output Dir:\', self._output_f)\n\n    def save_vis(self, image_name, images_dict, background=None, remapping_dict=None, grid=True, auto_show=False,\n                 ref_contour=None):\n        def exec_callback(f, ax, title, **kwargs):\n            assert self._output_f is not None\n            title = title.replace("" "", """").lower()\n            if self._postfix_as_name is True:\n                fname = os.path.join(self._output_f, image_name + \'_\' + title + \'.jpg\')\n            else:\n                fname = os.path.join(self._output_f, image_name + \'.jpg\')\n            f.tight_layout()\n            f.savefig(fname)\n            ax.cla()\n\n        self.add_vis_list(images_dict, background, remapping_dict, grid, exec_fn=exec_callback, ref_contour=ref_contour)\n        if auto_show:\n            plt.show()\n\n        plt.close()\n\n    def visualize(self, images_dict, background=None, remapping_dict=None, grid=True, merge_channels=True):\n        f, ax = self.add_vis_list(images_dict, background, remapping_dict, grid, merge_channels)\n        plt.show()\n'"
utils/dataloader.py,1,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#  * Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n#  * Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#  * Neither the name of NVIDIA CORPORATION nor the names of its\n#    contributors may be used to endorse or promote products derived\n#    from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS\'\' AND ANY\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nimport torch.utils.data as data\nimport torch\nfrom PIL import Image\nimport os\nimport os.path\nimport numpy as np\nimport cv2\n\n\n# from torchvision import transforms, datasets\n# Image file list reader function taken from https://github.com/pytorch/vision/issues/81\n\ndef default_flist_reader(root, flist):\n    """"""\n    flist format: impath label\\nimpath label\\n ...(same to caffe\'s filelist)\n    """"""\n    imlist = []\n    with open(flist, \'r\') as rf:\n        for line in rf.readlines():\n            splitted = line.strip().split()\n            if len(splitted) == 2:\n                impath, imlabel = splitted\n            elif len(splitted) == 1:\n                impath, imlabel = splitted[0], None\n            else:\n                raise ValueError(\'weird length ?\')\n            impath = impath.strip(\'../\')\n            imlist.append((impath, imlabel))\n\n    return imlist\n\n\ndef _is_bit_set(x, n):\n    if x & (1 << n):\n        return 1\n    else:\n        return 0\n\n\ndef _decode_integer(_int, channels, output_):\n    for c in range(channels):\n        output_[c] = _is_bit_set(_int, c)\n\n\ndef binary_file_to_channel_masks(bin_file, h, w, channels, seen_classes=None, ignore_pixel_id_map=(31, 255)):\n    array = np.fromfile(bin_file, dtype=np.uint32)\n    # i\'ll asssume this array is very sparse so this should be fast.\n    idxs = np.argwhere(array != 0)\n    arr_chn = np.zeros((array.shape[0], channels))\n    #\n    for idx in idxs:\n        for c in range(channels):\n            ignore_pixel = _is_bit_set(array[idx], ignore_pixel_id_map[0]) == 1\n            if ignore_pixel is True:\n                # print(\'ignoring pixel\')\n                arr_chn[idx, c] = ignore_pixel_id_map[1]  # 255\n            else:\n                arr_chn[idx, c] = _is_bit_set(array[idx], c)\n            # for debug?\n            if seen_classes is not None:\n                if arr_chn[idx, c] != 0:\n                    seen_classes.add(c)\n    return arr_chn.reshape(h, w, channels)\n\n\ndef seg_img_to_Kchannels(imfile, klasses):\n    image = np.array(Image.open(imfile))\n    masks = [image == (i + 1) for i in range(klasses)]\n    # TODO maybe this need to handle ignore pixels at some point.\n    return np.stack(masks, axis=0).astype(np.uint8)\n\n\nclass ImageFilelist(data.Dataset):\n    def __init__(self, root, flist, n_classes, transform, shuffle_list, ref_gt=False, read_seg=False, *args, **kwargs):\n        self.root = root\n        self.imlist = default_flist_reader(root, flist)\n        if shuffle_list is True:\n            np.random.shuffle(self.imlist)\n\n        self.transform = transform\n        self.n_classes = n_classes\n        self.ignore_value = 255\n        self._compute_ref_gt = ref_gt\n        self._read_seg = read_seg\n\n    def _read_image(self, input_image_path, *args):\n        raise NotImplementedError()\n\n    def _read_gt(self, gtpath, *args):\n        raise NotImplementedError()\n\n    def _ref_gt(self, gt):\n        """"""\n\n        :param gt: C,H,W\n        :return:\n        """"""\n        raise NotImplementedError()\n\n    def __getitem__(self, index):\n        impath, gtpath = (\n            os.path.join(self.root, *self.imlist[index][0].split(\'/\')),\n            os.path.join(self.root, *self.imlist[index][1].split(\'/\'))\n        )\n        image = self._read_image(impath)\n        width, height = Image.open(impath).size\n        gt = self._read_gt(gtpath, (height, width))  # reading only the header..faster\n        image_info = {\'impath\': impath, \'gtpath\': gtpath, \'orig_size\': (height, width)}\n        return image_info, image, gt\n\n    def __len__(self):\n        return len(self.imlist)\n\n\nclass ValidationDataset(ImageFilelist):\n    def __init__(self, root, flist, n_classes, transform=None, crop_size=512):\n        super(ValidationDataset, self).__init__(root, flist, n_classes, transform, shuffle_list=False)\n        self._crop_size = crop_size\n\n    def _read_image(self, input_image_path, *args):\n        crop_size = self._crop_size\n        mean_value = (104.008, 116.669, 122.675)  # BGR\n        original_im = cv2.imread(input_image_path).astype(np.float32)\n        in_ = original_im\n        width, height = in_.shape[1], in_.shape[0]\n        if crop_size < width or crop_size < height:\n            raise ValueError(\'Input image size must be smaller than crop size!\')\n        elif crop_size == width and crop_size == height:\n            # (""WARNING *** skipping because of crop_size "")\n            pass\n        else:\n            pad_x = crop_size - width\n            pad_y = crop_size - height\n            in_ = cv2.copyMakeBorder(in_, 0, pad_y, 0, pad_x, cv2.BORDER_CONSTANT, value=mean_value)\n        in_ -= np.array(mean_value)\n        in_ = in_.transpose((2, 0, 1))  # HxWx3 -> 3xHxW\n        return in_\n\n    def _read_gt(self, gtpath, size):\n        gt_mask = binary_file_to_channel_masks(gtpath, size[0], size[1], self.n_classes, None)\n        crop_size = self._crop_size\n        width, height = gt_mask.shape[1], gt_mask.shape[0]\n        if crop_size < width or crop_size < height:\n            raise ValueError(\'Input gt size must be smaller than crop size!\')\n        elif crop_size == width and crop_size == height:\n            # (""WARNING GT *** skipping because of crop_size "")\n            pass\n        else:\n            pad_x = crop_size - width\n            pad_y = crop_size - height\n            gt_mask = cv2.copyMakeBorder(gt_mask, 0, pad_y, 0, pad_x, cv2.BORDER_CONSTANT,\n                                         value=[self.ignore_value] * 4)\n\n        return np.transpose(gt_mask, [2, 0, 1])\n\n\n'"
utils/vis_utils.py,0,"b""# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#  * Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n#  * Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#  * Neither the name of NVIDIA CORPORATION nor the names of its\n#    contributors may be used to endorse or promote products derived\n#    from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n\ndef read_resize_image(url, show=True, resize=0.5):\n    im = Image.open(url)\n    if resize < 1:\n        im = im.resize([int(resize * s) for s in im.size])\n    if show is True:\n        plt.imshow(np.array(im))\n        plt.show()\n    return im\n"""
