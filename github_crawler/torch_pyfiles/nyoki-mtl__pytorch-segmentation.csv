file_path,api_count,code
src/__init__.py,0,b''
src/eval_cityscapes.py,5,"b""import argparse\nimport yaml\nimport numpy as np\nfrom pathlib import Path\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom models.net import EncoderDecoderNet, SPPNet\nfrom dataset.cityscapes import CityscapesDataset\nfrom utils.preprocess import minmax_normalize\n\nvalid_classes = [7, 8, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33]\nid2cls_dict = dict(zip(range(19), valid_classes))\nid2cls_func = np.vectorize(id2cls_dict.get)\n\ndef predict(batched, tta_flag=False):\n    images, labels, names = batched\n    images_np = images.numpy().transpose(0, 2, 3, 1)\n    labels_np = labels.numpy()\n\n    images, labels = images.to(device), labels.to(device)\n    if tta_flag:\n        preds = model.tta(images, scales=scales, net_type=net_type)\n    else:\n        preds = model.pred_resize(images, images.shape[2:], net_type=net_type)\n    preds = preds.argmax(dim=1)\n    preds_np = preds.detach().cpu().numpy().astype(np.uint8)\n    return images_np, labels_np, preds_np, names\n\nparser = argparse.ArgumentParser()\nparser.add_argument('config_path')\nparser.add_argument('--tta', action='store_true')\nparser.add_argument('--vis', action='store_true')\nargs = parser.parse_args()\nconfig_path = Path(args.config_path)\ntta_flag = args.tta\nvis_flag = args.vis\n\nconfig = yaml.load(open(config_path))\nnet_config = config['Net']\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\nmodelname = config_path.stem\nmodel_path = Path('../model') / modelname / 'model.pth'\n\nif 'unet' in net_config['dec_type']:\n    net_type = 'unet'\n    model = EncoderDecoderNet(**net_config)\nelse:\n    net_type = 'deeplab'\n    model = SPPNet(**net_config)\nmodel.to(device)\nmodel.update_bn_eps()\n\nparam = torch.load(model_path)\nmodel.load_state_dict(param)\ndel param\n\nmodel.eval()\n\nbatch_size = 1\nscales = [0.25, 0.75, 1, 1.25]\nvalid_dataset = CityscapesDataset(split='valid', net_type=net_type)\nvalid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n\nif vis_flag:\n    images_list = []\n    labels_list = []\n    preds_list = []\n\n    with torch.no_grad():\n        for batched in valid_loader:\n            images_np, labels_np, preds_np, names = predict(batched)\n            images_list.append(images_np)\n            labels_list.append(labels_np)\n            preds_list.append(preds_np)\n            if len(images_list) == 4:\n                break\n\n    images = np.concatenate(images_list)\n    labels = np.concatenate(labels_list)\n    preds = np.concatenate(preds_list)\n\n    ignore_pixel = labels == 255\n    preds[ignore_pixel] = 20\n    labels[ignore_pixel] = 20\n\n    fig, axes = plt.subplots(4, 3, figsize=(12, 10))\n    plt.tight_layout()\n\n    axes[0, 0].set_title('input image')\n    axes[0, 1].set_title('prediction')\n    axes[0, 2].set_title('ground truth')\n\n    for ax, img, lbl, pred in zip(axes, images, labels, preds):\n        ax[0].imshow(minmax_normalize(img, norm_range=(0, 1), orig_range=(-1, 1)))\n        ax[1].imshow(pred)\n        ax[2].imshow(lbl)\n        ax[0].set_xticks([])\n        ax[0].set_yticks([])\n        ax[1].set_xticks([])\n        ax[1].set_yticks([])\n        ax[2].set_xticks([])\n        ax[2].set_yticks([])\n\n    plt.savefig('eval.png')\n    plt.close()\nelse:\n    output_dir = Path('../output/cityscapes_val') / (str(modelname) + '_tta' if tta_flag else modelname)\n    output_dir.mkdir(parents=True)\n\n    with torch.no_grad():\n        for batched in tqdm(valid_loader):\n            _, _, preds_np, names = predict(batched)\n            preds_np = id2cls_func(preds_np).astype(np.uint8)\n            for name, pred in zip(names, preds_np):\n                Image.fromarray(pred).save(output_dir / f'{name}.png')\n"""
src/train.py,12,"b'import pickle\nimport argparse\nimport yaml\nimport numpy as np\nimport albumentations as albu\nfrom collections import OrderedDict\nfrom pathlib import Path\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\nfrom models.net import EncoderDecoderNet, SPPNet\nfrom losses.multi import MultiClassCriterion\nfrom logger.log import debug_logger\nfrom logger.plot import history_ploter\nfrom utils.optimizer import create_optimizer\nfrom utils.metrics import compute_iou_batch\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'config_path\')\nargs = parser.parse_args()\nconfig_path = Path(args.config_path)\nconfig = yaml.load(open(config_path))\nnet_config = config[\'Net\']\ndata_config = config[\'Data\']\ntrain_config = config[\'Train\']\nloss_config = config[\'Loss\']\nopt_config = config[\'Optimizer\']\ndevice = torch.device(\'cuda:0\' if torch.cuda.is_available() else \'cpu\')\nt_max = opt_config[\'t_max\']\n\nmax_epoch = train_config[\'max_epoch\']\nbatch_size = train_config[\'batch_size\']\nfp16 = train_config[\'fp16\']\nresume = train_config[\'resume\']\npretrained_path = train_config[\'pretrained_path\']\n\n# Network\nif \'unet\' in net_config[\'dec_type\']:\n    net_type = \'unet\'\n    model = EncoderDecoderNet(**net_config)\nelse:\n    net_type = \'deeplab\'\n    model = SPPNet(**net_config)\n\ndataset = data_config[\'dataset\']\nif dataset == \'pascal\':\n    from dataset.pascal_voc import PascalVocDataset as Dataset\n    net_config[\'output_channels\'] = 21\n    classes = np.arange(1, 21)\nelif dataset == \'cityscapes\':\n    from dataset.cityscapes import CityscapesDataset as Dataset\n    net_config[\'output_channels\'] = 19\n    classes = np.arange(1, 19)\nelse:\n    raise NotImplementedError\ndel data_config[\'dataset\']\n\nmodelname = config_path.stem\noutput_dir = Path(\'../model\') / modelname\noutput_dir.mkdir(exist_ok=True)\nlog_dir = Path(\'../logs\') / modelname\nlog_dir.mkdir(exist_ok=True)\n\nlogger = debug_logger(log_dir)\nlogger.debug(config)\nlogger.info(f\'Device: {device}\')\nlogger.info(f\'Max Epoch: {max_epoch}\')\n\n# Loss\nloss_fn = MultiClassCriterion(**loss_config).to(device)\nparams = model.parameters()\noptimizer, scheduler = create_optimizer(params, **opt_config)\n\n# history\nif resume:\n    with open(log_dir.joinpath(\'history.pkl\'), \'rb\') as f:\n        history_dict = pickle.load(f)\n        best_metrics = history_dict[\'best_metrics\']\n        loss_history = history_dict[\'loss\']\n        iou_history = history_dict[\'iou\']\n        start_epoch = len(iou_history)\n        for _ in range(start_epoch):\n            scheduler.step()\nelse:\n    start_epoch = 0\n    best_metrics = 0\n    loss_history = []\n    iou_history = []\n\n# Dataset\naffine_augmenter = albu.Compose([albu.HorizontalFlip(p=.5),\n                                 # Rotate(5, p=.5)\n                                 ])\n# image_augmenter = albu.Compose([albu.GaussNoise(p=.5),\n#                                 albu.RandomBrightnessContrast(p=.5)])\nimage_augmenter = None\ntrain_dataset = Dataset(affine_augmenter=affine_augmenter, image_augmenter=image_augmenter,\n                        net_type=net_type, **data_config)\nvalid_dataset = Dataset(split=\'valid\', net_type=net_type, **data_config)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4,\n                          pin_memory=True, drop_last=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)\n\n# To device\nmodel = model.to(device)\n\n# Pretrained model\nif pretrained_path:\n    logger.info(f\'Resume from {pretrained_path}\')\n    param = torch.load(pretrained_path)\n    model.load_state_dict(param)\n    del param\n\n# fp16\nif fp16:\n    from apex import fp16_utils\n    model = fp16_utils.BN_convert_float(model.half())\n    optimizer = fp16_utils.FP16_Optimizer(optimizer, verbose=False, dynamic_loss_scale=True)\n    logger.info(\'Apply fp16\')\n\n# Restore model\nif resume:\n    model_path = output_dir.joinpath(f\'model_tmp.pth\')\n    logger.info(f\'Resume from {model_path}\')\n    param = torch.load(model_path)\n    model.load_state_dict(param)\n    del param\n    opt_path = output_dir.joinpath(f\'opt_tmp.pth\')\n    param = torch.load(opt_path)\n    optimizer.load_state_dict(param)\n    del param\n\n# Train\nfor i_epoch in range(start_epoch, max_epoch):\n    logger.info(f\'Epoch: {i_epoch}\')\n    logger.info(f\'Learning rate: {optimizer.param_groups[0][""lr""]}\')\n\n    train_losses = []\n    train_ious = []\n    model.train()\n    with tqdm(train_loader) as _tqdm:\n        for batched in _tqdm:\n            images, labels, _ = batched\n            if fp16:\n                images = images.half()\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            preds = model(images)\n            if net_type == \'deeplab\':\n                preds = F.interpolate(preds, size=labels.shape[1:], mode=\'bilinear\', align_corners=True)\n            if fp16:\n                loss = loss_fn(preds.float(), labels)\n            else:\n                loss = loss_fn(preds, labels)\n\n            preds_np = preds.detach().cpu().numpy()\n            labels_np = labels.detach().cpu().numpy()\n            iou = compute_iou_batch(np.argmax(preds_np, axis=1), labels_np, classes)\n\n            _tqdm.set_postfix(OrderedDict(seg_loss=f\'{loss.item():.5f}\', iou=f\'{iou:.3f}\'))\n            train_losses.append(loss.item())\n            train_ious.append(iou)\n\n            if fp16:\n                optimizer.backward(loss)\n            else:\n                loss.backward()\n            optimizer.step()\n\n    scheduler.step()\n\n    train_loss = np.mean(train_losses)\n    train_iou = np.nanmean(train_ious)\n    logger.info(f\'train loss: {train_loss}\')\n    logger.info(f\'train iou: {train_iou}\')\n\n    torch.save(model.state_dict(), output_dir.joinpath(\'model_tmp.pth\'))\n    torch.save(optimizer.state_dict(), output_dir.joinpath(\'opt_tmp.pth\'))\n\n    if (i_epoch + 1) % 1 == 0:\n        valid_losses = []\n        valid_ious = []\n        model.eval()\n        with torch.no_grad():\n            with tqdm(valid_loader) as _tqdm:\n                for batched in _tqdm:\n                    images, labels, _ = batched\n                    if fp16:\n                        images = images.half()\n                    images, labels = images.to(device), labels.to(device)\n                    preds = model.tta(images, net_type=net_type)\n                    if fp16:\n                        loss = loss_fn(preds.float(), labels)\n                    else:\n                        loss = loss_fn(preds, labels)\n\n                    preds_np = preds.detach().cpu().numpy()\n                    labels_np = labels.detach().cpu().numpy()\n                    iou = compute_iou_batch(np.argmax(preds_np, axis=1), labels_np, classes)\n\n                    _tqdm.set_postfix(OrderedDict(seg_loss=f\'{loss.item():.5f}\', iou=f\'{iou:.3f}\'))\n                    valid_losses.append(loss.item())\n                    valid_ious.append(iou)\n\n        valid_loss = np.mean(valid_losses)\n        valid_iou = np.mean(valid_ious)\n        logger.info(f\'valid seg loss: {valid_loss}\')\n        logger.info(f\'valid iou: {valid_iou}\')\n\n        if best_metrics < valid_iou:\n            best_metrics = valid_iou\n            logger.info(\'Best Model!\')\n            torch.save(model.state_dict(), output_dir.joinpath(\'model.pth\'))\n            torch.save(optimizer.state_dict(), output_dir.joinpath(\'opt.pth\'))\n    else:\n        valid_loss = None\n        valid_iou = None\n\n    loss_history.append([train_loss, valid_loss])\n    iou_history.append([train_iou, valid_iou])\n    history_ploter(loss_history, log_dir.joinpath(\'loss.png\'))\n    history_ploter(iou_history, log_dir.joinpath(\'iou.png\'))\n\n    history_dict = {\'loss\': loss_history,\n                    \'iou\': iou_history,\n                    \'best_metrics\': best_metrics}\n    with open(log_dir.joinpath(\'history.pkl\'), \'wb\') as f:\n        pickle.dump(history_dict, f)\n'"
src/converter/convert_mobilenetv2.py,8,"b""import argparse\nfrom pathlib import Path\n\nimport tensorflow as tf\nimport torch\nfrom models.net import SPPNet\n\n\ndef convert_mobilenetv2(ckpt_path, num_classes):\n    def conv_converter(pt_layer, tf_layer_name, depthwise=False, bias=False):\n        if depthwise:\n            pt_layer.weight.data = torch.Tensor(\n                reader.get_tensor(f'{tf_layer_name}/depthwise_weights').transpose(2, 3, 0, 1))\n        else:\n            pt_layer.weight.data = torch.Tensor(reader.get_tensor(f'{tf_layer_name}/weights').transpose(3, 2, 0, 1))\n\n        if bias:\n            pt_layer.bias.data = torch.Tensor(reader.get_tensor(f'{tf_layer_name}/biases'))\n\n    def bn_converter(pt_layer, tf_layer_name):\n        pt_layer.bias.data = torch.Tensor(reader.get_tensor(f'{tf_layer_name}/beta'))\n        pt_layer.weight.data = torch.Tensor(reader.get_tensor(f'{tf_layer_name}/gamma'))\n        pt_layer.running_mean.data = torch.Tensor(reader.get_tensor(f'{tf_layer_name}/moving_mean'))\n        pt_layer.running_var.data = torch.Tensor(reader.get_tensor(f'{tf_layer_name}/moving_variance'))\n\n    def block_converter(pt_layer, tf_layer_name):\n        if hasattr(pt_layer, 'expand'):\n            conv_converter(pt_layer.expand.conv, f'{tf_layer_name}/expand')\n            bn_converter(pt_layer.expand.bn, f'{tf_layer_name}/expand/BatchNorm')\n\n        conv_converter(pt_layer.depthwise.conv, f'{tf_layer_name}/depthwise', depthwise=True)\n        bn_converter(pt_layer.depthwise.bn, f'{tf_layer_name}/depthwise/BatchNorm')\n\n        conv_converter(pt_layer.project.conv, f'{tf_layer_name}/project')\n        bn_converter(pt_layer.project.bn, f'{tf_layer_name}/project/BatchNorm')\n\n    reader = tf.train.NewCheckpointReader(ckpt_path)\n    model = SPPNet(num_classes, enc_type='mobilenetv2', dec_type='maspp')\n\n    # MobileNetV2\n    conv_converter(model.encoder.conv, 'MobilenetV2/Conv')\n    bn_converter(model.encoder.bn, 'MobilenetV2/Conv/BatchNorm')\n\n    block_converter(model.encoder.block0, 'MobilenetV2/expanded_conv')\n    block_converter(model.encoder.block1, 'MobilenetV2/expanded_conv_1')\n    block_converter(model.encoder.block2, 'MobilenetV2/expanded_conv_2')\n    block_converter(model.encoder.block3, 'MobilenetV2/expanded_conv_3')\n    block_converter(model.encoder.block4, 'MobilenetV2/expanded_conv_4')\n    block_converter(model.encoder.block5, 'MobilenetV2/expanded_conv_5')\n    block_converter(model.encoder.block6, 'MobilenetV2/expanded_conv_6')\n    block_converter(model.encoder.block7, 'MobilenetV2/expanded_conv_7')\n    block_converter(model.encoder.block8, 'MobilenetV2/expanded_conv_8')\n    block_converter(model.encoder.block9, 'MobilenetV2/expanded_conv_9')\n    block_converter(model.encoder.block10, 'MobilenetV2/expanded_conv_10')\n    block_converter(model.encoder.block11, 'MobilenetV2/expanded_conv_11')\n    block_converter(model.encoder.block12, 'MobilenetV2/expanded_conv_12')\n    block_converter(model.encoder.block13, 'MobilenetV2/expanded_conv_13')\n    block_converter(model.encoder.block14, 'MobilenetV2/expanded_conv_14')\n    block_converter(model.encoder.block15, 'MobilenetV2/expanded_conv_15')\n    block_converter(model.encoder.block16, 'MobilenetV2/expanded_conv_16')\n\n    # SPP\n    conv_converter(model.spp.aspp0.conv, 'aspp0')\n    bn_converter(model.spp.aspp0.bn, 'aspp0/BatchNorm')\n    conv_converter(model.spp.image_pooling.conv, 'image_pooling')\n    bn_converter(model.spp.image_pooling.bn, 'image_pooling/BatchNorm')\n    conv_converter(model.spp.conv, 'concat_projection')\n    bn_converter(model.spp.bn, 'concat_projection/BatchNorm')\n\n    # Logits\n    conv_converter(model.logits, 'logits/semantic', bias=True)\n\n    return model\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('ckpt_path')\n    parser.add_argument('num_classes', type=int)\n    parser.add_argument('output_path')\n    args = parser.parse_args()\n\n    ckpt_path = args.ckpt_path\n    num_classes = args.num_classes\n    output_path = Path(args.output_path)\n    output_path.parent.mkdir()\n\n    model = convert_mobilenetv2(ckpt_path, num_classes)\n    torch.save(model.state_dict(), output_path)\n"""
src/converter/convert_xception65.py,8,"b""import argparse\nfrom pathlib import Path\n\nimport tensorflow as tf\nimport torch\nfrom models.net import SPPNet\n\n\ndef convert_xception65(ckpt_path, num_classes):\n    def conv_converter(pt_layer, tf_layer_name, depthwise=False, bias=False):\n        if depthwise:\n            pt_layer.weight.data = torch.Tensor(\n                reader.get_tensor(f'{tf_layer_name}/depthwise_weights').transpose(2, 3, 0, 1))\n        else:\n            pt_layer.weight.data = torch.Tensor(reader.get_tensor(f'{tf_layer_name}/weights').transpose(3, 2, 0, 1))\n\n        if bias:\n            pt_layer.bias.data = torch.Tensor(reader.get_tensor(f'{tf_layer_name}/biases'))\n\n    def bn_converter(pt_layer, tf_layer_name):\n        pt_layer.bias.data = torch.Tensor(reader.get_tensor(f'{tf_layer_name}/beta'))\n        pt_layer.weight.data = torch.Tensor(reader.get_tensor(f'{tf_layer_name}/gamma'))\n        pt_layer.running_mean.data = torch.Tensor(reader.get_tensor(f'{tf_layer_name}/moving_mean'))\n        pt_layer.running_var.data = torch.Tensor(reader.get_tensor(f'{tf_layer_name}/moving_variance'))\n\n    def sepconv_converter(pt_layer, tf_layer_name):\n        conv_converter(pt_layer.depthwise, f'{tf_layer_name}_depthwise', True)\n        bn_converter(pt_layer.bn_depth, f'{tf_layer_name}_depthwise/BatchNorm')\n        conv_converter(pt_layer.pointwise, f'{tf_layer_name}_pointwise')\n        bn_converter(pt_layer.bn_point, f'{tf_layer_name}_pointwise/BatchNorm')\n\n    def block_converter(pt_block, tf_block_name):\n        if pt_block.skip_connection_type == 'conv':\n            conv_converter(pt_block.conv, f'{tf_block_name}/shortcut')\n            bn_converter(pt_block.bn, f'{tf_block_name}/shortcut/BatchNorm')\n\n        sepconv_converter(pt_block.sep_conv1.block, f'{tf_block_name}/separable_conv1')\n        sepconv_converter(pt_block.sep_conv2.block, f'{tf_block_name}/separable_conv2')\n        sepconv_converter(pt_block.sep_conv3.block, f'{tf_block_name}/separable_conv3')\n\n    reader = tf.train.NewCheckpointReader(ckpt_path)\n    model = SPPNet(num_classes, enc_type='xception65', dec_type='aspp', output_stride=8)\n\n    # Xception\n    ## Entry flow\n    conv_converter(model.encoder.conv1, 'xception_65/entry_flow/conv1_1')\n    bn_converter(model.encoder.bn1, 'xception_65/entry_flow/conv1_1/BatchNorm')\n    conv_converter(model.encoder.conv2, 'xception_65/entry_flow/conv1_2')\n    bn_converter(model.encoder.bn2, 'xception_65/entry_flow/conv1_2/BatchNorm')\n    block_converter(model.encoder.block1, 'xception_65/entry_flow/block1/unit_1/xception_module')\n    block_converter(model.encoder.block2, 'xception_65/entry_flow/block2/unit_1/xception_module')\n    block_converter(model.encoder.block3, 'xception_65/entry_flow/block3/unit_1/xception_module')\n    ## Middle flow\n    block_converter(model.encoder.block4, 'xception_65/middle_flow/block1/unit_1/xception_module')\n    block_converter(model.encoder.block5, 'xception_65/middle_flow/block1/unit_2/xception_module')\n    block_converter(model.encoder.block6, 'xception_65/middle_flow/block1/unit_3/xception_module')\n    block_converter(model.encoder.block7, 'xception_65/middle_flow/block1/unit_4/xception_module')\n    block_converter(model.encoder.block8, 'xception_65/middle_flow/block1/unit_5/xception_module')\n    block_converter(model.encoder.block9, 'xception_65/middle_flow/block1/unit_6/xception_module')\n    block_converter(model.encoder.block10, 'xception_65/middle_flow/block1/unit_7/xception_module')\n    block_converter(model.encoder.block11, 'xception_65/middle_flow/block1/unit_8/xception_module')\n    block_converter(model.encoder.block12, 'xception_65/middle_flow/block1/unit_9/xception_module')\n    block_converter(model.encoder.block13, 'xception_65/middle_flow/block1/unit_10/xception_module')\n    block_converter(model.encoder.block14, 'xception_65/middle_flow/block1/unit_11/xception_module')\n    block_converter(model.encoder.block15, 'xception_65/middle_flow/block1/unit_12/xception_module')\n    block_converter(model.encoder.block16, 'xception_65/middle_flow/block1/unit_13/xception_module')\n    block_converter(model.encoder.block17, 'xception_65/middle_flow/block1/unit_14/xception_module')\n    block_converter(model.encoder.block18, 'xception_65/middle_flow/block1/unit_15/xception_module')\n    block_converter(model.encoder.block19, 'xception_65/middle_flow/block1/unit_16/xception_module')\n    ## Exit flow\n    block_converter(model.encoder.block20, 'xception_65/exit_flow/block1/unit_1/xception_module')\n    block_converter(model.encoder.block21, 'xception_65/exit_flow/block2/unit_1/xception_module')\n\n    # ASPP\n    conv_converter(model.spp.aspp0.conv, 'aspp0')\n    bn_converter(model.spp.aspp0.bn, 'aspp0/BatchNorm')\n    sepconv_converter(model.spp.aspp1.block, 'aspp1')\n    sepconv_converter(model.spp.aspp2.block, 'aspp2')\n    sepconv_converter(model.spp.aspp3.block, 'aspp3')\n\n    conv_converter(model.spp.image_pooling.conv, 'image_pooling')\n    bn_converter(model.spp.image_pooling.bn, 'image_pooling/BatchNorm')\n    conv_converter(model.spp.conv, 'concat_projection')\n    bn_converter(model.spp.bn, 'concat_projection/BatchNorm')\n\n    # Decoder\n    conv_converter(model.decoder.conv, 'decoder/feature_projection0')\n    bn_converter(model.decoder.bn, 'decoder/feature_projection0/BatchNorm')\n\n    sepconv_converter(model.decoder.sep1.block, 'decoder/decoder_conv0')\n    sepconv_converter(model.decoder.sep2.block, 'decoder/decoder_conv1')\n\n    # Logits\n    conv_converter(model.logits, 'logits/semantic', bias=True)\n\n    return model\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('ckpt_path')\n    parser.add_argument('num_classes', type=int)\n    parser.add_argument('output_path')\n    args = parser.parse_args()\n\n    ckpt_path = args.ckpt_path\n    num_classes = args.num_classes\n    output_path = Path(args.output_path)\n    output_path.parent.mkdir()\n\n    model = convert_xception65(ckpt_path, num_classes)\n    torch.save(model.state_dict(), output_path)\n"""
src/dataset/__init__.py,0,b''
src/dataset/apolloscape.py,2,"b""import numpy as np\nfrom PIL import Image\nfrom pathlib import Path\nimport albumentations as albu\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\n\n# class definitions -> http://apolloscape.auto/scene.html\nn_classes = 36\nvoid_classes = [0, 1, 17, 34, 162, 35, 163, 37, 165, 38, 166, 39, 167, 40, 168, 50, 65, 66, 67, 81, 82, 83, 84, 85, 86, 97, 98, 99, 100, 113, 255]\nvalid_classes = [33, 161, 36, 164, 49, 81]\nclass_map = dict(zip(valid_classes, range(n_classes)))\n\n\nclass ApolloscapeDataset(Dataset):\n    def __init__(self,\n                 base_dir='../../data/apolloscape',\n                 road_record_list=[{'road':'road02_seg','record':[22, 23, 24, 25, 26]}, {'road':'road03_seg', 'record':[7, 8, 9, 10, 11, 12]}],\n                 split='train',\n                 ignore_index=255,\n                 debug=False):\n        self.debug = debug\n        self.base_dir = Path(base_dir)\n        self.ignore_index = ignore_index\n        self.split = split\n        self.img_paths = []\n        self.lbl_paths = []\n\n        for road_record in road_record_list:\n          self.road_dir = self.base_dir / Path(road_record['road'])\n          self.record_list = road_record['record']\n\n          for record in self.record_list:\n            img_paths_tmp = self.road_dir.glob(f'ColorImage/Record{record:03}/Camera 5/*.jpg')\n            lbl_paths_tmp = self.road_dir.glob(f'Label/Record{record:03}/Camera 5/*.png')\n\n            img_paths_basenames = {Path(img_path.name).stem for img_path in img_paths_tmp}\n            lbl_paths_basenames = {Path(lbl_path.name).stem.replace('_bin', '') for lbl_path in lbl_paths_tmp}\n\n            intersection_basenames = img_paths_basenames & lbl_paths_basenames\n\n            img_paths_intersection = [self.road_dir / Path(f'ColorImage/Record{record:03}/Camera 5/{intersection_basename}.jpg')\n                                      for intersection_basename in intersection_basenames]\n            lbl_paths_intersection = [self.road_dir / Path(f'Label/Record{record:03}/Camera 5/{intersection_basename}_bin.png')\n                                      for intersection_basename in intersection_basenames]\n\n            self.img_paths += img_paths_intersection\n            self.lbl_paths += lbl_paths_intersection\n\n        self.img_paths.sort()\n        self.lbl_paths.sort()\n        print(len(self.img_paths), len(self.lbl_paths))\n        assert len(self.img_paths) == len(self.lbl_paths)\n\n        self.resizer = albu.Resize(height=512, width=1024)\n        self.augmenter = albu.Compose([albu.HorizontalFlip(p=0.5),\n                                       # albu.RandomRotate90(p=0.5),\n                                       albu.Rotate(limit=10, p=0.5),\n                                       # albu.CLAHE(p=0.2),\n                                       # albu.RandomContrast(p=0.2),\n                                       # albu.RandomBrightness(p=0.2),\n                                       # albu.RandomGamma(p=0.2),\n                                       # albu.GaussNoise(p=0.2),\n                                       # albu.Cutout(p=0.2)\n                                       ])\n        self.img_transformer = transforms.Compose([transforms.ToTensor(),\n                                                   transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                                                        std=[0.229, 0.224, 0.225])])\n        self.lbl_transformer = torch.LongTensor\n\n    def __len__(self):\n        return len(self.img_paths)\n\n    def __getitem__(self, index):\n        img_path = self.img_paths[index]\n        lbl_path = self.lbl_paths[index]\n\n        img = np.array(Image.open(img_path))\n        lbl = np.array(Image.open(lbl_path))\n        for c in void_classes:\n            lbl[lbl == c] = self.ignore_index\n        for c in valid_classes:\n            lbl[lbl == c] = class_map[c]\n\n        resized = self.resizer(image=img, mask=lbl)\n        img, lbl = resized['image'], resized['mask']\n\n        if self.split == 'train':\n            augmented = self.augmenter(image=img, mask=lbl)\n            img, lbl = augmented['image'], augmented['mask']\n\n        if self.debug:\n            print(np.unique(lbl))\n        else:\n            img = self.img_transformer(img)\n            lbl = self.lbl_transformer(lbl)\n\n        return img, lbl, img_path.stem\n\n\nif __name__ == '__main__':\n    import matplotlib\n    matplotlib.use('Agg')\n    import matplotlib.pyplot as plt\n\n    dataset = ApolloscapeDataset(base_dir='../../data/apolloscape', debug=True)\n    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n    print(len(dataset))\n\n    for i, batched in enumerate(dataloader):\n        images, labels, _ = batched\n        if i == 0:\n            fig, axes = plt.subplots(8, 2, figsize=(10, 30))\n            plt.tight_layout()\n            for j in range(8):\n                axes[j][0].imshow(images[j])\n                axes[j][1].imshow(labels[j])\n            plt.savefig('apolloscape.png')\n            plt.close()\n        break\n"""
src/dataset/cityscapes.py,4,"b""from functools import partial\nimport numpy as np\nfrom PIL import Image\nfrom pathlib import Path\nimport albumentations as albu\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom utils.preprocess import minmax_normalize, meanstd_normalize\nfrom utils.custum_aug import PadIfNeededRightBottom\n\n\nclass CityscapesDataset(Dataset):\n    n_classes = 19\n    void_classes = [0, 1, 2, 3, 4, 5, 6, 9, 10, 14, 15, 16, 18, 29, 30, -1]\n    valid_classes = [7, 8, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33]\n    class_map = dict(zip(valid_classes, range(n_classes)))\n\n    def __init__(self, base_dir='../data/cityscapes', split='train',\n                 affine_augmenter=None, image_augmenter=None, target_size=(1024, 2048),\n                 net_type='unet', ignore_index=255, debug=False):\n        self.debug = debug\n        self.base_dir = Path(base_dir)\n        assert net_type in ['unet', 'deeplab']\n        self.net_type = net_type\n        self.ignore_index = ignore_index\n        self.split = 'val' if split == 'valid' else split\n\n        self.img_paths = sorted(self.base_dir.glob(f'leftImg8bit/{self.split}/*/*leftImg8bit.png'))\n        self.lbl_paths = sorted(self.base_dir.glob(f'gtFine/{self.split}/*/*gtFine_labelIds.png'))\n        assert len(self.img_paths) == len(self.lbl_paths)\n\n        # Resize\n        if isinstance(target_size, str):\n            target_size = eval(target_size)\n        if self.split == 'train':\n            if self.net_type == 'deeplab':\n                target_size = (target_size[0] + 1, target_size[1] + 1)\n            self.resizer = albu.Compose([albu.RandomScale(scale_limit=(-0.5, 0.5), p=1.0),\n                                         PadIfNeededRightBottom(min_height=target_size[0], min_width=target_size[1],\n                                                                value=0, ignore_index=self.ignore_index, p=1.0),\n                                         albu.RandomCrop(height=target_size[0], width=target_size[1], p=1.0)])\n        else:\n            self.resizer = None\n\n        # Augment\n        if self.split == 'train':\n            self.affine_augmenter = affine_augmenter\n            self.image_augmenter = image_augmenter\n        else:\n            self.affine_augmenter = None\n            self.image_augmenter = None\n\n    def __len__(self):\n        return len(self.img_paths)\n\n    def __getitem__(self, index):\n        img_path = self.img_paths[index]\n        img = np.array(Image.open(img_path))\n        if self.split == 'test':\n            # Resize (Scale & Pad & Crop)\n            if self.net_type == 'unet':\n                img = minmax_normalize(img)\n                img = meanstd_normalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n            else:\n                img = minmax_normalize(img, norm_range=(-1, 1))\n            if self.resizer:\n                resized = self.resizer(image=img)\n                img = resized['image']\n            img = img.transpose(2, 0, 1)\n            img = torch.FloatTensor(img)\n            return img\n        else:\n            lbl_path = self.lbl_paths[index]\n            lbl = np.array(Image.open(lbl_path))\n            lbl = self.encode_mask(lbl)\n            # ImageAugment (RandomBrightness, AddNoise...)\n            if self.image_augmenter:\n                augmented = self.image_augmenter(image=img)\n                img = augmented['image']\n            # Resize (Scale & Pad & Crop)\n            if self.net_type == 'unet':\n                img = minmax_normalize(img)\n                img = meanstd_normalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n            else:\n                img = minmax_normalize(img, norm_range=(-1, 1))\n            if self.resizer:\n                resized = self.resizer(image=img, mask=lbl)\n                img, lbl = resized['image'], resized['mask']\n            # AffineAugment (Horizontal Flip, Rotate...)\n            if self.affine_augmenter:\n                augmented = self.affine_augmenter(image=img, mask=lbl)\n                img, lbl = augmented['image'], augmented['mask']\n\n            if self.debug:\n                print(lbl_path)\n                print(np.unique(lbl))\n            else:\n                img = img.transpose(2, 0, 1)\n                img = torch.FloatTensor(img)\n                lbl = torch.LongTensor(lbl)\n            return img, lbl, img_path.stem\n\n    def encode_mask(self, lbl):\n        for c in self.void_classes:\n            lbl[lbl == c] = self.ignore_index\n        for c in self.valid_classes:\n            lbl[lbl == c] = self.class_map[c]\n        return lbl\n\n\nif __name__ == '__main__':\n    import matplotlib\n\n    matplotlib.use('Agg')\n    import matplotlib.pyplot as plt\n    from utils.custum_aug import Rotate\n\n    affine_augmenter = albu.Compose([albu.HorizontalFlip(p=.5),\n                                     # Rotate(5, p=.5)\n                                     ])\n    # image_augmenter = albu.Compose([albu.GaussNoise(p=.5),\n    #                                 albu.RandomBrightnessContrast(p=.5)])\n    image_augmenter = None\n    dataset = CityscapesDataset(split='train', net_type='deeplab', ignore_index=19, debug=True,\n                                affine_augmenter=affine_augmenter, image_augmenter=image_augmenter)\n    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n    print(len(dataset))\n\n    for i, batched in enumerate(dataloader):\n        images, labels, _ = batched\n        if i == 0:\n            fig, axes = plt.subplots(8, 2, figsize=(20, 48))\n            plt.tight_layout()\n            for j in range(8):\n                axes[j][0].imshow(minmax_normalize(images[j], norm_range=(0, 1), orig_range=(-1, 1)))\n                axes[j][1].imshow(labels[j])\n                axes[j][0].set_xticks([])\n                axes[j][0].set_yticks([])\n                axes[j][1].set_xticks([])\n                axes[j][1].set_yticks([])\n            plt.savefig('dataset/cityscapes.png')\n            plt.close()\n        break\n"""
src/dataset/pascal_voc.py,4,"b""from functools import partial\nimport numpy as np\nfrom PIL import Image\nfrom pathlib import Path\nimport albumentations as albu\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom utils.preprocess import minmax_normalize, meanstd_normalize\nfrom utils.custum_aug import PadIfNeededRightBottom\n\n\nclass PascalVocDataset(Dataset):\n    n_classes = 21\n\n    def __init__(self, base_dir='../data/pascal_voc_2012/VOCdevkit/VOC2012', split='train_aug',\n                 affine_augmenter=None, image_augmenter=None, target_size=(512, 512),\n                 net_type='unet', ignore_index=255, debug=False):\n        self.debug = debug\n        self.base_dir = Path(base_dir)\n        assert net_type in ['unet', 'deeplab']\n        self.net_type = net_type\n        self.ignore_index = ignore_index\n        self.split = split\n\n        valid_ids = self.base_dir / 'ImageSets' / 'Segmentation' / 'val.txt'\n        with open(valid_ids, 'r') as f:\n            valid_ids = f.readlines()\n        if self.split == 'valid':\n            lbl_dir = 'SegmentationClass'\n            img_ids = valid_ids\n        else:\n            valid_set = set([valid_id.strip() for valid_id in valid_ids])\n            lbl_dir = 'SegmentationClassAug' if 'aug' in split else 'SegmentationClass'\n            all_set = set([p.name[:-4] for p in self.base_dir.joinpath(lbl_dir).iterdir()])\n            img_ids = list(all_set - valid_set)\n        self.img_paths = [(self.base_dir / 'JPEGImages' / f'{img_id.strip()}.jpg') for img_id in img_ids]\n        self.lbl_paths = [(self.base_dir / lbl_dir / f'{img_id.strip()}.png') for img_id in img_ids]\n\n        # Resize\n        if isinstance(target_size, str):\n            target_size = eval(target_size)\n        if 'train' in self.split:\n            if self.net_type == 'deeplab':\n                target_size = (target_size[0] + 1, target_size[1] + 1)\n            self.resizer = albu.Compose([albu.RandomScale(scale_limit=(-0.5, 0.5), p=1.0),\n                                         PadIfNeededRightBottom(min_height=target_size[0], min_width=target_size[1],\n                                                                value=0, ignore_index=self.ignore_index, p=1.0),\n                                         albu.RandomCrop(height=target_size[0], width=target_size[1], p=1.0)])\n        else:\n            # self.resizer = None\n            self.resizer = albu.Compose([PadIfNeededRightBottom(min_height=target_size[0], min_width=target_size[1],\n                                                                value=0, ignore_index=self.ignore_index, p=1.0),\n                                         albu.Crop(x_min=0, x_max=target_size[1],\n                                                   y_min=0, y_max=target_size[0])])\n\n        # Augment\n        if 'train' in self.split:\n            self.affine_augmenter = affine_augmenter\n            self.image_augmenter = image_augmenter\n        else:\n            self.affine_augmenter = None\n            self.image_augmenter = None\n\n    def __len__(self):\n        return len(self.img_paths)\n\n    def __getitem__(self, index):\n        img_path = self.img_paths[index]\n        img = np.array(Image.open(img_path))\n        if self.split == 'test':\n            # Resize (Scale & Pad & Crop)\n            if self.net_type == 'unet':\n                img = minmax_normalize(img)\n                img = meanstd_normalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n            else:\n                img = minmax_normalize(img, norm_range=(-1, 1))\n            if self.resizer:\n                resized = self.resizer(image=img)\n                img = resized['image']\n            img = img.transpose(2, 0, 1)\n            img = torch.FloatTensor(img)\n            return img\n        else:\n            lbl_path = self.lbl_paths[index]\n            lbl = np.array(Image.open(lbl_path))\n            lbl[lbl == 255] = 0\n            # ImageAugment (RandomBrightness, AddNoise...)\n            if self.image_augmenter:\n                augmented = self.image_augmenter(image=img)\n                img = augmented['image']\n            # Resize (Scale & Pad & Crop)\n            if self.net_type == 'unet':\n                img = minmax_normalize(img)\n                img = meanstd_normalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n            else:\n                img = minmax_normalize(img, norm_range=(-1, 1))\n            if self.resizer:\n                resized = self.resizer(image=img, mask=lbl)\n                img, lbl = resized['image'], resized['mask']\n            # AffineAugment (Horizontal Flip, Rotate...)\n            if self.affine_augmenter:\n                augmented = self.affine_augmenter(image=img, mask=lbl)\n                img, lbl = augmented['image'], augmented['mask']\n\n            if self.debug:\n                print(lbl_path)\n                print(lbl.shape)\n                print(np.unique(lbl))\n            else:\n                img = img.transpose(2, 0, 1)\n                img = torch.FloatTensor(img)\n                lbl = torch.LongTensor(lbl)\n            return img, lbl, img_path.stem\n\n\nif __name__ == '__main__':\n    import matplotlib\n    matplotlib.use('Agg')\n    import matplotlib.pyplot as plt\n    from utils.custum_aug import Rotate\n\n    affine_augmenter = albu.Compose([albu.HorizontalFlip(p=.5),\n                                     Rotate(5, p=.5)\n                                     ])\n    # image_augmenter = albu.Compose([albu.GaussNoise(p=.5),\n    #                                 albu.RandomBrightnessContrast(p=.5)])\n    image_augmenter = None\n    dataset = PascalVocDataset(affine_augmenter=affine_augmenter, image_augmenter=image_augmenter, split='valid',\n                               net_type='deeplab', ignore_index=21, target_size=(512, 512), debug=True)\n    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n    print(len(dataset))\n\n    for i, batched in enumerate(dataloader):\n        images, labels, _ = batched\n        if i == 0:\n            fig, axes = plt.subplots(8, 2, figsize=(20, 48))\n            plt.tight_layout()\n            for j in range(8):\n                axes[j][0].imshow(minmax_normalize(images[j], norm_range=(0, 1), orig_range=(-1, 1)))\n                axes[j][1].imshow(labels[j])\n                axes[j][0].set_xticks([])\n                axes[j][0].set_yticks([])\n                axes[j][1].set_xticks([])\n                axes[j][1].set_yticks([])\n            plt.savefig('dataset/pascal_voc.png')\n            plt.close()\n        break\n"""
src/logger/__init__.py,0,b''
src/logger/log.py,0,"b""from logging import getLogger, StreamHandler, INFO, DEBUG, Formatter, FileHandler\n\n\ndef debug_logger(log_dir):\n    logger = getLogger('train')\n    logger.setLevel(DEBUG)\n\n    fmt = Formatter('%(asctime)s %(name)s %(lineno)d [%(levelname)s][%(funcName)s] %(message)s')\n\n    sh = StreamHandler()\n    sh.setLevel(INFO)\n    sh.setFormatter(fmt)\n    logger.addHandler(sh)\n\n    fh = FileHandler(filename=log_dir.joinpath('debug.txt'), mode='w')\n    fh.setLevel(DEBUG)\n    fh.setFormatter(fmt)\n    logger.addHandler(fh)\n    return logger\n"""
src/logger/plot.py,0,"b""import numpy as np\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\n\n\ndef history_ploter(history, path):\n    history = np.asarray(history)\n    title = path.name[:-4]\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    x = np.arange(len(history))\n    if history.ndim == 1:\n        y = history\n        ax.plot(x[y != None], y[y != None])\n    else:\n        y = history[:, 0]\n        ax.plot(x[y != None], y[y != None], label='train')\n        y = history[:, 1]\n        ax.plot(x[y != None], y[y != None], label='valid')\n        ax.legend()\n    ax.set_title(title)\n    plt.savefig(str(path))\n    plt.close()\n"""
src/models/__init__.py,0,b''
src/models/common.py,1,"b""from collections import OrderedDict\nimport torch.nn as nn\n\n\nclass _ActivatedBatchNorm(nn.Module):\n    def __init__(self, num_features, activation='relu', slope=0.01, **kwargs):\n        super().__init__()\n        self.bn = nn.BatchNorm2d(num_features, **kwargs)\n        if activation == 'relu':\n            self.act = nn.ReLU(inplace=True)\n        elif activation == 'leaky_relu':\n            self.act = nn.LeakyReLU(negative_slope=slope, inplace=True)\n        elif activation == 'elu':\n            self.act = nn.ELU(inplace=True)\n        else:\n            self.act = None\n\n    def forward(self, x):\n        x = self.bn(x)\n        if self.act:\n            x = self.act(x)\n        return x\n\n\nclass SeparableConv2d(nn.Module):\n    def __init__(self, inplanes, planes, kernel_size=3, stride=1, dilation=1, relu_first=True):\n        super().__init__()\n        depthwise = nn.Conv2d(inplanes, inplanes, kernel_size,\n                              stride=stride, padding=dilation,\n                              dilation=dilation, groups=inplanes, bias=False)\n        bn_depth = nn.BatchNorm2d(inplanes)\n        pointwise = nn.Conv2d(inplanes, planes, 1, bias=False)\n        bn_point = nn.BatchNorm2d(planes)\n\n        if relu_first:\n            self.block = nn.Sequential(OrderedDict([('relu', nn.ReLU()),\n                                                    ('depthwise', depthwise),\n                                                    ('bn_depth', bn_depth),\n                                                    ('pointwise', pointwise),\n                                                    ('bn_point', bn_point)\n                                                    ]))\n        else:\n            self.block = nn.Sequential(OrderedDict([('depthwise', depthwise),\n                                                    ('bn_depth', bn_depth),\n                                                    ('relu1', nn.ReLU()),\n                                                    ('pointwise', pointwise),\n                                                    ('bn_point', bn_point),\n                                                    ('relu2', nn.ReLU())\n                                                    ]))\n\n    def forward(self, x):\n        return self.block(x)\n\n\n# import os\n# actbn_env = os.environ.get('INPLACE_ABN')\n# if actbn_env:\n#     from .inplace_abn import InPlaceABN\n#     ActivatedBatchNorm = InPlaceABN\n# else:\n#     ActivatedBatchNorm = _ActivatedBatchNorm\n\nActivatedBatchNorm = _ActivatedBatchNorm\n"""
src/models/decoder.py,6,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .common import ActivatedBatchNorm, SeparableConv2d\nfrom .ibn import ImprovedIBNaDecoderBlock\nfrom .scse import SELayer, SCSEBlock\nfrom .oc import BaseOC\n\n\nclass DecoderUnetSCSE(nn.Module):\n    def __init__(self, in_channels, middle_channels, out_channels):\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.Conv2d(in_channels, middle_channels, kernel_size=3, padding=1),\n            ActivatedBatchNorm(middle_channels),\n            SCSEBlock(middle_channels),\n            nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=4, stride=2, padding=1)\n        )\n\n    def forward(self, *args):\n        x = torch.cat(args, 1)\n        return self.block(x)\n\n\nclass DecoderUnetSEIBN(nn.Module):\n    def __init__(self, in_channels, middle_channels, out_channels):\n        super().__init__()\n        self.block = nn.Sequential(\n            SELayer(in_channels),\n            ImprovedIBNaDecoderBlock(in_channels, out_channels)\n        )\n\n    def forward(self, *args):\n        x = torch.cat(args, 1)\n        return self.block(x)\n\n\nclass DecoderUnetOC(nn.Module):\n    def __init__(self, in_channels, middle_channels, out_channels):\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.Conv2d(in_channels, middle_channels, kernel_size=3, padding=1),\n            ActivatedBatchNorm(middle_channels),\n            BaseOC(in_channels=middle_channels,\n                   out_channels=middle_channels,\n                   dropout=0.2),\n            nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=4, stride=2, padding=1),\n        )\n\n    def forward(self, *args):\n        x = torch.cat(args, 1)\n        return self.block(x)\n\n\nclass DecoderSPP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Conv2d(256, 48, 1, bias=False)\n        self.bn = nn.BatchNorm2d(48)\n        self.relu = nn.ReLU(inplace=True)\n        self.sep1 = SeparableConv2d(304, 256, relu_first=False)\n        self.sep2 = SeparableConv2d(256, 256, relu_first=False)\n\n    def forward(self, x, low_level_feat):\n        x = F.interpolate(x, size=low_level_feat.shape[2:], mode='bilinear', align_corners=True)\n        low_level_feat = self.conv(low_level_feat)\n        low_level_feat = self.bn(low_level_feat)\n        low_level_feat = self.relu(low_level_feat)\n        x = torch.cat((x, low_level_feat), dim=1)\n        x = self.sep1(x)\n        x = self.sep2(x)\n        return x\n\n\ndef create_decoder(dec_type):\n    if dec_type == 'unet_scse':\n        return DecoderUnetSCSE\n    elif dec_type == 'unet_seibn':\n        return DecoderUnetSEIBN\n    elif dec_type == 'unet_oc':\n        return DecoderUnetOC\n    else:\n        raise NotImplementedError\n"""
src/models/encoder.py,1,"b'import torch.nn as nn\nfrom torchvision import models\nimport pretrainedmodels\nfrom .xception import Xception65\nfrom .mobilenet import MobileNetV2\n\n\ndef resnet(name, pretrained=False):\n    def get_channels(layer):\n        block = layer[-1]\n        if isinstance(block, models.resnet.BasicBlock):\n            return block.conv2.out_channels\n        elif isinstance(block, models.resnet.Bottleneck):\n            return block.conv3.out_channels\n        raise RuntimeError(""unknown resnet block: {}"".format(block))\n\n    if name == \'resnet18\':\n        resnet = models.resnet18(pretrained=pretrained)\n    elif name == \'resnet34\':\n        resnet = models.resnet34(pretrained=pretrained)\n    elif name == \'resnet50\':\n        resnet = models.resnet50(pretrained=pretrained)\n    elif name == \'resnet101\':\n        resnet = models.resnet101(pretrained=pretrained)\n    elif name == \'resnet152\':\n        resnet = models.resnet152(pretrained=pretrained)\n    else:\n        return NotImplemented\n\n    layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)\n    layer0.out_channels = resnet.bn1.num_features\n    resnet.layer1.out_channels = get_channels(resnet.layer1)\n    resnet.layer2.out_channels = get_channels(resnet.layer2)\n    resnet.layer3.out_channels = get_channels(resnet.layer3)\n    resnet.layer4.out_channels = get_channels(resnet.layer4)\n    return [layer0, resnet.layer1, resnet.layer2, resnet.layer3, resnet.layer4]\n\n\ndef resnext(name, pretrained=False):\n    if name in [\'resnext101_32x4d\', \'resnext101_64x4d\']:\n        pretrained = \'imagenet\' if pretrained else None\n        resnext = pretrainedmodels.__dict__[name](num_classes=1000, pretrained=pretrained)\n    else:\n        return NotImplemented\n\n    layer0 = nn.Sequential(resnext.features[0],\n                           resnext.features[1],\n                           resnext.features[2],\n                           resnext.features[3])\n    layer1 = resnext.features[4]\n    layer2 = resnext.features[5]\n    layer3 = resnext.features[6]\n    layer4 = resnext.features[7]\n\n    layer0.out_channels = 64\n    layer1.out_channels = 256\n    layer2.out_channels = 512\n    layer3.out_channels = 1024\n    layer4.out_channels = 2048\n    return [layer0, layer1, layer2, layer3, layer4]\n\n\ndef se_net(name, pretrained=False):\n    if name in [\'se_resnet50\', \'se_resnet101\', \'se_resnet152\',\n                \'se_resnext50_32x4d\', \'se_resnext101_32x4d\', \'senet154\']:\n        pretrained = \'imagenet\' if pretrained else None\n        senet = pretrainedmodels.__dict__[name](num_classes=1000, pretrained=pretrained)\n    else:\n        return NotImplemented\n\n    layer0 = senet.layer0\n    layer1 = senet.layer1\n    layer2 = senet.layer2\n    layer3 = senet.layer3\n    layer4 = senet.layer4\n\n    layer0.out_channels = senet.layer1[0].conv1.in_channels\n    layer1.out_channels = senet.layer1[-1].conv3.out_channels\n    layer2.out_channels = senet.layer2[-1].conv3.out_channels\n    layer3.out_channels = senet.layer3[-1].conv3.out_channels\n    layer4.out_channels = senet.layer4[-1].conv3.out_channels\n\n    return [layer0, layer1, layer2, layer3, layer4]\n\n\ndef create_encoder(enc_type, output_stride=8, pretrained=True):\n    if enc_type.startswith(\'resnet\'):\n        return resnet(enc_type, pretrained)\n    elif enc_type.startswith(\'resnext\'):\n        return resnext(enc_type, pretrained)\n    elif enc_type.startswith(\'se\'):\n        return se_net(enc_type, pretrained)\n    elif enc_type == \'xception65\':\n        return Xception65(output_stride)\n    elif enc_type == \'mobilenetv2\':\n        return MobileNetV2(pretrained)\n    else:\n        raise NotImplementedError\n'"
src/models/ibn.py,3,"b'import torch\nimport torch.nn as nn\nfrom .common import ActivatedBatchNorm\n\n\nclass IBN(nn.Module):\n    def __init__(self, planes):\n        super().__init__()\n        half1 = int(planes / 2)\n        self.half = half1\n        half2 = planes - half1\n        self.IN = nn.Sequential(nn.InstanceNorm2d(half1, affine=True),\n                                nn.ReLU(inplace=True))\n        self.BN = ActivatedBatchNorm(half2)\n\n    def forward(self, x):\n        split = torch.split(x, self.half, 1)\n        out1 = self.IN(split[0].contiguous())\n        out2 = self.BN(split[1].contiguous())\n        out = torch.cat((out1, out2), 1)\n        return out\n\n\nclass ImprovedIBNaDecoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n\n        self.block = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels // 4, 1),\n            IBN(in_channels // 4),\n            nn.ConvTranspose2d(in_channels // 4, in_channels // 4, 4, stride=2, padding=1),\n            ActivatedBatchNorm(in_channels // 4),\n            nn.Conv2d(in_channels // 4, out_channels, 1),\n            ActivatedBatchNorm(out_channels)\n        )\n\n    def forward(self, x):\n        return self.block(x)\n'"
src/models/mobilenet.py,3,"b""import torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom collections import OrderedDict\n\n\nclass ExpandedConv(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, dilation=1,\n                 expand_ratio=6, skip_connection=False):\n        super().__init__()\n\n        self.stride = stride\n        self.kernel_size = 3\n        self.dilation = dilation\n        self.expand_ratio = expand_ratio\n        self.skip_connection = skip_connection\n        middle_channels = in_channels * expand_ratio\n\n        if self.expand_ratio != 1:\n            # pointwise\n            self.expand = nn.Sequential(OrderedDict(\n                [('conv', nn.Conv2d(in_channels, middle_channels, 1, bias=False)),\n                 ('bn', nn.BatchNorm2d(middle_channels)),\n                 ('relu', nn.ReLU6(inplace=True))\n                 ]))\n\n        # depthwise\n        self.depthwise = nn.Sequential(OrderedDict(\n            [('conv', nn.Conv2d(middle_channels, middle_channels, 3, stride, dilation, dilation, groups=middle_channels, bias=False)),\n             ('bn', nn.BatchNorm2d(middle_channels)),\n             ('relu', nn.ReLU6(inplace=True))\n             ]))\n\n        # project\n        self.project = nn.Sequential(OrderedDict(\n            [('conv', nn.Conv2d(middle_channels, out_channels, 1, bias=False)),\n             ('bn', nn.BatchNorm2d(out_channels))\n             ]))\n\n    def forward(self, x):\n        if self.expand_ratio != 1:\n            residual = self.project(self.depthwise(self.expand(x)))\n        else:\n            residual = self.project(self.depthwise(x))\n\n        if self.skip_connection:\n            outputs = x + residual\n        else:\n            outputs = residual\n        return outputs\n\n\nclass MobileNetV2(nn.Module):\n    def __init__(self, pretrained=False, model_path='../model/mobilenetv2_encoder/model.pth'):\n        super().__init__()\n\n        self.conv = nn.Conv2d(3, 32, 3, stride=2, padding=1, bias=False)\n        self.bn = nn.BatchNorm2d(32)\n        self.relu = nn.ReLU6()\n\n        self.block0 = ExpandedConv(32, 16, expand_ratio=1)\n        self.block1 = ExpandedConv(16, 24, stride=2)\n        self.block2 = ExpandedConv(24, 24, skip_connection=True)\n        self.block3 = ExpandedConv(24, 32, stride=2)\n        self.block4 = ExpandedConv(32, 32, skip_connection=True)\n        self.block5 = ExpandedConv(32, 32, skip_connection=True)\n        self.block6 = ExpandedConv(32, 64)\n        self.block7 = ExpandedConv(64, 64, dilation=2, skip_connection=True)\n        self.block8 = ExpandedConv(64, 64, dilation=2, skip_connection=True)\n        self.block9 = ExpandedConv(64, 64, dilation=2, skip_connection=True)\n        self.block10 = ExpandedConv(64, 96, dilation=2)\n        self.block11 = ExpandedConv(96, 96, dilation=2, skip_connection=True)\n        self.block12 = ExpandedConv(96, 96, dilation=2, skip_connection=True)\n        self.block13 = ExpandedConv(96, 160, dilation=2)\n        self.block14 = ExpandedConv(160, 160, dilation=4, skip_connection=True)\n        self.block15 = ExpandedConv(160, 160, dilation=4, skip_connection=True)\n        self.block16 = ExpandedConv(160, 320, dilation=4)\n\n        if pretrained:\n            self.load_pretrained_model(model_path)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.block0(x)\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.block4(x)\n        x = self.block5(x)\n        x = self.block6(x)\n        x = self.block7(x)\n        x = self.block8(x)\n        x = self.block9(x)\n        x = self.block10(x)\n        x = self.block11(x)\n        x = self.block12(x)\n        x = self.block13(x)\n        x = self.block14(x)\n        x = self.block15(x)\n        x = self.block16(x)\n        return x\n\n    def load_pretrained_model(self, model_path):\n        self.load_state_dict(torch.load(model_path))\n        print(f'Load from {model_path}!')\n"""
src/models/net.py,3,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .common import ActivatedBatchNorm\nfrom .encoder import create_encoder\nfrom .decoder import create_decoder\nfrom .spp import create_spp, create_mspp\nfrom .tta import SegmentatorTTA\n\n\nclass EncoderDecoderNet(nn.Module, SegmentatorTTA):\n    def __init__(self, output_channels=19, enc_type='resnet50', dec_type='unet_scse',\n                 num_filters=16, pretrained=False):\n        super().__init__()\n        self.output_channels = output_channels\n        self.enc_type = enc_type\n        self.dec_type = dec_type\n\n        assert enc_type in ['resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152',\n                            'resnext101_32x4d', 'resnext101_64x4d',\n                            'se_resnet50', 'se_resnet101', 'se_resnet152',\n                            'se_resnext50_32x4d', 'se_resnext101_32x4d', 'senet154']\n        assert dec_type in ['unet_scse', 'unet_seibn', 'unet_oc']\n\n        encoder = create_encoder(enc_type, pretrained)\n        Decoder = create_decoder(dec_type)\n\n        self.encoder1 = encoder[0]\n        self.encoder2 = encoder[1]\n        self.encoder3 = encoder[2]\n        self.encoder4 = encoder[3]\n        self.encoder5 = encoder[4]\n\n        self.pool = nn.MaxPool2d(2, 2)\n        self.center = Decoder(self.encoder5.out_channels, num_filters * 32 * 2, num_filters * 32)\n\n        self.decoder5 = Decoder(self.encoder5.out_channels + num_filters * 32, num_filters * 32 * 2,\n                                num_filters * 16)\n        self.decoder4 = Decoder(self.encoder4.out_channels + num_filters * 16, num_filters * 16 * 2,\n                                num_filters * 8)\n        self.decoder3 = Decoder(self.encoder3.out_channels + num_filters * 8, num_filters * 8 * 2, num_filters * 4)\n        self.decoder2 = Decoder(self.encoder2.out_channels + num_filters * 4, num_filters * 4 * 2, num_filters * 2)\n        self.decoder1 = Decoder(self.encoder1.out_channels + num_filters * 2, num_filters * 2 * 2, num_filters)\n\n        self.logits = nn.Sequential(\n            nn.Conv2d(num_filters * (16 + 8 + 4 + 2 + 1), 64, kernel_size=1, padding=0),\n            ActivatedBatchNorm(64),\n            nn.Conv2d(64, self.output_channels, kernel_size=1)\n        )\n\n    def forward(self, x):\n        img_size = x.shape[2:]\n\n        e1 = self.encoder1(x)\n        e2 = self.encoder2(e1)\n        e3 = self.encoder3(e2)\n        e4 = self.encoder4(e3)\n        e5 = self.encoder5(e4)\n\n        c = self.center(self.pool(e5))\n        e1_up = F.interpolate(e1, scale_factor=2, mode='bilinear', align_corners=False)\n\n        d5 = self.decoder5(c, e5)\n        d4 = self.decoder4(d5, e4)\n        d3 = self.decoder3(d4, e3)\n        d2 = self.decoder2(d3, e2)\n        d1 = self.decoder1(d2, e1_up)\n\n        u5 = F.interpolate(d5, img_size, mode='bilinear', align_corners=False)\n        u4 = F.interpolate(d4, img_size, mode='bilinear', align_corners=False)\n        u3 = F.interpolate(d3, img_size, mode='bilinear', align_corners=False)\n        u2 = F.interpolate(d2, img_size, mode='bilinear', align_corners=False)\n\n        # Hyper column\n        d = torch.cat((d1, u2, u3, u4, u5), 1)\n        logits = self.logits(d)\n\n        return logits\n\n\nclass SPPNet(nn.Module, SegmentatorTTA):\n    def __init__(self, output_channels=19, enc_type='xception65', dec_type='aspp', output_stride=8):\n        super().__init__()\n        self.output_channels = output_channels\n        self.enc_type = enc_type\n        self.dec_type = dec_type\n\n        assert enc_type in ['xception65', 'mobilenetv2']\n        assert dec_type in ['oc_base', 'oc_asp', 'spp', 'aspp', 'maspp']\n\n        self.encoder = create_encoder(enc_type, output_stride=output_stride, pretrained=False)\n        if enc_type == 'mobilenetv2':\n            self.spp = create_mspp(dec_type)\n        else:\n            self.spp, self.decoder = create_spp(dec_type, output_stride=output_stride)\n        self.logits = nn.Conv2d(256, output_channels, 1)\n\n    def forward(self, inputs):\n        if self.enc_type == 'mobilenetv2':\n            x = self.encoder(inputs)\n            x = self.spp(x)\n            x = self.logits(x)\n            return x\n        else:\n            x, low_level_feat = self.encoder(inputs)\n            x = self.spp(x)\n            x = self.decoder(x, low_level_feat)\n            x = self.logits(x)\n            return x\n\n    def update_bn_eps(self):\n        for m in self.encoder.named_modules():\n            if isinstance(m[1], nn.BatchNorm2d):\n                m[1].eps = 1e-3\n\n    def freeze_bn(self):\n        for m in self.modules():\n            if isinstance(m, nn.modules.batchnorm._BatchNorm):\n                m.eval()\n                # for p in m.parameters():\n                #     p.requires_grad = False\n\n    def get_1x_lr_params(self):\n        for p in self.encoder.parameters():\n            yield p\n\n    def get_10x_lr_params(self):\n        modules = [self.spp, self.logits]\n        if hasattr(self, 'decoder'):\n            modules.append(self.decoder)\n\n        for module in modules:\n            for p in module.parameters():\n                yield p\n"""
src/models/oc.py,4,"b'""""""\nOCNet: Object Context Network for Scene Parsing\nhttps://github.com/PkuRainBow/OCNet\n""""""\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom .common import ActivatedBatchNorm\n\n\nclass SelfAttentionBlock2D(nn.Module):\n    """"""\n    The basic implementation for self-attention block/non-local block\n    Input:\n        N X C X H X W\n    Parameters:\n        in_channels       : the dimension of the input feature map\n        key_channels      : the dimension after the key/query transform\n        value_channels    : the dimension after the value transform\n        scale             : choose the scale to downsample the input feature maps (save memory cost)\n    Return:\n        N X C X H X W\n        position-aware context features.(w/o concate or add with the input)\n    """"""\n\n    def __init__(self, in_channels, key_channels, value_channels, out_channels=None, scale=1):\n        super().__init__()\n        self.scale = scale\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.key_channels = key_channels\n        self.value_channels = value_channels\n        if out_channels is None:\n            self.out_channels = in_channels\n        self.pool = nn.MaxPool2d(kernel_size=(scale, scale))\n        self.f_key = nn.Sequential(\n            nn.Conv2d(in_channels=self.in_channels, out_channels=self.key_channels, kernel_size=1),\n            ActivatedBatchNorm(self.key_channels)\n        )\n        self.f_query = self.f_key\n        self.f_value = nn.Conv2d(in_channels=self.in_channels, out_channels=self.value_channels, kernel_size=1)\n        self.W = nn.Conv2d(in_channels=self.value_channels, out_channels=self.out_channels, kernel_size=1)\n        nn.init.constant_(self.W.weight, 0)\n        nn.init.constant_(self.W.bias, 0)\n\n    def forward(self, x):\n        batch_size, h, w = x.size(0), x.size(2), x.size(3)\n        if self.scale > 1:\n            x = self.pool(x)\n\n        value = self.f_value(x).view(batch_size, self.value_channels, -1)\n        value = value.permute(0, 2, 1)  # b, h*w, v\n        query = self.f_query(x).view(batch_size, self.key_channels, -1)\n        query = query.permute(0, 2, 1)  # b, h*w, k\n        key = self.f_key(x).view(batch_size, self.key_channels, -1)  # b, k, h*w\n\n        sim_map = torch.matmul(query, key)\n        sim_map = (self.key_channels ** -.5) * sim_map\n        sim_map = F.softmax(sim_map, dim=-1)  # b * h*w * h*w\n\n        context = torch.matmul(sim_map, value)  # b * h*w * v\n        context = context.permute(0, 2, 1).contiguous()\n        context = context.view(batch_size, self.value_channels, *x.size()[2:])\n        context = self.W(context)\n        if self.scale > 1:\n            context = F.interpolate(context, size=(h, w), mode=\'bilinear\', align_corners=True)\n        return context\n\n\nclass BaseOC_Context(nn.Module):\n    """"""\n    Output only the context features.\n    Parameters:\n        in_features / out_features: the channels of the input / output feature maps.\n        dropout: specify the dropout ratio\n        fusion: We provide two different fusion method, ""concat"" or ""add""\n        size: we find that directly learn the attention weights on even 1/8 feature maps is hard.\n    Return:\n        features after ""concat"" or ""add""\n    """"""\n\n    def __init__(self, in_channels, out_channels, key_channels, value_channels, dropout=0.05, sizes=(1,)):\n        super().__init__()\n        self.stages = nn.ModuleList(\n            [SelfAttentionBlock2D(in_channels, key_channels, value_channels, out_channels, size) for size in sizes])\n        self.conv_bn_dropout = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0),\n            ActivatedBatchNorm(out_channels),\n            nn.Dropout2d(dropout)\n        )\n\n    def forward(self, feats):\n        priors = [stage(feats) for stage in self.stages]\n        context = priors[0]\n        for i in range(1, len(priors)):\n            context += priors[i]\n        output = self.conv_bn_dropout(context)\n        return output\n\n\nclass BaseOC(nn.Module):\n    def __init__(self, in_channels=2048, out_channels=256, dropout=0.05):\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            ActivatedBatchNorm(out_channels),\n            BaseOC_Context(in_channels=out_channels, out_channels=out_channels,\n                           key_channels=out_channels // 2, value_channels=out_channels // 2, dropout=dropout))\n\n    def forward(self, x):\n        return self.block(x)\n\n\nclass ASPOC(nn.Module):\n    def __init__(self, in_channels=2048, out_channels=256, output_stride=8):\n        super().__init__()\n        if output_stride == 16:\n            dilations = [6, 12, 18]\n        elif output_stride == 8:\n            dilations = [12, 24, 36]\n        else:\n            raise NotImplementedError\n\n        self.context = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, dilation=1, bias=True),\n            ActivatedBatchNorm(out_channels),\n            BaseOC_Context(in_channels=out_channels, out_channels=out_channels,\n                           key_channels=out_channels // 2, value_channels=out_channels,\n                           dropout=0, sizes=([2])))\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0, dilation=1, bias=False),\n            ActivatedBatchNorm(out_channels))\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=dilations[0], dilation=dilations[0],\n                      bias=False),\n            ActivatedBatchNorm(out_channels))\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=dilations[1], dilation=dilations[1],\n                      bias=False),\n            ActivatedBatchNorm(out_channels))\n        self.conv5 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=dilations[2], dilation=dilations[2],\n                      bias=False),\n            ActivatedBatchNorm(out_channels))\n        self.conv_bn_dropout = nn.Sequential(\n            nn.Conv2d(out_channels * 5, out_channels, kernel_size=1, padding=0, dilation=1, bias=False),\n            ActivatedBatchNorm(out_channels),\n            nn.Dropout2d(0.1)\n        )\n\n    def forward(self, x):\n        _, _, h, w = x.size()\n        feat1 = self.context(x)\n        feat2 = self.conv2(x)\n        feat3 = self.conv3(x)\n        feat4 = self.conv4(x)\n        feat5 = self.conv5(x)\n\n        out = torch.cat((feat1, feat2, feat3, feat4, feat5), 1)\n        output = self.conv_bn_dropout(out)\n        return output\n'"
src/models/scse.py,6,"b'import torch\nimport torch.nn as nn\n\n\nclass SELayer(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super().__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, int(channel / reduction), bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(int(channel / reduction), channel, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y.expand_as(x)\n\n\nclass SCSEBlock(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super().__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.channel_excitation = nn.Sequential(nn.Linear(channel, int(channel // reduction)),\n                                                nn.ReLU(inplace=True),\n                                                nn.Linear(int(channel // reduction), channel))\n        self.spatial_se = nn.Conv2d(channel, 1, kernel_size=1,\n                                    stride=1, padding=0, bias=False)\n\n    def forward(self, x):\n        bahs, chs, _, _ = x.size()\n\n        # Returns a new tensor with the same data as the self tensor but of a different size.\n        chn_se = self.avg_pool(x).view(bahs, chs)\n        chn_se = torch.sigmoid(self.channel_excitation(chn_se).view(bahs, chs, 1, 1))\n        chn_se = torch.mul(x, chn_se)\n\n        spa_se = torch.sigmoid(self.spatial_se(x))\n        spa_se = torch.mul(x, spa_se)\n        return torch.add(chn_se, 1, spa_se)\n'"
src/models/spp.py,5,"b""from collections import OrderedDict\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom .common import ActivatedBatchNorm, SeparableConv2d\nfrom .oc import BaseOC, ASPOC\n\n\nclass SPP(nn.Module):\n    def __init__(self, in_channels=2048, out_channels=256, pyramids=(1, 2, 3, 6)):\n        super().__init__()\n        stages = []\n        for p in pyramids:\n            stages.append(nn.Sequential(\n                nn.AdaptiveAvgPool2d(p),\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n                ActivatedBatchNorm(out_channels)\n            ))\n        self.stages = nn.ModuleList(stages)\n        self.bottleneck = nn.Sequential(\n            nn.Conv2d(in_channels + out_channels * len(pyramids), out_channels, kernel_size=1),\n            ActivatedBatchNorm(out_channels)\n        )\n\n    def forward(self, x):\n        x_size = x.size()\n        out = [x]\n        for stage in self.stages:\n            out.append(F.interpolate(stage(x), size=x_size[2:], mode='bilinear', align_corners=False))\n        out = self.bottleneck(torch.cat(out, 1))\n        return out\n\n\nclass ASPP(nn.Module):\n    def __init__(self, in_channels=2048, out_channels=256, output_stride=8):\n        super().__init__()\n        if output_stride == 16:\n            dilations = [6, 12, 18]\n        elif output_stride == 8:\n            dilations = [12, 24, 36]\n        else:\n            raise NotImplementedError\n        # dilations = [6, 12, 18]\n\n        self.aspp0 = nn.Sequential(OrderedDict([('conv', nn.Conv2d(in_channels, out_channels, 1, bias=False)),\n                                                ('bn', nn.BatchNorm2d(out_channels)),\n                                                ('relu', nn.ReLU(inplace=True))]))\n        self.aspp1 = SeparableConv2d(in_channels, out_channels, dilation=dilations[0], relu_first=False)\n        self.aspp2 = SeparableConv2d(in_channels, out_channels, dilation=dilations[1], relu_first=False)\n        self.aspp3 = SeparableConv2d(in_channels, out_channels, dilation=dilations[2], relu_first=False)\n\n        self.image_pooling = nn.Sequential(OrderedDict([('gap', nn.AdaptiveAvgPool2d((1, 1))),\n                                                        ('conv', nn.Conv2d(in_channels, out_channels, 1, bias=False)),\n                                                        ('bn', nn.BatchNorm2d(out_channels)),\n                                                        ('relu', nn.ReLU(inplace=True))]))\n\n        self.conv = nn.Conv2d(out_channels*5, out_channels, 1, bias=False)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.dropout = nn.Dropout2d(p=0.1)\n\n    def forward(self, x):\n        pool = self.image_pooling(x)\n        pool = F.interpolate(pool, size=x.shape[2:], mode='bilinear', align_corners=True)\n\n        x0 = self.aspp0(x)\n        x1 = self.aspp1(x)\n        x2 = self.aspp2(x)\n        x3 = self.aspp3(x)\n        x = torch.cat((pool, x0, x1, x2, x3), dim=1)\n\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n\n        return x\n\n\nclass MobileASPP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.aspp0 = nn.Sequential(OrderedDict([('conv', nn.Conv2d(320, 256, 1, bias=False)),\n                                                ('bn', nn.BatchNorm2d(256)),\n                                                ('relu', nn.ReLU(inplace=True))]))\n        self.image_pooling = nn.Sequential(OrderedDict([('gap', nn.AdaptiveAvgPool2d((1, 1))),\n                                                        ('conv', nn.Conv2d(320, 256, 1, bias=False)),\n                                                        ('bn', nn.BatchNorm2d(256)),\n                                                        ('relu', nn.ReLU(inplace=True))]))\n\n        self.conv = nn.Conv2d(512, 256, 1, bias=False)\n        self.bn = nn.BatchNorm2d(256)\n        self.relu = nn.ReLU(inplace=True)\n        self.dropout = nn.Dropout2d(p=0.1)\n\n    def forward(self, x):\n        pool = self.image_pooling(x)\n        pool = F.interpolate(pool, size=x.shape[2:], mode='bilinear', align_corners=True)\n\n        x = self.aspp0(x)\n        x = torch.cat((pool, x), dim=1)\n\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n\n        return x\n\n\nclass SPPDecoder(nn.Module):\n    def __init__(self, in_channels, reduced_layer_num=48):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, reduced_layer_num, 1, bias=False)\n        self.bn = nn.BatchNorm2d(reduced_layer_num)\n        self.relu = nn.ReLU(inplace=True)\n        self.sep1 = SeparableConv2d(256+reduced_layer_num, 256, relu_first=False)\n        self.sep2 = SeparableConv2d(256, 256, relu_first=False)\n\n    def forward(self, x, low_level_feat):\n        x = F.interpolate(x, size=low_level_feat.shape[2:], mode='bilinear', align_corners=True)\n        low_level_feat = self.conv(low_level_feat)\n        low_level_feat = self.bn(low_level_feat)\n        low_level_feat = self.relu(low_level_feat)\n        x = torch.cat((x, low_level_feat), dim=1)\n        x = self.sep1(x)\n        x = self.sep2(x)\n        return x\n\n\ndef create_spp(dec_type, in_channels=2048, middle_channels=256, output_stride=8):\n    if dec_type == 'spp':\n        return SPP(in_channels, middle_channels), SPPDecoder(middle_channels)\n    elif dec_type == 'aspp':\n        return ASPP(in_channels, middle_channels, output_stride), SPPDecoder(middle_channels)\n    elif dec_type == 'oc_base':\n        return BaseOC(in_channels, middle_channels), SPPDecoder(middle_channels)\n    elif dec_type in 'oc_asp':\n        return ASPOC(in_channels, middle_channels, output_stride), SPPDecoder(middle_channels)\n    else:\n        raise NotImplementedError\n\n\ndef create_mspp(dec_type):\n    if dec_type == 'spp':\n        return SPP(320, 256)\n    elif dec_type == 'aspp':\n        return ASPP(320, 256, 8)\n    elif dec_type == 'oc_base':\n        return BaseOC(320, 256)\n    elif dec_type == 'oc_asp':\n        return ASPOC(320, 256, 8)\n    elif dec_type == 'maspp':\n        return MobileASPP()\n    elif dec_type == 'maspp_dec':\n        return MobileASPP(), SPPDecoder(24, reduced_layer_num=12)\n    else:\n        raise NotImplementedError\n"""
src/models/tta.py,1,"b""import torch.nn.functional as F\r\n\r\nclass SegmentatorTTA(object):\r\n    @staticmethod\r\n    def hflip(x):\r\n        return x.flip(3)\r\n\r\n    @staticmethod\r\n    def vflip(x):\r\n        return x.flip(2)\r\n\r\n    @staticmethod\r\n    def trans(x):\r\n        return x.transpose(2, 3)\r\n\r\n    def pred_resize(self, x, size, net_type='unet'):\r\n        h, w = size\r\n        if net_type == 'unet':\r\n            pred = self.forward(x)\r\n            if x.shape[2:] == size:\r\n                return pred\r\n            else:\r\n                return F.interpolate(pred, size=(h, w), mode='bilinear', align_corners=True)\r\n        else:\r\n            pred = self.forward(F.pad(x, (0, 1, 0, 1)))\r\n            return F.interpolate(pred, size=(h+1, w+1), mode='bilinear', align_corners=True)[..., :h, :w]\r\n\r\n    def tta(self, x, scales=None, net_type='unet'):\r\n        size = x.shape[2:]\r\n        if scales is None:\r\n            seg_sum = self.pred_resize(x, size, net_type)\r\n            seg_sum += self.hflip(self.pred_resize(self.hflip(x), size, net_type))\r\n            return seg_sum / 2\r\n        else:\r\n            # scale = 1\r\n            seg_sum = self.pred_resize(x, size, net_type)\r\n            seg_sum += self.hflip(self.pred_resize(self.hflip(x), size, net_type))\r\n            for scale in scales:\r\n                scaled = F.interpolate(x, scale_factor=scale, mode='bilinear', align_corners=True)\r\n                seg_sum += self.pred_resize(scaled, size, net_type)\r\n                seg_sum += self.hflip(self.pred_resize(self.hflip(scaled), size, net_type))\r\n            return seg_sum / ((len(scales) + 1) * 2)\r\n\r\n"""
src/models/xception.py,2,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .common import SeparableConv2d\n\n\nclass XceptionBlock(nn.Module):\n    def __init__(self, channel_list, stride=1, dilation=1, skip_connection_type='conv', relu_first=True, low_feat=False):\n        super().__init__()\n\n        assert len(channel_list) == 4\n        self.skip_connection_type = skip_connection_type\n        self.relu_first = relu_first\n        self.low_feat = low_feat\n\n        if self.skip_connection_type == 'conv':\n            self.conv = nn.Conv2d(channel_list[0], channel_list[-1], 1, stride=stride, bias=False)\n            self.bn = nn.BatchNorm2d(channel_list[-1])\n\n        self.sep_conv1 = SeparableConv2d(channel_list[0], channel_list[1],\n                                         dilation=dilation, relu_first=relu_first)\n        self.sep_conv2 = SeparableConv2d(channel_list[1], channel_list[2],\n                                         dilation=dilation, relu_first=relu_first)\n        self.sep_conv3 = SeparableConv2d(channel_list[2], channel_list[3],\n                                         dilation=dilation, relu_first=relu_first, stride=stride)\n\n    def forward(self, inputs):\n        sc1 = self.sep_conv1(inputs)\n        sc2 = self.sep_conv2(sc1)\n        residual = self.sep_conv3(sc2)\n\n        if self.skip_connection_type == 'conv':\n            shortcut = self.conv(inputs)\n            shortcut = self.bn(shortcut)\n            outputs = residual + shortcut\n        elif self.skip_connection_type == 'sum':\n            outputs = residual + inputs\n        elif self.skip_connection_type == 'none':\n            outputs = residual\n        else:\n            raise ValueError('Unsupported skip connection type.')\n\n        if self.low_feat:\n            return outputs, sc2\n        else:\n            return outputs\n\n\nclass Xception65(nn.Module):\n    def __init__(self, output_stride=8):\n        super().__init__()\n\n        if output_stride == 16:\n            entry_block3_stride = 2\n            middle_block_dilation = 1\n            exit_block_dilations = (1, 2)\n        elif output_stride == 8:\n            entry_block3_stride = 1\n            middle_block_dilation = 2\n            exit_block_dilations = (2, 4)\n        else:\n            raise NotImplementedError\n\n        # Entry flow\n        self.conv1 = nn.Conv2d(3, 32, 3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.relu = nn.ReLU()\n\n        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(64)\n\n        self.block1 = XceptionBlock([64, 128, 128, 128], stride=2)\n        self.block2 = XceptionBlock([128, 256, 256, 256], stride=2, low_feat=True)\n        self.block3 = XceptionBlock([256, 728, 728, 728], stride=entry_block3_stride)\n\n        # Middle flow (16 units)\n        self.block4 = XceptionBlock([728, 728, 728, 728], dilation=middle_block_dilation, skip_connection_type='sum')\n        self.block5 = XceptionBlock([728, 728, 728, 728], dilation=middle_block_dilation, skip_connection_type='sum')\n        self.block6 = XceptionBlock([728, 728, 728, 728], dilation=middle_block_dilation, skip_connection_type='sum')\n        self.block7 = XceptionBlock([728, 728, 728, 728], dilation=middle_block_dilation, skip_connection_type='sum')\n        self.block8 = XceptionBlock([728, 728, 728, 728], dilation=middle_block_dilation, skip_connection_type='sum')\n        self.block9 = XceptionBlock([728, 728, 728, 728], dilation=middle_block_dilation, skip_connection_type='sum')\n        self.block10 = XceptionBlock([728, 728, 728, 728], dilation=middle_block_dilation, skip_connection_type='sum')\n        self.block11 = XceptionBlock([728, 728, 728, 728], dilation=middle_block_dilation, skip_connection_type='sum')\n        self.block12 = XceptionBlock([728, 728, 728, 728], dilation=middle_block_dilation, skip_connection_type='sum')\n        self.block13 = XceptionBlock([728, 728, 728, 728], dilation=middle_block_dilation, skip_connection_type='sum')\n        self.block14 = XceptionBlock([728, 728, 728, 728], dilation=middle_block_dilation, skip_connection_type='sum')\n        self.block15 = XceptionBlock([728, 728, 728, 728], dilation=middle_block_dilation, skip_connection_type='sum')\n        self.block16 = XceptionBlock([728, 728, 728, 728], dilation=middle_block_dilation, skip_connection_type='sum')\n        self.block17 = XceptionBlock([728, 728, 728, 728], dilation=middle_block_dilation, skip_connection_type='sum')\n        self.block18 = XceptionBlock([728, 728, 728, 728], dilation=middle_block_dilation, skip_connection_type='sum')\n        self.block19 = XceptionBlock([728, 728, 728, 728], dilation=middle_block_dilation, skip_connection_type='sum')\n\n        # Exit flow\n        self.block20 = XceptionBlock([728, 728, 1024, 1024], dilation=exit_block_dilations[0])\n        self.block21 = XceptionBlock([1024, 1536, 1536, 2048], dilation=exit_block_dilations[1],\n                                     skip_connection_type='none', relu_first=False)\n\n    def forward(self, x):\n        # Entry flow\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n\n        x = self.block1(x)\n        x, low_level_feat = self.block2(x)  # b, h//4, w//4, 256\n        x = self.block3(x)  # b, h//8, w//8, 728\n\n        # Middle flow\n        x = self.block4(x)\n        x = self.block5(x)\n        x = self.block6(x)\n        x = self.block7(x)\n        x = self.block8(x)\n        x = self.block9(x)\n        x = self.block10(x)\n        x = self.block11(x)\n        x = self.block12(x)\n        x = self.block13(x)\n        x = self.block14(x)\n        x = self.block15(x)\n        x = self.block16(x)\n        x = self.block17(x)\n        x = self.block18(x)\n        x = self.block19(x)\n\n        # Exit flow\n        x = self.block20(x)\n        x = self.block21(x)\n\n        return x, low_level_feat\n"""
src/utils/__init__.py,0,b''
src/utils/custum_aug.py,0,"b'import random\nimport cv2\nimport numpy as np\nfrom albumentations.core.transforms_interface import to_tuple, ImageOnlyTransform, DualTransform\n\n\ndef apply_motion_blur(image, count):\n    """"""\n    https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n    """"""\n    image_t = image.copy()\n    imshape = image_t.shape\n    size = 15\n    kernel_motion_blur = np.zeros((size, size))\n    kernel_motion_blur[int((size - 1) / 2), :] = np.ones(size)\n    kernel_motion_blur = kernel_motion_blur / size\n    i = imshape[1] * 3 // 4 - 10 * count\n    while i <= imshape[1]:\n        image_t[:, i:, :] = cv2.filter2D(image_t[:, i:, :], -1, kernel_motion_blur)\n        image_t[:, :imshape[1] - i, :] = cv2.filter2D(image_t[:, :imshape[1] - i, :], -1, kernel_motion_blur)\n        i += imshape[1] // 25 - count\n        count += 1\n    color_image = image_t\n    return color_image\n\n\ndef rotate(img, angle, interpolation, border_mode, border_value=None):\n    height, width = img.shape[:2]\n    matrix = cv2.getRotationMatrix2D((width / 2, height / 2), angle, 1.0)\n    img = cv2.warpAffine(img, matrix, (width, height),\n                         flags=interpolation, borderMode=border_mode, borderValue=border_value)\n    return img\n\n\nclass AddSpeed(ImageOnlyTransform):\n    def __init__(self, speed_coef=-1, p=.5):\n        super().__init__(p)\n        assert speed_coef == -1 or 0 <= speed_coef <= 1\n        self.speed_coef = speed_coef\n\n    def apply(self, img, count=7, **params):\n        return apply_motion_blur(img, count)\n\n    def get_params(self):\n        if self.speed_coef == -1:\n            return {\'count\': int(15 * random.uniform(0, 1))}\n        else:\n            return {\'count\': int(15 * self.speed_coef)}\n\n\nclass Rotate(DualTransform):\n    def __init__(self, limit=90, interpolation=cv2.INTER_LINEAR,\n                 border_mode=cv2.BORDER_REFLECT_101, border_value=255, always_apply=False, p=.5):\n        super().__init__(always_apply, p)\n        self.limit = to_tuple(limit)\n        self.interpolation = interpolation\n        self.border_mode = border_mode\n        self.border_value = border_value\n\n    def apply(self, img, angle=0, **params):\n        return rotate(img, angle, interpolation=self.interpolation, border_mode=self.border_mode)\n\n    def apply_to_mask(self, img, angle=0, **params):\n        return rotate(img, angle, interpolation=cv2.INTER_NEAREST,\n                      border_mode=cv2.BORDER_CONSTANT, border_value=self.border_value)\n\n    def get_params(self):\n        return {\'angle\': random.uniform(self.limit[0], self.limit[1])}\n\n\nclass PadIfNeededRightBottom(DualTransform):\n    def __init__(self, min_height=769, min_width=769, border_mode=cv2.BORDER_CONSTANT,\n                 value=0, ignore_index=255, always_apply=False, p=1.0):\n        super().__init__(always_apply, p)\n        self.min_height = min_height\n        self.min_width = min_width\n        self.border_mode = border_mode\n        self.value = value\n        self.ignore_index = ignore_index\n\n    def apply(self, img, **params):\n        img_height, img_width = img.shape[:2]\n        pad_height = max(0, self.min_height-img_height)\n        pad_width = max(0, self.min_width-img_width)\n        return np.pad(img, ((0, pad_height), (0, pad_width), (0, 0)), \'constant\', constant_values=self.value)\n\n    def apply_to_mask(self, img, **params):\n        img_height, img_width = img.shape[:2]\n        pad_height = max(0, self.min_height-img_height)\n        pad_width = max(0, self.min_width-img_width)\n        return np.pad(img, ((0, pad_height), (0, pad_width)), \'constant\', constant_values=self.ignore_index)\n'"
src/utils/functional.py,0,"b""import numpy as np\n\n\ndef logsumexp(a, axis=None, b=None, keepdims=False, return_sign=False):\n    if b is not None:\n        a, b = np.broadcast_arrays(a, b)\n        if np.any(b == 0):\n            a = a + 0.  # promote to at least float\n            a[b == 0] = -np.inf\n\n    a_max = np.amax(a, axis=axis, keepdims=True)\n\n    if a_max.ndim > 0:\n        a_max[~np.isfinite(a_max)] = 0\n    elif not np.isfinite(a_max):\n        a_max = 0\n\n    if b is not None:\n        b = np.asarray(b)\n        tmp = b * np.exp(a - a_max)\n    else:\n        tmp = np.exp(a - a_max)\n\n    # suppress warnings about log of zero\n    with np.errstate(divide='ignore'):\n        s = np.sum(tmp, axis=axis, keepdims=keepdims)\n        if return_sign:\n            sgn = np.sign(s)\n            s *= sgn  # /= makes more sense but we need zero -> zero\n        out = np.log(s)\n\n    if not keepdims:\n        a_max = np.squeeze(a_max, axis=axis)\n    out += a_max\n\n    if return_sign:\n        return out, sgn\n    else:\n        return out\n\n\ndef softmax(x, axis=None):\n    return np.exp(x - logsumexp(x, axis=axis, keepdims=True))"""
src/utils/metrics.py,0,"b""import numpy as np\nimport warnings\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef compute_ious(pred, label, classes, ignore_index=255, only_present=True):\n    pred[label == ignore_index] = 0\n    ious = []\n    for c in classes:\n        label_c = label == c\n        if only_present and np.sum(label_c) == 0:\n            ious.append(np.nan)\n            continue\n        pred_c = pred == c\n        intersection = np.logical_and(pred_c, label_c).sum()\n        union = np.logical_or(pred_c, label_c).sum()\n        if union != 0:\n            ious.append(intersection / union)\n    return ious if ious else [1]\n\n\ndef compute_iou_batch(preds, labels, classes=None):\n    iou = np.nanmean([np.nanmean(compute_ious(pred, label, classes)) for pred, label in zip(preds, labels)])\n    return iou\n\n\ndef iou_analyzer(preds, labels, tods):\n    mIoU = np.nanmean([np.nanmean(compute_ious(pred, label, [1, 2, 3, 4])) for pred, label in zip(preds, labels)])\n    print(f'Valid mIoU: {mIoU:.3f}\\n')\n\n    class_names = ['car', 'person', 'signal', 'road']\n    tod_names = ['morning', 'day', 'night']\n    iou_dict = {tod_name: dict(zip(class_names, [[] for _ in range(len(class_names))])) for tod_name in tod_names}\n    for pred, label, tod in zip(preds, labels, tods):\n        iou_per_class = compute_ious(pred, label, [1, 2, 3, 4])\n        for iou, class_name in zip(iou_per_class, class_names):\n            iou_dict[tod][class_name].append(iou)\n\n    for tod_name in tod_names:\n        print(f'\\n---{tod_name}---')\n        for k, v in iou_dict[tod_name].items():\n            print(f'{k}: {np.nanmean(v):.3f}')\n\n    print('\\n---ALL---')\n    for class_name in class_names:\n        ious = []\n        for tod_name in tod_names:\n            ious += iou_dict[tod_name][class_name]\n        print(f'{class_name}: {np.nanmean(ious):.3f}')\n"""
src/utils/optimizer.py,1,"b""import torch.optim as optim\nfrom .scheduler import CosineWithRestarts\n\n\ndef create_optimizer(params, mode='adam', base_lr=1e-3, t_max=10):\n    if mode == 'adam':\n        optimizer = optim.Adam(params, base_lr)\n    elif mode == 'sgd':\n        optimizer = optim.SGD(params, base_lr, momentum=0.9, weight_decay=4e-5)\n    else:\n        raise NotImplementedError(mode)\n\n    scheduler = CosineWithRestarts(optimizer, t_max)\n\n    return optimizer, scheduler\n"""
src/utils/preprocess.py,0,"b""import numpy as np\nimport cv2\n\n\ndef minmax_normalize(img, norm_range=(0, 1), orig_range=(0, 255)):\n    # range(0, 1)\n    norm_img = (img - orig_range[0]) / (orig_range[1] - orig_range[0])\n    # range(min_value, max_value)\n    norm_img = norm_img * (norm_range[1] - norm_range[0]) + norm_range[0]\n    return norm_img\n\n\ndef meanstd_normalize(img, mean, std):\n    mean = np.asarray(mean)\n    std = np.asarray(std)\n    norm_img = (img - mean) / std\n    return norm_img\n\n\ndef padding(img, pad, constant_values=0):\n    pad_img = np.pad(img, pad, 'constant', constant_values=constant_values)\n    return pad_img\n\n\ndef clahe(img, clip=2, grid=8):\n    img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n    _clahe = cv2.createCLAHE(clipLimit=clip, tileGridSize=(grid, grid))\n    img_yuv[:, :, 0] = _clahe.apply(img_yuv[:, :, 0])\n    img_equ = cv2.cvtColor(img_yuv, cv2.COLOR_LAB2BGR)\n    return img_equ\n"""
src/utils/scheduler.py,4,"b'import numpy as np\r\nimport torch\r\n""""""\r\nhttps://github.com/allenai/allennlp/pull/1647/files\r\n""""""\r\n\r\n\r\nclass CosineWithRestarts(torch.optim.lr_scheduler._LRScheduler):  # pylint: disable=protected-access\r\n    """"""\r\n    Cosine annealing with restarts.\r\n    This is decribed in the paper https://arxiv.org/abs/1608.03983.\r\n    Parameters\r\n    ----------\r\n    optimizer : ``torch.optim.Optimizer``\r\n    t_max : ``int``\r\n        The maximum number of iterations within the first cycle.\r\n    eta_min : ``float``, optional (default=0)\r\n        The minimum learning rate.\r\n    last_epoch : ``int``, optional (default=-1)\r\n        The index of the last epoch. This is used when restarting.\r\n    factor : ``float``, optional (default=1)\r\n        The factor by which the cycle length (``T_max``) increases after each restart.\r\n    """"""\r\n    def __init__(self,\r\n                 optimizer: torch.optim.Optimizer,\r\n                 t_max: int,\r\n                 eta_min: float = 0.,\r\n                 last_epoch: int = -1,\r\n                 factor: float = 1.) -> None:\r\n        assert t_max > 0\r\n        assert eta_min >= 0\r\n        self.t_max = t_max\r\n        self.eta_min = eta_min\r\n        self.factor = factor\r\n        self._last_restart: int = 0\r\n        self._cycle_counter: int = 0\r\n        self._cycle_factor: float = 1.\r\n        self._updated_cycle_len: int = t_max\r\n        self._initialized: bool = False\r\n        super(CosineWithRestarts, self).__init__(optimizer, last_epoch)\r\n    def get_lr(self):\r\n        """"""Get updated learning rate.""""""\r\n        # HACK: We need to check if this is the first time ``self.get_lr()`` was called,\r\n        # since ``torch.optim.lr_scheduler._LRScheduler`` will call ``self.get_lr()``\r\n        # when first initialized, but the learning rate should remain unchanged\r\n        # for the first epoch.\r\n        if not self._initialized:\r\n            self._initialized = True\r\n            return self.base_lrs\r\n        step = self.last_epoch + 1\r\n        self._cycle_counter = step - self._last_restart\r\n        lrs = [\r\n                self.eta_min + ((lr - self.eta_min) / 2) * (\r\n                        np.cos(\r\n                                np.pi *\r\n                                (self._cycle_counter % self._updated_cycle_len) /\r\n                                self._updated_cycle_len\r\n                        ) + 1\r\n                )\r\n                for lr in self.base_lrs\r\n        ]\r\n        if self._cycle_counter % self._updated_cycle_len == 0:\r\n            # Adjust the cycle length.\r\n            self._cycle_factor *= self.factor\r\n            self._cycle_counter = 0\r\n            self._updated_cycle_len = int(self._cycle_factor * self.t_max)\r\n            self._last_restart = step\r\n        return lrs\r\n'"
src/utils/visualize.py,0,"b""import numpy as np\nfrom PIL import Image\n\n\nn_classes = 5\nvalid_colors = [(0, 0, 0),\n                (0, 0, 255),\n                (255, 0, 0),\n                (255, 255, 0),\n                (69, 47, 142),\n                ]\nclass_map = dict(zip(valid_colors, range(n_classes)))\nown_mask = np.array(Image.open('../preprocess/own_mask010.png')).astype(bool)\n\n\ndef encode_mask(color_mask):\n    valid_mask = np.zeros((color_mask.shape[0], color_mask.shape[1]), dtype=np.uint8)\n    colors = valid_colors[1:]\n\n    for c in colors:\n        tmp_index = color_mask == c\n        index = np.einsum('ij,ij,ij->ij', tmp_index[:, :, 0], tmp_index[:, :, 1], tmp_index[:, :, 2])\n        valid_mask[index] = class_map[c]\n    valid_mask[own_mask] = 0\n    return valid_mask\n\n\ndef label_colormap(n=256):\n    def bitget(byteval, idx):\n        return (byteval & (1 << idx)) != 0\n\n    cmap = np.zeros((n, 3))\n    for i in range(0, n):\n        id = i\n        r, g, b = 0, 0, 0\n        for j in range(0, 8):\n            r = np.bitwise_or(r, (bitget(id, 0) << 7 - j))\n            g = np.bitwise_or(g, (bitget(id, 1) << 7 - j))\n            b = np.bitwise_or(b, (bitget(id, 2) << 7 - j))\n            id = (id >> 3)\n        cmap[i, 0] = r\n        cmap[i, 1] = g\n        cmap[i, 2] = b\n    cmap = cmap.astype(np.float32) / 255\n    return cmap\n\n\ndef label2rgb(lbl, img=None, n_labels=n_classes, ignore_index=255, alpha=0.3, to_gray=False):\n    cmap = label_colormap(n_labels)\n    cmap = (cmap * 255).astype(np.uint8)\n\n    lbl_viz = cmap[lbl]\n    lbl_viz[lbl == ignore_index] = (0, 0, 0)  # unlabeled\n\n    if img is not None:\n        if to_gray:\n            img = Image.fromarray(img).convert('LA')\n            img = np.asarray(img.convert('RGB'))\n        lbl_viz = alpha * lbl_viz + (1 - alpha) * img\n        lbl_viz = lbl_viz.astype(np.uint8)\n\n    return lbl_viz\n"""
src/losses/binary/__init__.py,1,"b""import torch.nn as nn\n\nfrom .focal_loss import FocalLoss\nfrom .lovasz_loss import LovaszLoss\nfrom .dice_loss import DiceLoss, MixedDiceBCELoss\n\n\nclass BinaryClassCriterion(nn.Module):\n    def __init__(self, loss_type='BCE', **kwargs):\n        super().__init__()\n        if loss_type == 'BCE':\n            self.criterion = nn.BCEWithLogitsLoss(**kwargs)\n        elif loss_type == 'Focal':\n            self.criterion = FocalLoss(**kwargs)\n        elif loss_type == 'Lovasz':\n            self.criterion = LovaszLoss(**kwargs)\n        elif loss_type == 'Dice':\n            self.criterion = DiceLoss(**kwargs)\n        elif loss_type == 'MixedDiceBCE':\n            self.criterion = MixedDiceBCELoss(**kwargs)\n        else:\n            raise NotImplementedError\n\n    def forward(self, preds, labels):\n        loss = self.criterion(preds, labels)\n        return loss\n"""
src/losses/binary/dice_loss.py,4,"b'import torch\nimport torch.nn as nn\n\n\nclass DiceLoss(nn.Module):\n    def __init__(self, smooth=0, eps=1e-7):\n        super().__init__()\n        self.smooth = smooth\n        self.eps = eps\n\n    def forward(self, preds, labels):\n        return 1 - (2 * torch.sum(preds * labels) + self.smooth) / \\\n                        (torch.sum(preds) + torch.sum(labels) + self.smooth + self.eps)\n\n\nclass MixedDiceBCELoss(nn.Module):\n    def __init__(self, dice_weight=0.2, bce_weight=0.9):\n        super().__init__()\n        self.dice_loss = DiceLoss()\n        self.bce_loss = nn.BCELoss()\n        self.dice_weight = dice_weight\n        self.bce_weight = bce_weight\n\n    def forward(self, preds, labels):\n        preds = torch.sigmoid(preds)\n        loss = self.dice_weight * self.dice_loss(preds, labels) + self.bce_weight * self.bce_loss(preds, labels)\n        return loss\n'"
src/losses/binary/focal_loss.py,2,"b'""""""\nhttps://arxiv.org/abs/1708.02002\n""""""\nimport torch\nimport torch.nn as nn\n\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.25, gamma=2, weight=None, ignore_index=255):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.weight = weight\n        self.ignore_index = ignore_index\n        self.bce_fn = nn.BCEWithLogitsLoss(weight=self.weight)\n\n    def forward(self, preds, labels):\n        if self.ignore_index is not None:\n            mask = labels != self.ignore_index\n            labels = labels[mask]\n            preds = preds[mask]\n\n        logpt = -self.bce_fn(preds, labels)\n        pt = torch.exp(logpt)\n        loss = -((1 - pt) ** self.gamma) * self.alpha * logpt\n        return loss\n'"
src/losses/binary/lovasz_loss.py,4,"b'""""""\nLovasz-Softmax and Jaccard hinge loss in PyTorch\nMaxim Berman 2018 ESAT-PSI KU Leuven (MIT License)\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef lovasz_grad(gt_sorted):\n    """"""\n    Computes gradient of the Lovasz extension w.r.t sorted errors\n    See Alg. 1 in paper\n    """"""\n    p = len(gt_sorted)\n    gts = gt_sorted.sum()\n    intersection = gts - gt_sorted.float().cumsum(0)\n    union = gts + (1 - gt_sorted).float().cumsum(0)\n    jaccard = 1 - intersection / union\n    if p > 1:  # cover 1-pixel case\n        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n    return jaccard\n\n\ndef hinge(pred, label):\n    signs = 2 * label - 1\n    errors = 1 - pred * signs\n    return errors\n\n\ndef lovasz_hinge_flat(logits, labels, ignore_index):\n    """"""\n    Binary Lovasz hinge loss\n      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n      labels: [P] Tensor, binary ground truth labels (0 or 1)\n      ignore_index: label to ignore\n    """"""\n    logits = logits.contiguous().view(-1)\n    labels = labels.contiguous().view(-1)\n    if ignore_index is not None:\n        mask = labels != ignore_index\n        logits = logits[mask]\n        labels = labels[mask]\n    errors = hinge(logits, labels)\n    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n    perm = perm.data\n    gt_sorted = labels[perm]\n    grad = lovasz_grad(gt_sorted)\n    loss = torch.dot(F.elu(errors_sorted) + 1, grad)\n    return loss\n\n\nclass LovaszLoss(nn.Module):\n    """"""\n    Binary Lovasz hinge loss\n      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n      labels: [P] Tensor, binary ground truth labels (0 or 1)\n      ignore_index: label to ignore\n    """"""\n    def __init__(self, ignore_index=None):\n        super().__init__()\n        self.ignore_index = ignore_index\n\n    def forward(self, logits, labels):\n        return lovasz_hinge_flat(logits, labels, self.ignore_index)\n'"
src/losses/multi/__init__.py,1,"b""import torch.nn as nn\r\n\r\nfrom .focal_loss import FocalLoss\r\nfrom .lovasz_loss import LovaszSoftmax\r\nfrom .ohem_loss import OhemCrossEntropy2d\r\nfrom .softiou_loss import SoftIoULoss\r\n\r\n\r\nclass MultiClassCriterion(nn.Module):\r\n    def __init__(self, loss_type='CrossEntropy', **kwargs):\r\n        super().__init__()\r\n        if loss_type == 'CrossEntropy':\r\n            self.criterion = nn.CrossEntropyLoss(**kwargs)\r\n        elif loss_type == 'Focal':\r\n            self.criterion = FocalLoss(**kwargs)\r\n        elif loss_type == 'Lovasz':\r\n            self.criterion = LovaszSoftmax(**kwargs)\r\n        elif loss_type == 'OhemCrossEntropy':\r\n            self.criterion = OhemCrossEntropy2d(**kwargs)\r\n        elif loss_type == 'SoftIOU':\r\n            self.criterion = SoftIoULoss(**kwargs)\r\n        else:\r\n            raise NotImplementedError\r\n\r\n    def forward(self, preds, labels):\r\n        loss = self.criterion(preds, labels)\r\n        return loss\r\n"""
src/losses/multi/focal_loss.py,2,"b'""""""\r\nhttps://arxiv.org/abs/1708.02002\r\n""""""\r\nimport torch\r\nimport torch.nn as nn\r\n\r\n\r\nclass FocalLoss(nn.Module):\r\n    def __init__(self, alpha=0.5, gamma=2, weight=None, ignore_index=255):\r\n        super().__init__()\r\n        self.alpha = alpha\r\n        self.gamma = gamma\r\n        self.weight = weight\r\n        self.ignore_index = ignore_index\r\n        self.ce_fn = nn.CrossEntropyLoss(weight=self.weight, ignore_index=self.ignore_index)\r\n\r\n    def forward(self, preds, labels):\r\n        logpt = -self.ce_fn(preds, labels)\r\n        pt = torch.exp(logpt)\r\n        loss = -((1 - pt) ** self.gamma) * self.alpha * logpt\r\n        return loss\r\n'"
src/losses/multi/lovasz_loss.py,5,"b'""""""\r\nLovasz-Softmax and Jaccard hinge loss in PyTorch\r\nMaxim Berman 2018 ESAT-PSI KU Leuven (MIT License)\r\n""""""\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\n\r\ndef lovasz_grad(gt_sorted):\r\n    """"""\r\n    Computes gradient of the Lovasz extension w.r.t sorted errors\r\n    See Alg. 1 in paper\r\n    """"""\r\n    p = len(gt_sorted)\r\n    gts = gt_sorted.sum()\r\n    intersection = gts - gt_sorted.float().cumsum(0)\r\n    union = gts + (1 - gt_sorted).float().cumsum(0)\r\n    jaccard = 1 - intersection / union\r\n    if p > 1:  # cover 1-pixel case\r\n        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\r\n    return jaccard\r\n\r\n\r\ndef lovasz_softmax_flat(prb, lbl, ignore_index, only_present):\r\n    """"""\r\n    Multi-class Lovasz-Softmax loss\r\n      prb: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\r\n      lbl: [P] Tensor, ground truth labels (between 0 and C - 1)\r\n      ignore_index: void class labels\r\n      only_present: average only on classes present in ground truth\r\n    """"""\r\n    C = prb.shape[0]\r\n    prb = prb.permute(1, 2, 0).contiguous().view(-1, C)  # H * W, C\r\n    lbl = lbl.view(-1)  # H * W\r\n    if ignore_index is not None:\r\n        mask = lbl != ignore_index\r\n        if mask.sum() == 0:\r\n            return torch.mean(prb * 0)\r\n        prb = prb[mask]\r\n        lbl = lbl[mask]\r\n\r\n    total_loss = 0\r\n    cnt = 0\r\n    for c in range(C):\r\n        fg = (lbl == c).float()  # foreground for class c\r\n        if only_present and fg.sum() == 0:\r\n            continue\r\n        errors = (fg - prb[:, c]).abs()\r\n        errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\r\n        perm = perm.data\r\n        fg_sorted = fg[perm]\r\n        total_loss += torch.dot(errors_sorted, lovasz_grad(fg_sorted))\r\n        cnt += 1\r\n    return total_loss / cnt\r\n\r\n\r\nclass LovaszSoftmax(nn.Module):\r\n    """"""\r\n    Multi-class Lovasz-Softmax loss\r\n      logits: [B, C, H, W] class logits at each prediction (between -\\infty and \\infty)\r\n      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\r\n      ignore_index: void class labels\r\n      only_present: average only on classes present in ground truth\r\n    """"""\r\n    def __init__(self, ignore_index=None, only_present=True):\r\n        super().__init__()\r\n        self.ignore_index = ignore_index\r\n        self.only_present = only_present\r\n\r\n    def forward(self, logits, labels):\r\n        probas = F.softmax(logits, dim=1)\r\n        total_loss = 0\r\n        batch_size = logits.shape[0]\r\n        for prb, lbl in zip(probas, labels):\r\n            total_loss += lovasz_softmax_flat(prb, lbl, self.ignore_index, self.only_present)\r\n        return total_loss / batch_size\r\n'"
src/losses/multi/ohem_loss.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\n\n# Adapted from OCNet Repository (https://github.com/PkuRainBow/OCNet)\nclass OhemCrossEntropy2d(nn.Module):\n    def __init__(self, thresh=0.6, min_kept=0, weight=None, ignore_index=255):\n        super().__init__()\n        self.ignore_label = ignore_index\n        self.thresh = float(thresh)\n        self.min_kept = int(min_kept)\n        self.criterion = nn.CrossEntropyLoss(weight=weight, ignore_index=ignore_index)\n\n    def forward(self, predict, target):\n        """"""\n            Args:\n                predict:(n, c, h, w)\n                target:(n, h, w)\n        """"""\n\n        n, c, h, w = predict.size()\n        input_label = target.data.cpu().numpy().ravel().astype(np.int32)\n        x = np.rollaxis(predict.data.cpu().numpy(), 1).reshape((c, -1))\n        input_prob = np.exp(x - x.max(axis=0).reshape((1, -1)))\n        input_prob /= input_prob.sum(axis=0).reshape((1, -1))\n\n        valid_flag = input_label != self.ignore_label\n        valid_inds = np.where(valid_flag)[0]\n        label = input_label[valid_flag]\n        num_valid = valid_flag.sum()\n        if self.min_kept >= num_valid:\n            print(\'Labels: {}\'.format(num_valid))\n        elif num_valid > 0:\n            prob = input_prob[:, valid_flag]\n            pred = prob[label, np.arange(len(label), dtype=np.int32)]\n            threshold = self.thresh\n            if self.min_kept > 0:\n                index = pred.argsort()\n                threshold_index = index[min(len(index), self.min_kept) - 1]\n                if pred[threshold_index] > self.thresh:\n                    threshold = pred[threshold_index]\n            kept_flag = pred <= threshold\n            valid_inds = valid_inds[kept_flag]\n            print(\'hard ratio: {} = {} / {} \'.format(round(len(valid_inds)/num_valid, 4), len(valid_inds), num_valid))\n\n        label = input_label[valid_inds].copy()\n        input_label.fill(self.ignore_label)\n        input_label[valid_inds] = label\n        print(np.sum(input_label != self.ignore_label))\n        target = torch.from_numpy(input_label.reshape(target.size())).long().cuda()\n\n        return self.criterion(predict, target)\n'"
src/losses/multi/softiou_loss.py,3,"b'import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\n\r\nclass SoftIoULoss(nn.Module):\r\n    def __init__(self, n_classes):\r\n        super(SoftIoULoss, self).__init__()\r\n        self.n_classes = n_classes\r\n\r\n    @staticmethod\r\n    def to_one_hot(tensor, n_classes):\r\n        n, h, w = tensor.size()\r\n        one_hot = torch.zeros(n, n_classes, h, w).scatter_(1, tensor.view(n, 1, h, w), 1)\r\n        return one_hot\r\n\r\n    def forward(self, logit, target):\r\n        # logit => N x Classes x H x W\r\n        # target => N x H x W\r\n\r\n        N = len(logit)\r\n\r\n        pred = F.softmax(logit, dim=1)\r\n        target_onehot = self.to_one_hot(target, self.n_classes)\r\n\r\n        # Numerator Product\r\n        inter = pred * target_onehot\r\n        # Sum over all pixels N x C x H x W => N x C\r\n        inter = inter.view(N, self.n_classes, -1).sum(2)\r\n\r\n        # Denominator\r\n        union = pred + target_onehot - (pred * target_onehot)\r\n        # Sum over all pixels N x C x H x W => N x C\r\n        union = union.view(N, self.n_classes, -1).sum(2)\r\n\r\n        loss = inter / (union + 1e-16)\r\n\r\n        # Return average loss over classes and batch\r\n        return -loss.mean()\r\n'"
src/losses/multi/sym_loss.py,6,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass SoftCrossEntropy(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, logits, labels, valid_mask=None):\n        if valid_mask is not None:\n            loss = 0\n            batch_size = logits.shape[0]\n            for logit, lbl, val_msk in zip(logits, labels, valid_mask):\n                logit = logit[:, val_msk]\n                lbl = lbl[:, val_msk]\n                loss -= torch.mean(torch.mul(F.log_softmax(logit, dim=0), F.softmax(lbl, dim=0)))\n            return loss / batch_size\n        else:\n            return torch.mean(torch.mul(F.log_softmax(logits, dim=1), F.softmax(labels, dim=1)))\n\n\nclass KlLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, logits, labels, valid_mask=None):\n        if valid_mask is not None:\n            loss = 0\n            batch_size = logits.shape[0]\n            for logit, lbl, val_msk in zip(logits, labels, valid_mask):\n                logit = logit[:, val_msk]\n                lbl = lbl[:, val_msk]\n                loss += torch.mean(F.kl_div(F.log_softmax(logit, dim=0), F.softmax(lbl, dim=0), reduction='none'))\n            return loss / batch_size\n        else:\n            return torch.mean(F.kl_div(F.log_softmax(logits, dim=1), F.softmax(labels, dim=1), reduction='none'))\n"""
