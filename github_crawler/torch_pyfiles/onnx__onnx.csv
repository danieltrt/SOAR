file_path,api_count,code
setup.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom distutils.spawn import find_executable\nfrom distutils import sysconfig, log\nimport setuptools\nimport setuptools.command.build_py\nimport setuptools.command.develop\nimport setuptools.command.build_ext\n\nfrom collections import namedtuple\nfrom contextlib import contextmanager\nimport glob\nimport os\nimport shlex\nimport subprocess\nimport sys\nimport platform\nfrom textwrap import dedent\nimport multiprocessing\n\n\nTOP_DIR = os.path.realpath(os.path.dirname(__file__))\nSRC_DIR = os.path.join(TOP_DIR, \'onnx\')\nTP_DIR = os.path.join(TOP_DIR, \'third_party\')\nCMAKE_BUILD_DIR = os.path.join(TOP_DIR, \'.setuptools-cmake-build\')\n\nWINDOWS = (os.name == \'nt\')\n\nCMAKE = find_executable(\'cmake3\') or find_executable(\'cmake\')\nMAKE = find_executable(\'make\')\n\ninstall_requires = []\nsetup_requires = []\ntests_require = []\nextras_require = {}\n\n################################################################################\n# Global variables for controlling the build variant\n################################################################################\n\n# Default value is set to TRUE\\1 to keep the settings same as the current ones.\n# However going forward the recomemded way to is to set this to False\\0\nUSE_MSVC_STATIC_RUNTIME = bool(os.getenv(\'USE_MSVC_STATIC_RUNTIME\', \'1\') == \'1\')\nONNX_ML = not bool(os.getenv(\'ONNX_ML\') == \'0\')\nONNX_VERIFY_PROTO3 = bool(os.getenv(\'ONNX_VERIFY_PROTO3\') == \'1\')\nONNX_NAMESPACE = os.getenv(\'ONNX_NAMESPACE\', \'onnx\')\nONNX_BUILD_TESTS = bool(os.getenv(\'ONNX_BUILD_TESTS\') == \'1\')\n\nDEBUG = bool(os.getenv(\'DEBUG\'))\nCOVERAGE = bool(os.getenv(\'COVERAGE\'))\n\n################################################################################\n# Version\n################################################################################\n\ntry:\n    git_version = subprocess.check_output([\'git\', \'rev-parse\', \'HEAD\'],\n                                          cwd=TOP_DIR).decode(\'ascii\').strip()\nexcept (OSError, subprocess.CalledProcessError):\n    git_version = None\n\nwith open(os.path.join(TOP_DIR, \'VERSION_NUMBER\')) as version_file:\n    VersionInfo = namedtuple(\'VersionInfo\', [\'version\', \'git_version\'])(\n        version=version_file.read().strip(),\n        git_version=git_version\n    )\n\n################################################################################\n# Pre Check\n################################################################################\n\nassert CMAKE, \'Could not find ""cmake"" executable!\'\n\n################################################################################\n# Utilities\n################################################################################\n\n\n@contextmanager\ndef cd(path):\n    if not os.path.isabs(path):\n        raise RuntimeError(\'Can only cd to absolute path, got: {}\'.format(path))\n    orig_path = os.getcwd()\n    os.chdir(path)\n    try:\n        yield\n    finally:\n        os.chdir(orig_path)\n\n\n################################################################################\n# Customized commands\n################################################################################\n\n\nclass ONNXCommand(setuptools.Command):\n    user_options = []\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n\nclass create_version(ONNXCommand):\n    def run(self):\n        with open(os.path.join(SRC_DIR, \'version.py\'), \'w\') as f:\n            f.write(dedent(\'\'\'\\\n            # This file is generated by setup.py. DO NOT EDIT!\n\n            from __future__ import absolute_import\n            from __future__ import division\n            from __future__ import print_function\n            from __future__ import unicode_literals\n\n            version = \'{version}\'\n            git_version = \'{git_version}\'\n            \'\'\'.format(**dict(VersionInfo._asdict()))))\n\n\nclass cmake_build(setuptools.Command):\n    """"""\n    Compiles everything when `python setupmnm.py build` is run using cmake.\n\n    Custom args can be passed to cmake by specifying the `CMAKE_ARGS`\n    environment variable.\n\n    The number of CPUs used by `make` can be specified by passing `-j<ncpus>`\n    to `setup.py build`.  By default all CPUs are used.\n    """"""\n    user_options = [\n        (str(\'jobs=\'), str(\'j\'), str(\'Specifies the number of jobs to use with make\'))\n    ]\n\n    built = False\n\n    def initialize_options(self):\n        self.jobs = None\n\n    def finalize_options(self):\n        if sys.version_info[0] >= 3:\n            self.set_undefined_options(\'build\', (\'parallel\', \'jobs\'))\n        if self.jobs is None and os.getenv(""MAX_JOBS"") is not None:\n            self.jobs = os.getenv(""MAX_JOBS"")\n        self.jobs = multiprocessing.cpu_count() if self.jobs is None else int(self.jobs)\n\n    def run(self):\n        if cmake_build.built:\n            return\n        cmake_build.built = True\n        if not os.path.exists(CMAKE_BUILD_DIR):\n            os.makedirs(CMAKE_BUILD_DIR)\n\n        with cd(CMAKE_BUILD_DIR):\n            build_type = \'Release\'\n            # configure\n            cmake_args = [\n                CMAKE,\n                \'-DPYTHON_INCLUDE_DIR={}\'.format(sysconfig.get_python_inc()),\n                \'-DPYTHON_EXECUTABLE={}\'.format(sys.executable),\n                \'-DBUILD_ONNX_PYTHON=ON\',\n                \'-DCMAKE_EXPORT_COMPILE_COMMANDS=ON\',\n                \'-DONNX_NAMESPACE={}\'.format(ONNX_NAMESPACE),\n                \'-DPY_EXT_SUFFIX={}\'.format(sysconfig.get_config_var(\'EXT_SUFFIX\') or \'\'),\n            ]\n            if COVERAGE:\n                cmake_args.append(\'-DONNX_COVERAGE=ON\')\n            if COVERAGE or DEBUG:\n                # in order to get accurate coverage information, the\n                # build needs to turn off optimizations\n                build_type = \'Debug\'\n            cmake_args.append(\'-DCMAKE_BUILD_TYPE=%s\' % build_type)\n            if WINDOWS:\n                cmake_args.extend([\n                    # we need to link with libpython on windows, so\n                    # passing python version to window in order to\n                    # find python in cmake\n                    \'-DPY_VERSION={}\'.format(\'{0}.{1}\'.format(*sys.version_info[:2])),\n                ])\n                if USE_MSVC_STATIC_RUNTIME:\n                    cmake_args.append(\'-DONNX_USE_MSVC_STATIC_RUNTIME=ON\')\n                if platform.architecture()[0] == \'64bit\':\n                    cmake_args.extend([\'-A\', \'x64\', \'-T\', \'host=x64\'])\n                else:\n                    cmake_args.extend([\'-A\', \'Win32\', \'-T\', \'host=x86\'])\n            if ONNX_ML:\n                cmake_args.append(\'-DONNX_ML=1\')\n            if ONNX_VERIFY_PROTO3:\n                cmake_args.append(\'-DONNX_VERIFY_PROTO3=1\')\n            if ONNX_BUILD_TESTS:\n                cmake_args.append(\'-DONNX_BUILD_TESTS=ON\')\n            if \'CMAKE_ARGS\' in os.environ:\n                extra_cmake_args = shlex.split(os.environ[\'CMAKE_ARGS\'])\n                # prevent crossfire with downstream scripts\n                del os.environ[\'CMAKE_ARGS\']\n                log.info(\'Extra cmake args: {}\'.format(extra_cmake_args))\n                cmake_args.extend(extra_cmake_args)\n            cmake_args.append(TOP_DIR)\n            subprocess.check_call(cmake_args)\n\n            build_args = [CMAKE, \'--build\', os.curdir]\n            if WINDOWS:\n                build_args.extend([\'--config\', build_type])\n                build_args.extend([\'--\', \'/maxcpucount:{}\'.format(self.jobs)])\n            else:\n                build_args.extend([\'--\', \'-j\', str(self.jobs)])\n            subprocess.check_call(build_args)\n\n\nclass build_py(setuptools.command.build_py.build_py):\n    def run(self):\n        self.run_command(\'create_version\')\n        self.run_command(\'cmake_build\')\n\n        generated_python_files = \\\n            glob.glob(os.path.join(CMAKE_BUILD_DIR, \'onnx\', \'*.py\')) + \\\n            glob.glob(os.path.join(CMAKE_BUILD_DIR, \'onnx\', \'*.pyi\'))\n\n        for src in generated_python_files:\n            dst = os.path.join(\n                TOP_DIR, os.path.relpath(src, CMAKE_BUILD_DIR))\n            self.copy_file(src, dst)\n\n        return setuptools.command.build_py.build_py.run(self)\n\n\nclass develop(setuptools.command.develop.develop):\n    def run(self):\n        self.run_command(\'build_py\')\n        setuptools.command.develop.develop.run(self)\n\n\nclass build_ext(setuptools.command.build_ext.build_ext):\n    def run(self):\n        self.run_command(\'cmake_build\')\n        setuptools.command.build_ext.build_ext.run(self)\n\n    def build_extensions(self):\n        for ext in self.extensions:\n            fullname = self.get_ext_fullname(ext.name)\n            filename = os.path.basename(self.get_ext_filename(fullname))\n\n            lib_path = CMAKE_BUILD_DIR\n            if os.name == \'nt\':\n                debug_lib_dir = os.path.join(lib_path, ""Debug"")\n                release_lib_dir = os.path.join(lib_path, ""Release"")\n                if os.path.exists(debug_lib_dir):\n                    lib_path = debug_lib_dir\n                elif os.path.exists(release_lib_dir):\n                    lib_path = release_lib_dir\n            src = os.path.join(lib_path, filename)\n            dst = os.path.join(os.path.realpath(self.build_lib), ""onnx"", filename)\n            self.copy_file(src, dst)\n\n\nclass mypy_type_check(ONNXCommand):\n    description = \'Run MyPy type checker\'\n\n    def run(self):\n        """"""Run command.""""""\n        onnx_script = os.path.realpath(os.path.join(os.path.dirname(os.path.abspath(__file__)), ""tools/mypy-onnx.py""))\n        returncode = subprocess.call([sys.executable, onnx_script])\n        sys.exit(returncode)\n\n\ncmdclass = {\n    \'create_version\': create_version,\n    \'cmake_build\': cmake_build,\n    \'build_py\': build_py,\n    \'develop\': develop,\n    \'build_ext\': build_ext,\n    \'typecheck\': mypy_type_check,\n}\n\n################################################################################\n# Extensions\n################################################################################\n\next_modules = [\n    setuptools.Extension(\n        name=str(\'onnx.onnx_cpp2py_export\'),\n        sources=[])\n]\n\n################################################################################\n# Packages\n################################################################################\n\n# no need to do fancy stuff so far\npackages = setuptools.find_packages()\n\ninstall_requires.extend([\n    \'protobuf\',\n    \'numpy\',\n    \'six\',\n    \'typing>=3.6.4; python_version < ""3.5""\',\n    \'typing-extensions>=3.6.2.1\',\n])\n\n################################################################################\n# Test\n################################################################################\n\nsetup_requires.append(\'pytest-runner\')\ntests_require.append(\'pytest\')\ntests_require.append(\'nbval\')\ntests_require.append(\'tabulate\')\n\nif sys.version_info[0] == 3:\n    # Mypy doesn\'t work with Python 2\n    extras_require[\'mypy\'] = [\'mypy==0.600\']\n\n################################################################################\n# Final\n################################################################################\n\nsetuptools.setup(\n    name=""onnx"",\n    version=VersionInfo.version,\n    description=""Open Neural Network Exchange"",\n    ext_modules=ext_modules,\n    cmdclass=cmdclass,\n    packages=packages,\n    license=\'MIT\',\n    include_package_data=True,\n    install_requires=install_requires,\n    setup_requires=setup_requires,\n    tests_require=tests_require,\n    extras_require=extras_require,\n    author=\'ONNX\',\n    author_email=\'onnx-technical-discuss@lists.lfai.foundation\',\n    url=\'https://github.com/onnx/onnx\',\n    entry_points={\n        \'console_scripts\': [\n            \'check-model = onnx.bin.checker:check_model\',\n            \'check-node = onnx.bin.checker:check_node\',\n            \'backend-test-tools = onnx.backend.test.cmd_tools:main\',\n        ]\n    },\n)\n'"
onnx/__init__.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport os\n\nfrom .onnx_cpp2py_export import ONNX_ML\nfrom onnx.external_data_helper import load_external_data_for_model, write_external_data_tensors\nfrom .onnx_pb import *  # noqa\nfrom .onnx_operators_pb import * # noqa\nfrom .version import version as __version__  # noqa\n\n# Import common subpackages so they\'re available when you \'import onnx\'\nimport onnx.helper  # noqa\nimport onnx.checker  # noqa\nimport onnx.defs  # noqa\n\nimport google.protobuf.message\n\nfrom typing import Union, Text, IO, Optional, cast, TypeVar, Any\nfrom six import string_types\n\n\n# f should be either readable or a file path\ndef _load_bytes(f):  # type: (Union[IO[bytes], Text]) -> bytes\n    if hasattr(f, \'read\') and callable(cast(IO[bytes], f).read):\n        s = cast(IO[bytes], f).read()\n    else:\n        with open(cast(Text, f), \'rb\') as readable:\n            s = readable.read()\n    return s\n\n\n# str should be bytes,\n# f should be either writable or a file path\ndef _save_bytes(str, f):  # type: (bytes, Union[IO[bytes], Text]) -> None\n    if hasattr(f, \'write\') and callable(cast(IO[bytes], f).write):\n        cast(IO[bytes], f).write(str)\n    else:\n        with open(cast(Text, f), \'wb\') as writable:\n            writable.write(str)\n\n\n# f should be either a readable file or a file path\ndef _get_file_path(f):  # type: (Union[IO[bytes], Text]) -> Optional[Text]\n    if isinstance(f, string_types):\n        return os.path.abspath(f)\n    if hasattr(f, \'name\'):\n        return os.path.abspath(f.name)\n    return None\n\n\ndef _serialize(proto):  # type: (Union[bytes, google.protobuf.message.Message]) -> bytes\n    \'\'\'\n    Serialize a in-memory proto to bytes\n\n    @params\n    proto is a in-memory proto, such as a ModelProto, TensorProto, etc\n\n    @return\n    Serialized proto in bytes\n    \'\'\'\n    if isinstance(proto, bytes):\n        return proto\n    elif hasattr(proto, \'SerializeToString\') and callable(proto.SerializeToString):\n        result = proto.SerializeToString()\n        return result\n    else:\n        raise TypeError(\'No SerializeToString method is detected. \'\n                         \'neither proto is a str.\\ntype is {}\'.format(type(proto)))\n\n\n_Proto = TypeVar(\'_Proto\', bound=google.protobuf.message.Message)\n\n\ndef _deserialize(s, proto):  # type: (bytes, _Proto) -> _Proto\n    \'\'\'\n    Parse bytes into a in-memory proto\n\n    @params\n    s is bytes containing serialized proto\n    proto is a in-memory proto object\n\n    @return\n    The proto instance filled in by s\n    \'\'\'\n    if not isinstance(s, bytes):\n        raise ValueError(\'Parameter s must be bytes, but got type: {}\'.format(type(s)))\n\n    if not (hasattr(proto, \'ParseFromString\') and callable(proto.ParseFromString)):\n        raise ValueError(\'No ParseFromString method is detected. \'\n                         \'\\ntype is {}\'.format(type(proto)))\n\n    decoded = cast(Optional[int], proto.ParseFromString(s))\n    if decoded is not None and decoded != len(s):\n        raise google.protobuf.message.DecodeError(\n            ""Protobuf decoding consumed too few bytes: {} out of {}"".format(\n                decoded, len(s)))\n    return proto\n\n\ndef load_model(f, format=None, load_external_data=True):  # type: (Union[IO[bytes], Text], Optional[Any], bool) -> ModelProto\n    \'\'\'\n    Loads a serialized ModelProto into memory\n\n    @params\n    f can be a file-like object (has ""read"" function) or a string containing a file name\n    format is for future use\n\n    @return\n    Loaded in-memory ModelProto\n    \'\'\'\n    s = _load_bytes(f)\n    model = load_model_from_string(s, format=format)\n\n    if load_external_data:\n        model_filepath = _get_file_path(f)\n        if model_filepath:\n            base_dir = os.path.dirname(model_filepath)\n            load_external_data_for_model(model, base_dir)\n\n    return model\n\n\ndef load_tensor(f, format=None):  # type: (Union[IO[bytes], Text], Optional[Any]) -> TensorProto\n    \'\'\'\n    Loads a serialized TensorProto into memory\n\n    @params\n    f can be a file-like object (has ""read"" function) or a string containing a file name\n    format is for future use\n\n    @return\n    Loaded in-memory TensorProto\n    \'\'\'\n    s = _load_bytes(f)\n    return load_tensor_from_string(s, format=format)\n\n\ndef load_model_from_string(s, format=None):  # type: (bytes, Optional[Any]) -> ModelProto\n    \'\'\'\n    Loads a binary string (bytes) that contains serialized ModelProto\n\n    @params\n    s is a string, which contains serialized ModelProto\n    format is for future use\n\n    @return\n    Loaded in-memory ModelProto\n    \'\'\'\n    return _deserialize(s, ModelProto())\n\n\ndef load_tensor_from_string(s, format=None):  # type: (bytes, Optional[Any]) -> TensorProto\n    \'\'\'\n    Loads a binary string (bytes) that contains serialized TensorProto\n\n    @params\n    s is a string, which contains serialized TensorProto\n    format is for future use\n\n    @return\n    Loaded in-memory TensorProto\n    \'\'\'\n    return _deserialize(s, TensorProto())\n\n\ndef save_model(proto, f, format=None):  # type: (Union[ModelProto, bytes], Union[IO[bytes], Text], Optional[Any]) -> None\n    \'\'\'\n    Saves the ModelProto to the specified path.\n\n    @params\n    proto should be a in-memory ModelProto\n    f can be a file-like object (has ""write"" function) or a string containing a file name\n    format is for future use\n    \'\'\'\n    if isinstance(proto, bytes):\n        proto = _deserialize(proto, ModelProto())\n\n    model_filepath = _get_file_path(f)\n    if model_filepath:\n        basepath = os.path.dirname(model_filepath)\n        proto = write_external_data_tensors(proto, basepath)\n\n    s = _serialize(proto)\n    _save_bytes(s, f)\n\n\ndef save_tensor(proto, f):  # type: (TensorProto, Union[IO[bytes], Text]) -> None\n    \'\'\'\n    Saves the TensorProto to the specified path.\n\n    @params\n    proto should be a in-memory TensorProto\n    f can be a file-like object (has ""write"" function) or a string containing a file name\n    format is for future use\n    \'\'\'\n    s = _serialize(proto)\n    _save_bytes(s, f)\n\n\n# For backward compatibility\nload = load_model\nload_from_string = load_model_from_string\nsave = save_model\n'"
onnx/checker.py,0,"b'""""""onnx checker\n\nThis implements graphalities that allows us to check whether a serialized\nproto is legal.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport functools\n\nfrom onnx import (ValueInfoProto,\n                  AttributeProto,\n                  TensorProto,\n                  SparseTensorProto,\n                  NodeProto,\n                  ModelProto,\n                  GraphProto,\n                  IR_VERSION)\nimport onnx.onnx_cpp2py_export.checker as C\nimport onnx.defs\nfrom google.protobuf.message import Message\nfrom typing import TypeVar, Callable, Any, Type, cast, Union, Text\nfrom six import string_types\nimport onnx.shape_inference\n\n\n# TODO: This thing where we reserialize the protobuf back into the\n# string, only to deserialize it at the call site, is really goofy.\n# Stop doing that.\n\n\n# NB: Please don\'t edit this context!\nDEFAULT_CONTEXT = C.CheckerContext()\nDEFAULT_CONTEXT.ir_version = IR_VERSION\n# TODO: Maybe ONNX-ML should also be defaulted?\nDEFAULT_CONTEXT.opset_imports = {\'\': onnx.defs.onnx_opset_version()}\n\n\nFuncType = TypeVar(\'FuncType\', bound=Callable[..., Any])\n\n\n# TODO: This really doesn\'t seem worth the metaprogramming...\ndef _create_checker(proto_type):  # type: (Type[Message]) -> Callable[[FuncType], FuncType]\n    def decorator(py_func):  # type: (FuncType) -> FuncType\n        @functools.wraps(py_func)\n        def checker(proto, ctx=DEFAULT_CONTEXT):  # type: (Message, C.CheckerContext) -> Any\n            if not isinstance(proto, proto_type):\n                raise RuntimeError(\n                    \'You cannot pass an object that is not of type {}\'.format(\n                        proto_type.__name__))\n            return getattr(C, py_func.__name__)(\n                proto.SerializeToString(), ctx)\n        return cast(FuncType, checker)\n    return decorator\n\n\n@_create_checker(ValueInfoProto)\ndef check_value_info(value_info, ctx=DEFAULT_CONTEXT):  # type: (ValueInfoProto, C.CheckerContext) -> None\n    pass\n\n\n@_create_checker(TensorProto)\ndef check_tensor(tensor, ctx=DEFAULT_CONTEXT):  # type: (TensorProto, C.CheckerContext) -> None\n    pass\n\n\n@_create_checker(AttributeProto)\ndef check_attribute(attr, ctx=DEFAULT_CONTEXT):  # type: (AttributeProto, C.CheckerContext) -> None\n    pass\n\n\n@_create_checker(NodeProto)\ndef check_node(node, ctx=DEFAULT_CONTEXT):  # type: (NodeProto, C.CheckerContext) -> None\n    pass\n\n\n@_create_checker(GraphProto)\ndef check_graph(graph, ctx=DEFAULT_CONTEXT):  # type: (GraphProto, C.CheckerContext) -> None\n    pass\n\n\ndef check_sparse_tensor(sparse, ctx=DEFAULT_CONTEXT):  # type: (SparseTensorProto, C.CheckerContext) -> None\n    C.check_sparse_tensor(sparse.SerializeToString(), ctx)\n\n\ndef check_model(model, full_check=False):  # type: (Union[ModelProto, Text], bool) -> None\n    if isinstance(model, string_types):\n        C.check_model_path(model)\n        m = onnx.load(model)\n    else:\n        C.check_model(model.SerializeToString())\n        m = model\n    if full_check:\n        onnx.shape_inference.infer_shapes(m, True)\n\n\nValidationError = C.ValidationError\n'"
onnx/external_data_helper.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport uuid\nimport os\nfrom itertools import chain\nfrom typing import Iterable, Text, Optional\nfrom .onnx_pb import TensorProto, ModelProto\n\n\nclass ExternalDataInfo(object):\n\n    def __init__(self, tensor):  # type: (TensorProto) -> None\n        self.location = \'\'\n        self.offset = None\n        self.length = None\n        self.checksum = None\n        self.basepath = \'\'\n\n        for entry in tensor.external_data:\n            setattr(self, entry.key, entry.value)\n\n        if self.offset:\n            self.offset = int(self.offset)\n\n        if self.length:\n            self.length = int(self.length)\n\n\ndef load_external_data_for_tensor(tensor, base_dir):  # type: (TensorProto, Text) -> None\n    """"""\n    Load data from an external file for tensor.\n\n    @params\n    tensor: a TensorProto object.\n    base_dir: directory that contains the external data.\n    """"""\n    if tensor.HasField(""raw_data""):  # already loaded\n        return\n    info = ExternalDataInfo(tensor)\n    file_location = _sanitize_path(info.location)\n    external_data_file_path = os.path.join(base_dir, file_location)\n\n    with open(external_data_file_path, \'rb\') as data_file:\n\n        if info.offset:\n            data_file.seek(info.offset)\n\n        if info.length:\n            tensor.raw_data = data_file.read(info.length)\n        else:\n            tensor.raw_data = data_file.read()\n\n\ndef load_external_data_for_model(model, base_dir):  # type: (ModelProto, Text) -> None\n    """"""\n    Loads external tensors into model\n\n    @params\n    model: ModelProto to load external data to\n    base_dir: directory that contains external data\n    """"""\n    for tensor in _get_all_tensors(model):\n        if uses_external_data(tensor):\n            load_external_data_for_tensor(tensor, base_dir)\n\n\ndef set_external_data(tensor,  # type: TensorProto\n                      location,  # type: Text\n                      offset=None,  # type: Optional[int]\n                      length=None,  # type: Optional[int]\n                      checksum=None,  # type: Optional[Text]\n                      basepath=None  # type: Optional[Text]\n                      ):  # type: (...) -> None\n    del tensor.external_data[:]\n    tensor.data_location = TensorProto.EXTERNAL\n    for (k, v) in {\n        \'location\': location,\n        \'offset\': int(offset) if offset is not None else None,\n        \'length\': int(length) if length is not None else None,\n        \'checksum\': checksum,\n        \'basepath\': basepath\n    }.items():\n        if v is not None:\n            entry = tensor.external_data.add()\n            entry.key = k\n            entry.value = str(v)\n\n\ndef convert_model_to_external_data(model, all_tensors_to_one_file=True, location=None):\n    # type: (ModelProto, bool, Optional[Text]) -> None\n    """"""\n    call to set all tensors as external data. save_model saves all the tensors data as external data after calling this function.\n    @params\n    model: ModelProto to be converted.\n    all_tensors_to_one_file: If true, save all tensors to one external file specified by location.\n                             If false, save each tensor to a file named with the tensor name.\n    location: specify the external file that all tensors to save to.\n              If not specified, will use the model name.\n    """"""\n    if all_tensors_to_one_file:\n        file_name = Text(uuid.uuid1())\n        if location:\n            file_name = location\n        for tensor in _get_all_tensors(model):\n            set_external_data(tensor, file_name)\n    else:\n        for tensor in _get_all_tensors(model):\n            set_external_data(tensor, tensor.name)\n\n\ndef convert_model_from_external_data(model):  # type: (ModelProto) -> None\n    """"""\n    call to set all tensors data as embedded data. save_model saves all the tensors data as embedded data after calling this function.\n    @params\n    model: ModelProto to be converted.\n    """"""\n    for tensor in _get_all_tensors(model):\n        if uses_external_data(tensor):\n            if not tensor.HasField(""raw_data""):\n                raise ValueError(""raw_data field doesn\'t exist."")\n            del tensor.external_data[:]\n            tensor.data_location = TensorProto.DEFAULT\n\n\ndef save_external_data(tensor, base_path):  # type: (TensorProto, Text) -> None\n    """"""\n    Write tensor data to an external file according to information in the `external_data` field.\n\n    @params\n    tensor: Tensor object to be serialized\n    base_path: System path of a folder where tensor data is to be stored\n    """"""\n    info = ExternalDataInfo(tensor)\n    external_data_file_path = os.path.join(base_path, info.location)\n\n    # Retrieve the tensor\'s data from raw_data or load external file\n    if not tensor.HasField(""raw_data""):\n        raise ValueError(""raw_data field doesn\'t exist."")\n\n    # Create file if it doesn\'t exist\n    if not os.path.isfile(external_data_file_path):\n        open(external_data_file_path, \'ab\').close()\n\n    # Open file for reading and writing at random locations (\'r+b\')\n    with open(external_data_file_path, \'r+b\') as data_file:\n        data_file.seek(0, 2)\n        if info.offset is not None:\n            # Pad file to required offset if needed\n            file_size = data_file.tell()\n            if info.offset > file_size:\n                data_file.write(b""\\0"" * (info.offset - file_size))\n\n            data_file.seek(info.offset)\n        offset = data_file.tell()\n        data_file.write(tensor.raw_data)\n        set_external_data(tensor, info.location, offset, data_file.tell() - offset)\n\n\ndef _get_all_tensors(onnx_model_proto):  # type: (ModelProto) -> Iterable[TensorProto]\n    """"""Scan an ONNX model for all tensors and return as an iterator.""""""\n    return chain(_get_initializer_tensors(onnx_model_proto),\n                 _get_attribute_tensors(onnx_model_proto))\n\n\ndef _get_initializer_tensors(onnx_model_proto):  # type: (ModelProto) -> Iterable[TensorProto]\n    """"""Create an iterator of initializer tensors from ONNX model.""""""\n    for initializer in onnx_model_proto.graph.initializer:\n        yield initializer\n\n\ndef _get_attribute_tensors(onnx_model_proto):  # type: (ModelProto) -> Iterable[TensorProto]\n    """"""Create an iterator of tensors from node attributes of an ONNX model.""""""\n    for node in onnx_model_proto.graph.node:\n        for attribute in node.attribute:\n            if attribute.HasField(""t""):\n                yield attribute.t\n            for tensor in attribute.tensors:\n                yield tensor\n\n\ndef _sanitize_path(path):  # type: (Text) -> Text\n    """"""Remove path components which would allow traversing up a directory tree from a base path.\n\n    Note: This method is currently very basic and should be expanded.\n    """"""\n    return path.lstrip(\'/.\')\n\n\ndef uses_external_data(tensor):  # type: (TensorProto) -> bool\n    """"""Return true if the tensor stores data in an external location.""""""\n    return tensor.HasField(""data_location"") and tensor.data_location == TensorProto.EXTERNAL\n\n\ndef remove_external_data_field(tensor, field_key):  # type: (TensorProto, Text) -> None\n    """"""\n    Remove a field from a Tensor\'s external_data key-value store.\n\n    Modifies tensor object in place.\n\n    @params\n    tensor: Tensor object from which value will be removed\n    field_key: The key of the field to be removed\n    """"""\n    for (i, field) in enumerate(tensor.external_data):\n        if field.key == field_key:\n            del tensor.external_data[i]\n\n\ndef write_external_data_tensors(model, filepath):  # type: (ModelProto, Text) -> ModelProto\n    """"""\n    Write external data of all tensors to files on disk.\n\n    Note: This function also strips basepath information from all tensors\' external_data fields.\n\n    @params\n    model: Model object which is the source of tensors to serialize.\n    filepath: System path to the directory which should be treated as base path for external data.\n\n    @return\n    The modified model object.\n    """"""\n    for tensor in _get_all_tensors(model):\n        if uses_external_data(tensor):\n            save_external_data(tensor, filepath)\n            tensor.ClearField(str(\'raw_data\'))\n\n    return model\n'"
onnx/gen_proto.py,0,"b'#!/usr/bin/env python\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport argparse\nimport io\nimport os\nimport re\nimport glob\nimport subprocess\nfrom textwrap import dedent\n\nautogen_header = """"""\\\n//\n// WARNING: This file is automatically generated!  Please edit onnx.in.proto.\n//\n\n\n""""""\n\nLITE_OPTION = \'\'\'\n\n// For using protobuf-lite\noption optimize_for = LITE_RUNTIME;\n\n\'\'\'\n\nDEFAULT_PACKAGE_NAME = ""onnx""\n\nIF_ONNX_ML_REGEX = re.compile(r\'\\s*//\\s*#if\\s+ONNX-ML\\s*$\')\nENDIF_ONNX_ML_REGEX = re.compile(r\'\\s*//\\s*#endif\\s*$\')\nELSE_ONNX_ML_REGEX = re.compile(r\'\\s*//\\s*#else\\s*$\')\n\n\nMYPY = False\nif MYPY:\n    from typing import Iterable, Text\n\n\ndef process_ifs(lines, onnx_ml):  # type: (Iterable[Text], bool) -> Iterable[Text]\n    in_if = 0\n    for line in lines:\n        if IF_ONNX_ML_REGEX.match(line):\n            assert 0 == in_if\n            in_if = 1\n        elif ELSE_ONNX_ML_REGEX.match(line):\n            assert 1 == in_if\n            in_if = 2\n        elif ENDIF_ONNX_ML_REGEX.match(line):\n            assert (1 == in_if or 2 == in_if)\n            in_if = 0\n        else:\n            if 0 == in_if:\n                yield line\n            elif (1 == in_if and onnx_ml):\n                yield line\n            elif (2 == in_if and not onnx_ml):\n                yield line\n\n\nIMPORT_REGEX = re.compile(r\'(\\s*)import\\s*""([^""]*)\\.proto"";\\s*$\')\nPACKAGE_NAME_REGEX = re.compile(r\'\\{PACKAGE_NAME\\}\')\nML_REGEX = re.compile(r\'(.*)\\-ml\')\n\n\ndef process_package_name(lines, package_name):  # type: (Iterable[Text], Text) -> Iterable[Text]\n    need_rename = (package_name != DEFAULT_PACKAGE_NAME)\n    for line in lines:\n        m = IMPORT_REGEX.match(line) if need_rename else None\n        if m:\n            include_name = m.group(2)\n            ml = ML_REGEX.match(include_name)\n            if ml:\n                include_name = ""{}_{}-ml"".format(ml.group(1), package_name)\n            else:\n                include_name = ""{}_{}"".format(include_name, package_name)\n            yield m.group(1) + \'import ""{}.proto"";\'.format(include_name)\n        else:\n            yield PACKAGE_NAME_REGEX.sub(package_name, line)\n\n\nPROTO_SYNTAX_REGEX = re.compile(r\'(\\s*)syntax\\s*=\\s*""proto2""\\s*;\\s*$\')\nOPTIONAL_REGEX = re.compile(r\'(\\s*)optional\\s(.*)$\')\n\n\ndef convert_to_proto3(lines):  # type: (Iterable[Text]) -> Iterable[Text]\n    for line in lines:\n        # Set the syntax specifier\n        m = PROTO_SYNTAX_REGEX.match(line)\n        if m:\n            yield m.group(1) + \'syntax = ""proto3"";\'\n            continue\n\n        # Remove optional keywords\n        m = OPTIONAL_REGEX.match(line)\n        if m:\n            yield m.group(1) + m.group(2)\n            continue\n\n        # Rewrite import\n        m = IMPORT_REGEX.match(line)\n        if m:\n            yield m.group(1) + \'import ""{}.proto3"";\'.format(m.group(2))\n            continue\n\n        yield line\n\n\ndef gen_proto3_code(protoc_path, proto3_path, include_path, cpp_out, python_out):  # type: (Text, Text, Text, Text, Text) -> None\n    print(""Generate pb3 code using {}"".format(protoc_path))\n    build_args = [protoc_path, proto3_path, \'-I\', include_path]\n    build_args.extend([\'--cpp_out\', cpp_out, \'--python_out\', python_out])\n    subprocess.check_call(build_args)\n\n\ndef translate(source, proto, onnx_ml, package_name):  # type: (Text, int, bool, Text) -> Text\n    lines = source.splitlines()  # type: Iterable[Text]\n    lines = process_ifs(lines, onnx_ml=onnx_ml)\n    lines = process_package_name(lines, package_name=package_name)\n    if proto == 3:\n        lines = convert_to_proto3(lines)\n    else:\n        assert proto == 2\n    return ""\\n"".join(lines)  # TODO: not Windows friendly\n\n\ndef qualify(f, pardir=os.path.realpath(os.path.dirname(__file__))):  # type: (Text, Text) -> Text\n    return os.path.join(pardir, f)\n\n\ndef convert(stem, package_name, output, do_onnx_ml=False, lite=False, protoc_path=\'\'):  # type: (Text, Text, Text, bool, bool, Text) -> None\n    proto_in = qualify(""{}.in.proto"".format(stem))\n    need_rename = (package_name != DEFAULT_PACKAGE_NAME)\n    if do_onnx_ml:\n        proto_base = ""{}_{}-ml"".format(stem, package_name) if need_rename else ""{}-ml"".format(stem)\n    else:\n        proto_base = ""{}_{}"".format(stem, package_name) if need_rename else ""{}"".format(stem)\n    proto = qualify(""{}.proto"".format(proto_base), pardir=output)\n    proto3 = qualify(""{}.proto3"".format(proto_base), pardir=output)\n\n    print(""Processing {}"".format(proto_in))\n    with io.open(proto_in, \'r\') as fin:\n        source = fin.read()\n        print(""Writing {}"".format(proto))\n        with io.open(proto, \'w\', newline=\'\') as fout:\n            fout.write(autogen_header)\n            fout.write(translate(source, proto=2, onnx_ml=do_onnx_ml, package_name=package_name))\n            if lite:\n                fout.write(LITE_OPTION)\n        print(""Writing {}"".format(proto3))\n        with io.open(proto3, \'w\', newline=\'\') as fout:\n            fout.write(autogen_header)\n            fout.write(translate(source, proto=3, onnx_ml=do_onnx_ml, package_name=package_name))\n            if lite:\n                fout.write(LITE_OPTION)\n        if protoc_path:\n            porto3_dir = os.path.dirname(proto3)\n            base_dir = os.path.dirname(porto3_dir)\n            gen_proto3_code(protoc_path, proto3, base_dir, base_dir, base_dir)\n            pb3_files = glob.glob(os.path.join(porto3_dir, \'*.proto3.*\'))\n            for pb3_file in pb3_files:\n                print(""Removing {}"".format(pb3_file))\n                os.remove(pb3_file)\n\n        if need_rename:\n            if do_onnx_ml:\n                proto_header = qualify(""{}-ml.pb.h"".format(stem), pardir=output)\n            else:\n                proto_header = qualify(""{}.pb.h"".format(stem), pardir=output)\n            print(""Writing {}"".format(proto_header))\n            with io.open(proto_header, \'w\', newline=\'\') as fout:\n                fout.write(""#pragma once\\n"")\n                fout.write(""#include \\""{}.pb.h\\""\\n"".format(proto_base))\n\n    # Generate py mapping\n    # ""-"" is invalid in python module name, replaces \'-\' with \'_\'\n    pb_py = qualify(\'{}_pb.py\'.format(stem.replace(\'-\', \'_\')), pardir=output)\n    if need_rename:\n        pb2_py = qualify(\'{}_pb2.py\'.format(proto_base.replace(\'-\', \'_\')), pardir=output)\n    else:\n        if do_onnx_ml:\n            pb2_py = qualify(\'{}_ml_pb2.py\'.format(stem.replace(\'-\', \'_\')), pardir=output)\n        else:\n            pb2_py = qualify(\'{}_pb2.py\'.format(stem.replace(\'-\', \'_\')), pardir=output)\n\n    print(\'generating {}\'.format(pb_py))\n    with open(pb_py, \'w\') as f:\n        f.write(str(dedent(\'\'\'\\\n        # This file is generated by setup.py. DO NOT EDIT!\n\n        from __future__ import absolute_import\n        from __future__ import division\n        from __future__ import print_function\n        from __future__ import unicode_literals\n\n        from .{} import *  # noqa\n        \'\'\'.format(os.path.splitext(os.path.basename(pb2_py))[0]))))\n\n\ndef main():  # type: () -> None\n    parser = argparse.ArgumentParser(\n        description=\'Generates .proto file variations from .in.proto\')\n    parser.add_argument(\'-p\', \'--package\', default=\'onnx\',\n                        help=\'package name in the generated proto files\'\n                        \' (default: %(default)s)\')\n    parser.add_argument(\'-m\', \'--ml\', action=\'store_true\', help=\'ML mode\')\n    parser.add_argument(\'-l\', \'--lite\', action=\'store_true\',\n                        help=\'generate lite proto to use with protobuf-lite\')\n    parser.add_argument(\'-o\', \'--output\',\n                        default=os.path.realpath(os.path.dirname(__file__)),\n                        help=\'output directory (default: %(default)s)\')\n    parser.add_argument(\'--protoc_path\',\n                        default=\'\',\n                        help=\'path to protoc for proto3 file validation\')\n    parser.add_argument(\'stems\', nargs=\'*\', default=[\'onnx\', \'onnx-operators\'],\n                        help=\'list of .in.proto file stems \'\n                        \'(default: %(default)s)\')\n    args = parser.parse_args()\n\n    if not os.path.exists(args.output):\n        os.makedirs(args.output)\n\n    for stem in args.stems:\n        convert(stem,\n                package_name=args.package,\n                output=args.output,\n                do_onnx_ml=args.ml,\n                lite=args.lite,\n                protoc_path=args.protoc_path)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
onnx/helper.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport collections\nimport numbers\nfrom six import text_type, integer_types, binary_type\n\nimport google.protobuf.message\nfrom onnx import TensorProto, SparseTensorProto, AttributeProto, ValueInfoProto, TensorShapeProto, \\\n    NodeProto, ModelProto, GraphProto, OperatorSetIdProto, TypeProto, IR_VERSION\nimport onnx.defs as defs\nfrom onnx import mapping\nfrom onnx.mapping import STORAGE_TENSOR_TYPE_TO_FIELD\nfrom typing import Text, Sequence, Any, Optional, Dict, Union, TypeVar, Callable, Tuple, List, cast\nimport numpy as np  # type: ignore\n\n\ndef make_node(\n        op_type,  # type: Text\n        inputs,  # type: Sequence[Text]\n        outputs,  # type: Sequence[Text]\n        name=None,  # type: Optional[Text]\n        doc_string=None,  # type: Optional[Text]\n        domain=None,  # type: Optional[Text]\n        **kwargs  # type: Any\n):  # type: (...) -> NodeProto\n    """"""Construct a NodeProto.\n\n    Arguments:\n        op_type (string): The name of the operator to construct\n        inputs (list of string): list of input names\n        outputs (list of string): list of output names\n        name (string, default None): optional unique identifier for NodeProto\n        doc_string (string, default None): optional documentation string for NodeProto\n        domain (string, default None): optional domain for NodeProto.\n            If it\'s None, we will just use default domain (which is empty)\n        **kwargs (dict): the attributes of the node.  The acceptable values\n            are documented in :func:`make_attribute`.\n    """"""\n\n    node = NodeProto()\n    node.op_type = op_type\n    node.input.extend(inputs)\n    node.output.extend(outputs)\n    if name:\n        node.name = name\n    if doc_string:\n        node.doc_string = doc_string\n    if domain is not None:\n        node.domain = domain\n    if kwargs:\n        node.attribute.extend(\n            make_attribute(key, value)\n            for key, value in sorted(kwargs.items()))\n    return node\n\n\ndef make_operatorsetid(\n        domain,  # type: Text\n        version,  # type: int\n):  # type: (...) -> OperatorSetIdProto\n    """"""Construct an OperatorSetIdProto.\n\n    Arguments:\n        domain (string): The domain of the operator set id\n        version (integer): Version of operator set id\n    """"""\n    operatorsetid = OperatorSetIdProto()\n    operatorsetid.domain = domain\n    operatorsetid.version = version\n    return operatorsetid\n\n\ndef make_graph(\n    nodes,  # type: Sequence[NodeProto]\n    name,  # type: Text\n    inputs,  # type: Sequence[ValueInfoProto]\n    outputs,  # type: Sequence[ValueInfoProto]\n    initializer=None,  # type: Optional[Sequence[TensorProto]]\n    doc_string=None,  # type: Optional[Text]\n    value_info=[],  # type: Sequence[ValueInfoProto]\n):  # type: (...) -> GraphProto\n    if initializer is None:\n        initializer = []\n    if value_info is None:\n        value_info = []\n    graph = GraphProto()\n    graph.node.extend(nodes)\n    graph.name = name\n    graph.input.extend(inputs)\n    graph.output.extend(outputs)\n    graph.initializer.extend(initializer)\n    graph.value_info.extend(value_info)\n    if doc_string:\n        graph.doc_string = doc_string\n    return graph\n\n\ndef make_opsetid(domain, version):  # type: (Text, int) -> OperatorSetIdProto\n    opsetid = OperatorSetIdProto()\n    opsetid.domain = domain\n    opsetid.version = version\n    return opsetid\n\n\ndef make_model(graph, **kwargs):  # type: (GraphProto, **Any) -> ModelProto\n    model = ModelProto()\n    # Touch model.ir_version so it is stored as the version from which it is\n    # generated.\n    model.ir_version = IR_VERSION\n    model.graph.CopyFrom(graph)\n\n    opset_imports = None  # type: Optional[Sequence[OperatorSetIdProto]]\n    opset_imports = kwargs.pop(\'opset_imports\', None)  # type: ignore\n    if opset_imports is not None:\n        model.opset_import.extend(opset_imports)\n    else:\n        # Default import\n        imp = model.opset_import.add()\n        imp.version = defs.onnx_opset_version()\n\n    for k, v in kwargs.items():\n        # TODO: Does this work with repeated fields?\n        setattr(model, k, v)\n    return model\n\n\ndef set_model_props(model, dict_value):  # type: (ModelProto, Dict[Text, Text]) -> None\n    del model.metadata_props[:]\n    for (k, v) in dict_value.items():\n        entry = model.metadata_props.add()\n        entry.key = k\n        entry.value = v\n        # model.metadata_properties.append(entry)\n\n\ndef split_complex_to_pairs(ca):  # type: (Sequence[np.complex64]) -> Sequence[int]\n    return [(ca[i // 2].real if (i % 2 == 0) else ca[i // 2].imag)\n            for i in range(len(ca) * 2)]\n\n\ndef make_tensor(\n        name,  # type: Text\n        data_type,  # type: int\n        dims,  # type: Sequence[int]\n        vals,  # type: Any\n        raw=False  # type: bool\n):  # type: (...) -> TensorProto\n    \'\'\'\n    Make a TensorProto with specified arguments.  If raw is False, this\n    function will choose the corresponding proto field to store the\n    values based on data_type. If raw is True, use ""raw_data"" proto\n    field to store the values, and values should be of type bytes in\n    this case.\n    \'\'\'\n    tensor = TensorProto()\n    tensor.data_type = data_type\n    tensor.name = name\n\n    if data_type == TensorProto.STRING:\n        assert not raw, ""Can not use raw_data to store string type""\n\n    if (data_type == TensorProto.COMPLEX64\n            or data_type == TensorProto.COMPLEX128):\n        vals = split_complex_to_pairs(vals)\n    if raw:\n        tensor.raw_data = vals\n    else:\n        field = mapping.STORAGE_TENSOR_TYPE_TO_FIELD[\n            mapping.TENSOR_TYPE_TO_STORAGE_TENSOR_TYPE[data_type]]\n        getattr(tensor, field).extend(vals)\n\n    tensor.dims.extend(dims)\n    return tensor\n\n\ndef make_sparse_tensor(\n    values,   # type: TensorProto\n    indices,   # type: TensorProto\n    dims   # type: Sequence[int]\n):  # type: (...) -> SparseTensorProto\n    sparse = SparseTensorProto()\n    sparse.values.CopyFrom(values)\n    sparse.indices.CopyFrom(indices)\n    sparse.dims.extend(dims)\n    return sparse\n\n\ndef _to_bytes_or_false(val):  # type: (Union[Text, bytes]) -> Union[bytes, bool]\n    """"""An internal graph to convert the input to a bytes or to False.\n\n    The criteria for conversion is as follows and should be python 2 and 3\n    compatible:\n    - If val is py2 str or py3 bytes: return bytes\n    - If val is py2 unicode or py3 str: return val.decode(\'utf-8\')\n    - Otherwise, return False\n    """"""\n    if isinstance(val, bytes):\n        return val\n    try:\n        return val.encode(\'utf-8\')\n    except AttributeError:\n        return False\n\n\ndef make_attribute(\n        key,  # type: Text\n        value,  # type: Any\n        doc_string=None  # type: Optional[Text]\n):  # type: (...) -> AttributeProto\n    """"""Makes an AttributeProto based on the value type.""""""\n    attr = AttributeProto()\n    attr.name = key\n    if doc_string:\n        attr.doc_string = doc_string\n\n    is_iterable = isinstance(value, collections.Iterable)\n    bytes_or_false = _to_bytes_or_false(value)\n    # First, singular cases\n    # float\n    if isinstance(value, float):\n        attr.f = value\n        attr.type = AttributeProto.FLOAT\n    # integer\n    elif isinstance(value, numbers.Integral):\n        attr.i = cast(int, value)\n        attr.type = AttributeProto.INT\n    # string\n    elif bytes_or_false is not False:\n        assert isinstance(bytes_or_false, bytes)\n        attr.s = bytes_or_false\n        attr.type = AttributeProto.STRING\n    elif isinstance(value, TensorProto):\n        attr.t.CopyFrom(value)\n        attr.type = AttributeProto.TENSOR\n    elif isinstance(value, SparseTensorProto):\n        attr.sparse_tensor.CopyFrom(value)\n        attr.type = AttributeProto.SPARSE_TENSOR\n    elif isinstance(value, GraphProto):\n        attr.g.CopyFrom(value)\n        attr.type = AttributeProto.GRAPH\n    # third, iterable cases\n    elif is_iterable:\n        byte_array = [_to_bytes_or_false(v) for v in value]\n        if all(isinstance(v, float) for v in value):\n            attr.floats.extend(value)\n            attr.type = AttributeProto.FLOATS\n        elif all(isinstance(v, numbers.Integral) for v in value):\n            # Turn np.int32/64 into Python built-in int.\n            attr.ints.extend(int(v) for v in value)\n            attr.type = AttributeProto.INTS\n        elif all(map(lambda bytes_or_false: bytes_or_false is not False, byte_array)):\n            attr.strings.extend(cast(List[bytes], byte_array))\n            attr.type = AttributeProto.STRINGS\n        elif all(isinstance(v, TensorProto) for v in value):\n            attr.tensors.extend(value)\n            attr.type = AttributeProto.TENSORS\n        elif all(isinstance(v, SparseTensorProto) for v in value):\n            attr.sparse_tensors.extend(value)\n            attr.type = AttributeProto.SPARSE_TENSORS\n        elif all(isinstance(v, GraphProto) for v in value):\n            attr.graphs.extend(value)\n            attr.type = AttributeProto.GRAPHS\n        else:\n            raise ValueError(\n                ""You passed in an iterable attribute but I cannot figure out ""\n                ""its applicable type."")\n    else:\n        raise TypeError(\n            \'value ""{}"" is not valid attribute data type.\'.format(value))\n    return attr\n\n\ndef get_attribute_value(attr):  # type: (AttributeProto) -> Any\n    if attr.type == AttributeProto.FLOAT:\n        return attr.f\n    if attr.type == AttributeProto.INT:\n        return attr.i\n    if attr.type == AttributeProto.STRING:\n        return attr.s\n    if attr.type == AttributeProto.TENSOR:\n        return attr.t\n    if attr.type == AttributeProto.GRAPH:\n        return attr.g\n    if attr.type == AttributeProto.FLOATS:\n        return list(attr.floats)\n    if attr.type == AttributeProto.INTS:\n        return list(attr.ints)\n    if attr.type == AttributeProto.STRINGS:\n        return list(attr.strings)\n    if attr.type == AttributeProto.TENSORS:\n        return list(attr.tensors)\n    if attr.type == AttributeProto.GRAPHS:\n        return list(attr.graphs)\n    raise ValueError(""Unsupported ONNX attribute: {}"".format(attr))\n\n\ndef make_empty_tensor_value_info(name):  # type: (Text) -> ValueInfoProto\n    value_info_proto = ValueInfoProto()\n    value_info_proto.name = name\n    return value_info_proto\n\n\ndef make_tensor_value_info(\n        name,  # type: Text\n        elem_type,  # type: int\n        shape,  # type: Optional[Sequence[Union[Text, int]]]\n        doc_string="""",  # type: Text\n        shape_denotation=None,  # type: Optional[List[Text]]\n):  # type: (...) -> ValueInfoProto\n    """"""Makes a ValueInfoProto based on the data type and shape.""""""\n    value_info_proto = ValueInfoProto()\n    value_info_proto.name = name\n    if doc_string:\n        value_info_proto.doc_string = doc_string\n\n    tensor_type_proto = value_info_proto.type.tensor_type\n    tensor_type_proto.elem_type = elem_type\n\n    tensor_shape_proto = tensor_type_proto.shape\n\n    if shape is not None:\n        # You might think this is a no-op (extending a normal Python\n        # list by [] certainly is), but protobuf lists work a little\n        # differently; if a field is never set, it is omitted from the\n        # resulting protobuf; a list that is explicitly set to be\n        # empty will get an (empty) entry in the protobuf. This\n        # difference is visible to our consumers, so make sure we emit\n        # an empty shape!\n        tensor_shape_proto.dim.extend([])\n\n        if shape_denotation:\n            if len(shape_denotation) != len(shape):\n                raise ValueError(\n                    \'Invalid shape_denotation. \'\n                    \'Must be of the same length as shape.\')\n\n        for i, d in enumerate(shape):\n            dim = tensor_shape_proto.dim.add()\n            if d is None:\n                pass\n            elif isinstance(d, integer_types):\n                dim.dim_value = d\n            elif isinstance(d, text_type):\n                dim.dim_param = d\n            else:\n                raise ValueError(\n                    \'Invalid item in shape: {}. \'\n                    \'Needs to of integer_types or text_type.\'.format(d))\n\n            if shape_denotation:\n                dim.denotation = shape_denotation[i]\n\n    return value_info_proto\n\n\ndef make_sequence_value_info(\n        name,  # type: Text\n        elem_type,  # type: int\n        shape,  # type: Optional[Sequence[Union[Text, int]]]\n        doc_string="""",  # type: Text\n        elem_shape_denotation=None,  # type: Optional[List[Text]]\n):  # type: (...) -> ValueInfoProto\n    """"""Makes a ValueInfoProto based on the data type and shape for Sequence.""""""\n    value_info_proto = ValueInfoProto()\n    value_info_proto.name = name\n    if doc_string:\n        value_info_proto.doc_string = doc_string\n\n    sequence_type_proto = value_info_proto.type.sequence_type\n    sequence_type_proto.elem_type.tensor_type.elem_type = elem_type\n\n    tensor_value_info = make_tensor_value_info(name, elem_type, shape, doc_string, elem_shape_denotation)\n\n    if shape is not None:\n        sequence_type_proto.elem_type.tensor_type.shape.CopyFrom(tensor_value_info.type.tensor_type.shape)\n\n    return value_info_proto\n\n\ndef _sanitize_str(s):  # type: (Union[Text, bytes]) -> Text\n    if isinstance(s, text_type):\n        sanitized = s\n    elif isinstance(s, binary_type):\n        sanitized = s.decode(\'utf-8\', errors=\'ignore\')\n    else:\n        sanitized = str(s)\n    if len(sanitized) < 64:\n        return sanitized\n    return sanitized[:64] + \'...<+len=%d>\' % (len(sanitized) - 64)\n\n\ndef printable_attribute(attr, subgraphs=False):  # type: (AttributeProto, bool) -> Union[Text, Tuple[Text, List[GraphProto]]]\n    content = []\n    content.append(attr.name)\n    content.append(""="")\n\n    def str_float(f):  # type: (float) -> Text\n        # NB: Different Python versions print different numbers of trailing\n        # decimals, specifying this explicitly keeps it consistent for all\n        # versions\n        return \'{:.15g}\'.format(f)\n\n    def str_int(i):  # type: (int) -> Text\n        # NB: In Python 2, longs will repr() as \'2L\', which is ugly and\n        # unnecessary.  Explicitly format it to keep it consistent.\n        return \'{:d}\'.format(i)\n\n    def str_str(s):  # type: (Text) -> Text\n        return repr(s)\n\n    _T = TypeVar(\'_T\')  # noqa\n\n    def str_list(str_elem, xs):  # type: (Callable[[_T], Text], Sequence[_T]) -> Text\n        return \'[\' + \', \'.join(map(str_elem, xs)) + \']\'\n\n    # for now, this logic should continue to work as long as we are running on a proto3\n    # implementation. If/when we switch to proto3, we will need to use attr.type\n\n    # To support printing subgraphs, if we find a graph attribute, print out\n    # its name here and pass the graph itself up to the caller for later\n    # printing.\n    graphs = []\n    if attr.HasField(""f""):\n        content.append(str_float(attr.f))\n    elif attr.HasField(""i""):\n        content.append(str_int(attr.i))\n    elif attr.HasField(""s""):\n        # TODO: Bit nervous about Python 2 / Python 3 determinism implications\n        content.append(repr(_sanitize_str(attr.s)))\n    elif attr.HasField(""t""):\n        if len(attr.t.dims) > 0:\n            content.append(""<Tensor>"")\n        else:\n            # special case to print scalars\n            field = STORAGE_TENSOR_TYPE_TO_FIELD[attr.t.data_type]\n            content.append(\'<Scalar Tensor {}>\'.format(str(getattr(attr.t, field))))\n    elif attr.HasField(""g""):\n        content.append(""<graph {}>"".format(attr.g.name))\n        graphs.append(attr.g)\n    elif attr.floats:\n        content.append(str_list(str_float, attr.floats))\n    elif attr.ints:\n        content.append(str_list(str_int, attr.ints))\n    elif attr.strings:\n        # TODO: Bit nervous about Python 2 / Python 3 determinism implications\n        content.append(str(list(map(_sanitize_str, attr.strings))))\n    elif attr.tensors:\n        content.append(""[<Tensor>, ...]"")\n    elif attr.graphs:\n        content.append(\'[\')\n        for i, g in enumerate(attr.graphs):\n            comma = \',\' if i != len(attr.graphs) - 1 else \'\'\n            content.append(\'<graph {}>{}\'.format(g.name, comma))\n        content.append(\']\')\n        graphs.extend(attr.graphs)\n    else:\n        content.append(""<Unknown>"")\n    if subgraphs:\n        return \' \'.join(content), graphs\n    else:\n        return \' \'.join(content)\n\n\ndef printable_dim(dim):  # type: (TensorShapeProto.Dimension) -> Text\n    which = dim.WhichOneof(\'value\')\n    assert which is not None\n    return str(getattr(dim, which))\n\n\ndef printable_type(t):  # type: (TypeProto) -> Text\n    if t.WhichOneof(\'value\') == ""tensor_type"":\n        s = TensorProto.DataType.Name(t.tensor_type.elem_type)\n        if t.tensor_type.HasField(\'shape\'):\n            if len(t.tensor_type.shape.dim):\n                s += str(\', \' + \'x\'.join(map(printable_dim, t.tensor_type.shape.dim)))\n            else:\n                s += str(\', scalar\')\n        return s\n    if t.WhichOneof(\'value\') is None:\n        return """"\n    return \'Unknown type {}\'.format(t.WhichOneof(\'value\'))\n\n\ndef printable_value_info(v):  # type: (ValueInfoProto) -> Text\n    s = \'%{}\'.format(v.name)\n    if v.type:\n        s = \'{}[{}]\'.format(s, printable_type(v.type))\n    return s\n\n\ndef printable_tensor_proto(t):  # type: (TensorProto) -> Text\n    s = \'%{}[\'.format(t.name)\n    s += TensorProto.DataType.Name(t.data_type)\n    if t.dims is not None:\n        if len(t.dims):\n            s += str(\', \' + \'x\'.join(map(str, t.dims)))\n        else:\n            s += str(\', scalar\')\n    s += \']\'\n    return s\n\n\ndef printable_node(node, prefix=\'\', subgraphs=False):  # type: (NodeProto, Text, bool) -> Union[Text, Tuple[Text, List[GraphProto]]]\n    content = []\n    if len(node.output):\n        content.append(\n            \', \'.join([\'%{}\'.format(name) for name in node.output]))\n        content.append(\'=\')\n    # To deal with nested graphs\n    graphs = []  # type: List[GraphProto]\n    printed_attrs = []\n    for attr in node.attribute:\n        if subgraphs:\n            printed_attr, gs = printable_attribute(attr, subgraphs)\n            assert isinstance(gs, list)\n            graphs.extend(gs)\n            printed_attrs.append(printed_attr)\n        else:\n            printed = printable_attribute(attr)\n            assert isinstance(printed, Text)\n            printed_attrs.append(printed)\n    printed_attributes = \', \'.join(sorted(printed_attrs))\n    printed_inputs = \', \'.join([\'%{}\'.format(name) for name in node.input])\n    if node.attribute:\n        content.append(""{}[{}]({})"".format(node.op_type, printed_attributes, printed_inputs))\n    else:\n        content.append(""{}({})"".format(node.op_type, printed_inputs))\n    if subgraphs:\n        return prefix + \' \'.join(content), graphs\n    else:\n        return prefix + \' \'.join(content)\n\n\ndef printable_graph(graph, prefix=\'\'):  # type: (GraphProto, Text) -> Text\n    content = []\n    indent = prefix + \'  \'\n    # header\n    header = [\'graph\', graph.name]\n    initializers = {t.name for t in graph.initializer}\n    if len(graph.input):\n        header.append(""("")\n        in_strs = []  # required inputs\n        in_with_init_strs = []  # optional inputs with initializer providing default value\n        for inp in graph.input:\n            if inp.name not in initializers:\n                in_strs.append(printable_value_info(inp))\n            else:\n                in_with_init_strs.append(printable_value_info(inp))\n        if in_strs:\n            content.append(prefix + \' \'.join(header))\n            header = []\n            for line in in_strs:\n                content.append(prefix + \'  \' + line)\n        header.append("")"")\n\n        if in_with_init_strs:\n            header.append(""optional inputs with matching initializers ("")\n            content.append(prefix + \' \'.join(header))\n            header = []\n            for line in in_with_init_strs:\n                content.append(prefix + \'  \' + line)\n            header.append("")"")\n\n        # from IR 4 onwards an initializer is not required to have a matching graph input\n        # so output the name, type and shape of those as well\n        if len(in_with_init_strs) < len(initializers):\n            graph_inputs = {i.name for i in graph.input}\n            init_strs = [printable_tensor_proto(i) for i in graph.initializer\n                         if i.name not in graph_inputs]\n            header.append(""initializers ("")\n            content.append(prefix + \' \'.join(header))\n            header = []\n            for line in init_strs:\n                content.append(prefix + \'  \' + line)\n            header.append("")"")\n\n    header.append(\'{\')\n    content.append(prefix + \' \'.join(header))\n    graphs = []  # type: List[GraphProto]\n    # body\n    for node in graph.node:\n        pn, gs = printable_node(node, indent, subgraphs=True)\n        assert isinstance(gs, list)\n        content.append(pn)\n        graphs.extend(gs)\n    # tail\n    tail = [\'return\']\n    if len(graph.output):\n        tail.append(\n            \', \'.join([\'%{}\'.format(out.name) for out in graph.output]))\n    content.append(indent + \' \'.join(tail))\n    # closing bracket\n    content.append(prefix + \'}\')\n    for g in graphs:\n        content.append(\'\\n\' + printable_graph(g))\n    return \'\\n\'.join(content)\n\n\ndef strip_doc_string(proto):  # type: (google.protobuf.message.Message) -> None\n    """"""\n    Empties `doc_string` field on any nested protobuf messages\n    """"""\n    assert isinstance(proto, google.protobuf.message.Message)\n    for descriptor in proto.DESCRIPTOR.fields:\n        if descriptor.name == \'doc_string\':\n            proto.ClearField(descriptor.name)\n        elif descriptor.type == descriptor.TYPE_MESSAGE:\n            if descriptor.label == descriptor.LABEL_REPEATED:\n                for x in getattr(proto, descriptor.name):\n                    strip_doc_string(x)\n            elif proto.HasField(descriptor.name):\n                strip_doc_string(getattr(proto, descriptor.name))\n'"
onnx/mapping.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom onnx import TensorProto\nfrom typing import Text, Any\nimport numpy as np  # type: ignore\n\nTENSOR_TYPE_TO_NP_TYPE = {\n    int(TensorProto.FLOAT): np.dtype('float32'),\n    int(TensorProto.UINT8): np.dtype('uint8'),\n    int(TensorProto.INT8): np.dtype('int8'),\n    int(TensorProto.UINT16): np.dtype('uint16'),\n    int(TensorProto.INT16): np.dtype('int16'),\n    int(TensorProto.INT32): np.dtype('int32'),\n    int(TensorProto.INT64): np.dtype('int64'),\n    int(TensorProto.BOOL): np.dtype('bool'),\n    int(TensorProto.FLOAT16): np.dtype('float16'),\n    int(TensorProto.DOUBLE): np.dtype('float64'),\n    int(TensorProto.COMPLEX64): np.dtype('complex64'),\n    int(TensorProto.COMPLEX128): np.dtype('complex128'),\n    int(TensorProto.UINT32): np.dtype('uint32'),\n    int(TensorProto.UINT64): np.dtype('uint64'),\n    int(TensorProto.STRING): np.dtype(np.object)\n}\n\nNP_TYPE_TO_TENSOR_TYPE = {v: k for k, v in TENSOR_TYPE_TO_NP_TYPE.items()}\n\nTENSOR_TYPE_TO_STORAGE_TENSOR_TYPE = {\n    int(TensorProto.FLOAT): int(TensorProto.FLOAT),\n    int(TensorProto.UINT8): int(TensorProto.INT32),\n    int(TensorProto.INT8): int(TensorProto.INT32),\n    int(TensorProto.UINT16): int(TensorProto.INT32),\n    int(TensorProto.INT16): int(TensorProto.INT32),\n    int(TensorProto.INT32): int(TensorProto.INT32),\n    int(TensorProto.INT64): int(TensorProto.INT64),\n    int(TensorProto.BOOL): int(TensorProto.INT32),\n    int(TensorProto.FLOAT16): int(TensorProto.UINT16),\n    int(TensorProto.BFLOAT16): int(TensorProto.UINT16),\n    int(TensorProto.DOUBLE): int(TensorProto.DOUBLE),\n    int(TensorProto.COMPLEX64): int(TensorProto.FLOAT),\n    int(TensorProto.COMPLEX128): int(TensorProto.DOUBLE),\n    int(TensorProto.UINT32): int(TensorProto.UINT32),\n    int(TensorProto.UINT64): int(TensorProto.UINT64),\n    int(TensorProto.STRING): int(TensorProto.STRING),\n}\n\nSTORAGE_TENSOR_TYPE_TO_FIELD = {\n    int(TensorProto.FLOAT): 'float_data',\n    int(TensorProto.INT32): 'int32_data',\n    int(TensorProto.INT64): 'int64_data',\n    int(TensorProto.UINT16): 'int32_data',\n    int(TensorProto.DOUBLE): 'double_data',\n    int(TensorProto.COMPLEX64): 'float_data',\n    int(TensorProto.COMPLEX128): 'double_data',\n    int(TensorProto.UINT32): 'uint64_data',\n    int(TensorProto.UINT64): 'uint64_data',\n    int(TensorProto.STRING): 'string_data',\n    int(TensorProto.BOOL): 'int32_data',\n}\n"""
onnx/numpy_helper.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport sys\nimport platform\n\nimport numpy as np  # type: ignore\nfrom onnx import TensorProto\nfrom onnx import mapping\nfrom six import text_type, binary_type\nfrom typing import Sequence, Any, Optional, Text, List\n\nif platform.system() != \'AIX\' and sys.byteorder != \'little\':\n    raise RuntimeError(\n        \'Numpy helper for tensor/ndarray is not available on big endian \'\n        \'systems yet.\')\n\n\ndef combine_pairs_to_complex(fa):  # type: (Sequence[int]) -> Sequence[np.complex64]\n    return [complex(fa[i * 2], fa[i * 2 + 1]) for i in range(len(fa) // 2)]\n\n\ndef to_array(tensor):  # type: (TensorProto) -> np.ndarray[Any]\n    """"""Converts a tensor def object to a numpy array.\n\n    Inputs:\n        tensor: a TensorProto object.\n    Returns:\n        arr: the converted array.\n    """"""\n    if tensor.HasField(""segment""):\n        raise ValueError(\n            ""Currently not supporting loading segments."")\n    if tensor.data_type == TensorProto.UNDEFINED:\n        raise TypeError(""The element type in the input tensor is not defined."")\n\n    tensor_dtype = tensor.data_type\n    np_dtype = mapping.TENSOR_TYPE_TO_NP_TYPE[tensor_dtype]\n    storage_type = mapping.TENSOR_TYPE_TO_STORAGE_TENSOR_TYPE[tensor_dtype]\n    storage_np_dtype = mapping.TENSOR_TYPE_TO_NP_TYPE[storage_type]\n    storage_field = mapping.STORAGE_TENSOR_TYPE_TO_FIELD[storage_type]\n    dims = tensor.dims\n\n    if tensor.data_type == TensorProto.STRING:\n        utf8_strings = getattr(tensor, storage_field)\n        ss = list(s.decode(\'utf-8\') for s in utf8_strings)\n        return np.asarray(ss).astype(np_dtype).reshape(dims)\n\n    if tensor.HasField(""raw_data""):\n        # Raw_bytes support: using frombuffer.\n        return np.frombuffer(\n            tensor.raw_data,\n            dtype=np_dtype).reshape(dims)\n    else:\n        data = getattr(tensor, storage_field),  # type: Sequence[np.complex64]\n        if (tensor_dtype == TensorProto.COMPLEX64\n                or tensor_dtype == TensorProto.COMPLEX128):\n            data = combine_pairs_to_complex(data)\n        return (\n            np.asarray(\n                data,\n                dtype=storage_np_dtype)\n            .astype(np_dtype)\n            .reshape(dims)\n        )\n\n\ndef from_array(arr, name=None):  # type: (np.ndarray[Any], Optional[Text]) -> TensorProto\n    """"""Converts a numpy array to a tensor def.\n\n    Inputs:\n        arr: a numpy array.\n        name: (optional) the name of the tensor.\n    Returns:\n        tensor_def: the converted tensor def.\n    """"""\n    tensor = TensorProto()\n    tensor.dims.extend(arr.shape)\n    if name:\n        tensor.name = name\n\n    if arr.dtype == np.object:\n        # Special care for strings.\n        tensor.data_type = mapping.NP_TYPE_TO_TENSOR_TYPE[arr.dtype]\n        # TODO: Introduce full string support.\n        # We flatten the array in case there are 2-D arrays are specified\n        # We throw the error below if we have a 3-D array or some kind of other\n        # object. If you want more complex shapes then follow the below instructions.\n        # Unlike other types where the shape is automatically inferred from\n        # nested arrays of values, the only reliable way now to feed strings\n        # is to put them into a flat array then specify type astype(np.object)\n        # (otherwise all strings may have different types depending on their length)\n        # and then specify shape .reshape([x, y, z])\n        flat_array = arr.flatten()\n        for e in flat_array:\n            if isinstance(e, text_type):\n                tensor.string_data.append(e.encode(\'utf-8\'))\n            elif isinstance(e, np.ndarray):\n                for s in e:\n                    if isinstance(s, text_type):\n                        tensor.string_data.append(s.encode(\'utf-8\'))\n            else:\n                raise NotImplementedError(\n                    ""Unrecognized object in the object array, expect a string, or array of bytes: "", str(type(e)))\n        return tensor\n\n    # For numerical types, directly use numpy raw bytes.\n    try:\n        dtype = mapping.NP_TYPE_TO_TENSOR_TYPE[arr.dtype]\n    except KeyError:\n        raise RuntimeError(\n            ""Numpy data type not understood yet: {}"".format(str(arr.dtype)))\n    tensor.data_type = dtype\n    tensor.raw_data = arr.tobytes()  # note: tobytes() is only after 1.9.\n\n    return tensor\n'"
onnx/optimizer.py,0,"b'# ATTENTION: The code in this file is highly EXPERIMENTAL.\n# Adventurous users should note that the APIs will probably change.\n\n""""""onnx optimizer\n\nThis enables users to optimize their models.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport onnx\nimport onnx.onnx_cpp2py_export.optimizer as C\nfrom onnx import ModelProto\nfrom typing import Text, Sequence, Optional\n\n""""""Apply the optimization on the serialized ModelProto.\n\nArguments:\n    input (ModelProto): model\n    names (list of string): list of optimization names\n\nReturn:\n    return (ModelProto) optimized model\n\nSupported pass names:\n    -- nop\n    -- eliminate_identity\n    -- eliminate_nop_transpose\n    -- eliminate_nop_pad\n    -- eliminate_unused_initializer\n    -- fuse_consecutive_squeezes\n    -- fuse_consecutive_transposes\n    -- fuse_add_bias_into_conv\n    -- fuse_transpose_into_gemm\n""""""\n\nget_available_passes = C.get_available_passes\n\n\ndef optimize(model, passes=None, fixed_point=False):  # type: (ModelProto, Optional[Sequence[Text]], bool) -> ModelProto\n    if passes is None:\n        passes = [\'eliminate_nop_transpose\',\n                  \'eliminate_nop_pad\',\n                  \'fuse_consecutive_transposes\',\n                  \'fuse_transpose_into_gemm\']\n    if not isinstance(model, ModelProto):\n        raise ValueError(\'Optimizer only accepts ModelProto, incorrect type: {}\'.format(type(model)))\n\n    model_str = model.SerializeToString()\n    if fixed_point:\n        optimized_model_str = C.optimize_fixedpoint(model_str, passes)\n    else:\n        optimized_model_str = C.optimize(model_str, passes)\n\n    return onnx.load_from_string(optimized_model_str)\n'"
onnx/shape_inference.py,0,"b'""""""onnx shape inference. Shape inference is not guaranteed to be\ncomplete.\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport onnx\nimport onnx.onnx_cpp2py_export.shape_inference as C\nfrom onnx import ModelProto\n\n""""""Apply shape inference to the provided ModelProto.\n\nInferred shapes are added to the value_info field of the graph.\n\nIf the inferred values conflict with values already provided in the\ngraph, that means that the provided values are invalid (or there is a\nbug in shape inference), and the result is unspecified.\n\nArguments:\n    input (ModelProto,bool): ModelProto\n\nReturn:\n    return (ModelProto) model with inferred shape information\n""""""\n\n\ndef infer_shapes(model, check_type=False):  # type: (ModelProto,bool) -> ModelProto\n    if not isinstance(model, ModelProto):\n        raise TypeError(\'Shape inference only accepts ModelProto, \'\n                         \'incorrect type: {}\'.format(type(model)))\n    model_str = model.SerializeToString()\n    inferred_model_str = C.infer_shapes(model_str, check_type)\n    return onnx.load_from_string(inferred_model_str)\n'"
onnx/utils.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport onnx.checker\nimport onnx.helper\nimport onnx.optimizer\nimport onnx.shape_inference\n\nfrom onnx import ModelProto\n\n\ndef polish_model(model):  # type: (ModelProto) -> ModelProto\n    '''\n        This function combines several useful utility functions together.\n    '''\n    onnx.checker.check_model(model)\n    onnx.helper.strip_doc_string(model)\n    model = onnx.shape_inference.infer_shapes(model)\n    model = onnx.optimizer.optimize(model)\n    onnx.checker.check_model(model)\n    return model\n"""
onnx/version_converter.py,0,"b'""""""onnx version converter\n\nThis enables users to convert their models between different opsets within the\ndefault domain ("""" or ""ai.onnx"").\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport onnx\nimport onnx.onnx_cpp2py_export.version_converter as C\nfrom onnx import ModelProto\nfrom typing import Text, Sequence\n\n""""""Apply the version conversion on the serialized ModelProto.\n\nArguments:\n    input (ModelProto): model\n    target_version (int): target opset version\n\nReturn:\n    return (ModelProto) converted model\n\nRaises Exceptions:\n    RuntimeError when some necessary conversion is not supported\n\nSupported adapters:\n    --Add from Opset 7 to Opset 6\n    --Add from Opset 6 to Opset 5\n    --Add from Opset 6 to Opset 7\n    --Add from Opset 5 to Opset 6\n    --Mul from Opset 6 to Opset 7\n    --Mul from Opset 7 to Opset 6\n    --Mul from Opset 6 to Opset 5\n    --Mul from Opset 5 to Opset 6\n    --Gemm from Opset 7 to Opset 6\n    --Gemm from Opset 6 to Opset 5\n    --Gemm from Opset 6 to Opset 7\n    --Gemm from Opset 5 to Opset 6\n    --Relu from Opset 6 to Opset 5\n    --Relu from Opset 5 to Opset 6\n    --BatchNorm from Opset 7 to Opset 6\n    --BatchNorm from Opset 6 to Opset 7\n    --BatchNorm from Opset 6 to Opset 5\n    --BatchNorm from Opset 5 to Opset 6\n    --Concat from Opset 4 to Opset 3\n    --Concat from Opset 3 to Opset 4\n    --Reshape from Opset 5 to Opset 4\n    --Reshape from Opset 4 to Opset 5\n    --Sum from Opset 7 to Opset 8\n    --Sum from Opset 8 to Opset 7\n    --Sum from Opset 6 to Opset 5\n    --Sum from Opset 5 to Opset 6\n    --MaxPool from Opset 8 to Opset 7\n    --MaxPool from Opset 7 to Opset 8\n    --AveragePool from Opset 7 to Opset 6\n    --AveragePool from Opset 6 to Opset 7\n    --Dropout from Opset 7 to Opset 6\n    --Dropout from Opset 6 to Opset 5\n    --Dropout from Opset 6 to Opset 7\n    --Dropout from Opset 5 to Opset 6\n\nUnsupported adapters:\n    --Min from Opset 8 to Opset 7\n    --Min from Opset 7 to Opset 8\n    --Min from Opset 6 to Opset 5\n    --Min from Opset 5 to Opset 6\n    --Mean from Opset 8 to Opset 7\n    --Mean from Opset 7 to Opset 8\n    --Mean from Opset 6 to Opset 5\n    --Mean from Opset 5 to Opset 6\n    --Max from Opset 8 to Opset 7\n    --Max from Opset 7 to Opset 8\n    --Max from Opset 6 to Opset 5\n    --Max from Opset 5 to Opset 6\n    --Xor from Opset 6 to Opset 7\n    --Xor from Opset 7 to Opset 6\n    --Upsample from Opset 6 to Opset 7\n    --Upsample from Opset 7 to Opset 6\n    --Sub from Opset 6 to Opset 7\n    --Sub from Opset 7 to Opset 6\n    --Sub from Opset 6 to Opset 5\n    --Sub from Opset 5 to Opset 6\n    --RNN from Opset 6 to Opset 7\n    --RNN from Opset 7 to Opset 6\n    --Pow from Opset 6 to Opset 7\n    --Pow from Opset 7 to Opset 6\n    --PRelu from Opset 6 to Opset 7\n    --PRelu from Opset 7 to Opset 6\n    --PRelu from Opset 6 to Opset 5\n    --PRelu from Opset 5 to Opset 6\n    --Or from Opset 6 to Opset 7\n    --Or from Opset 7 to Opset 6\n    --Less from Opset 6 to Opset 7\n    --Less from Opset 7 to Opset 6\n    --LSTM from Opset 6 to Opset 7\n    --LSTM from Opset 7 to Opset 6\n    --Greater from Opset 6 to Opset 7\n    --Greater from Opset 7 to Opset 6\n    --GRU from Opset 6 to Opset 7\n    --GRU from Opset 7 to Opset 6\n    --GRU from Opset 3 to Opset 2\n    --GRU from Opset 2 to Opset 3\n    --Equal from Opset 6 to Opset 7\n    --Equal from Opset 7 to Opset 6\n    --Div from Opset 6 to Opset 7\n    --Div from Opset 7 to Opset 6\n    --Div from Opset 6 to Opset 5\n    --Div from Opset 5 to Opset 6\n    --And from Opset 6 to Opset 7\n    --And from Opset 7 to Opset 6\n    --And from Opset 6 to Opset 5\n    --And from Opset 5 to Opset 6\n    --Tile from Opset 6 to Opset 5\n    --Tile from Opset 5 to Opset 6\n    --Sqrt from Opset 6 to Opset 5\n    --Sqrt from Opset 5 to Opset 6\n    --Sigmoid from opset 6 to opset 5\n    --Sigmoid from opset 5 to opset 6\n    --Selu from opset 6 to opset 5\n    --Selu from opset 5 to opset 6\n    --Reciprocal from opset 6 to opset 5\n    --Reciprocal from opset 5 to opset 6\n    --Neg from opset 6 to opset 5\n    --Neg from opset 5 to opset 6\n    --Log from opset 6 to opset 5\n    --Log from opset 5 to opset 6\n    --LeakyRelu from opset 6 to opset 5\n    --LeakyRelu from opset 5 to opset 6\n    --InstanceNormalization from opset 6 to opset 5\n    --InstanceNormalization from opset 5 to opset 6\n    --HardSigmoid from opset 6 to opset 5\n    --HardSigmoid from opset 5 to opset 6\n    --Floor from opset 6 to opset 5\n    --Floor from opset 5 to opset 6\n    --Exp from opset 6 to opset 5\n    --Exp from opset 5 to opset 6\n    --Elu from opset 6 to opset 5\n    --Elu from opset 5 to opset 6\n    --Clip from opset 6 to opset 5\n    --Clip from opset 5 to opset 6\n    --Ceil from opset 6 to opset 5\n    --Ceil from opset 5 to opset 6\n    --Cast from opset 6 to opset 5\n    --Cast from opset 5 to opset 6\n    --Abs from opset 6 to opset 5\n    --Abs from opset 5 to opset 6\n    --Split from opset 2 to opset 1\n    --Split from opset 1 to opset 2\n    --Pad from opset 2 to opset 1\n    --Pad from opset 1 to opset 2\n    --LpPool from opset 2 to opset 1\n    --LpPool from opset 1 to opset 2\n    --GlobalLpPool from opset 2 to opset 1\n    --GlobalLpPool from opset 1 to opset 2\n""""""\n\n\ndef convert_version(model, target_version):  # type: (ModelProto, int) -> ModelProto\n    if not isinstance(model, ModelProto):\n        raise ValueError(\'VersionConverter only accepts ModelProto as model, incorrect type: {}\'.format(type(model)))\n    if not isinstance(target_version, int):\n        raise ValueError(\'VersionConverter only accepts int as target_version, incorrect type: {}\'.format(type(target_version)))\n    model_str = model.SerializeToString()\n    converted_model_str = C.convert_version(model_str, target_version)\n    return onnx.load_from_string(converted_model_str)\n'"
tools/gen_coverage_report.py,0,"b""#!/usr/bin/env python\n\nimport argparse\nimport os\nimport subprocess\nimport tempfile\n\nMYPY = False\nif MYPY:\n    from typing import Text\n\n\ndef parse_args():  # type: () -> argparse.Namespace\n    parser = argparse.ArgumentParser(os.path.basename(__file__))\n    parser.add_argument('-r', '--root',\n                        default=os.path.dirname(\n                            os.path.dirname(os.path.abspath(__file__))),\n                        help='onnx root directory (default: %(default)s)')\n    parser.add_argument('-o', '--out', required=True,\n                        help='output directory')\n    return parser.parse_args()\n\n\ndef gen_trace_file(root_dir, out_path):  # type: (Text, Text) -> None\n    subprocess.check_output([\n        'lcov',\n        '-c',\n        '-d',\n        root_dir,\n        '--no-external',\n        '--path',\n        root_dir,\n        '-o',\n        out_path])\n\n    subprocess.check_output([\n        'lcov',\n        '-r',\n        out_path,\n        os.path.join(root_dir, 'third_party', '*'),\n        '-o',\n        out_path])\n\n    subprocess.check_output([\n        'lcov',\n        '-r',\n        out_path,\n        os.path.join(root_dir, '.setuptools-cmake-build', '*'),\n        '-o',\n        out_path\n    ])\n\n\ndef gen_html_files(root_dir, trace_path, out_dir):  # type: (Text, Text, Text) -> None\n    subprocess.check_output([\n        'genhtml',\n        trace_path,\n        '-p',\n        root_dir,\n        '-o',\n        out_dir,\n    ])\n\n\ndef main():  # type: () -> None\n    args = parse_args()\n\n    root = os.path.abspath(args.root)\n    out = os.path.abspath(args.out)\n    if not os.path.exists(out):\n        os.makedirs(out)\n\n    trace_path = os.path.join(out, 'onnx-coverage.info')\n    gen_trace_file(root, trace_path)\n\n    html_dir = os.path.join(out, 'html')\n    gen_html_files(root, trace_path, html_dir)\n\n    print('Static HTML files have been generated at:\\n\\t{}'.format(html_dir))\n\n\nif __name__ == '__main__':\n    main()\n"""
tools/mypy-onnx.py,0,"b'#!/usr/bin/env python\n\nimport subprocess\nimport os\n\n\ndef main():  # type: () -> None\n    try:\n        root_folder = os.path.realpath(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n        os.chdir(root_folder)\n\n        subprocess.check_call([""mypy"", "".""])\n        subprocess.check_call([""mypy"", ""--py2"", "".""])\n\n        exit(0)\n    except subprocess.CalledProcessError:\n        # Catch this exception because we don\'t want it to output a backtrace that would clutter the mypy output\n        exit(1)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tools/protoc-gen-mypy.py,0,"b'#!/usr/bin/env python\n\n# Taken from https://github.com/dropbox/mypy-protobuf/blob/d984389124eae6dbbb517f766b9266bb32171510/python/protoc-gen-mypy\n# (Apache 2.0 License)\n# with own fixes to\n# - appease flake8\n# - exit without error when protobuf isn\'t installed\n# - fix recognition of whether an identifier is defined locally\n#   (unfortunately, we use a python package name ONNX_NAMESPACE_FOO_BAR_FOR_CI\n#    on CI, which by the original protoc-gen-mypy script was recognized to be\n#    camel case and therefore handled as an entry in the local package)\n\n\n""""""Protoc Plugin to generate mypy stubs. Loosely based on @zbarsky\'s go implementation""""""\nfrom __future__ import (\n    absolute_import,\n    division,\n    print_function,\n)\n\nimport sys\nfrom collections import defaultdict\nfrom contextlib import contextmanager\n\ntry:\n    import google.protobuf.descriptor_pb2 as d_typed\n    import six\n    from google.protobuf.compiler import plugin_pb2 as plugin\nexcept ImportError as e:\n    sys.stderr.write(\'Failed to generate mypy stubs: {}\\n\'.format(e))\n    sys.exit(0)\n\nMYPY = False\nif MYPY:\n    from typing import (\n        Any,\n        Callable,\n        Dict,\n        Generator,\n        List,\n        Set,\n        Text,\n        cast,\n    )\nelse:\n    # Provide minimal mypy identifiers to make code run without typing module present\n    Text = None\n\n    def cast(type, value):\n        return value\n\n\n# Hax to get around fact that google protobuf libraries aren\'t in typeshed yet\nd = d_typed  # type: Any\n\nGENERATED = ""@ge"" + ""nerated""  # So phabricator doesn\'t think this file is generated\nHEADER = ""# {} by generate_proto_mypy_stubs.py.  Do not edit!\\n"".format(GENERATED)\n\n\nclass PkgWriter(object):\n    """"""Writes a single pyi file""""""\n\n    def __init__(self, fd, descriptors):\n        # type: (d.FileDescriptorProto, Descriptors) -> None\n        self.fd = fd\n        self.descriptors = descriptors\n        self.lines = []  # type: List[Text]\n        self.indent = """"\n\n        # dictionary of x->y for `from {x} import {y}`\n        self.imports = defaultdict(set)  # type: Dict[Text, Set[Text]]\n        self.locals = set()  # type: Set[Text]\n\n    def _import(self, path, name):\n        # type: (Text, Text) -> Text\n        """"""Imports a stdlib path and returns a handle to it\n        eg. self._import(""typing"", ""Optional"") -> ""Optional""\n        """"""\n        imp = path.replace(\'/\', \'.\')\n        self.imports[imp].add(name)\n        return name\n\n    def _import_message(self, type_name):\n        # type: (d.FieldDescriptorProto) -> Text\n        """"""Import a referenced message and return a handle""""""\n        name = cast(Text, type_name)\n\n        if name[0] == \'.\' and name[1].isupper() and name[2].islower():\n            # Message defined in this file\n            return name[1:]\n\n        message_fd = self.descriptors.message_to_fd[name]\n        if message_fd.name == self.fd.name:\n            # message defined in this package\n            split = name.split(\'.\')\n            for i, segment in enumerate(split):\n                if segment and segment[0].isupper() and segment[1].islower():\n                    return ""."".join(split[i:])\n\n        # Not in package. Must import\n        split = name.split(""."")\n        for i, segment in enumerate(split):\n            if segment and segment[0].isupper() and segment[1].islower():\n                assert message_fd.name.endswith(\'.proto\')\n                import_name = self._import(message_fd.name[:-6].replace(\'-\', \'_\') + ""_pb2"", segment)\n                remains = ""."".join(split[i + 1:])\n                if not remains:\n                    return import_name\n                raise AssertionError(""Don\'t support nested imports yet"")\n                # return new_nested_import(import_name, remains)\n\n        raise AssertionError(""Could not parse local name "" + name)\n\n    @contextmanager  # type: ignore\n    def _indent(self):\n        # type: () -> Generator\n        self.indent = self.indent + ""    ""\n        yield\n        self.indent = self.indent[:-4]\n\n    def _write_line(self, line, *args):\n        # type: (Text, *Text) -> None\n        self.lines.append(self.indent + line.format(*args))\n\n    def write_enums(self, enums):\n        # type: (List[d.EnumDescriptorProto]) -> None\n        line = self._write_line\n        for enum in enums:\n            line(""class {}(int):"", enum.name)\n            with self._indent():\n                line(""@classmethod"")\n                line(""def Name(cls, number: int) -> str: ..."")\n                line(""@classmethod"")\n                line(""def Value(cls, name: str) -> int: ..."")\n                line(""@classmethod"")\n                line(""def keys(cls) -> {}[str]: ..."",\n                    self._import(""typing"", ""List""))\n                line(""@classmethod"")\n                line(""def values(cls) -> {}[int]: ..."",\n                    self._import(""typing"", ""List""))\n                line(""@classmethod"")\n                line(""def items(cls) -> {}[{}[str, int]]: ..."",\n                    self._import(""typing"", ""List""),\n                    self._import(""typing"", ""Tuple""))\n\n            for val in enum.value:\n                line(""{} = {}({}, {})"", val.name, self._import(""typing"", ""cast""), enum.name, val.number)\n            line("""")\n\n    def write_messages(self, messages, prefix):\n        # type: (List[d.DescriptorProto], Text) -> None\n        line = self._write_line\n        message_class = self._import(""google.protobuf.message"", ""Message"")\n\n        for desc in messages:\n            self.locals.add(desc.name)\n            qualified_name = prefix + desc.name\n            line(""class {}({}):"", desc.name, message_class)\n            with self._indent():\n                # Nested enums/messages\n                self.write_enums(desc.enum_type)\n                self.write_messages(desc.nested_type, qualified_name + ""."")\n\n                # Scalar fields\n                for field in [f for f in desc.field if is_scalar(f)]:\n                    if field.label == d.FieldDescriptorProto.LABEL_REPEATED:\n                        container = self._import(""google.protobuf.internal.containers"", ""RepeatedScalarFieldContainer"")\n                        line(""{} = ... # type: {}[{}]"", field.name, container, self.python_type(field))\n                    else:\n                        line(""{} = ... # type: {}"", field.name, self.python_type(field))\n                line("""")\n\n                # Getters for non-scalar fields\n                for field in [f for f in desc.field if not is_scalar(f)]:\n                    line(""@property"")\n                    if field.label == d.FieldDescriptorProto.LABEL_REPEATED:\n                        msg = self.descriptors.messages[field.type_name]\n                        if msg.options.map_entry:\n                            # map generates a special Entry wrapper message\n                            container = self._import(""typing"", ""MutableMapping"")\n                            line(""def {}(self) -> {}[{}, {}]: ..."", field.name, container, self.python_type(msg.field[0]), self.python_type(msg.field[1]))\n                        else:\n                            container = self._import(""google.protobuf.internal.containers"", ""RepeatedCompositeFieldContainer"")\n                            line(""def {}(self) -> {}[{}]: ..."", field.name, container, self.python_type(field))\n                    else:\n                        line(""def {}(self) -> {}: ..."", field.name, self.python_type(field))\n                    line("""")\n\n                # Constructor\n                line(""def __init__(self,"")\n                with self._indent():\n                    # Required args\n                    for field in [f for f in desc.field if f.label == d.FieldDescriptorProto.LABEL_REQUIRED]:\n                        line(""{} : {},"", field.name, self.python_type(field))\n                    for field in [f for f in desc.field if f.label != d.FieldDescriptorProto.LABEL_REQUIRED]:\n                        if field.label == d.FieldDescriptorProto.LABEL_REPEATED:\n                            if field.type_name != \'\' and self.descriptors.messages[field.type_name].options.map_entry:\n                                msg = self.descriptors.messages[field.type_name]\n                                line(""{} : {}[{}[{}, {}]] = None,"", field.name, self._import(""typing"", ""Optional""),\n                                    self._import(""typing"", ""Mapping""), self.python_type(msg.field[0]), self.python_type(msg.field[1]))\n                            else:\n                                line(""{} : {}[{}[{}]] = None,"", field.name, self._import(""typing"", ""Optional""),\n                                  self._import(""typing"", ""Iterable""), self.python_type(field))\n                        else:\n                            line(""{} : {}[{}] = None,"", field.name, self._import(""typing"", ""Optional""),\n                              self.python_type(field))\n                    line("") -> None: ..."")\n\n                # Standard message methods\n                line(""@classmethod"")\n                line(""def FromString(cls, s: bytes) -> {}: ..."", qualified_name)\n                line(""def MergeFrom(self, other_msg: {}) -> None: ..."", message_class)\n                line(""def CopyFrom(self, other_msg: {}) -> None: ..."", message_class)\n            line("""")\n\n    def write_services(self, services):\n        # type: (d.ServiceDescriptorProto) -> None\n        line = self._write_line\n\n        for service in services:\n            # The service definition interface\n            line(""class {}({}, metaclass={}):"", service.name, self._import(""google.protobuf.service"", ""Service""), self._import(""abc"", ""ABCMeta""))\n            with self._indent():\n                for method in service.method:\n                    line(""@{}"", self._import(""abc"", ""abstractmethod""))\n                    line(""def {}(self,"", method.name)\n                    with self._indent():\n                        line(""rpc_controller: {},"", self._import(""google.protobuf.service"", ""RpcController""))\n                        line(""request: {},"", self._import_message(method.input_type))\n                        line(""done: {}[{}[[{}], None]],"",\n                          self._import(""typing"", ""Optional""),\n                          self._import(""typing"", ""Callable""),\n                          self._import_message(method.output_type))\n                    line("") -> {}[{}]: ..."", self._import(""concurrent.futures"", ""Future""), self._import_message(method.output_type))\n\n            # The stub client\n            line(""class {}({}):"", service.name + ""_Stub"", service.name)\n            with self._indent():\n                line(""def __init__(self, rpc_channel: {}) -> None: ..."",\n                  self._import(""google.protobuf.service"", ""RpcChannel""))\n\n    def python_type(self, field):\n        # type: (d.FieldDescriptorProto) -> Text\n        mapping = {\n            d.FieldDescriptorProto.TYPE_DOUBLE: lambda: ""float"",\n            d.FieldDescriptorProto.TYPE_FLOAT: lambda: ""float"",\n\n            d.FieldDescriptorProto.TYPE_INT64: lambda: ""int"",\n            d.FieldDescriptorProto.TYPE_UINT64: lambda: ""int"",\n            d.FieldDescriptorProto.TYPE_FIXED64: lambda: ""int"",\n            d.FieldDescriptorProto.TYPE_SFIXED64: lambda: ""int"",\n            d.FieldDescriptorProto.TYPE_SINT64: lambda: ""int"",\n            d.FieldDescriptorProto.TYPE_INT32: lambda: ""int"",\n            d.FieldDescriptorProto.TYPE_UINT32: lambda: ""int"",\n            d.FieldDescriptorProto.TYPE_FIXED32: lambda: ""int"",\n            d.FieldDescriptorProto.TYPE_SFIXED32: lambda: ""int"",\n            d.FieldDescriptorProto.TYPE_SINT32: lambda: ""int"",\n\n            d.FieldDescriptorProto.TYPE_BOOL: lambda: ""bool"",\n            d.FieldDescriptorProto.TYPE_STRING: lambda: self._import(""typing"", ""Text""),\n            d.FieldDescriptorProto.TYPE_BYTES: lambda: ""bytes"",\n\n            d.FieldDescriptorProto.TYPE_ENUM: lambda: self._import_message(field.type_name),\n            d.FieldDescriptorProto.TYPE_MESSAGE: lambda: self._import_message(field.type_name),\n            d.FieldDescriptorProto.TYPE_GROUP: lambda: self._import_message(field.type_name),\n        }  # type: Dict[int, Callable[[], Text]]\n\n        assert field.type in mapping, ""Unrecognized type: "" + field.type\n        return mapping[field.type]()\n\n    def write(self):\n        # type: () -> Text\n        imports = []\n        for pkg, items in six.iteritems(self.imports):\n            imports.append(u""from {} import ("".format(pkg))\n            for item in sorted(items):\n                imports.append(u""    {},"".format(item))\n            imports.append(u"")\\n"")\n\n        return ""\\n"".join(imports + self.lines)\n\n\ndef is_scalar(fd):\n    # type: (d.FileDescriptorProto) -> bool\n    return not (\n        fd.type == d.FieldDescriptorProto.TYPE_MESSAGE\n        or fd.type == d.FieldDescriptorProto.TYPE_GROUP\n    )\n\n\ndef generate_mypy_stubs(descriptors, response):\n    # type: (Descriptors, plugin.CodeGeneratorResponse) -> None\n    for name, fd in six.iteritems(descriptors.to_generate):\n        pkg_writer = PkgWriter(fd, descriptors)\n        pkg_writer.write_enums(fd.enum_type)\n        pkg_writer.write_messages(fd.message_type, """")\n        pkg_writer.write_services(fd.service)\n\n        assert name == fd.name\n        assert fd.name.endswith(\'.proto\')\n        output = response.file.add()\n        output.name = fd.name[:-6].replace(\'-\', \'_\') + \'_pb2.pyi\'\n        output.content = HEADER + pkg_writer.write()\n        print(""Writing mypy to"", output.name, file=sys.stderr)\n\n\nclass Descriptors(object):\n\n    def __init__(self, request):\n        # type: (plugin.CodeGeneratorRequest) -> None\n        files = {f.name: f for f in request.proto_file}\n        to_generate = {n: files[n] for n in request.file_to_generate}\n        self.files = files  # type: Dict[Text, d.FileDescriptorProto]\n        self.to_generate = to_generate  # type: Dict[Text, d.FileDescriptorProto]\n        self.messages = {}  # type: Dict[Text, d.DescriptorProto]\n        self.message_to_fd = {}  # type: Dict[Text, d.FileDescriptorProto]\n\n        def _add_enums(enums, prefix, fd):\n            # type: (d.EnumDescriptorProto, d.FileDescriptorProto) -> None\n            for enum in enums:\n                self.message_to_fd[prefix + enum.name] = fd\n\n        def _add_messages(messages, prefix, fd):\n            # type: (d.DescriptorProto, d.FileDescriptorProto) -> None\n            for message in messages:\n                self.messages[prefix + message.name] = message\n                self.message_to_fd[prefix + message.name] = fd\n                sub_prefix = prefix + message.name + "".""\n                _add_messages(message.nested_type, sub_prefix, fd)\n                _add_enums(message.enum_type, sub_prefix, fd)\n\n        for fd in request.proto_file:\n            start_prefix = ""."" + fd.package + "".""\n            _add_messages(fd.message_type, start_prefix, fd)\n            _add_enums(fd.enum_type, start_prefix, fd)\n\n\ndef main():\n    # type: () -> None\n    # Read request message from stdin\n    if six.PY3:\n        data = sys.stdin.buffer.read()\n    else:\n        data = sys.stdin.read()\n\n    # Parse request\n    request = plugin.CodeGeneratorRequest()\n    request.ParseFromString(data)\n\n    # Create response\n    response = plugin.CodeGeneratorResponse()\n\n    # Generate mypy\n    generate_mypy_stubs(Descriptors(request), response)\n\n    # Serialise response message\n    output = response.SerializeToString()\n\n    # Write to stdout\n    if six.PY3:\n        sys.stdout.buffer.write(output)\n    else:\n        sys.stdout.write(output)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
onnx/backend/__init__.py,0,b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n'
onnx/backend/base.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom collections import namedtuple\nfrom typing import Text, Sequence, Any, Type, Tuple, NewType, Optional, Dict\n\nimport six\nimport numpy  # type: ignore\n\nimport onnx.checker\nimport onnx.onnx_cpp2py_export.checker as c_checker\nfrom onnx import ModelProto, NodeProto, IR_VERSION\n\n\nclass DeviceType(object):\n    _Type = NewType(\'_Type\', int)\n    CPU = _Type(0)  # type: _Type\n    CUDA = _Type(1)  # type: _Type\n\n\nclass Device(object):\n    \'\'\'\n    Describes device type and device id\n    syntax: device_type:device_id(optional)\n    example: \'CPU\', \'CUDA\', \'CUDA:1\'\n    \'\'\'\n\n    def __init__(self, device):  # type: (Text) -> None\n        options = device.split(\':\')\n        self.type = getattr(DeviceType, options[0])\n        self.device_id = 0\n        if len(options) > 1:\n            self.device_id = int(options[1])\n\n\ndef namedtupledict(typename, field_names, *args, **kwargs):  # type: (Text, Sequence[Text], *Any, **Any) -> Type[Tuple[Any, ...]]\n    field_names_map = {n: i for i, n in enumerate(field_names)}\n    # Some output names are invalid python identifier, e.g. ""0""\n    kwargs.setdefault(str(\'rename\'), True)\n    data = namedtuple(typename, field_names, *args, **kwargs)  # type: ignore\n\n    def getitem(self, key):  # type: (Any, Any) -> Any\n        if isinstance(key, six.string_types):\n            key = field_names_map[key]\n        return super(type(self), self).__getitem__(key)  # type: ignore\n    data.__getitem__ = getitem\n    return data\n\n\nclass BackendRep(object):\n    def run(self, inputs, **kwargs):  # type: (Any, **Any) -> Tuple[Any, ...]\n        pass\n\n\nclass Backend(object):\n    @classmethod\n    def is_compatible(cls,\n                      model,  # type: ModelProto\n                      device=\'CPU\',  # type: Text\n                      **kwargs  # type: Any\n                      ):  # type: (...) -> bool\n        # Return whether the model is compatible with the backend.\n        return True\n\n    @classmethod\n    def prepare(cls,\n                model,  # type: ModelProto\n                device=\'CPU\',  # type: Text\n                **kwargs  # type: Any\n                ):  # type: (...) -> Optional[BackendRep]\n        # TODO Remove Optional from return type\n        onnx.checker.check_model(model)\n        return None\n\n    @classmethod\n    def run_model(cls,\n                  model,  # type: ModelProto\n                  inputs,  # type: Any\n                  device=\'CPU\',  # type: Text\n                  **kwargs  # type: Any\n                  ):  # type: (...) -> Tuple[Any, ...]\n        backend = cls.prepare(model, device, **kwargs)\n        assert backend is not None\n        return backend.run(inputs)\n\n    @classmethod\n    def run_node(cls,\n                 node,  # type: NodeProto\n                 inputs,  # type: Any\n                 device=\'CPU\',  # type: Text\n                 outputs_info=None,  # type: Optional[Sequence[Tuple[numpy.dtype, Tuple[int, ...]]]]\n                 **kwargs  # type: Dict[Text, Any]\n                 ):  # type: (...) -> Optional[Tuple[Any, ...]]\n        \'\'\'Simple run one operator and return the results.\n        Args:\n            outputs_info: a list of tuples, which contains the element type and\n            shape of each output. First element of the tuple is the dtype, and\n            the second element is the shape. More use case can be found in\n            https://github.com/onnx/onnx/blob/master/onnx/backend/test/runner/__init__.py\n        \'\'\'\n        # TODO Remove Optional from return type\n        if \'opset_version\' in kwargs:\n            special_context = c_checker.CheckerContext()\n            special_context.ir_version = IR_VERSION\n            special_context.opset_imports = {\'\': kwargs[\'opset_version\']}  # type: ignore\n            onnx.checker.check_node(node, special_context)\n        else:\n            onnx.checker.check_node(node)\n        return None\n\n    @classmethod\n    def supports_device(cls, device):  # type: (Text) -> bool\n        """"""\n        Checks whether the backend is compiled with particular device support.\n        In particular it\'s used in the testing suite.\n        """"""\n        return True\n'"
onnx/bin/__init__.py,0,b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n'
onnx/bin/checker.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport argparse\n\nfrom onnx import load, checker, NodeProto\n\n\ndef check_model():  # type: () -> None\n    parser = argparse.ArgumentParser('check-model')\n    parser.add_argument('model_pb', type=argparse.FileType('rb'))\n    args = parser.parse_args()\n\n    model = load(args.model_pb)\n    checker.check_model(model)\n\n\ndef check_node():  # type: () -> None\n    parser = argparse.ArgumentParser('check-node')\n    parser.add_argument('node_pb', type=argparse.FileType('rb'))\n    args = parser.parse_args()\n\n    node = NodeProto()\n    node.ParseFromString(args.node_pb.read())\n    checker.check_node(node)\n"""
onnx/defs/__init__.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom onnx import AttributeProto, FunctionProto\nimport onnx.onnx_cpp2py_export.defs as C\n\nfrom collections import defaultdict\nfrom typing import List, Dict\n\nONNX_DOMAIN = """"\nONNX_ML_DOMAIN = \'ai.onnx.ml\'\nAI_ONNX_PREVIEW_TRAINING_DOMAIN = \'ai.onnx.preview.training\'\n\n\nhas = C.has_schema\nget_schema = C.get_schema\nget_all_schemas = C.get_all_schemas\nget_all_schemas_with_history = C.get_all_schemas_with_history\n\n\ndef onnx_opset_version():  # type: () -> int\n    return C.schema_version_map()[ONNX_DOMAIN][1]\n\n\n@property  # type: ignore\ndef _Function_proto(self):  # type: ignore\n    func_proto = FunctionProto()\n    func_proto.ParseFromString(self._function_body)\n    return func_proto\n\n\nOpSchema = C.OpSchema  # type: ignore\nC.OpSchema.function_body = _Function_proto  # type: ignore\n\n\n@property  # type: ignore\ndef _Attribute_default_value(self):  # type: ignore\n    attr = AttributeProto()\n    attr.ParseFromString(self._default_value)\n    return attr\n\n\nOpSchema.Attribute.default_value = _Attribute_default_value  # type: ignore\n\n\ndef get_function_ops():  # type: () -> List[OpSchema]\n    schemas = C.get_all_schemas()\n    return [schema for schema in schemas if schema.has_function or schema.has_context_dependent_function]  # type: ignore\n'"
onnx/defs/gen_doc.py,0,"b'#!/usr/bin/env python\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom collections import defaultdict\nimport io\nimport os\nimport sys\n\nimport numpy as np  # type: ignore\n\nfrom onnx import defs, FunctionProto, helper, OperatorStatus\nfrom onnx.defs import OpSchema, ONNX_DOMAIN, ONNX_ML_DOMAIN\nfrom onnx.backend.test.case import collect_snippets\nfrom onnx.backend.sample.ops import collect_sample_implementations\nfrom typing import Any, Text, Sequence, Dict, List, Type, Set, Tuple\n\n\nSNIPPETS = collect_snippets()\nSAMPLE_IMPLEMENTATIONS = collect_sample_implementations()\nONNX_ML = not bool(os.getenv(\'ONNX_ML\') == \'0\')\n\n\next = \'-ml.md\' if ONNX_ML else \'.md\'\n\n\ndef display_number(v):  # type: (int) -> Text\n    if defs.OpSchema.is_infinite(v):\n        return \'&#8734;\'\n    return Text(v)\n\n\ndef should_render_domain(domain):  # type: (Text) -> bool\n    if domain == ONNX_ML_DOMAIN and not ONNX_ML:\n        return False\n    if ONNX_ML and domain != ONNX_ML_DOMAIN:\n        return False\n    return True\n\n\ndef format_name_with_domain(domain, schema_name):  # type: (Text, Text) -> Text\n    if domain:\n        return \'{}.{}\'.format(domain, schema_name)\n    return schema_name\n\n\ndef display_attr_type(v):  # type: (OpSchema.AttrType) -> Text\n    assert isinstance(v, OpSchema.AttrType)\n    s = Text(v)\n    s = s[s.rfind(\'.\') + 1:].lower()\n    if s[-1] == \'s\':\n        s = \'list of \' + s\n    return s\n\n\ndef display_domain(domain):  # type: (Text) -> Text\n    if domain:\n        return ""the \'{}\' operator set"".format(domain)\n    return ""the default ONNX operator set""\n\n\ndef display_domain_short(domain):  # type: (Text) -> Text\n    if domain:\n        return domain\n    return \'ai.onnx (default)\'\n\n\ndef display_version_link(name, version):  # type: (Text, int) -> Text\n    changelog_md = \'Changelog\' + ext\n    name_with_ver = \'{}-{}\'.format(name, version)\n    return \'<a href=""{}#{}"">{}</a>\'.format(changelog_md, name_with_ver, name_with_ver)\n\n\ndef generate_formal_parameter_tags(formal_parameter):  # type: (OpSchema.FormalParameter) -> Text\n    tags = []  # type: List[Text]\n    if OpSchema.FormalParameterOption.Optional == formal_parameter.option:\n        tags = [""optional""]\n    elif OpSchema.FormalParameterOption.Variadic == formal_parameter.option:\n        if formal_parameter.isHomogeneous:\n            tags = [""variadic""]\n        else:\n            tags = [""variadic"", ""heterogeneous""]\n    differentiable = OpSchema.DifferentiationCategory.Differentiable  # type: OpSchema.DifferentiationCategory\n    non_differentiable = OpSchema.DifferentiationCategory.NonDifferentiable  # type: OpSchema.DifferentiationCategory\n    if differentiable == formal_parameter.differentiationCategory:\n        tags.append(\'differentiable\')\n    elif non_differentiable == formal_parameter.differentiationCategory:\n        tags.append(\'non-differentiable\')\n\n    return \'\' if len(tags) == 0 else \' (\' + \', \'.join(tags) + \')\'\n\n\ndef display_schema(schema, versions):  # type: (OpSchema, Sequence[OpSchema]) -> Text\n    s = \'\'\n\n    # doc\n    if schema.doc:\n        s += \'\\n\'\n        s += \'\\n\'.join(\'  \' + line\n                       for line in schema.doc.lstrip().splitlines())\n        s += \'\\n\'\n\n    # since version\n    s += \'\\n#### Version\\n\'\n    if schema.support_level == OpSchema.SupportType.EXPERIMENTAL:\n        s += \'\\nNo versioning maintained for experimental ops.\'\n    else:\n        s += \'\\nThis version of the operator has been \' + (\'deprecated\' if schema.deprecated else \'available\') + \' since version {}\'.format(schema.since_version)\n        s += \' of {}.\\n\'.format(display_domain(schema.domain))\n        if len(versions) > 1:\n            # TODO: link to the Changelog.md\n            s += \'\\nOther versions of this operator: {}\\n\'.format(\n                \', \'.join(display_version_link(format_name_with_domain(v.domain, v.name),\n                                               v.since_version) for v in versions[:-1]))\n\n    # If this schema is deprecated, don\'t display any of the following sections\n    if schema.deprecated:\n        return s\n\n    # attributes\n    if schema.attributes:\n        s += \'\\n#### Attributes\\n\\n\'\n        s += \'<dl>\\n\'\n        for _, attr in sorted(schema.attributes.items()):\n            # option holds either required or default value\n            opt = \'\'\n            if attr.required:\n                opt = \'required\'\n            elif attr.default_value.name:\n                default_value = helper.get_attribute_value(attr.default_value)\n\n                def format_value(value):  # type: (Any) -> Text\n                    if isinstance(value, float):\n                        formatted = str(np.round(value, 5))\n                        # use default formatting, unless too long.\n                        if (len(formatted) > 10):\n                            formatted = str(""({:e})"".format(value))\n                        return formatted\n                    elif isinstance(value, (bytes, bytearray)) and sys.version_info[0] == 3:\n                        return str(value.decode(\'utf-8\'))\n                    return str(value)\n\n                if isinstance(default_value, list):\n                    default_value = [format_value(val) for val in default_value]\n                else:\n                    default_value = format_value(default_value)\n                opt = \'default is {}\'.format(default_value)\n\n            s += \'<dt><tt>{}</tt> : {}{}</dt>\\n\'.format(\n                attr.name,\n                display_attr_type(attr.type),\n                \' ({})\'.format(opt) if opt else \'\')\n            s += \'<dd>{}</dd>\\n\'.format(attr.description)\n        s += \'</dl>\\n\'\n\n    # inputs\n    s += \'\\n#### Inputs\'\n    if schema.min_input != schema.max_input:\n        s += \' ({} - {})\'.format(display_number(schema.min_input),\n                                 display_number(schema.max_input))\n    s += \'\\n\\n\'\n    if schema.inputs:\n        s += \'<dl>\\n\'\n        for input in schema.inputs:\n            option_str = generate_formal_parameter_tags(input)\n            s += \'<dt><tt>{}</tt>{} : {}</dt>\\n\'.format(input.name, option_str, input.typeStr)\n            s += \'<dd>{}</dd>\\n\'.format(input.description)\n        s += \'</dl>\\n\'\n\n    # outputs\n    s += \'\\n#### Outputs\'\n    if schema.min_output != schema.max_output:\n        s += \' ({} - {})\'.format(display_number(schema.min_output),\n                                 display_number(schema.max_output))\n    s += \'\\n\\n\'\n\n    if schema.outputs:\n        s += \'<dl>\\n\'\n        for output in schema.outputs:\n            option_str = generate_formal_parameter_tags(output)\n            s += \'<dt><tt>{}</tt>{} : {}</dt>\\n\'.format(output.name, option_str, output.typeStr)\n            s += \'<dd>{}</dd>\\n\'.format(output.description)\n        s += \'</dl>\\n\'\n\n    # type constraints\n    s += \'\\n#### Type Constraints\'\n    s += \'\\n\\n\'\n    if schema.type_constraints:\n        s += \'<dl>\\n\'\n        for type_constraint in schema.type_constraints:\n            allowedTypes = type_constraint.allowed_type_strs\n            if (len(allowedTypes) > 0):\n                allowedTypeStr = allowedTypes[0]\n            for allowedType in allowedTypes[1:]:\n                allowedTypeStr += \', \' + allowedType\n            s += \'<dt><tt>{}</tt> : {}</dt>\\n\'.format(\n                type_constraint.type_param_str, allowedTypeStr)\n            s += \'<dd>{}</dd>\\n\'.format(type_constraint.description)\n        s += \'</dl>\\n\'\n\n    # Function Body\n    # TODO: this should be refactored to show the function body graph\'s picture (DAG).\n    #if schema.has_function or schema.has_context_dependent_function:  # type: ignore\n    #    s += \'\\n#### Function\\n\'\n    #    s += \'\\nThe Function can be represented as a function.\\n\'\n\n    return s\n\n\ndef support_level_str(level):  # type: (OpSchema.SupportType) -> Text\n    return \\\n        ""<sub>experimental</sub> "" if level == OpSchema.SupportType.EXPERIMENTAL else """"\n\n\ndef main(args):  # type: (Type[Args]) -> None\n    with io.open(args.changelog, \'w\', newline=\'\') as fout:\n        fout.write(\'## Operator Changelog\\n\')\n        fout.write(\n            ""*This file is automatically generated from the\\n""\n            ""            [def files](/onnx/defs) via [this script](/onnx/defs/gen_doc.py).\\n""\n            ""            Do not modify directly and instead edit operator definitions.*\\n""\n            ""\\n""\n            ""For an operator input/output\'s differentiability, it can be differentiable,\\n""\n            ""            non-differentiable, or undefined. If a variable\'s differentiability\\n""\n            ""            is not specified, that variable has undefined differentiability.\\n"")\n\n        # domain -> version -> [schema]\n        dv_index = defaultdict(lambda: defaultdict(list))  # type: Dict[Text, Dict[int, List[OpSchema]]]\n        for schema in defs.get_all_schemas_with_history():\n            dv_index[schema.domain][schema.since_version].append(schema)\n\n        fout.write(\'\\n\')\n\n        for domain, versionmap in sorted(dv_index.items()):\n            if not should_render_domain(domain):\n                continue\n\n            s = \'# {}\\n\'.format(display_domain_short(domain))\n\n            for version, unsorted_schemas in sorted(versionmap.items()):\n                s += \'## Version {} of {}\\n\'.format(version, display_domain(domain))\n                for schema in sorted(unsorted_schemas, key=lambda s: s.name):\n                    name_with_ver = \'{}-{}\'.format(format_name_with_domain(domain, schema.name),\n                                                   schema.since_version)\n                    s += (\'### <a name=""{}""></a>**{}**\' + (\' (deprecated)\' if schema.deprecated else \'\') + \'</a>\\n\').format(name_with_ver, name_with_ver)\n                    s += display_schema(schema, [schema])\n                    s += \'\\n\'\n\n            fout.write(s)\n\n    with io.open(args.output, \'w\', newline=\'\', encoding=""utf-8"") as fout:\n        fout.write(\'## Operator Schemas\\n\')\n        fout.write(\n            ""*This file is automatically generated from the\\n""\n            ""            [def files](/onnx/defs) via [this script](/onnx/defs/gen_doc.py).\\n""\n            ""            Do not modify directly and instead edit operator definitions.*\\n""\n            ""\\n""\n            ""For an operator input/output\'s differentiability, it can be differentiable,\\n""\n            ""            non-differentiable, or undefined. If a variable\'s differentiability\\n""\n            ""            is not specified, that variable has undefined differentiability.\\n"")\n\n        # domain -> support level -> name -> [schema]\n        index = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))  # type: Dict[Text, Dict[int, Dict[Text, List[OpSchema]]]]\n        for schema in defs.get_all_schemas_with_history():\n            index[schema.domain][int(schema.support_level)][schema.name].append(schema)\n\n        fout.write(\'\\n\')\n\n        # Preprocess the Operator Schemas\n        # [(domain, [(support_level, [(schema name, current schema, all versions schemas)])])]\n        operator_schemas = list()  # type: List[Tuple[Text, List[Tuple[int, List[Tuple[Text, OpSchema, List[OpSchema]]]]]]]\n        exsting_ops = set()  # type: Set[Text]\n        for domain, _supportmap in sorted(index.items()):\n            if not should_render_domain(domain):\n                continue\n\n            processed_supportmap = list()\n            for _support, _namemap in sorted(_supportmap.items()):\n                processed_namemap = list()\n                for n, unsorted_versions in sorted(_namemap.items()):\n                    versions = sorted(unsorted_versions, key=lambda s: s.since_version)\n                    schema = versions[-1]\n                    if schema.name in exsting_ops:\n                        continue\n                    exsting_ops.add(schema.name)\n                    processed_namemap.append((n, schema, versions))\n                processed_supportmap.append((_support, processed_namemap))\n            operator_schemas.append((domain, processed_supportmap))\n\n        # Table of contents\n        for domain, supportmap in operator_schemas:\n            s = \'* {}\\n\'.format(display_domain_short(domain))\n            fout.write(s)\n            function_ops = list()\n            for _, namemap in supportmap:\n                for n, schema, versions in namemap:\n                    if schema.has_function or schema.has_context_dependent_function:  # type: ignore\n                        function_ops.append((n, schema, versions))\n                        continue\n                    s = \'  * {}<a href=""#{}"">{}</a>\\n\'.format(\n                        support_level_str(schema.support_level),\n                        format_name_with_domain(domain, n),\n                        format_name_with_domain(domain, n))\n                    fout.write(s)\n            if len(function_ops):\n                fout.write(\'\\n\')\n                fout.write(\'  **Functions**\\n\')\n                for n, schema, versions in function_ops:\n                    s = \'  * {}<a href=""#{}"">{}</a>\\n\'.format(\n                        support_level_str(schema.support_level),\n                        format_name_with_domain(domain, n),\n                        format_name_with_domain(domain, n))\n                    fout.write(s)\n\n        fout.write(\'\\n\')\n\n        for domain, supportmap in operator_schemas:\n            s = \'## {}\\n\'.format(display_domain_short(domain))\n            fout.write(s)\n\n            for _, namemap in supportmap:\n                for op_type, schema, versions in namemap:\n                    # op_type\n                    s = (\'### {}<a name=""{}""></a><a name=""{}"">**{}**\' + (\' (deprecated)\' if schema.deprecated else \'\') + \'</a>\\n\').format(\n                        support_level_str(schema.support_level),\n                        format_name_with_domain(domain, op_type),\n                        format_name_with_domain(domain, op_type.lower()),\n                        format_name_with_domain(domain, op_type))\n\n                    s += display_schema(schema, versions)\n\n                    s += \'\\n\\n\'\n\n                    if op_type in SNIPPETS:\n                        s += \'#### Examples\\n\\n\'\n                        for summary, code in sorted(SNIPPETS[op_type]):\n                            s += \'<details>\\n\'\n                            s += \'<summary>{}</summary>\\n\\n\'.format(summary)\n                            s += \'```python\\n{}\\n```\\n\\n\'.format(code)\n                            s += \'</details>\\n\'\n                            s += \'\\n\\n\'\n                    if op_type.lower() in SAMPLE_IMPLEMENTATIONS:\n                        s += \'#### Sample Implementation\\n\\n\'\n                        s += \'<details>\\n\'\n                        s += \'<summary>{}</summary>\\n\\n\'.format(op_type)\n                        s += \'```python\\n{}\\n```\\n\\n\'.format(SAMPLE_IMPLEMENTATIONS[op_type.lower()])\n                        s += \'</details>\\n\'\n                        s += \'\\n\\n\'\n\n                    fout.write(s)\n\n\nif __name__ == \'__main__\':\n    base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))\n    docs_dir = os.path.join(base_dir, \'docs\')\n\n    class Args(object):\n        output = os.path.join(docs_dir, \'Operators\' + ext)\n        changelog = os.path.join(docs_dir, \'Changelog\' + ext)\n    main(Args)\n'"
onnx/defs/gen_shape_inference_information.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom onnx import defs\n\n\ndef main():  # type: () -> None\n    # domain -> support level -> name -> [schema]\n    with_inference = []\n    without_inference = []\n    for schema in defs.get_all_schemas():\n        domain, name, has_inference = schema.domain, schema.name, schema.has_type_and_shape_inference_function\n        elem = (domain, name)\n        if has_inference:\n            with_inference.append(elem)\n        else:\n            without_inference.append(elem)\n    print(len(with_inference), 'operators have a type/shape inference function.')\n    print(len(without_inference), 'do not. These are:')\n    for domain, name in sorted(without_inference):\n        print(domain, name)\n\n\nif __name__ == '__main__':\n    main()\n"""
onnx/frontend/__init__.py,0,b''
onnx/test/basic_test.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom onnx import AttributeProto, NodeProto, GraphProto, ModelProto, TensorProto, IR_VERSION\n\nimport io\nimport onnx\nimport os\nimport tempfile\nimport unittest\n\nfrom onnx import helper\n\n\nclass TestBasicFunctions(unittest.TestCase):\n\n    def _simple_model(self):  # type: () -> ModelProto\n        # Create a ModelProto.\n        model = ModelProto()\n        model.ir_version = IR_VERSION\n        return model\n\n    def _simple_tensor(self):  # type: () -> TensorProto\n        # Create a TensorProto.\n        tensor = helper.make_tensor(\n            name='test-tensor',\n            data_type=TensorProto.FLOAT,\n            dims=(2, 3, 4),\n            vals=[x + 0.5 for x in range(24)]\n        )\n        return tensor\n\n    def test_save_and_load_model(self):  # type: () -> None\n        proto = self._simple_model()\n        cls = ModelProto\n        proto_string = onnx._serialize(proto)\n\n        # Test if input is string\n        loaded_proto = onnx.load_model_from_string(proto_string)\n        self.assertTrue(proto == loaded_proto)\n\n        # Test if input has a read function\n        f = io.BytesIO()\n        onnx.save_model(proto_string, f)\n        f = io.BytesIO(f.getvalue())\n        loaded_proto = onnx.load_model(f, cls)\n        self.assertTrue(proto == loaded_proto)\n\n        # Test if input is a file name\n        try:\n            fi = tempfile.NamedTemporaryFile(delete=False)\n            onnx.save_model(proto, fi)\n            fi.close()\n\n            loaded_proto = onnx.load_model(fi.name, cls)\n            self.assertTrue(proto == loaded_proto)\n        finally:\n            os.remove(fi.name)\n\n    def test_save_and_load_tensor(self):  # type: () -> None\n        proto = self._simple_tensor()\n        cls = TensorProto\n        proto_string = onnx._serialize(proto)\n\n        # Test if input is string\n        loaded_proto = onnx.load_tensor_from_string(proto_string)\n        self.assertTrue(proto == loaded_proto)\n\n        # Test if input has a read function\n        f = io.BytesIO()\n        onnx.save_tensor(loaded_proto, f)\n        f = io.BytesIO(f.getvalue())\n        loaded_proto = onnx.load_tensor(f, cls)\n        self.assertTrue(proto == loaded_proto)\n\n        # Test if input is a file name\n        try:\n            tfile = tempfile.NamedTemporaryFile(delete=False)\n            onnx.save_tensor(proto, tfile)\n            tfile.close()\n\n            loaded_proto = onnx.load_tensor(tfile.name, cls)\n            self.assertTrue(proto == loaded_proto)\n        finally:\n            os.remove(tfile.name)\n\n    def test_existence(self):  # type: () -> None\n        try:\n            AttributeProto\n            NodeProto\n            GraphProto\n            ModelProto\n        except Exception as e:\n            self.fail(\n                'Did not find proper onnx protobufs. Error is: {}'\n                .format(e))\n\n    def test_version_exists(self):  # type: () -> None\n        model = ModelProto()\n        # When we create it, graph should not have a version string.\n        self.assertFalse(model.HasField('ir_version'))\n        # We should touch the version so it is annotated with the current\n        # ir version of the running ONNX\n        model.ir_version = IR_VERSION\n        model_string = model.SerializeToString()\n        model.ParseFromString(model_string)\n        self.assertTrue(model.HasField('ir_version'))\n        # Check if the version is correct.\n        self.assertEqual(model.ir_version, IR_VERSION)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
onnx/test/checker_test.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\nimport unittest\n\nfrom typing import Sequence\nimport numpy as np  # type: ignore\n\nfrom onnx import checker, helper\nfrom onnx import TensorProto, GraphProto, SparseTensorProto\nimport onnx.onnx_cpp2py_export.checker as C\nimport onnx.defs\n\n\nclass TestChecker(unittest.TestCase):\n    @property\n    def _sample_float_tensor(self):  # type: () -> TensorProto\n        np_array = np.random.randn(2, 3).astype(np.float32)\n        return helper.make_tensor(\n            name=\'test\',\n            data_type=TensorProto.FLOAT,\n            dims=(2, 3),\n            vals=np_array.reshape(6).tolist()\n        )\n\n    def test_check_node(self):  # type: () -> None\n        node = helper.make_node(\n            ""Relu"", [""X""], [""Y""], name=""test"")\n\n        checker.check_node(node)\n\n    def test_check_node_input_marked_optional(self):  # type: () -> None\n        # GivenTensorFill\'s input is marked optional, hence it is used in this test.\n        node = helper.make_node(\n            ""GivenTensorFill"", [], [""Y""], name=""test"")\n        checker.check_node(node)\n\n        # Explicitly pass the empty string as optional\n        node = helper.make_node(\n            ""GivenTensorFill"", [""""], [""Y""], name=""test"")\n\n        # Input of RELU is not optional\n        node = helper.make_node(\n            ""Relu"", [""""], [""Y""], name=""test"")\n        self.assertRaises(checker.ValidationError, checker.check_node, node)\n\n    def test_check_graph_ir_version_3(self):  # type: () -> None\n        ctx = C.CheckerContext()\n        ctx.ir_version = 3\n        ctx.opset_imports = {\'\': onnx.defs.onnx_opset_version()}\n\n        def check_ir_version_3(g):   # type: (GraphProto) -> None\n            checker.check_graph(g, ctx)\n\n        node = helper.make_node(\n            ""Relu"", [""X""], [""Y""], name=""test"")\n        graph = helper.make_graph(\n            [node],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, [1, 2])],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, [1, 2])])\n        check_ir_version_3(graph)\n\n        graph.initializer.extend([self._sample_float_tensor])\n\n        graph.initializer[0].name = \'no-exist\'\n\n        self.assertRaises(checker.ValidationError, check_ir_version_3, graph)\n\n        graph.initializer[0].name = \'X\'\n        check_ir_version_3(graph)\n\n    def test_check_graph(self):  # type: () -> None\n        node = helper.make_node(\n            ""Relu"", [""X""], [""Y""], name=""test"")\n        graph = helper.make_graph(\n            [node],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, [1, 2])],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, [1, 2])])\n        checker.check_graph(graph)\n\n        graph.initializer.extend([self._sample_float_tensor])\n\n        graph.initializer[0].name = \'no-exist\'\n        checker.check_graph(graph)\n\n        graph.initializer[0].name = \'X\'\n        checker.check_graph(graph)\n\n    def test_check_graph_optional_input(self):  # type: () -> None\n        # GivenTensorFill\'s input is marked optional, hence it is used in this test.\n        node = helper.make_node(\n            ""GivenTensorFill"", [""""], [""Y""], name=""test"")\n        graph = helper.make_graph(\n            [node],\n            ""test"",\n            [],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, [1, 2])])\n        checker.check_graph(graph)\n\n    def test_check_graph_ssa(self):  # type: () -> None\n        relu1 = helper.make_node(\n            ""Relu"", [""X""], [""Z""], name=""relu1"")\n        relu2 = helper.make_node(\n            ""Relu"", [""Y""], [""Z""], name=""relu2"")\n\n        graph = helper.make_graph(\n            [relu1, relu2],\n            ""test"",\n            inputs=[\n                helper.make_tensor_value_info(""X"", TensorProto.FLOAT, [1, 2]),\n                helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, [1, 2])\n            ],\n            outputs=[\n                helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, [1, 2])\n            ]\n        )\n        self.assertRaises(checker.ValidationError, checker.check_graph, graph)\n\n    def test_check_graph_topologically_sorted(self):  # type: () -> None\n        n1 = helper.make_node(\n            ""Scale"", [""X""], [""Y""], scale=2., name=""n1"")\n        n2 = helper.make_node(\n            ""Scale"", [""Y""], [""Z""], scale=3., name=""n2"")\n\n        graph = helper.make_graph(\n            [n2, n1],\n            ""test"",\n            inputs=[\n                helper.make_tensor_value_info(""X"", TensorProto.FLOAT, [1, 2])\n            ],\n            outputs=[\n                helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, [1, 2])\n            ]\n        )\n        self.assertRaises(checker.ValidationError, checker.check_graph, graph)\n\n    def test_check_model(self):  # type: () -> None\n        node = helper.make_node(\n            ""Relu"", [""X""], [""Y""], name=""test"")\n        graph = helper.make_graph(\n            [node],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, [1, 2])],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, [1, 2])])\n        model = helper.make_model(graph, producer_name=\'test\')\n\n        checker.check_model(model)\n\n    def test_check_old_model(self):  # type: () -> None\n        node = helper.make_node(\n            ""Pad"", [""X""], [""Y""], paddings=(0, 0, 0, 0))\n        graph = helper.make_graph(\n            [node],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, [1, 2])],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, [1, 2])])\n        onnx_id = helper.make_opsetid("""", 1)\n        model = helper.make_model(graph, producer_name=\'test\', opset_imports=[onnx_id])\n\n        checker.check_model(model)\n\n    def test_check_tensor(self):  # type: () -> None\n        tensor = self._sample_float_tensor\n        checker.check_tensor(tensor)\n\n        tensor.raw_data = np.random.randn(2, 3).astype(np.float32).tobytes()\n        self.assertRaises(checker.ValidationError, checker.check_tensor, tensor)\n\n    def test_check_string_tensor(self):  # type: () -> None\n        tensor = TensorProto()\n        tensor.data_type = TensorProto.STRING\n        tensor.dims.append(1)\n        tensor.string_data.append(\'Test\'.encode(\'utf-8\'))\n        checker.check_tensor(tensor)\n\n        del tensor.string_data[:]\n        tensor.raw_data = \'Test\'.encode(\'utf-8\')\n        # string data should not be stored in raw_data field\n        self.assertRaises(checker.ValidationError, checker.check_tensor, tensor)\n\n    def test_check_tensor_mismatched_field(self):  # type: () -> None\n        tensor = self._sample_float_tensor\n        tensor.data_type = TensorProto.INT32\n        self.assertRaises(checker.ValidationError, checker.check_tensor, tensor)\n\n    def test_nested_graph(self):  # type: () -> None\n        n1 = helper.make_node(\n            ""Scale"", [""X""], [""Y""], scale=2., name=""n1"")\n        n2 = helper.make_node(\n            ""Scale"", [""Y""], [""Z""], scale=3., name=""n2"")\n\n        graph = helper.make_graph(\n            [n1, n2],\n            ""nested"",\n            inputs=[\n                helper.make_tensor_value_info(""X"", TensorProto.FLOAT, [1, 2])\n            ],\n            outputs=[\n                helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, [1, 2])\n            ]\n        )\n\n        i1 = helper.make_node(\n            ""If"", [""cond""], [""Z""], then_branch=graph, else_branch=graph)\n\n        graph = helper.make_graph(\n            [i1],\n            ""test"",\n            inputs=[\n                helper.make_tensor_value_info(""cond"", TensorProto.BOOL, [1]),\n                helper.make_tensor_value_info(""X"", TensorProto.FLOAT, [1, 2])\n            ],\n            outputs=[helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, [1, 2])],\n        )\n\n        checker.check_graph(graph)\n        #self.assertRaises(checker.ValidationError, checker.check_graph, graph)\n\n    def test_nested_graph_without_subgraph_input_shape(self):  # type: () -> None\n        n1 = helper.make_node(\n            ""Scale"", [""X""], [""Y""], scale=2., name=""n1"")\n        n2 = helper.make_node(\n            ""Scale"", [""Y""], [""Z""], scale=3., name=""n2"")\n\n        input_x = onnx.ValueInfoProto()\n        input_x.name = ""X""\n        graph = helper.make_graph(\n            [n1, n2],\n            ""nested"",\n            inputs=[\n                input_x\n            ],\n            outputs=[\n                helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, [1, 2])\n            ]\n        )\n\n        i1 = helper.make_node(\n            ""If"", [""cond""], [""Z""], then_branch=graph, else_branch=graph)\n\n        graph = helper.make_graph(\n            [i1],\n            ""test"",\n            inputs=[\n                helper.make_tensor_value_info(""cond"", TensorProto.BOOL, [1]),\n                helper.make_tensor_value_info(""X"", TensorProto.FLOAT, [1, 2])\n            ],\n            outputs=[helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, [1, 2])],\n        )\n\n        checker.check_graph(graph)\n\n    @property\n    def _sample_0_elem_tensor(self):  # type: () -> TensorProto\n        np_array = np.random.randn(0, 3).astype(np.float32)\n        return helper.make_tensor(\n            name=\'test\',\n            data_type=TensorProto.FLOAT,\n            dims=(0, 3),\n            vals=np_array.reshape(0).tolist()\n        )\n\n    def test_check_tensor_zero_elem(self):  # type: () -> None\n        tensor = self._sample_0_elem_tensor\n        checker.check_tensor(tensor)\n\n    def test_check_removed_experimental_op(self):  # type: () -> None\n        node = helper.make_node(\n            ""ConstantFill"", [], [""Y""], name=""test"", shape=[1, 2])\n        checker.check_node(node)\n\n    def test_skip_schema_check_on_non_standard_domain(self):  # type: () -> None\n        node = helper.make_node(\n            ""NonExistOp"", [""X""], [""Y""], name=""test"", domain=""test.domain"")\n        graph = helper.make_graph(\n            [node],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, [1, 2])],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, [1, 2])])\n        onnx_id = helper.make_opsetid(""test.domain"", 1)\n        model = helper.make_model(graph, producer_name=\'test\',\n                                  opset_imports=[onnx_id])\n        checker.check_model(model)\n\n    def make_sparse(self,\n                    shape,  # type: Sequence[int]\n                    values,  # type: Sequence[int]\n                    indices_shape,  # type: Sequence[int]\n                    indices  # type: Sequence[int]\n                    ):  # type: (...) -> SparseTensorProto\n        sparse = SparseTensorProto()\n        sparse.dims.extend(shape)\n        nnz = len(values)\n        sparse.values.CopyFrom(helper.make_tensor(\'spval\', TensorProto.INT64, (nnz,), values))\n        sparse.indices.CopyFrom(helper.make_tensor(\'spind\', TensorProto.INT64, indices_shape, indices))\n        return sparse\n\n    def test_check_sparse_tensor(self):  # type: () -> None\n        sparse = self.make_sparse([100], [13, 17, 19], [3], [9, 27, 81])\n        checker.check_sparse_tensor(sparse)\n\n    def test_check_sparse_tensor_invalid_index(self):  # type: () -> None\n        # index value 181 is out-of-range\n        sparse = self.make_sparse([100], [13, 17, 19], [3], [9, 27, 181])\n        self.assertRaises(checker.ValidationError, checker.check_sparse_tensor, sparse)\n\n    def test_check_sparse_tensor_unordered(self):  # type: () -> None\n        # index values are not in sorted order\n        sparse = self.make_sparse([100], [13, 17, 19], [3], [27, 9, 81])\n        self.assertRaises(checker.ValidationError, checker.check_sparse_tensor, sparse)\n\n    def test_check_sparse_tensor_coo_format(self):  # type: () -> None\n        sparse = self.make_sparse([10, 10], [13, 17, 19], [3, 2], [0, 9, 2, 7, 8, 1])\n        checker.check_sparse_tensor(sparse)\n\n    def test_check_sparse_tensor_coo_format_invalid_index(self):  # type: () -> None\n        sparse = self.make_sparse([10, 10], [13, 17, 19], [3, 2], [0, 9, 0, 27, 8, 1])\n        self.assertRaises(checker.ValidationError, checker.check_sparse_tensor, sparse)\n\n    def test_check_sparse_tensor_coo_format_invalid_shape(self):  # type: () -> None\n        sparse = self.make_sparse([10, 10], [13, 17, 19], [2, 3], [0, 9, 2, 7, 8, 1])\n        self.assertRaises(checker.ValidationError, checker.check_sparse_tensor, sparse)\n\n    def test_check_sparse_tensor_coo_format_invalid_dim2(self):  # type: () -> None\n        sparse = self.make_sparse([10, 10], [13, 17, 19], [3, 1], [0, 1, 2])\n        self.assertRaises(checker.ValidationError, checker.check_sparse_tensor, sparse)\n\n    def test_check_sparse_matmul(self):  # type: () -> None\n        M = 5\n        N = 10\n        # Create ValueInfoProto for input X of shape [N]\n        X = helper.make_tensor_value_info(\'X\', TensorProto.FLOAT, [N])\n        # Create a [M,N] sparse-matrix constant C\n        sparse_tensor = self.make_sparse([M, N], [2, 3, 1], [3], [3, 11, 37])\n        node1 = helper.make_node(\'Constant\', [], [\'C\'], sparse_value=sparse_tensor)\n        # Create ValueInfoProto for output Y of shape [M]\n        Y = helper.make_tensor_value_info(\'Y\', TensorProto.FLOAT, [M])\n        # Compute Y = C X\n        node2 = helper.make_node(\'MatMul\', [\'C\', \'X\'], [\'Y\'])\n        # create graph\n        graph = helper.make_graph([node1, node2], ""sparse_matmul"", [X], [Y])\n        # check graph\n        checker.check_graph(graph)\n\n    def test_check_model_unsupported_input_type(self):  # type: () -> None\n        N = 10\n        X = helper.make_tensor_value_info(\'X\', TensorProto.BOOL, [N])\n        Y = helper.make_tensor_value_info(\'Y\', TensorProto.FLOAT, [N])\n        Z = helper.make_tensor_value_info(\'Z\', TensorProto.FLOAT, [N])\n        onnx_id = helper.make_opsetid("""", 6)\n        node = helper.make_node(\'Add\', [\'X\', \'Y\'], [\'Z\'])\n        graph = helper.make_graph([node], ""test_add_input"", [X, Y], [Z])\n        model = helper.make_model(graph, producer_name=\'test\', opset_imports=[onnx_id])\n        self.assertRaises(checker.ValidationError, checker.check_model, model, True)\n\n    def test_check_modle_inconsistent_type(self):  # type: () -> None\n        N = 10\n        X = helper.make_tensor_value_info(\'X\', TensorProto.FLOAT, [N])\n        Y = helper.make_tensor_value_info(\'Y\', TensorProto.INT32, [N])\n        Z = helper.make_tensor_value_info(\'Z\', TensorProto.FLOAT, [N])\n        onnx_id = helper.make_opsetid("""", 6)\n        node = helper.make_node(\'Add\', [\'X\', \'Y\'], [\'Z\'])\n        graph = helper.make_graph([node], ""test_add_input"", [X, Y], [Z])\n        model = helper.make_model(graph, producer_name=\'test\', opset_imports=[onnx_id])\n        self.assertRaises(checker.ValidationError, checker.check_model, model, True)\n\n    def test_check_model_unsupported_output_type(self):  # type: () -> None\n        N = 10\n        X = helper.make_tensor_value_info(\'X\', TensorProto.FLOAT, [N])\n        Y = helper.make_tensor_value_info(\'Y\', TensorProto.FLOAT, [N])\n        Z = helper.make_tensor_value_info(\'Z\', TensorProto.BOOL, [N])\n        onnx_id = helper.make_opsetid("""", 6)\n        node = helper.make_node(\'Add\', [\'X\', \'Y\'], [\'Z\'])\n        graph = helper.make_graph([node], ""test_add_input"", [X, Y], [Z])\n        model = helper.make_model(graph, producer_name=\'test\', opset_imports=[onnx_id])\n        self.assertRaises(RuntimeError, checker.check_model, model, True)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
onnx/test/elu_test.py,0,"b""import unittest\n\nfrom onnx import defs, checker, helper\n\n\nclass TestRelu(unittest.TestCase):\n\n    def test_elu(self):  # type: () -> None\n        self.assertTrue(defs.has('Elu'))\n        node_def = helper.make_node(\n            'Elu', ['X'], ['Y'], alpha=1.0)\n        checker.check_node(node_def)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
onnx/test/helper_test.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport random\n\nimport numpy as np  # type: ignore\n\nfrom onnx import helper, defs, numpy_helper, checker\nfrom onnx import AttributeProto, TensorProto, GraphProto\nfrom typing import Text, Any, List\n\nimport unittest\n\n\nclass TestHelperAttributeFunctions(unittest.TestCase):\n\n    def test_attr_float(self):  # type: () -> None\n        # float\n        attr = helper.make_attribute(""float"", 1.)\n        self.assertEqual(attr.name, ""float"")\n        self.assertEqual(attr.f, 1.)\n        checker.check_attribute(attr)\n        # float with scientific\n        attr = helper.make_attribute(""float"", 1e10)\n        self.assertEqual(attr.name, ""float"")\n        self.assertEqual(attr.f, 1e10)\n        checker.check_attribute(attr)\n\n    def test_attr_int(self):  # type: () -> None\n        # integer\n        attr = helper.make_attribute(""int"", 3)\n        self.assertEqual(attr.name, ""int"")\n        self.assertEqual(attr.i, 3)\n        checker.check_attribute(attr)\n        # long integer\n        attr = helper.make_attribute(""int"", 5)\n        self.assertEqual(attr.name, ""int"")\n        self.assertEqual(attr.i, 5)\n        checker.check_attribute(attr)\n        # octinteger\n        attr = helper.make_attribute(""int"", 0o1701)\n        self.assertEqual(attr.name, ""int"")\n        self.assertEqual(attr.i, 0o1701)\n        checker.check_attribute(attr)\n        # hexinteger\n        attr = helper.make_attribute(""int"", 0x1701)\n        self.assertEqual(attr.name, ""int"")\n        self.assertEqual(attr.i, 0x1701)\n        checker.check_attribute(attr)\n\n    def test_attr_doc_string(self):  # type: () -> None\n        attr = helper.make_attribute(""a"", ""value"")\n        self.assertEqual(attr.name, ""a"")\n        self.assertEqual(attr.doc_string, """")\n        attr = helper.make_attribute(""a"", ""value"", ""doc"")\n        self.assertEqual(attr.name, ""a"")\n        self.assertEqual(attr.doc_string, ""doc"")\n\n    def test_attr_string(self):  # type: () -> None\n        # bytes\n        attr = helper.make_attribute(""str"", b""test"")\n        self.assertEqual(attr.name, ""str"")\n        self.assertEqual(attr.s, b""test"")\n        checker.check_attribute(attr)\n        # unspecified\n        attr = helper.make_attribute(""str"", ""test"")\n        self.assertEqual(attr.name, ""str"")\n        self.assertEqual(attr.s, b""test"")\n        checker.check_attribute(attr)\n        # unicode\n        attr = helper.make_attribute(""str"", u""test"")\n        self.assertEqual(attr.name, ""str"")\n        self.assertEqual(attr.s, b""test"")\n        checker.check_attribute(attr)\n        # empty str\n        attr = helper.make_attribute(""str"", """")\n        self.assertEqual(attr.name, ""str"")\n        self.assertEqual(helper.get_attribute_value(attr), b"""")\n        checker.check_attribute(attr)\n\n    def test_attr_repeated_float(self):  # type: () -> None\n        attr = helper.make_attribute(""floats"", [1.0, 2.0])\n        self.assertEqual(attr.name, ""floats"")\n        self.assertEqual(list(attr.floats), [1.0, 2.0])\n        checker.check_attribute(attr)\n\n    def test_attr_repeated_int(self):  # type: () -> None\n        attr = helper.make_attribute(""ints"", [1, 2])\n        self.assertEqual(attr.name, ""ints"")\n        self.assertEqual(list(attr.ints), [1, 2])\n        checker.check_attribute(attr)\n\n    def test_attr_repeated_str(self):  # type: () -> None\n        attr = helper.make_attribute(""strings"", [""str1"", ""str2""])\n        self.assertEqual(attr.name, ""strings"")\n        self.assertEqual(list(attr.strings), [b""str1"", b""str2""])\n        checker.check_attribute(attr)\n\n    def test_attr_repeated_tensor_proto(self):  # type: () -> None\n        tensors = [\n            helper.make_tensor(\n                name=\'a\',\n                data_type=TensorProto.FLOAT,\n                dims=(1,),\n                vals=np.ones(1).tolist()\n            ),\n            helper.make_tensor(\n                name=\'b\',\n                data_type=TensorProto.FLOAT,\n                dims=(1,),\n                vals=np.ones(1).tolist()\n            )]\n        attr = helper.make_attribute(""tensors"", tensors)\n        self.assertEqual(attr.name, ""tensors"")\n        self.assertEqual(list(attr.tensors), tensors)\n        checker.check_attribute(attr)\n\n    def test_attr_repeated_graph_proto(self):  # type: () -> None\n        graphs = [GraphProto(), GraphProto()]\n        graphs[0].name = ""a""\n        graphs[1].name = ""b""\n        attr = helper.make_attribute(""graphs"", graphs)\n        self.assertEqual(attr.name, ""graphs"")\n        self.assertEqual(list(attr.graphs), graphs)\n        checker.check_attribute(attr)\n\n    def test_is_attr_legal(self):  # type: () -> None\n        # no name, no field\n        attr = AttributeProto()\n        self.assertRaises(checker.ValidationError, checker.check_attribute, attr)\n        # name, but no field\n        attr = AttributeProto()\n        attr.name = ""test""\n        self.assertRaises(checker.ValidationError, checker.check_attribute, attr)\n        # name, with two fields\n        attr = AttributeProto()\n        attr.name = ""test""\n        attr.f = 1.0\n        attr.i = 2\n        self.assertRaises(checker.ValidationError, checker.check_attribute, attr)\n\n    def test_is_attr_legal_verbose(self):  # type: () -> None\n\n        def _set(attr, type, var, value):  # type: (AttributeProto, AttributeProto.AttributeType, Text, Any) -> None\n            setattr(attr, var, value)\n            setattr(attr, \'type\', type)\n\n        def _extend(attr, type, var, value):  # type: (AttributeProto, AttributeProto.AttributeType, List[Any], Any) -> None\n            var.extend(value)\n            setattr(attr, \'type\', type)\n\n        SET_ATTR = [\n            (lambda attr: _set(attr, AttributeProto.FLOAT, ""f"", 1.0)),\n            (lambda attr: _set(attr, AttributeProto.INT, ""i"", 1)),\n            (lambda attr: _set(attr, AttributeProto.STRING, ""s"", b""str"")),\n            (lambda attr: _extend(attr, AttributeProto.FLOATS, attr.floats, [1.0, 2.0])),\n            (lambda attr: _extend(attr, AttributeProto.INTS, attr.ints, [1, 2])),\n            (lambda attr: _extend(attr, AttributeProto.STRINGS, attr.strings, [b""a"", b""b""])),\n        ]\n        # Randomly set one field, and the result should be legal.\n        for _i in range(100):\n            attr = AttributeProto()\n            attr.name = ""test""\n            random.choice(SET_ATTR)(attr)\n            checker.check_attribute(attr)\n        # Randomly set two fields, and then ensure helper function catches it.\n        for _i in range(100):\n            attr = AttributeProto()\n            attr.name = ""test""\n            for func in random.sample(SET_ATTR, 2):\n                func(attr)\n            self.assertRaises(checker.ValidationError,\n                              checker.check_attribute,\n                              attr)\n\n\nclass TestHelperNodeFunctions(unittest.TestCase):\n\n    def test_node_no_arg(self):  # type: () -> None\n        self.assertTrue(defs.has(""Relu""))\n        node_def = helper.make_node(\n            ""Relu"", [""X""], [""Y""], name=""test"")\n        self.assertEqual(node_def.op_type, ""Relu"")\n        self.assertEqual(node_def.name, ""test"")\n        self.assertEqual(list(node_def.input), [""X""])\n        self.assertEqual(list(node_def.output), [""Y""])\n\n    def test_attr_doc_string(self):  # type: () -> None\n        node_def = helper.make_node(\n            ""Relu"", [""X""], [""Y""], name=""test"", doc_string=""doc"")\n        self.assertEqual(node_def.doc_string, ""doc"")\n\n    def test_node_with_arg(self):  # type: () -> None\n        self.assertTrue(defs.has(""Relu""))\n        # Note: Relu actually does not need an arg, but let\'s\n        # test it.\n        node_def = helper.make_node(\n            ""Relu"", [""X""], [""Y""],\n            arg_value=1)\n        self.assertEqual(node_def.op_type, ""Relu"")\n        self.assertEqual(list(node_def.input), [""X""])\n        self.assertEqual(list(node_def.output), [""Y""])\n        self.assertEqual(len(node_def.attribute), 1)\n        self.assertEqual(\n            node_def.attribute[0],\n            helper.make_attribute(""arg_value"", 1))\n\n    def test_node_domain(self):  # type: () -> None\n        node_def = helper.make_node(\n            ""Relu"", [""X""], [""Y""], name=""test"", doc_string=""doc"", domain=""test.domain"")\n        self.assertEqual(node_def.domain, ""test.domain"")\n\n    def test_graph(self):  # type: () -> None\n        node_def1 = helper.make_node(\n            ""Relu"", [""X""], [""Y""])\n        node_def2 = helper.make_node(\n            ""Add"", [""X"", ""Y""], [""Z""])\n        value_info = [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, [1, 2])]\n        graph = helper.make_graph(\n            [node_def1, node_def2],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, [1, 2])],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, [1, 2])],\n            doc_string=None,\n            value_info=value_info,\n        )\n        self.assertEqual(graph.name, ""test"")\n        self.assertEqual(len(graph.node), 2)\n        self.assertEqual(graph.node[0], node_def1)\n        self.assertEqual(graph.node[1], node_def2)\n        self.assertEqual(graph.doc_string, """")\n        self.assertEqual(graph.value_info[0], value_info[0])\n\n    def test_graph_docstring(self):  # type: () -> None\n        graph = helper.make_graph([], ""my graph"", [], [], None, ""my docs"")\n        self.assertEqual(graph.name, ""my graph"")\n        self.assertEqual(graph.doc_string, ""my docs"")\n\n    def test_model(self):  # type: () -> None\n        node_def = helper.make_node(\n            ""Relu"", [""X""], [""Y""])\n        graph_def = helper.make_graph(\n            [node_def],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, [1, 2])],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, [1, 2])])\n        self.assertRaises(AttributeError, helper.make_model, graph_def, xxx=1)\n        model_def = helper.make_model(graph_def, producer_name=\'test\')\n        self.assertEqual(model_def.producer_name, \'test\')\n\n    def test_model_docstring(self):  # type: () -> None\n        graph = helper.make_graph([], ""my graph"", [], [])\n        model_def = helper.make_model(graph, doc_string=\'test\')\n        # models may have their own documentation, but don\'t have a name\n        # their name is the domain-qualified name of the underlying graph.\n        self.assertFalse(hasattr(model_def, ""name""))\n        self.assertEqual(model_def.doc_string, \'test\')\n\n    def test_model_metadata_props(self):  # type: () -> None\n        graph = helper.make_graph([], ""my graph"", [], [])\n        model_def = helper.make_model(graph, doc_string=\'test\')\n        helper.set_model_props(model_def, {\'Title\': \'my graph\', \'Keywords\': \'test;graph\'})\n        checker.check_model(model_def)\n        helper.set_model_props(model_def, {\'Title\': \'my graph\', \'Keywords\': \'test;graph\'})\n        checker.check_model(model_def)  # helper replaces, so no dupe\n\n        dupe = model_def.metadata_props.add()\n        dupe.key = \'Title\'\n        dupe.value = \'Other\'\n        self.assertRaises(checker.ValidationError, checker.check_model, model_def)\n\n\nclass TestHelperTensorFunctions(unittest.TestCase):\n\n    def test_make_tensor(self):  # type: () -> None\n        np_array = np.random.randn(2, 3).astype(np.float32)\n\n        tensor = helper.make_tensor(\n            name=\'test\',\n            data_type=TensorProto.FLOAT,\n            dims=(2, 3),\n            vals=np_array.reshape(6).tolist()\n        )\n        self.assertEqual(tensor.name, \'test\')\n        np.testing.assert_equal(np_array, numpy_helper.to_array(tensor))\n\n        # use raw_data field to store the data\n        tensor = helper.make_tensor(\n            name=\'test\',\n            data_type=TensorProto.FLOAT,\n            dims=(2, 3),\n            vals=np_array.reshape(6).tobytes(),\n            raw=True,\n        )\n        np.testing.assert_equal(np_array, numpy_helper.to_array(tensor))\n\n        string_list = list(s.encode(\'utf-8\') for s in [\'Amy\', \'Billy\', \'Cindy\', \'David\'])\n        tensor = helper.make_tensor(\n            name=\'test\',\n            data_type=TensorProto.STRING,\n            dims=(2, 2),\n            vals=string_list,\n            raw=False\n        )\n        self.assertEqual(string_list, list(tensor.string_data))\n\n    def test_make_sparse_tensor(self):  # type: () -> None\n        values = [1.1, 2.2, 3.3, 4.4, 5.5]\n        values_tensor = helper.make_tensor(\n            name=\'test\',\n            data_type=TensorProto.FLOAT,\n            dims=(5, ),\n            vals=values\n        )\n        indices = [1, 3, 5, 7, 9]\n        indices_tensor = helper.make_tensor(\n            name=\'test_indices\',\n            data_type=TensorProto.INT64,\n            dims=(5, ),\n            vals=indices\n        )\n        dense_shape = [10]\n        sparse = helper.make_sparse_tensor(values_tensor, indices_tensor, dense_shape)\n        self.assertEqual(sparse.values, values_tensor)\n        self.assertEqual(sparse.indices, indices_tensor)\n        self.assertEqual(sparse.dims, dense_shape)\n\n    def test_make_tensor_value_info(self):  # type: () -> None\n        vi = helper.make_tensor_value_info(\'X\', TensorProto.FLOAT, (2, 4))\n        checker.check_value_info(vi)\n\n        # scalar value\n        vi = helper.make_tensor_value_info(\'Y\', TensorProto.FLOAT, ())\n        checker.check_value_info(vi)\n\n\nclass TestPrintableGraph(unittest.TestCase):\n\n    def test_initializer_with_matching_graph_input(self):  # type: () -> None\n        add = helper.make_node(""Add"", [""X"", ""Y_Initializer""], [""Z""])\n        value_info = [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, [1])]\n\n        graph = helper.make_graph(\n            [add],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, [1]),\n             helper.make_tensor_value_info(""Y_Initializer"", TensorProto.FLOAT, [1])],  # inputs\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, [1])],  # outputs\n            [helper.make_tensor(""Y_Initializer"", TensorProto.FLOAT, [1], [1])],  # initializers\n            doc_string=None,\n            value_info=value_info\n        )\n\n        graph_str = helper.printable_graph(graph)\n        self.assertTrue(\'\'\') optional inputs with matching initializers (\n  %Y_Initializer[FLOAT, 1]\'\'\' in graph_str, graph_str)\n\n    def test_initializer_no_matching_graph_input(self):  # type: () -> None\n        add = helper.make_node(""Add"", [""X"", ""Y_Initializer""], [""Z""])\n        value_info = [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, [1])]\n\n        graph = helper.make_graph(\n            [add],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, [1])],  # inputs\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, [1])],  # outputs\n            [helper.make_tensor(""Y_Initializer"", TensorProto.FLOAT, [1], [1])],  # initializers\n            doc_string=None,\n            value_info=value_info\n        )\n\n        graph_str = helper.printable_graph(graph)\n        self.assertTrue(\'\'\') initializers (\n  %Y_Initializer[FLOAT, 1]\'\'\' in graph_str, graph_str)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
onnx/test/numpy_helper_test.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nfrom onnx import numpy_helper\n\nimport unittest\n\n\nclass TestNumpyHelper(unittest.TestCase):\n\n    def _test_numpy_helper_float_type(self, dtype):  # type: (np.number) -> None\n        a = np.random.rand(13, 37).astype(dtype)\n        tensor_def = numpy_helper.from_array(a, ""test"")\n        self.assertEqual(tensor_def.name, ""test"")\n        a_recover = numpy_helper.to_array(tensor_def)\n        np.testing.assert_equal(a, a_recover)\n\n    def _test_numpy_helper_int_type(self, dtype):  # type: (np.number) -> None\n        a = np.random.randint(\n            np.iinfo(dtype).min,\n            np.iinfo(dtype).max,\n            dtype=dtype,\n            size=(13, 37))\n        tensor_def = numpy_helper.from_array(a, ""test"")\n        self.assertEqual(tensor_def.name, ""test"")\n        a_recover = numpy_helper.to_array(tensor_def)\n        np.testing.assert_equal(a, a_recover)\n\n    def test_float(self):  # type: () -> None\n        self._test_numpy_helper_float_type(np.float32)\n\n    def test_uint8(self):  # type: () -> None\n        self._test_numpy_helper_int_type(np.uint8)\n\n    def test_int8(self):  # type: () -> None\n        self._test_numpy_helper_int_type(np.int8)\n\n    def test_uint16(self):  # type: () -> None\n        self._test_numpy_helper_int_type(np.uint16)\n\n    def test_int16(self):  # type: () -> None\n        self._test_numpy_helper_int_type(np.int16)\n\n    def test_int32(self):  # type: () -> None\n        self._test_numpy_helper_int_type(np.int32)\n\n    def test_int64(self):  # type: () -> None\n        self._test_numpy_helper_int_type(np.int64)\n\n    def test_string(self):  # type: () -> None\n        a = np.array([\'Amy\', \'Billy\', \'Cindy\', \'David\']).astype(np.object)\n        tensor_def = numpy_helper.from_array(a, ""test"")\n        self.assertEqual(tensor_def.name, ""test"")\n        a_recover = numpy_helper.to_array(tensor_def)\n        np.testing.assert_equal(a, a_recover)\n\n    def test_bool(self):  # type: () -> None\n        a = np.random.randint(2, size=(13, 37)).astype(np.bool)\n        tensor_def = numpy_helper.from_array(a, ""test"")\n        self.assertEqual(tensor_def.name, ""test"")\n        a_recover = numpy_helper.to_array(tensor_def)\n        np.testing.assert_equal(a, a_recover)\n\n    def test_float16(self):  # type: () -> None\n        self._test_numpy_helper_float_type(np.float32)\n\n    def test_complex64(self):  # type: () -> None\n        self._test_numpy_helper_float_type(np.complex64)\n\n    def test_complex128(self):  # type: () -> None\n        self._test_numpy_helper_float_type(np.complex128)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
onnx/test/optimizer_test.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom onnx import checker, helper, ModelProto, TensorProto, GraphProto, NodeProto, shape_inference\nfrom typing import Sequence, Text, Any, Tuple, List, Callable\nfrom onnx import numpy_helper\n\nimport numpy as np  # type: ignore\n\nimport onnx.optimizer\nimport unittest\n\n\nclass TestOptimizer(unittest.TestCase):\n\n    def _optimized(self, graph, opts, fixed_point=False, **kwargs):  # type: (GraphProto, Sequence[Text], bool, **Any) -> ModelProto\n        orig_model = helper.make_model(graph, producer_name=\'onnx-test\', **kwargs)\n        optimized_model = onnx.optimizer.optimize(orig_model, opts, fixed_point)\n        checker.check_model(optimized_model)\n        return optimized_model\n\n    # input_types and output_types are lists of triples of (name, type, shape)\n    def _make_fake_loop_op(self,\n                           body_nodes,   # type: Sequence[NodeProto]\n                           input_types,  # type: Sequence[Tuple[TensorProto.DataType, Sequence[int], Text]]\n                           output_types  # type: Sequence[Tuple[TensorProto.DataType, Sequence[int], Text]]\n                           ):  # type: (...) -> List[NodeProto]\n        zero = helper.make_tensor(\n            ""trip_count_value"", TensorProto.INT32, (), [10])\n        true = helper.make_tensor(""condition"", TensorProto.BOOL, (), [True])\n        # lcd is a dummy loop-carried dependency that only exists because\n        # right now the schema checker is broken and assumes a variadic\n        # input needs at least one value.\n        graph_inputs = [helper.make_tensor_value_info(""i"", TensorProto.INT32, ()),\n                        helper.make_tensor_value_info(""cond"", TensorProto.BOOL, ())]\n        for type, shape, name in input_types:\n            graph_inputs.append(\n                helper.make_tensor_value_info(""_"" + name, type, shape))\n        graph_outputs = [helper.make_tensor_value_info(\n            ""cond"", TensorProto.BOOL, ())]\n        for type, shape, name in output_types:\n            graph_outputs.append(\n                helper.make_tensor_value_info(""_"" + name, type, shape))\n        body_graph = helper.make_graph(body_nodes, ""body_graph"", graph_inputs,\n                                       graph_outputs)\n        loop_inputs = [""trip_count"", ""condition""]\n        loop_inputs.extend([name for _, _, name in input_types])\n        # TODO: fix checker to accept 0-input variadic inputs\n        if len(loop_inputs) == 2:\n            loop_inputs.append("""")\n        loop_outputs = [name for _, _, name in output_types]\n        retval_nodes = [\n            helper.make_node(""Constant"", [], [""trip_count""], value=zero),\n            helper.make_node(""Constant"", [], [""condition""], value=true),\n            helper.make_node(""Loop"", loop_inputs, loop_outputs, body=body_graph)\n        ]\n        return retval_nodes\n\n    def _make_fake_if_op(self,\n                         true_nodes,   # type: Sequence[NodeProto]\n                         false_nodes,  # type: Sequence[NodeProto]\n                         output_types  # type: Sequence[Tuple[TensorProto.DataType, Sequence[int], Text]]\n                         ):  # type: (...) -> List[NodeProto]\n        true = helper.make_tensor(""condition"", TensorProto.BOOL, (), [True])\n        true_graph = helper.make_graph(true_nodes, ""true_graph"", [], [])\n        false_graph = helper.make_graph(false_nodes, ""false_graph"", [], [])\n        if_inputs = [""condition""]\n        if_outputs = [name for _, _, name in output_types]\n        retval_nodes = [\n            helper.make_node(""Constant"", [], [""condition""], value=true),\n            helper.make_node(""If"", if_inputs, if_outputs, then_branch=true_graph,\n                             else_branch=false_graph)\n        ]\n        return retval_nodes\n\n    # fn is a function that takes a single node as argument\n    def _visit_all_nodes_recursive(self, graph, fn):  # type: (GraphProto, Callable[[NodeProto], None]) -> None\n        for node in graph.node:\n            fn(node)\n            for attr in node.attribute:\n                if attr.g is not None:\n                    self._visit_all_nodes_recursive(attr.g, fn)\n                if len(attr.graphs):\n                    for gr in attr.graphs:\n                        self._visit_all_nodes_recursive(gr, fn)\n\n    def test_get_available_passes(self):  # type: () -> None\n        # FIXME does not guarantees to be listing all\n        graph = helper.make_graph([], ""dummy_graph"", [], [])\n        list_of_passes = onnx.optimizer.get_available_passes()\n        assert isinstance(list_of_passes, (list)) and len(list_of_passes) > 0\n        for pass_name in list_of_passes:\n            # If pass_name is invalid it throws a RuntimeError\n            self._optimized(graph, [pass_name])\n\n    def test_eliminate_identity_single_use(self):  # type: () -> None\n        nodes = [helper.make_node(""Identity"", [""X""], [""Y""])]\n        nodes.extend(self._make_fake_loop_op(\n            [helper.make_node(""Identity"", [""_Y""], [""_Y2""])],\n            [(TensorProto.FLOAT, (5,), ""Y"")],\n            [(TensorProto.FLOAT, (5,), ""Y2"")]))\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (5,))],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (5,)),\n             helper.make_tensor_value_info(""Y2"", TensorProto.FLOAT, (5,))])\n        optimized_model = self._optimized(graph, [""eliminate_identity""])\n\n        # All identity nodes should have been eliminated\n        def check_identity(node):  # type: (NodeProto) -> None\n            assert node.op_type != ""Identity""\n        self._visit_all_nodes_recursive(optimized_model.graph, check_identity)\n        # Use of the output from the Identity node in the main graph should\n        # have been replaced with the input to the identity node\n        assert len(optimized_model.graph.output) == 2\n        assert optimized_model.graph.output[0].name == ""X""\n        # Use of the output from the Identity node in the loop graph should\n        # have been replaced with the input to that identity node\n        assert len(optimized_model.graph.node[2].attribute[0].g.output) == 2\n        assert optimized_model.graph.node[2].attribute[0].g.output[1].name == ""_Y""\n\n    def test_eliminate_identity_graph_output(self):  # type: () -> None\n        add = helper.make_node(""Add"", [""X"", ""Y""], [""A""])\n        identity = helper.make_node(""Identity"", [""A""], [""B""])\n        graph = helper.make_graph(\n            [add, identity],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (5,)),\n             helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (5,))],\n            [helper.make_tensor_value_info(""B"", TensorProto.FLOAT, (5,))])\n        optimized_model = self._optimized(graph, [""eliminate_identity""])\n\n        for node in optimized_model.graph.node:\n            assert node.op_type != ""Identity""\n        assert len(optimized_model.graph.node) == 1\n\n    def test_eliminate_identity_multiple_uses(self):  # type: () -> None\n        identity = helper.make_node(""Identity"", [""X""], [""Y""])\n        add = helper.make_node(""Add"", [""Z"", ""Y""], [""A""])\n        mul = helper.make_node(""Mul"", [""A"", ""Y""], [""B""])\n        graph = helper.make_graph(\n            [identity, add, mul],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (5,)),\n             helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (5,))],\n            [helper.make_tensor_value_info(""B"", TensorProto.FLOAT, (5,))])\n        optimized_model = self._optimized(graph, [""eliminate_identity""])\n\n        for node in optimized_model.graph.node:\n            assert node.op_type != ""Identity""\n        assert len(optimized_model.graph.node) == 2\n\n    def test_nop_transpose_graph_output(self):  # type: () -> None\n        add = helper.make_node(""Add"", [""X"", ""Y""], [""A""])\n        trans = helper.make_node(""Transpose"", [""A""], [""B""], perm=[0, 1])\n        graph = helper.make_graph(\n            [add, trans],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (2, 3)),\n             helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (2, 3))],\n            [helper.make_tensor_value_info(""B"", TensorProto.FLOAT, (2, 3))])\n        # The existence of shape infos of graoh outputs is checked in _optimized\n        optimized_model = self._optimized(graph, [""eliminate_nop_transpose""])\n\n        def check_transpose(node):  # type: (NodeProto) -> None\n            assert node.op_type != ""Transpose""\n        self._visit_all_nodes_recursive(optimized_model.graph, check_transpose)\n        assert len(optimized_model.graph.node) == 1\n\n    def test_nop_transpose(self):  # type: () -> None\n        nodes = [helper.make_node(""Transpose"", [""X""], [""Y""], perm=[0, 1])]\n        nodes.extend(self._make_fake_loop_op(\n            [helper.make_node(""Transpose"", [""_Y""], [""_Y2""], perm=[0, 1])],\n            [(TensorProto.FLOAT, (2, 3), ""Y"")],\n            [(TensorProto.FLOAT, (2, 3), ""Y2"")]))\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (2, 3))],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (2, 3)),\n             helper.make_tensor_value_info(""Y2"", TensorProto.FLOAT, (2, 3))])\n        optimized_model = self._optimized(graph, [""eliminate_nop_transpose""])\n\n        def check_transpose(node):  # type: (NodeProto) -> None\n            assert node.op_type != ""Transpose""\n        self._visit_all_nodes_recursive(optimized_model.graph, check_transpose)\n        # Use of the output from the Transpose node in the main graph should\n        # have been replaced with the input to the identity node\n        assert len(optimized_model.graph.output) == 2\n        assert optimized_model.graph.output[0].name == ""X""\n        # Use of the output from the Transpose node in the loop graph should\n        # have been replaced with the input to that identity node\n        assert len(optimized_model.graph.node[2].attribute[0].g.output) == 2\n        assert optimized_model.graph.node[2].attribute[0].g.output[1].name == ""_Y""\n\n    def test_nop_transpose_default(self):  # type: () -> None\n        trans = helper.make_node(""Transpose"", [""X""], [""Y""])\n        graph = helper.make_graph(\n            [trans],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (2, 3))],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (3, 2))])\n        optimized_model = self._optimized(graph, [""eliminate_nop_transpose""])\n\n        assert len(list(optimized_model.graph.node)) == 1\n        assert optimized_model.graph.node[0].op_type == ""Transpose""\n\n    def test_nop_pad_opset10(self):  # type: () -> None\n        nodes = [helper.make_node(""Pad"", [""X""], [""Y""], pads=[0, 0])]\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (2, 3))],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (2, 3))])\n        assert len(graph.node) == 1\n        optimized_model = self._optimized(graph, [""eliminate_nop_pad""], False, opset_imports=[helper.make_opsetid("""", 10)])\n\n        def check_pad(node):  # type: (NodeProto) -> None\n            assert node.op_type != ""Pad""\n        self._visit_all_nodes_recursive(optimized_model.graph, check_pad)\n        assert len(optimized_model.graph.output) == 1\n        assert optimized_model.graph.output[0].name == ""X""\n        assert len(optimized_model.graph.node) == 0\n\n    def test_nop_pad_graph_output(self):  # type: () -> None\n        add = helper.make_node(""Add"", [""X"", ""Y""], [""A""])\n        pad = helper.make_node(""Pad"", [""A"", ""Pads""], [""B""])\n        graph = helper.make_graph(\n            [add, pad],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (5,)),\n             helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (5,)),\n             helper.make_tensor_value_info(""Pads"", TensorProto.INT64, (2,))],\n            [helper.make_tensor_value_info(""B"", TensorProto.FLOAT, (5,))],\n            [helper.make_tensor(""Pads"", TensorProto.INT64,\n                dims=(2,),\n                vals=np.array([0, 0]).astype(np.int64).tobytes(),\n                raw=True)])\n        # The existence of shape infos of graoh outputs is checked in _optimized\n        optimized_model = self._optimized(graph, [""eliminate_nop_pad""])\n\n        def check_pad(node):  # type: (NodeProto) -> None\n            assert node.op_type != ""Pad""\n        self._visit_all_nodes_recursive(optimized_model.graph, check_pad)\n        assert len(optimized_model.graph.node) == 1\n\n    def test_nop_pad(self):  # type: () -> None\n        nodes = [helper.make_node(""Pad"", [""X"", ""Pads""], [""Y""])]\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (2, 3)),\n             helper.make_tensor_value_info(""Pads"", TensorProto.INT64, (4,))],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (2, 3))],\n            [helper.make_tensor(""Pads"", TensorProto.INT64,\n                dims=(4,),\n                vals=np.array([0, 0, 0, 0]).astype(np.int64).tobytes(),\n                raw=True)])\n        assert len(graph.node) == 1\n        optimized_model = self._optimized(graph, [""eliminate_nop_pad""])\n\n        def check_pad(node):  # type: (NodeProto) -> None\n            assert node.op_type != ""Pad""\n        self._visit_all_nodes_recursive(optimized_model.graph, check_pad)\n        assert len(optimized_model.graph.output) == 1\n        assert optimized_model.graph.output[0].name == ""X""\n        assert len(optimized_model.graph.node) == 0\n\n    def test_nop_pad_default_opset10(self):  # type: () -> None\n        trans = helper.make_node(""Pad"", [""X""], [""Y""], pads=[0, 1])\n        graph = helper.make_graph(\n            [trans],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (2, 3))],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (2, 4))])\n        optimized_model = self._optimized(graph, [""eliminate_nop_pad""], False, opset_imports=[helper.make_opsetid("""", 10)])\n\n        assert len(list(optimized_model.graph.node)) == 1\n        assert optimized_model.graph.node[0].op_type == ""Pad""\n\n    def test_nop_pad_default(self):  # type: () -> None\n        trans = helper.make_node(""Pad"", [""X"", ""Pads""], [""Y""])\n        graph = helper.make_graph(\n            [trans],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (2, 3)),\n             helper.make_tensor_value_info(""Pads"", TensorProto.INT64, (4,))],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (2, 4))],\n            [helper.make_tensor(""Pads"", TensorProto.INT64,\n                dims=(4,),\n                vals=np.array([0, 1, 0, 0]).astype(np.int64).tobytes(),\n                raw=True)])\n        optimized_model = self._optimized(graph, [""eliminate_nop_pad""])\n\n        assert len(list(optimized_model.graph.node)) == 1\n        assert optimized_model.graph.node[0].op_type == ""Pad""\n\n    def test_eliminate_unused_initializer(self):  # type: () -> None\n        add = helper.make_node(""Add"", [""X"", ""Y""], [""Z""])\n        graph = helper.make_graph(\n            [add],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (1, 2)),\n             helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (1, 2))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (1, 2))],\n            [helper.make_tensor(""A"", TensorProto.FLOAT,\n                                dims=(2, 3),\n                                vals=np.random.randn(2, 3).astype(\n                                    np.float32).tobytes(),\n                                raw=True)])\n        optimized_model = self._optimized(\n            graph, [""eliminate_unused_initializer""])\n\n        assert len(list(optimized_model.graph.initializer)) == 0\n\n    def test_eliminate_unused_initializer_input(self):  # type: () -> None\n        add = helper.make_node(""Add"", [""X"", ""Y""], [""Z""])\n        graph = helper.make_graph(\n            [add],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (1, 2)),\n             helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (1, 2)),\n             helper.make_tensor_value_info(""A"", TensorProto.FLOAT, (2, 3))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (1, 2))],\n            [helper.make_tensor(""A"", TensorProto.FLOAT,\n                                dims=(2, 3),\n                                vals=np.random.randn(2, 3).astype(\n                                    np.float32).tobytes(),\n                                raw=True)])\n        optimized_model = self._optimized(\n            graph, [""eliminate_unused_initializer""])\n\n        assert len(list(optimized_model.graph.initializer)) == 0\n        assert len(optimized_model.graph.input) == 2\n\n    def test_eliminate_unused_initializer_no_eliminate_used_default(self):  # type: () -> None\n        add = helper.make_node(""Add"", [""X"", ""A""], [""Z""])\n        graph = helper.make_graph(\n            [add],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (1, 2)),\n             helper.make_tensor_value_info(""A"", TensorProto.FLOAT, (1, 2))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (1, 2))],\n            [helper.make_tensor(""A"", TensorProto.FLOAT,\n                                dims=(1, 2),\n                                vals=np.random.randn(1, 2).astype(\n                                    np.float32).tobytes(),\n                                raw=True)])\n        optimized_model = self._optimized(\n            graph, [""eliminate_unused_initializer""])\n\n        assert len(list(optimized_model.graph.initializer)) == 1\n\n    def test_eliminate_unused_initializer_no_eliminate_used(self):  # type: () -> None\n        nodes = [helper.make_node(""Add"", [""X"", ""A""], [""Z""])]\n        nodes.extend(self._make_fake_loop_op(\n            [helper.make_node(""Add"", [""_X"", ""_A""], [""_Z2""])],\n            [(TensorProto.FLOAT, (1, 2), ""X""),\n             (TensorProto.FLOAT, (1, 2), ""A"")],\n            [(TensorProto.FLOAT, (1, 2), ""Z2"")]))\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (1, 2)),\n             helper.make_tensor_value_info(""A"", TensorProto.FLOAT, (1, 2))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (1, 2))],\n            [helper.make_tensor(""A"", TensorProto.FLOAT,\n                                dims=(1, 2),\n                                vals=np.random.randn(1, 2).astype(\n                                    np.float32).tobytes(),\n                                raw=True)])\n        optimized_model = self._optimized(\n            graph, [""eliminate_unused_initializer""])\n\n        # Add, Constant (trip count), Constant (cond), Loop\n        assert len(list(optimized_model.graph.node)) == 4\n        assert optimized_model.graph.node[0].op_type == ""Add""\n        assert optimized_model.graph.output[0].name == ""Z""\n        # Add\n        assert len(optimized_model.graph.node[3].attribute[0].g.node) == 1\n        assert optimized_model.graph.node[3].attribute[0].g.node[0].op_type == \'Add\'\n        assert optimized_model.graph.node[3].attribute[0].g.output[1].name == \'_Z2\'\n\n        assert len(list(optimized_model.graph.initializer)) == 1\n\n    def test_eliminate_unused_initializer_no_eliminate_output(self):  # type: () -> None\n        add = helper.make_node(""Add"", [""X"", ""Y""], [""Z""])\n        graph = helper.make_graph(\n            [add],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (1, 2)),\n             helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (1, 2)),\n             helper.make_tensor_value_info(""A"", TensorProto.FLOAT, (2, 3))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (1, 2)),\n             helper.make_tensor_value_info(""A"", TensorProto.FLOAT, (2, 3))],\n            [helper.make_tensor(""A"", TensorProto.FLOAT,\n                                dims=(2, 3),\n                                vals=np.random.randn(2, 3).astype(\n                                    np.float32).tobytes(),\n                                raw=True)])\n        optimized_model = self._optimized(\n            graph, [""eliminate_unused_initializer""])\n\n        assert len(list(optimized_model.graph.initializer)) == 1\n        assert ""Z"" in [o.name for o in optimized_model.graph.output]\n\n    def test_extract_constant_to_initializer(self):  # type: () -> None\n        conv = helper.make_node(""Conv"", [""X"", ""Y""], [""Z""])\n        constant = helper.make_node(""Constant"", [], [""A""],\n                                    value=helper.make_tensor(\n                                        name=""bias"",\n                                        data_type=TensorProto.FLOAT,\n                                        dims=(16, 1, 1),\n                                        vals=np.random.randn(16).astype(np.float32).tolist()))\n        add = helper.make_node(""Add"", [""Z"", ""A""], [""B""])\n        graph = helper.make_graph(\n            [conv, constant, add],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (1, 5, 3, 3)),\n             helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (16, 5, 3, 3))],\n            [helper.make_tensor_value_info(\n                ""B"", TensorProto.FLOAT, (1, 16, 1, 1))],\n        )\n        optimized_model = self._optimized(\n            graph, [""extract_constant_to_initializer""])\n        self.assertEqual(\n            set(vi.name for vi in optimized_model.graph.input),\n            {\'X\', \'Y\', \'A\'})\n\n        self.assertEqual(len(optimized_model.graph.initializer), 1)\n        init = optimized_model.graph.initializer[0]\n        self.assertEqual(init.name, \'A\')\n        self.assertEqual(init.dims, [16, 1, 1])\n        self.assertEqual(init.data_type, TensorProto.FLOAT)\n\n        self.assertEqual(\n            [n.op_type for n in optimized_model.graph.node], [\'Conv\', \'Add\'])\n\n    def test_fuse_concats(self):  # type: () -> None\n        nodes = [helper.make_node(""Concat"", [""A"", ""B"", ""C""], [""X""], axis=0),\n                 helper.make_node(""Concat"", [""D"", ""E"", ""F""], [""Y""], axis=0),\n                 helper.make_node(""Concat"", [""X"", ""G"", ""Y""], [""Z""], axis=0)]\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""A"", TensorProto.FLOAT, (2, 3, 4)),\n            helper.make_tensor_value_info(""B"", TensorProto.FLOAT, (4, 3, 4)),\n            helper.make_tensor_value_info(""C"", TensorProto.FLOAT, (2, 3, 4)),\n            helper.make_tensor_value_info(""D"", TensorProto.FLOAT, (4, 3, 4)),\n            helper.make_tensor_value_info(""E"", TensorProto.FLOAT, (2, 3, 4)),\n            helper.make_tensor_value_info(""F"", TensorProto.FLOAT, (4, 3, 4)),\n            helper.make_tensor_value_info(""G"", TensorProto.FLOAT, (4, 3, 4))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (18, 3, 4))])\n        optimized_model = self._optimized(\n            graph, [""fuse_consecutive_concats""], True)  # two passes are needed to simplify the graph to its simplest state.\n\n        assert len(optimized_model.graph.node) == 1\n        assert len(optimized_model.graph.node[0].input) == 7\n        assert optimized_model.graph.node[0].input == [\n            ""A"", ""B"", ""C"", ""G"", ""D"", ""E"", ""F""]\n        assert optimized_model.graph.node[0].op_type == ""Concat""\n\n    def test_fuse_concats_different_axis(self):  # type: () -> None\n        nodes = [helper.make_node(""Concat"", [""A"", ""B"", ""C""], [""X""], axis=0),\n                 helper.make_node(""Concat"", [""D"", ""E"", ""F""], [""Y""], axis=1),\n                 helper.make_node(""Concat"", [""X"", ""Y""], [""Z""], axis=2)]\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""A"", TensorProto.FLOAT, (2, 3, 4)),\n            helper.make_tensor_value_info(""B"", TensorProto.FLOAT, (4, 3, 4)),\n            helper.make_tensor_value_info(""C"", TensorProto.FLOAT, (2, 3, 4)),\n            helper.make_tensor_value_info(""D"", TensorProto.FLOAT, (4, 3, 4)),\n            helper.make_tensor_value_info(""E"", TensorProto.FLOAT, (4, 3, 4)),\n            helper.make_tensor_value_info(""F"", TensorProto.FLOAT, (4, 3, 4))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (18, 3, 4))])\n        optimized_model = self._optimized(\n            graph, [""fuse_consecutive_concats""], True)  # two passes are needed to simplify the graph to its simplest state.\n\n        assert optimized_model.graph == graph\n\n    def test_fuse_transpose(self):  # type: () -> None\n        nodes = [helper.make_node(""Transpose"", [""X""], [""Y""], perm=[1, 0, 2]),\n                 helper.make_node(""Transpose"", [""Y""], [""Z""], perm=[2, 0, 1]),\n                 helper.make_node(""Transpose"", [""Z""], [""A""], perm=[2, 0, 1])]\n        nodes.extend(self._make_fake_loop_op(\n            [helper.make_node(""Transpose"", [""_X""], [""_Y2""], perm=[1, 0, 2]),\n             helper.make_node(""Transpose"", [""_Y2""], [""_Y3""], perm=[2, 0, 1]),\n             helper.make_node(""Transpose"", [""_Y3""], [""_Y4""], perm=[2, 0, 1])],\n            [(TensorProto.FLOAT, (2, 3), ""X"")],\n            [(TensorProto.FLOAT, (2, 3), ""Y4"")]))\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (2, 3, 4))],\n            [helper.make_tensor_value_info(""A"", TensorProto.FLOAT, (2, 4, 3)),\n             helper.make_tensor_value_info(""Y4"", TensorProto.FLOAT, (4, 3, 2))])\n        original_model = helper.make_model(graph)\n        shape_inference.infer_shapes(original_model)\n        optimized_model = self._optimized(\n            graph, [""fuse_consecutive_transposes""])\n        shape_inference.infer_shapes(optimized_model)\n\n        # Transpose, Constant (trip count), Constant (cond), Loop\n        assert len(list(optimized_model.graph.node)) == 4\n        # Transpose\n        assert len(optimized_model.graph.node[3].attribute[0].g.node) == 1\n\n    def test_fuse_transpose_default_graph_output(self):  # type: () -> None\n        add = helper.make_node(""Add"", [""X"", ""Y""], [""A""])\n        trans1 = helper.make_node(""Transpose"", [""A""], [""B""])\n        trans2 = helper.make_node(""Transpose"", [""B""], [""C""])\n        graph = helper.make_graph(\n            [add, trans1, trans2],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (2, 3)),\n             helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (2, 3))],\n            [helper.make_tensor_value_info(""C"", TensorProto.FLOAT, (2, 3))])\n        # The existence of shape infos of graoh outputs is checked in _optimized\n        optimized_model = self._optimized(graph, [""fuse_consecutive_transposes""])\n\n        def check_transpose(node):  # type: (NodeProto) -> None\n            assert node.op_type != ""Transpose""\n        self._visit_all_nodes_recursive(optimized_model.graph, check_transpose)\n        assert len(optimized_model.graph.node) == 1\n\n    def test_fuse_transpose_default(self):  # type: () -> None\n        trans1 = helper.make_node(""Transpose"", [""X""], [""Y""])\n        trans2 = helper.make_node(""Transpose"", [""Y""], [""Z""])\n        graph = helper.make_graph(\n            [trans1, trans2],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (2, 3, 4))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (2, 3, 4))])\n        optimized_model = self._optimized(\n            graph, [""fuse_consecutive_transposes""])\n\n        assert len(list(optimized_model.graph.node)) == 0\n\n    def test_fuse_transpose_default_no_fuse(self):  # type: () -> None\n        trans1 = helper.make_node(""Transpose"", [""X""], [""Y""])\n        trans2 = helper.make_node(""Transpose"", [""Y""], [""Z""], perm=[0, 1, 2])\n        graph = helper.make_graph(\n            [trans1, trans2],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (2, 3, 4))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (4, 3, 2))])\n        optimized_model = self._optimized(\n            graph, [""fuse_consecutive_transposes""])\n\n        assert len(list(optimized_model.graph.node)) == 2\n        for node in optimized_model.graph.node:\n            assert node.op_type == ""Transpose""\n\n    def test_fuse_transpose_into_gemm(self):  # type: () -> None\n        nodes = [helper.make_node(""Transpose"", [""X""], [""A""], perm=[1, 0]),\n                 helper.make_node(""Transpose"", [""Y""], [""B""], perm=[1, 0]),\n                 helper.make_node(""Gemm"", [""A"", ""B"", ""C""], [""Z""])]\n        nodes.extend(self._make_fake_loop_op(\n            [helper.make_node(""Transpose"", [""_X""], [""_A""], perm=[1, 0]),\n             helper.make_node(""Transpose"", [""_Y""], [""_B""], perm=[1, 0]),\n             helper.make_node(""Gemm"", [""_A"", ""_B"", ""_C""], [""_Z2""])],\n            [(TensorProto.FLOAT, (2, 3), ""X""),\n             (TensorProto.FLOAT, (5, 2), ""Y""),\n             (TensorProto.FLOAT, (3, 5), ""C"")],\n            [(TensorProto.FLOAT, (2, 3), ""Z2"")]))\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (2, 3)),\n             helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (5, 2)),\n             helper.make_tensor_value_info(""C"", TensorProto.FLOAT, (3, 5))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (3, 5))])\n        optimized_model = self._optimized(graph, [""fuse_transpose_into_gemm""])\n\n        # Gemm, Constant (trip count), Constant (cond), Loop\n        assert len(list(optimized_model.graph.node)) == 4\n        assert optimized_model.graph.node[0].op_type == ""Gemm""\n        # Gemm\n        assert len(optimized_model.graph.node[3].attribute[0].g.node) == 1\n        assert optimized_model.graph.node[3].attribute[0].g.node[0].op_type == ""Gemm""\n\n    def test_fuse_add_bias_into_conv_use_weight_shape(self):  # type: () -> None\n        nodes = [helper.make_node(""Conv"", [""X"", ""Y""], [""Z""]),\n                 helper.make_node(""Add"", [""Z"", ""A""], [""B""])]\n        nodes.extend(self._make_fake_loop_op(\n            [helper.make_node(""Conv"", [""_X"", ""_Y""], [""_Z""]),\n             helper.make_node(""Add"", [""_Z"", ""_A""], [""_B2""])],\n            [(TensorProto.FLOAT, (1, 5, 3, 3), ""X""),\n             (TensorProto.FLOAT, (16, 5, 3, 3), ""Y""),\n             (TensorProto.FLOAT, (16, 1, 1), ""A"")],\n            [(TensorProto.FLOAT, (1, 16, 3, 3), ""B2"")]))\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (1, 5, 3, 3)),\n             helper.make_tensor_value_info(\n                 ""Y"", TensorProto.FLOAT, (16, 5, 3, 3)),\n             helper.make_tensor_value_info(""A"", TensorProto.FLOAT, (16, 1, 1))],\n            [helper.make_tensor_value_info(\n                ""B"", TensorProto.FLOAT, (1, 16, 1, 1))],\n        )\n        optimized_model = self._optimized(graph, [""fuse_add_bias_into_conv""])\n\n        # Squeeze, Conv, Constant (trip count), Constant (condition), Loop\n        assert len(list(optimized_model.graph.node)) == 5\n        assert optimized_model.graph.node[0].op_type == \'Squeeze\'\n        assert optimized_model.graph.node[1].op_type == \'Conv\'\n        assert optimized_model.graph.output[0].name == \'Z\'\n        # Squeeze, Conv\n        assert len(optimized_model.graph.node[4].attribute[0].g.node) == 2\n        assert optimized_model.graph.node[4].attribute[0].g.node[0].op_type == \'Squeeze\'\n        assert optimized_model.graph.node[4].attribute[0].g.node[1].op_type == \'Conv\'\n        # Output 1 since 0 is \'cond\'\n        assert optimized_model.graph.node[4].attribute[0].g.output[1].name == \'_Z\'\n\n    def test_fuse_add_bias_into_conv_use_weight_shape_with_tile(self):  # type: () -> None\n        conv = helper.make_node(""Conv"", [""X"", ""Y""], [""Z""])\n        add = helper.make_node(""Add"", [""Z"", ""A""], [""B""])\n        graph = helper.make_graph(\n            [conv, add],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (1, 5, 3, 3)),\n             helper.make_tensor_value_info(\n                 ""Y"", TensorProto.FLOAT, (16, 5, 3, 3)),\n             helper.make_tensor_value_info(""A"", TensorProto.FLOAT, (1,))],\n            [helper.make_tensor_value_info(\n                ""B"", TensorProto.FLOAT, (1, 16, 1, 1))],\n        )\n        optimized_model = self._optimized(graph, [""fuse_add_bias_into_conv""])\n\n        assert len(list(optimized_model.graph.node)) == 3\n        assert len(optimized_model.graph.value_info) == 1\n        assert optimized_model.graph.value_info[0].type.tensor_type.elem_type == TensorProto.INT64\n        assert len(\n            optimized_model.graph.value_info[0].type.tensor_type.shape.dim) == 1\n        assert optimized_model.graph.node[0].op_type == \'Constant\'\n        assert optimized_model.graph.node[1].op_type == \'Tile\'\n        assert optimized_model.graph.node[2].op_type == \'Conv\'\n        assert optimized_model.graph.output[0].name == \'Z\'\n\n    def test_fuse_add_bias_into_conv_use_conv_shape(self):  # type: () -> None\n        sub = helper.make_node(""Sub"", [""M"", ""N""], [""Y""])\n        conv = helper.make_node(""Conv"", [""X"", ""Y""], [""Z""])\n        add = helper.make_node(""Add"", [""Z"", ""A""], [""B""])\n        graph = helper.make_graph(\n            [sub, conv, add],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (1, 5, 3, 3)),\n             helper.make_tensor_value_info(\n                 ""M"", TensorProto.FLOAT, (16, 5, 3, 3)),\n             helper.make_tensor_value_info(\n                 ""N"", TensorProto.FLOAT, (16, 5, 3, 3)),\n             helper.make_tensor_value_info(""A"", TensorProto.FLOAT, (1, 16, 1, 1))],\n            [helper.make_tensor_value_info(\n                ""B"", TensorProto.FLOAT, (1, 16, 1, 1))],\n            value_info=[\n                helper.make_tensor_value_info(\n                    ""Z"", TensorProto.FLOAT, (1, 16, 1, 1))\n            ],\n        )\n        optimized_model = self._optimized(graph, [""fuse_add_bias_into_conv""])\n\n        assert len(optimized_model.graph.node) == 3\n        assert optimized_model.graph.node[0].op_type == \'Sub\'\n        assert optimized_model.graph.node[1].op_type == \'Squeeze\'\n        assert optimized_model.graph.node[2].op_type == \'Conv\'\n        assert optimized_model.graph.output[0].name == \'Z\'\n        assert optimized_model.graph.output[0].type.tensor_type.elem_type == TensorProto.FLOAT\n        assert len(\n            optimized_model.graph.output[0].type.tensor_type.shape.dim) == 4\n\n    def test_fuse_add_bias_into_conv_use_move_constant(self):  # type: () -> None\n        conv = helper.make_node(""Conv"", [""X"", ""Y""], [""Z""])\n        constant = helper.make_node(""Constant"", [], [""A""],\n                                    value=helper.make_tensor(\n                                        name=""bias"",\n                                        data_type=TensorProto.FLOAT,\n                                        dims=(16, 1, 1),\n                                        vals=np.random.randn(16).astype(np.float32).tolist()))\n        add = helper.make_node(""Add"", [""Z"", ""A""], [""B""])\n        graph = helper.make_graph(\n            [conv, constant, add],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (1, 5, 3, 3)),\n             helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (16, 5, 3, 3))],\n            [helper.make_tensor_value_info(\n                ""B"", TensorProto.FLOAT, (1, 16, 1, 1))],\n            value_info=[\n                helper.make_tensor_value_info(\n                    ""A"", TensorProto.FLOAT, (16, 1, 1)),\n            ]\n        )\n        optimized_model = self._optimized(graph, [""fuse_add_bias_into_conv""])\n\n        assert len(optimized_model.graph.node) == 3\n        assert optimized_model.graph.node[0].op_type == \'Constant\'\n        assert optimized_model.graph.node[1].op_type == \'Squeeze\'\n        assert optimized_model.graph.node[2].op_type == \'Conv\'\n        assert optimized_model.graph.output[0].name == \'Z\'\n        assert optimized_model.graph.output[0].type.tensor_type.elem_type == TensorProto.FLOAT\n        assert len(\n            optimized_model.graph.output[0].type.tensor_type.shape.dim) == 4\n\n    def test_fuse_add_bias_into_conv_squeeze_1d_bias_no_fuse(self):  # type: () -> None\n        conv = helper.make_node(""Conv"", [""X"", ""Y""], [""Z""])\n        add = helper.make_node(""Add"", [""Z"", ""A""], [""B""])\n        graph = helper.make_graph(\n            [conv, add],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (1, 5, 3, 3)),\n             helper.make_tensor_value_info(\n                 ""Y"", TensorProto.FLOAT, (16, 5, 3, 3)),\n             helper.make_tensor_value_info(""A"", TensorProto.FLOAT, (3,))],\n            [helper.make_tensor_value_info(\n                ""B"", TensorProto.FLOAT, (1, 16, 1, 3))],\n            value_info=[\n                helper.make_tensor_value_info(\n                    ""Z"", TensorProto.FLOAT, (1, 16, 1, 1)),\n            ]\n        )\n        optimized_model = self._optimized(graph, [""fuse_add_bias_into_conv""])\n\n        assert len(list(optimized_model.graph.node)) == 2\n        assert optimized_model.graph.node[0].op_type == \'Conv\'\n        assert optimized_model.graph.node[1].op_type == \'Add\'\n\n    def test_fuse_add_bias_into_conv_squeeze_3d_bias_no_fuse(self):  # type: () -> None\n        conv = helper.make_node(""Conv"", [""X"", ""Y""], [""Z""])\n        add = helper.make_node(""Add"", [""Z"", ""A""], [""B""])\n        graph = helper.make_graph(\n            [conv, add],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (1, 5, 3, 3)),\n             helper.make_tensor_value_info(\n                 ""Y"", TensorProto.FLOAT, (16, 5, 3, 3)),\n             helper.make_tensor_value_info(""A"", TensorProto.FLOAT, (16, 3, 3))],\n            [helper.make_tensor_value_info(\n                ""B"", TensorProto.FLOAT, (1, 16, 3, 3))],\n            value_info=[\n                helper.make_tensor_value_info(\n                    ""Z"", TensorProto.FLOAT, (1, 16, 1, 1)),\n            ]\n        )\n        optimized_model = self._optimized(graph, [""fuse_add_bias_into_conv""])\n\n        assert len(list(optimized_model.graph.node)) == 2\n        assert optimized_model.graph.node[0].op_type == \'Conv\'\n        assert optimized_model.graph.node[1].op_type == \'Add\'\n\n    def test_fuse_add_bias_into_conv_squeeze_4d_bias_no_fuse(self):  # type: () -> None\n        conv = helper.make_node(""Conv"", [""X"", ""Y""], [""Z""])\n        add = helper.make_node(""Add"", [""Z"", ""A""], [""B""])\n        graph = helper.make_graph(\n            [conv, add],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (1, 5, 3, 3)),\n             helper.make_tensor_value_info(\n                 ""Y"", TensorProto.FLOAT, (16, 5, 3, 3)),\n             helper.make_tensor_value_info(""A"", TensorProto.FLOAT, (1, 16, 3, 3))],\n            [helper.make_tensor_value_info(\n                ""B"", TensorProto.FLOAT, (1, 16, 3, 3))]\n        )\n        optimized_model = self._optimized(graph, [""fuse_add_bias_into_conv""])\n\n        assert len(list(optimized_model.graph.node)) == 2\n        assert optimized_model.graph.node[0].op_type == \'Conv\'\n        assert optimized_model.graph.node[1].op_type == \'Add\'\n\n    def test_fuse_matmul_add_bias_into_gemm(self):  # type: () -> None\n        matmul = helper.make_node(""MatMul"", [""X"", ""Y""], [""Z""])\n        add = helper.make_node(""Add"", [""Z"", ""B""], [""A""])\n        graph = helper.make_graph(\n            [matmul, add],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (32, 10)),\n             helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (10, 16)),\n             helper.make_tensor_value_info(""B"", TensorProto.FLOAT, (16,))],\n            [helper.make_tensor_value_info(""A"", TensorProto.FLOAT, (32, 16))]\n        )\n        optimized_model = self._optimized(graph, [""fuse_matmul_add_bias_into_gemm""])\n\n        assert len(list(optimized_model.graph.node)) == 1\n        assert optimized_model.graph.node[0].op_type == ""Gemm""\n\n    def test_fuse_matmul_add_bias_into_gemm_2d_bias(self):  # type: () -> None\n        matmul = helper.make_node(""MatMul"", [""X"", ""Y""], [""Z""])\n        add = helper.make_node(""Add"", [""Z"", ""B""], [""A""])\n        graph = helper.make_graph(\n            [matmul, add],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (32, 10)),\n             helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (10, 16)),\n             helper.make_tensor_value_info(""B"", TensorProto.FLOAT, (1, 16))],\n            [helper.make_tensor_value_info(""A"", TensorProto.FLOAT, (32, 16))]\n        )\n        optimized_model = self._optimized(graph, [""fuse_matmul_add_bias_into_gemm""])\n\n        assert len(list(optimized_model.graph.node)) == 1\n        assert optimized_model.graph.node[0].op_type == ""Gemm""\n\n    def test_fuse_matmul_add_bias_into_gemm_2d_bias_same_shape(self):  # type: () -> None\n        matmul = helper.make_node(""MatMul"", [""X"", ""Y""], [""Z""])\n        add = helper.make_node(""Add"", [""Z"", ""B""], [""A""])\n        graph = helper.make_graph(\n            [matmul, add],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (32, 10)),\n             helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (10, 16)),\n             helper.make_tensor_value_info(""B"", TensorProto.FLOAT, (32, 16))],\n            [helper.make_tensor_value_info(""A"", TensorProto.FLOAT, (32, 16))]\n        )\n        optimized_model = self._optimized(graph, [""fuse_matmul_add_bias_into_gemm""])\n\n        assert len(list(optimized_model.graph.node)) == 1\n        assert optimized_model.graph.node[0].op_type == ""Gemm""\n\n    def test_fuse_matmul_add_bias_into_gemm_2d_bias_bcast_no_fuse(self):  # type: () -> None\n        matmul = helper.make_node(""MatMul"", [""X"", ""Y""], [""Z""])\n        add = helper.make_node(""Add"", [""Z"", ""B""], [""A""])\n        graph = helper.make_graph(\n            [matmul, add],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (1, 10)),\n             helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (10, 16)),\n             helper.make_tensor_value_info(""B"", TensorProto.FLOAT, (16, 16))],\n            [helper.make_tensor_value_info(""A"", TensorProto.FLOAT, (16, 16))]\n        )\n        optimized_model = self._optimized(graph, [""fuse_matmul_add_bias_into_gemm""])\n\n        assert optimized_model.graph == graph\n\n    def test_fuse_matmul_add_bias_into_gemm_3d_matmul_no_fuse(self):  # type: () -> None\n        matmul = helper.make_node(""MatMul"", [""X"", ""Y""], [""Z""])\n        add = helper.make_node(""Add"", [""Z"", ""B""], [""A""])\n        graph = helper.make_graph(\n            [matmul, add],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (2, 3, 4)),\n             helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (2, 4, 3)),\n             helper.make_tensor_value_info(""B"", TensorProto.FLOAT, (3, 3))],\n            [helper.make_tensor_value_info(""A"", TensorProto.FLOAT, (2, 3, 3))]\n        )\n        optimized_model = self._optimized(graph, [""fuse_matmul_add_bias_into_gemm""])\n\n        assert optimized_model.graph == graph\n\n    def test_fuse_matmul_add_bias_into_gemm_3d_bias_no_fuse(self):  # type: () -> None\n        matmul = helper.make_node(""MatMul"", [""X"", ""Y""], [""Z""])\n        add = helper.make_node(""Add"", [""Z"", ""B""], [""A""])\n        graph = helper.make_graph(\n            [matmul, add],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (32, 10)),\n             helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (10, 16)),\n             helper.make_tensor_value_info(""B"", TensorProto.FLOAT, (4, 1, 16))],\n            [helper.make_tensor_value_info(""A"", TensorProto.FLOAT, (32, 16))]\n        )\n        optimized_model = self._optimized(graph, [""fuse_matmul_add_bias_into_gemm""])\n\n        assert optimized_model.graph == graph\n\n    def test_fuse_matmul_add_bias_into_gemm_multiple_use_no_fuse(self):  # type: () -> None\n        matmul = helper.make_node(""MatMul"", [""X"", ""Y""], [""Z""])\n        identity = helper.make_node(""Identity"", [""Z""], [""A1""])\n        add = helper.make_node(""Add"", [""Z"", ""B""], [""A2""])\n        graph = helper.make_graph(\n            [matmul, add, identity],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (32, 10)),\n             helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (10, 16)),\n             helper.make_tensor_value_info(""B"", TensorProto.FLOAT, (1, 16))],\n            [helper.make_tensor_value_info(""A1"", TensorProto.FLOAT, (32, 16)),\n             helper.make_tensor_value_info(""A2"", TensorProto.FLOAT, (32, 16))]\n        )\n        optimized_model = self._optimized(graph, [""fuse_matmul_add_bias_into_gemm""])\n\n        assert optimized_model.graph == graph\n\n    def test_fuse_pad_into_conv_no_optional_value_opset10(self):  # type: () -> None\n        pad = helper.make_node(\n            ""Pad"",\n            [""X""],\n            [""P""],\n            mode=""constant"",\n            pads=[0, 0, 0, 0, 0, 0, 1, 1]\n        )\n        conv = helper.make_node(""Conv"", [""P"", ""Y""], [""Z""])\n        graph = helper.make_graph(\n            [pad, conv],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (1, 5, 2, 2)),\n             helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (16, 5, 3, 3))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (1, 16, 1, 1))]\n        )\n        optimized_model = self._optimized(graph, [""fuse_pad_into_conv""], False, opset_imports=[helper.make_opsetid("""", 10)])\n\n        assert len(list(optimized_model.graph.node)) == 1\n        assert optimized_model.graph.node[0].op_type == ""Conv""\n        assert optimized_model.graph.node[0].attribute[0].name == ""pads""\n        assert list(optimized_model.graph.node[0].attribute[0].ints) == [0, 0, 1, 1]\n\n    def test_fuse_pad_into_conv_no_optional_value(self):  # type: () -> None\n        pad = helper.make_node(\n            ""Pad"",\n            [""X"", ""Pads""],\n            [""P""],\n            mode=""constant""\n        )\n        conv = helper.make_node(""Conv"", [""P"", ""Y""], [""Z""])\n        graph = helper.make_graph(\n            [pad, conv],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (1, 5, 2, 2)),\n             helper.make_tensor_value_info(""Pads"", TensorProto.INT64, (8,)),\n             helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (16, 5, 3, 3))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (1, 16, 1, 1))],\n            [helper.make_tensor(""Pads"", TensorProto.INT64,\n             dims=(8,),\n             vals=np.array([0, 0, 0, 0, 0, 0, 1, 1]).astype(np.int64).tobytes(),\n             raw=True)])\n        optimized_model = self._optimized(graph, [""fuse_pad_into_conv""])\n\n        assert len(list(optimized_model.graph.node)) == 1\n        assert optimized_model.graph.node[0].op_type == ""Conv""\n        assert optimized_model.graph.node[0].attribute[0].name == ""pads""\n        assert list(optimized_model.graph.node[0].attribute[0].ints) == [0, 0, 1, 1]\n\n    def test_fuse_pad_into_conv_with_optional_value(self):  # type: () -> None\n        pad = helper.make_node(\n            ""Pad"",\n            [""X"", ""Pads"", ""Constant_value""],\n            [""P""],\n            mode=""constant""\n        )\n        conv = helper.make_node(""Conv"", [""P"", ""Y""], [""Z""])\n        graph = helper.make_graph(\n            [pad, conv],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (1, 5, 2, 2)),\n             helper.make_tensor_value_info(""Pads"", TensorProto.INT64, (8,)),\n             helper.make_tensor_value_info(""Constant_value"", TensorProto.FLOAT, ()),\n             helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (16, 5, 3, 3))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (1, 16, 1, 1))],\n            [helper.make_tensor(""Pads"", TensorProto.INT64,\n             dims=(8,),\n             vals=np.array([0, 0, 0, 0, 0, 0, 1, 1]).astype(np.int64).tobytes(),\n             raw=True),\n             helper.make_tensor(""Constant_value"", TensorProto.FLOAT,\n             dims=(),\n             vals=np.array([0]).astype(np.float32).tobytes(),\n             raw=True)])\n        optimized_model = self._optimized(graph, [""fuse_pad_into_conv""])\n\n        assert len(list(optimized_model.graph.node)) == 1\n        assert optimized_model.graph.node[0].op_type == ""Conv""\n        assert optimized_model.graph.node[0].attribute[0].name == ""pads""\n        assert list(optimized_model.graph.node[0].attribute[0].ints) == [0, 0, 1, 1]\n\n    def test_fuse_pad_into_conv_with_nonzero_optional_value(self):  # type: () -> None\n        pad = helper.make_node(\n            ""Pad"",\n            [""X"", ""Pads"", ""Constant_value""],\n            [""P""],\n            mode=""constant""\n        )\n        conv = helper.make_node(""Conv"", [""P"", ""Y""], [""Z""])\n        graph = helper.make_graph(\n            [pad, conv],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (1, 5, 2, 2)),\n             helper.make_tensor_value_info(""Pads"", TensorProto.INT64, (8,)),\n             helper.make_tensor_value_info(""Constant_value"", TensorProto.FLOAT, ()),\n             helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (16, 5, 3, 3))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (1, 16, 1, 1))],\n            [helper.make_tensor(""Pads"", TensorProto.INT64,\n             dims=(8,),\n             vals=np.array([0, 0, 0, 0, 0, 0, 1, 1]).astype(np.int64).tobytes(),\n             raw=True),\n             helper.make_tensor(""Constant_value"", TensorProto.FLOAT,\n             dims=(),\n             vals=np.array([25]).astype(np.float32).tobytes(),  # non-zero Constant_value -> so no pad\n             raw=True)])\n        optimized_model = self._optimized(graph, [""fuse_pad_into_conv""])\n\n        assert optimized_model.graph == graph\n\n    def test_fuse_pad_into_conv_1d_opset10(self):  # type: () -> None\n        pad = helper.make_node(\n            ""Pad"",\n            [""X""],\n            [""P""],\n            mode=""constant"",\n            pads=[0, 0, 1, 0, 0, 1]\n        )\n        conv = helper.make_node(""Conv"", [""P"", ""Y""], [""Z""])\n        graph = helper.make_graph(\n            [pad, conv],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (1, 5, 30)),\n             helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (16, 5, 32))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (1, 16, 1))]\n        )\n        optimized_model = self._optimized(graph, [""fuse_pad_into_conv""], False, opset_imports=[helper.make_opsetid("""", 10)])\n\n        assert len(list(optimized_model.graph.node)) == 1\n        assert optimized_model.graph.node[0].op_type == ""Conv""\n        assert optimized_model.graph.node[0].attribute[0].name == ""pads""\n        assert list(optimized_model.graph.node[0].attribute[0].ints) == [1, 1]\n\n    def test_fuse_pad_into_conv_1d(self):  # type: () -> None\n        pad = helper.make_node(\n            ""Pad"",\n            [""X"", ""Pads""],\n            [""P""],\n            mode=""constant""\n        )\n        conv = helper.make_node(""Conv"", [""P"", ""Y""], [""Z""])\n        graph = helper.make_graph(\n            [pad, conv],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (1, 5, 30)),\n             helper.make_tensor_value_info(""Pads"", TensorProto.INT64, (6,)),\n             helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (16, 5, 32))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (1, 16, 1))],\n            [helper.make_tensor(""Pads"", TensorProto.INT64,\n             dims=(6,),\n             vals=np.array([0, 0, 1, 0, 0, 1]).astype(np.int64).tobytes(),\n             raw=True)])\n        optimized_model = self._optimized(graph, [""fuse_pad_into_conv""])\n\n        assert len(list(optimized_model.graph.node)) == 1\n        assert optimized_model.graph.node[0].op_type == ""Conv""\n        assert optimized_model.graph.node[0].attribute[0].name == ""pads""\n        assert list(optimized_model.graph.node[0].attribute[0].ints) == [1, 1]\n\n    def test_fuse_pad_into_conv_existing_conv_pad_opset10(self):  # type: () -> None\n        pad = helper.make_node(\n            ""Pad"",\n            [""X""],\n            [""P""],\n            mode=""constant"",\n            pads=[0, 0, 0, 0, 0, 0, 1, 1]\n        )\n        conv = helper.make_node(\n            ""Conv"",\n            [""P"", ""Y""],\n            [""Z""],\n            pads=[1, 1, 0, 0]\n        )\n        graph = helper.make_graph(\n            [pad, conv],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (1, 5, 2, 2)),\n             helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (16, 5, 4, 4))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (1, 16, 1, 1))]\n        )\n        optimized_model = self._optimized(graph, [""fuse_pad_into_conv""], False, opset_imports=[helper.make_opsetid("""", 10)])\n\n        assert len(list(optimized_model.graph.node)) == 1\n        assert optimized_model.graph.node[0].op_type == ""Conv""\n        assert optimized_model.graph.node[0].attribute[0].name == ""pads""\n        assert list(optimized_model.graph.node[0].attribute[0].ints) == [1, 1, 1, 1]\n\n    def test_fuse_pad_into_conv_existing_conv_pad(self):  # type: () -> None\n        pad = helper.make_node(\n            ""Pad"",\n            [""X"", ""Pads""],\n            [""P""],\n            mode=""constant""\n        )\n        conv = helper.make_node(\n            ""Conv"",\n            [""P"", ""Y""],\n            [""Z""],\n            pads=[1, 1, 0, 0]\n        )\n        graph = helper.make_graph(\n            [pad, conv],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (1, 5, 2, 2)),\n             helper.make_tensor_value_info(""Pads"", TensorProto.INT64, (8,)),\n             helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (16, 5, 4, 4))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (1, 16, 1, 1))],\n            [helper.make_tensor(""Pads"", TensorProto.INT64,\n             dims=(8,),\n             vals=np.array([0, 0, 0, 0, 0, 0, 1, 1]).astype(np.int64).tobytes(),\n             raw=True)])\n        optimized_model = self._optimized(graph, [""fuse_pad_into_conv""])\n\n        assert len(list(optimized_model.graph.node)) == 1\n        assert optimized_model.graph.node[0].op_type == ""Conv""\n        assert optimized_model.graph.node[0].attribute[0].name == ""pads""\n        assert list(optimized_model.graph.node[0].attribute[0].ints) == [1, 1, 1, 1]\n\n    def test_fuse_pad_into_conv_pad_feature_no_fuse_opset10(self):  # type: () -> None\n        pad = helper.make_node(\n            ""Pad"",\n            [""X""],\n            [""P""],\n            mode=""constant"",\n            pads=[0, 1, 0, 0, 0, 0, 0, 0]\n        )\n        conv = helper.make_node(""Conv"", [""P"", ""Y""], [""Z""])\n        graph = helper.make_graph(\n            [pad, conv],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (1, 4, 3, 3)),\n             helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (16, 5, 3, 3))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (1, 16, 1, 1))]\n        )\n        optimized_model = self._optimized(graph, [""fuse_pad_into_conv""], False, opset_imports=[helper.make_opsetid("""", 10)])\n\n        assert optimized_model.graph == graph\n\n    def test_fuse_pad_into_conv_pad_feature_no_fuse(self):  # type: () -> None\n        pad = helper.make_node(\n            ""Pad"",\n            [""X"", ""Pads""],\n            [""P""],\n            mode=""constant""\n        )\n        conv = helper.make_node(""Conv"", [""P"", ""Y""], [""Z""])\n        graph = helper.make_graph(\n            [pad, conv],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (1, 4, 3, 3)),\n             helper.make_tensor_value_info(""Pads"", TensorProto.INT64, (8,)),\n             helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (16, 5, 3, 3))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (1, 16, 1, 1))],\n            [helper.make_tensor(""Pads"", TensorProto.INT64,\n             dims=(8,),\n             vals=np.array([0, 1, 0, 0, 0, 0, 0, 0]).astype(np.int64).tobytes(),\n             raw=True)])\n        optimized_model = self._optimized(graph, [""fuse_pad_into_conv""])\n\n        assert optimized_model.graph == graph\n\n    def test_fuse_pad_into_conv_negative_pad_no_fuse_opset10(self):  # type: () -> None\n        pad = helper.make_node(\n            ""Pad"",\n            [""X""],\n            [""P""],\n            mode=""constant"",\n            pads=[0, 0, 0, 0, 0, 0, -1, -1]\n        )\n        conv = helper.make_node(""Conv"", [""P"", ""Y""], [""Z""])\n        graph = helper.make_graph(\n            [pad, conv],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (1, 5, 4, 4)),\n             helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (16, 5, 3, 3))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (1, 16, 1, 1))]\n        )\n        optimized_model = self._optimized(graph, [""fuse_pad_into_conv""], False, opset_imports=[helper.make_opsetid("""", 10)])\n\n        assert optimized_model.graph == graph\n\n    def test_fuse_pad_into_conv_negative_pad_no_fuse(self):  # type: () -> None\n        pad = helper.make_node(\n            ""Pad"",\n            [""X"", ""Pads""],\n            [""P""],\n            mode=""constant""\n        )\n        conv = helper.make_node(""Conv"", [""P"", ""Y""], [""Z""])\n        graph = helper.make_graph(\n            [pad, conv],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (1, 5, 4, 4)),\n             helper.make_tensor_value_info(""Pads"", TensorProto.INT64, (8,)),\n             helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (16, 5, 3, 3))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (1, 16, 1, 1))],\n            [helper.make_tensor(""Pads"", TensorProto.INT64,\n             dims=(8,),\n             vals=np.array([0, 0, 0, 0, 0, 0, -1, -1]).astype(np.int64).tobytes(),\n             raw=True)])\n        optimized_model = self._optimized(graph, [""fuse_pad_into_conv""])\n\n        assert optimized_model.graph == graph\n\n    def test_fuse_pad_into_conv_reflection_pad_no_fuse_opset10(self):  # type: () -> None\n        pad = helper.make_node(\n            ""Pad"",\n            [""X""],\n            [""P""],\n            mode=""reflect"",\n            pads=[0, 0, 0, 0, 0, 0, 1, 1]\n        )\n        conv = helper.make_node(""Conv"", [""P"", ""Y""], [""Z""])\n        graph = helper.make_graph(\n            [pad, conv],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (1, 5, 2, 2)),\n             helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (16, 5, 3, 3))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (1, 16, 1, 1))]\n        )\n        optimized_model = self._optimized(graph, [""fuse_pad_into_conv""], False, opset_imports=[helper.make_opsetid("""", 10)])\n\n        assert optimized_model.graph == graph\n\n    def test_fuse_pad_into_conv_reflection_pad_no_fuse(self):  # type: () -> None\n        pad = helper.make_node(\n            ""Pad"",\n            [""X"", ""Pads""],\n            [""P""],\n            mode=""reflect""\n        )\n        conv = helper.make_node(""Conv"", [""P"", ""Y""], [""Z""])\n        graph = helper.make_graph(\n            [pad, conv],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (1, 5, 2, 2)),\n             helper.make_tensor_value_info(""Pads"", TensorProto.INT64, (8,)),\n             helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (16, 5, 3, 3))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (1, 16, 1, 1))],\n            [helper.make_tensor(""Pads"", TensorProto.INT64,\n             dims=(8,),\n             vals=np.array([0, 0, 0, 0, 0, 0, 1, 1]).astype(np.int64).tobytes(),\n             raw=True)])\n        optimized_model = self._optimized(graph, [""fuse_pad_into_conv""])\n\n        assert optimized_model.graph == graph\n\n    def test_fuse_consecutive_squeezes(self):  # type: () -> None\n        nodes = [helper.make_node(""Squeeze"", [""X""], [""Y""], axes=[0, 4, 5]),\n                 helper.make_node(""Squeeze"", [""Y""], [""Z""], axes=[0, 3])]\n        nodes.extend(self._make_fake_loop_op(\n            [helper.make_node(""Squeeze"", [""_X""], [""_Y""], axes=[0, 4, 5]),\n             helper.make_node(""Squeeze"", [""_Y""], [""_Z2""], axes=[0, 3])],\n            [(TensorProto.FLOAT, (1, 1, 2, 3, 1, 1, 1, 1, 8, 9), ""X"")],\n            [(TensorProto.FLOAT, (2, 3, 1, 8, 9), ""Z2"")]))\n\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(\n                ""X"", TensorProto.FLOAT, (1, 1, 2, 3, 1, 1, 1, 1, 8, 9))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (2, 3, 1, 8, 9))])\n        optimized_model = self._optimized(graph, [""fuse_consecutive_squeezes""])\n\n        # Squeeze, Constant (trip count), Constant (cond), Loop\n        assert optimized_model.graph.node[0].op_type == ""Squeeze""\n        assert list(optimized_model.graph.node[0].attribute[0].ints) == [\n            0, 1, 4, 5, 6]\n        assert len(list(optimized_model.graph.node)) == 4\n\n    def test_fuse_consecutive_squeezes_default(self):  # type: () -> None\n        squeeze1 = helper.make_node(""Squeeze"", [""X""], [""Y""], axes=[0, 4, 5])\n        squeeze2 = helper.make_node(""Squeeze"", [""Y""], [""Z""], axes=[0, 3])\n        squeeze3 = helper.make_node(""Squeeze"", [""Z""], [""A""], axes=[2])\n        nodes = [squeeze1, squeeze2, squeeze3]\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(\n                ""X"", TensorProto.FLOAT, (1, 1, 2, 3, 1, 1, 1, 1, 8, 9))],\n            [helper.make_tensor_value_info(""A"", TensorProto.FLOAT, (2, 3, 8, 9))])\n        optimized_model = self._optimized(graph, [""fuse_consecutive_squeezes""])\n\n        assert optimized_model.graph.node[0].op_type == ""Squeeze""\n        assert list(optimized_model.graph.node[0].attribute[0].ints) == [\n            0, 1, 4, 5, 6, 7]\n        assert len(list(optimized_model.graph.node)) == 1\n\n    def test_fuse_consecutive_squeezes_random(self):  # type: () -> None\n        x_shape = [1, 1, 1, 3, 4, 1, 6, 1, 1, 9]\n        s1_one_indices = [i for i, a in enumerate(x_shape) if a == 1]\n        s1_axes = np.random.choice(s1_one_indices, size=np.random.randint(low=1, high=len(s1_one_indices) - 1),\n                                   replace=False)\n        s2_x_shape = [a for i, a in enumerate(x_shape) if i not in s1_axes]\n        s2_one_indices = [i for i, a in enumerate(s2_x_shape) if a == 1]\n        s2_axes = s2_one_indices\n\n        squeeze1 = helper.make_node(""Squeeze"", [""X""], [""Y""], axes=s1_axes)\n        squeeze2 = helper.make_node(""Squeeze"", [""Y""], [""Z""], axes=s2_axes)\n        nodes = [squeeze1, squeeze2]\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, x_shape)],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (3, 4, 6, 9))])\n        optimized_model = self._optimized(graph, [""fuse_consecutive_squeezes""])\n\n        assert optimized_model.graph.node[0].op_type == ""Squeeze""\n        assert list(optimized_model.graph.node[0].attribute[0].ints) == [\n            0, 1, 2, 5, 7, 8]\n        assert len(list(optimized_model.graph.node)) == 1\n\n    def test_fuse_consecutive_squeezes_multi_uses(self):  # type: () -> None\n        squeeze1 = helper.make_node(""Squeeze"", [""X""], [""Y""], axes=[0, 4, 5])\n        add = helper.make_node(""Add"", [""Y"", ""A""], [""Z2""])\n        squeeze2 = helper.make_node(""Squeeze"", [""Y""], [""Z""], axes=[0, 3])\n        graph = helper.make_graph(\n            [squeeze1, add, squeeze2],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (1, 1, 2, 3, 1, 1, 1, 1, 8, 9)),\n             helper.make_tensor_value_info(""A"", TensorProto.FLOAT, (1,))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (2, 3, 1, 8, 9)),\n             helper.make_tensor_value_info(""Z2"", TensorProto.FLOAT, (1, 2, 3, 1, 1, 8, 9))])\n        optimized_model = self._optimized(graph, [""fuse_consecutive_squeezes""])\n\n        assert optimized_model.graph.node[0].op_type == ""Squeeze""\n        assert list(optimized_model.graph.node[0].attribute[0].ints) == [\n            0, 4, 5]\n        assert optimized_model.graph.node[2].op_type == ""Squeeze""\n        assert optimized_model.graph.node[2].input == [""X""]\n        assert list(optimized_model.graph.node[2].attribute[0].ints) == [\n            0, 1, 4, 5, 6]\n        assert len(list(optimized_model.graph.node)) == 3\n\n    def test_fuse_consecutive_softmax_log_axis(self):  # type: () -> None\n        for axis in range(3):\n            softmax = helper.make_node(""Softmax"", [""X""], [""Y""], axis=axis)\n            log = helper.make_node(""Log"", [""Y""], [""Z""])\n            graph = helper.make_graph(\n                [softmax, log],\n                ""test"",\n                [helper.make_tensor_value_info(\n                    ""X"", TensorProto.FLOAT, (5, 7, 11))],\n                [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (5, 7, 11))])\n            optimized_model = self._optimized(\n                graph, [""fuse_consecutive_log_softmax""])\n\n            assert optimized_model.graph.output[0].type.tensor_type.elem_type == TensorProto.FLOAT\n            assert len(optimized_model.graph.output) == 1\n            assert len(optimized_model.graph.node) == 1\n            assert optimized_model.graph.node[0].op_type == ""LogSoftmax""\n            assert optimized_model.graph.node[0].attribute[0].name == ""axis""\n            assert optimized_model.graph.node[0].attribute[0].i == axis\n\n    def test_fuse_consecutive_softmax_log_side_effect(self):  # type: () -> None\n        softmax = helper.make_node(""Softmax"", [""X""], [""Y""], axis=2)\n        log = helper.make_node(""Log"", [""Y""], [""Z""])\n        graph = helper.make_graph(\n            [softmax, log],\n            ""test"",\n            [helper.make_tensor_value_info(\n                ""X"", TensorProto.FLOAT, (5, 7, 11))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (5, 7, 11)),\n             helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (5, 7, 11))])\n        optimized_model = self._optimized(\n            graph, [""fuse_consecutive_log_softmax""])\n\n        assert graph == optimized_model.graph\n\n    def test_fuse_consecutive_softmax_log_multiple_out(self):  # type: () -> None\n        softmax = helper.make_node(""Softmax"", [""X""], [""Y""], axis=2)\n        log = helper.make_node(""Log"", [""Y""], [""Z""])\n        exp = helper.make_node(""Exp"", [""Z""], [""Z1""])\n        graph = helper.make_graph(\n            [softmax, log, exp],\n            ""test"",\n            [helper.make_tensor_value_info(\n                ""X"", TensorProto.FLOAT, (5, 7, 11))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (5, 7, 11)),\n             helper.make_tensor_value_info(""Z1"", TensorProto.FLOAT, (5, 7, 11))])\n        optimized_model = self._optimized(\n            graph, [""fuse_consecutive_log_softmax""])\n\n        assert len(optimized_model.graph.output) == 2\n        assert len(optimized_model.graph.node) == 2\n        assert optimized_model.graph.output[0].type.tensor_type.elem_type == TensorProto.FLOAT\n        assert optimized_model.graph.output[1].type.tensor_type.elem_type == TensorProto.FLOAT\n        assert optimized_model.graph.node[0].op_type == ""LogSoftmax""\n        assert optimized_model.graph.node[0].attribute[0].name == ""axis""\n        assert optimized_model.graph.node[0].attribute[0].i == 2\n        assert optimized_model.graph.node[1].op_type == ""Exp""\n\n    def test_preserve_value_info(self):  # type: () -> None\n        trans1 = helper.make_node(""Transpose"", [""X""], [""Y""], perm=[1, 0, 2])\n        trans2 = helper.make_node(""Transpose"", [""Y""], [""Z""], perm=[2, 0, 1])\n        trans3 = helper.make_node(""Transpose"", [""Z""], [""A""], perm=[2, 0, 1])\n        graph = helper.make_graph(\n            [trans1, trans2, trans3],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (2, 3, 4))],\n            [helper.make_tensor_value_info(""A"", TensorProto.FLOAT, (2, 4, 3))])\n        vi = helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (3, 2, 4))\n        graph.value_info.extend([vi])\n        optimized_model = self._optimized(graph, [""nop""])\n        assert list(optimized_model.graph.value_info) == [vi]\n        assert len(list(optimized_model.graph.node)) == 3\n\n    def test_split(self):  # type: () -> None\n        node = onnx.helper.make_node(\n            \'Constant\',\n            inputs=[],\n            outputs=[\'X\'],\n            value=onnx.helper.make_tensor(\n                name=\'X\',\n                data_type=TensorProto.FLOAT,\n                dims=[1],\n                vals=[5],\n            ),\n        )\n        graph = helper.make_graph(\n            [node],\n            \'test-optimize-split\',\n            [],\n            [helper.make_tensor_value_info(\'X\', TensorProto.FLOAT, (1,))])\n\n        init_model = self._optimized(graph, [\'split_init\'])\n        self.assertEqual(len(init_model.graph.node), 1)\n        self.assertEqual(len(init_model.graph.output), 1)\n        self.assertEqual(init_model.graph.node[0].op_type, \'Constant\')\n\n        predict_model = self._optimized(graph, [\'split_predict\'])\n        self.assertEqual(len(predict_model.graph.node), 0)\n        self.assertEqual(len(predict_model.graph.input), 1)\n        self.assertEqual(predict_model.graph.input[0].name, \'X\')\n\n    def test_lift_lex_loop(self):  # type: () -> None\n        nodes = [helper.make_node(""Identity"", [""X""], [""Y""])]\n        nodes.extend(self._make_fake_loop_op(\n            [helper.make_node(""Identity"", [""X""], [""_Y2""]),\n             helper.make_node(""Identity"", [""Y""], [""_Y3""])],\n            [],\n            [(TensorProto.FLOAT, (5,), ""Y2""),\n             (TensorProto.FLOAT, (5,), ""Y3"")]))\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (5,))],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (5,)),\n             helper.make_tensor_value_info(""Y2"", TensorProto.FLOAT, (5,))])\n        optimized_model = self._optimized(graph, [""lift_lexical_references""])\n        assert len(optimized_model.graph.node) == 4\n        # body_graph, __control_inputs\n        assert len(optimized_model.graph.node[3].attribute) == 2\n        assert optimized_model.graph.node[3].attribute[1].name == ""__control_inputs""\n        assert optimized_model.graph.node[3].attribute[1].strings[0] == b""X""\n        assert optimized_model.graph.node[3].attribute[1].strings[1] == b""Y""\n\n    def test_lift_lex_if(self):  # type: () -> None\n        nodes = [helper.make_node(""Identity"", [""X""], [""Y""])]\n        nodes.extend(self._make_fake_if_op(\n            [helper.make_node(""Identity"", [""X""], [""_Y2""]),\n             helper.make_node(""Identity"", [""Y""], [""_Y3""])],\n            [helper.make_node(""Identity"", [""X""], [""_Y2""]),\n             helper.make_node(""Identity"", [""X""], [""_Y3""])],\n            [(TensorProto.FLOAT, (5,), ""Y2""),\n             (TensorProto.FLOAT, (5,), ""Y3"")]))\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (5,))],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (5,)),\n             helper.make_tensor_value_info(""Y2"", TensorProto.FLOAT, (5,))])\n        # ""If"" node now diverges from ONNX schema. Disable checking.\n        optimized_model = self._optimized(graph, [""lift_lexical_references""])\n\n        # Identity, Constant (condition), If\n        assert len(optimized_model.graph.node) == 3\n        # else_branch, then_branch, __control_inputs\n        assert len(optimized_model.graph.node[2].attribute) == 3\n        assert optimized_model.graph.node[2].attribute[2].name == ""__control_inputs""\n        assert optimized_model.graph.node[2].attribute[2].strings[0] == b""X""\n        assert optimized_model.graph.node[2].attribute[2].strings[1] == b""Y""\n\n    def test_fuse_bn_into_conv_simple(self):  # type: () -> None\n        for (tensor_type, np_type) in [(TensorProto.FLOAT, np.float32), (TensorProto.DOUBLE, np.float64)]:\n            conv = helper.make_node(""Conv"", [""X"", ""W"", ""B""], [""Y""])\n            bn = helper.make_node(""BatchNormalization"", [\n                                  ""Y"", ""scale"", ""b"", ""mean"", ""var""], [""Z""])\n\n            W = np.random.randn(3, 2, 5, 5).astype(np_type) + 2\n            B = np.random.randn(3,).astype(np_type) + 2\n            scale = np.random.randn(3,).astype(np_type) + 2\n            b = np.random.randn(3,).astype(np_type) + 2\n            mean = np.random.randn(3,).astype(np_type) + 2\n            var = np.abs(np.random.randn(3,).astype(np_type)) + 2\n\n            initializers = [\n                helper.make_tensor(name, tensor_type,\n                                   npa.shape, npa.tobytes(), raw=True)\n                for name, npa in [(\'W\', W), (\'B\', B), (\'scale\', scale), (\'b\', b), (\'mean\', mean), (\'var\', var)]\n            ]\n            graph = helper.make_graph(\n                [conv, bn],\n                ""test"",\n                [helper.make_tensor_value_info(""X"", tensor_type, (5, 2, 28, 28)),\n                 helper.make_tensor_value_info(""W"", tensor_type, (3, 2, 5, 5)),\n                 helper.make_tensor_value_info(""B"", tensor_type, (3,)),\n                 helper.make_tensor_value_info(""scale"", tensor_type, (3,)),\n                 helper.make_tensor_value_info(""b"", tensor_type, (3,)),\n                 helper.make_tensor_value_info(""mean"", tensor_type, (3,)),\n                 helper.make_tensor_value_info(""var"", tensor_type, (3,))],\n                [helper.make_tensor_value_info(\n                    ""Z"", tensor_type, (5, 3, 24, 24))],\n                initializer=initializers,\n                value_info=[\n                    helper.make_tensor_value_info(\n                        ""Y"", tensor_type, (5, 3, 24, 24))\n                ]\n            )\n            optimized_model = self._optimized(graph, [""fuse_bn_into_conv""])\n\n            self.assertEqual(len(optimized_model.graph.node), 1)\n            self.assertEqual(optimized_model.graph.node[0].op_type, \'Conv\')\n            self.assertEqual(len(optimized_model.graph.initializer), 2)\n            new_W = numpy_helper.to_array(optimized_model.graph.initializer[0])\n            new_b = numpy_helper.to_array(optimized_model.graph.initializer[1])\n\n            f = scale / np.sqrt(var + 1e-5)\n            np.testing.assert_almost_equal((B - mean) * f + b, new_b)\n            np.testing.assert_almost_equal(\n                W * f[:, np.newaxis, np.newaxis, np.newaxis], new_W)\n\n    def _internal_test_deadend_elimination(self, fixed):  # type: (bool) -> None\n        softmax = helper.make_node(""Softmax"", [""X""], [""Y""], axis=2)\n        log = helper.make_node(""Log"", [""Y""], [""Z""])\n        exp = helper.make_node(""Exp"", [""Z""], [""Z1""])\n        exp1 = helper.make_node(""Log"", [""Z""], [""Z2""])\n        exp2 = helper.make_node(""Sqrt"", [""Z1""], [""Z3""])\n        graph = helper.make_graph(\n            [softmax, log, exp, exp1, exp2],\n            ""test"",\n            [helper.make_tensor_value_info(\n                ""X"", TensorProto.FLOAT, (5, 7, 11))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (5, 7, 11))])\n        optimized_model = self._optimized(\n            graph, [""eliminate_deadend""], fixed)\n        assert len(optimized_model.graph.output) == 1\n        assert len(optimized_model.graph.node) == 2\n        assert optimized_model.graph.output[0].type.tensor_type.elem_type == TensorProto.FLOAT\n        assert optimized_model.graph.node[0].op_type == ""Softmax""\n        assert optimized_model.graph.node[0].attribute[0].name == ""axis""\n        assert optimized_model.graph.node[0].attribute[0].i == 2\n        assert optimized_model.graph.node[1].op_type == ""Log""\n\n    def test_deadend_elimination_simple(self):  # type: () -> None\n        self._internal_test_deadend_elimination(False)\n\n    def test_deadend_elimination_simple_fixed(self):  # type: () -> None\n        self._internal_test_deadend_elimination(True)\n\n    def test_eliminate_nop_monotone_argmax_basic_no_node_axis(self):  # type: () -> None\n        for node_name in [""Log"", ""Exp"", ""Sqrt""]:\n            for axis in range(3):\n                node = helper.make_node(node_name, [""X""], [""Y""])\n                argmax = helper.make_node(""ArgMax"", [""Y""], [""Z""], axis=axis)\n                graph = helper.make_graph(\n                    [node, argmax],\n                    ""test"",\n                    [helper.make_tensor_value_info(\n                        ""X"", TensorProto.FLOAT, (5, 7, 11))],\n                    [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (5, 7, 11))])\n                optimized_model = self._optimized(\n                    graph, [""eliminate_nop_monotone_argmax""])\n                assert len(optimized_model.graph.output) == 1\n                assert len(optimized_model.graph.node) == 1\n                assert optimized_model.graph.output[0].type.tensor_type.elem_type == TensorProto.FLOAT\n                assert optimized_model.graph.node[0].op_type == ""ArgMax""\n                assert optimized_model.graph.node[0].attribute[0].name == ""axis""\n                assert optimized_model.graph.node[0].attribute[0].i == axis\n\n    def test_eliminate_nop_monotone_argmax_basic_with_node_axis(self):  # type: () -> None\n        for node_name in [""Softmax"", ""LogSoftmax""]:\n            for axis_n in range(3):\n                for axis_max in range(3):\n                    node = helper.make_node(node_name, [""X""], [""Y""], axis=axis_n)\n                    argmax = helper.make_node(""ArgMax"", [""Y""], [""Z""], axis=axis_max)\n                    graph = helper.make_graph(\n                        [node, argmax],\n                        ""test"",\n                        [helper.make_tensor_value_info(\n                            ""X"", TensorProto.FLOAT, (5, 7, 11))],\n                        [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (5, 7, 11))])\n                    optimized_model = self._optimized(\n                        graph, [""eliminate_nop_monotone_argmax""])\n                    if axis_max == axis_n:\n                        assert len(optimized_model.graph.output) == 1\n                        assert len(optimized_model.graph.node) == 1\n                        assert optimized_model.graph.output[0].type.tensor_type.elem_type == TensorProto.FLOAT\n                        assert optimized_model.graph.node[0].op_type == ""ArgMax""\n                        assert optimized_model.graph.node[0].attribute[0].name == ""axis""\n                        assert optimized_model.graph.node[0].attribute[0].i == axis_max\n                    else:\n                        assert optimized_model.graph == graph\n\n    def test_eliminate_nop_monotone_argmax_multiple_out(self):  # type: () -> None\n        for node_name in [""Log"", ""Exp"", ""Sqrt""]:\n            for axis in range(3):\n                node = helper.make_node(node_name, [""X""], [""Y""])\n                node2 = helper.make_node(node_name, [""Y""], [""Z1""])\n                argmax = helper.make_node(""ArgMax"", [""Y""], [""Z""], axis=axis)\n                graph = helper.make_graph(\n                    [node, node2, argmax],\n                    ""test"",\n                    [helper.make_tensor_value_info(\n                        ""X"", TensorProto.FLOAT, (5, 7, 11))],\n                    [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (5, 7, 11)),\n                     helper.make_tensor_value_info(""Z1"", TensorProto.FLOAT, (5, 7, 11))])\n                optimized_model = self._optimized(\n                    graph, [""eliminate_nop_monotone_argmax""])\n                assert optimized_model.graph == graph\n\n    def test_eliminate_nop_monotone_argmax_consecutive(self):  # type: () -> None\n        def _assertion(graph, optimized_model, axis_aligned, true_axis):  # type: (GraphProto, ModelProto, bool, int) -> None\n            if axis_aligned:\n                assert len(optimized_model.graph.output) == 1\n                assert len(optimized_model.graph.node) == 1\n                assert optimized_model.graph.output[0].type.tensor_type.elem_type == TensorProto.FLOAT\n                assert optimized_model.graph.node[0].op_type == ""ArgMax""\n                assert optimized_model.graph.node[0].attribute[0].name == ""axis""\n                assert optimized_model.graph.node[0].attribute[0].i == true_axis\n            else:\n                assert optimized_model.graph == graph\n        # no axis X no axis test\n        for node_name_0 in [""Log"", ""Exp"", ""Sqrt""]:\n            for node_name_1 in [""Log"", ""Exp"", ""Sqrt""]:\n                for axis in range(3):\n                    node = helper.make_node(node_name_0, [""X""], [""Y""])\n                    node2 = helper.make_node(node_name_1, [""Y""], [""Y1""])\n                    argmax = helper.make_node(""ArgMax"", [""Y1""], [""Z""], axis=axis)\n                    graph = helper.make_graph(\n                        [node, node2, argmax],\n                        ""test"",\n                        [helper.make_tensor_value_info(\n                            ""X"", TensorProto.FLOAT, (5, 7, 11))],\n                        [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (5, 7, 11))])\n                    optimized_model = self._optimized(\n                        graph, [""eliminate_nop_monotone_argmax""], True)\n                    _assertion(graph, optimized_model, True, axis)\n        # no axis X axis test\n        for node_name_0 in [""Log"", ""Exp"", ""Sqrt""]:\n            for node_name_1 in [""Softmax"", ""LogSoftmax""]:\n                for axis_0 in range(3):\n                    for axis_1 in range(3):\n                        node = helper.make_node(node_name_0, [""X""], [""Y""])\n                        node2 = helper.make_node(node_name_1, [""Y""], [""Y1""], axis=axis_0)\n                        argmax = helper.make_node(""ArgMax"", [""Y1""], [""Z""], axis=axis_1)\n                        graph = helper.make_graph(\n                            [node, node2, argmax],\n                            ""test"",\n                            [helper.make_tensor_value_info(\n                                ""X"", TensorProto.FLOAT, (5, 7, 11))],\n                            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (5, 7, 11))])\n                        optimized_model = self._optimized(\n                            graph, [""eliminate_nop_monotone_argmax""], True)\n                        _assertion(graph, optimized_model, axis_0 == axis_1, axis_1)\n        # axis X axis test\n        for node_name_0 in [""Softmax"", ""LogSoftmax""]:\n            for node_name_1 in [""Softmax"", ""LogSoftmax""]:\n                for axis_0 in range(3):\n                    for axis_1 in range(3):\n                        for axis_2 in range(3):\n                            node = helper.make_node(node_name_0, [""X""], [""Y""], axis=axis_0)\n                            node2 = helper.make_node(node_name_1, [""Y""], [""Y1""], axis=axis_1)\n                            argmax = helper.make_node(""ArgMax"", [""Y1""], [""Z""], axis=axis_2)\n                            graph = helper.make_graph(\n                                [node, node2, argmax],\n                                ""test"",\n                                [helper.make_tensor_value_info(\n                                    ""X"", TensorProto.FLOAT, (5, 7, 11))],\n                                [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (5, 7, 11))])\n                            optimized_model = self._optimized(\n                                graph, [""eliminate_nop_monotone_argmax""], True)\n                            if axis_0 == axis_1:  # we can reduce both of the monotonic ops\n                                _assertion(graph, optimized_model, axis_1 == axis_2, axis_2)\n                            elif axis_1 == axis_2:  # we can reduce one of the monotonic ops\n                                assert len(optimized_model.graph.output) == 1\n                                assert len(optimized_model.graph.node) == 2\n                                assert optimized_model.graph.output[0].type.tensor_type.elem_type == TensorProto.FLOAT\n                                assert optimized_model.graph.node[-1].op_type == ""ArgMax""\n                                assert optimized_model.graph.node[-1].attribute[0].name == ""axis""\n                                assert optimized_model.graph.node[-1].attribute[0].i == axis_2\n                            else:  # we can\'t reduce anything\n                                assert optimized_model.graph == graph\n\n    def test_eliminate_nop_dropout(self):  # type: () -> None\n        node = helper.make_node(""Dropout"", [""X""], [""Y""])\n        node1 = helper.make_node(""Log"", [""Y""], [""Z""])\n        graph = helper.make_graph(\n            [node, node1],\n            ""test"",\n            [helper.make_tensor_value_info(\n                ""X"", TensorProto.FLOAT, (5, 7))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (5, 7))])\n        optimized_model = self._optimized(\n            graph, [""eliminate_nop_dropout""], False)\n\n        # we don\'t want to eliminate the dropoutin opset 12,\n        # even when it\';s an optional parameter (defaults to 0)\n        assert optimized_model.graph == graph\n\n    def test_eliminate_nop_dropout_opset11_graph_output(self):  # type: () -> None\n        node = helper.make_node(""Log"", [""X""], [""Y""])\n        node1 = helper.make_node(""Dropout"", [""Y""], [""Z""], ratio=0.0)\n        graph = helper.make_graph(\n            [node, node1],\n            ""test"",\n            [helper.make_tensor_value_info(\n                ""X"", TensorProto.FLOAT, (5, 7))],\n            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (5, 7))])\n        optimized_model = self._optimized(\n            graph, [""eliminate_nop_dropout""], False, opset_imports=[helper.make_opsetid("""", 11)])\n\n        assert len(optimized_model.graph.output) == 1\n        assert len(optimized_model.graph.node) == 1\n        assert optimized_model.graph.node[0].op_type == ""Log""\n\n    def test_eliminate_nop_dropout_opset11(self):  # type: () -> None\n        for ratio in [0.0, 0.5]:\n            node = helper.make_node(""Dropout"", [""X""], [""Y""], ratio=ratio)\n            node1 = helper.make_node(""Log"", [""Y""], [""Z""])\n            graph = helper.make_graph(\n                [node, node1],\n                ""test"",\n                [helper.make_tensor_value_info(\n                    ""X"", TensorProto.FLOAT, (5, 7))],\n                [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (5, 7))])\n            optimized_model = self._optimized(\n                graph, [""eliminate_nop_dropout""], False, opset_imports=[helper.make_opsetid("""", 11)])\n\n            if ratio > 0.0:\n                assert optimized_model.graph == graph\n            else:\n                assert len(optimized_model.graph.output) == 1\n                assert len(optimized_model.graph.node) == 1\n                assert optimized_model.graph.node[0].op_type == ""Log""\n\n    def test_fuse_reduction_unsqueeze(self):  # type: () -> None\n        def _calculate_post_transform_shape(input_shape, reduction_axes, unsqueeze_axes, keepdim):  # type: (Tuple[int, ...], List[int], List[int], bool) -> Tuple[int, ...]\n            post_reduce_shape = None\n            if keepdim:\n                post_reduce_shape = tuple([(x if i not in reduction_axes else 1) for i, x in enumerate(input_shape)])\n            else:\n                post_reduce_shape = tuple([x for i, x in enumerate(input_shape) if i not in reduction_axes])\n            post_unsqueeze_shape = list(post_reduce_shape)\n            for ax in unsqueeze_axes:\n                post_unsqueeze_shape.insert(ax, 1)\n            return tuple(post_unsqueeze_shape)\n\n        for reduction in [""ReduceL1"", ""ReduceL2"", ""ReduceLogSum"",\n                          ""ReduceLogSumExp"", ""ReduceMax"", ""ReduceMean"",\n                          ""ReduceMin"", ""ReduceProd"", ""ReduceSum"", ""ReduceSumSquare""]:\n            for axes1 in [[1], [1, 2], [2]]:\n                for axes2 in [[1], [1, 2], [2]]:\n                    for keepdim in [False, True]:\n                        input_shape = (5, 7, 9)\n                        output_shape = _calculate_post_transform_shape(input_shape, axes1, axes2, keepdim)  # type: Tuple[int, ...]\n                        node = helper.make_node(reduction, [""X""], [""Y""], axes=axes1, keepdims=keepdim)\n                        node1 = helper.make_node(""Unsqueeze"", [""Y""], [""Z""], axes=axes2)\n                        graph = helper.make_graph(\n                            [node, node1],\n                            ""test"",\n                            [helper.make_tensor_value_info(\n                                ""X"", TensorProto.FLOAT, input_shape)],\n                            [helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, output_shape)])\n                        optimized_model = self._optimized(\n                            graph, [""fuse_consecutive_reduce_unsqueeze""], False)\n\n                        if keepdim or axes1 != axes2:\n                            assert optimized_model.graph == graph\n                        else:\n                            assert len(optimized_model.graph.output) == 1\n                            assert len(optimized_model.graph.node) == 1\n                            assert optimized_model.graph.output[0].type.tensor_type.elem_type == TensorProto.FLOAT\n                            assert optimized_model.graph.node[-1].op_type == reduction\n                            assert optimized_model.graph.node[-1].attribute[0].name == ""axes""\n                            assert optimized_model.graph.node[-1].attribute[0].ints == axes1\n                            optimized_output_shape = tuple(x.dim_value for x in optimized_model.graph.output[0].type.tensor_type.shape.dim)\n                            assert optimized_output_shape == output_shape\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
onnx/test/relu_test.py,0,"b""import unittest\n\nfrom onnx import defs, helper\n\n\nclass TestRelu(unittest.TestCase):\n\n    def test_relu(self):  # type: () -> None\n        self.assertTrue(defs.has('Relu'))\n        helper.make_node(\n            'Relu', ['X'], ['Y'])\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
onnx/test/schema_test.py,0,"b'import unittest\n\nfrom onnx import defs, AttributeProto\n\n\nclass TestSchema(unittest.TestCase):\n\n    def test_get_schema(self):  # type: () -> None\n        defs.get_schema(""Relu"")\n\n    def test_typecheck(self):  # type: () -> None\n        defs.get_schema(""Conv"")\n\n    def test_attr_default_value(self):  # type: () -> None\n        v = defs.get_schema(\n            ""BatchNormalization"").attributes[\'epsilon\'].default_value\n        self.assertEqual(type(v), AttributeProto)\n        self.assertEqual(v.type, AttributeProto.FLOAT)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
onnx/test/shape_inference_test.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom onnx import checker, helper, TensorProto, NodeProto, GraphProto, ValueInfoProto, ModelProto, ONNX_ML, SparseTensorProto\nfrom onnx.defs import ONNX_DOMAIN, ONNX_ML_DOMAIN, AI_ONNX_PREVIEW_TRAINING_DOMAIN\nfrom onnx.helper import make_node, make_tensor, make_tensor_value_info, make_empty_tensor_value_info, make_opsetid, make_sequence_value_info\nfrom typing import Sequence, Union, Text, Tuple, List, Any, Optional\nimport onnx.shape_inference\nimport unittest\nimport os\nimport numpy as np  # type: ignore\n\n\nclass TestShapeInference(unittest.TestCase):\n    def _make_graph(self,\n                    seed_values,  # type: Sequence[Union[Text, Tuple[Text, TensorProto.DataType, Any]]]\n                    nodes,  # type: List[NodeProto]\n                    value_info,  # type: List[ValueInfoProto]\n                    initializer=None  # type: Optional[Sequence[TensorProto]]\n                    ):  # type: (...) -> GraphProto\n        if initializer is None:\n            initializer = []\n        names_in_initializer = set(x.name for x in initializer)\n        input_value_infos = []\n        # If the starting values are not also initializers,\n        # introduce the starting values as the output of reshape,\n        # so that the sizes are guaranteed to be unknown\n        for seed_value in seed_values:\n            if isinstance(seed_value, tuple):\n                seed_name = seed_value[0]\n                seed_value_info = make_tensor_value_info(*seed_value)\n            else:\n                seed_name = seed_value\n                seed_value_info = make_empty_tensor_value_info(seed_value)\n\n            if seed_name in names_in_initializer:\n                input_value_infos.append(seed_value_info)\n            else:\n                value_info.append(seed_value_info)\n                input_value_infos.append(make_tensor_value_info(\'SEED_\' + seed_name, TensorProto.UNDEFINED, ()))\n                input_value_infos.append(make_tensor_value_info(\'UNKNOWN_SHAPE_\' + seed_name, TensorProto.UNDEFINED, ()))\n                nodes[:0] = [make_node(""Reshape"", [\'SEED_\' + seed_name, \'UNKNOWN_SHAPE_\' + seed_name], [seed_name])]\n        return helper.make_graph(nodes, ""test"", input_value_infos, [], initializer=initializer, value_info=value_info)\n\n    def _inferred(self, graph, **kwargs):  # type: (GraphProto, **Any) -> ModelProto\n        kwargs[str(\'producer_name\')] = \'onnx-test\'\n        orig_model = helper.make_model(graph, **kwargs)\n        inferred_model = onnx.shape_inference.infer_shapes(orig_model)\n        checker.check_model(inferred_model)\n        return inferred_model\n\n    def _assert_inferred(self, graph, vis, **kwargs):  # type: (GraphProto, List[ValueInfoProto], **Any) -> None\n        names_in_vis = set(x.name for x in vis)\n        vis = list(x for x in graph.value_info if x.name not in names_in_vis) + vis\n        inferred_model = self._inferred(graph, **kwargs)\n        inferred_vis = list(inferred_model.graph.value_info)\n        vis = list(sorted(vis, key=lambda x: x.name))\n        inferred_vis = list(sorted(inferred_vis, key=lambda x: x.name))\n        if vis == inferred_vis:\n            return\n        # otherwise some custom logic to give a nicer diff\n        vis_names = set(x.name for x in vis)\n        inferred_vis_names = set(x.name for x in inferred_vis)\n        assert vis_names == inferred_vis_names, (vis_names, inferred_vis_names)\n        for vi, inferred_vi in zip(vis, inferred_vis):\n            assert vi == inferred_vi, \'\\n%s\\n%s\\n\' % (vi, inferred_vi)\n        assert False\n\n    def test_empty_graph(self):  # type: () -> None\n        graph = self._make_graph(\n            [\'y\'],\n            [], [])\n        self._assert_inferred(graph, [])\n\n    def _identity_prop(self, op, **kwargs):  # type: (Text, **Any) -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (30, 4, 5))],\n            [make_node(op, \'x\', \'y\', **kwargs)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (30, 4, 5))])\n\n    def test_transpose(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""X"", TensorProto.FLOAT, (2, 3, 4))],\n            [make_node(""Transpose"", [""X""], [""Y""], perm=[1, 0, 2])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(""Y"", TensorProto.FLOAT, (3, 2, 4))])\n\n    def test_transpose_preexisting(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""X"", TensorProto.FLOAT, (2, 3, 4))],\n            [make_node(""Transpose"", [""X""], [""Y""], perm=[1, 0, 2])],\n            [make_tensor_value_info(""Y"", TensorProto.FLOAT, None)])\n        self._assert_inferred(graph, [make_tensor_value_info(""Y"", TensorProto.FLOAT, (3, 2, 4))])\n\n    def test_transpose_partial(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""X"", TensorProto.FLOAT, (2, 3, 4))],\n            [make_node(""Transpose"", [""X""], [""Y""], perm=[1, 0, 2])],\n            [make_tensor_value_info(""Y"", TensorProto.UNDEFINED, (3, ""a"", ""b""))])  # type: ignore\n        self._assert_inferred(graph, [make_tensor_value_info(""Y"", TensorProto.FLOAT, (3, 2, 4))])\n\n    def test_transpose_preexisting_incorrect_shape(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""X"", TensorProto.FLOAT, (2, 3, 4))],\n            [make_node(""Transpose"", [""X""], [""Y""], perm=[1, 0, 2])],\n            [make_tensor_value_info(""Y"", TensorProto.FLOAT, (5, 5, 5))])\n        self.assertRaises(RuntimeError, self._inferred, graph)\n\n    def test_transpose_preexisting_incorrect_type(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""X"", TensorProto.FLOAT, (2, 3, 4))],\n            [make_node(""Transpose"", [""X""], [""Y""], perm=[1, 0, 2])],\n            [make_tensor_value_info(""Y"", TensorProto.STRING, (3, 2, 4))])\n        self.assertRaises(RuntimeError, self._inferred, graph)\n\n    def _make_matmul_test_all_dims_known(self, shape1, shape2):  # type: (Sequence[int], Sequence[int]) -> None\n        expected_out_shape = np.matmul(np.arange(np.product(shape1)).reshape(shape1),\n                                       np.arange(np.product(shape2)).reshape(shape2)).shape\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, shape1),\n             (\'y\', TensorProto.FLOAT, shape2)],\n            [make_node(\'MatMul\', [\'x\', \'y\'], [\'z\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, expected_out_shape)])\n\n    def test_matmul_all_dims_known(self):  # type: () -> None\n        self._make_matmul_test_all_dims_known((2,), (2,))\n\n        self._make_matmul_test_all_dims_known((4, 2), (2, 4))\n        self._make_matmul_test_all_dims_known((5, 2), (2, 4))\n        self._make_matmul_test_all_dims_known((5, 2), (2, 1))\n        self._make_matmul_test_all_dims_known((1, 2), (2, 3))\n        self._make_matmul_test_all_dims_known((2,), (2, 3))\n        self._make_matmul_test_all_dims_known((4, 2), (2,))\n        self._make_matmul_test_all_dims_known((1, 4, 2), (3, 2, 3))\n        self._make_matmul_test_all_dims_known((3, 4, 2), (3, 2, 3))\n        self._make_matmul_test_all_dims_known((5, 1, 4, 2), (1, 3, 2, 3))\n        self._make_matmul_test_all_dims_known((4, 2), (3, 2, 3))\n\n    def _make_matmul_test_allow_unknown(self, shape1, shape2, expected_out_shape):  # type: (Any, Any, Any) -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, shape1),\n             (\'y\', TensorProto.FLOAT, shape2)],\n            [make_node(\'MatMul\', [\'x\', \'y\'], [\'z\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, expected_out_shape)])\n\n    def test_matmul_allow_unknown(self):  # type: () -> None\n        self._make_matmul_test_allow_unknown((None,), (None,), ())\n        self._make_matmul_test_allow_unknown((3,), (None,), ())\n        self._make_matmul_test_allow_unknown((2,), (2, ""a""), (""a"",))\n        self._make_matmul_test_allow_unknown((4, 2), (2, ""a""), (4, ""a""))\n        self._make_matmul_test_allow_unknown((4, None), (2, ""a""), (4, ""a""))\n        self._make_matmul_test_allow_unknown((4, None), (None, ""a""), (4, ""a""))\n        self._make_matmul_test_allow_unknown((1, 4, 2), (""a"", 2, 5), (""a"", 4, 5))\n        self._make_matmul_test_allow_unknown((1, 3, 4, 2), (""a"", 2, 5), (1, 3, 4, 5))\n        self._make_matmul_test_allow_unknown((3,), None, None)\n        self._make_matmul_test_allow_unknown(None, None, None)\n\n    def test_cast(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""x"", TensorProto.FLOAT, (2, 4, 3))],\n            [make_node(""Cast"", [""x""], [""y""], to=TensorProto.UINT8)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(""y"", TensorProto.UINT8, (2, 4, 3))])\n\n    def test_concat(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""x"", TensorProto.FLOAT, (2, 4, 3)),\n             (""y"", TensorProto.FLOAT, (7, 4, 3))],\n            [make_node(""Concat"", [\'x\', \'y\'], [\'z\'], axis=0)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, (9, 4, 3))])\n\n    def test_concat_missing_shape(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""x"", TensorProto.FLOAT, (2, 4, 3)),\n             ""y"",\n             (""z"", TensorProto.FLOAT, (None, None, None))],\n            [make_node(""Concat"", [\'x\', \'y\', \'z\'], [\'out\'], axis=0)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'out\', TensorProto.FLOAT, None)])\n\n    def test_concat_3d_axis_2(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (2, 2, 2)),\n             (\'y\', TensorProto.FLOAT, (2, 2, 2))],\n            [make_node(\'Concat\', [\'x\', \'y\'], [\'z\'], axis=2)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, (2, 2, 4))])\n\n    def test_concat_param(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""x"", TensorProto.FLOAT, (""a"", 2)),\n             (""y"", TensorProto.FLOAT, (""a"", 3))],\n            [make_node(""Concat"", [\'x\', \'y\'], [\'z\'], axis=1)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, (""a"", 5))])\n\n    def test_concat_param_single_input(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""x"", TensorProto.FLOAT, (""a"", 2))],\n            [make_node(""Concat"", [\'x\'], [\'z\'], axis=0)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, (""a"", 2))])\n\n    def test_reshape_dynamic_shape(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.UINT8, (2, 4, 3)),\n             (\'shape\', TensorProto.UNDEFINED, (2,))],\n            [make_node(""Reshape"", [\'x\', \'shape\'], [\'y\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.UINT8, None)])\n\n    def test_reshape_static_shape(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.UINT8, (2, 4, 3)),\n             (\'shape\', TensorProto.INT64, (2,))],\n            [make_node(""Reshape"", [\'x\', \'shape\'], [\'y\'])],\n            [],\n            initializer=[make_tensor(\'shape\', TensorProto.INT64, (2,), (3, 8))])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.UINT8, (3, 8))])\n\n    def test_reshape_static_shape_inferred(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.UINT8, (2, 4, 3)),\n             (\'shape\', TensorProto.INT64, (3,))],\n            [make_node(""Reshape"", [\'x\', \'shape\'], [\'y\'])],\n            [],\n            initializer=[make_tensor(\'shape\', TensorProto.INT64, (3,), (0, 3, -1))])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.UINT8, (2, 3, 4))])\n\n    def test_reshape_static_shape_constant(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.UINT8, (2, 4, 3))],\n            [make_node(""Constant"", [], [\'shape\'],\n                       value=make_tensor(\'shape\', TensorProto.INT64, (2,), (3, 8))),\n             make_node(""Reshape"", [\'x\', \'shape\'], [\'y\'])],\n            [])\n        self._assert_inferred(graph, [\n            make_tensor_value_info(\'shape\', TensorProto.INT64, (2,)),\n            make_tensor_value_info(\'y\', TensorProto.UINT8, (3, 8))])\n\n    def test_upsample(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.INT32, (2, 4, 3, 5)),\n             (\'scales\', TensorProto.FLOAT, (4,))],\n            [make_node(""Upsample"", [\'x\', \'scales\'], [\'y\'])],\n            [],\n            initializer=[make_tensor(\'scales\', TensorProto.FLOAT, (4,), (1.0, 1.1, 1.3, 1.9))])\n        self._assert_inferred(\n            graph,\n            [make_tensor_value_info(\'y\', TensorProto.INT32, (2, 4, 3, 9))],\n            opset_imports=[helper.make_opsetid(ONNX_DOMAIN, 9)])\n\n    def test_upsample_raw_data(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.INT32, (2, 4, 3, 5)),\n             (\'scales\', TensorProto.FLOAT, (4,))],\n            [make_node(""Upsample"", [\'x\', \'scales\'], [\'y\'])],\n            [],\n            initializer=[make_tensor(\'scales\', TensorProto.FLOAT, (4,),\n                                     vals=np.array([1.0, 1.1, 1.3, 1.9], dtype=\'<f4\').tobytes(), raw=True)])  # Feed raw bytes (force little endian ordering like onnx standard) for test purpose\n        self._assert_inferred(\n            graph,\n            [make_tensor_value_info(\'y\', TensorProto.INT32, (2, 4, 3, 9))],\n            opset_imports=[helper.make_opsetid(ONNX_DOMAIN, 9)])\n\n    def test_upsample_raw_data_v7(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.INT32, (1, 3, 4, 5))],\n            [make_node(""Upsample"", [\'x\'], [\'y\'], scales=[2.0, 1.1, 2.3, 1.9])],\n            [])\n        self._assert_inferred(\n            graph,\n            [make_tensor_value_info(\'y\', TensorProto.INT32, (2, 3, 9, 9))],\n            opset_imports=[helper.make_opsetid(ONNX_DOMAIN, 7)])\n\n    def test_expand(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.INT32, (3, 1)),\n             (\'shape\', TensorProto.INT64, (3,))],\n            [make_node(""Expand"", [\'x\', \'shape\'], [\'y\'])],\n            [],\n            initializer=[make_tensor(\'shape\', TensorProto.INT64, (3,), (2, 1, 6))])\n        self._assert_inferred(\n            graph,\n            [make_tensor_value_info(\'y\', TensorProto.INT32, (2, 3, 6))])\n\n    def test_expand_scalar_input(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.INT32, ()),\n             (\'shape\', TensorProto.INT64, (2,))],\n            [make_node(""Expand"", [\'x\', \'shape\'], [\'y\'])],\n            [],\n            initializer=[make_tensor(\'shape\', TensorProto.INT64, (2,), (4, 8))])\n        self._assert_inferred(\n            graph,\n            [make_tensor_value_info(\'y\', TensorProto.INT32, (4, 8))])\n\n    def test_expand_raw_data(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.INT32, (3, 1)),\n             (\'shape\', TensorProto.INT64, (2,))],\n            [make_node(""Expand"", [\'x\', \'shape\'], [\'y\'])],\n            [],\n            initializer=[make_tensor(\'shape\', TensorProto.INT64, (2,),\n                                     vals=np.array([3, 4], dtype=\'<i8\').tobytes(), raw=True)])  # Feed raw bytes (force little endian ordering like onnx standard) for test purpose\n        self._assert_inferred(\n            graph,\n            [make_tensor_value_info(\'y\', TensorProto.INT32, (3, 4))])\n\n    def test_resize_size(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.INT32, (2, 4, 3, 5)),\n             (\'roi\', TensorProto.FLOAT, (8,)),\n             (\'scales\', TensorProto.FLOAT, (4,)),\n             (\'sizes\', TensorProto.INT64, (4,))],\n            [make_node(""Resize"", [\'x\', \'roi\', \'scales\', \'sizes\'], [\'y\'])],\n            [],\n            initializer=[make_tensor(\'sizes\', TensorProto.INT64, (4,), (3, 5, 6, 7))])\n        self._assert_inferred(\n            graph,\n            [make_tensor_value_info(\'y\', TensorProto.INT32, (3, 5, 6, 7))])\n\n    def test_resize_scale(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.INT32, (2, 4, 3, 5)),\n             (\'roi\', TensorProto.FLOAT, (8,)),\n             (\'scales\', TensorProto.FLOAT, (4,))],\n            [make_node(""Resize"", [\'x\', \'roi\', \'scales\'], [\'y\'])],\n            [],\n            initializer=[make_tensor(\'scales\', TensorProto.FLOAT, (4,), (1.0, 1.1, 1.3, 1.9))])\n        self._assert_inferred(\n            graph,\n            [make_tensor_value_info(\'y\', TensorProto.INT32, (2, 4, 3, 9))])\n\n    def test_resize_scale_raw_data(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.INT32, (1, 3, 4, 5)),\n             (\'roi\', TensorProto.FLOAT, (8,)),\n             (\'scales\', TensorProto.FLOAT, (4,))],\n            [make_node(""Resize"", [\'x\', \'roi\', \'scales\'], [\'y\'])],\n            [],\n            initializer=[make_tensor(\'scales\', TensorProto.FLOAT, (4,),\n                                     vals=np.array([2.0, 1.1, 2.3, 1.9], dtype=\'<f4\').tobytes(), raw=True)])\n        self._assert_inferred(\n            graph,\n            [make_tensor_value_info(\'y\', TensorProto.INT32, (2, 3, 9, 9))])\n\n    def test_shape(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (2, 4, 3))],\n            [make_node(""Shape"", [\'x\'], [\'y\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.INT64, (3,))])\n\n    def test_size(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (2, 4, 3))],\n            [make_node(""Size"", [\'x\'], [\'y\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.INT64, ())])\n\n    def test_gather(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (4, 3)),\n             (\'i\', TensorProto.INT64, (2,))],\n            [make_node(""Gather"", [\'x\', \'i\'], [\'y\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (2, 3))])  # type: ignore\n\n    def test_gather_axis1(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (4, 3, 5)),\n             (\'i\', TensorProto.INT64, (1, 2))],\n            [make_node(""Gather"", [\'x\', \'i\'], [\'y\'], axis=1)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (4, 1, 2, 5))])  # type: ignore\n\n    def test_gather_into_scalar(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (3,)),\n             (\'i\', TensorProto.INT64, ())],\n            [make_node(""Gather"", [\'x\', \'i\'], [\'y\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, ())])\n\n    def test_gather_elements(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (2, 2)),\n             (\'i\', TensorProto.INT64, (2, 2))],\n            [make_node(""GatherElements"", [\'x\', \'i\'], [\'y\'], axis=1)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (2, 2))])  # type: ignore\n\n    def test_gather_elements_axis0(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (3, 3)),\n             (\'i\', TensorProto.INT64, (2, 3))],\n            [make_node(""GatherElements"", [\'x\', \'i\'], [\'y\'], axis=0)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (2, 3))])  # type: ignore\n\n    def test_scatter(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (3, 3)),\n             (\'i\', TensorProto.INT64, (2, 3)),\n             (\'u\', TensorProto.FLOAT, (2, 3))],\n            [make_node(""Scatter"", [\'x\', \'i\', \'u\'], [\'y\'])],\n            [])\n        self._assert_inferred(\n            graph,\n            [make_tensor_value_info(\'y\', TensorProto.FLOAT, (3, 3))],\n            opset_imports=[helper.make_opsetid(ONNX_DOMAIN, 10)])  # type: ignore\n\n    def test_scatter_axis1(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (1, 5)),\n             (\'i\', TensorProto.INT64, (1, 2)),\n             (\'u\', TensorProto.FLOAT, (1, 2))],\n            [make_node(""Scatter"", [\'x\', \'i\', \'u\'], [\'y\'], axis=1)],\n            [])\n        self._assert_inferred(\n            graph,\n            [make_tensor_value_info(\'y\', TensorProto.FLOAT, (1, 5))],\n            opset_imports=[helper.make_opsetid(ONNX_DOMAIN, 10)])  # type: ignore\n\n    def test_scatter_elements(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (3, 3)),\n             (\'i\', TensorProto.INT64, (2, 3)),\n             (\'u\', TensorProto.FLOAT, (2, 3))],\n            [make_node(""ScatterElements"", [\'x\', \'i\', \'u\'], [\'y\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (3, 3))])  # type: ignore\n\n    def test_scatter_elements_axis1(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (1, 5)),\n             (\'i\', TensorProto.INT64, (1, 2)),\n             (\'u\', TensorProto.FLOAT, (1, 2))],\n            [make_node(""ScatterElements"", [\'x\', \'i\', \'u\'], [\'y\'], axis=1)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (1, 5))])  # type: ignore\n\n    def test_scatternd(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (4, 5, 6)),\n             (\'indices\', TensorProto.INT64, (3, 3, 2)),\n             (\'updates\', TensorProto.FLOAT, (3, 3, 6))],\n            [make_node(""ScatterND"", [\'x\', \'indices\', \'updates\'], [\'y\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (4, 5, 6))])  # type: ignore\n\n    def test_scatternd_noshape(self):  # type: () -> None\n        # The shape of \'x_reshaped\' cannot be inferred, since it is the output of a dynamic reshape.\n        # Thus the shape of \'y\' is also None.\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (4, 5, 6)),\n             (\'indices\', TensorProto.INT64, (3, 3, 2)),\n             (\'updates\', TensorProto.FLOAT, (3, 3, 6)),\n             (\'shape\', TensorProto.UNDEFINED, (2,))],\n            [make_node(""Reshape"", [\'x\', \'shape\'], [\'x_reshaped\']),\n             make_node(""ScatterND"", [\'x_reshaped\', \'indices\', \'updates\'], [\'y\'])],\n            [])\n        self._assert_inferred(graph, [\n            make_tensor_value_info(\'x_reshaped\', TensorProto.FLOAT, None),\n            make_tensor_value_info(\'y\', TensorProto.FLOAT, None)])  # type: ignore\n\n    def test_squeeze(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (1, 3, 1, 1, 2, 1))],\n            [make_node(\'Squeeze\', \'x\', \'y\', axes=[0, 2, 3, 5])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (3, 2))])\n\n    def test_unsqueeze_regular(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (3, 2))],\n            [make_node(\'Unsqueeze\', \'x\', \'y\', axes=[0, 1, 3, 5])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (1, 1, 3, 1, 2, 1))])\n\n    def test_unsqueeze_unsorted_axes(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (3, 4, 5))],\n            [make_node(\'Unsqueeze\', \'x\', \'y\', axes=[4, 0])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (1, 3, 4, 5, 1))])\n\n    def test_unsqueeze_negative_axes(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (3, 4, 5))],\n            [make_node(\'Unsqueeze\', \'x\', \'y\', axes=[0, -1])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (1, 3, 4, 5, 1))])\n\n    def test_slice_without_input_shape(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (3, 2)), (\'starts\', TensorProto.INT64, (1,)), (\'ends\', TensorProto.INT64, (1,))],\n            [make_node(\'Slice\', [\'x\', \'starts\', \'ends\'], [\'y\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, None)])\n\n    def test_slice_with_input_shape(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (3, 2)), (\'starts\', TensorProto.INT64, (2, )), (\'ends\', TensorProto.INT64, (2, ))],\n            [make_node(\'Slice\', [\'x\', \'starts\', \'ends\'], [\'y\'])],\n            [],\n            initializer=[make_tensor(\'starts\', TensorProto.INT64, (2, ),\n                                      vals=np.array([1, 0], dtype=\'<i8\').tobytes(), raw=True),  # Feed raw bytes (force little endian ordering like onnx standard) for test purpose\n                         make_tensor(\'ends\', TensorProto.INT64, (2, ), (2, 2))])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (1, 2))])\n\n    def test_slice_with_input_shape_containing_dim_params(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (1, \'a\', 1)),\n             (\'starts\', TensorProto.INT64, (3,)),\n             (\'ends\', TensorProto.INT64, (3,))],\n            [make_node(\'Slice\', [\'x\', \'starts\', \'ends\'], [\'y\'])],\n            [],\n            initializer=[make_tensor(\'starts\', TensorProto.INT64, (3,), (0, 0, 0)),\n                            make_tensor(\'ends\', TensorProto.INT64, (3,), (1, 1, 1))])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (1, None, 1))])  # type: ignore\n\n    def test_slice_with_input_shape_steps(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (5, 6, 7)),\n             (\'starts\', TensorProto.INT64, (3,)),\n             (\'ends\', TensorProto.INT64, (3,)),\n             (\'axes\'),\n             (\'steps\', TensorProto.INT64, (3,))],\n            [make_node(\'Slice\', [\'x\', \'starts\', \'ends\', \'axes\', \'steps\'], [\'y\'])],\n            [],\n            initializer=[make_tensor(\'starts\', TensorProto.INT64, (3,), (1, 0, 0)),\n                         make_tensor(\'ends\', TensorProto.INT64, (3,), (2, 6, 6)),\n                         make_tensor(\'steps\', TensorProto.INT64, (3,), (1, 4, 3))])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (1, 2, 2))])\n\n    def test_slice_with_input_shape_axes(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (3, 6, 2)),\n             (\'starts\', TensorProto.INT64, (2,)),\n             (\'ends\', TensorProto.INT64, (2,)),\n             (\'axes\', TensorProto.INT64, (2,)),\n             (\'steps\')],\n            [make_node(\'Slice\', [\'x\', \'starts\', \'ends\', \'axes\', \'steps\'], [\'y\'])],\n            [],\n            initializer=[make_tensor(\'starts\', TensorProto.INT64, (2,), (1, 0)),\n                         make_tensor(\'ends\', TensorProto.INT64, (2,), (2, 2)),\n                         make_tensor(\'axes\', TensorProto.INT64, (2,), (0, 2))])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (1, 6, 2))])\n\n    def test_slice_unsorted_axes(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (3, 2)),\n             (\'starts\', TensorProto.INT64, (2,)),\n             (\'ends\', TensorProto.INT64, (2,)),\n             (\'axes\', TensorProto.INT64, (2,))],\n            [make_node(\'Slice\', [\'x\', \'starts\', \'ends\', \'axes\'], \'y\')],\n            [],\n            initializer=[make_tensor(\'starts\', TensorProto.INT64, (2,), (1, 0)),\n                         make_tensor(\'ends\', TensorProto.INT64, (2,), (2, 2)),\n                         make_tensor(\'axes\', TensorProto.INT64, (2,), (1, 0))])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (2, 1))])  # can handle unsorted axes\n\n    def test_slice_giant_number(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (3, 2)),\n             (\'starts\', TensorProto.INT64, (2,)),\n             (\'ends\', TensorProto.INT64, (2,)),\n             (\'axes\', TensorProto.INT64, (2,))],\n            [make_node(\'Slice\', [\'x\', \'starts\', \'ends\', \'axes\'], \'y\')],\n            [],\n            initializer=[make_tensor(\'starts\', TensorProto.INT64, (2,), (1, 0)),\n                         make_tensor(\'ends\', TensorProto.INT64, (2,), (200, 22000)),\n                         make_tensor(\'axes\', TensorProto.INT64, (2,), (0, 1))])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (2, 2))])\n\n    def test_slice_giant_step(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (3, 2)),\n             (\'starts\', TensorProto.INT64, (2,)),\n             (\'ends\', TensorProto.INT64, (2,)),\n             (\'axes\', TensorProto.INT64, (2,)),\n             (\'steps\', TensorProto.INT64, (2,))],\n            [make_node(\'Slice\', [\'x\', \'starts\', \'ends\', \'axes\', \'steps\'], \'y\')],\n            [],\n            initializer=[make_tensor(\'starts\', TensorProto.INT64, (2,), (1, 0)),\n                         make_tensor(\'ends\', TensorProto.INT64, (2,), (200, 200)),\n                         make_tensor(\'axes\', TensorProto.INT64, (2,), (0, 1)),\n                         make_tensor(\'steps\', TensorProto.INT64, (2,), (1, 200))])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (2, 1))])\n\n    def test_slice_negative_end(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (3, 2)),\n             (\'starts\', TensorProto.INT64, (2,)),\n             (\'ends\', TensorProto.INT64, (2,)),\n             (\'axes\', TensorProto.INT64, (2,))],\n            [make_node(\'Slice\', [\'x\', \'starts\', \'ends\', \'axes\'], \'y\')],\n            [],\n            initializer=[make_tensor(\'starts\', TensorProto.INT64, (2,), (1, 0)),\n                         make_tensor(\'ends\', TensorProto.INT64, (2,), (200, -1)),  # negative end means begin from end of a dimension (here end = 2 - 1 = 1)\n                         make_tensor(\'axes\', TensorProto.INT64, (2,), (0, 1))])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (2, 1))])  # type: ignore\n\n    def test_slice_negative_start(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (3, 2)),\n             (\'starts\', TensorProto.INT64, (2,)),\n             (\'ends\', TensorProto.INT64, (2,)),\n             (\'axes\', TensorProto.INT64, (2,))],\n            [make_node(\'Slice\', [\'x\', \'starts\', \'ends\', \'axes\'], \'y\')],\n            [],\n            initializer=[make_tensor(\'starts\', TensorProto.INT64, (2,), (1, -2)),  # negative start means begin from end of a dimension (here end = 2 - 2 = 0)\n                         make_tensor(\'ends\', TensorProto.INT64, (2,), (200, 3)),\n                         make_tensor(\'axes\', TensorProto.INT64, (2,), (0, 1))])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (2, 2))])  # type: ignore\n\n    def test_slice_negative_step(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (3, 4)),\n             (\'starts\', TensorProto.INT64, (2,)),\n             (\'ends\', TensorProto.INT64, (2,)),\n             (\'axes\', TensorProto.INT64, (2,)),\n             (\'steps\', TensorProto.INT64, (2,))],\n            [make_node(\'Slice\', [\'x\', \'starts\', \'ends\', \'axes\', \'steps\'], \'y\')],\n            [],\n            initializer=[make_tensor(\'starts\', TensorProto.INT64, (2,), (1, 4)),  # 4 will be clamped to 3 since we are negative stepping\n                         make_tensor(\'ends\', TensorProto.INT64, (2,), (200, 0)),\n                         make_tensor(\'axes\', TensorProto.INT64, (2,), (0, 1)),\n                         make_tensor(\'steps\', TensorProto.INT64, (2,), (1, -1))])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (2, 3))])  # type: ignore\n\n    def test_slice_variable_copy(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (""a"", 2)),\n             (\'starts\', TensorProto.INT64, (1,)),\n             (\'ends\', TensorProto.INT64, (1,)),\n             (\'axes\', TensorProto.INT64, (1,))],\n            [make_node(\'Slice\', [\'x\', \'starts\', \'ends\', \'axes\'], \'y\')],\n            [],\n            initializer=[make_tensor(\'starts\', TensorProto.INT64, (1,), (1,)),\n                         make_tensor(\'ends\', TensorProto.INT64, (1,), (200,)),\n                         make_tensor(\'axes\', TensorProto.INT64, (1,), (1,))])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (""a"", 1))])  # type: ignore\n\n    def test_slice_variable_input_types(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.DOUBLE, (3, 2)),\n             (\'starts\', TensorProto.INT32, (2,)),\n             (\'ends\', TensorProto.INT32, (2,)),\n             (\'axes\', TensorProto.INT32, (2,))],\n            [make_node(\'Slice\', [\'x\', \'starts\', \'ends\', \'axes\'], \'y\')],\n            [],\n            initializer=[make_tensor(\'starts\', TensorProto.INT32, (2,), (1, 0)),\n                         make_tensor(\'ends\', TensorProto.INT32, (2,), (200, 22000)),\n                         make_tensor(\'axes\', TensorProto.INT32, (2,), (0, 1))])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.DOUBLE, (2, 2))])\n\n    def test_conv(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (3, 4, 5, 6, 7)),\n             (\'y\', TensorProto.FLOAT, (5, 4, 2, 4, 3))],\n            [make_node(\'Conv\', [\'x\', \'y\'], \'z\', pads=[0, 1, 1, 0, 0, 1], dilations=[1, 2, 2], strides=[1, 1, 2])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, (3, 5, 4, 1, 3))])\n\n    def test_conv_1d_simple(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (30, 4, 5)),\n             (\'y\', TensorProto.FLOAT, (50, 4, 2))],\n            [make_node(\'Conv\', [\'x\', \'y\'], \'z\', dilations=[1])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, (30, 50, 4))])\n\n    def test_conv_dilations(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (30, 4, 8, 8, 8)),\n             (\'y\', TensorProto.FLOAT, (50, 4, 3, 3, 3))],\n            [make_node(\'Conv\', [\'x\', \'y\'], \'z\', dilations=[1, 2, 3])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, (30, 50, 6, 4, 2))])\n\n    def test_conv_strides(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (30, 4, 8, 8, 8)),\n             (\'y\', TensorProto.FLOAT, (50, 4, 3, 3, 3))],\n            [make_node(\'Conv\', [\'x\', \'y\'], \'z\', strides=[1, 2, 3])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, (30, 50, 6, 3, 2))])\n\n    def test_conv_pads(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (30, 4, 7, 6, 4)),\n             (\'y\', TensorProto.FLOAT, (50, 4, 3, 3, 3))],\n            [make_node(\'Conv\', [\'x\', \'y\'], \'z\', pads=[1, 1, 2, 0, 1, 2])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, (30, 50, 6, 6, 6))])\n\n    def test_conv_auto_pad(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (30, 4, 7, 6, 4)),\n             (\'y\', TensorProto.FLOAT, (50, 4, 4, 3, 2))],\n            [make_node(\'Conv\', [\'x\', \'y\'], \'z\', auto_pad=\'SAME_UPPER\')],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, (30, 50, 7, 6, 4))])\n\n    def test_conv_auto_pad_dilation(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (30, 4, 65, 64, 63)),\n             (\'y\', TensorProto.FLOAT, (50, 4, 4, 3, 2))],\n            [make_node(\'Conv\', [\'x\', \'y\'], \'z\', auto_pad=\'SAME_UPPER\', dilations=[2, 3, 4])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, (30, 50, 65, 64, 63))])\n\n    def test_conv_group(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (30, 4, 8, 8, 8)),\n             (\'y\', TensorProto.FLOAT, (4, 1, 8, 8, 8))],\n            [make_node(\'Conv\', [\'x\', \'y\'], \'z\', group=4)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, (30, 4, 1, 1, 1))])\n\n    def test_conv_only_one_pos(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (30, 4, 5)),\n             (\'y\', TensorProto.FLOAT, (50, 4, 5))],\n            [make_node(\'Conv\', [\'x\', \'y\'], \'z\', strides=[2])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, (30, 50, 1))])\n\n    def test_conv_partial_missing_shape(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (30, 4, None, 6, 4)),\n             (\'y\', TensorProto.FLOAT, (50, 4, 3, 3, 3))],\n            [make_node(\'Conv\', [\'x\', \'y\'], \'z\', pads=[1, 1, 2, 0, 1, 2])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, (30, 50, None, 6, 6))])  # type: ignore\n\n    def test_conv_partial_missing_weight_shape(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (30, 4, 7, 6, 4)),\n             (\'y\', TensorProto.FLOAT, (50, 4, None, 3, 3))],\n            [make_node(\'Conv\', [\'x\', \'y\'], \'z\', pads=[1, 1, 2, 0, 1, 2])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, None)])\n\n    def test_relu(self):  # type: () -> None\n        self._identity_prop(\'Relu\')\n\n    def test_add(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (30, 4, 5)),\n             (\'y\', TensorProto.FLOAT, (30, 4, 5))],\n            [make_node(\'Add\', [\'x\', \'y\'], \'z\')],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, (30, 4, 5))])\n\n    def test_pow(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (30, 4, 5)),\n             (\'y\', TensorProto.FLOAT, (30, 4, 5))],\n            [make_node(\'Pow\', [\'x\', \'y\'], \'z\')],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, (30, 4, 5))])\n\n    def test_bitshift(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.UINT32, (2, 3, 1)),\n             (\'y\', TensorProto.UINT32, (2, 3, 1))],\n            [make_node(\'BitShift\', [\'x\', \'y\'], \'z\', direction=""RIGHT"")],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.UINT32, (2, 3, 1))])\n\n    def test_bitshift_broadcast_to_first(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.UINT32, (16, 4, 1)),\n             (\'y\', TensorProto.UINT32, (1,))],\n            [make_node(\'BitShift\', [\'x\', \'y\'], \'z\', direction=""RIGHT"")],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.UINT32, (16, 4, 1))])\n\n    def test_bitshift_broadcast_to_second(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.UINT32, (1,)),\n             (\'y\', TensorProto.UINT32, (2, 3, 1))],\n            [make_node(\'BitShift\', [\'x\', \'y\'], \'z\', direction=""RIGHT"")],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.UINT32, (2, 3, 1))])\n\n    def test_sum_single(self):  # type: () -> None\n        self._identity_prop(\'Sum\')\n\n    def test_sum_multi(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (30, 4, 5)),\n             (\'y\', TensorProto.FLOAT, (30, 4, 5)),\n             (\'z\', TensorProto.FLOAT, (30, 4, 5))],\n            [make_node(\'Sum\', [\'x\', \'y\', \'z\'], [\'out\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'out\', TensorProto.FLOAT, (30, 4, 5))])\n\n    def test_sum_multi_broadcasting(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (30, 1, 5)),\n             (\'y\', TensorProto.FLOAT, (""a"", 4, 1)),\n             (\'z\', TensorProto.FLOAT, (4, ""b""))],\n            [make_node(\'Sum\', [\'x\', \'y\', \'z\'], [\'out\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'out\', TensorProto.FLOAT, (30, 4, 5))])\n\n    def test_sum_broadcasting_param(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (""a"", 1, 5)),\n             (\'y\', TensorProto.FLOAT, (""a"", 4, 1))],\n            [make_node(\'Sum\', [\'x\', \'y\'], [\'out\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'out\', TensorProto.FLOAT, (""a"", 4, 5))])\n\n    def test_random_normal(self):  # type: () -> None\n        graph = self._make_graph(\n            [],\n            [make_node(\'RandomNormal\', [], [\'out\'], dtype=TensorProto.DOUBLE, shape=(3, 4, 5))],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'out\', TensorProto.DOUBLE, (3, 4, 5))])\n\n    def test_random_normal_like(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""X"", TensorProto.FLOAT, (2, 3, 4))],\n            [make_node(\'RandomNormalLike\', [\'X\'], [\'out\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'out\', TensorProto.FLOAT, (2, 3, 4))])\n\n    def test_random_normal_like_with_dtype(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""X"", TensorProto.FLOAT, (2, 3, 4))],\n            [make_node(\'RandomNormalLike\', [\'X\'], [\'out\'], dtype=TensorProto.DOUBLE,)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'out\', TensorProto.DOUBLE, (2, 3, 4))])\n\n    def _logical_binary_op(self, op, input_type):  # type: (Text, TensorProto.DataType) -> None\n        graph = self._make_graph(\n            [(\'x\', input_type, (30, 4, 5)),\n             (\'y\', input_type, (30, 4, 5))],\n            [make_node(op, [\'x\', \'y\'], \'z\')],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.BOOL, (30, 4, 5))])\n\n    def _logical_binary_op_with_broadcasting(self, op, input_type):  # type: (Text, TensorProto.DataType) -> None\n        graph = self._make_graph(\n            [(\'x\', input_type, (1, 5)),\n             (\'y\', input_type, (30, 4, 5))],\n            [make_node(op, [\'x\', \'y\'], \'z\')],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.BOOL, (30, 4, 5))])\n\n    def test_logical_and(self):  # type: () -> None\n        self._logical_binary_op(\'And\', TensorProto.BOOL)\n        self._logical_binary_op_with_broadcasting(\'And\', TensorProto.BOOL)\n\n    def test_logical_or(self):  # type: () -> None\n        self._logical_binary_op(\'Or\', TensorProto.BOOL)\n        self._logical_binary_op_with_broadcasting(\'Or\', TensorProto.BOOL)\n\n    def test_logical_xor(self):  # type: () -> None\n        self._logical_binary_op(\'Xor\', TensorProto.BOOL)\n        self._logical_binary_op_with_broadcasting(\'Xor\', TensorProto.BOOL)\n\n    def test_greater(self):  # type: () -> None\n        self._logical_binary_op(\'Greater\', TensorProto.BOOL)\n        self._logical_binary_op_with_broadcasting(\'Greater\', TensorProto.BOOL)\n\n    def test_less(self):  # type: () -> None\n        self._logical_binary_op(\'Less\', TensorProto.BOOL)\n        self._logical_binary_op_with_broadcasting(\'Less\', TensorProto.BOOL)\n\n    def test_equal(self):  # type: () -> None\n        self._logical_binary_op(\'Equal\', TensorProto.BOOL)\n        self._logical_binary_op_with_broadcasting(\'Equal\', TensorProto.BOOL)\n\n    def test_logical_not(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.BOOL, (30, 4, 5))],\n            [make_node(\'Not\', [\'x\'], \'z\')],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.BOOL, (30, 4, 5))])\n\n    def test_less_or_equal(self):  # type: () -> None\n        self._logical_binary_op(\'LessOrEqual\', TensorProto.BOOL)\n        self._logical_binary_op_with_broadcasting(\'LessOrEqual\', TensorProto.BOOL)\n\n    def test_greater_or_equal(self):  # type: () -> None\n        self._logical_binary_op(\'GreaterOrEqual\', TensorProto.BOOL)\n        self._logical_binary_op_with_broadcasting(\'GreaterOrEqual\', TensorProto.BOOL)\n\n    def test_flatten(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (2, 3, 4, 5))],\n            [make_node(\'Flatten\', [\'x\'], [\'z\'], axis=2)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, (6, 20))])\n\n    def test_flatten_default_axis(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (2, 3, 4, 5))],\n            [make_node(\'Flatten\', [\'x\'], [\'z\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, (2, 60))])\n\n    def test_flatten_zero_axis(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (2, 3, 4, 5))],\n            [make_node(\'Flatten\', [\'x\'], [\'z\'], axis=0)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, (1, 120))])\n\n    def test_flatten_unknown_dim(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (2, \'N\', 4, 5))],\n            [make_node(\'Flatten\', [\'x\'], [\'z\'], axis=2)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, (None, 20))])  # type: ignore\n\n    def test_space_to_depth(self):  # type: () -> None\n        b = 10\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (2, 3, 100, 100))],\n            [make_node(\'SpaceToDepth\', [\'x\'], [\'z\'], blocksize=b)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, (2, 300, 10, 10))])\n\n    def test_space_to_depth_unknown_dim(self):  # type: () -> None\n        b = 10\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (2, \'N\', 100, 100))],\n            [make_node(\'SpaceToDepth\', [\'x\'], [\'z\'], blocksize=b)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, (2, None, 10, 10))])  # type: ignore\n\n    def test_depth_to_space(self):  # type: () -> None\n        b = 10\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (2, 300, 10, 10))],\n            [make_node(\'DepthToSpace\', [\'x\'], [\'z\'], blocksize=b, mode=\'DCR\')],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, (2, 3, 100, 100))])\n\n    def _rnn_forward(self, seqlen, batchsize, inpsize, hiddensize):  # type: (int, int, int, int) -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (seqlen, batchsize, inpsize)),\n             (\'w\', TensorProto.FLOAT, (1, hiddensize, inpsize)),\n             (\'r\', TensorProto.FLOAT, (1, hiddensize, hiddensize))],\n            [make_node(\'RNN\', [\'x\', \'w\', \'r\'], [\'all\', \'last\'], hidden_size=hiddensize)],\n            [])\n        self._assert_inferred(graph, [\n            make_tensor_value_info(\'all\', TensorProto.FLOAT, (seqlen, 1, batchsize, hiddensize)),\n            make_tensor_value_info(\'last\', TensorProto.FLOAT, (1, batchsize, hiddensize))])\n\n    def test_rnn_forward(self):  # type: () -> None\n        self._rnn_forward(64, 32, 10, 4)\n\n    def _rnn_bidirectional(self, seqlen, batchsize, inpsize, hiddensize):  # type: (int, int, int, int) -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (seqlen, batchsize, inpsize)),\n             (\'w\', TensorProto.FLOAT, (2, hiddensize, inpsize)),\n             (\'r\', TensorProto.FLOAT, (2, hiddensize, hiddensize))],\n            [make_node(\'RNN\', [\'x\', \'w\', \'r\'], [\'all\', \'last\'], hidden_size=hiddensize,\n                direction=""bidirectional"")],\n            [])\n        self._assert_inferred(graph, [\n            make_tensor_value_info(\'all\', TensorProto.FLOAT, (seqlen, 2, batchsize, hiddensize)),\n            make_tensor_value_info(\'last\', TensorProto.FLOAT, (2, batchsize, hiddensize))])\n\n    def test_rnn_bidirectional(self):  # type: () -> None\n        self._rnn_bidirectional(64, 32, 10, 4)\n\n    def _lstm_forward(self, seqlen, batchsize, inpsize, hiddensize):  # type: (int, int, int, int) -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (seqlen, batchsize, inpsize)),\n             (\'w\', TensorProto.FLOAT, (1, 4 * hiddensize, inpsize)),\n             (\'r\', TensorProto.FLOAT, (1, 4 * hiddensize, hiddensize))],\n            [make_node(\'LSTM\', [\'x\', \'w\', \'r\'], [\'all\', \'hidden\', \'last\'], hidden_size=hiddensize)],\n            [])\n        self._assert_inferred(graph, [\n            make_tensor_value_info(\'all\', TensorProto.FLOAT, (seqlen, 1, batchsize, hiddensize)),\n            make_tensor_value_info(\'hidden\', TensorProto.FLOAT, (1, batchsize, hiddensize)),\n            make_tensor_value_info(\'last\', TensorProto.FLOAT, (1, batchsize, hiddensize))])\n\n    def test_lstm_forward(self):  # type: () -> None\n        self._lstm_forward(64, 32, 10, 4)\n\n    def test_topk_default_axis(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (3, 4, 5, 10))],\n            [make_node(\'TopK\', [\'x\', \'k\'], [\'y\', \'z\'])],\n            [],\n            initializer=[make_tensor(\'k\', TensorProto.INT64, (1,), (2,))])\n        self._assert_inferred(graph,\n                              [make_tensor_value_info(\'y\', TensorProto.FLOAT, (3, 4, 5, 2)),\n                               make_tensor_value_info(\'z\', TensorProto.INT64, (3, 4, 5, 2))])\n\n    def test_topk(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (3, 4, 5, 10))],\n            [make_node(\'TopK\', [\'x\', \'k\'], [\'y\', \'z\'], axis=2)],\n            [],\n            initializer=[make_tensor(\'k\', TensorProto.INT64, (1,), (2,))])\n        self._assert_inferred(graph,\n                              [make_tensor_value_info(\'y\', TensorProto.FLOAT, (3, 4, 2, 10)),\n                               make_tensor_value_info(\'z\', TensorProto.INT64, (3, 4, 2, 10))])\n\n    def test_topk_raw_data(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (3, 4, 5, 10))],\n            [make_node(\'TopK\', [\'x\', \'k\'], [\'y\', \'z\'], axis=2)],\n            [],\n            initializer=[make_tensor(\'k\', TensorProto.INT64, (1,),\n                                      vals=np.array([3], dtype=\'<i8\').tobytes(), raw=True)])  # Feed raw bytes (force little endian ordering like onnx standard) for test purpose\n        self._assert_inferred(graph,\n                              [make_tensor_value_info(\'y\', TensorProto.FLOAT, (3, 4, 3, 10)),\n                               make_tensor_value_info(\'z\', TensorProto.INT64, (3, 4, 3, 10))])\n\n    def test_topk_missing_k_value_output_rank_check(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (3, 4, 5, 10)),\n            (\'k\', TensorProto.INT64, (1,))],\n            [make_node(\'TopK\', [\'x\', \'k\'], [\'y\', \'z\'], axis=2)],\n            [])\n        self._assert_inferred(graph,\n                              [make_tensor_value_info(\'y\', TensorProto.FLOAT, (None, None, None, None)),  # type: ignore\n                               make_tensor_value_info(\'z\', TensorProto.INT64, (None, None, None, None))])  # type: ignore\n\n    def test_gemm(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (7, 5)),\n             (\'y\', TensorProto.FLOAT, (5, 11)),\n             (\'z\', TensorProto.FLOAT, None)],\n            [make_node(\'Gemm\', [\'x\', \'y\', \'z\'], [\'out\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'out\', TensorProto.FLOAT, (7, 11))])\n\n    def test_gemm_transA(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (5, 7)),\n             (\'y\', TensorProto.FLOAT, (5, 11)),\n             (\'z\', TensorProto.FLOAT, None)],\n            [make_node(\'Gemm\', [\'x\', \'y\', \'z\'], [\'out\'], transA=1)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'out\', TensorProto.FLOAT, (7, 11))])\n\n    def test_gemm_transB(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (7, 5)),\n             (\'y\', TensorProto.FLOAT, (11, 5)),\n             (\'z\', TensorProto.FLOAT, None)],\n            [make_node(\'Gemm\', [\'x\', \'y\', \'z\'], [\'out\'], transB=1)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'out\', TensorProto.FLOAT, (7, 11))])\n\n    def test_gemm_transA_and_transB(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (5, 7)),\n             (\'y\', TensorProto.FLOAT, (11, 5)),\n             (\'z\', TensorProto.FLOAT, None)],\n            [make_node(\'Gemm\', [\'x\', \'y\', \'z\'], [\'out\'], transA=1, transB=1)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'out\', TensorProto.FLOAT, (7, 11))])\n\n    def test_gemm_no_bias(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (13, 7)),\n             (\'y\', TensorProto.FLOAT, (7, 17))],\n            [make_node(\'Gemm\', [\'x\', \'y\'], [\'out\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'out\', TensorProto.FLOAT, (13, 17))])\n\n    def test_reduce_op_shape_2_axis(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (24, 4, 11))],\n            [make_node(\'ReduceL1\', \'x\', \'y\', axes=(1, 2), keepdims=0)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (24,))])\n\n    def test_reduce_op_shape_keep_dims(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (24, 4, 11))],\n            [make_node(\'ReduceL1\', \'x\', \'y\', axes=(1, 2), keepdims=1)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (24, 1, 1))])\n\n    def test_reduce_op_shape_default_value(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (24, 4, 11))],\n            [make_node(\'ReduceL1\', \'x\', \'y\')],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (1, 1, 1))])\n\n    def test_reduce_op_shape_no_axes_do_not_keep_dims(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (24, 4, 11))],\n            [make_node(\'ReduceL1\', \'x\', \'y\', keepdims=0)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, tuple())])\n\n    def test_reduce_op_shape_negative_axis(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (24, 4, 11))],\n            [make_node(\'ReduceL1\', \'x\', \'y\', axes=(-1, -2))],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (24, 1, 1))])\n\n    def test_argmax_shape(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (24, 4, 11))],\n            [make_node(\'ArgMax\', \'x\', \'y\', axis=1, keepdims=1)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.INT64, (24, 1, 11))])\n\n    def test_argmax_shape_keepdims(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (24, 4, 11))],\n            [make_node(\'ArgMax\', \'x\', \'y\', axis=0, keepdims=0)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.INT64, (4, 11))])\n\n    def test_argmax_shape_default_value(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (24, 4, 11))],\n            [make_node(\'ArgMax\', \'x\', \'y\')],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.INT64, (1, 4, 11))])\n\n    def test_argmax_shape_negative_axis(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (24, 4, 11))],\n            [make_node(\'ArgMax\', \'x\', \'y\', axis=-2)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.INT64, (24, 1, 11))])\n\n    def test_dropout(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'data\', TensorProto.FLOAT, (3, 4, 5,)),\n             (\'ratio\', TensorProto.FLOAT, ())],\n            [make_node(\'Dropout\', [\'data\', \'ratio\'], [\'out\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'out\', TensorProto.FLOAT, (3, 4, 5,))])\n\n    def test_LRN(self):  # type: () -> None\n        self._identity_prop(\'LRN\', alpha=0.5, beta=0.5, size=1)\n\n    def test_batch_norm(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (3, 4, 5, 6, 7)),\n             (\'scale\', TensorProto.FLOAT, (4,)),\n             (\'b\', TensorProto.FLOAT, (4,)),\n             (\'mean\', TensorProto.FLOAT, (4,)),\n             (\'var\', TensorProto.FLOAT, (4,))],\n            [make_node(\'BatchNormalization\', [\'x\', \'scale\', \'b\', \'mean\', \'var\'], [\'out\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'out\', TensorProto.FLOAT, (3, 4, 5, 6, 7))])\n\n    def test_split_negative_axis(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (2, 4))],\n            [make_node(\'Split\', [\'x\'], [\'y\', \'z\'], axis=-1)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (2, 2)),\n                                      make_tensor_value_info(\'z\', TensorProto.FLOAT, (2, 2))])\n\n    def test_split_with_split_attribute(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (2, 4))],\n            [make_node(\'Split\', [\'x\'], [\'y\', \'z\'], axis=1, split=[3, 1])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (2, 3)),\n                                      make_tensor_value_info(\'z\', TensorProto.FLOAT, (2, 1))])\n\n    def test_split_with_split_attribute_unknown_split_dim(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (2, \'a\', \'b\'))],\n            [make_node(\'Split\', [\'x\'], [\'y\', \'z\'], axis=1, split=[3, 1])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (2, None, \'b\')),  # type: ignore\n                                      make_tensor_value_info(\'z\', TensorProto.FLOAT, (2, None, \'b\'))])  # type: ignore\n\n    def test_split_from_GLU(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (5, 6, 7))],\n            [make_node(\'Split\', [\'x\'], [\'y\', \'z\'], axis=1)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (5, 3, 7)),\n                                      make_tensor_value_info(\'z\', TensorProto.FLOAT, (5, 3, 7))])\n\n    def test_GLU_partial(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (5, 6, 7))],\n            [make_node(\'Split\', [\'x\'], [\'y\', \'z\'], axis=1),\n             make_node(\'Sigmoid\', [\'z\'], [\'a\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (5, 3, 7)),\n                                      make_tensor_value_info(\'z\', TensorProto.FLOAT, (5, 3, 7)),\n                                      make_tensor_value_info(\'a\', TensorProto.FLOAT, (5, 3, 7))])\n\n    def test_GLU(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (5, 6, 7))],\n            [make_node(\'Split\', [\'x\'], [\'y\', \'z\'], axis=1),\n             make_node(\'Sigmoid\', [\'z\'], [\'a\']),\n             make_node(\'Mul\', [\'y\', \'a\'], [\'b\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (5, 3, 7)),\n                                      make_tensor_value_info(\'z\', TensorProto.FLOAT, (5, 3, 7)),\n                                      make_tensor_value_info(\'a\', TensorProto.FLOAT, (5, 3, 7)),\n                                      make_tensor_value_info(\'b\', TensorProto.FLOAT, (5, 3, 7))])\n\n    def test_softmax_2d(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (4, 5))],\n            [make_node(\'Softmax\', [\'x\'], \'z\')],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, (4, 5))])\n\n    def test_softmax_3d(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (4, 5, 6))],\n            [make_node(\'Softmax\', [\'x\'], \'z\')],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, (4, 5, 6))])\n\n    def test_hardmax_2d(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (4, 5))],\n            [make_node(\'Hardmax\', [\'x\'], \'z\')],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, (4, 5))])\n\n    def test_hardmax_3d(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (4, 5, 6))],\n            [make_node(\'Hardmax\', [\'x\'], \'z\')],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, (4, 5, 6))])\n\n    def test_logsoftmax_2d(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (4, 5))],\n            [make_node(\'LogSoftmax\', [\'x\'], \'z\')],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, (4, 5))])\n\n    def test_logsoftmax_3d(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (4, 5, 6))],\n            [make_node(\'LogSoftmax\', [\'x\'], \'z\')],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, (4, 5, 6))])\n\n    def test_logsoftmax_3d_negative_axis(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (4, 5, 6))],\n            [make_node(\'LogSoftmax\', [\'x\'], \'z\', axis=-1)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, (4, 5, 6))])\n\n    def test_maxpool(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""X"", TensorProto.FLOAT, (5, 3, 4, 4))],\n            [make_node(""MaxPool"", [""X""], [""Y""], kernel_shape=[2, 2])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(""Y"", TensorProto.FLOAT, (5, 3, 3, 3))])\n\n    def test_maxpool_with_indices(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""X"", TensorProto.FLOAT, (5, 3, 4, 4))],\n            [make_node(""MaxPool"", [""X""], [""Y"", ""Z""], kernel_shape=[2, 2])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(""Y"", TensorProto.FLOAT, (5, 3, 3, 3)),\n                                      make_tensor_value_info(""Z"", TensorProto.INT64, (5, 3, 3, 3))])\n\n    def test_maxpool_3D(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""X"", TensorProto.FLOAT, (5, 3, 4, 4, 4))],\n            [make_node(""MaxPool"", [""X""], [""Y""], kernel_shape=[2, 2, 2])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(""Y"", TensorProto.FLOAT, (5, 3, 3, 3, 3))])\n\n    def test_maxpool_with_padding(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""X"", TensorProto.FLOAT, (5, 3, 4, 4))],\n            [make_node(""MaxPool"", [""X""], [""Y""], kernel_shape=[2, 2], pads=[1, 1, 2, 2])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(""Y"", TensorProto.FLOAT, (5, 3, 6, 6))])\n\n    def test_maxpool_with_padding_and_stride(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""X"", TensorProto.FLOAT, (5, 3, 4, 4))],\n            [make_node(""MaxPool"", [""X""], [""Y""], kernel_shape=[2, 2], pads=[1, 1, 2, 2], strides=[2, 2])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(""Y"", TensorProto.FLOAT, (5, 3, 3, 3))])\n\n    def test_maxpool_with_floor_mode(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""X"", TensorProto.FLOAT, (32, 288, 35, 35))],\n            [make_node(""MaxPool"", [""X""], [""Y""], kernel_shape=[2, 2], strides=[2, 2], ceil_mode=False)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(""Y"", TensorProto.FLOAT, (32, 288, 17, 17))])\n\n    def test_maxpool_with_ceil_mode(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""X"", TensorProto.FLOAT, (32, 288, 35, 35))],\n            [make_node(""MaxPool"", [""X""], [""Y""], kernel_shape=[2, 2], strides=[2, 2], ceil_mode=True)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(""Y"", TensorProto.FLOAT, (32, 288, 18, 18))])\n\n    def test_maxpool_ceil(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""X"", TensorProto.FLOAT, (1, 1, 4, 4))],\n            [make_node(""MaxPool"", [""X""], [""Y""], kernel_shape=[3, 3], strides=[2, 2], ceil_mode=True)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(""Y"", TensorProto.FLOAT, (1, 1, 2, 2))])\n\n    def test_maxpool_with_dilations(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""X"", TensorProto.FLOAT, (5, 3, 4, 4))],\n            [make_node(""MaxPool"", [""X""], [""Y""], kernel_shape=[2, 2], dilations=[2, 2])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(""Y"", TensorProto.FLOAT, (5, 3, 2, 2))])\n\n    def test_maxpool_with_same_upper_padding_and_stride(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""X"", TensorProto.FLOAT, (5, 3, 4, 4))],\n            [make_node(""MaxPool"", [""X""], [""Y""], auto_pad=""SAME_UPPER"", kernel_shape=[2, 2], strides=[2, 2])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(""Y"", TensorProto.FLOAT, (5, 3, 2, 2))])\n\n    def test_maxpool_with_same_upper_padding_and_stride_and_dilation(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""X"", TensorProto.FLOAT, (5, 3, 4, 4))],\n            [make_node(""MaxPool"", [""X""], [""Y""], auto_pad=""SAME_UPPER"", kernel_shape=[2, 2], strides=[2, 2], dilations=[2, 3])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(""Y"", TensorProto.FLOAT, (5, 3, 2, 2))])\n\n    def test_maxpool_with_same_upper_padding_and_stride_one(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""X"", TensorProto.FLOAT, (5, 3, 4, 4))],\n            [make_node(""MaxPool"", [""X""], [""Y""], auto_pad=""SAME_UPPER"", kernel_shape=[2, 2], strides=[1, 1])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(""Y"", TensorProto.FLOAT, (5, 3, 4, 4))])\n\n    def test_maxpool_with_same_lower_padding_and_stride(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""X"", TensorProto.FLOAT, (5, 3, 9, 9))],\n            [make_node(""MaxPool"", [""X""], [""Y""], auto_pad=""SAME_LOWER"", kernel_shape=[2, 2], strides=[2, 2])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(""Y"", TensorProto.FLOAT, (5, 3, 5, 5))])\n\n    def test_maxpool_with_same_lower_padding_and_stride_and_dilation(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""X"", TensorProto.FLOAT, (5, 3, 9, 9))],\n            [make_node(""MaxPool"", [""X""], [""Y""], auto_pad=""SAME_LOWER"", kernel_shape=[2, 2], strides=[2, 2], dilations=[2, 3])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(""Y"", TensorProto.FLOAT, (5, 3, 5, 5))])\n\n    def test_maxpool_with_same_lower_padding_and_big_stride(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""X"", TensorProto.FLOAT, (5, 3, 4, 4))],\n            [make_node(""MaxPool"", [""X""], [""Y""], auto_pad=""SAME_LOWER"", kernel_shape=[2, 2], strides=[4, 4])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(""Y"", TensorProto.FLOAT, (5, 3, 1, 1))])\n\n    def test_averagepool(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""X"", TensorProto.FLOAT, (5, 3, 4, 4))],\n            [make_node(""AveragePool"", [""X""], [""Y""], kernel_shape=[2, 2])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(""Y"", TensorProto.FLOAT, (5, 3, 3, 3))])\n\n    def test_averagepool_3D(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""X"", TensorProto.FLOAT, (5, 3, 4, 4, 4))],\n            [make_node(""AveragePool"", [""X""], [""Y""], kernel_shape=[2, 2, 2])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(""Y"", TensorProto.FLOAT, (5, 3, 3, 3, 3))])\n\n    def test_averagepool_with_padding(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""X"", TensorProto.FLOAT, (5, 3, 4, 4))],\n            [make_node(""AveragePool"", [""X""], [""Y""], kernel_shape=[2, 2], pads=[1, 1, 2, 2])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(""Y"", TensorProto.FLOAT, (5, 3, 6, 6))])\n\n    def test_averagepool_with_padding_and_stride(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""X"", TensorProto.FLOAT, (5, 3, 4, 4))],\n            [make_node(""AveragePool"", [""X""], [""Y""], kernel_shape=[2, 2], pads=[1, 1, 2, 2], strides=[2, 2])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(""Y"", TensorProto.FLOAT, (5, 3, 3, 3))])\n\n    def test_averagepool_ceil(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""X"", TensorProto.FLOAT, (1, 1, 4, 4))],\n            [make_node(""AveragePool"", [""X""], [""Y""], kernel_shape=[3, 3], strides=[2, 2], ceil_mode=True)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(""Y"", TensorProto.FLOAT, (1, 1, 2, 2))])\n\n    def test_lppool(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""X"", TensorProto.FLOAT, (5, 3, 4, 4))],\n            [make_node(""LpPool"", [""X""], [""Y""], kernel_shape=[2, 2])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(""Y"", TensorProto.FLOAT, (5, 3, 3, 3))])\n\n    def test_lppool_3D(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""X"", TensorProto.FLOAT, (5, 3, 4, 4, 4))],\n            [make_node(""LpPool"", [""X""], [""Y""], kernel_shape=[2, 2, 2])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(""Y"", TensorProto.FLOAT, (5, 3, 3, 3, 3))])\n\n    def test_lppool_with_padding(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""X"", TensorProto.FLOAT, (5, 3, 4, 4))],\n            [make_node(""LpPool"", [""X""], [""Y""], kernel_shape=[2, 2], pads=[1, 1, 2, 2])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(""Y"", TensorProto.FLOAT, (5, 3, 6, 6))])\n\n    def test_lppool_with_padding_and_stride(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""X"", TensorProto.FLOAT, (5, 3, 4, 4))],\n            [make_node(""LpPool"", [""X""], [""Y""], kernel_shape=[2, 2], pads=[1, 1, 2, 2], strides=[2, 2])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(""Y"", TensorProto.FLOAT, (5, 3, 3, 3))])\n\n    def test_roipool(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""X"", TensorProto.FLOAT, (5, 3, 4, 4)),\n            (""rois"", TensorProto.INT64, (2, 5))],\n            [make_node(""MaxRoiPool"", [""X"", ""rois""], [""Y""], pooled_shape=[2, 2])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(""Y"", TensorProto.FLOAT, (2, 3, 2, 2))])\n\n    def test_lp_norm(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (3, 4, 5, 6, 7))],\n            [make_node(\'LpNormalization\', [\'x\'], [\'out\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'out\', TensorProto.FLOAT, (3, 4, 5, 6, 7))])\n\n    def test_instance_norm(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (3, 4, 5, 6, 7)),\n             (\'scale\', TensorProto.FLOAT, (4,)),\n             (\'b\', TensorProto.FLOAT, (4,))],\n            [make_node(\'InstanceNormalization\', [\'x\', \'scale\', \'b\'], [\'out\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'out\', TensorProto.FLOAT, (3, 4, 5, 6, 7))])\n\n    def test_global_maxpool(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""X"", TensorProto.FLOAT, (5, 3, 4, 4))],\n            [make_node(""GlobalMaxPool"", [""X""], [""Y""])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(""Y"", TensorProto.FLOAT, (5, 3, 1, 1))])\n\n    def test_global_averagepool(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""X"", TensorProto.FLOAT, (5, 3, 4, 4))],\n            [make_node(""GlobalAveragePool"", [""X""], [""Y""])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(""Y"", TensorProto.FLOAT, (5, 3, 1, 1))])\n\n    def test_global_lppool(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""X"", TensorProto.FLOAT, (5, 3, 4, 4))],\n            [make_node(""GlobalLpPool"", [""X""], [""Y""])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(""Y"", TensorProto.FLOAT, (5, 3, 1, 1))])\n\n    def test_conv_transpose(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'X\', TensorProto.FLOAT, (25, 48, 16, 16)),\n             (\'W\', TensorProto.FLOAT, (48, 32, 3, 3))],\n            [make_node(\'ConvTranspose\', [\'X\', \'W\'], \'Y\', strides=[2, 2])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'Y\', TensorProto.FLOAT, (25, 32, 33, 33))])\n\n    def test_conv_transpose_with_pads(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'X\', TensorProto.FLOAT, (25, 48, 16, 16)),\n             (\'W\', TensorProto.FLOAT, (48, 32, 3, 3))],\n            [make_node(\'ConvTranspose\', [\'X\', \'W\'], \'Y\', strides=[2, 2], pads=[1, 1, 2, 2])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'Y\', TensorProto.FLOAT, (25, 32, 30, 30))])\n\n    def test_conv_transpose_with_output_shape(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'X\', TensorProto.FLOAT, (25, 48, 16, 16)),\n             (\'W\', TensorProto.FLOAT, (48, 32, 3, 3))],\n            [make_node(\'ConvTranspose\', [\'X\', \'W\'], \'Y\', strides=[2, 2], pads=[1, 1, 2, 2], output_shape=[36, 36])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'Y\', TensorProto.FLOAT, (25, 32, 36, 36))])\n\n    def test_conv_transpose_with_kernel_shape(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'X\', TensorProto.FLOAT, (25, 48, 16, 16)),\n             (\'W\', TensorProto.FLOAT, (48, 32, None, None))],\n            [make_node(\'ConvTranspose\', [\'X\', \'W\'], \'Y\', kernel_shape=[3, 3], strides=[2, 2], pads=[1, 1, 2, 2])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'Y\', TensorProto.FLOAT, (25, 32, 30, 30))])\n\n    def test_conv_transpose_with_dilations(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'X\', TensorProto.FLOAT, (25, 48, 16, 16)),\n             (\'W\', TensorProto.FLOAT, (48, 32, 3, 3))],\n            [make_node(\'ConvTranspose\', [\'X\', \'W\'], \'Y\', strides=[2, 2], pads=[1, 1, 2, 2], dilations=[3, 3])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'Y\', TensorProto.FLOAT, (25, 32, 34, 34))])\n\n    def test_conv_transpose_with_group(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'X\', TensorProto.FLOAT, (25, 48, 16, 16)),\n             (\'W\', TensorProto.FLOAT, (48, 32, 3, 3))],\n            [make_node(\'ConvTranspose\', [\'X\', \'W\'], \'Y\', strides=[2, 2], pads=[1, 1, 2, 2], group=2)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'Y\', TensorProto.FLOAT, (25, 64, 30, 30))])\n\n    def test_conv_transpose_with_group_and_output_shape(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'X\', TensorProto.FLOAT, (25, 48, 16, 16)),\n             (\'W\', TensorProto.FLOAT, (48, 32, 3, 3))],\n            [make_node(\'ConvTranspose\', [\'X\', \'W\'], \'Y\', strides=[2, 2], pads=[1, 1, 2, 2], group=2, output_shape=[36, 36])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'Y\', TensorProto.FLOAT, (25, 64, 36, 36))])\n\n    def test_mvn_function_output_shape(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'X\', TensorProto.FLOAT, (25, 48, 16, 16))],\n            [make_node(\'MeanVarianceNormalization\', \'X\', \'Y\', axes=[0, 2, 3])],\n            []\n        )\n        self._assert_inferred(graph, [make_tensor_value_info(\'Y\', TensorProto.FLOAT, (25, 48, 16, 16))])\n\n    def test_scan(self):    # type: () -> None\n        batch_size = 1\n        seq_len = \'sequence\'\n        input_size = 2\n        loop_state_size = 3\n\n        # can\'t use self._make_graph for the subgraph as it add more inputs for the Reshape operations it inserts.\n        # this breaks the subgraph inferencing as it expects the number of inputs passed from Scan to match\n        # the GraphProto, but Scan knows nothing about the additional inputs.\n        input_value_infos = [make_tensor_value_info(\'loop_state_in\', TensorProto.UNDEFINED, None),\n                             make_tensor_value_info(\'input\', TensorProto.UNDEFINED, None)]\n        output_value_infos = [make_tensor_value_info(\'loop_state_out\', TensorProto.UNDEFINED, None),\n                              make_tensor_value_info(\'output\', TensorProto.UNDEFINED, None)]\n\n        subgraph = helper.make_graph(\n            [make_node(\'Identity\', [\'loop_state_in\'], [\'loop_state_out\']),\n             make_node(\'Identity\', [\'input\'], [\'output\'])],\n            ""subgraph"",\n            input_value_infos,\n            output_value_infos\n        )\n\n        graph = self._make_graph(\n            [(\'loop_state_orig\', TensorProto.FLOAT, (batch_size, loop_state_size)),\n             (\'scan_input\', TensorProto.FLOAT, (batch_size, seq_len, input_size))],\n            [make_node(\'Scan\', [\'\', \'loop_state_orig\', \'scan_input\'], [\'loop_state_final\', \'scan_output\'],\n                       num_scan_inputs=1, body=subgraph)],\n            []\n        )\n\n        self._assert_inferred(\n            graph,\n            [make_tensor_value_info(\'loop_state_final\', TensorProto.FLOAT, (batch_size, loop_state_size)),\n             make_tensor_value_info(\'scan_output\', TensorProto.FLOAT, (batch_size, seq_len, input_size))],\n            opset_imports=[helper.make_opsetid(ONNX_DOMAIN, 8)])\n\n    def test_scan_opset9(self):    # type: () -> None\n        seq_len = \'sequence\'\n        input_size = 2\n        loop_state_size = 3\n\n        # can\'t use self._make_graph for the subgraph as it add more inputs for the Reshape operations it inserts.\n        # this breaks the subgraph inferencing as it expects the number of inputs passed from Scan to match\n        # the GraphProto, but Scan knows nothing about the additional inputs.\n        input_value_infos = [make_tensor_value_info(\'loop_state_in\', TensorProto.UNDEFINED, None),\n                             make_tensor_value_info(\'input\', TensorProto.UNDEFINED, None)]\n        output_value_infos = [make_tensor_value_info(\'loop_state_out\', TensorProto.UNDEFINED, None),\n                              make_tensor_value_info(\'output\', TensorProto.UNDEFINED, None)]\n\n        subgraph = helper.make_graph(\n            [make_node(\'Identity\', [\'loop_state_in\'], [\'loop_state_out\']),\n             make_node(\'Identity\', [\'input\'], [\'output\'])],\n            ""subgraph"",\n            input_value_infos,\n            output_value_infos\n        )\n\n        graph = self._make_graph(\n            [(\'loop_state_orig\', TensorProto.FLOAT, (loop_state_size,)),\n             (\'scan_input\', TensorProto.FLOAT, (seq_len, input_size))],\n            [make_node(\'Scan\', [\'loop_state_orig\', \'scan_input\'], [\'loop_state_final\', \'scan_output\'],\n                       num_scan_inputs=1, body=subgraph)],\n            []\n        )\n\n        self._assert_inferred(\n            graph,\n            [make_tensor_value_info(\'loop_state_final\', TensorProto.FLOAT, (loop_state_size,)),\n             make_tensor_value_info(\'scan_output\', TensorProto.FLOAT, (seq_len, input_size))],\n            opset_imports=[helper.make_opsetid(ONNX_DOMAIN, 9)])\n\n    def test_scan_opset9_axes(self):    # type: () -> None\n        axis_0_len = \'axis0\'\n        seq_len = \'sequence\'\n        input_size = 2\n        loop_state_size = 3\n\n        # can\'t use self._make_graph for the subgraph as it add more inputs for the Reshape operations it inserts.\n        # this breaks the subgraph inferencing as it expects the number of inputs passed from Scan to match\n        # the GraphProto, but Scan knows nothing about the additional inputs.\n        input_value_infos = [make_tensor_value_info(\'loop_state_in\', TensorProto.UNDEFINED, None),\n                             make_tensor_value_info(\'input\', TensorProto.UNDEFINED, None)]\n        output_value_infos = [make_tensor_value_info(\'loop_state_out\', TensorProto.UNDEFINED, None),\n                              make_tensor_value_info(\'output\', TensorProto.UNDEFINED, None)]\n\n        subgraph = helper.make_graph(\n            [make_node(\'Identity\', [\'loop_state_in\'], [\'loop_state_out\']),\n             make_node(\'Identity\', [\'input\'], [\'output\'])],\n            ""subgraph"",\n            input_value_infos,\n            output_value_infos\n        )\n\n        graph = self._make_graph(\n            [(\'loop_state_orig\', TensorProto.FLOAT, (loop_state_size,)),\n             (\'scan_input\', TensorProto.FLOAT, (axis_0_len, seq_len, input_size))],\n            [make_node(\'Scan\', [\'loop_state_orig\', \'scan_input\'], [\'loop_state_final\', \'scan_output\'],\n                       num_scan_inputs=1, body=subgraph, scan_input_axes=[1])],\n            []\n        )\n\n        self._assert_inferred(\n            graph,\n            [make_tensor_value_info(\'loop_state_final\', TensorProto.FLOAT, (loop_state_size,)),\n             make_tensor_value_info(\'scan_output\', TensorProto.FLOAT, (seq_len, axis_0_len, input_size))],\n            opset_imports=[helper.make_opsetid(ONNX_DOMAIN, 9)])\n\n    def test_scan_opset9_output_axes(self):    # type: () -> None\n        axis_0_len = \'axis0\'\n        seq_len = \'sequence\'\n        input_size = 2\n        loop_state_size = 3\n\n        input_value_infos = [make_tensor_value_info(\'loop_state_in\', TensorProto.UNDEFINED, None),\n                             make_tensor_value_info(\'input\', TensorProto.UNDEFINED, None)]\n        output_value_infos = [make_tensor_value_info(\'loop_state_out\', TensorProto.UNDEFINED, None),\n                              make_tensor_value_info(\'output\', TensorProto.UNDEFINED, None)]\n\n        subgraph = helper.make_graph(\n            [make_node(\'Identity\', [\'loop_state_in\'], [\'loop_state_out\']),\n             make_node(\'Identity\', [\'input\'], [\'output\'])],\n            ""subgraph"",\n            input_value_infos,\n            output_value_infos\n        )\n\n        graph = self._make_graph(\n            [(\'loop_state_orig\', TensorProto.FLOAT, (loop_state_size,)),\n             (\'scan_input\', TensorProto.FLOAT, (axis_0_len, seq_len, input_size))],\n            [make_node(\'Scan\', [\'loop_state_orig\', \'scan_input\'], [\'loop_state_final\', \'scan_output\'],\n                       num_scan_inputs=1, body=subgraph, scan_input_axes=[1], scan_output_axes=[1])],\n            []\n        )\n\n        self._assert_inferred(\n            graph,\n            [make_tensor_value_info(\'loop_state_final\', TensorProto.FLOAT, (loop_state_size,)),\n             make_tensor_value_info(\'scan_output\', TensorProto.FLOAT, (axis_0_len, seq_len, input_size))],\n            opset_imports=[helper.make_opsetid(ONNX_DOMAIN, 9)])\n\n    def test_scan_opset9_negative_axes(self):    # type: () -> None\n        axis_0_len = \'axis0\'\n        seq_len = \'sequence\'\n        input_size = 2\n        loop_state_size = 3\n\n        input_value_infos = [make_tensor_value_info(\'loop_state_in\', TensorProto.UNDEFINED, None),\n                             make_tensor_value_info(\'input\', TensorProto.UNDEFINED, None)]\n        output_value_infos = [make_tensor_value_info(\'loop_state_out\', TensorProto.UNDEFINED, None),\n                              make_tensor_value_info(\'output\', TensorProto.UNDEFINED, None)]\n\n        subgraph = helper.make_graph(\n            [make_node(\'Identity\', [\'loop_state_in\'], [\'loop_state_out\']),\n             make_node(\'Identity\', [\'input\'], [\'output\'])],\n            ""subgraph"",\n            input_value_infos,\n            output_value_infos\n        )\n\n        graph = self._make_graph(\n            [(\'loop_state_orig\', TensorProto.FLOAT, (loop_state_size,)),\n             (\'scan_input\', TensorProto.FLOAT, (axis_0_len, seq_len, input_size))],\n            [make_node(\'Scan\', [\'loop_state_orig\', \'scan_input\'], [\'loop_state_final\', \'scan_output\'],\n                       num_scan_inputs=1, body=subgraph, scan_input_axes=[-2], scan_output_axes=[-2])],\n            []\n        )\n\n        self._assert_inferred(\n            graph,\n            [make_tensor_value_info(\'loop_state_final\', TensorProto.FLOAT, (loop_state_size,)),\n             make_tensor_value_info(\'scan_output\', TensorProto.FLOAT, (axis_0_len, seq_len, input_size))],\n            opset_imports=[helper.make_opsetid(ONNX_DOMAIN, 9)])\n\n    def test_if_ver1(self):  # type: () -> None\n\n        # Create a simple If node where the \'then\' subgraph adds to the current value, and the \'else\' subgraph\n        # subtracts.\n        # can\'t use self._make_graph for the subgraphs as that add more inputs for the Reshape operations it inserts.\n        # this breaks the subgraph inferencing as it expects the subgraphs to have zero inputs\n        then_subgraph = helper.make_graph(\n            [make_node(\'Add\', [\'current_value\', \'add_value\'], [\'then_output\'])],\n            ""then_subgraph"",\n            [],  # no inputs\n            [make_tensor_value_info(\'then_output\', TensorProto.UNDEFINED, None)],\n        )\n\n        else_subgraph = helper.make_graph(\n            [make_node(\'Sub\', [\'current_value\', \'sub_value\'], [\'else_output\'])],\n            ""else_subgraph"",\n            [],  # no inputs\n            [make_tensor_value_info(\'else_output\', TensorProto.UNDEFINED, None)],\n        )\n\n        graph = self._make_graph(\n            [(\'cond\', TensorProto.BOOL, (1,)),\n             (\'current_value\', TensorProto.FLOAT, (1,)),\n             (\'add_value\', TensorProto.FLOAT, (1,)),\n             (\'sub_value\', TensorProto.FLOAT, (1,))],\n            [make_node(\'If\', [\'cond\'], [\'if_output\'],\n                       then_branch=then_subgraph, else_branch=else_subgraph)],\n            []\n        )\n\n        self._assert_inferred(\n            graph,\n            [make_tensor_value_info(\'if_output\', TensorProto.FLOAT, (1,))],\n            opset_imports=[make_opsetid(ONNX_DOMAIN, 10)])\n\n    def test_if(self):  # type: () -> None\n\n        # Create a simple If node where the \'then\' subgraph adds to the current value, and the \'else\' subgraph\n        # subtracts.\n        # can\'t use self._make_graph for the subgraphs as that add more inputs for the Reshape operations it inserts.\n        # this breaks the subgraph inferencing as it expects the subgraphs to have zero inputs\n        then_subgraph = helper.make_graph(\n            [make_node(\'Add\', [\'current_value\', \'add_value\'], [\'then_output\'])],\n            ""then_subgraph"",\n            [],  # no inputs\n            [make_tensor_value_info(\'then_output\', TensorProto.UNDEFINED, None)],\n        )\n\n        else_subgraph = helper.make_graph(\n            [make_node(\'Sub\', [\'current_value\', \'sub_value\'], [\'else_output\'])],\n            ""else_subgraph"",\n            [],  # no inputs\n            [make_tensor_value_info(\'else_output\', TensorProto.UNDEFINED, None)],\n        )\n\n        graph = self._make_graph(\n            [(\'cond\', TensorProto.BOOL, (1,)),\n             (\'current_value\', TensorProto.FLOAT, (1,)),\n             (\'add_value\', TensorProto.FLOAT, (1,)),\n             (\'sub_value\', TensorProto.FLOAT, (1,))],\n            [make_node(\'If\', [\'cond\'], [\'if_output\'],\n                       then_branch=then_subgraph, else_branch=else_subgraph)],\n            []\n        )\n\n        self._assert_inferred(graph, [make_tensor_value_info(\'if_output\', TensorProto.FLOAT, (1,))])\n\n    def test_if_with_different_shapes_in_then_else_branches(self):  # type: () -> None\n\n        # Create a simple If node where the \'then\' subgraph adds to the current value, and the \'else\' subgraph\n        # subtracts.\n        # can\'t use self._make_graph for the subgraphs as that add more inputs for the Reshape operations it inserts.\n        # this breaks the subgraph inferencing as it expects the subgraphs to have zero inputs\n        then_subgraph = helper.make_graph(\n            [make_node(\'Add\', [\'current_value\', \'add_value\'], [\'then_output\'])],\n            ""then_subgraph"",\n            [],  # no inputs\n            [make_tensor_value_info(\'then_output\', TensorProto.UNDEFINED, (1,))],\n        )\n\n        else_subgraph = helper.make_graph(\n            [make_node(\'Sub\', [\'current_value\', \'sub_value\'], [\'else_output\'])],\n            ""else_subgraph"",\n            [],  # no inputs\n            [make_tensor_value_info(\'else_output\', TensorProto.UNDEFINED, (5,))],\n        )\n\n        graph = self._make_graph(\n            [(\'cond\', TensorProto.BOOL, (1,)),\n             (\'current_value\', TensorProto.FLOAT, (1,)),\n             (\'add_value\', TensorProto.FLOAT, (1,)),\n             (\'sub_value\', TensorProto.FLOAT, (5,))],\n            [make_node(\'If\', [\'cond\'], [\'if_output\'],\n                       then_branch=then_subgraph, else_branch=else_subgraph)],\n            []\n        )\n\n        self._assert_inferred(graph, [make_tensor_value_info(\'if_output\', TensorProto.FLOAT, (None,))])  # type: ignore\n\n    def test_maxunpool_shape_without_output_shape(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'xT\', TensorProto.FLOAT, (1, 1, 2, 2)),\n             (\'xI\', TensorProto.FLOAT, (1, 1, 2, 2))],\n            [make_node(\'MaxUnpool\', [\'xT\', \'xI\'], \'Y\', kernel_shape=[2, 2], strides=[2, 2])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'Y\', TensorProto.FLOAT, (1, 1, 4, 4))])\n\n    def test_maxunpool_shape_with_output_shape(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'xT\', TensorProto.FLOAT, (1, 1, 2, 2)),\n             (\'xI\', TensorProto.FLOAT, (1, 1, 2, 2)),\n             (\'output_shape\', TensorProto.FLOAT, (4, ))],\n            [make_node(\'MaxUnpool\', [\'xT\', \'xI\', \'output_shape\'], \'Y\', kernel_shape=[2, 2], strides=[2, 2])],\n            [make_tensor_value_info(""Y"", TensorProto.FLOAT, None)])\n        self._assert_inferred(graph, [make_tensor_value_info(""Y"", TensorProto.FLOAT, None)])\n\n    def test_onehot_without_axis(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'indices\', TensorProto.INT64, (2, 2)),\n             (\'depth\', TensorProto.INT64, ()),\n             (\'values\', TensorProto.FLOAT, (2, ))],\n            [make_node(\'OneHot\', [\'indices\', \'depth\', \'values\'], \'Y\')],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'Y\', TensorProto.FLOAT, (2, 2, None))])  # type: ignore\n\n    def test_onehot_with_axis(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'indices\', TensorProto.INT64, (2, 3, 5)),\n             (\'depth\', TensorProto.INT64, (1, )),\n             (\'values\', TensorProto.FLOAT, (2, ))],\n            [make_node(\'OneHot\', [\'indices\', \'depth\', \'values\'], \'Y\', axis=1)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'Y\', TensorProto.FLOAT, (2, None, 3, 5))])  # type: ignore\n\n    def test_loop(self):    # type: () -> None\n        # can\'t use self._make_graph for the subgraph as it add more inputs for the Reshape operations it inserts.\n        # this breaks the subgraph inferencing as it expects the number of inputs passed from Loop to match\n        # the GraphProto, but Loop knows nothing about the additional inputs.\n        input_value_infos = [make_tensor_value_info(\'iter_num_in\', TensorProto.INT64, (1,)),\n                             make_tensor_value_info(\'cond_in\', TensorProto.UNDEFINED, None),\n                             make_tensor_value_info(\'loop_state_in\', TensorProto.UNDEFINED, ())]\n        output_value_infos = [make_tensor_value_info(\'cond_out\', TensorProto.UNDEFINED, None),\n                              make_tensor_value_info(\'loop_state_out\', TensorProto.UNDEFINED, None),\n                              make_tensor_value_info(\'output\', TensorProto.FLOAT, (3,))]\n\n        subgraph = helper.make_graph(\n            [make_node(\'Identity\', [\'cond_in\'], [\'cond_out\']),\n             make_node(\'Identity\', [\'loop_state_in\'], [\'loop_state_out\']),\n             make_node(\'Identity\', [\'outer_scope_input\'], [\'output\'])],\n            ""subgraph"",\n            input_value_infos,\n            output_value_infos\n        )\n\n        graph = self._make_graph(\n            [(\'max_trip_count\', TensorProto.INT64, (1,)),\n             (\'cond_orig\', TensorProto.FLOAT, (1,)),\n             (\'loop_state_orig\', TensorProto.FLOAT, (2,)),\n             (\'outer_scope_input\', TensorProto.FLOAT, (3,))],\n            [make_node(\'Loop\', [\'max_trip_count\', \'cond_orig\', \'loop_state_orig\'], [\'loop_state_final\', \'loop_output\'],\n                       body=subgraph)],\n            []\n        )\n\n        self._assert_inferred(\n            graph,\n            [make_tensor_value_info(\'loop_state_final\', TensorProto.FLOAT, None),  # shape may change between iterations\n             make_tensor_value_info(\'loop_output\', TensorProto.FLOAT, (None, 3))])  # type: ignore\n\n    def test_loop_no_state(self):    # type: () -> None\n        input_value_infos = [make_tensor_value_info(\'iter_num_in\', TensorProto.INT64, (1,)),\n                             make_tensor_value_info(\'cond_in\', TensorProto.UNDEFINED, None)]\n        output_value_infos = [make_tensor_value_info(\'cond_out\', TensorProto.UNDEFINED, None),\n                              make_tensor_value_info(\'output\', TensorProto.FLOAT, (3,))]\n\n        subgraph = helper.make_graph(\n            [make_node(\'Identity\', [\'cond_in\'], [\'cond_out\']),\n             make_node(\'Identity\', [\'outer_scope_input\'], [\'output\'])],\n            ""subgraph"",\n            input_value_infos,\n            output_value_infos\n        )\n\n        graph = self._make_graph(\n            [(\'max_trip_count\', TensorProto.INT64, (1,)),\n             (\'cond_orig\', TensorProto.FLOAT, (1,)),\n             (\'outer_scope_input\', TensorProto.FLOAT, (3,))],\n            [make_node(\'Loop\', [\'max_trip_count\', \'cond_orig\'], [\'loop_output\'],\n                       body=subgraph)],\n            []\n        )\n\n        self._assert_inferred(\n            graph,\n            [make_tensor_value_info(\'loop_output\', TensorProto.FLOAT, (None, 3))])  # type: ignore\n\n    def test_constantofshape_with_input_shape(self):  # type: () -> None\n        graph = self._make_graph([],\n            [make_node(""Constant"", [], [\'shape\'],\n                       value=make_tensor(\'shape\', TensorProto.INT64, (3,), (3, 4, 5))),\n             make_node(""ConstantOfShape"", [\'shape\'], [\'y\'], value=make_tensor(\'value\', TensorProto.INT32, (1, ), (2, )))],\n            [])\n        self._assert_inferred(graph,\n            [make_tensor_value_info(\'shape\', TensorProto.INT64, (3,)),\n             make_tensor_value_info(\'y\', TensorProto.INT32, (3, 4, 5))])  # type: ignore\n\n    def test_constantofshape_without_input_shape(self):  # type: () -> None\n        graph = self._make_graph([(\'shape\', TensorProto.INT64, (3, ))],\n            [make_node(""ConstantOfShape"", [\'shape\'], [\'y\'], value=make_tensor(\'value\', TensorProto.UINT8, (1, ), (2, )))],\n            [])\n        self._assert_inferred(graph,\n            [make_tensor_value_info(\'y\', TensorProto.UINT8, (None, None, None))])  # type: ignore\n\n    def test_constantofshape_with_shape_zero(self):  # type: () -> None\n        graph = self._make_graph([],\n            [make_node(""Constant"", [], [\'shape\'],\n                       value=make_tensor(\'shape\', TensorProto.INT64, (3,), (0,))),\n             make_node(""ConstantOfShape"", [\'shape\'], [\'y\'], value=make_tensor(\'value\', TensorProto.INT32, (1, ), (2, )))],\n            [])\n        self._assert_inferred(graph,\n            [make_tensor_value_info(\'shape\', TensorProto.INT64, (3,)),\n             make_tensor_value_info(\'y\', TensorProto.INT32, (0,))])  # type: ignore\n\n    def test_convinteger(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.UINT8, (3, 4, 5, 6, 7)),\n             (\'y\', TensorProto.UINT8, (5, 4, 2, 4, 3))],\n            [make_node(\'ConvInteger\', [\'x\', \'y\'], \'z\', pads=[0, 1, 1, 0, 0, 1], dilations=[1, 2, 2], strides=[1, 1, 2])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.INT32, (3, 5, 4, 1, 3))])\n\n    def test_convinetger_dilations(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.UINT8, (30, 4, 8, 8, 8)),\n             (\'y\', TensorProto.INT8, (50, 4, 3, 3, 3)),\n             (\'x_zero_point\', TensorProto.UINT8, ()),\n             (\'y_zero_point\', TensorProto.UINT8, ())],\n            [make_node(\'ConvInteger\', [\'x\', \'y\', \'x_zero_point\', \'y_zero_point\'], \'z\', dilations=[1, 2, 3])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.INT32, (30, 50, 6, 4, 2))])\n\n    def test_convinteger_strides(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.INT8, (30, 4, 8, 8, 8)),\n             (\'y\', TensorProto.INT8, (50, 4, 3, 3, 3)),\n             (\'x_zero_point\', TensorProto.UINT8, ()),\n             (\'y_zero_point\', TensorProto.UINT8, ())],\n            [make_node(\'ConvInteger\', [\'x\', \'y\', \'x_zero_point\', \'y_zero_point\'], \'z\', strides=[1, 2, 3])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.INT32, (30, 50, 6, 3, 2))])\n\n    def test_convineteger_pads(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.UINT8, (30, 4, 7, 6, 4)),\n             (\'y\', TensorProto.INT8, (50, 4, 3, 3, 3))],\n            [make_node(\'ConvInteger\', [\'x\', \'y\'], \'z\', pads=[1, 1, 2, 0, 1, 2])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.INT32, (30, 50, 6, 6, 6))])\n\n    def test_convineteger_group(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.INT8, (30, 4, 8, 8, 8)),\n             (\'y\', TensorProto.INT8, (4, 1, 8, 8, 8))],\n            [make_node(\'ConvInteger\', [\'x\', \'y\'], \'z\', group=4)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.INT32, (30, 4, 1, 1, 1))])\n\n    def test_convineteger_partial_missing_shape(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.UINT8, (30, 4, None, 6, 4)),\n             (\'y\', TensorProto.UINT8, (50, 4, 3, 3, 3)),\n             (\'x_zero_point\', TensorProto.UINT8, ()),\n             (\'y_zero_point\', TensorProto.UINT8, ())],\n            [make_node(\'ConvInteger\', [\'x\', \'y\', \'x_zero_point\', \'y_zero_point\'], \'z\', pads=[1, 1, 2, 0, 1, 2])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.INT32, (30, 50, None, 6, 6))])  # type: ignore\n\n    def test_convineteger_partial_missing_weight_shape(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.UINT8, (30, 4, 7, 6, 4)),\n             (\'y\', TensorProto.UINT8, (50, 4, None, 3, 3))],\n            [make_node(\'ConvInteger\', [\'x\', \'y\'], \'z\', pads=[1, 1, 2, 0, 1, 2])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.INT32, None)])\n\n    def test_qlinearconv(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.UINT8, (3, 4, 5, 6, 7)),\n             (\'x_scale\', TensorProto.FLOAT, ()),\n             (\'x_zero_point\', TensorProto.UINT8, ()),\n             (\'w\', TensorProto.UINT8, (5, 4, 2, 4, 3)),\n             (\'w_scale\', TensorProto.FLOAT, ()),\n             (\'w_zero_point\', TensorProto.UINT8, ()),\n             (\'y_scale\', TensorProto.FLOAT, ()),\n             (\'y_zero_point\', TensorProto.UINT8, ())],\n            [make_node(\'QLinearConv\', [\'x\', \'x_scale\', \'x_zero_point\', \'w\', \'w_scale\', \'w_zero_point\', \'y_scale\', \'y_zero_point\'], \'y\', pads=[0, 1, 1, 0, 0, 1], dilations=[1, 2, 2], strides=[1, 1, 2])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.UINT8, (3, 5, 4, 1, 3))])\n\n    def test_qlinearconv_dilations(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.UINT8, (30, 4, 8, 8, 8)),\n             (\'x_scale\', TensorProto.FLOAT, ()),\n             (\'x_zero_point\', TensorProto.UINT8, ()),\n             (\'w\', TensorProto.UINT8, (50, 4, 3, 3, 3)),\n             (\'w_scale\', TensorProto.FLOAT, ()),\n             (\'w_zero_point\', TensorProto.UINT8, ()),\n             (\'y_scale\', TensorProto.FLOAT, ()),\n             (\'y_zero_point\', TensorProto.UINT8, ())],\n            [make_node(\'QLinearConv\', [\'x\', \'x_scale\', \'x_zero_point\', \'w\', \'w_scale\', \'w_zero_point\', \'y_scale\', \'y_zero_point\'], \'y\', dilations=[1, 2, 3])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.UINT8, (30, 50, 6, 4, 2))])\n\n    def test_qlinearconv_strides(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.INT8, (30, 4, 8, 8, 8)),\n             (\'x_scale\', TensorProto.FLOAT, ()),\n             (\'x_zero_point\', TensorProto.INT8, ()),\n             (\'w\', TensorProto.INT8, (50, 4, 3, 3, 3)),\n             (\'w_scale\', TensorProto.FLOAT, ()),\n             (\'w_zero_point\', TensorProto.INT8, ()),\n             (\'y_scale\', TensorProto.FLOAT, ()),\n             (\'y_zero_point\', TensorProto.INT8, ())],\n            [make_node(\'QLinearConv\', [\'x\', \'x_scale\', \'x_zero_point\', \'w\', \'w_scale\', \'w_zero_point\', \'y_scale\', \'y_zero_point\'], \'y\', strides=[1, 2, 3])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.INT8, (30, 50, 6, 3, 2))])\n\n    def test_qlinearconv_pads(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.UINT8, (30, 4, 7, 6, 4)),\n             (\'x_scale\', TensorProto.FLOAT, ()),\n             (\'x_zero_point\', TensorProto.UINT8, ()),\n             (\'w\', TensorProto.INT8, (50, 4, 3, 3, 3)),\n             (\'w_scale\', TensorProto.FLOAT, ()),\n             (\'w_zero_point\', TensorProto.INT8, ()),\n             (\'y_scale\', TensorProto.FLOAT, ()),\n             (\'y_zero_point\', TensorProto.UINT8, ())],\n            [make_node(\'QLinearConv\', [\'x\', \'x_scale\', \'x_zero_point\', \'w\', \'w_scale\', \'w_zero_point\', \'y_scale\', \'y_zero_point\'], \'y\', pads=[1, 1, 2, 0, 1, 2])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.UINT8, (30, 50, 6, 6, 6))])\n\n    def test_qlinearconv_group(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.INT8, (30, 4, 8, 8, 8)),\n             (\'x_scale\', TensorProto.FLOAT, ()),\n             (\'x_zero_point\', TensorProto.INT8, ()),\n             (\'w\', TensorProto.INT8, (4, 1, 8, 8, 8)),\n             (\'w_scale\', TensorProto.FLOAT, ()),\n             (\'w_zero_point\', TensorProto.INT8, ()),\n             (\'y_scale\', TensorProto.FLOAT, ()),\n             (\'y_zero_point\', TensorProto.INT8, ())],\n            [make_node(\'QLinearConv\', [\'x\', \'x_scale\', \'x_zero_point\', \'w\', \'w_scale\', \'w_zero_point\', \'y_scale\', \'y_zero_point\'], \'y\', group=4)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.INT8, (30, 4, 1, 1, 1))])\n\n    def test_qlinearconv_partial_missing_shape(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.UINT8, (30, 4, None, 6, 4)),\n             (\'x_scale\', TensorProto.FLOAT, ()),\n             (\'x_zero_point\', TensorProto.UINT8, ()),\n             (\'w\', TensorProto.UINT8, (50, 4, 3, 3, 3)),\n             (\'w_scale\', TensorProto.FLOAT, ()),\n             (\'w_zero_point\', TensorProto.UINT8, ()),\n             (\'y_scale\', TensorProto.FLOAT, ()),\n             (\'y_zero_point\', TensorProto.UINT8, ())],\n            [make_node(\'QLinearConv\', [\'x\', \'x_scale\', \'x_zero_point\', \'w\', \'w_scale\', \'w_zero_point\', \'y_scale\', \'y_zero_point\'], \'y\', pads=[1, 1, 2, 0, 1, 2])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.UINT8, (30, 50, None, 6, 6))])  # type: ignore\n\n    def test_qlinearconv_partial_missing_weight_shape(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.UINT8, (30, 4, 7, 6, 4)),\n             (\'x_scale\', TensorProto.FLOAT, ()),\n             (\'x_zero_point\', TensorProto.UINT8, ()),\n             (\'w\', TensorProto.UINT8, (50, 4, None, 3, 3)),\n             (\'w_scale\', TensorProto.FLOAT, ()),\n             (\'w_zero_point\', TensorProto.UINT8, ()),\n             (\'y_scale\', TensorProto.FLOAT, ()),\n             (\'y_zero_point\', TensorProto.UINT8, ())],\n            [make_node(\'QLinearConv\', [\'x\', \'x_scale\', \'x_zero_point\', \'w\', \'w_scale\', \'w_zero_point\', \'y_scale\', \'y_zero_point\'], \'y\', pads=[1, 1, 2, 0, 1, 2])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.UINT8, None)])\n\n    def _make_qlinearmatmul_test(self, shape1, shape2):  # type: (Sequence[int], Sequence[int]) -> None\n        expected_out_shape = np.matmul(np.arange(np.product(shape1)).reshape(shape1),\n                                       np.arange(np.product(shape2)).reshape(shape2)).shape\n        graph = self._make_graph(\n            [(\'a\', TensorProto.UINT8, shape1),\n             (\'a_scale\', TensorProto.FLOAT, ()),\n             (\'a_zero_point\', TensorProto.UINT8, ()),\n             (\'b\', TensorProto.UINT8, shape2),\n             (\'b_scale\', TensorProto.FLOAT, ()),\n             (\'b_zero_point\', TensorProto.UINT8, ()),\n             (\'y_scale\', TensorProto.FLOAT, ()),\n             (\'y_zero_point\', TensorProto.UINT8, ())],\n            [make_node(\'QLinearMatMul\', [\'a\', \'a_scale\', \'a_zero_point\', \'b\', \'b_scale\', \'b_zero_point\', \'y_scale\', \'y_zero_point\'], [\'y\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.UINT8, expected_out_shape)])\n\n    def test_qlinearmatmul(self):  # type: () -> None\n        self._make_qlinearmatmul_test((3,), (3,))\n        self._make_qlinearmatmul_test((4, 2), (2, 4))\n        self._make_qlinearmatmul_test((2,), (2, 3))\n        self._make_qlinearmatmul_test((4, 2), (2,))\n        self._make_qlinearmatmul_test((5, 1, 4, 2), (1, 3, 2, 3))\n        self._make_qlinearmatmul_test((4, 2), (3, 2, 3))\n\n    def _make_qlinearmatmul_test_allow_unknown(self, shape1, shape2, expected_out_shape):  # type: (Any, Any, Any) -> None\n        graph = self._make_graph(\n            [(\'a\', TensorProto.UINT8, shape1),\n             (\'a_scale\', TensorProto.FLOAT, ()),\n             (\'a_zero_point\', TensorProto.UINT8, ()),\n             (\'b\', TensorProto.UINT8, shape2),\n             (\'b_scale\', TensorProto.FLOAT, ()),\n             (\'b_zero_point\', TensorProto.UINT8, ()),\n             (\'y_scale\', TensorProto.FLOAT, ()),\n             (\'y_zero_point\', TensorProto.UINT8, ())],\n            [make_node(\'QLinearMatMul\', [\'a\', \'a_scale\', \'a_zero_point\', \'b\', \'b_scale\', \'b_zero_point\', \'y_scale\', \'y_zero_point\'], [\'y\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.UINT8, expected_out_shape)])\n\n    def test_qlinearmatmul_allow_unknown(self):  # type: () -> None\n        self._make_qlinearmatmul_test_allow_unknown((None,), (None,), ())\n        self._make_qlinearmatmul_test_allow_unknown((3,), (None,), ())\n        self._make_qlinearmatmul_test_allow_unknown((2,), (2, ""a""), (""a"",))\n        self._make_qlinearmatmul_test_allow_unknown((4, 2), (2, ""a""), (4, ""a""))\n        self._make_qlinearmatmul_test_allow_unknown((4, None), (2, ""a""), (4, ""a""))\n        self._make_qlinearmatmul_test_allow_unknown((4, None), (None, ""a""), (4, ""a""))\n        self._make_qlinearmatmul_test_allow_unknown((1, 4, 2), (""a"", 2, 5), (""a"", 4, 5))\n        self._make_qlinearmatmul_test_allow_unknown((1, 3, 4, 2), (""a"", 2, 5), (1, 3, 4, 5))\n        self._make_qlinearmatmul_test_allow_unknown(None, (""a"", 2, 5), None)\n        self._make_qlinearmatmul_test_allow_unknown(None, None, None)\n\n    def _make_matmulinteger_test(self, shape1, shape2):  # type: (Sequence[int], Sequence[int]) -> None\n        expected_out_shape = np.matmul(np.arange(np.product(shape1)).reshape(shape1),\n                                       np.arange(np.product(shape2)).reshape(shape2)).shape\n        graph = self._make_graph(\n            [(\'A\', TensorProto.UINT8, shape1),\n             (\'B\', TensorProto.UINT8, shape2),\n             (\'a_zero_point\', TensorProto.UINT8, ()),\n             (\'b_zero_point\', TensorProto.UINT8, ())],\n            [make_node(\'MatMulInteger\', [\'A\', \'B\', \'a_zero_point\', \'b_zero_point\'], [\'Y\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'Y\', TensorProto.INT32, expected_out_shape)])\n\n    def test_matmulinteger(self):  # type: () -> None\n        self._make_matmulinteger_test((2,), (2,))\n        self._make_matmulinteger_test((1, 2), (2, 3))\n        self._make_matmulinteger_test((2,), (2, 3))\n        self._make_matmulinteger_test((4, 2), (2,))\n        self._make_matmulinteger_test((5, 1, 4, 2), (1, 3, 2, 3))\n        self._make_matmulinteger_test((4, 2), (3, 2, 3))\n\n    def test_quantizelinear(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (30, 4, 5)),\n             (\'y_scale\', TensorProto.FLOAT, ()),\n             (\'y_zero_point\', TensorProto.UINT8, ())],\n            [make_node(\'QuantizeLinear\', [\'x\', \'y_scale\', \'y_zero_point\'], [\'y\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.UINT8, (30, 4, 5))])\n\n    def test_dequantizelinear(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.UINT8, (30, 4, 5)),\n             (\'x_scale\', TensorProto.FLOAT, ()),\n             (\'x_zero_point\', TensorProto.UINT8, ())],\n            [make_node(\'DequantizeLinear\', [\'x\', \'x_scale\', \'x_zero_point\'], [\'y\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (30, 4, 5))])\n\n    def test_reversesequence(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (4, 5, 6)),\n             (\'sequence_lens\', TensorProto.INT64, (5,))],\n            [make_node(\'ReverseSequence\', [\'x\', \'sequence_lens\'], [\'y\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (4, 5, 6))])\n\n    def test_unique_without_axis(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'X\', TensorProto.FLOAT, (2, 4, 2))],\n            [make_node(\'Unique\', [\'X\'], [\'Y\', \'indices\', \'inverse_indices\', \'counts\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'Y\', TensorProto.FLOAT, (None,)),  # type: ignore\n                                      make_tensor_value_info(\'indices\', TensorProto.INT64, (None,)),  # type: ignore\n                                      make_tensor_value_info(\'inverse_indices\', TensorProto.INT64, (None,)),  # type: ignore\n                                      make_tensor_value_info(\'counts\', TensorProto.INT64, (None,))])  # type: ignore\n\n    def test_unique_with_axis(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'X\', TensorProto.FLOAT, (2, 4, 2))],\n            [make_node(\'Unique\', [\'X\'], [\'Y\', \'indices\', \'inverse_indices\', \'counts\'], axis=1)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'Y\', TensorProto.FLOAT, (2, None, 2)),  # type: ignore\n                                      make_tensor_value_info(\'indices\', TensorProto.INT64, (None,)),  # type: ignore\n                                      make_tensor_value_info(\'inverse_indices\', TensorProto.INT64, (None,)),  # type: ignore\n                                      make_tensor_value_info(\'counts\', TensorProto.INT64, (None,))])  # type: ignore\n\n    def test_det(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'X\', TensorProto.FLOAT, (3, 3))],\n            [make_node(\'Det\', [\'X\'], [\'Y\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'Y\', TensorProto.FLOAT, ())])\n\n        graph = self._make_graph(\n            [(\'X\', TensorProto.FLOAT, (4, 5, 6, 7, 7))],\n            [make_node(\'Det\', [\'X\'], [\'Y\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'Y\', TensorProto.FLOAT, (4, 5, 6))])\n\n    def test_tile(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (4, 5, 6)),\n             (\'repeats\', TensorProto.INT64, (3,))],\n            [make_node(\'Tile\', [\'x\', \'repeats\'], [\'y\'])],\n            [],\n            initializer=[make_tensor(\'repeats\', TensorProto.INT64, (3,), (1, 2, 3))])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (4, 10, 18))])\n\n    def test_tile_raw_input_data(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (4, 5, 6)),\n             (\'repeats\', TensorProto.INT64, (3,))],\n            [make_node(\'Tile\', [\'x\', \'repeats\'], [\'y\'])],\n            [],\n            initializer=[make_tensor(\'repeats\', TensorProto.INT64, (3,),\n                                     vals=np.array([1, 2, 3], dtype=\'<i8\').tobytes(), raw=True)])  # Feed raw bytes (force little endian ordering like onnx standard) for test purpose\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (4, 10, 18))])\n\n    def test_tile_rank_inference(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (4, 5, 6)),\n             (\'repeats\', TensorProto.INT64, (3,))],\n            [make_node(\'Tile\', [\'x\', \'repeats\'], [\'y\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (None, None, None))])  # type: ignore\n\n    def test_linearclassifier_1D_input(self):  # type: () -> None\n        if ONNX_ML:\n            graph = self._make_graph(\n                [(\'x\', TensorProto.FLOAT, (5,))],\n                [make_node(\'LinearClassifier\', [\'x\'], [\'y\', \'z\'], domain=ONNX_ML_DOMAIN, coefficients=[0.0008, -0.0008], intercepts=[2.0, 2.0], classlabels_ints=[1, 2])],\n                [])\n            self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.INT64, (1,)),\n                                          make_tensor_value_info(\'z\', TensorProto.FLOAT, (1, 2))],\n                                          opset_imports=[make_opsetid(ONNX_ML_DOMAIN, 1), make_opsetid(ONNX_DOMAIN, 11)])\n\n    def test_linearclassifier_2D_input(self):  # type: () -> None\n        if ONNX_ML:\n            graph = self._make_graph(\n                [(\'x\', TensorProto.FLOAT, (4, 5))],\n                [make_node(\'LinearClassifier\', [\'x\'], [\'y\', \'z\'], domain=ONNX_ML_DOMAIN, coefficients=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6], intercepts=[2.0, 2.0, 3.0], classlabels_ints=[1, 2, 3])],\n                [])\n            self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.INT64, (4,)),\n                                          make_tensor_value_info(\'z\', TensorProto.FLOAT, (4, 3))],\n                                          opset_imports=[make_opsetid(ONNX_ML_DOMAIN, 1), make_opsetid(ONNX_DOMAIN, 11)])\n\n    def test_roialign_symbolic(self):   # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (\'N\', \'C\', \'H\', \'W\')),\n             (\'rois\', TensorProto.FLOAT, (\'num_rois\', 4)),\n             (\'batch_indices\', TensorProto.INT64, (\'num_rois\',))],\n            [make_node(\'RoiAlign\', [\'x\', \'rois\', \'batch_indices\'], [\'y\'], output_height=10, output_width=5)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (\'num_rois\', \'C\', 10, 5))])  # type: ignore\n\n    def test_roialign_symbolic_defaults(self):   # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (\'N\', \'C\', \'H\', \'W\')),\n             (\'rois\', TensorProto.FLOAT, (\'num_rois\', 4)),\n             (\'batch_indices\', TensorProto.INT64, (\'num_rois\',))],\n            [make_node(\'RoiAlign\', [\'x\', \'rois\', \'batch_indices\'], [\'y\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (\'num_rois\', \'C\', 1, 1))])  # type: ignore\n\n    def test_roialign_num_rois(self):   # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (\'N\', \'C\', \'H\', \'W\')),\n             (\'rois\', TensorProto.FLOAT, (\'num_rois\', 4)),\n             (\'batch_indices\', TensorProto.INT64, (15,))],\n            [make_node(\'RoiAlign\', [\'x\', \'rois\', \'batch_indices\'], [\'y\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (15, \'C\', 1, 1))])  # type: ignore\n\n    def test_label_encoder_string_int64(self):  # type: () -> None\n        if ONNX_ML:\n            string_list = [\'A\', \'m\', \'y\']\n            float_list = [94.17, 36.00]\n            int64_list = [12, 28, 86]\n            graph = self._make_graph(\n                [(\'x\', TensorProto.STRING, (6, 1))],\n                [make_node(\'LabelEncoder\', [\'x\'], [\'y\'], domain=ONNX_ML_DOMAIN,\n                           keys_strings=string_list, values_int64s=int64_list)], [])\n            self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.INT64, (6, 1))],\n                                          opset_imports=[make_opsetid(ONNX_ML_DOMAIN, 2), make_opsetid(ONNX_DOMAIN, 11)])\n\n            graph = self._make_graph(\n                [(\'x\', TensorProto.INT64, (2, 3))],\n                [make_node(\'LabelEncoder\', [\'x\'], [\'y\'], domain=ONNX_ML_DOMAIN,\n                           keys_int64s=int64_list, values_strings=string_list)], [])\n            self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.STRING, (2, 3))],\n                                  opset_imports=[make_opsetid(ONNX_ML_DOMAIN, 2), make_opsetid(ONNX_DOMAIN, 11)])\n\n            graph = self._make_graph(\n                [(\'x\', TensorProto.FLOAT, (2,))],\n                [make_node(\'LabelEncoder\', [\'x\'], [\'y\'], domain=ONNX_ML_DOMAIN,\n                           keys_floats=float_list, values_int64s=int64_list)], [])\n            self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.INT64, (2,))],\n                                  opset_imports=[make_opsetid(ONNX_ML_DOMAIN, 2), make_opsetid(ONNX_DOMAIN, 11)])\n\n            graph = self._make_graph(\n                [(\'x\', TensorProto.INT64, (8,))],\n                [make_node(\'LabelEncoder\', [\'x\'], [\'y\'], domain=ONNX_ML_DOMAIN,\n                           keys_int64s=int64_list, values_floats=float_list)], [])\n            self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (8,))],\n                                  opset_imports=[make_opsetid(ONNX_ML_DOMAIN, 2), make_opsetid(ONNX_DOMAIN, 11)])\n\n            graph = self._make_graph(\n                [(\'x\', TensorProto.FLOAT, ())],\n                [make_node(\'LabelEncoder\', [\'x\'], [\'y\'], domain=ONNX_ML_DOMAIN,\n                           keys_floats=float_list, values_strings=string_list)], [])\n            self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.STRING, ())],\n                                  opset_imports=[make_opsetid(ONNX_ML_DOMAIN, 2), make_opsetid(ONNX_DOMAIN, 11)])\n\n            graph = self._make_graph(\n                [(\'x\', TensorProto.STRING, (1, 2))],\n                [make_node(\'LabelEncoder\', [\'x\'], [\'y\'], domain=ONNX_ML_DOMAIN,\n                           keys_strings=string_list, values_floats=float_list)], [])\n            self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (1, 2))],\n                                  opset_imports=[make_opsetid(ONNX_ML_DOMAIN, 2), make_opsetid(ONNX_DOMAIN, 11)])\n\n    def make_sparse(self,\n                    shape,  # type: Sequence[int]\n                    values,  # type: Sequence[int]\n                    indices_shape,  # type: Sequence[int]\n                    indices  # type: Sequence[int]\n                    ):  # type: (...) -> SparseTensorProto\n        sparse = SparseTensorProto()\n        sparse.dims.extend(shape)\n        nnz = len(values)\n        sparse.values.CopyFrom(helper.make_tensor(\'spval\', TensorProto.INT64, (nnz,), values))\n        sparse.indices.CopyFrom(helper.make_tensor(\'spind\', TensorProto.INT64, indices_shape, indices))\n        return sparse\n\n    def test_constant_sparse(self):  # type: () -> None\n        y_shape = [100]\n        y_value = self.make_sparse(y_shape, [13, 17, 19], [3], [9, 27, 81])\n        graph = self._make_graph(\n            [],\n            [make_node(\'Constant\', [], [\'y\'], sparse_value=y_value)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.INT64, y_shape)])  # type: ignore\n\n    def test_constant_value_int(self):  # type: () -> None\n        graph = self._make_graph(\n            [],\n            [make_node(\'Constant\', [], [\'y\'], value_int=42)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.INT64, [])])\n\n    def test_constant_value_ints(self):  # type: () -> None\n        value_ints = [1, 2, 3]\n        graph = self._make_graph(\n            [],\n            [make_node(\'Constant\', [], [\'y\'], value_ints=value_ints)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.INT64, [len(value_ints)])])\n\n    def test_constant_value_float(self):  # type: () -> None\n        graph = self._make_graph(\n            [],\n            [make_node(\'Constant\', [], [\'y\'], value_float=1.42)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, [])])\n\n    def test_constant_value_floats(self):  # type: () -> None\n        value_floats = [1.0, 1.1, 1.2]\n        graph = self._make_graph(\n            [],\n            [make_node(\'Constant\', [], [\'y\'], value_floats=value_floats)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, [len(value_floats)])])\n\n    def test_constant_value_string(self):  # type: () -> None\n        graph = self._make_graph(\n            [],\n            [make_node(\'Constant\', [], [\'y\'], value_string=""String value"")],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.STRING, [])])\n\n    def test_constant_value_strings(self):  # type: () -> None\n        value_strings = [""o"", ""n"", ""n"", ""x""]\n        graph = self._make_graph(\n            [],\n            [make_node(\'Constant\', [], [\'y\'], value_strings=value_strings)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.STRING, [len(value_strings)])])\n\n    def test_range(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'start\', TensorProto.FLOAT, ()),\n             (\'limit\', TensorProto.FLOAT, ()),\n             (\'delta\', TensorProto.FLOAT, ())],\n            [make_node(\'Range\', [\'start\', \'limit\', \'delta\'], [\'output\'])],\n            [],\n            initializer=[make_tensor(\'start\', TensorProto.FLOAT, (), (1,)),\n                         make_tensor(\'limit\', TensorProto.FLOAT, (), (5,)),\n                         make_tensor(\'delta\', TensorProto.FLOAT, (), (2,))])\n        self._assert_inferred(graph, [make_tensor_value_info(\'output\', TensorProto.FLOAT, (2,))])\n\n    def test_range_rank_inference(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'start\', TensorProto.INT32, ()),\n             (\'limit\', TensorProto.INT32, ()),\n             (\'delta\', TensorProto.INT32, ())],\n            [make_node(\'Range\', [\'start\', \'limit\', \'delta\'], [\'output\'])],\n            [],\n            initializer=[make_tensor(\'start\', TensorProto.INT32, (), (1,)),\n                         make_tensor(\'limit\', TensorProto.INT32, (), (5,))])  # Missing \'delta\' initializer\n        self._assert_inferred(graph, [make_tensor_value_info(\'output\', TensorProto.INT32, (None,))])  # type: ignore\n\n    def test_gathernd(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (4, 5, 6)),\n             (\'indices\', TensorProto.INT64, (2,))],\n            [make_node(\'GatherND\', [\'x\', \'indices\'], [\'y\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (6,))])\n\n    def test_gathernd_batchdim_1(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (2, 2, 2)),\n             (\'indices\', TensorProto.INT64, (2, 1))],\n            [make_node(\'GatherND\', [\'x\', \'indices\'], [\'y\'], batch_dims=1)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (2, 2))])\n\n    def test_cumsum(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (2, 3)),\n             (\'axis\', TensorProto.FLOAT, (1,))],\n            [make_node(\'CumSum\', [\'x\', \'axis\'], \'z\')],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, (2, 3))])\n\n    def test_nonmaxsuppression(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'boxes\', TensorProto.FLOAT, (1, 3, 4)),\n             (\'scores\', TensorProto.FLOAT, (1, 5, 3))],\n            [make_node(\'NonMaxSuppression\', [\'boxes\', \'scores\'], [\'y\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.INT64, (None, 3))])  # type: ignore\n\n    def test_sequence_empty(self):  # type: () -> None\n        graph = self._make_graph(\n            [],\n            [make_node(\'SequenceEmpty\', [], [\'output\'])],\n            [])\n        self._assert_inferred(graph, [make_sequence_value_info(\'output\', TensorProto.FLOAT, None)])  # type: ignore\n\n    def test_sequence_construct(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'input1\', TensorProto.FLOAT, (2, 3, 4)),\n             (\'input2\', TensorProto.FLOAT, (2, 3, 4)),\n             (\'input3\', TensorProto.FLOAT, (2, 3, 4))],\n            [make_node(\'SequenceConstruct\', [\'input1\', \'input2\', \'input3\'], [\'output_sequence\'])],\n            [])\n        self._assert_inferred(graph,\n            [make_sequence_value_info(\'output_sequence\', TensorProto.FLOAT, (2, 3, 4))])  # type: ignore\n\n    def test_sequence_construct_one_input(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'input1\', TensorProto.FLOAT, (2, 3, 4))],\n            [make_node(\'SequenceConstruct\', [\'input1\'], [\'output_sequence\'])],\n            [])\n        self._assert_inferred(graph,\n            [make_sequence_value_info(\'output_sequence\', TensorProto.FLOAT, (2, 3, 4))])  # type: ignore\n\n    def test_sequence_construct_diff_rank(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'input1\', TensorProto.FLOAT, (2, 3, 4)),\n             (\'input2\', TensorProto.FLOAT, (2, 3)),\n             (\'input3\', TensorProto.FLOAT, (2, 3))],\n            [make_node(\'SequenceConstruct\', [\'input1\', \'input2\', \'input3\'], [\'output_sequence\'])],\n            [])\n        self._assert_inferred(graph,\n            [make_sequence_value_info(\'output_sequence\', TensorProto.FLOAT, None)])  # type: ignore\n\n    def test_sequence_construct_diff_dim_size(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'input1\', TensorProto.FLOAT, (2, 3, 4)),\n             (\'input2\', TensorProto.FLOAT, (2, 3, 5)),\n             (\'input3\', TensorProto.FLOAT, (2, 3, 6))],\n            [make_node(\'SequenceConstruct\', [\'input1\', \'input2\', \'input3\'], [\'output_sequence\'])],\n            [])\n        self._assert_inferred(graph,\n            [make_sequence_value_info(\'output_sequence\', TensorProto.FLOAT, (2, 3, None))])  # type: ignore\n\n    def test_sequence_insert(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'input1\', TensorProto.FLOAT, (2, 3, 4)),\n             (\'input2\', TensorProto.FLOAT, (2, 3, 4)),\n             (\'input3\', TensorProto.FLOAT, (2, 3, 4)),\n             (\'input4\', TensorProto.FLOAT, (2, 3, 4))],\n            [make_node(\'SequenceConstruct\', [\'input1\', \'input2\', \'input3\'], [\'in_sequence\']),\n             make_node(\'SequenceInsert\', [\'in_sequence\', \'input4\'], [\'output_sequence\'])],\n            [])\n        self._assert_inferred(\n            graph,\n            [make_sequence_value_info(\'in_sequence\', TensorProto.FLOAT, (2, 3, 4)),\n             make_sequence_value_info(\'output_sequence\', TensorProto.FLOAT, (2, 3, 4))])  # type: ignore\n\n    def test_sequence_insert_diff_rank(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'input1\', TensorProto.FLOAT, (2, 3, 4)),\n             (\'input2\', TensorProto.FLOAT, (2, 3, 4)),\n             (\'input3\', TensorProto.FLOAT, (2, 3, 4)),\n             (\'input4\', TensorProto.FLOAT, (2, 3))],\n            [make_node(\'SequenceConstruct\', [\'input1\', \'input2\', \'input3\'], [\'in_sequence\']),\n             make_node(\'SequenceInsert\', [\'in_sequence\', \'input4\'], [\'output_sequence\'])],\n            [])\n        self._assert_inferred(\n            graph,\n            [make_sequence_value_info(\'in_sequence\', TensorProto.FLOAT, (2, 3, 4)),\n             make_sequence_value_info(\'output_sequence\', TensorProto.FLOAT, None)])  # type: ignore\n\n    def test_sequence_insert_diff_shape(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'input1\', TensorProto.FLOAT, (2, 3, 4)),\n             (\'input2\', TensorProto.FLOAT, (2, 3, 4)),\n             (\'input3\', TensorProto.FLOAT, (2, 5, 4)),\n             (\'input4\', TensorProto.FLOAT, (2, 5, 2))],\n            [make_node(\'SequenceConstruct\', [\'input1\', \'input2\', \'input3\'], [\'in_sequence\']),\n             make_node(\'SequenceInsert\', [\'in_sequence\', \'input4\'], [\'output_sequence\'])],\n            [])\n        self._assert_inferred(\n            graph,\n            [make_sequence_value_info(\'in_sequence\', TensorProto.FLOAT, (2, None, 4)),  # type: ignore\n             make_sequence_value_info(\'output_sequence\', TensorProto.FLOAT, (2, None, None))])  # type: ignore\n\n    def test_sequence_at(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'input1\', TensorProto.FLOAT, (2, 3, 4)),\n             (\'input2\', TensorProto.FLOAT, (2, 3, 4)),\n             (\'input3\', TensorProto.FLOAT, (2, 3, 4)),\n             (\'ind\', TensorProto.INT64, ())],\n            [make_node(\'SequenceConstruct\', [\'input1\', \'input2\', \'input3\'], [\'in_sequence\']),\n             make_node(\'SequenceAt\', [\'in_sequence\', \'ind\'], [\'output\'])],\n            [])\n        self._assert_inferred(\n            graph,\n            [make_sequence_value_info(\'in_sequence\', TensorProto.FLOAT, (2, 3, 4)),\n             make_tensor_value_info(\'output\', TensorProto.FLOAT, (2, 3, 4))])  # type: ignore\n\n    def test_sequence_at_unknown_shape(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'input1\', TensorProto.FLOAT, (2, 3, 4)),\n             (\'input2\', TensorProto.FLOAT, (2, 3)),\n             (\'input3\', TensorProto.FLOAT, (2, 3, 4)),\n             (\'ind\', TensorProto.INT64, ())],\n            [make_node(\'SequenceConstruct\', [\'input1\', \'input2\', \'input3\'], [\'in_sequence\']),\n             make_node(\'SequenceAt\', [\'in_sequence\', \'ind\'], [\'output\'])],\n            [])\n        self._assert_inferred(\n            graph,\n            [make_sequence_value_info(\'in_sequence\', TensorProto.FLOAT, None),\n             make_tensor_value_info(\'output\', TensorProto.FLOAT, None)])  # type: ignore\n\n    def test_sequence_at_unknown_dim_size(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'input1\', TensorProto.FLOAT, (2, 3, 4)),\n             (\'input2\', TensorProto.FLOAT, (2, 3, 5)),\n             (\'input3\', TensorProto.FLOAT, (2, 3, 4)),\n             (\'ind\', TensorProto.INT64, ())],\n            [make_node(\'SequenceConstruct\', [\'input1\', \'input2\', \'input3\'], [\'in_sequence\']),\n             make_node(\'SequenceAt\', [\'in_sequence\', \'ind\'], [\'output\'])],\n            [])\n        self._assert_inferred(\n            graph,\n            [make_sequence_value_info(\'in_sequence\', TensorProto.FLOAT, (2, 3, None)),  # type: ignore\n             make_tensor_value_info(\'output\', TensorProto.FLOAT, (2, 3, None))])  # type: ignore\n\n    def test_sequence_erase(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'input1\', TensorProto.FLOAT, (2, 3, 4)),\n             (\'input2\', TensorProto.FLOAT, (2, 3, 4)),\n             (\'input3\', TensorProto.FLOAT, (2, 3, 4)),\n             (\'ind\', TensorProto.INT64, ())],\n            [make_node(\'SequenceConstruct\', [\'input1\', \'input2\', \'input3\'], [\'in_sequence\']),\n             make_node(\'SequenceErase\', [\'in_sequence\', \'ind\'], [\'output_sequence\'])],\n            [])\n        self._assert_inferred(\n            graph,\n            [make_sequence_value_info(\'in_sequence\', TensorProto.FLOAT, (2, 3, 4)),\n             make_sequence_value_info(\'output_sequence\', TensorProto.FLOAT, (2, 3, 4))])  # type: ignore\n\n    def test_sequence_erase_diff_dim_size(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'input1\', TensorProto.FLOAT, (2, 3, \'x\')),\n             (\'input2\', TensorProto.FLOAT, (2, 3, \'x\')),\n             (\'input3\', TensorProto.FLOAT, (2, 5, \'x\')),\n             (\'ind\', TensorProto.INT64, ())],\n            [make_node(\'SequenceConstruct\', [\'input1\', \'input2\', \'input3\'], [\'in_sequence\']),\n             make_node(\'SequenceErase\', [\'in_sequence\', \'ind\'], [\'output_sequence\'])],\n            [])\n        self._assert_inferred(\n            graph,\n            [make_sequence_value_info(\'in_sequence\', TensorProto.FLOAT, (2, None, \'x\')),  # type: ignore\n             make_sequence_value_info(\'output_sequence\', TensorProto.FLOAT, (2, None, \'x\'))])  # type: ignore\n\n    def test_sequence_length(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'input1\', TensorProto.FLOAT, (2, 3, \'x\')),\n             (\'input2\', TensorProto.FLOAT, (2, 3, \'x\')),\n             (\'input3\', TensorProto.FLOAT, (2, 3, \'x\'))],\n            [make_node(\'SequenceConstruct\', [\'input1\', \'input2\', \'input3\'], [\'in_sequence\']),\n             make_node(\'SequenceLength\', [\'in_sequence\'], [\'len\'])],\n            [])\n        self._assert_inferred(\n            graph,\n            [make_sequence_value_info(\'in_sequence\', TensorProto.FLOAT, (2, 3, \'x\')),\n             make_tensor_value_info(\'len\', TensorProto.INT64, ())])  # type: ignore\n\n    def test_split_to_sequence(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'input\', TensorProto.FLOAT, (6, 4)),\n             (\'split\', TensorProto.INT32, (2,))],\n            [make_node(\'SplitToSequence\', [\'input\', \'split\'], [\'output_sequence\'])],\n            [],\n            initializer=[make_tensor(\'split\', TensorProto.INT32, (), (3, 3))])\n        self._assert_inferred(graph,\n            [make_sequence_value_info(\'output_sequence\', TensorProto.FLOAT, (3, 4))])  # type: ignore\n\n    def test_split_to_sequence_scalar(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'input\', TensorProto.FLOAT, (6, 4)),\n             (\'split\', TensorProto.INT32, ())],\n            [make_node(\'SplitToSequence\', [\'input\', \'split\'], [\'output_sequence\'])],\n            [],\n            initializer=[make_tensor(\'split\', TensorProto.INT32, (), (2, ))])\n        self._assert_inferred(graph,\n            [make_sequence_value_info(\'output_sequence\', TensorProto.FLOAT, (2, 4))])  # type: ignore\n\n    def test_split_to_sequence_keepdims(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'input\', TensorProto.FLOAT, (6, 4))],\n            [make_node(\'SplitToSequence\', [\'input\'], [\'output_sequence\'], keepdims=1)],\n            [])\n        self._assert_inferred(graph,\n            [make_sequence_value_info(\'output_sequence\', TensorProto.FLOAT, (1, 4))])  # type: ignore\n\n    def test_split_to_sequence_not_keepdims(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'input\', TensorProto.FLOAT, (6, 4))],\n            [make_node(\'SplitToSequence\', [\'input\'], [\'output_sequence\'], keepdims=0)],\n            [])\n        self._assert_inferred(graph,\n            [make_sequence_value_info(\'output_sequence\', TensorProto.FLOAT, (4, ))])  # type: ignore\n\n    def test_split_to_sequence_ignore_keepdims(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'input\', TensorProto.FLOAT, (6, 4)),\n             (\'split\', TensorProto.INT32, (2,))],\n            [make_node(\'SplitToSequence\', [\'input\', \'split\'], [\'output_sequence\'], keepdims=0)],\n            [],\n            initializer=[make_tensor(\'split\', TensorProto.INT32, (), (3, 3))])\n        self._assert_inferred(graph,\n            [make_sequence_value_info(\'output_sequence\', TensorProto.FLOAT, (3, 4))])  # type: ignore\n\n    def test_split_to_sequence_axis(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'input\', TensorProto.FLOAT, (6, 4))],\n            [make_node(\'SplitToSequence\', [\'input\'], [\'output_sequence\'], axis=1)],\n            [])\n        self._assert_inferred(graph,\n            [make_sequence_value_info(\'output_sequence\', TensorProto.FLOAT, (6, 1))])  # type: ignore\n\n    def test_split_to_sequence_neg_axis(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'input\', TensorProto.FLOAT, (6, 4))],\n            [make_node(\'SplitToSequence\', [\'input\'], [\'output_sequence\'], axis=-2)],\n            [])\n        self._assert_inferred(graph,\n            [make_sequence_value_info(\'output_sequence\', TensorProto.FLOAT, (1, 4))])  # type: ignore\n\n    def test_split_to_sequence_split_sizes(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'input\', TensorProto.FLOAT, (6, 4)),\n             (\'split\', TensorProto.INT32, (3,))],\n            [make_node(\'SplitToSequence\', [\'input\', \'split\'], [\'output_sequence\'])],\n            [],\n            initializer=[make_tensor(\'split\', TensorProto.INT32, (), (2, 1, 3))])\n        self._assert_inferred(graph,\n            [make_sequence_value_info(\'output_sequence\', TensorProto.FLOAT, (None, 4))])  # type: ignore\n\n    def test_split_to_sequence_non_divisible(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'input\', TensorProto.FLOAT, (6, 4)),\n             (\'split\', TensorProto.INT32, ())],\n            [make_node(\'SplitToSequence\', [\'input\', \'split\'], [\'output_sequence\'])],\n            [],\n            initializer=[make_tensor(\'split\', TensorProto.INT32, (), (4, ))])\n        self._assert_inferred(graph,\n            [make_sequence_value_info(\'output_sequence\', TensorProto.FLOAT, (None, 4))])  # type: ignore\n\n    def test_concat_from_sequence(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'input1\', TensorProto.FLOAT, (2, 3, \'x\')),\n             (\'input2\', TensorProto.FLOAT, (2, 3, \'x\')),\n             (\'input3\', TensorProto.FLOAT, (2, 3, \'x\'))],\n            [make_node(\'SequenceConstruct\', [\'input1\', \'input2\', \'input3\'], [\'in_sequence\']),\n             make_node(\'ConcatFromSequence\', [\'in_sequence\'], [\'out\'], axis=0)],\n            [])\n        self._assert_inferred(\n            graph,\n            [make_sequence_value_info(\'in_sequence\', TensorProto.FLOAT, (2, 3, \'x\')),\n             make_tensor_value_info(\'out\', TensorProto.FLOAT, (None, 3, \'x\'))])  # type: ignore\n\n    def test_concat_from_sequence_unknown_shape(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'input1\', TensorProto.FLOAT, (2, 3, \'x\')),\n             (\'input2\', TensorProto.FLOAT, (2, 3)),\n             (\'input3\', TensorProto.FLOAT, (2, 3, \'x\'))],\n            [make_node(\'SequenceConstruct\', [\'input1\', \'input2\', \'input3\'], [\'in_sequence\']),\n             make_node(\'ConcatFromSequence\', [\'in_sequence\'], [\'out\'], axis=0)],\n            [])\n        self._assert_inferred(\n            graph,\n            [make_sequence_value_info(\'in_sequence\', TensorProto.FLOAT, None),\n             make_tensor_value_info(\'out\', TensorProto.FLOAT, None)])  # type: ignore\n\n    def test_concat_from_sequence_unknown_dim_size(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'input1\', TensorProto.FLOAT, (2, 3, \'x\')),\n             (\'input2\', TensorProto.FLOAT, (2, 4, \'x\')),\n             (\'input3\', TensorProto.FLOAT, (2, 3, \'x\'))],\n            [make_node(\'SequenceConstruct\', [\'input1\', \'input2\', \'input3\'], [\'in_sequence\']),\n             make_node(\'ConcatFromSequence\', [\'in_sequence\'], [\'out\'], axis=0)],\n            [])\n        self._assert_inferred(\n            graph,\n            [make_sequence_value_info(\'in_sequence\', TensorProto.FLOAT, (2, None, \'x\')),  # type: ignore\n             make_tensor_value_info(\'out\', TensorProto.FLOAT, (None, None, \'x\'))])  # type: ignore\n\n    def test_concat_from_sequence_axis(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'input1\', TensorProto.FLOAT, (2, 3, \'x\')),\n             (\'input2\', TensorProto.FLOAT, (2, 4, \'x\')),\n             (\'input3\', TensorProto.FLOAT, (2, 3, \'x\'))],\n            [make_node(\'SequenceConstruct\', [\'input1\', \'input2\', \'input3\'], [\'in_sequence\']),\n             make_node(\'ConcatFromSequence\', [\'in_sequence\'], [\'out\'], axis=2)],\n            [])\n        self._assert_inferred(\n            graph,\n            [make_sequence_value_info(\'in_sequence\', TensorProto.FLOAT, (2, None, \'x\')),  # type: ignore\n             make_tensor_value_info(\'out\', TensorProto.FLOAT, (2, None, None))])  # type: ignore\n\n    def test_concat_from_sequence_neg_axis(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'input1\', TensorProto.FLOAT, (2, 3, \'x\')),\n             (\'input2\', TensorProto.FLOAT, (2, 4, \'x\')),\n             (\'input3\', TensorProto.FLOAT, (2, 3, \'x\'))],\n            [make_node(\'SequenceConstruct\', [\'input1\', \'input2\', \'input3\'], [\'in_sequence\']),\n             make_node(\'ConcatFromSequence\', [\'in_sequence\'], [\'out\'], axis=-3)],\n            [])\n        self._assert_inferred(\n            graph,\n            [make_sequence_value_info(\'in_sequence\', TensorProto.FLOAT, (2, None, \'x\')),  # type: ignore\n             make_tensor_value_info(\'out\', TensorProto.FLOAT, (None, None, \'x\'))])  # type: ignore\n\n    def test_concat_from_sequence_new_axis(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'input1\', TensorProto.FLOAT, (2, 3, \'x\')),\n             (\'input2\', TensorProto.FLOAT, (2, 3, \'x\')),\n             (\'input3\', TensorProto.FLOAT, (2, 3, \'x\'))],\n            [make_node(\'SequenceConstruct\', [\'input1\', \'input2\', \'input3\'], [\'in_sequence\']),\n             make_node(\'ConcatFromSequence\', [\'in_sequence\'], [\'out\'], axis=2, new_axis=1)],\n            [])\n        self._assert_inferred(\n            graph,\n            [make_sequence_value_info(\'in_sequence\', TensorProto.FLOAT, (2, 3, \'x\')),\n             make_tensor_value_info(\'out\', TensorProto.FLOAT, (2, 3, None, \'x\'))])  # type: ignore\n\n    def test_concat_from_sequence_neg_new_axis(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'input1\', TensorProto.FLOAT, (2, 3, \'x\')),\n             (\'input2\', TensorProto.FLOAT, (2, 3, \'x\')),\n             (\'input3\', TensorProto.FLOAT, (2, 3, \'x\'))],\n            [make_node(\'SequenceConstruct\', [\'input1\', \'input2\', \'input3\'], [\'in_sequence\']),\n             make_node(\'ConcatFromSequence\', [\'in_sequence\'], [\'out\'], axis=-1, new_axis=1)],\n            [])\n        self._assert_inferred(\n            graph,\n            [make_sequence_value_info(\'in_sequence\', TensorProto.FLOAT, (2, 3, \'x\')),\n             make_tensor_value_info(\'out\', TensorProto.FLOAT, (2, 3, \'x\', None))])  # type: ignore\n\n    def test_adagrad(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'R\', TensorProto.FLOAT, ()),  # scalar\'s shape is ()\n             (\'T\', TensorProto.INT64, ()),  # scalar\'s shape is ()\n             (\'X\', TensorProto.FLOAT, (1, 2)),\n             (\'G\', TensorProto.FLOAT, (1, 2)),\n             (\'H\', TensorProto.FLOAT, (1, 2))],\n            [make_node(\'Adagrad\', [\'R\', \'T\', \'X\', \'G\', \'H\'], [\'X_new\', \'H_new\'],\n                       domain=AI_ONNX_PREVIEW_TRAINING_DOMAIN)],\n            [])\n\n        self._assert_inferred(\n            graph,\n            [make_tensor_value_info(\'X_new\', TensorProto.FLOAT, (1, 2)),\n             make_tensor_value_info(\'H_new\', TensorProto.FLOAT, (1, 2))],\n            opset_imports=[helper.make_opsetid(ONNX_DOMAIN, 12), helper.make_opsetid(AI_ONNX_PREVIEW_TRAINING_DOMAIN, 1)])\n\n    def test_adagrad_multiple(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'R\', TensorProto.FLOAT, ()),  # scalar\'s shape is ()\n             (\'T\', TensorProto.INT64, ()),  # scalar\'s shape is ()\n             (\'X1\', TensorProto.FLOAT, (1, 2)),\n             (\'X2\', TensorProto.FLOAT, (3, 4)),\n             (\'G1\', TensorProto.FLOAT, (1, 2)),\n             (\'G2\', TensorProto.FLOAT, (3, 4)),\n             (\'H1\', TensorProto.FLOAT, (1, 2)),\n             (\'H2\', TensorProto.FLOAT, (3, 4))],\n            [make_node(\'Adagrad\', [\'R\', \'T\', \'X1\', \'X2\', \'G1\', \'G2\', \'H1\', \'H2\'],\n                       [\'X1_new\', \'X2_new\', \'H1_new\', \'H2_new\'],\n                       domain=AI_ONNX_PREVIEW_TRAINING_DOMAIN)],\n            [])\n\n        self._assert_inferred(graph,\n            [make_tensor_value_info(\'X1_new\', TensorProto.FLOAT, (1, 2)),\n             make_tensor_value_info(\'X2_new\', TensorProto.FLOAT, (3, 4)),\n             make_tensor_value_info(\'H1_new\', TensorProto.FLOAT, (1, 2)),\n             make_tensor_value_info(\'H2_new\', TensorProto.FLOAT, (3, 4))],\n            opset_imports=[helper.make_opsetid(ONNX_DOMAIN, 12), helper.make_opsetid(AI_ONNX_PREVIEW_TRAINING_DOMAIN, 1)])\n\n    def test_momentum(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'R\', TensorProto.FLOAT, ()),  # scalar\'s shape is ()\n             (\'T\', TensorProto.INT64, ()),  # scalar\'s shape is ()\n             (\'X\', TensorProto.FLOAT, (1, 2)),\n             (\'G\', TensorProto.FLOAT, (1, 2)),\n             (\'V\', TensorProto.FLOAT, (1, 2))],\n            [make_node(\'Momentum\', [\'R\', \'T\', \'X\', \'G\', \'V\'], [\'X_new\', \'V_new\'],\n             alpha=0.9, beta=1.0, norm_coefficient=0.02, mode=\'standard\',\n             domain=AI_ONNX_PREVIEW_TRAINING_DOMAIN)],\n            [])\n        self._assert_inferred(\n            graph,\n            [make_tensor_value_info(\'X_new\', TensorProto.FLOAT, (1, 2)),\n             make_tensor_value_info(\'V_new\', TensorProto.FLOAT, (1, 2))],\n            opset_imports=[helper.make_opsetid(ONNX_DOMAIN, 12), helper.make_opsetid(AI_ONNX_PREVIEW_TRAINING_DOMAIN, 1)])\n\n    def test_momentum_multiple(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'R\', TensorProto.FLOAT, ()),  # scalar\'s shape is ()\n             (\'T\', TensorProto.INT64, ()),  # scalar\'s shape is ()\n             (\'X1\', TensorProto.FLOAT, (1, 2)),\n             (\'X2\', TensorProto.FLOAT, (3, 4)),\n             (\'G1\', TensorProto.FLOAT, (1, 2)),\n             (\'G2\', TensorProto.FLOAT, (3, 4)),\n             (\'V1\', TensorProto.FLOAT, (1, 2)),\n             (\'V2\', TensorProto.FLOAT, (3, 4))],\n            [make_node(\'Momentum\', [\'R\', \'T\', \'X1\', \'X2\', \'G1\', \'G2\', \'V1\', \'V2\'],\n             [\'X1_new\', \'X2_new\', \'V1_new\', \'V2_new\'],\n             alpha=0.9, beta=1.0, norm_coefficient=0.02, mode=\'nesterov\',\n             domain=AI_ONNX_PREVIEW_TRAINING_DOMAIN)],\n            [])\n\n        self._assert_inferred(\n            graph,\n            [make_tensor_value_info(\'X1_new\', TensorProto.FLOAT, (1, 2)),\n             make_tensor_value_info(\'X2_new\', TensorProto.FLOAT, (3, 4)),\n             make_tensor_value_info(\'V1_new\', TensorProto.FLOAT, (1, 2)),\n             make_tensor_value_info(\'V2_new\', TensorProto.FLOAT, (3, 4))],\n            opset_imports=[helper.make_opsetid(ONNX_DOMAIN, 12), helper.make_opsetid(AI_ONNX_PREVIEW_TRAINING_DOMAIN, 1)])\n\n    def test_adam(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'R\', TensorProto.FLOAT, ()),  # scalar\'s shape is ()\n             (\'T\', TensorProto.INT64, ()),  # scalar\'s shape is ()\n             (\'X\', TensorProto.FLOAT, (1, 2)),\n             (\'G\', TensorProto.FLOAT, (1, 2)),\n             (\'V\', TensorProto.FLOAT, (1, 2)),\n             (\'H\', TensorProto.FLOAT, (1, 2))],\n            [make_node(\'Adam\', [\'R\', \'T\', \'X\', \'G\', \'V\', \'H\'], [\'X_new\', \'V_new\', \'H_new\'],\n             domain=AI_ONNX_PREVIEW_TRAINING_DOMAIN,\n             alpha=0.9, beta=1.0, norm_coefficient=0.02)],\n            [])\n\n        infos = [make_tensor_value_info(\'X_new\', TensorProto.FLOAT, (1, 2)),\n                 make_tensor_value_info(\'V_new\', TensorProto.FLOAT, (1, 2)),\n                 make_tensor_value_info(\'H_new\', TensorProto.FLOAT, (1, 2))]\n\n        self._assert_inferred(\n            graph,\n            infos,\n            opset_imports=[make_opsetid(AI_ONNX_PREVIEW_TRAINING_DOMAIN, 1), make_opsetid(ONNX_DOMAIN, 12)])\n\n    def test_adam_multiple(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'R\', TensorProto.FLOAT, ()),  # scalar\'s shape is ()\n             (\'T\', TensorProto.INT64, ()),  # scalar\'s shape is ()\n             (\'X1\', TensorProto.FLOAT, (1, 2)),\n             (\'X2\', TensorProto.FLOAT, (3, 4)),\n             (\'G1\', TensorProto.FLOAT, (1, 2)),\n             (\'G2\', TensorProto.FLOAT, (3, 4)),\n             (\'V1\', TensorProto.FLOAT, (1, 2)),\n             (\'V2\', TensorProto.FLOAT, (3, 4)),\n             (\'H1\', TensorProto.FLOAT, (1, 2)),\n             (\'H2\', TensorProto.FLOAT, (3, 4))],\n            [make_node(\'Adam\', [\'R\', \'T\', \'X1\', \'X2\', \'G1\', \'G2\', \'V1\', \'V2\', \'H1\', \'H2\'],\n             [\'X1_new\', \'X2_new\', \'V1_new\', \'V2_new\', \'H1_new\', \'H2_new\'],\n             domain=AI_ONNX_PREVIEW_TRAINING_DOMAIN,\n             alpha=0.9, beta=1.0, norm_coefficient=0.02)],\n            [])\n\n        infos = [make_tensor_value_info(\'X1_new\', TensorProto.FLOAT, (1, 2)),\n                 make_tensor_value_info(\'X2_new\', TensorProto.FLOAT, (3, 4)),\n                 make_tensor_value_info(\'V1_new\', TensorProto.FLOAT, (1, 2)),\n                 make_tensor_value_info(\'V2_new\', TensorProto.FLOAT, (3, 4)),\n                 make_tensor_value_info(\'H1_new\', TensorProto.FLOAT, (1, 2)),\n                 make_tensor_value_info(\'H2_new\', TensorProto.FLOAT, (3, 4))]\n\n        self._assert_inferred(\n            graph,\n            infos,\n            opset_imports=[make_opsetid(AI_ONNX_PREVIEW_TRAINING_DOMAIN, 1), make_opsetid(ONNX_DOMAIN, 12)])\n\n    def test_pad_opset10(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (1, None, 2))],\n            [make_node(\'Pad\', \'x\', \'y\', pads=[1, 3, 1, 1, 0, 1])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (3, None, 4))], opset_imports=[helper.make_opsetid(ONNX_DOMAIN, 10)])  # type: ignore\n\n    def test_constant_pad_2d_opset10(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (2, 3, 4, 4))],\n            [make_node(\'Pad\', \'x\', \'y\', pads=[0, 0, 3, 1, 0, 0, 4, 2], mode=""constant"", value=2.0)],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (2, 3, 11, 7))], opset_imports=[helper.make_opsetid(ONNX_DOMAIN, 10)])\n\n    def test_pad(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (1, None, 2)),\n             (\'pads\', TensorProto.INT64, (6,))],\n            [make_node(\'Pad\', [\'x\', \'pads\'], \'y\')],\n            [],\n            initializer=[make_tensor(\'pads\', TensorProto.INT64, (6,), (1, 3, 1, 1, 0, 1,))])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (3, None, 4))])  # type: ignore\n\n    def test_gatherelements_basic(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (6,)),\n             (\'indices\', TensorProto.INT64, (2,))],\n            [make_node(\'GatherElements\', [\'x\', \'indices\'], [\'y\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (2,))])\n\n    def test_gatherelements_indices_missing_shape(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (6,)),\n             (\'indices\', TensorProto.INT64, None)],  # type: ignore\n            [make_node(\'GatherElements\', [\'x\', \'indices\'], [\'y\'])],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, None)])  # type: ignore\n\n    def test_einsum_transpose(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (3, 4))],\n            [make_node(\'Einsum\', [\'x\'], [\'y\'], equation=\'ij->ji\')],\n            [],)\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (None, None))])  # type: ignore\n\n    def test_einsum_dot(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (1,)),\n             (\'y\', TensorProto.FLOAT, (1,))],\n            [make_node(\'Einsum\', [\'x\', \'y\'], [\'z\'], equation=\'i,i->\')],\n            [],)\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, ())])  # type: ignore\n\n    def test_einsum_scalar(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, ()),\n             (\'y\', TensorProto.FLOAT, ())],\n            [make_node(\'Einsum\', [\'x\', \'y\'], [\'z\'], equation=\',->\')],\n            [],)\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, ())])  # type: ignore\n\n    def test_einsum_outer_prod(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (3, 5)),\n             (\'y\', TensorProto.FLOAT, (7, 9))],\n            [make_node(\'Einsum\', [\'x\', \'y\'], [\'z\'], equation=\'ij,ab->ijab\')],\n            [],)\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, (None, None, None, None))])  # type: ignore\n\n    def test_einsum_sum_along_dim(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (3, 4))],\n            [make_node(\'Einsum\', [\'x\'], [\'y\'], equation=\'i j->i \')],\n            [],)\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (None, ))])  # type: ignore\n\n    def test_einsum_ellipsis(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (3, 4, 4))],\n            [make_node(\'Einsum\', [\'x\'], [\'y\'], equation=\'... ii ->... i\')],\n            [],)\n        self._assert_inferred(graph, [make_tensor_value_info(\'y\', TensorProto.FLOAT, (None, None))])  # type: ignore\n\n    def test_einsum_ellipsis_2(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (2, 2, 2)),\n             (\'y\', TensorProto.FLOAT, (2, 2, 2))],\n            [make_node(\'Einsum\', [\'x\', \'y\'], [\'z\'], equation=\'...ij,...jk->...ik\')],\n            [], )\n        self._assert_inferred(graph,\n                              [make_tensor_value_info(\'z\', TensorProto.FLOAT, (None, None, None))])  # type: ignore\n\n    def test_einsum_ellipsis_3(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (2, 2, 2)),\n             (\'y\', TensorProto.FLOAT, (2, 2, 2))],\n            [make_node(\'Einsum\', [\'x\', \'y\'], [\'z\'], equation=\'...ij,...jk\')],\n            [], )\n        self._assert_inferred(graph,\n                              [make_tensor_value_info(\'z\', TensorProto.FLOAT, (None, None, None))])  # type: ignore\n\n    def test_einsum_contraction(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (5, 6, 7, 8)),\n             (\'y\', TensorProto.FLOAT, (8, 9, 10))],\n            [make_node(\'Einsum\', [\'x\', \'y\'], [\'z\'], equation=\'abcd,dfg->abcfg\')],\n            [], )\n        self._assert_inferred(graph,\n                              [make_tensor_value_info(\'z\', TensorProto.FLOAT, (None, None, None, None, None))])  # type: ignore\n\n    def test_einsum_contraction_2(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (3, 4, 5)),\n             (\'y\', TensorProto.FLOAT, (3, 5))],\n            [make_node(\'Einsum\', [\'x\', \'y\'], [\'z\'], equation=\'ijk,ik->jk\')],\n            [], )\n        self._assert_inferred(graph,\n                              [make_tensor_value_info(\'z\', TensorProto.FLOAT, (None, None))])  # type: ignore\n\n    def test_einsum_batch_matmul(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (5, 2, 3)),\n             (\'y\', TensorProto.FLOAT, (5, 3, 4))],\n            [make_node(\'Einsum\', [\'x\', \'y\'], [\'z\'], equation=\'bij , b jk-> bik\')],\n            [],)\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, (None, None, None))])  # type: ignore\n\n    def test_einsum_left_hand_eqn(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'x\', TensorProto.FLOAT, (2, 3)),\n             (\'y\', TensorProto.FLOAT, (3, 4))],\n            [make_node(\'Einsum\', [\'x\', \'y\'], [\'z\'], equation=\'ij,kl\')],\n            [],)\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, (None, None, None, None))])  # type: ignore\n\n    def test_einsum_incorrect_num_inputs(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""x"", TensorProto.FLOAT, (2, 3)),\n             (""y"", TensorProto.FLOAT, (2, 3)),\n             (""z"", TensorProto.FLOAT, (2, 3))],\n            [make_node(\'Einsum\', [\'x\', \'y\'], [\'z\'], equation=\'i,...j, k, l-> i\')],\n            [])\n        self.assertRaises(checker.ValidationError, self._inferred, graph)\n\n    def test_negative_log_likehood_shape_is_NCdd(self):  # type: () -> None\n        N, C = 3, 4\n        graph = self._make_graph(\n            [(\'input\', TensorProto.FLOAT, (N, C)),\n             (\'target\', TensorProto.INT64, (N,))],\n            [make_node(\'NegativeLogLikelihoodLoss\', [\'input\', \'target\'], [\'loss\'], reduction=\'none\')],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'loss\', TensorProto.FLOAT, (N, ))])  # type: ignore\n\n    def test_negative_log_likehood_shape_is_NC_with_weight(self):  # type: () -> None\n        N, C = 3, 4\n        graph = self._make_graph(\n            [(\'input\', TensorProto.FLOAT, (N, C)),\n             (\'target\', TensorProto.INT64, (N,)),\n             (\'weight\', TensorProto.FLOAT, (C,))],\n            [make_node(\'NegativeLogLikelihoodLoss\', [\'input\', \'target\', \'weight\'], [\'loss\'], reduction=\'none\')],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'loss\', TensorProto.FLOAT, (N, ))])  # type: ignore\n\n    def test_negative_log_likehood_shape_is_NC_reduction_mean(self):  # type: () -> None\n        N, C = 3, 4\n        graph = self._make_graph(\n            [(\'input\', TensorProto.FLOAT, (N, C)),\n             (\'target\', TensorProto.INT64, (N,))],\n            [make_node(\'NegativeLogLikelihoodLoss\', [\'input\', \'target\'], [\'loss\'], reduction=\'mean\')],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'loss\', TensorProto.FLOAT, ())])  # type: ignore\n\n    def test_negative_log_likehood_shape_is_NC_with_weight_reduction_mean(self):  # type: () -> None\n        N, C = 3, 4\n        graph = self._make_graph(\n            [(\'input\', TensorProto.FLOAT, (N, C)),\n             (\'target\', TensorProto.INT64, (N,)),\n             (\'weight\', TensorProto.FLOAT, (C,))],\n            [make_node(\'NegativeLogLikelihoodLoss\', [\'input\', \'target\', \'weight\'], [\'loss\'], reduction=\'mean\')],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'loss\', TensorProto.FLOAT, ())])  # type: ignore\n\n    def test_negative_log_likehood_shape_is_NCd1d2(self):  # type: () -> None\n        N, C, d1, d2 = 3, 4, 5, 6\n        graph = self._make_graph(\n            [(""input"", TensorProto.FLOAT, (N, C, d1, d2)),\n             (""target"", TensorProto.INT64, (N, d1, d2))],\n            [make_node(\'NegativeLogLikelihoodLoss\', [\'input\', \'target\'], [\'loss\'], reduction=\'none\')],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'loss\', TensorProto.FLOAT, (N, d1, d2))])  # type: ignore\n\n    def test_negative_log_likehood_shape_is_NCd1d2_with_weight(self):  # type: () -> None\n        N, C, d1, d2 = 3, 4, 5, 6\n        graph = self._make_graph(\n            [(""input"", TensorProto.FLOAT, (N, C, d1, d2)),\n             (""target"", TensorProto.INT64, (N, d1, d2)),\n             (""weight"", TensorProto.FLOAT, (C,))],\n            [make_node(\'NegativeLogLikelihoodLoss\', [\'input\', \'target\', \'weight\'], [\'loss\'], reduction=\'none\')],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'loss\', TensorProto.FLOAT, (N, d1, d2))])  # type: ignore\n\n    def test_negative_log_likehood_shape_is_NCd1d2_reduction_sum(self):  # type: () -> None\n        N, C, d1, d2 = 3, 4, 5, 6\n        graph = self._make_graph(\n            [(""input"", TensorProto.FLOAT, (N, C, d1, d2)),\n             (""target"", TensorProto.INT64, (N, d1, d2))],\n            [make_node(\'NegativeLogLikelihoodLoss\', [\'input\', \'target\'], [\'loss\'], reduction=\'sum\')],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'loss\', TensorProto.FLOAT, ())])  # type: ignore\n\n    def test_negative_log_likehood_shape_is_NCd1d2_with_weight_reduction_mean(self):  # type: () -> None\n        N, C, d1, d2 = 3, 4, 5, 6\n        graph = self._make_graph(\n            [(""input"", TensorProto.FLOAT, (N, C, d1, d2)),\n             (""target"", TensorProto.INT64, (N, d1, d2)),\n             (""weight"", TensorProto.FLOAT, (C,))],\n            [make_node(\'NegativeLogLikelihoodLoss\', [\'input\', \'target\', \'weight\'], [\'loss\'], reduction=\'mean\')],\n            [])\n        self._assert_inferred(graph, [make_tensor_value_info(\'loss\', TensorProto.FLOAT, ())])  # type: ignore\n\n    def test_negative_log_likehood_input_target_shape_mismatch(self):  # type: () -> None\n        N, C, d1, d2 = 3, 4, 5, 6\n        graph = self._make_graph(\n            [(""input"", TensorProto.FLOAT, (N, d1, d2)),\n             (""target"", TensorProto.INT64, (N, d1 + 1, d2)),\n             (""weight"", TensorProto.FLOAT, (C,)),\n             (""loss"", TensorProto.FLOAT, ())],\n            [make_node(\'NegativeLogLikelihoodLoss\', [\'input\', \'target\', \'weight\'], [\'loss\'], reduction=\'mean\')],\n            [])\n        self.assertRaises(checker.ValidationError, self._inferred, graph)\n\n    def test_negative_log_likehood_input_weight_shape_mismatch(self):  # type: () -> None\n        N, C, d1, d2 = 3, 4, 5, 6\n        graph = self._make_graph(\n            [(""input"", TensorProto.FLOAT, (N, C, d1, d2)),\n             (""target"", TensorProto.INT64, (N, d1, d2)),\n             (""weight"", TensorProto.FLOAT, (C + 1,)),\n             (""loss"", TensorProto.FLOAT, (N, d1, d2))],\n            [make_node(\'NegativeLogLikelihoodLoss\', [\'input\', \'target\', \'weight\'], [\'loss\'], reduction=\'none\')],\n            [])\n        self.assertRaises(checker.ValidationError, self._inferred, graph)\n\n    def test_softmax_cross_entropy_none(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""x"", TensorProto.FLOAT, (2, 3)),\n             (""y"", TensorProto.FLOAT, (2,))],\n            [make_node(\'SoftmaxCrossEntropyLoss\', [\'x\', \'y\'], [\'z\'], reduction=\'none\')],\n            [],)\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, (2,))])  # type: ignore\n\n    def test_softmax_cross_entropy_mean(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""x"", TensorProto.FLOAT, (2, 3)),\n             (""y"", TensorProto.FLOAT, (2,))],\n            [make_node(\'SoftmaxCrossEntropyLoss\', [\'x\', \'y\'], [\'z\'], reduction=\'mean\')],\n            [],)\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, ())])  # type: ignore\n\n    def test_softmax_cross_entropy_none_NCD1D2(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""x"", TensorProto.FLOAT, (2, 3, 5, 8)),\n             (""y"", TensorProto.FLOAT, (2, 5, 8))],\n            [make_node(\'SoftmaxCrossEntropyLoss\', [\'x\', \'y\'], [\'z\'], reduction=\'none\')],\n            [],)\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, (2, 5, 8))])  # type: ignore\n\n    def test_softmax_cross_entropy_mean_NCD1D2(self):  # type: () -> None\n        graph = self._make_graph(\n            [(""x"", TensorProto.FLOAT, (2, 3, 4, 5)),\n             (""y"", TensorProto.FLOAT, (2, 4, 5))],\n            [make_node(\'SoftmaxCrossEntropyLoss\', [\'x\', \'y\'], [\'z\'], reduction=\'mean\')],\n            [],)\n        self._assert_inferred(graph, [make_tensor_value_info(\'z\', TensorProto.FLOAT, ())])  # type: ignore\n\n    def test_celu_function_output_shape(self):  # type: () -> None\n        graph = self._make_graph(\n            [(\'X\', TensorProto.FLOAT, (25, 48, 16, 16))],\n            [make_node(\'Celu\', [\'X\'], [\'Y\'], alpha=2.0)],\n            []\n        )\n        self._assert_inferred(graph, [make_tensor_value_info(\'Y\', TensorProto.FLOAT, (25, 48, 16, 16))])\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
onnx/test/test_backend_test.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport itertools\nimport os\nimport platform\nimport unittest\nimport onnx.backend.base\nimport onnx.backend.test\n\nfrom onnx.backend.base import Device, DeviceType\nfrom onnx.backend.test.runner import BackendIsNotSupposedToImplementIt\nimport onnx.shape_inference\nimport onnx.version_converter\nfrom typing import Optional, Text, Any, Tuple, Sequence\nfrom onnx import NodeProto, ModelProto, TensorProto\nimport numpy  # type: ignore\n\n# The following just executes the fake backend through the backend test\n# infrastructure. Since we don\'t have full reference implementation of all ops\n# in ONNX repo, it\'s impossible to produce the proper results. However, we can\n# run \'checker\' (that\'s what base Backend class does) to verify that all tests\n# fed are actually well-formed ONNX models.\n#\n# If everything is fine, all the tests would be marked as ""skipped"".\n#\n# We don\'t enable report in this test because the report collection logic itself\n# fails when models are mal-formed.\n\n\nclass DummyBackend(onnx.backend.base.Backend):\n    @classmethod\n    def prepare(cls,\n                model,  # type: ModelProto\n                device=\'CPU\',  # type: Text\n                **kwargs  # type: Any\n                ):  # type: (...) -> Optional[onnx.backend.base.BackendRep]\n        super(DummyBackend, cls).prepare(model, device, **kwargs)\n\n        # test shape inference\n        model = onnx.shape_inference.infer_shapes(model)\n        value_infos = {vi.name: vi for vi in itertools.chain(model.graph.value_info, model.graph.output)}\n\n        if do_enforce_test_coverage_whitelist(model):\n            for node in model.graph.node:\n                for i, output in enumerate(node.output):\n                    if node.op_type == \'Dropout\' and i != 0:\n                        continue\n                    assert output in value_infos\n                    tt = value_infos[output].type.tensor_type\n                    assert tt.elem_type != TensorProto.UNDEFINED\n                    for dim in tt.shape.dim:\n                        assert dim.WhichOneof(\'value\') == \'dim_value\'\n\n        raise BackendIsNotSupposedToImplementIt(\n            ""This is the dummy backend test that doesn\'t verify the results but does run the checker"")\n\n    @classmethod\n    def run_node(cls,\n                 node,  # type: NodeProto\n                 inputs,  # type: Any\n                 device=\'CPU\',  # type: Text\n                 outputs_info=None,  # type: Optional[Sequence[Tuple[numpy.dtype, Tuple[int, ...]]]]\n                 **kwargs  # type: Any\n                 ):  # type: (...) -> Optional[Tuple[Any, ...]]\n        super(DummyBackend, cls).run_node(node, inputs, device=device, outputs_info=outputs_info)\n        raise BackendIsNotSupposedToImplementIt(\n            ""This is the dummy backend test that doesn\'t verify the results but does run the checker"")\n\n    @classmethod\n    def supports_device(cls, device):  # type: (Text) -> bool\n        d = Device(device)\n        if d.type == DeviceType.CPU:\n            return True\n        return False\n\n\ntest_coverage_whitelist = set(\n    [\'bvlc_alexnet\', \'densenet121\', \'inception_v1\', \'inception_v2\',\n     \'resnet50\', \'shufflenet\', \'SingleRelu\', \'squeezenet_old\', \'vgg19\', \'zfnet\'])\n\n\ndef do_enforce_test_coverage_whitelist(model):  # type: (ModelProto) -> bool\n    if model.graph.name not in test_coverage_whitelist:\n        return False\n    for node in model.graph.node:\n        if node.op_type in set([\'RNN\', \'LSTM\', \'GRU\']):\n            return False\n    return True\n\n\nbackend_test = onnx.backend.test.BackendTest(DummyBackend, __name__)\nif os.getenv(\'APPVEYOR\'):\n    backend_test.exclude(r\'(test_vgg19|test_zfnet)\')\nif platform.architecture()[0] == \'32bit\':\n    backend_test.exclude(r\'(test_vgg19|test_zfnet|test_bvlc_alexnet)\')\n\n# import all test cases at global scope to make them visible to python.unittest\nglobals().update(backend_test\n                 .test_cases)\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
onnx/test/test_external_data.py,0,"b'import tempfile\nimport unittest\nimport uuid\n\nimport numpy as np  # type: ignore\nimport shutil\n\nimport os\nimport os.path as Path\n\nimport onnx\nfrom onnx import checker, helper\nfrom onnx import ModelProto, TensorProto\nfrom onnx.external_data_helper import set_external_data\nfrom onnx.external_data_helper import convert_model_to_external_data\nfrom onnx.external_data_helper import convert_model_from_external_data\nfrom onnx.external_data_helper import load_external_data_for_model\nfrom onnx.numpy_helper import to_array, from_array\nfrom typing import Any, Tuple, Text, List\n\n\nclass TestLoadExternalData(unittest.TestCase):\n\n    def setUp(self):  # type: () -> None\n        self.temp_dir = tempfile.mkdtemp()  # type: Text\n        self.initializer_value = np.arange(6).reshape(3, 2).astype(np.float32) + 512\n        self.attribute_value = np.arange(6).reshape(2, 3).astype(np.float32) + 256\n        self.model_filename = self.create_test_model()\n\n    def tearDown(self):  # type: () -> None\n        shutil.rmtree(self.temp_dir)\n\n    def get_temp_model_filename(self):  # type: () -> Text\n        return os.path.join(self.temp_dir, str(uuid.uuid4()) + \'.onnx\')\n\n    def create_external_data_tensor(self, value, tensor_name):  # type: (List[Any], Text) -> TensorProto\n        tensor = from_array(np.array(value))\n        tensor.name = tensor_name\n        tensor_filename = ""{}.bin"".format(tensor_name)\n        set_external_data(tensor, location=tensor_filename)\n\n        with open(os.path.join(self.temp_dir, tensor_filename), \'wb\') as data_file:\n            data_file.write(tensor.raw_data)\n        tensor.ClearField(\'raw_data\')\n        tensor.data_location = onnx.TensorProto.EXTERNAL\n        return tensor\n\n    def create_test_model(self):  # type: () -> Text\n\n        constant_node = onnx.helper.make_node(\n            \'Constant\',\n            inputs=[],\n            outputs=[\'values\'],\n            value=self.create_external_data_tensor(self.attribute_value, ""attribute_value"")\n        )\n\n        initializers = [self.create_external_data_tensor(self.initializer_value, ""input_value"")]\n        inputs = [helper.make_tensor_value_info(""input_value"",\n                                                onnx.TensorProto.FLOAT,\n                                                self.initializer_value.shape)]\n\n        graph = helper.make_graph([constant_node], ""test_graph"",\n                                  inputs=inputs, outputs=[],\n                                  initializer=initializers)\n        model = helper.make_model(graph)\n\n        model_filename = os.path.join(self.temp_dir, ""model.onnx"")\n        with open(model_filename, ""wb"") as model_file:\n            model_file.write(model.SerializeToString())\n\n        return model_filename\n\n    def test_check_model(self):  # type: () -> None\n        checker.check_model(self.model_filename)\n\n    def test_load_external_data(self):  # type: () -> None\n        model = onnx.load_model(self.model_filename)\n        initializer_tensor = model.graph.initializer[0]\n        self.assertTrue(np.allclose(to_array(initializer_tensor), self.initializer_value))\n\n        attribute_tensor = model.graph.node[0].attribute[0].t\n        self.assertTrue(np.allclose(to_array(attribute_tensor), self.attribute_value))\n\n    def test_load_external_data_for_model(self):  # type: () -> None\n        model = onnx.load_model(self.model_filename, load_external_data=False)\n        load_external_data_for_model(model, self.temp_dir)\n        initializer_tensor = model.graph.initializer[0]\n        self.assertTrue(np.allclose(to_array(initializer_tensor), self.initializer_value))\n\n        attribute_tensor = model.graph.node[0].attribute[0].t\n        self.assertTrue(np.allclose(to_array(attribute_tensor), self.attribute_value))\n\n    def test_save_external_data(self):  # type: () -> None\n        model = onnx.load_model(self.model_filename)\n\n        temp_dir = os.path.join(self.temp_dir, ""save_copy"")\n        os.mkdir(temp_dir)\n        new_model_filename = os.path.join(temp_dir, \'model.onnx\')\n        onnx.save_model(model, new_model_filename)\n\n        new_model = onnx.load_model(new_model_filename)\n        initializer_tensor = new_model.graph.initializer[0]\n        self.assertTrue(np.allclose(to_array(initializer_tensor), self.initializer_value))\n\n        attribute_tensor = new_model.graph.node[0].attribute[0].t\n        self.assertTrue(np.allclose(to_array(attribute_tensor), self.attribute_value))\n\n\nclass TestLoadExternalDataSingleFile(unittest.TestCase):\n\n    def setUp(self):  # type: () -> None\n        self.temp_dir = tempfile.mkdtemp()  # type: Text\n        self.initializer_value = np.arange(6).reshape(3, 2).astype(np.float32) + 512\n        self.attribute_value = np.arange(6).reshape(2, 3).astype(np.float32) + 256\n        self.model_filename = self.create_test_model()\n\n    def tearDown(self):  # type: () -> None\n        shutil.rmtree(self.temp_dir)\n\n    def get_temp_model_filename(self):  # type: () -> Text\n        return os.path.join(self.temp_dir, str(uuid.uuid4()) + \'.onnx\')\n\n    def create_external_data_tensors(self, tensors_data):  # type: (List[Tuple[List[Any],Any]]) -> List[TensorProto]\n        tensor_filename = ""tensors.bin""\n        tensors = []\n\n        with open(os.path.join(self.temp_dir, tensor_filename), \'ab\') as data_file:\n            for (value, tensor_name) in tensors_data:\n                tensor = from_array(np.array(value))\n                offset = data_file.tell()\n                if offset % 4096 != 0:\n                    data_file.write(b""\\0"" * (4096 - offset % 4096))\n                    offset = offset + 4096 - offset % 4096\n\n                data_file.write(tensor.raw_data)\n                set_external_data(tensor, location=tensor_filename, offset=offset, length=data_file.tell() - offset)\n                tensor.name = tensor_name\n                tensor.ClearField(""raw_data"")\n                tensor.data_location = onnx.TensorProto.EXTERNAL\n                tensors.append(tensor)\n\n        return tensors\n\n    def create_test_model(self):  # type: () -> Text\n        tensors = self.create_external_data_tensors([\n            (self.attribute_value, ""attribute_value""),\n            (self.initializer_value, ""input_value""),\n        ])\n\n        constant_node = onnx.helper.make_node(\n            \'Constant\',\n            inputs=[],\n            outputs=[\'values\'],\n            value=tensors[0]\n        )\n\n        inputs = [helper.make_tensor_value_info(""input_value"",\n                                                onnx.TensorProto.FLOAT,\n                                                self.initializer_value.shape)]\n\n        graph = helper.make_graph([constant_node], ""test_graph"",\n                                  inputs=inputs, outputs=[],\n                                  initializer=[tensors[1]])\n        model = helper.make_model(graph)\n\n        model_filename = os.path.join(self.temp_dir, \'model.onnx\')\n        with open(model_filename, ""wb"") as model_file:\n            model_file.write(model.SerializeToString())\n        return model_filename\n\n    def test_check_model(self):  # type: () -> None\n        checker.check_model(self.model_filename)\n\n    def test_load_external_single_file_data(self):  # type: () -> None\n        model = onnx.load_model(self.model_filename)\n\n        initializer_tensor = model.graph.initializer[0]\n        self.assertTrue(np.allclose(to_array(initializer_tensor), self.initializer_value))\n\n        attribute_tensor = model.graph.node[0].attribute[0].t\n        self.assertTrue(np.allclose(to_array(attribute_tensor), self.attribute_value))\n\n    def test_save_external_single_file_data(self):  # type: () -> None\n        model = onnx.load_model(self.model_filename)\n\n        temp_dir = os.path.join(self.temp_dir, ""save_copy"")\n        os.mkdir(temp_dir)\n        new_model_filename = os.path.join(temp_dir, \'model.onnx\')\n        onnx.save_model(model, new_model_filename)\n\n        new_model = onnx.load_model(new_model_filename)\n        initializer_tensor = new_model.graph.initializer[0]\n        self.assertTrue(np.allclose(to_array(initializer_tensor), self.initializer_value))\n\n        attribute_tensor = new_model.graph.node[0].attribute[0].t\n        self.assertTrue(np.allclose(to_array(attribute_tensor), self.attribute_value))\n\n\nclass TestSaveAllTensorsAsExternalData(unittest.TestCase):\n\n    def setUp(self):  # type: () -> None\n        self.temp_dir = tempfile.mkdtemp()  # type: Text\n        self.initializer_value = np.arange(6).reshape(3, 2).astype(np.float32) + 512\n        self.attribute_value = np.arange(6).reshape(2, 3).astype(np.float32) + 256\n        self.model = self.create_test_model()\n\n    def tearDown(self):  # type: () -> None\n        shutil.rmtree(self.temp_dir)\n\n    def get_temp_model_filename(self):  # type: () -> Text\n        return os.path.join(self.temp_dir, str(uuid.uuid4()) + \'.onnx\')\n\n    def create_data_tensors(self, tensors_data):  # type: (List[Tuple[List[Any],Any]]) -> List[TensorProto]\n        tensors = []\n        for (value, tensor_name) in tensors_data:\n            tensor = from_array(np.array(value))\n            tensor.name = tensor_name\n            tensors.append(tensor)\n\n        return tensors\n\n    def create_test_model(self):  # type: () -> ModelProto\n        tensors = self.create_data_tensors([\n            (self.attribute_value, ""attribute_value""),\n            (self.initializer_value, ""input_value""),\n        ])\n\n        constant_node = onnx.helper.make_node(\n            \'Constant\',\n            inputs=[],\n            outputs=[\'values\'],\n            value=tensors[0]\n        )\n\n        inputs = [helper.make_tensor_value_info(""input_value"",\n                                                onnx.TensorProto.FLOAT,\n                                                self.initializer_value.shape)]\n\n        graph = helper.make_graph([constant_node], ""test_graph"",\n                                  inputs=inputs, outputs=[],\n                                  initializer=[tensors[1]])\n        return helper.make_model(graph)\n\n    def test_check_model(self):  # type: () -> None\n        checker.check_model(self.model)\n\n    def test_convert_model_to_from_one_file(self):  # type: () -> None\n        model_file_path = self.get_temp_model_filename()\n        external_data_file = str(uuid.uuid4())\n        convert_model_to_external_data(self.model, location=external_data_file)\n        onnx.save_model(self.model, model_file_path)\n        self.assertTrue(Path.isfile(model_file_path))\n        self.assertTrue(Path.isfile(os.path.join(self.temp_dir, external_data_file)))\n        model = onnx.load_model(model_file_path)\n        initializer_tensor = model.graph.initializer[0]\n        self.assertTrue(np.allclose(to_array(initializer_tensor), self.initializer_value))\n\n        attribute_tensor = model.graph.node[0].attribute[0].t\n        self.assertTrue(np.allclose(to_array(attribute_tensor), self.attribute_value))\n\n        # test convert model from external data\n        convert_model_from_external_data(model)\n        model_file_path = self.get_temp_model_filename()\n        onnx.save_model(model, model_file_path)\n        model = onnx.load_model(model_file_path)\n        initializer_tensor = model.graph.initializer[0]\n        self.assertFalse(len(initializer_tensor.external_data))\n        self.assertEqual(initializer_tensor.data_location, TensorProto.DEFAULT)\n        self.assertTrue(np.allclose(to_array(initializer_tensor), self.initializer_value))\n\n        attribute_tensor = model.graph.node[0].attribute[0].t\n        self.assertFalse(len(initializer_tensor.external_data))\n        self.assertEqual(attribute_tensor.data_location, TensorProto.DEFAULT)\n        self.assertTrue(np.allclose(to_array(attribute_tensor), self.attribute_value))\n\n    def test_convert_model_to_external_data_one_file_per_tensor(self):  # type: () -> None\n        model_file_path = self.get_temp_model_filename()\n        convert_model_to_external_data(self.model, all_tensors_to_one_file=False)\n        onnx.save_model(self.model, model_file_path)\n        self.assertTrue(Path.isfile(model_file_path))\n        self.assertTrue(Path.isfile(os.path.join(self.temp_dir, ""input_value"")))\n        self.assertTrue(Path.isfile(os.path.join(self.temp_dir, ""attribute_value"")))\n        model = onnx.load_model(model_file_path)\n        initializer_tensor = model.graph.initializer[0]\n        self.assertTrue(np.allclose(to_array(initializer_tensor), self.initializer_value))\n\n        attribute_tensor = model.graph.node[0].attribute[0].t\n        self.assertTrue(np.allclose(to_array(attribute_tensor), self.attribute_value))\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
onnx/test/tools_test.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport unittest\nimport onnx\nfrom onnx.tools import update_model_dims\nfrom onnx import helper, TensorProto\n\n\nclass TestToolsFunctions(unittest.TestCase):\n    def test_update_inputs_outputs_dim(self):  # type: () -> None\n        node_def = helper.make_node(\n            ""Conv"",\n            inputs=[\'x\', \'W\'],\n            outputs=[\'y\'],\n            kernel_shape=[3, 3],\n            strides=[2, 2],\n        )\n        graph_def = helper.make_graph(\n            [node_def],\n            \'test\',\n            [helper.make_tensor_value_info(\'x\', TensorProto.FLOAT, [1, 1, 5, 5]),\n             helper.make_tensor_value_info(\'W\', TensorProto.FLOAT, [1, 1, 3, 3])],\n            [helper.make_tensor_value_info(\'y\', TensorProto.FLOAT, [1, 1, 2, 2])]\n        )\n        model_def = helper.make_model(graph_def, producer_name=\'test\')\n        updated_def = update_model_dims.update_inputs_outputs_dims(\n            model_def,\n            {\n                ""x"": [1, 1, \'x1\', -1],\n                ""W"": [1, 1, 3, 3],\n            },\n            {\n                ""y"": [1, 1, -1, -1],\n            })\n        onnx.checker.check_model(updated_def)\n        self.assertEqual(updated_def.graph.input[0].type.tensor_type.shape.dim[2].dim_param, \'x1\')\n        self.assertEqual(updated_def.graph.input[0].type.tensor_type.shape.dim[3].dim_param, \'x_3\')\n        self.assertEqual(updated_def.graph.output[0].type.tensor_type.shape.dim[2].dim_param, \'y_2\')\n        self.assertEqual(updated_def.graph.output[0].type.tensor_type.shape.dim[3].dim_param, \'y_3\')\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
onnx/test/utils_test.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport unittest\nimport onnx.utils\nfrom onnx import helper, TensorProto\n\n\nclass TestUtilityFunctions(unittest.TestCase):\n    def test_polish_model(self):  # type: () -> None\n        node_def = helper.make_node(\n            ""Relu"", [""X""], [""Y""], doc_string=""ABC"")\n        graph_def = helper.make_graph(\n            [node_def],\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, [1, 2])],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, [1, 2])])\n        model_def = helper.make_model(graph_def, producer_name=\'test\')\n        polished_def = onnx.utils.polish_model(model_def)\n        self.assertEqual(polished_def.producer_name, \'test\')\n        self.assertEqual(len(polished_def.graph.node), 1)\n        self.assertFalse(polished_def.graph.node[0].HasField(\'doc_string\'))\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
onnx/test/version_converter_test.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom onnx import checker, helper, ModelProto, TensorProto, GraphProto, NodeProto, OperatorSetIdProto\nfrom typing import Sequence, Text, Tuple, List, Callable\nfrom onnx import numpy_helper\n\nimport numpy as np  # type: ignore\nimport struct\n\nimport onnx.version_converter\nimport unittest\n\n\nclass TestVersionConverter(unittest.TestCase):\n\n    def _converted(\n            self,\n            graph,  # type: GraphProto\n            initial_version,  # type: OperatorSetIdProto\n            target_version  # type: int\n    ):  # type: (...) -> ModelProto\n        orig_model = helper.make_model(graph, producer_name=\'onnx-test\', opset_imports=[initial_version])\n        # print(type(orig_model))\n        converted_model = onnx.version_converter.convert_version(orig_model,\n                target_version)\n        checker.check_model(converted_model)\n        return converted_model\n\n    # Test 1: Backwards Incompatible Conversion: Reshape: 8 -> 2\n    def test_backwards_incompatible(self):  # type: () -> None\n        def test():  # type: () -> None\n            nodes = [helper.make_node(\'Add\', [""W"", ""Z""], [""shape""]),\n                        helper.make_node(\'Reshape\', [""X"", ""shape""], [""A""]),\n                        helper.make_node(\'Add\', [""A"", ""W""], [""Y""])]\n            graph = helper.make_graph(\n                nodes,\n                ""test"",\n                [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (5,)),\n                    helper.make_tensor_value_info(""W"", TensorProto.FLOAT, (1,)),\n                    helper.make_tensor_value_info(""Z"", TensorProto.FLOAT, (1,))],\n                [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (5,))])\n            self._converted(graph, helper.make_operatorsetid("""", 8), 2)\n        self.assertRaises(RuntimeError, test)\n\n    # Test 2: Backwards Compatible Conversion (No Adaptations): Add: 3 -> 2\n    def test_backwards_compatible(self):  # type: () -> None\n        nodes = [helper.make_node(\'Add\', [""X1"", ""X2""], [""Y""])]\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""X1"", TensorProto.FLOAT, (5,)),\n                helper.make_tensor_value_info(""X2"", TensorProto.FLOAT, (5,))],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (5,))])\n        converted_model = self._converted(graph, helper.make_operatorsetid(\n            """", 3), 2)\n        # Assert equality of graph and converted_model\n        assert converted_model.graph.node[0].op_type == ""Add""\n        assert converted_model.opset_import[0].version == 2\n\n    # Test 3: Non-Existent Op Conversion: Cos: 8 -> 6\n    def test_non_existent_op(self):  # type: () -> None\n        def test():  # type: () -> None\n            nodes = [helper.make_node(\'Cos\', [""X""], [""Y""])]\n            graph = helper.make_graph(\n                nodes,\n                ""test"",\n                [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (5,))],\n                [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (5,))])\n            self._converted(graph, helper.make_operatorsetid("""", 8), 6)\n        self.assertRaises(RuntimeError, test)\n\n    # Test Add Adapter: 8 -> 5\n    def test_add_8_5(self):  # type: () -> None\n        nodes = [helper.make_node(\'Add\', [""X1"", ""X2""], [""Y""])]\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""X1"", TensorProto.FLOAT, (5,)),\n                helper.make_tensor_value_info(""X2"", TensorProto.FLOAT, (1,))],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (5,))])\n        converted_model = self._converted(graph, helper.make_operatorsetid(\n            """", 8), 5)\n        # Assert equality of graph and converted_model\n        assert converted_model.graph.node[0].op_type == ""Add""\n        assert converted_model.opset_import[0].version == 5\n\n    # Test Add Adapter: 5 -> 8\n    def test_add_5_8(self):  # type: () -> None\n        nodes = [helper.make_node(\'Add\', [""X1"", ""X2""], [""Y""])]\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""X1"", TensorProto.FLOAT, (5,)),\n                helper.make_tensor_value_info(""X2"", TensorProto.FLOAT, (1,))],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (5,))])\n        converted_model = self._converted(graph, helper.make_operatorsetid(\n            """", 5), 8)\n        # Assert equality of graph and converted_model\n        assert converted_model.graph.node[0].op_type == ""Add""\n        assert converted_model.opset_import[0].version == 8\n\n    # Test Add Adapter: 5 -> 8, requiring insertion of an Unsqueeze node\n    def test_add_5_8_with_unsqueeze(self):  # type: () -> None\n        nodes = [helper.make_node(\'Add\', [""X1"", ""X2""], [""Y""], axis=0, broadcast=1)]\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""X1"", TensorProto.FLOAT, (5, 2)),\n                helper.make_tensor_value_info(""X2"", TensorProto.FLOAT, (5,))],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (5,))])\n        converted_model = self._converted(graph, helper.make_operatorsetid(\n            """", 5), 8)\n        # Assert equality of graph and converted_model\n        assert converted_model.graph.node[0].op_type == ""Unsqueeze""\n        assert converted_model.graph.node[1].op_type == ""Add""\n        assert converted_model.opset_import[0].version == 8\n\n    # Test Mul Adapter: 8 -> 5\n    def test_mul_8_5(self):  # type: () -> None\n        nodes = [helper.make_node(\'Mul\', [""X1"", ""X2""], [""Y""])]\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""X1"", TensorProto.FLOAT, (5,)),\n                helper.make_tensor_value_info(""X2"", TensorProto.FLOAT, (1,))],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (5,))])\n        converted_model = self._converted(graph, helper.make_operatorsetid(\n            """", 8), 5)\n        # Assert equality of graph and converted_model\n        assert converted_model.graph.node[0].op_type == ""Mul""\n        assert converted_model.opset_import[0].version == 5\n\n    # Test Mul Adapter: 5 -> 8\n    def test_mul_5_8(self):  # type: () -> None\n        nodes = [helper.make_node(\'Mul\', [""X1"", ""X2""], [""Y""])]\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""X1"", TensorProto.FLOAT, (5,)),\n                helper.make_tensor_value_info(""X2"", TensorProto.FLOAT, (1,))],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (5,))])\n        converted_model = self._converted(graph, helper.make_operatorsetid(\n            """", 5), 8)\n        # Assert equality of graph and converted_model\n        assert converted_model.graph.node[0].op_type == ""Mul""\n        assert converted_model.opset_import[0].version == 8\n\n    # Test Gemm Adapter: 1 -> 8\n    def test_gemm_up(self):  # type: () -> None\n        nodes = [helper.make_node(\'Gemm\', [""A"", ""B"", ""C""], [""Y""])]\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""A"", TensorProto.FLOAT, (5, 5,)),\n                helper.make_tensor_value_info(""B"", TensorProto.FLOAT, (5, 5,)),\n                helper.make_tensor_value_info(""C"", TensorProto.FLOAT, (5, 5,))],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (5, 5,))])\n        converted_model = self._converted(graph, helper.make_operatorsetid(\n            """", 1), 8)\n        # Assert equality of graph and converted_model\n        assert converted_model.graph.node[0].op_type == ""Gemm""\n        assert converted_model.opset_import[0].version == 8\n\n    # Test Gemm Adapter: 8 -> 1\n    def test_gemm_down(self):  # type: () -> None\n        nodes = [helper.make_node(\'Gemm\', [""A"", ""B"", ""C""], [""Y""])]\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""A"", TensorProto.FLOAT, (5, 5,)),\n                helper.make_tensor_value_info(""B"", TensorProto.FLOAT, (5, 5,)),\n                helper.make_tensor_value_info(""C"", TensorProto.FLOAT, (5, 5,))],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (5, 5,))])\n        converted_model = self._converted(graph, helper.make_operatorsetid(\n            """", 8), 1)\n        # Assert equality of graph and converted_model\n        assert converted_model.graph.node[0].op_type == ""Gemm""\n        assert converted_model.opset_import[0].version == 1\n\n    # Test Relu Adapter: 5 -> 7\n    def test_relu_5_7(self):  # type: () -> None\n        nodes = [helper.make_node(\'Relu\', [""X""], [""Y""])]\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (5,))],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (5,))])\n        converted_model = self._converted(graph, helper.make_operatorsetid(\n            """", 5), 7)\n        # Assert equality of graph and converted_model\n        assert converted_model.graph.node[0].op_type == ""Relu""\n        assert converted_model.opset_import[0].version == 7\n\n    # Test Relu Adapter: 7 -> 5\n    def test_relu_7_5(self):  # type: () -> None\n        nodes = [helper.make_node(\'Relu\', [""X""], [""Y""])]\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (5,))],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (5,))])\n        converted_model = self._converted(graph, helper.make_operatorsetid(\n            """", 7), 5)\n        # Assert equality of graph and converted_model\n        assert converted_model.graph.node[0].op_type == ""Relu""\n        assert converted_model.opset_import[0].version == 5\n\n    # Test BatchNormalization Adapter: 8 -> 5\n    def test_batch_normalization_8_5(self):  # type: () -> None\n        nodes = [helper.make_node(\'BatchNormalization\', [""X"", ""scale"", ""B"",\n            ""mean"", ""var""], [""Y""])]\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (5,)),\n                helper.make_tensor_value_info(""scale"", TensorProto.FLOAT, (1,)),\n                helper.make_tensor_value_info(""B"", TensorProto.FLOAT, (1,)),\n                helper.make_tensor_value_info(""mean"", TensorProto.FLOAT, (1,)),\n                helper.make_tensor_value_info(""var"", TensorProto.FLOAT, (1,))],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (5,))])\n        converted_model = self._converted(graph, helper.make_operatorsetid(\n            """", 8), 5)\n        # Assert equality of graph and converted_model\n        assert converted_model.graph.node[0].op_type == ""BatchNormalization""\n        assert converted_model.opset_import[0].version == 5\n\n    # Test BatchNormalization Adapter: 5 -> 8\n    def test_batch_normalization_5_8(self):  # type: () -> None\n        nodes = [helper.make_node(\'BatchNormalization\', [""X"", ""scale"", ""B"",\n            ""mean"", ""var""], [""Y""])]\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (5,)),\n                helper.make_tensor_value_info(""scale"", TensorProto.FLOAT, (1,)),\n                helper.make_tensor_value_info(""B"", TensorProto.FLOAT, (1,)),\n                helper.make_tensor_value_info(""mean"", TensorProto.FLOAT, (1,)),\n                helper.make_tensor_value_info(""var"", TensorProto.FLOAT, (1,))],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (5,))])\n        converted_model = self._converted(graph, helper.make_operatorsetid(\n            """", 5), 8)\n        # Assert equality of graph and converted_model\n        assert converted_model.graph.node[0].op_type == ""BatchNormalization""\n        assert converted_model.opset_import[0].version == 8\n\n    # Test Concat Adapter: 3 -> 5\n    def test_concat_3_5(self):  # type: () -> None\n        nodes = [helper.make_node(\'Concat\', [""X1"", ""X2"", ""X3"",\n            ""X4"", ""X5""], [""Y""])]\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""X1"", TensorProto.FLOAT, (5,)),\n                helper.make_tensor_value_info(""X2"", TensorProto.FLOAT, (1,)),\n                helper.make_tensor_value_info(""X3"", TensorProto.FLOAT, (1,)),\n                helper.make_tensor_value_info(""X4"", TensorProto.FLOAT, (1,)),\n                helper.make_tensor_value_info(""X5"", TensorProto.FLOAT, (1,))],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (5,))])\n        converted_model = self._converted(graph, helper.make_operatorsetid(\n            """", 3), 5)\n        # Assert equality of graph and converted_model\n        assert converted_model.graph.node[0].op_type == ""Concat""\n        assert converted_model.opset_import[0].version == 5\n\n    # Test Concat Adapter: 5 -> 3\n    def test_concat_5_3(self):  # type: () -> None\n        nodes = [helper.make_node(\'Concat\', [""X1"", ""X2"", ""X3"",\n            ""X4"", ""X5""], [""Y""])]\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""X1"", TensorProto.FLOAT, (5,)),\n                helper.make_tensor_value_info(""X2"", TensorProto.FLOAT, (1,)),\n                helper.make_tensor_value_info(""X3"", TensorProto.FLOAT, (1,)),\n                helper.make_tensor_value_info(""X4"", TensorProto.FLOAT, (1,)),\n                helper.make_tensor_value_info(""X5"", TensorProto.FLOAT, (1,))],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (5,))])\n        converted_model = self._converted(graph, helper.make_operatorsetid(\n            """", 5), 3)\n        # Assert equality of graph and converted_model\n        assert converted_model.graph.node[0].op_type == ""Concat""\n        assert converted_model.opset_import[0].version == 3\n\n    # Test Reshape Adapter: 6 -> 4\n    def test_reshape_6_4(self):  # type: () -> None\n        nodes = [helper.make_node(\'Constant\', [], [""shape""],\n                    value=helper.make_tensor("""", TensorProto.INT64, [1],\n                        [5])),\n                    helper.make_node(\'Reshape\', [""X"", ""shape""], [""Y""])]\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (5,))],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (5,))])\n        converted_model = self._converted(graph, helper.make_operatorsetid(\n            """", 6), 4)\n        # Assert equality of graph and converted_model\n        assert converted_model.graph.node[0].op_type == ""Reshape""\n        assert converted_model.opset_import[0].version == 4\n\n    # Test Reshape Adapter: 4 -> 6\n    def test_reshape_4_6(self):  # type: () -> None\n        nodes = [helper.make_node(\'Reshape\', [""X""], [""Y""], shape=[5])]\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (5,))],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (5,))])\n        converted_model = self._converted(graph, helper.make_operatorsetid(\n            """", 4), 6)\n        # Assert equality of graph and converted_model\n        assert converted_model.graph.node[0].op_type == ""Reshape""\n        assert converted_model.opset_import[0].version == 6\n\n    # Test Sum Adapter: 7 -> 8\n    def test_sum_7_8(self):  # type: () -> None\n        nodes = [helper.make_node(\'Sum\', [""data_0"", ""data_1"", ""data_2"",\n            ""data_3"", ""data_4""], [""sum""])]\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""data_0"", TensorProto.FLOAT, (5,)),\n                helper.make_tensor_value_info(""data_1"", TensorProto.FLOAT, (5,)),\n                helper.make_tensor_value_info(""data_2"", TensorProto.FLOAT, (5,)),\n                helper.make_tensor_value_info(""data_3"", TensorProto.FLOAT, (5,)),\n                helper.make_tensor_value_info(""data_4"", TensorProto.FLOAT, (5,))],\n            [helper.make_tensor_value_info(""sum"", TensorProto.FLOAT, (5,))])\n        converted_model = self._converted(graph, helper.make_operatorsetid(\n            """", 7), 8)\n        # Assert equality of graph and converted_model\n        assert converted_model.graph.node[0].op_type == ""Sum""\n        assert converted_model.opset_import[0].version == 8\n\n    # Test Sum Adapter: 5 -> 8\n    def test_sum_5_8(self):  # type: () -> None\n        nodes = [helper.make_node(\'Sum\', [""data_0"", ""data_1"", ""data_2"",\n            ""data_3"", ""data_4""], [""sum""])]\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""data_0"", TensorProto.FLOAT, (5,)),\n                helper.make_tensor_value_info(""data_1"", TensorProto.FLOAT, (5,)),\n                helper.make_tensor_value_info(""data_2"", TensorProto.FLOAT, (5,)),\n                helper.make_tensor_value_info(""data_3"", TensorProto.FLOAT, (5,)),\n                helper.make_tensor_value_info(""data_4"", TensorProto.FLOAT, (5,))],\n            [helper.make_tensor_value_info(""sum"", TensorProto.FLOAT, (5,))])\n        converted_model = self._converted(graph, helper.make_operatorsetid(\n            """", 5), 7)\n        # Assert equality of graph and converted_model\n        assert converted_model.graph.node[0].op_type == ""Sum""\n        assert converted_model.opset_import[0].version == 7\n\n    # Test Sum Adapter: 8 -> 5\n    def test_sum_8_5(self):  # type: () -> None\n        nodes = [helper.make_node(\'Sum\', [""data_0"", ""data_1"", ""data_2"",\n            ""data_3"", ""data_4""], [""sum""])]\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""data_0"", TensorProto.FLOAT, (5,)),\n                helper.make_tensor_value_info(""data_1"", TensorProto.FLOAT, (5,)),\n                helper.make_tensor_value_info(""data_2"", TensorProto.FLOAT, (5,)),\n                helper.make_tensor_value_info(""data_3"", TensorProto.FLOAT, (5,)),\n                helper.make_tensor_value_info(""data_4"", TensorProto.FLOAT, (5,))],\n            [helper.make_tensor_value_info(""sum"", TensorProto.FLOAT, (5,))])\n        converted_model = self._converted(graph, helper.make_operatorsetid(\n            """", 8), 5)\n        # Assert equality of graph and converted_model\n        assert converted_model.graph.node[0].op_type == ""Sum""\n        assert converted_model.opset_import[0].version == 5\n\n    # Test AveragePool Adapter: 1 -> 8\n    def test_averagepool_up(self):  # type: () -> None\n        nodes = [helper.make_node(\'AveragePool\', [""X""], [""Y""], kernel_shape=[1, 1])]\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (5, 5,))],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (5, 5,))])\n        converted_model = self._converted(graph, helper.make_operatorsetid(\n            """", 1), 8)\n        # Assert equality of graph and converted_model\n        assert converted_model.graph.node[0].op_type == ""AveragePool""\n        assert converted_model.opset_import[0].version == 8\n\n    # Test AveragePool Adapter: 8 -> 1\n    def test_averagepool_down(self):  # type: () -> None\n        nodes = [helper.make_node(\'AveragePool\', [""X""], [""Y""], kernel_shape=[1, 1])]\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (5, 5,))],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (5, 5,))])\n        converted_model = self._converted(graph, helper.make_operatorsetid(\n            """", 8), 1)\n        # Assert equality of graph and converted_model\n        assert converted_model.graph.node[0].op_type == ""AveragePool""\n        assert converted_model.opset_import[0].version == 1\n\n    # Test Dropout Adapter: 1 -> 8\n    def test_dropout_up(self):  # type: () -> None\n        nodes = [helper.make_node(\'Dropout\', [""data""], [""output""], is_test=1)]\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""data"", TensorProto.FLOAT, (5, 5,))],\n            [helper.make_tensor_value_info(""output"", TensorProto.FLOAT, (5, 5,))])\n        converted_model = self._converted(graph, helper.make_operatorsetid(\n            """", 1), 8)\n        # Assert equality of graph and converted_model\n        assert converted_model.graph.node[0].op_type == ""Dropout""\n        assert converted_model.opset_import[0].version == 8\n\n    # Test Dropout Adapter: 8 -> 1\n    def test_dropout_down(self):  # type: () -> None\n        nodes = [helper.make_node(\'Dropout\', [""data""], [""output""])]\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""data"", TensorProto.FLOAT, (5, 5,))],\n            [helper.make_tensor_value_info(""output"", TensorProto.FLOAT, (5, 5,))])\n        converted_model = self._converted(graph, helper.make_operatorsetid(\n            """", 8), 1)\n        # Assert equality of graph and converted_model\n        assert converted_model.graph.node[0].op_type == ""Dropout""\n        assert converted_model.opset_import[0].version == 1\n\n    # Test Max Adapter: 7 -> 8\n    def test_max_7_8(self):  # type: () -> None\n        from_opset = 7\n        to_opset = 8\n        data_type = TensorProto.FLOAT\n        data_shape = (2, 3, 4)\n\n        nodes = [onnx.helper.make_node(\n            ""Max"",\n            inputs=[""X""],\n            outputs=[""Y""]\n        )]\n\n        graph = helper.make_graph(\n            nodes,\n            ""test_max"",\n            [onnx.helper.make_tensor_value_info(""X"", data_type, data_shape)],\n            [onnx.helper.make_tensor_value_info(""Y"", data_type, data_shape)])\n\n        converted_model = self._converted(graph, helper.make_operatorsetid("""", from_opset), to_opset)\n\n        assert converted_model.graph.node[0].op_type == ""Max""\n        assert converted_model.graph.output[0].type.tensor_type.elem_type == data_type\n        assert converted_model.opset_import[0].version == to_opset\n\n    # Test Min Adapter: 7 -> 8\n    def test_min_7_8(self):  # type: () -> None\n        from_opset = 7\n        to_opset = 8\n        data_type = TensorProto.FLOAT\n        data_shape = (2, 3, 4)\n\n        nodes = [onnx.helper.make_node(\n            ""Min"",\n            inputs=[""X""],\n            outputs=[""Y""]\n        )]\n\n        graph = helper.make_graph(\n            nodes,\n            ""test_min"",\n            [onnx.helper.make_tensor_value_info(""X"", data_type, data_shape)],\n            [onnx.helper.make_tensor_value_info(""Y"", data_type, data_shape)])\n\n        converted_model = self._converted(graph, helper.make_operatorsetid("""", from_opset), to_opset)\n\n        assert converted_model.graph.node[0].op_type == ""Min""\n        assert converted_model.graph.output[0].type.tensor_type.elem_type == data_type\n        assert converted_model.opset_import[0].version == to_opset\n\n    # Test Mean Adapter: 7 -> 8\n    def test_mean_7_8(self):  # type: () -> None\n        from_opset = 7\n        to_opset = 8\n        data_type = TensorProto.FLOAT\n        data_shape = (3,)\n\n        nodes = [onnx.helper.make_node(\n            ""Mean"",\n            inputs=[""X""],\n            outputs=[""Y""]\n        )]\n\n        graph = helper.make_graph(\n            nodes,\n            ""test_mean"",\n            [onnx.helper.make_tensor_value_info(""X"", data_type, data_shape)],\n            [onnx.helper.make_tensor_value_info(""Y"", data_type, data_shape)])\n\n        converted_model = self._converted(graph, helper.make_operatorsetid("""", from_opset), to_opset)\n\n        assert converted_model.graph.node[0].op_type == ""Mean""\n        assert converted_model.graph.output[0].type.tensor_type.elem_type == data_type\n        assert converted_model.opset_import[0].version == to_opset\n\n    # Test MaxPool Adapter: 1 -> 8\n    def test_maxpool_up(self):  # type: () -> None\n        nodes = [helper.make_node(\'MaxPool\', [""X""], [""Y""], kernel_shape=[1, 1])]\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (5, 5,))],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (5, 5,))])\n        converted_model = self._converted(graph, helper.make_operatorsetid(\n            """", 1), 8)\n        # Assert equality of graph and converted_model\n        assert converted_model.graph.node[0].op_type == ""MaxPool""\n        assert converted_model.opset_import[0].version == 8\n\n    # Test MaxPool Adapter: 8 -> 1\n    def test_maxpool_down(self):  # type: () -> None\n        nodes = [helper.make_node(\'MaxPool\', [""X""], [""Y""], kernel_shape=[1, 1])]\n        graph = helper.make_graph(\n            nodes,\n            ""test"",\n            [helper.make_tensor_value_info(""X"", TensorProto.FLOAT, (5, 5,))],\n            [helper.make_tensor_value_info(""Y"", TensorProto.FLOAT, (5, 5,))])\n        converted_model = self._converted(graph, helper.make_operatorsetid(\n            """", 8), 1)\n        # Assert equality of graph and converted_model\n        assert converted_model.graph.node[0].op_type == ""MaxPool""\n        assert converted_model.opset_import[0].version == 1\n\n    # Test BatchNormalization Adapter: 8 -> 9\n    def test_batch_normalization_8_9(self):  # type: () -> None\n        from_opset = 8\n        to_opset = 9\n        data_type = TensorProto.FLOAT\n\n        nodes = [helper.make_node(\n            \'BatchNormalization\',\n            inputs=[""x"", ""s"", ""bias"", ""mean"", ""var""],\n            outputs=[""y""]\n        )]\n\n        input_shape = (1, 2, 1, 3)\n        x = helper.make_tensor_value_info(""x"", data_type, input_shape)\n        scale = helper.make_tensor_value_info(""s"", data_type, [input_shape[1]])\n        B = helper.make_tensor_value_info(""bias"", data_type, [input_shape[1]])\n        mean = helper.make_tensor_value_info(""mean"", data_type, [input_shape[1]])\n        var = helper.make_tensor_value_info(""var"", data_type, [input_shape[1]])\n        y = helper.make_tensor_value_info(""y"", data_type, input_shape)\n\n        graph = helper.make_graph(\n            nodes,\n            ""test_batchnormalization_8_9"",\n            [x, scale, B, mean, var],\n            [y]\n        )\n\n        converted_model = self._converted(graph, helper.make_operatorsetid("""", from_opset), to_opset)\n\n        assert converted_model.graph.node[0].op_type == ""BatchNormalization""\n        assert converted_model.opset_import[0].version == to_opset\n\n    # Test BatchNormalization Adapter: 9 -> 8\n    def test_batchnormalization_9_8(self):  # type: () -> None\n        from_opset = 9\n        to_opset = 8\n        data_type = TensorProto.FLOAT\n\n        nodes = [onnx.helper.make_node(\n            \'BatchNormalization\',\n            inputs=[\'X\', \'scale\', \'B\', \'mean\', \'var\'],\n            outputs=[\'Y\'],\n        )]\n\n        input_shape = (2, 3, 4, 5)\n        x = onnx.helper.make_tensor_value_info(""X"", data_type, input_shape)\n        scale = onnx.helper.make_tensor_value_info(""scale"", data_type, [input_shape[1]])\n        B = onnx.helper.make_tensor_value_info(""B"", data_type, [input_shape[1]])\n        mean = onnx.helper.make_tensor_value_info(""mean"", data_type, [input_shape[1]])\n        var = onnx.helper.make_tensor_value_info(""var"", data_type, [input_shape[1]])\n        y = onnx.helper.make_tensor_value_info(""Y"", data_type, input_shape)\n\n        graph = onnx.helper.make_graph(\n            nodes, ""test_batchnormalization"", [x, scale, B, mean, var], [y]\n        )\n\n        converted_model = self._converted(graph, helper.make_operatorsetid("""", from_opset), to_opset)\n\n        assert converted_model.graph.node[0].op_type == ""BatchNormalization""\n        assert converted_model.opset_import[0].version == to_opset\n\n    # Test Constant Adapter: 8 -> 9\n    def test_constant_8_9(self):  # type: () -> None\n        from_opset = 8\n        to_opset = 9\n        data_type = TensorProto.FLOAT\n\n        output_shape = [2, 3, 4]\n        output_value = np.arange(24)\n\n        nodes = [helper.make_node(\n            ""Constant"",\n            inputs=[],\n            outputs=[""Y""],\n            value=helper.make_tensor("""", data_type, output_shape, output_value))]\n\n        graph = helper.make_graph(\n            nodes,\n            ""test_constant"",\n            [],\n            [onnx.helper.make_tensor_value_info(""Y"", data_type, output_shape)])\n\n        converted_model = self._converted(graph, helper.make_operatorsetid("""", from_opset), to_opset)\n\n        assert converted_model.graph.node[0].op_type == ""Constant""\n        assert converted_model.graph.output[0].type.tensor_type.elem_type == data_type\n        assert converted_model.opset_import[0].version == to_opset\n\n    # Test Constant Adapter: 9 -> 8\n    def test_constant_9_8(self):  # type: () -> None\n        from_opset = 9\n        to_opset = 8\n        data_type = TensorProto.UINT64\n\n        output_shape = [2, 3, 4]\n        output_value = np.arange(24)\n\n        nodes = [helper.make_node(\n            ""Constant"",\n            inputs=[],\n            outputs=[""Y""],\n            value=helper.make_tensor("""", data_type, output_shape, output_value))]\n\n        graph = helper.make_graph(\n            nodes,\n            ""test_constant"",\n            [],\n            [onnx.helper.make_tensor_value_info(""Y"", data_type, output_shape)])\n\n        converted_model = self._converted(graph, helper.make_operatorsetid("""", from_opset), to_opset)\n\n        assert converted_model.graph.node[0].op_type == ""Constant""\n        assert converted_model.graph.output[0].type.tensor_type.elem_type == data_type\n        assert converted_model.opset_import[0].version == to_opset\n\n    # Test Flatten Adapter: 8 -> 9\n    def test_flatten_8_9(self):  # type: () -> None\n        from_opset = 8\n        to_opset = 9\n        data_type = TensorProto.FLOAT\n\n        nodes = [onnx.helper.make_node(\n            ""Flatten"",\n            inputs=[""X""],\n            outputs=[""Y""],\n            axis=1\n        )]\n\n        graph = helper.make_graph(\n            nodes,\n            ""test_flatten"",\n            [onnx.helper.make_tensor_value_info(""X"", data_type, [2, 3, 4])],\n            [onnx.helper.make_tensor_value_info(""Y"", data_type, [2, 12])])\n\n        converted_model = self._converted(graph, helper.make_operatorsetid("""", from_opset), to_opset)\n\n        assert converted_model.graph.node[0].op_type == ""Flatten""\n        assert converted_model.graph.output[0].type.tensor_type.elem_type == data_type\n        assert converted_model.opset_import[0].version == to_opset\n\n    # Test Flatten Adapter: 9 -> 8\n    def test_flatten_9_8(self):  # type: () -> None\n        from_opset = 9\n        to_opset = 8\n        data_type = TensorProto.UINT64\n\n        nodes = [onnx.helper.make_node(\n            ""Flatten"",\n            inputs=[""X""],\n            outputs=[""Y""],\n            axis=1\n        )]\n\n        graph = helper.make_graph(\n            nodes,\n            ""test_flatten"",\n            [onnx.helper.make_tensor_value_info(""X"", data_type, [2, 3, 4])],\n            [onnx.helper.make_tensor_value_info(""Y"", data_type, [2, 12])])\n\n        converted_model = self._converted(graph, helper.make_operatorsetid("""", from_opset), to_opset)\n\n        assert converted_model.graph.node[1].op_type == ""Flatten""\n        assert converted_model.graph.output[0].type.tensor_type.elem_type == data_type\n        assert converted_model.opset_import[0].version == to_opset\n\n    # Test PRelu Adapter: 8 -> 9\n    def test_prelu_8_9(self):  # type: () -> None\n        from_opset = 8\n        to_opset = 9\n        data_type = TensorProto.FLOAT\n\n        nodes = [onnx.helper.make_node(\n            ""PRelu"",\n            inputs=[""X"", ""Slope""],\n            outputs=[""Y""]\n        )]\n\n        input_shape = [2, 3, 4]\n        graph = helper.make_graph(\n            nodes,\n            ""test_prelu"",\n            [onnx.helper.make_tensor_value_info(""X"", data_type, input_shape),\n             onnx.helper.make_tensor_value_info(""Slope"", data_type, input_shape)],\n            [onnx.helper.make_tensor_value_info(""Y"", data_type, input_shape)])\n\n        converted_model = self._converted(graph, helper.make_operatorsetid("""", from_opset), to_opset)\n\n        assert converted_model.graph.node[0].op_type == ""PRelu""\n        assert converted_model.graph.output[0].type.tensor_type.elem_type == data_type\n        assert converted_model.opset_import[0].version == to_opset\n\n    # Test PRelu Adapter: 9 -> 8\n    def test_prelu_9_8(self):  # type: () -> None\n        from_opset = 9\n        to_opset = 8\n        data_type = TensorProto.UINT64\n\n        nodes = [onnx.helper.make_node(\n            ""PRelu"",\n            inputs=[""X"", ""Slope""],\n            outputs=[""Y""]\n        )]\n\n        input_shape = [2, 3, 4]\n        graph = helper.make_graph(\n            nodes,\n            ""test_prelu"",\n            [onnx.helper.make_tensor_value_info(""X"", data_type, input_shape),\n             onnx.helper.make_tensor_value_info(""Slope"", data_type, input_shape)],\n            [onnx.helper.make_tensor_value_info(""Y"", data_type, input_shape)])\n\n        converted_model = self._converted(graph, helper.make_operatorsetid("""", from_opset), to_opset)\n\n        assert converted_model.graph.node[2].op_type == ""PRelu""\n        assert converted_model.graph.output[0].type.tensor_type.elem_type == data_type\n        assert converted_model.opset_import[0].version == to_opset\n\n    # Test Greater Adapter: 8 -> 9\n    def test_greater_8_9(self):  # type: () -> None\n        from_opset = 8\n        to_opset = 9\n        data_type = TensorProto.FLOAT\n\n        nodes = [onnx.helper.make_node(\n            ""Greater"",\n            inputs=[""X1"", ""X2""],\n            outputs=[""Y""]\n        )]\n\n        input_shape = [2, 3, 4]\n        graph = helper.make_graph(\n            nodes,\n            ""test_greater"",\n            [onnx.helper.make_tensor_value_info(""X1"", data_type, input_shape),\n             onnx.helper.make_tensor_value_info(""X2"", data_type, input_shape)],\n            [onnx.helper.make_tensor_value_info(""Y"", TensorProto.BOOL, input_shape)])\n\n        converted_model = self._converted(graph, helper.make_operatorsetid("""", from_opset), to_opset)\n\n        assert converted_model.graph.node[0].op_type == ""Greater""\n        assert converted_model.graph.output[0].type.tensor_type.elem_type == TensorProto.BOOL\n        assert converted_model.opset_import[0].version == to_opset\n\n    # Test Greater Adapter: 9 -> 8\n    def test_greater_9_8(self):  # type: () -> None\n        from_opset = 9\n        to_opset = 8\n        data_type = TensorProto.UINT64\n\n        nodes = [onnx.helper.make_node(\n            ""Greater"",\n            inputs=[""X1"", ""X2""],\n            outputs=[""Y""]\n        )]\n\n        input_shape = [2, 3, 4]\n        graph = helper.make_graph(\n            nodes,\n            ""test_greater"",\n            [onnx.helper.make_tensor_value_info(""X1"", data_type, input_shape),\n             onnx.helper.make_tensor_value_info(""X2"", data_type, input_shape)],\n            [onnx.helper.make_tensor_value_info(""Y"", TensorProto.BOOL, input_shape)])\n\n        converted_model = self._converted(graph, helper.make_operatorsetid("""", from_opset), to_opset)\n\n        assert converted_model.graph.node[2].op_type == ""Greater""\n        assert converted_model.graph.output[0].type.tensor_type.elem_type == TensorProto.BOOL\n        assert converted_model.opset_import[0].version == to_opset\n\n    # Test Less Adapter: 8 -> 9\n    def test_less_8_9(self):  # type: () -> None\n        from_opset = 8\n        to_opset = 9\n        data_type = TensorProto.FLOAT\n\n        nodes = [onnx.helper.make_node(\n            ""Less"",\n            inputs=[""X1"", ""X2""],\n            outputs=[""Y""]\n        )]\n\n        input_shape = [2, 3, 4]\n        graph = helper.make_graph(\n            nodes,\n            ""test_less"",\n            [onnx.helper.make_tensor_value_info(""X1"", data_type, input_shape),\n             onnx.helper.make_tensor_value_info(""X2"", data_type, input_shape)],\n            [onnx.helper.make_tensor_value_info(""Y"", TensorProto.BOOL, input_shape)])\n\n        converted_model = self._converted(graph, helper.make_operatorsetid("""", from_opset), to_opset)\n\n        assert converted_model.graph.node[0].op_type == ""Less""\n        assert converted_model.graph.output[0].type.tensor_type.elem_type == TensorProto.BOOL\n        assert converted_model.opset_import[0].version == to_opset\n\n    # Test Less Adapter: 9 -> 8\n    def test_less_9_8(self):  # type: () -> None\n        from_opset = 9\n        to_opset = 8\n        data_type = TensorProto.UINT64\n\n        nodes = [onnx.helper.make_node(\n            ""Less"",\n            inputs=[""X1"", ""X2""],\n            outputs=[""Y""]\n        )]\n\n        input_shape = [2, 3, 4]\n        graph = helper.make_graph(\n            nodes,\n            ""test_less"",\n            [onnx.helper.make_tensor_value_info(""X1"", data_type, input_shape),\n             onnx.helper.make_tensor_value_info(""X2"", data_type, input_shape)],\n            [onnx.helper.make_tensor_value_info(""Y"", TensorProto.BOOL, input_shape)])\n\n        converted_model = self._converted(graph, helper.make_operatorsetid("""", from_opset), to_opset)\n\n        assert converted_model.graph.node[2].op_type == ""Less""\n        assert converted_model.graph.output[0].type.tensor_type.elem_type == TensorProto.BOOL\n        assert converted_model.opset_import[0].version == to_opset\n\n    # Test MatMul Adapter: 8 -> 9\n    def test_matmul_8_9(self):  # type: () -> None\n        from_opset = 8\n        to_opset = 9\n        data_type = TensorProto.FLOAT\n\n        nodes = [onnx.helper.make_node(\n            ""MatMul"",\n            inputs=[""X1"", ""X2""],\n            outputs=[""Y""]\n        )]\n\n        graph = helper.make_graph(\n            nodes,\n            ""test_matmul"",\n            [onnx.helper.make_tensor_value_info(""X1"", data_type, [3, 4]),\n             onnx.helper.make_tensor_value_info(""X2"", data_type, [4, 3])],\n            [onnx.helper.make_tensor_value_info(""Y"", data_type, [3, 3])])\n\n        converted_model = self._converted(graph, helper.make_operatorsetid("""", from_opset), to_opset)\n\n        assert converted_model.graph.node[0].op_type == ""MatMul""\n        assert converted_model.graph.output[0].type.tensor_type.elem_type == data_type\n        assert converted_model.opset_import[0].version == to_opset\n\n    # Test MatMul Adapter: 9 -> 8\n    def test_matmul_9_8(self):  # type: () -> None\n        from_opset = 9\n        to_opset = 8\n        data_type = TensorProto.UINT64\n\n        nodes = [onnx.helper.make_node(\n            ""MatMul"",\n            inputs=[""X1"", ""X2""],\n            outputs=[""Y""]\n        )]\n\n        graph = helper.make_graph(\n            nodes,\n            ""test_matmul"",\n            [onnx.helper.make_tensor_value_info(""X1"", data_type, [3, 4]),\n             onnx.helper.make_tensor_value_info(""X2"", data_type, [4, 3])],\n            [onnx.helper.make_tensor_value_info(""Y"", data_type, [3, 3])])\n\n        converted_model = self._converted(graph, helper.make_operatorsetid("""", from_opset), to_opset)\n\n        assert converted_model.graph.node[2].op_type == ""MatMul""\n        assert converted_model.graph.output[0].type.tensor_type.elem_type == data_type\n        assert converted_model.opset_import[0].version == to_opset\n\n    # Test Gemm Adapter: 8 -> 9\n    def test_gemm_8_9(self):  # type: () -> None\n        from_opset = 8\n        to_opset = 9\n        data_type = TensorProto.FLOAT\n\n        nodes = [onnx.helper.make_node(\n            ""Gemm"",\n            inputs=[""X1"", ""X2"", ""X3""],\n            outputs=[""Y""]\n        )]\n\n        graph = helper.make_graph(\n            nodes,\n            ""test_gemm"",\n            [onnx.helper.make_tensor_value_info(""X1"", data_type, [3, 4]),\n             onnx.helper.make_tensor_value_info(""X2"", data_type, [4, 3]),\n             onnx.helper.make_tensor_value_info(""X3"", data_type, [3, 3])],\n            [onnx.helper.make_tensor_value_info(""Y"", data_type, [3, 3])])\n\n        converted_model = self._converted(graph, helper.make_operatorsetid("""", from_opset), to_opset)\n\n        assert converted_model.graph.node[0].op_type == ""Gemm""\n        assert converted_model.graph.output[0].type.tensor_type.elem_type == data_type\n        assert converted_model.opset_import[0].version == to_opset\n\n    # Test Gemm Adapter: 9 -> 8\n    def test_gemm_9_8(self):  # type: () -> None\n        from_opset = 9\n        to_opset = 8\n        data_type = TensorProto.UINT64\n\n        nodes = [onnx.helper.make_node(\n            ""Gemm"",\n            inputs=[""X1"", ""X2"", ""X3""],\n            outputs=[""Y""]\n        )]\n\n        graph = helper.make_graph(\n            nodes,\n            ""test_gemm"",\n            [onnx.helper.make_tensor_value_info(""X1"", data_type, [3, 4]),\n             onnx.helper.make_tensor_value_info(""X2"", data_type, [4, 3]),\n             onnx.helper.make_tensor_value_info(""X3"", data_type, [3, 3])],\n            [onnx.helper.make_tensor_value_info(""Y"", data_type, [3, 3])])\n\n        converted_model = self._converted(graph, helper.make_operatorsetid("""", from_opset), to_opset)\n\n        assert converted_model.graph.node[3].op_type == ""Gemm""\n        assert converted_model.graph.output[0].type.tensor_type.elem_type == data_type\n        assert converted_model.opset_import[0].version == to_opset\n\n    # Test Upsample Adapter: 8 -> 9\n    def test_upsample_8_9(self):  # type: () -> None\n        from_opset = 8\n        to_opset = 9\n        data_type = TensorProto.FLOAT\n\n        nodes = [onnx.helper.make_node(\n            ""Upsample"",\n            inputs=[""X""],\n            outputs=[""Y""],\n            mode=""nearest"",\n            scales=[1.0, 1.0, 2.0, 3.0],\n        )]\n\n        graph = helper.make_graph(\n            nodes,\n            ""test_upsample_8_9"",\n            [onnx.helper.make_tensor_value_info(""X"", data_type, [1, 1, 2, 2])],\n            [onnx.helper.make_tensor_value_info(""Y"", data_type, [1, 1, 4, 6])]\n        )\n\n        converted_model = self._converted(graph, helper.make_operatorsetid("""", from_opset), to_opset)\n\n        assert len(converted_model.graph.node) == 1\n        assert converted_model.graph.node[0].op_type == ""Upsample""\n        assert len(converted_model.graph.node[0].attribute) == 1\n        assert converted_model.graph.node[0].attribute[0].name == ""mode""\n        assert converted_model.opset_import[0].version == to_opset\n\n    # Test Helper for Upsample Adapter: 9 -> 8\n    def helper_upsample_with_initializer(self, raw_scale=False):  # type: (bool) -> None\n        from_opset = 9\n        to_opset = 8\n        data_type = TensorProto.FLOAT\n\n        nodes = [onnx.helper.make_node(\n            ""Upsample"",\n            inputs=[""X"", ""Scales""],\n            outputs=[""Y""],\n            mode=""nearest""\n        )]\n\n        scale_value = [1.0, 1.0, 2.0, 3.0]\n        scale_tensor = onnx.helper.make_tensor(""Scales"", onnx.TensorProto.FLOAT, [4], bytes(struct.pack(""4f"", *scale_value)) if raw_scale else scale_value, raw_scale)\n\n        graph = helper.make_graph(\n            nodes,\n            ""test_upsample"",\n            [onnx.helper.make_tensor_value_info(""X"", data_type, [1, 1, 2, 2]),\n             onnx.helper.make_tensor_value_info(""Scales"", data_type, [4])],\n            [onnx.helper.make_tensor_value_info(""Y"", data_type, [1, 1, 4, 6])],\n            [scale_tensor])\n\n        converted_model = self._converted(graph, helper.make_operatorsetid("""", from_opset), to_opset)\n\n        assert converted_model.graph.node[0].op_type == ""Upsample""\n        assert len(converted_model.graph.initializer) == 0\n        assert len(converted_model.graph.node[0].attribute) == 2\n        assert converted_model.graph.node[0].attribute[1].name == ""scales""\n        assert converted_model.opset_import[0].version == to_opset\n\n    # Test Helper for Upsample Adapter: 9 -> 8\n    def helper_upsample_with_constant(self, raw_scale=False):  # type: (bool) -> None\n        from_opset = 9\n        to_opset = 8\n        data_type = TensorProto.FLOAT\n\n        scale_value = [1.0, 1.0, 2.0, 3.0]\n        scale_tensor = onnx.helper.make_tensor(""const_value"", onnx.TensorProto.FLOAT, [4], bytes(struct.pack(""4f"", *scale_value)) if raw_scale else scale_value, raw_scale)\n        nodes = [\n            onnx.helper.make_node(\n                \'Constant\',\n                inputs=[],\n                outputs=[\'Constant_Output\'],\n                value=scale_tensor),\n            onnx.helper.make_node(\n                ""Upsample"",\n                inputs=[""X"", ""Constant_Output""],\n                outputs=[""Y""],\n                mode=""nearest"")]\n\n        graph = helper.make_graph(\n            nodes,\n            ""test_upsample"",\n            [onnx.helper.make_tensor_value_info(""X"", data_type, [1, 1, 2, 2])],\n            [onnx.helper.make_tensor_value_info(""Y"", data_type, [1, 1, 4, 6])],\n            value_info=[onnx.helper.make_tensor_value_info(""Constant_Output"", data_type, [4])])\n\n        converted_model = self._converted(graph, helper.make_operatorsetid("""", from_opset), to_opset)\n\n        assert len(converted_model.graph.node) == 1\n        assert converted_model.graph.node[0].op_type == ""Upsample""\n        assert len(converted_model.graph.node[0].attribute) == 2\n        assert converted_model.graph.node[0].attribute[1].name == ""scales""\n        assert converted_model.opset_import[0].version == to_opset\n\n    # Test Upsample Adapter: 9 -> 8\n    def test_upsample_with_constant_node_9_8(self):  # type: () -> None\n        self.helper_upsample_with_constant(raw_scale=False)\n\n    # Test Upsample Adapter: 9 -> 8\n    def test_upsample_with_initializer_9_8(self):  # type: () -> None\n        self.helper_upsample_with_initializer(raw_scale=False)\n\n    # Test Upsample Adapter: 9 -> 8\n    def test_upsample_with_raw_initializer_9_8(self):  # type: () -> None\n        self.helper_upsample_with_constant(raw_scale=True)\n\n    # Test Upsample Adapter: 9 -> 8\n    def test_upsample_with_raw_constant_node_9_8(self):  # type: () -> None\n        self.helper_upsample_with_constant(raw_scale=True)\n\n    # Test Scan Adapter: 8 -> 9\n    def test_scan_8_9(self):  # type: () -> None\n        from_opset = 8\n        to_opset = 9\n        data_type = TensorProto.FLOAT\n\n        node1 = onnx.helper.make_node(""Add"", inputs=[""sum_in"", ""next""], outputs=[""sum_out""],)\n        node2 = onnx.helper.make_node(""Identity"", inputs=[""sum_out""], outputs=[""scan_out""],)\n        g = onnx.helper.make_graph(\n            [node1, node2],\n            ""scan_body"",\n            [onnx.helper.make_tensor_value_info(""sum_in"", data_type, [2]),\n             onnx.helper.make_tensor_value_info(""next"", data_type, [2])],\n            [onnx.helper.make_tensor_value_info(""sum_out"", data_type, [2]),\n             onnx.helper.make_tensor_value_info(""scan_out"", data_type, [2])]\n        )\n\n        nodes = [onnx.helper.make_node(\n            ""Scan"",\n            inputs=["""", ""initial"", ""x""],\n            outputs=[""y"", ""z""],\n            body=g,\n            num_scan_inputs=1,\n        )]\n\n        seq_lens = onnx.helper.make_empty_tensor_value_info("" "")\n        initial = onnx.helper.make_tensor_value_info(""initial"", data_type, [1, 2])\n        x = onnx.helper.make_tensor_value_info(""x"", data_type, [1, 3, 2])\n        y = onnx.helper.make_tensor_value_info(""y"", data_type, [1, 2])\n        z = onnx.helper.make_tensor_value_info(""z"", data_type, [1, 3, 2])\n\n        graph = onnx.helper.make_graph(\n            nodes, ""test_scan_8_9"", [seq_lens, initial, x], [y, z]\n        )\n\n        converted_model = self._converted(graph, helper.make_operatorsetid("""", from_opset), to_opset)\n\n        assert converted_model.graph.node[0].op_type == ""Scan""\n        assert converted_model.opset_import[0].version == to_opset\n\n    # Test Cast Adapter: 8 -> 9\n    def test_cast_8_9(self):  # type: () -> None\n        from_opset = 8\n        to_opset = 9\n        data_type_from = TensorProto.FLOAT\n        data_type_to = TensorProto.UINT32\n\n        nodes = [onnx.helper.make_node(\n            ""Cast"",\n            inputs=[""X""],\n            outputs=[""Y""],\n            to=TensorProto.UINT32\n        )]\n\n        graph = helper.make_graph(\n            nodes,\n            ""test_cast"",\n            [onnx.helper.make_tensor_value_info(""X"", data_type_from, [2, 3])],\n            [onnx.helper.make_tensor_value_info(""Y"", data_type_to, [2, 3])])\n\n        converted_model = self._converted(graph, helper.make_operatorsetid("""", from_opset), to_opset)\n\n        assert converted_model.graph.node[0].op_type == ""Cast""\n        assert converted_model.graph.output[0].type.tensor_type.elem_type == data_type_to\n        assert converted_model.opset_import[0].version == to_opset\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
onnx/tools/__init__.py,0,b''
onnx/tools/net_drawer.py,0,"b'# A library and utility for drawing ONNX nets. Most of this implementation has\n# been borrowed from the caffe2 implementation\n# https://github.com/caffe2/caffe2/blob/master/caffe2/python/net_drawer.py\n#\n# The script takes two required arguments:\n#   -input: a path to a serialized ModelProto .pb file.\n#   -output: a path to write a dot file representation of the graph\n#\n# Given this dot file representation, you can-for example-export this to svg\n# with the graphviz `dot` utility, like so:\n#\n#   $ dot -Tsvg my_output.dot -o my_output.svg\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport argparse\nfrom collections import defaultdict\nimport json\nfrom onnx import ModelProto, GraphProto, NodeProto\nimport pydot  # type: ignore\nfrom typing import Text, Any, Callable, Optional, Dict\n\n\nOP_STYLE = {\n    \'shape\': \'box\',\n    \'color\': \'#0F9D58\',\n    \'style\': \'filled\',\n    \'fontcolor\': \'#FFFFFF\'\n}\n\nBLOB_STYLE = {\'shape\': \'octagon\'}\n\n_NodeProducer = Callable[[NodeProto, int], pydot.Node]\n\n\ndef _escape_label(name):  # type: (Text) -> Text\n    # json.dumps is poor man\'s escaping\n    return json.dumps(name)\n\n\ndef _form_and_sanitize_docstring(s):  # type: (Text) -> Text\n    url = \'javascript:alert(\'\n    url += _escape_label(s).replace(\'""\', \'\\\'\').replace(\'<\', \'\').replace(\'>\', \'\')\n    url += \')\'\n    return url\n\n\ndef GetOpNodeProducer(embed_docstring=False, **kwargs):  # type: (bool, **Any) -> _NodeProducer\n    def ReallyGetOpNode(op, op_id):  # type: (NodeProto, int) -> pydot.Node\n        if op.name:\n            node_name = \'%s/%s (op#%d)\' % (op.name, op.op_type, op_id)\n        else:\n            node_name = \'%s (op#%d)\' % (op.op_type, op_id)\n        for i, input in enumerate(op.input):\n            node_name += \'\\n input\' + str(i) + \' \' + input\n        for i, output in enumerate(op.output):\n            node_name += \'\\n output\' + str(i) + \' \' + output\n        node = pydot.Node(node_name, **kwargs)\n        if embed_docstring:\n            url = _form_and_sanitize_docstring(op.doc_string)\n            node.set_URL(url)\n        return node\n    return ReallyGetOpNode\n\n\ndef GetPydotGraph(\n    graph,  # type: GraphProto\n    name=None,  # type: Optional[Text]\n    rankdir=\'LR\',  # type: Text\n    node_producer=None,  # type: Optional[_NodeProducer]\n    embed_docstring=False,  # type: bool\n):  # type: (...) -> pydot.Dot\n    if node_producer is None:\n        node_producer = GetOpNodeProducer(embed_docstring=embed_docstring, **OP_STYLE)\n    pydot_graph = pydot.Dot(name, rankdir=rankdir)\n    pydot_nodes = {}  # type: Dict[Text, pydot.Node]\n    pydot_node_counts = defaultdict(int)  # type: Dict[Text, int]\n    for op_id, op in enumerate(graph.node):\n        op_node = node_producer(op, op_id)\n        pydot_graph.add_node(op_node)\n        for input_name in op.input:\n            if input_name not in pydot_nodes:\n                input_node = pydot.Node(\n                    _escape_label(\n                        input_name + str(pydot_node_counts[input_name])),\n                    label=_escape_label(input_name),\n                    **BLOB_STYLE\n                )\n                pydot_nodes[input_name] = input_node\n            else:\n                input_node = pydot_nodes[input_name]\n            pydot_graph.add_node(input_node)\n            pydot_graph.add_edge(pydot.Edge(input_node, op_node))\n        for output_name in op.output:\n            if output_name in pydot_nodes:\n                pydot_node_counts[output_name] += 1\n            output_node = pydot.Node(\n                _escape_label(\n                    output_name + str(pydot_node_counts[output_name])),\n                label=_escape_label(output_name),\n                **BLOB_STYLE\n            )\n            pydot_nodes[output_name] = output_node\n            pydot_graph.add_node(output_node)\n            pydot_graph.add_edge(pydot.Edge(op_node, output_node))\n    return pydot_graph\n\n\ndef main():  # type: () -> None\n    parser = argparse.ArgumentParser(description=""ONNX net drawer"")\n    parser.add_argument(\n        ""--input"",\n        type=Text, required=True,\n        help=""The input protobuf file."",\n    )\n    parser.add_argument(\n        ""--output"",\n        type=Text, required=True,\n        help=""The output protobuf file."",\n    )\n    parser.add_argument(\n        ""--rankdir"", type=Text, default=\'LR\',\n        help=""The rank direction of the pydot graph."",\n    )\n    parser.add_argument(\n        ""--embed_docstring"", action=""store_true"",\n        help=""Embed docstring as javascript alert. Useful for SVG format."",\n    )\n    args = parser.parse_args()\n    model = ModelProto()\n    with open(args.input, \'rb\') as fid:\n        content = fid.read()\n        model.ParseFromString(content)\n    pydot_graph = GetPydotGraph(\n        model.graph,\n        name=model.graph.name,\n        rankdir=args.rankdir,\n        node_producer=GetOpNodeProducer(\n            embed_docstring=args.embed_docstring,\n            **OP_STYLE\n        ),\n    )\n    pydot_graph.write_dot(args.output)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
onnx/tools/update_model_dims.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom six import string_types\nfrom typing import Any, List, Text, Dict, Set\nfrom onnx import ModelProto, ValueInfoProto\n\nimport onnx.checker\n\n\ndef update_inputs_outputs_dims(model, input_dims, output_dims):  # type: (ModelProto, Dict[Text, List[Any]], Dict[Text, List[Any]]) -> ModelProto\n    """"""\n        This function updates the dimension sizes of the model\'s inputs and outputs to the values\n        provided in input_dims and output_dims. if the dim value provided is negative, a unique dim_param\n        will be set for that dimension.\n\n        Example. if we have the following shape for inputs and outputs:\n                shape(input_1) = (\'b\', 3, \'w\', \'h\')\n                shape(input_2) = (\'b\', 4)\n                and shape(output)  = (\'b\', \'d\', 5)\n\n            The parameters can be provided as:\n                input_dims = {\n                    ""input_1"": [\'b\', 3, \'w\', \'h\'],\n                    ""input_2"": [\'b\', 4],\n                }\n                output_dims = {\n                    ""output"": [\'b\', -1, 5]\n                }\n\n            Putting it together:\n                model = onnx.load(\'model.onnx\')\n                updated_model = update_inputs_outputs_dims(model, input_dims, output_dims)\n                onnx.save(updated_model, \'model.onnx\')\n    """"""\n    dim_param_set = set()  # type: Set[Text]\n\n    def init_dim_param_set(dim_param_set, value_infos):  # type: (Set[Text], List[ValueInfoProto]) -> None\n        for info in value_infos:\n            shape = info.type.tensor_type.shape\n            for dim in shape.dim:\n                if dim.HasField(\'dim_param\'):\n                    dim_param_set.add(dim.dim_param)  # type: ignore\n\n    init_dim_param_set(dim_param_set, model.graph.input)  # type: ignore\n    init_dim_param_set(dim_param_set, model.graph.output)  # type: ignore\n    init_dim_param_set(dim_param_set, model.graph.value_info)  # type: ignore\n\n    def update_dim(tensor, dim, j, name):  # type: (ValueInfoProto, Any, int, Text) -> None\n        dim_proto = tensor.type.tensor_type.shape.dim[j]\n        if isinstance(dim, int):\n            if dim >= 0:\n                if dim_proto.HasField(\'dim_value\') and dim_proto.dim_value != dim:\n                    raise ValueError(\'Unable to set dimension value to {} for axis {} of {}. Contradicts existing dimension value {}.\'\n                        .format(dim, j, name, dim_proto.dim_value))\n                dim_proto.dim_value = dim\n            else:\n                generated_dim_param = name + \'_\' + str(j)\n                if generated_dim_param in dim_param_set:\n                    raise ValueError(\'Unable to generate unique dim_param for axis {} of {}. Please manually provide a dim_param value.\'\n                        .format(j, name))\n                dim_proto.dim_param = generated_dim_param\n        elif isinstance(dim, string_types):\n            dim_proto.dim_param = dim\n        else:\n            raise ValueError(\'Only int or str is accepted as dimension value, incorrect type: {}\'.format(type(dim)))\n\n    for input in model.graph.input:\n        input_name = input.name\n        input_dim_arr = input_dims[input_name]\n        for j, dim in enumerate(input_dim_arr):\n            update_dim(input, dim, j, input_name)\n\n    for output in model.graph.output:\n        output_name = output.name\n        output_dim_arr = output_dims[output_name]\n        for j, dim in enumerate(output_dim_arr):\n            update_dim(output, dim, j, output_name)\n\n    onnx.checker.check_model(model)\n    return model\n'"
onnx/backend/sample/__init__.py,0,b''
onnx/backend/test/__init__.py,0,b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\n# for backward compatibility\nfrom .runner import Runner as BackendTest # noqa\n'
onnx/backend/test/cmd_tools.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport argparse\nimport json\nimport os\nimport shutil\n\nimport onnx.backend.test.case.node as node_test\nimport onnx.backend.test.case.model as model_test\nfrom onnx import numpy_helper\nfrom typing import Text\n\n\nTOP_DIR = os.path.realpath(os.path.dirname(__file__))\nDATA_DIR = os.path.join(TOP_DIR, 'data')\n\n\ndef generate_data(args):  # type: (argparse.Namespace) -> None\n\n    def prepare_dir(path):  # type: (Text) -> None\n        if os.path.exists(path):\n            shutil.rmtree(path)\n        os.makedirs(path)\n\n    cases = model_test.collect_testcases() + node_test.collect_testcases()\n    for case in cases:\n        output_dir = os.path.join(\n            args.output, case.kind, case.name)\n        prepare_dir(output_dir)\n        if case.kind == 'real':\n            with open(os.path.join(output_dir, 'data.json'), 'w') as fi:\n                json.dump({\n                    'url': case.url,\n                    'model_name': case.model_name,\n                    'rtol': case.rtol,\n                    'atol': case.atol,\n                }, fi, sort_keys=True)\n        else:\n            with open(os.path.join(output_dir, 'model.onnx'), 'wb') as f:\n                f.write(case.model.SerializeToString())\n            for i, (inputs, outputs) in enumerate(case.data_sets):\n                data_set_dir = os.path.join(\n                    output_dir, 'test_data_set_{}'.format(i))\n                prepare_dir(data_set_dir)\n                for j, input_np in enumerate(inputs):\n                    tensor = numpy_helper.from_array(\n                        input_np, case.model.graph.input[j].name)\n                    with open(os.path.join(\n                            data_set_dir, 'input_{}.pb'.format(j)), 'wb') as f:\n                        f.write(tensor.SerializeToString())\n                for j, output_np in enumerate(outputs):\n                    tensor = numpy_helper.from_array(\n                        output_np, case.model.graph.output[j].name)\n                    with open(os.path.join(\n                            data_set_dir, 'output_{}.pb'.format(j)), 'wb') as f:\n                        f.write(tensor.SerializeToString())\n\n\ndef parse_args():  # type: () -> argparse.Namespace\n    parser = argparse.ArgumentParser('backend-test-tools')\n    subparsers = parser.add_subparsers()\n\n    subparser = subparsers.add_parser('generate-data', help='convert testcases to test data')\n    subparser.add_argument('-o', '--output', default=DATA_DIR,\n                           help='output directory (default: %(default)s)')\n    subparser.set_defaults(func=generate_data)\n\n    return parser.parse_args()\n\n\ndef main():  # type: () -> None\n    args = parse_args()\n    args.func(args)\n\n\nif __name__ == '__main__':\n    main()\n"""
onnx/backend/test/stat_coverage.py,0,"b'#!/usr/bin/env python\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport os\nimport io\n\nfrom onnx import defs, load, AttributeProto\nfrom onnx.backend.test.case import collect_snippets\nfrom onnx.backend.test.runner import Runner\nfrom onnx.backend.test.loader import load_model_tests\nfrom typing import Any, IO, Sequence, Text, Dict, List\n\n\ndef is_ml(schemas):  # type: (Sequence[defs.OpSchema]) -> bool\n    for s in schemas:\n        if s.domain == \'ai.onnx.ml\':\n            return True\n    return False\n\n\ndef gen_outlines(f, ml):  # type: (IO[Any], bool) -> None\n    f.write(\'# Test Coverage Report\')\n    if ml:\n        f.write(\' (ONNX-ML Operators)\\n\')\n    else:\n        f.write(\' (ONNX Core Operators)\\n\')\n    f.write(\'## Outlines\\n\')\n    f.write(\'* [Node Test Coverage](#node-test-coverage)\\n\')\n    f.write(\'* [Model Test Coverage](#model-test-coverage)\\n\')\n    f.write(\'* [Overall Test Coverage](#overall-test-coverage)\\n\')\n\n\ncommon_covered = []  # type: Sequence[Text]\nexperimental_covered = []  # type: Sequence[Text]\n\n\ndef gen_node_test_coverage(schemas, f, ml):\n    # type: (Sequence[defs.OpSchema], IO[Any], bool) -> None\n    global common_covered\n    global experimental_covered\n    generators = set({\n        \'Multinomial\',\n        \'RandomNormal\',\n        \'RandomNormalLike\',\n        \'RandomUniform\',\n        \'RandomUniformLike\',\n    })\n    node_tests = collect_snippets()\n    common_covered = sorted([s.name for s in schemas\n            if s.name in node_tests\n            and s.support_level == defs.OpSchema.SupportType.COMMON\n            and (s.domain == \'ai.onnx.ml\') == ml])\n    common_no_cover = sorted([s.name for s in schemas\n            if s.name not in node_tests\n            and s.support_level == defs.OpSchema.SupportType.COMMON\n            and (s.domain == \'ai.onnx.ml\') == ml])\n    common_generator = sorted([name for name in common_no_cover\n            if name in generators])\n    experimental_covered = sorted([s.name for s in schemas\n            if s.name in node_tests\n            and s.support_level == defs.OpSchema.SupportType.EXPERIMENTAL\n            and (s.domain == \'ai.onnx.ml\') == ml])\n    experimental_no_cover = sorted([s.name for s in schemas\n            if s.name not in node_tests\n            and s.support_level == defs.OpSchema.SupportType.EXPERIMENTAL\n            and (s.domain == \'ai.onnx.ml\') == ml])\n    experimental_generator = sorted([name for name in experimental_no_cover\n            if name in generators])\n    num_common = len(common_covered) + len(common_no_cover) \\\n            - len(common_generator)\n    num_experimental = len(experimental_covered) + len(experimental_no_cover) \\\n            - len(experimental_generator)\n    f.write(\'# Node Test Coverage\\n\')\n    f.write(\'## Summary\\n\')\n    if num_common:\n        f.write(\'Node tests have covered {}/{} ({:.2f}%, {} generators excluded) \'\n                \'common operators.\\n\\n\'.format(\n                    len(common_covered), num_common,\n                    (len(common_covered) / float(num_common) * 100),\n                    len(common_generator)))\n    else:\n        f.write(\'Node tests have covered 0/0 (N/A) common operators. \\n\\n\')\n    if num_experimental:\n        f.write(\'Node tests have covered {}/{} ({:.2f}%, {} generators excluded) \'\n                \'experimental operators.\\n\\n\'.format(\n                    len(experimental_covered), num_experimental,\n                    (len(experimental_covered) / float(num_experimental) * 100),\n                    len(experimental_generator)))\n    else:\n        f.write(\'Node tests have covered 0/0 (N/A) experimental operators.\\n\\n\')\n    titles = [\'&#x1F49A;Covered Common Operators\',\n              \'&#x1F494;No Cover Common Operators\',\n              \'&#x1F49A;Covered Experimental Operators\',\n              \'&#x1F494;No Cover Experimental Operators\',\n              ]\n    all_lists = [common_covered, common_no_cover,\n            experimental_covered, experimental_no_cover]\n    for t in titles:\n        f.write(\'* [{}](#{})\\n\'.format(t[9:], t[9:].lower().replace(\' \', \'-\')))\n    f.write(\'\\n\')\n    for t, l in zip(titles, all_lists):\n        f.write(\'## {}\\n\'.format(t))\n        for s in l:\n            f.write(\'### {}\'.format(s))\n            if s in node_tests:\n                f.write(\'\\nThere are {} test cases, listed as following:\\n\'.format(\n                    len(node_tests[s])))\n                for summary, code in sorted(node_tests[s]):\n                    f.write(\'<details>\\n\')\n                    f.write(\'<summary>{}</summary>\\n\\n\'.format(summary))\n                    f.write(\'```python\\n{}\\n```\\n\\n\'.format(code))\n                    f.write(\'</details>\\n\')\n            else:\n                if s in generators:\n                    f.write(\' (random generator operator)\\n\')\n                else:\n                    f.write(\' (call for test cases)\\n\')\n            f.write(\'\\n\\n\')\n        f.write(\'<br/>\\n\\n\')\n\n\ndef gen_model_test_coverage(schemas, f, ml):\n    # type: (Sequence[defs.OpSchema], IO[Any], bool) -> None\n    f.write(\'# Model Test Coverage\\n\')\n    # Process schemas\n    schema_dict = dict()\n    for schema in schemas:\n        schema_dict[schema.name] = schema\n    # Load models from each model test using Runner.prepare_model_data\n    # Need to grab associated nodes\n    attrs = dict()  # type: Dict[Text, Dict[Text, List[Any]]]\n    model_paths = []  # type: List[Any]\n    for rt in load_model_tests(kind=\'real\'):\n        model_dir = Runner.prepare_model_data(rt)\n        model_paths.append(os.path.join(model_dir, \'model.onnx\'))\n    model_paths.sort()\n    model_written = False\n    for model_pb_path in model_paths:\n        model = load(model_pb_path)\n        if ml:\n            ml_present = False\n            for opset in model.opset_import:\n                if opset.domain == \'ai.onnx.ml\':\n                    ml_present = True\n            if not ml_present:\n                continue\n            else:\n                model_written = True\n        f.write(\'## {}\\n\'.format(model.graph.name))\n        # Deconstruct model\n        num_covered = 0\n        for node in model.graph.node:\n            if node.op_type in common_covered or node.op_type in experimental_covered:\n                num_covered += 1\n                # Add details of which nodes are/aren\'t covered\n                # Iterate through and store each node\'s attributes\n                for attr in node.attribute:\n                    if node.op_type not in attrs:\n                        attrs[node.op_type] = dict()\n                    if attr.name not in attrs[node.op_type]:\n                        attrs[node.op_type][attr.name] = []\n                    if attr.type == AttributeProto.FLOAT:\n                        if attr.f not in attrs[node.op_type][attr.name]:\n                            attrs[node.op_type][attr.name].append(attr.f)\n                    elif attr.type == AttributeProto.INT:\n                        if attr.i not in attrs[node.op_type][attr.name]:\n                            attrs[node.op_type][attr.name].append(attr.i)\n                    elif attr.type == AttributeProto.STRING:\n                        if attr.s not in attrs[node.op_type][attr.name]:\n                            attrs[node.op_type][attr.name].append(attr.s)\n                    elif attr.type == AttributeProto.TENSOR:\n                        if attr.t not in attrs[node.op_type][attr.name]:\n                            attrs[node.op_type][attr.name].append(attr.t)\n                    elif attr.type == AttributeProto.GRAPH:\n                        if attr.g not in attrs[node.op_type][attr.name]:\n                            attrs[node.op_type][attr.name].append(attr.g)\n                    elif attr.type == AttributeProto.FLOATS:\n                        if attr.floats not in attrs[node.op_type][attr.name]:\n                            attrs[node.op_type][attr.name].append(attr.floats)\n                    elif attr.type == AttributeProto.INTS:\n                        if attr.ints not in attrs[node.op_type][attr.name]:\n                            attrs[node.op_type][attr.name].append(attr.ints)\n                    elif attr.type == AttributeProto.STRINGS:\n                        if attr.strings not in attrs[node.op_type][attr.name]:\n                            attrs[node.op_type][attr.name].append(attr.strings)\n                    elif attr.type == AttributeProto.TENSORS:\n                        if attr.tensors not in attrs[node.op_type][attr.name]:\n                            attrs[node.op_type][attr.name].append(attr.tensors)\n                    elif attr.type == AttributeProto.GRAPHS:\n                        if attr.graphs not in attrs[node.op_type][attr.name]:\n                            attrs[node.op_type][attr.name].append(attr.graphs)\n        f.write(\'\\n{} has {} nodes. Of these, {} are covered by node tests ({}%)\\n\\n\\n\'.format(\n            model.graph.name, num_covered, len(model.graph.node), 100.0 * float(\n                num_covered) / float(len(model.graph.node))))\n        # Iterate through attrs, print\n        f.write(\'<details>\\n\')\n        f.write(\'<summary>nodes</summary>\\n\\n\')\n        for op in sorted(attrs):\n            f.write(\'<details>\\n\')\n            # Get total number of attributes for node schema\n            f.write(\'<summary>{}: {} out of {} attributes covered</summary>\\n\\n\'\n                    .format(op, len(attrs[op].keys()), len(schema_dict[op]\n                        .attributes)))\n            for attribute in sorted(schema_dict[op].attributes):\n                if attribute in attrs[op]:\n                    f.write(\'{}: {}\\n\'.format(attribute, len(attrs[op][attribute])))\n                else:\n                    f.write(\'{}: 0\\n\'.format(attribute))\n            f.write(\'</details>\\n\')\n        f.write(\'</details>\\n\\n\\n\')\n    if not model_written and ml:\n        f.write(\'No model tests present for selected domain\\n\')\n\n\ndef gen_overall_test_coverage(schemas, f, ml):\n    # type: (Sequence[defs.OpSchema], IO[Any], bool) -> None\n    f.write(\'# Overall Test Coverage\\n\')\n    f.write(\'## To be filled.\\n\')\n\n\ndef main():\n    # type: () -> None\n    base_dir = os.path.dirname(os.path.dirname(os.path.dirname(\n        os.path.dirname(os.path.realpath(__file__)))))\n    docs_dir = os.path.join(base_dir, \'docs\')\n    schemas = defs.get_all_schemas()\n\n    has_ml = is_ml(schemas)\n    fname = os.path.join(docs_dir, \'TestCoverage.md\')\n    with io.open(fname, \'w+\', newline=\'\', encoding=""utf-8"") as f:  # type: ignore\n        gen_outlines(f, False)\n        gen_node_test_coverage(schemas, f, False)\n        gen_model_test_coverage(schemas, f, False)\n        gen_overall_test_coverage(schemas, f, False)\n\n    if has_ml:\n        fname = os.path.join(docs_dir, \'TestCoverage-ml.md\')\n        with io.open(fname, \'w+\', newline=\'\', encoding=""utf-8"") as f:  # type: ignore\n            gen_outlines(f, True)\n            gen_node_test_coverage(schemas, f, True)\n            gen_model_test_coverage(schemas, f, True)\n            gen_overall_test_coverage(schemas, f, True)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
onnx/backend/sample/ops/__init__.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport importlib\nimport inspect\nimport sys\nimport pkgutil\nfrom typing import Dict, Text\nfrom types import ModuleType\n\n\ndef collect_sample_implementations():  # type: () -> Dict[Text, Text]\n    dict = {}  # type: Dict[Text, Text]\n    _recursive_scan(sys.modules[__name__], dict)\n    return dict\n\n\ndef _recursive_scan(package, dict):  # type: (ModuleType, Dict[Text, Text]) -> None\n    pkg_dir = package.__path__  # type: ignore\n    module_location = package.__name__\n    for _module_loader, name, ispkg in pkgutil.iter_modules(pkg_dir):  # type: ignore\n        module_name = ""{}.{}"".format(module_location, name)  # Module/package\n        module = importlib.import_module(module_name)\n        dict[name] = inspect.getsource(module)\n        if ispkg:\n            _recursive_scan(module, dict)\n'"
onnx/backend/sample/ops/abs.py,0,b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\n\ndef abs(input):  # type: (np.ndarray) -> np.ndarray\n    return np.abs(input)\n'
onnx/backend/test/case/__init__.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport sys\n\nfrom .base import Snippets\nfrom .utils import import_recursive\nfrom typing import Dict, Text, List, Tuple\n\n\ndef collect_snippets():  # type: () -> Dict[Text, List[Tuple[Text, Text]]]\n    import_recursive(sys.modules[__name__])\n    return Snippets\n'"
onnx/backend/test/case/base.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom collections import defaultdict\nimport inspect\nfrom textwrap import dedent\nfrom typing import Dict, Text, List, Tuple, Type, Sequence, Any\n\nimport numpy as np  # type: ignore\nfrom six import add_metaclass\n\n\ndef process_snippet(op_name, name, export):  # type: (Text, Text, Any) -> Tuple[Text, Text]\n    snippet_name = name[len(\'export_\'):] or op_name.lower()\n    source_code = dedent(inspect.getsource(export))\n    # remove the function signature line\n    lines = source_code.splitlines()\n    assert lines[0] == \'@staticmethod\'\n    assert lines[1].startswith(\'def export\')\n    return snippet_name, dedent(""\\n"".join(lines[2:]))\n\n\nSnippets = defaultdict(list)  # type: Dict[Text, List[Tuple[Text, Text]]]\n\n\nclass _Exporter(type):\n    exports = defaultdict(list)  # type: Dict[Text, List[Tuple[Text, Text]]]\n\n    def __init__(cls, name, bases, dct):  # type: (str, Tuple[Type[Any], ...], Dict[str, Any]) -> None\n        for k, v in dct.items():\n            if k.startswith(\'export\'):\n                if not isinstance(v, staticmethod):\n                    raise ValueError(\n                        \'Only staticmethods could be named as export.*\')\n                export = getattr(cls, k)\n                Snippets[name].append(process_snippet(name, k, export))\n                # export functions should call expect and so populate\n                # TestCases\n                np.random.seed(seed=0)\n                export()\n        super(_Exporter, cls).__init__(name, bases, dct)\n\n\n@add_metaclass(_Exporter)\nclass Base(object):\n    pass\n'"
onnx/backend/test/case/test_case.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom collections import namedtuple\n\nTestCase = namedtuple('TestCase', [\n    'name',\n    'model_name',\n    'url',\n    'model_dir',\n    'model',\n    'data_sets',\n    'kind',\n    'rtol',\n    'atol',\n])\n"""
onnx/backend/test/case/utils.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport importlib\nimport pkgutil\nfrom types import ModuleType\nfrom typing import Optional, List\n\nimport numpy as np  # type: ignore\n\n\nall_numeric_dtypes = [\n    np.int8, np.int16, np.int32, np.int64,\n    np.uint8, np.uint16, np.uint32, np.uint64,\n    np.float16, np.float32, np.float64,\n]\n\n\ndef import_recursive(package):  # type: (ModuleType) -> None\n    """"""\n    Takes a package and imports all modules underneath it\n    """"""\n    pkg_dir = None  # type: Optional[List[str]]\n    pkg_dir = package.__path__  # type: ignore\n    module_location = package.__name__\n    for (_module_loader, name, ispkg) in pkgutil.iter_modules(pkg_dir):\n        module_name = ""{}.{}"".format(module_location, name)  # Module/package\n        module = importlib.import_module(module_name)\n        if ispkg:\n            import_recursive(module)\n'"
onnx/backend/test/loader/__init__.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport json\nimport os\n\nfrom ..case.test_case import TestCase\nfrom typing import List, Text, Optional\n\nDATA_DIR = os.path.join(\n    os.path.dirname(os.path.realpath(os.path.dirname(__file__))),\n    \'data\')\n\n\ndef load_model_tests(\n    data_dir=DATA_DIR,  # type: Text\n    kind=None,  # type: Optional[Text]\n):  # type: (...) -> List[TestCase]\n    \'\'\'Load model test cases from on-disk data files.\n    \'\'\'\n\n    supported_kinds = os.listdir(data_dir)\n    if kind not in supported_kinds:\n        raise ValueError(""kind must be one of {}"".format(supported_kinds))\n\n    testcases = []\n\n    kind_dir = os.path.join(data_dir, kind)\n    for test_name in os.listdir(kind_dir):\n        case_dir = os.path.join(kind_dir, test_name)\n        # skip the non-dir files, such as generated __init__.py.\n        rtol = 1e-3\n        atol = 1e-7\n        if not os.path.isdir(case_dir):\n            continue\n        if os.path.exists(os.path.join(case_dir, \'model.onnx\')):\n            url = None\n            model_name = test_name[len(\'test_\')]\n            model_dir = case_dir  # type: Optional[Text]\n        else:\n            with open(os.path.join(case_dir, \'data.json\')) as f:\n                data = json.load(f)\n                url = data[\'url\']\n                model_name = data[\'model_name\']\n                rtol = data.get(\'rtol\', 1e-3)\n                atol = data.get(\'atol\', 1e-7)\n                model_dir = None\n        testcases.append(\n            TestCase(\n                name=test_name,\n                url=url,\n                model_name=model_name,\n                model_dir=model_dir,\n                model=None,\n                data_sets=None,\n                kind=kind,\n                rtol=rtol,\n                atol=atol,\n            ))\n\n    return testcases\n'"
onnx/backend/test/report/__init__.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport pytest  # type: ignore\n\nfrom .coverage import Coverage\nfrom typing import Dict, Text, Sequence, Any, List\n\n_coverage = Coverage()\n_marks = {}  # type: Dict[Text, Sequence[Any]]\n\n\ndef _add_mark(mark, bucket):  # type: (Any, Text) -> None\n    proto = mark.args[0]\n    if isinstance(proto, list):\n        assert len(proto) == 1\n        proto = proto[0]\n    if proto is not None:\n        _coverage.add_proto(proto, bucket, mark.args[1] == 'RealModel')\n\n\ndef pytest_runtest_call(item):  # type: (pytest.nodes.Item) -> None\n    mark = item.get_closest_marker('onnx_coverage')\n    if mark:\n        assert item.nodeid not in _marks\n        _marks[item.nodeid] = mark\n\n\ndef pytest_runtest_logreport(report):  # type: (Any) -> None\n    if (report.when == 'call'\n        and report.outcome == 'passed'\n            and report.nodeid in _marks):\n        mark = _marks[report.nodeid]\n        _add_mark(mark, 'passed')\n\n\n@pytest.hookimpl(trylast=True)  # type: ignore\ndef pytest_terminal_summary(terminalreporter, exitstatus):  # type: (pytest.terminal.TerminalReporter, int) -> None\n    for mark in _marks.values():\n        _add_mark(mark, 'loaded')\n    _coverage.report_text(terminalreporter)\n"""
onnx/backend/test/report/base.py,0,b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\n\nclass ReporterBase(object):\n    pass\n'
onnx/backend/test/report/coverage.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom collections import defaultdict, OrderedDict\nimport os\nimport csv\nimport datetime\n\nfrom tabulate import tabulate  # type: ignore\n\nimport onnx\nfrom onnx import defs, helper, GraphProto\nfrom typing import Optional, Text, Set, Dict, IO, List, Any\n\n_all_schemas = defs.get_all_schemas()\n\n\nclass AttrCoverage(object):\n    def __init__(self):  # type: () -> None\n        self.name = None  # type: Optional[Text]\n        self.values = set()  # type: Set[Text]\n\n    def add(self, attr):  # type: (onnx.AttributeProto) -> None\n        assert self.name in [None, attr.name]\n        self.name = attr.name\n        value = helper.get_attribute_value(attr)\n        # Turn list into tuple so we can put it into set\n        # As value can be string, don\'t blindly turn `collections.Iterable`\n        # into tuple.\n        if isinstance(value, list):\n            value = tuple(value)\n        self.values.add(str(value))\n\n\nclass NodeCoverage(object):\n    def __init__(self):  # type: () -> None\n        self.op_type = None  # type: Optional[Text]\n        self.attr_coverages = defaultdict(AttrCoverage)  # type: Dict[Text, AttrCoverage]\n\n    def add(self, node):  # type: (onnx.NodeProto) -> None\n        assert self.op_type in [None, node.op_type]\n\n        if self.op_type is None:\n            self.op_type = node.op_type\n            assert self.op_type is not None\n            self.schema = defs.get_schema(self.op_type)\n\n        for attr in node.attribute:\n            self.attr_coverages[attr.name].add(attr)\n\n\nclass ModelCoverage(object):\n    def __init__(self):  # type: () -> None\n        self.name = None  # type: Optional[Text]\n        self.graph = None  # type: Optional[GraphProto]\n        self.node_coverages = defaultdict(NodeCoverage)  # type: Dict[Text, NodeCoverage]\n\n    def add(self, model):  # type: (onnx.ModelProto) -> None\n        assert self.name in [None, model.graph.name]\n\n        if self.name is None:\n            self.name = model.graph.name\n            assert self.name is not None\n            self.graph = model.graph\n\n        for node in model.graph.node:\n            self.node_coverages[node.op_type].add(node)\n\n\nclass Coverage(object):\n    def __init__(self):  # type: () -> None\n        self.buckets = {\n            \'loaded\': defaultdict(NodeCoverage),\n            \'passed\': defaultdict(NodeCoverage),\n        }  # type: Dict[Text, Dict[Text, NodeCoverage]]\n        self.models = {\n            \'loaded\': defaultdict(ModelCoverage),\n            \'passed\': defaultdict(ModelCoverage),\n        }  # type: Dict[Text, Dict[Text, ModelCoverage]]\n\n    def add_node(self, node, bucket):  # type: (onnx.NodeProto, Text) -> None\n        self.buckets[bucket][node.op_type].add(node)\n\n    def add_graph(self, graph, bucket):  # type: (onnx.GraphProto, Text) -> None\n        for node in graph.node:\n            self.add_node(node, bucket)\n\n    def add_model(self, model, bucket, is_model):  # type: (onnx.ModelProto, Text, bool) -> None\n        self.add_graph(model.graph, bucket)\n        # Only add model if name does not start with test\n        if is_model:\n            self.models[bucket][model.graph.name].add(model)\n\n    def add_proto(self, proto, bucket, is_model):  # type: (onnx.ModelProto, Text, bool) -> None\n        assert isinstance(proto, onnx.ModelProto)\n        self.add_model(proto, bucket, is_model)\n\n    def report_text(self, writer):  # type: (IO[Text]) -> None\n        writer.write(\'---------- onnx coverage: ----------\\n\')\n        writer.write(\'Operators (passed/loaded/total): {}/{}/{}\\n\'.format(\n            len(self.buckets[\'passed\']),\n            len(self.buckets[\'loaded\']),\n            len(_all_schemas)))\n        writer.write(\'------------------------------------\\n\')\n\n        rows = []\n        passed = []\n        all_ops = []  # type: List[Text]\n        experimental = []  # type: List[Text]\n        for op_cov in self.buckets[\'passed\'].values():\n            covered_attrs = [\n                \'{}: {}\'.format(attr_cov.name, len(attr_cov.values))\n                for attr_cov in op_cov.attr_coverages.values()]\n            uncovered_attrs = [\n                \'{}: 0\'.format(attr)\n                for attr in op_cov.schema.attributes\n                if attr not in op_cov.attr_coverages\n            ]\n            attrs = sorted(covered_attrs) + sorted(uncovered_attrs)\n            if attrs:\n                attrs_column = os.linesep.join(attrs)\n            else:\n                attrs_column = \'No attributes\'\n            rows.append([op_cov.op_type, attrs_column])\n            passed.append(op_cov.op_type)\n        writer.write(tabulate(\n            rows,\n            headers=[\'Operator\', \'Attributes\\n(name: #values)\'],\n            tablefmt=\'plain\'))\n        if os.environ.get(str(\'CSVDIR\')) is not None:\n            self.report_csv(all_ops, passed, experimental)\n\n    # This function writes the coverage report to a set of CSV files for\n    # the Backend Scoreboard (onnx.ai/backend-scoreboard). To enable this\n    # feature, set a CSVDIR environment variable locally with the directory\n    # where you would like the files to be written, relative to the\n    # directory from which you\'re running pytest.  The format of the CSV\n    # files is a column naming each op or model and columns for each\n    # backend with indications of whether the tests passed or failed for\n    # each row.\n    def report_csv(self, all_ops, passed, experimental):  # type: (List[Text], List[Optional[Text]], List[Text]) -> None\n        for schema in _all_schemas:\n            if schema.domain == \'\' or schema.domain == \'ai.onnx\':\n                all_ops.append(schema.name)\n                if schema.support_level == defs.OpSchema.SupportType.EXPERIMENTAL:\n                    experimental.append(schema.name)\n        all_ops.sort()\n        nodes_path = os.path.join(str(os.environ.get(\'CSVDIR\')),  # type: ignore\n                \'nodes.csv\')  # type: ignore\n        models_path = os.path.join(str(os.environ.get(\'CSVDIR\')),  # type: ignore\n                \'models.csv\')  # type: ignore\n        existing_nodes = OrderedDict()  # type: OrderedDict[Text, Dict[str, str]]\n        existing_models = OrderedDict()  # type: OrderedDict[Text, Dict[str, str]]\n        frameworks = []  # type: List[str]\n        if os.path.isfile(nodes_path):\n            with open(nodes_path, \'r\') as nodes_file:\n                reader = csv.DictReader(nodes_file)\n                frameworks = list(reader.fieldnames)\n                for row in reader:\n                    op = row[str(\'Op\')]\n                    del row[str(\'Op\')]\n                    existing_nodes[str(op)] = row\n        if os.path.isfile(models_path):\n            with open(models_path, \'r\') as models_file:\n                reader = csv.DictReader(models_file)\n                for row in reader:\n                    model = row[str(\'Model\')]\n                    del row[str(\'Model\')]\n                    existing_models[str(model)] = row\n        backend = os.environ.get(str(\'BACKEND\'))\n        other_frameworks = frameworks[1:]\n        with open(nodes_path, \'w\') as nodes_file:\n            if str(\'Op\') not in frameworks:\n                frameworks.append(str(\'Op\'))\n            if backend not in frameworks:\n                frameworks.append(str(backend))\n            else:\n                other_frameworks.remove(str(backend))\n            node_writer = csv.DictWriter(nodes_file, fieldnames=frameworks)\n            node_writer.writeheader()\n            for node in all_ops:\n                node_name = node\n                if node in experimental:\n                    node_name = node + \' (Experimental)\'\n                if node_name not in existing_nodes:\n                    # Also add Skipped for other nodes\n                    existing_nodes[node_name] = OrderedDict()\n                    for other_framework in other_frameworks:\n                        existing_nodes[node_name][other_framework] = str(""Skipped!"")\n                if node in passed:\n                    existing_nodes[node_name][str(backend)] = str(""Passed!"")\n                else:\n                    existing_nodes[node_name][str(backend)] = str(""Failed!"")\n            summaries = dict()  # type: Dict[Any, Any]\n            if ""Summary"" in existing_nodes:\n                summaries = existing_nodes[""Summary""]\n                del existing_nodes[""Summary""]\n            summaries[str(backend)] = \\\n                ""{}/{} node tests passed"".format(len(passed), len(all_ops))\n            summaries[\'Op\'] = \'Summary\'\n            for node in existing_nodes:\n                existing_nodes[node][str(\'Op\')] = str(node)\n                node_writer.writerow(existing_nodes[node])\n            node_writer.writerow(summaries)\n        with open(models_path, \'w\') as models_file:\n            frameworks[0] = str(""Model"")\n            model_writer = csv.DictWriter(models_file, fieldnames=frameworks)\n            model_writer.writeheader()\n            # Consider both buckets\n            num_models = 0\n            for bucket in self.models:\n                for model in self.models[bucket]:  # type: ignore\n                    # Both analyze and run the model on the backend\n                    num_covered = 0\n                    for node in self.models[bucket][model].node_coverages:\n                        if node in passed:\n                            num_covered += 1\n                    # TODO: Identify if there are models that are being\n                    # skipped/not loaded, but that are in other frameworks\n                    msg = ""Passed!""\n                    if bucket == \'loaded\':\n                        if model in self.models[\'passed\']:\n                            continue\n                        msg = ""Failed!""\n                    num_models += 1\n                    if model not in existing_models:\n                        # Also add Skipped for other models\n                        existing_models[model] = OrderedDict()\n                        for other_framework in other_frameworks:\n                            existing_models[model][other_framework] = str(""Skipped!"")\n                    existing_models[model][str(backend)] = str(""{}/{} nodes covered: {}""\n                        .format(num_covered, len(self.models[bucket][model]\n                            .node_coverages), msg))\n            summaries.clear()\n            if ""Summary"" in existing_models:\n                summaries = existing_models[""Summary""]\n                del existing_models[""Summary""]\n            if str(backend) in summaries:\n                del summaries[str(backend)]\n            summaries[str(backend)] = ""{}/{} model tests passed"" \\\n                .format(len(self.models[\'passed\']), num_models)\n            summaries[\'Model\'] = \'Summary\'\n            for model in existing_models:  # type: ignore\n                existing_models[model][str(\'Model\')] = model\n                model_writer.writerow(existing_models[model])\n            model_writer.writerow(summaries)\n        with open(os.path.join(str(os.environ.get(\'CSVDIR\')),  # type: ignore\n                \'metadata.csv\'), \'w\') as metadata_file:  # type: ignore\n            metadata_writer = csv.writer(metadata_file)\n            metadata_writer.writerow([""Latest Update"", datetime.datetime.now().isoformat().replace(\'T\', \' \')])\n'"
onnx/backend/test/runner/__init__.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom collections import defaultdict\nimport functools\nimport glob\nimport os\nimport re\nimport shutil\nimport sys\nimport tarfile\nimport tempfile\nimport time\nimport unittest\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom onnx import helper, numpy_helper, NodeProto, ModelProto\nfrom onnx.backend.base import Backend\nfrom six.moves.urllib.request import urlretrieve\nfrom ..loader import load_model_tests\nfrom ..case.test_case import TestCase\nfrom .item import TestItem\nfrom typing import Optional, Pattern, Set, Dict, Text, Type, Sequence, Any, Callable, Union, Iterable, List\n\n\nclass BackendIsNotSupposedToImplementIt(unittest.SkipTest):\n    pass\n\n\ndef retry_excute(times):  # type: (int) -> Callable[[Callable[..., Any]], Callable[..., Any]]\n    assert times >= 1\n\n    def wrapper(func):  # type: (Callable[..., Any]) -> Callable[..., Any]\n        @functools.wraps(func)\n        def wrapped(*args, **kwargs):  # type: (*Any, **Any) -> Any\n            for i in range(1, times + 1):\n                try:\n                    return func(*args, **kwargs)\n                except Exception:\n                    print(\'{} times tried\'.format(i))\n                    if i == times:\n                        raise\n                    time.sleep(5 * i)\n        return wrapped\n    return wrapper\n\n\nclass Runner(object):\n\n    def __init__(self, backend, parent_module=None):  # type: (Type[Backend], Optional[str]) -> None\n        self.backend = backend\n        self._parent_module = parent_module\n        self._include_patterns = set()  # type: Set[Pattern[Text]]\n        self._exclude_patterns = set()  # type: Set[Pattern[Text]]\n\n        # This is the source of the truth of all test functions.\n        # Properties `test_cases`, `test_suite` and `tests` will be\n        # derived from it.\n        # {category: {name: func}}\n        self._test_items = defaultdict(dict)  # type: Dict[Text, Dict[Text, TestItem]]\n\n        for rt in load_model_tests(kind=\'node\'):\n            self._add_model_test(rt, \'Node\')\n\n        for rt in load_model_tests(kind=\'real\'):\n            self._add_model_test(rt, \'Real\')\n\n        for rt in load_model_tests(kind=\'simple\'):\n            self._add_model_test(rt, \'Simple\')\n\n        for ct in load_model_tests(kind=\'pytorch-converted\'):\n            self._add_model_test(ct, \'PyTorchConverted\')\n\n        for ot in load_model_tests(kind=\'pytorch-operator\'):\n            self._add_model_test(ot, \'PyTorchOperator\')\n\n    def _get_test_case(self, name):  # type: (Text) -> Type[unittest.TestCase]\n        test_case = type(str(name), (unittest.TestCase,), {})\n        if self._parent_module:\n            test_case.__module__ = self._parent_module\n        return test_case\n\n    def include(self, pattern):  # type: (Text) -> Runner\n        self._include_patterns.add(re.compile(pattern))\n        return self\n\n    def exclude(self, pattern):  # type: (Text) -> Runner\n        self._exclude_patterns.add(re.compile(pattern))\n        return self\n\n    def enable_report(self):  # type: () -> Runner\n        import pytest  # type: ignore\n\n        for category, items_map in self._test_items.items():\n            for name, item in items_map.items():\n                item.func = pytest.mark.onnx_coverage(item.proto, category)(item.func)\n        return self\n\n    @property\n    def _filtered_test_items(self):  # type: () -> Dict[Text, Dict[Text, TestItem]]\n        filtered = {}  # type: Dict[Text, Dict[Text, TestItem]]\n        for category, items_map in self._test_items.items():\n            filtered[category] = {}\n            for name, item in items_map.items():\n                if (self._include_patterns\n                    and (not any(include.search(name)\n                             for include in self._include_patterns))):\n                    item.func = unittest.skip(\n                        \'no matched include pattern\'\n                    )(item.func)\n                for exclude in self._exclude_patterns:\n                    if exclude.search(name):\n                        item.func = unittest.skip(\n                            \'matched exclude pattern ""{}""\'.format(\n                                exclude.pattern)\n                        )(item.func)\n                filtered[category][name] = item\n        return filtered\n\n    @property\n    def test_cases(self):  # type: () -> Dict[str, Type[unittest.TestCase]]\n        \'\'\'\n        List of test cases to be applied on the parent scope\n        Example usage:\n            globals().update(BackendTest(backend).test_cases)\n        \'\'\'\n        test_cases = {}\n        for category, items_map in self._filtered_test_items.items():\n            test_case_name = str(\'OnnxBackend{}Test\').format(category)\n            test_case = self._get_test_case(test_case_name)\n            for name, item in sorted(items_map.items()):\n                setattr(test_case, name, item.func)\n            test_cases[test_case_name] = test_case\n        return test_cases\n\n    @property\n    def test_suite(self):  # type: () -> unittest.TestSuite\n        \'\'\'\n        TestSuite that can be run by TestRunner\n        Example usage:\n            unittest.TextTestRunner().run(BackendTest(backend).test_suite)\n        \'\'\'\n        suite = unittest.TestSuite()\n        for case in sorted(self.test_cases.values()):\n            suite.addTests(unittest.defaultTestLoader.loadTestsFromTestCase(case))\n        return suite\n\n    # For backward compatibility (we used to expose `.tests`)\n    @property\n    def tests(self):  # type: () -> Type[unittest.TestCase]\n        \'\'\'\n        One single unittest.TestCase that hosts all the test functions\n        Example usage:\n            onnx_backend_tests = BackendTest(backend).tests\n        \'\'\'\n        tests = self._get_test_case(\'OnnxBackendTest\')\n        for items_map in sorted(self._filtered_test_items.values()):\n            for name, item in sorted(items_map.items()):\n                setattr(tests, name, item.func)\n        return tests\n\n    @classmethod\n    def assert_similar_outputs(cls, ref_outputs, outputs, rtol, atol):  # type: (Sequence[Any], Sequence[Any], float, float) -> None\n        np.testing.assert_equal(len(ref_outputs), len(outputs))\n        for i in range(len(outputs)):\n            np.testing.assert_equal(ref_outputs[i].dtype, outputs[i].dtype)\n            if ref_outputs[i].dtype == np.object:\n                np.testing.assert_array_equal(ref_outputs[i], outputs[i])\n            else:\n                np.testing.assert_allclose(\n                    ref_outputs[i],\n                    outputs[i],\n                    rtol=rtol,\n                    atol=atol)\n\n    @classmethod\n    @retry_excute(3)\n    def download_model(cls, model_test, model_dir, models_dir):  # type: (TestCase, Text, Text) -> None\n        # On Windows, NamedTemporaryFile can not be opened for a\n        # second time\n        download_file = tempfile.NamedTemporaryFile(delete=False)\n        try:\n            download_file.close()\n            print(\'Start downloading model {} from {}\'.format(\n                model_test.model_name,\n                model_test.url))\n            urlretrieve(model_test.url, download_file.name)\n            print(\'Done\')\n            with tarfile.open(download_file.name) as t:\n                t.extractall(models_dir)\n        except Exception as e:\n            print(\'Failed to prepare data for model {}: {}\'.format(\n                model_test.model_name, e))\n            raise\n        finally:\n            os.remove(download_file.name)\n\n    @classmethod\n    def prepare_model_data(cls, model_test):  # type: (TestCase) -> Text\n        onnx_home = os.path.expanduser(os.getenv(\'ONNX_HOME\', os.path.join(\'~\', \'.onnx\')))\n        models_dir = os.getenv(\'ONNX_MODELS\',\n                               os.path.join(onnx_home, \'models\'))\n        model_dir = os.path.join(models_dir, model_test.model_name)  # type: Text\n        if not os.path.exists(os.path.join(model_dir, \'model.onnx\')):\n            if os.path.exists(model_dir):\n                bi = 0\n                while True:\n                    dest = \'{}.old.{}\'.format(model_dir, bi)\n                    if os.path.exists(dest):\n                        bi += 1\n                        continue\n                    shutil.move(model_dir, dest)\n                    break\n            os.makedirs(model_dir)\n\n            cls.download_model(model_test=model_test, model_dir=model_dir, models_dir=models_dir)\n        return model_dir\n\n    def _add_test(self,\n                  category,  # type: Text\n                  test_name,  # type: Text\n                  test_func,  # type: Callable[..., Any]\n                  report_item,  # type: List[Optional[Union[ModelProto, NodeProto]]]\n                  devices=(\'CPU\', \'CUDA\'),  # type: Iterable[Text]\n                  ):  # type: (...) -> None\n        # We don\'t prepend the \'test_\' prefix to improve greppability\n        if not test_name.startswith(\'test_\'):\n            raise ValueError(\n                \'Test name must start with test_: {}\'.format(test_name))\n\n        def add_device_test(device):  # type: (Text) -> None\n            device_test_name = \'{}_{}\'.format(test_name, device.lower())\n            if device_test_name in self._test_items[category]:\n                raise ValueError(\n                    \'Duplicated test name ""{}"" in category ""{}""\'.format(\n                        device_test_name, category))\n\n            @unittest.skipIf(  # type: ignore\n                not self.backend.supports_device(device),\n                ""Backend doesn\'t support device {}"".format(device))\n            @functools.wraps(test_func)\n            def device_test_func(*args, **kwargs):  # type: (*Any, **Any) -> Any\n                try:\n                    return test_func(*args, device=device, **kwargs)\n                except BackendIsNotSupposedToImplementIt as e:\n                    # hacky verbose reporting\n                    if \'-v\' in sys.argv or \'--verbose\' in sys.argv:\n                        print(\'Test {} is effectively skipped: {}\'.format(\n                            device_test_name, e))\n\n            self._test_items[category][device_test_name] = TestItem(\n                device_test_func, report_item)\n\n        for device in devices:\n            add_device_test(device)\n\n    def _add_model_test(self, model_test, kind):  # type: (TestCase, Text) -> None\n        # model is loaded at runtime, note sometimes it could even\n        # never loaded if the test skipped\n        model_marker = [None]  # type: List[Optional[Union[ModelProto, NodeProto]]]\n\n        def run(test_self, device):  # type: (Any, Text) -> None\n            if model_test.model_dir is None:\n                model_dir = self.prepare_model_data(model_test)\n            else:\n                model_dir = model_test.model_dir\n            model_pb_path = os.path.join(model_dir, \'model.onnx\')\n            model = onnx.load(model_pb_path)\n            model_marker[0] = model\n            if hasattr(self.backend, \'is_compatible\') \\\n               and callable(self.backend.is_compatible) \\\n               and not self.backend.is_compatible(model):\n                raise unittest.SkipTest(\'Not compatible with backend\')\n            prepared_model = self.backend.prepare(model, device)\n            assert prepared_model is not None\n\n            # TODO after converting all npz files to protobuf, we can delete this.\n            for test_data_npz in glob.glob(\n                    os.path.join(model_dir, \'test_data_*.npz\')):\n                test_data = np.load(test_data_npz, encoding=\'bytes\')\n                inputs = list(test_data[\'inputs\'])\n                outputs = list(prepared_model.run(inputs))\n                ref_outputs = test_data[\'outputs\']\n                self.assert_similar_outputs(ref_outputs, outputs,\n                                            rtol=model_test.rtol,\n                                            atol=model_test.atol)\n\n            for test_data_dir in glob.glob(\n                    os.path.join(model_dir, ""test_data_set*"")):\n                inputs = []\n                inputs_num = len(glob.glob(os.path.join(test_data_dir, \'input_*.pb\')))\n                for i in range(inputs_num):\n                    input_file = os.path.join(test_data_dir, \'input_{}.pb\'.format(i))\n                    tensor = onnx.TensorProto()\n                    with open(input_file, \'rb\') as f:\n                        tensor.ParseFromString(f.read())\n                    inputs.append(numpy_helper.to_array(tensor))\n                ref_outputs = []\n                ref_outputs_num = len(glob.glob(os.path.join(test_data_dir, \'output_*.pb\')))\n                for i in range(ref_outputs_num):\n                    output_file = os.path.join(test_data_dir, \'output_{}.pb\'.format(i))\n                    tensor = onnx.TensorProto()\n                    with open(output_file, \'rb\') as f:\n                        tensor.ParseFromString(f.read())\n                    ref_outputs.append(numpy_helper.to_array(tensor))\n                outputs = list(prepared_model.run(inputs))\n                self.assert_similar_outputs(ref_outputs, outputs,\n                                            rtol=model_test.rtol,\n                                            atol=model_test.atol)\n\n        self._add_test(kind + \'Model\', model_test.name, run, model_marker)\n'"
onnx/backend/test/runner/item.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom typing import Callable, Any, Union, List, Optional\nfrom onnx import NodeProto, ModelProto\n\n\n# A container that hosts the test function and the associated\n# test item (ModelProto)\n\n\nclass TestItem(object):\n    def __init__(self, func, proto):  # type: (Callable[..., Any], List[Optional[Union[ModelProto, NodeProto]]]) -> None\n        self.func = func\n        self.proto = proto\n'"
onnx/backend/test/case/model/__init__.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport sys\n\nimport onnx.defs\nimport numpy as np  # type: ignore\nfrom onnx import ModelProto\nfrom typing import List, Optional, Text, Sequence\nfrom ..utils import import_recursive\nfrom ..test_case import TestCase\n\n_SimpleModelTestCases = []\n\n\ndef expect(model,  # type: ModelProto\n           inputs,  # type: Sequence[np.ndarray]\n           outputs,  # type: Sequence[np.ndarray]\n           name=None,  # type: Optional[Text]\n           ):  # type: (...) -> None\n    name = name or model.graph.name\n    _SimpleModelTestCases.append(\n        TestCase(\n            name=name,\n            model_name=model.graph.name,\n            url=None,\n            model_dir=None,\n            model=model,\n            data_sets=[(inputs, outputs)],\n            kind='simple',\n            rtol=1e-3,\n            atol=1e-7,\n        ))\n\n\nbase_model_opset_version = 10\nBASE_URL = 'https://s3.amazonaws.com/download.onnx/models/opset_{}'.format(\n    base_model_opset_version)\n\n\ndef collect_testcases():  # type: () -> List[TestCase]\n    '''Collect model test cases defined in python/numpy code and in model zoo.\n    '''\n\n    real_model_testcases = []\n\n    model_tests = [\n        ('test_bvlc_alexnet', 'bvlc_alexnet', 1e-3, 1e-7),\n        ('test_densenet121', 'densenet121', 2e-3, 1e-7),\n        ('test_inception_v1', 'inception_v1', 1e-3, 1e-7),\n        ('test_inception_v2', 'inception_v2', 1e-3, 1e-7),\n        ('test_resnet50', 'resnet50', 1e-3, 1e-7),\n        ('test_shufflenet', 'shufflenet', 1e-3, 1e-7),\n        ('test_squeezenet', 'squeezenet', 1e-3, 1e-7),\n        ('test_vgg19', 'vgg19', 1e-3, 1e-7),\n        ('test_zfnet512', 'zfnet512', 1e-3, 1e-7),\n    ]\n\n    for test_name, model_name, rtol, atol in model_tests:\n        url = '{}/{}.tar.gz'.format(BASE_URL, model_name)\n        real_model_testcases.append(TestCase(\n            name=test_name,\n            model_name=model_name,\n            url=url,\n            model_dir=None,\n            model=None,\n            data_sets=None,\n            kind='real',\n            rtol=rtol,\n            atol=atol,\n        ))\n\n    import_recursive(sys.modules[__name__])\n\n    return real_model_testcases + _SimpleModelTestCases\n"""
onnx/backend/test/case/model/expand.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\nfrom typing import Sequence\n\n\nclass ExpandDynamicShape(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n\n        def make_graph(node, input_shape, shape_shape, output_shape):  # type: (onnx.helper.NodeProto, Sequence[int], Sequence[int], Sequence[int]) -> onnx.helper.GraphProto\n            graph = onnx.helper.make_graph(\n                nodes=[node],\n                name=\'Expand\',\n                inputs=[onnx.helper.make_tensor_value_info(\'X\',\n                                                             onnx.TensorProto.FLOAT,\n                                                             input_shape),\n                          onnx.helper.make_tensor_value_info(\'shape\',\n                                                             onnx.TensorProto.INT64,\n                                                             shape_shape)],\n                outputs=[onnx.helper.make_tensor_value_info(\'Y\',\n                                                              onnx.TensorProto.FLOAT,\n                                                              output_shape)])\n            return graph\n\n        node = onnx.helper.make_node(\n            \'Expand\', [\'X\', \'shape\'], [\'Y\'], name=\'test\')\n        input_shape = [1, 3, 1]\n        x = np.ones(input_shape, dtype=np.float32)\n\n        #1st testcase\n        shape = np.array([3, 1], dtype=np.int64)\n        y = x * np.ones(shape, dtype=np.float32)\n        graph = make_graph(node, input_shape, shape.shape, y.shape)\n        model = onnx.helper.make_model(graph, producer_name=\'backend-test\')\n        expect(model, inputs=[x, shape], outputs=[y], name=""test_expand_shape_model1"")\n\n        #2nd testcase\n        shape = np.array([1, 3], dtype=np.int64)\n        y = x * np.ones(shape, dtype=np.float32)\n        graph = make_graph(node, input_shape, shape.shape, y.shape)\n        model = onnx.helper.make_model(graph, producer_name=\'backend-test\')\n        expect(model, inputs=[x, shape], outputs=[y], name=""test_expand_shape_model2"")\n\n        #3rd testcase\n        shape = np.array([3, 1, 3], dtype=np.int64)\n        y = x * np.ones(shape, dtype=np.float32)\n        graph = make_graph(node, input_shape, shape.shape, y.shape)\n        model = onnx.helper.make_model(graph, producer_name=\'backend-test\')\n        expect(model, inputs=[x, shape], outputs=[y], name=""test_expand_shape_model3"")\n\n        #4th testcase\n        shape = np.array([3, 3, 1, 3], dtype=np.int64)\n        y = x * np.ones(shape, dtype=np.float32)\n        graph = make_graph(node, input_shape, shape.shape, y.shape)\n        model = onnx.helper.make_model(graph, producer_name=\'backend-test\')\n        expect(model, inputs=[x, shape], outputs=[y], name=""test_expand_shape_model4"")\n'"
onnx/backend/test/case/model/gradient.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom onnx.defs import ONNX_DOMAIN, AI_ONNX_PREVIEW_TRAINING_DOMAIN\nfrom ..base import Base\nfrom . import expect\n\n\nclass Gradient(Base):\n\n    @staticmethod\n    def export_gradient_scalar_add():  # type: () -> None\n        add_node = onnx.helper.make_node('Add',\n                                         ['a', 'b'], ['c'], name='my_add')\n        gradient_node = onnx.helper.make_node(\n            'Gradient', ['a', 'b'],\n            ['dc_da', 'dc_db'], name='my_gradient',\n            domain=AI_ONNX_PREVIEW_TRAINING_DOMAIN,\n            xs=['a', 'b'], y='c')\n\n        a = np.array(1.0).astype(np.float32)\n        b = np.array(2.0).astype(np.float32)\n        c = a + b\n        # dc / da = d(a+b) / da = 1\n        dc_da = np.array(1).astype(np.float32)\n        # db / db = d(a+b) / db = 1\n        dc_db = np.array(1).astype(np.float32)\n\n        graph = onnx.helper.make_graph(\n            nodes=[add_node, gradient_node],\n            name='GradientOfAdd',\n            inputs=[\n                onnx.helper.make_tensor_value_info('a', onnx.TensorProto.FLOAT,\n                                                   []),\n                onnx.helper.make_tensor_value_info('b', onnx.TensorProto.FLOAT,\n                                                   [])],\n            outputs=[\n                onnx.helper.make_tensor_value_info('c', onnx.TensorProto.FLOAT,\n                                                   []),\n                onnx.helper.make_tensor_value_info('dc_da',\n                                                   onnx.TensorProto.FLOAT, []),\n                onnx.helper.make_tensor_value_info('dc_db',\n                                                   onnx.TensorProto.FLOAT, [])])\n        opsets = [\n            onnx.helper.make_operatorsetid(ONNX_DOMAIN, 12),\n            onnx.helper.make_operatorsetid(AI_ONNX_PREVIEW_TRAINING_DOMAIN, 1)]\n        model = onnx.helper.make_model(\n            graph,\n            producer_name='backend-test',\n            opset_imports=opsets)\n        expect(model, inputs=[a, b], outputs=[c, dc_da, dc_db],\n               name='test_gradient_of_add')\n\n    @staticmethod\n    def export_gradient_scalar_add_and_mul():  # type: () -> None\n        add_node = onnx.helper.make_node('Add',\n                                         ['a', 'b'], ['c'], name='my_add')\n        mul_node = onnx.helper.make_node('Mul',\n                                         ['c', 'a'], ['d'], name='my_mul')\n        gradient_node = onnx.helper.make_node(\n            'Gradient', ['a', 'b'],\n            ['dd_da', 'dd_db'], name='my_gradient',\n            domain=AI_ONNX_PREVIEW_TRAINING_DOMAIN,\n            xs=['a', 'b'], y='d')\n\n        a = np.array(1.0).astype(np.float32)\n        b = np.array(2.0).astype(np.float32)\n        c = a + b\n        # d = a * c = a * (a + b)\n        d = a * c\n        # dd / da = d(a*a+a*b) / da = 2 * a + b\n        dd_da = (2 * a + b).astype(np.float32)\n        # dd / db = d(a*a+a*b) / db = a\n        dd_db = a\n\n        graph = onnx.helper.make_graph(\n            nodes=[add_node, mul_node, gradient_node],\n            name='GradientOfTwoOperators',\n            inputs=[\n                onnx.helper.make_tensor_value_info('a', onnx.TensorProto.FLOAT,\n                                                   []),\n                onnx.helper.make_tensor_value_info('b', onnx.TensorProto.FLOAT,\n                                                   [])],\n            outputs=[\n                onnx.helper.make_tensor_value_info('d', onnx.TensorProto.FLOAT,\n                                                   []),\n                onnx.helper.make_tensor_value_info('dd_da',\n                                                   onnx.TensorProto.FLOAT, []),\n                onnx.helper.make_tensor_value_info('dd_db',\n                                                   onnx.TensorProto.FLOAT, [])])\n\n        opsets = [\n            onnx.helper.make_operatorsetid(ONNX_DOMAIN, 12),\n            onnx.helper.make_operatorsetid(AI_ONNX_PREVIEW_TRAINING_DOMAIN, 1)]\n        model = onnx.helper.make_model(graph,\n            producer_name='backend-test',\n            opset_imports=opsets)\n        expect(model, inputs=[a, b], outputs=[d, dd_da, dd_db],\n               name='test_gradient_of_add_and_mul')\n"""
onnx/backend/test/case/model/sequence.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nimport typing\nfrom ..base import Base\nfrom . import expect\nfrom onnx import TensorProto\nfrom typing import List, Optional, Text, Union\n\n\ndef SequenceEmptyImpl():  # type: () -> List[Optional[np.ndarray]]\n    return []\n\n\ndef SequenceConstructImpl(*tensors):  # type: (*np.ndarray) -> List[np.ndarray]\n    return list(tensors)\n\n\ndef SequenceInsertImpl(sequence, tensor, position=None):\n    # type: (List[np.ndarray], np.ndarray, Optional[int]) -> List[np.ndarray]\n    if position is None:\n        position = len(sequence)\n    sequence.insert(position, tensor)\n    return sequence\n\n\ndef SequenceAtImpl(sequence, position):\n    # type: (List[np.ndarray], int) -> np.ndarray\n    return sequence[position]\n\n\ndef SequenceEraseImpl(sequence, position=None):\n    # type: (List[np.ndarray], Optional[int]) -> List[Optional[np.ndarray]]\n    if position is None:\n        position = -1\n    del sequence[position]\n    return sequence\n\n\ndef SequenceLengthImpl(sequence):\n    # type: (List[np.ndarray]) -> np.int64\n    return np.int64(len(sequence))\n\n\ndef SplitToSequenceImpl(tensor, split=None, axis=0, keepdims=1):\n    # type: (np.ndarray, Optional[Union[int, List[int]]], int, int) -> List[np.ndarray]\n    dim_size = tensor.shape[axis]\n    if split is None:\n        split = 1\n        split_indices = [i * split + 1 for i in range(dim_size) if i * split + 1 < dim_size]\n        if not keepdims:\n            results = np.array_split(tensor, split_indices, axis)\n            return [np.squeeze(res, axis) for res in results]\n    if np.isscalar(split):\n        split_indices = [i * split + 1 for i in range(dim_size) if i * split + 1 < dim_size]  # type: ignore\n    else:\n        split_indices = np.cumsum(split) + 1\n    return np.array_split(tensor, split_indices, axis)  # type: ignore\n\n\ndef ConcatFromSequenceImpl(sequence, axis, new_axis=0):\n    # type: (List[np.ndarray], int, Optional[int]) -> np.ndarray\n    if not new_axis:\n        return np.concatenate(sequence, axis)\n    else:\n        return np.stack(sequence, axis)\n\n\nclass Sequence(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n\n        def make_graph(\n                nodes,  # type: List[onnx.helper.NodeProto]\n                input_shapes,  # type: List[Optional[typing.Sequence[Union[Text, int]]]]\n                output_shapes,  # type: List[Optional[typing.Sequence[Union[Text, int]]]]\n                input_names,  # type: List[Text]\n                output_names,  # type: List[Text]\n                input_types,  # type: List[TensorProto.DataType]\n                output_types,  # type: List[TensorProto.DataType]\n                initializers=None  # type: Optional[List[TensorProto]]\n        ):  # type: (...) -> onnx.helper.GraphProto\n            graph = onnx.helper.make_graph(\n                nodes=nodes,\n                name=\'Sequence\',\n                inputs=[\n                    onnx.helper.make_tensor_value_info(\n                        name,\n                        input_type,\n                        input_shape)\n                    for name, input_type, input_shape in zip(input_names, input_types, input_shapes)],\n                outputs=[\n                    onnx.helper.make_tensor_value_info(\n                        name,\n                        output_type,\n                        output_shape)\n                    for name, output_type, output_shape in zip(output_names, output_types, output_shapes)],\n                initializer=initializers)\n            return graph\n\n        #1st testcase - insert and at.\n        # 1. SequenceEmpty:         -> []\n        # 2. SequenceInsert(x):     -> [x]\n        # 3. SequenceInsert(y):     -> [x, y]\n        # 4. SequenceInsert(z, 1):  -> [x, z, y]\n        # 5. SequenceAt(2):         -> y\n        seq_empty_node = onnx.helper.make_node(\'SequenceEmpty\', [], [\'Seq_empty\'])\n        seq_insert_node = onnx.helper.make_node(\'SequenceInsert\', [\'Seq_empty\', \'X\'], [\'Seq_1\'])\n        seq_insert_node2 = onnx.helper.make_node(\'SequenceInsert\', [\'Seq_1\', \'Y\'], [\'Seq_2\'])\n        seq_insert_node3 = onnx.helper.make_node(\'SequenceInsert\', [\'Seq_2\', \'Z\', \'pos\'], [\'Seq_3\'])\n        seq_at_node = onnx.helper.make_node(\'SequenceAt\', [\'Seq_3\', \'pos_at\'], [\'out\'])\n\n        x_shape = [2, 3, 4]\n        y_shape = [1, 3, 4]\n        z_shape = [3, 3, 4]\n        out_shape = [None, 3, 4]\n\n        x = np.ones(x_shape, dtype=np.float32)\n        y = np.zeros(y_shape, dtype=np.float32)\n        z = np.ones(z_shape, dtype=np.float32) * 2\n        pos_val = 1\n        pos_at_val = 2\n\n        out = SequenceEmptyImpl()\n        out = SequenceInsertImpl(out, x)\n        out = SequenceInsertImpl(out, y)\n        out = SequenceInsertImpl(out, z, pos_val)\n        out = SequenceAtImpl(out, pos_at_val)\n        assert np.array_equal(out, y)\n\n        pos = onnx.helper.make_tensor(\'pos\', TensorProto.INT64, (), (pos_val, ))\n        pos_at = onnx.helper.make_tensor(\'pos_at\', TensorProto.INT64, (), (pos_at_val, ))\n\n        graph = make_graph(\n            [seq_empty_node, seq_insert_node, seq_insert_node2, seq_insert_node3, seq_at_node],\n            [x_shape, y_shape, z_shape, [], []],  # type: ignore\n            [out_shape],  # type: ignore\n            [\'X\', \'Y\', \'Z\', \'pos\', \'pos_at\'],\n            [\'out\'],\n            [onnx.TensorProto.FLOAT] * 3 + [onnx.TensorProto.INT64] * 2,  # type: ignore\n            [onnx.TensorProto.FLOAT],\n            [pos, pos_at])\n        model = onnx.helper.make_model(graph, producer_name=\'backend-test\')\n        expect(model, inputs=[x, y, z], outputs=[out], name=""test_sequence_model1"")\n\n        #2nd testcase - erase and at.\n        # 1. SequenceConstruct(x, y, z):    -> [x, y, z]\n        # 2. SequenceErase(1):              -> [x, z]\n        # 3. SequenceAt(1):                 -> z\n        seq_construct_node = onnx.helper.make_node(\'SequenceConstruct\', [\'X\', \'Y\', \'Z\'], [\'seq_1\'])\n        seq_erase_node = onnx.helper.make_node(\'SequenceErase\', [\'seq_1\', \'pos_erase\'], [\'seq_2\'])\n        seq_at_node = onnx.helper.make_node(\'SequenceAt\', [\'seq_2\', \'pos_at\'], [\'out\'])\n\n        tensor_shape = [2, 3, 4]\n\n        x = np.ones(tensor_shape, dtype=np.float32)\n        y = np.zeros(tensor_shape, dtype=np.float32)\n        z = np.ones(tensor_shape, dtype=np.float32) * 2\n        pos_erase_val = 1\n        pos_at_val = 1\n\n        out = SequenceConstructImpl(x, y, z)\n        out = SequenceEraseImpl(out, pos_erase_val)\n        out = SequenceAtImpl(out, pos_at_val)\n        assert np.array_equal(out, z)\n\n        pos_erase = onnx.helper.make_tensor(\'pos_erase\', TensorProto.INT64, (), (pos_erase_val, ))\n        pos_at = onnx.helper.make_tensor(\'pos_at\', TensorProto.INT64, (), (pos_at_val, ))\n\n        graph = make_graph(\n            [seq_construct_node, seq_erase_node, seq_at_node],\n            [tensor_shape, tensor_shape, tensor_shape, [], []],  # type: ignore\n            [tensor_shape],  # type: ignore\n            [\'X\', \'Y\', \'Z\', \'pos_erase\', \'pos_at\'],\n            [\'out\'],\n            [onnx.TensorProto.FLOAT] * 3 + [onnx.TensorProto.INT64] * 2,  # type: ignore\n            [onnx.TensorProto.FLOAT],\n            [pos_erase, pos_at])\n        model = onnx.helper.make_model(graph, producer_name=\'backend-test\')\n        expect(model, inputs=[x, y, z], outputs=[out], name=""test_sequence_model2"")\n\n        #3rd testcase - erase, insert and at, with negative index value.\n        # 1. SequenceConstruct(x, y, z):    -> [x, y, z]\n        # 2. SequenceErase(-3):             -> [y, z]\n        # 3. SequenceInsert(x, -1):         -> [y, x, z]\n        # 4. SequenceAt(-1):                -> z\n        seq_construct_node = onnx.helper.make_node(\'SequenceConstruct\', [\'X\', \'Y\', \'Z\'], [\'seq_1\'])\n        seq_erase_node = onnx.helper.make_node(\'SequenceErase\', [\'seq_1\', \'pos_erase\'], [\'seq_2\'])\n        seq_insert_node = onnx.helper.make_node(\'SequenceInsert\', [\'seq_2\', \'X\', \'pos_insert\'], [\'seq_3\'])\n        seq_at_node = onnx.helper.make_node(\'SequenceAt\', [\'seq_3\', \'pos_at\'], [\'out\'])\n\n        tensor_shape = [2, 3, 4]\n\n        x = np.ones(tensor_shape, dtype=np.float32)\n        y = np.zeros(tensor_shape, dtype=np.float32)\n        z = np.ones(tensor_shape, dtype=np.float32) * 2\n        pos_erase_val = -3\n        pos_insert_val = -1\n        pos_at_val = -1\n        out = SequenceConstructImpl(x, y, z)\n        out = SequenceEraseImpl(out, pos_erase_val)\n        out = SequenceInsertImpl(out, x, pos_insert_val)\n        out = SequenceAtImpl(out, pos_at_val)\n        assert np.array_equal(out, z)\n\n        pos_erase = onnx.helper.make_tensor(\'pos_erase\', TensorProto.INT64, (), (pos_erase_val, ))\n        pos_insert = onnx.helper.make_tensor(\'pos_insert\', TensorProto.INT64, (), (pos_insert_val, ))\n        pos_at = onnx.helper.make_tensor(\'pos_at\', TensorProto.INT64, (), (pos_at_val, ))\n\n        graph = make_graph(\n            [seq_construct_node, seq_erase_node, seq_insert_node, seq_at_node],\n            [tensor_shape, tensor_shape, tensor_shape, [], [], []],  # type: ignore\n            [tensor_shape],  # type: ignore\n            [\'X\', \'Y\', \'Z\', \'pos_erase\', \'pos_insert\', \'pos_at\'],\n            [\'out\'],\n            [onnx.TensorProto.FLOAT] * 3 + [onnx.TensorProto.INT64] * 3,  # type: ignore\n            [onnx.TensorProto.FLOAT],\n            [pos_erase, pos_insert, pos_at])\n        model = onnx.helper.make_model(graph, producer_name=\'backend-test\')\n        expect(model, inputs=[x, y, z], outputs=[out], name=""test_sequence_model3"")\n\n        #4th testcase - concat\n        seq_construct_node = onnx.helper.make_node(\'SequenceConstruct\', [\'X\', \'Y\', \'Z\'], [\'seq_1\'])\n        seq_concat_node = onnx.helper.make_node(\'ConcatFromSequence\', [\'seq_1\'], [\'out\'], axis=1)\n\n        tensor_shape = [2, 3, 4]\n        concat_out_shape = [2, None, 4]\n\n        x = np.ones(tensor_shape, dtype=np.float32)\n        y = np.zeros(tensor_shape, dtype=np.float32)\n        z = np.ones(tensor_shape, dtype=np.float32) * 2\n        out = SequenceConstructImpl(x, y, z)\n        concat_out = ConcatFromSequenceImpl(out, 1)\n\n        graph = make_graph(\n            [seq_construct_node, seq_concat_node],\n            [tensor_shape] * 3,  # type: ignore\n            [concat_out_shape],  # type: ignore\n            [\'X\', \'Y\', \'Z\'],\n            [\'out\'],\n            [onnx.TensorProto.FLOAT] * 3,  # type: ignore\n            [onnx.TensorProto.FLOAT])\n        model = onnx.helper.make_model(graph, producer_name=\'backend-test\')\n        expect(model, inputs=[x, y, z], outputs=[concat_out], name=""test_sequence_model4"")\n\n        #5th testcase - concat with new_axis = 1\n        seq_construct_node = onnx.helper.make_node(\'SequenceConstruct\', [\'X\', \'Y\', \'Z\'], [\'seq_1\'])\n        seq_concat_node = onnx.helper.make_node(\'ConcatFromSequence\', [\'seq_1\'], [\'out\'], axis=-1, new_axis=1)\n\n        tensor_shape = [2, 3, 4]\n        concat_out_shape = [2, 3, 4, 3]\n\n        x = np.ones(tensor_shape, dtype=np.float32)\n        y = np.zeros(tensor_shape, dtype=np.float32)\n        z = np.ones(tensor_shape, dtype=np.float32) * 2\n        out = SequenceConstructImpl(x, y, z)\n        concat_out = ConcatFromSequenceImpl(out, -1, 1)\n\n        graph = make_graph(\n            [seq_construct_node, seq_concat_node],\n            [tensor_shape] * 3,  # type: ignore\n            [concat_out_shape],  # type: ignore\n            [\'X\', \'Y\', \'Z\'],\n            [\'out\'],\n            [onnx.TensorProto.FLOAT] * 3,  # type: ignore\n            [onnx.TensorProto.FLOAT],)\n        model = onnx.helper.make_model(graph, producer_name=\'backend-test\')\n        expect(model, inputs=[x, y, z], outputs=[concat_out], name=""test_sequence_model5"")\n\n        #6th testcase - split and len\n        seq_split_node = onnx.helper.make_node(\'SplitToSequence\', [\'X\'], [\'seq_1\'], axis=-1)\n        seq_len_node = onnx.helper.make_node(\'SequenceLength\', [\'seq_1\'], [\'len\'])\n\n        tensor_shape = [2, 3, 4]\n        len_shape = []  # type: ignore\n\n        x = np.ones(tensor_shape, dtype=np.float32)\n        out = SplitToSequenceImpl(x, axis=-1)\n        out = SequenceLengthImpl(out)\n        assert np.array_equal(out, np.int64(4))\n\n        graph = onnx.helper.make_graph(\n            nodes=[seq_split_node, seq_len_node],\n            name=\'Sequence\',\n            inputs=[\n                onnx.helper.make_tensor_value_info(\n                    \'X\',\n                    onnx.TensorProto.FLOAT,\n                    tensor_shape)],\n            outputs=[\n                onnx.helper.make_tensor_value_info(\n                    \'len\',\n                    onnx.TensorProto.INT64,\n                    len_shape)])  # type: ignore\n\n        model = onnx.helper.make_model(graph, producer_name=\'backend-test\')\n        expect(model, inputs=[x], outputs=[out], name=""test_sequence_model6"")\n\n        #7th testcase - split with keepdims=0, and SequenceAt\n        seq_split_node = onnx.helper.make_node(\'SplitToSequence\', [\'X\'], [\'seq_1\'], axis=0, keepdims=0)\n        seq_at_node = onnx.helper.make_node(\'SequenceAt\', [\'seq_1\', \'pos_at\'], [\'out\'])\n\n        tensor_shape = [2, 3, 4]\n        out_shape = [3, 4]\n\n        x = np.random.rand(*tensor_shape)\n        pos_at_val = 1\n        out = SplitToSequenceImpl(x, axis=0, keepdims=0)\n        out = SequenceAtImpl(out, pos_at_val)\n        assert np.array_equal(out, x[pos_at_val])\n\n        pos_at = onnx.helper.make_tensor(\'pos_at\', TensorProto.INT64, (), (pos_at_val, ))\n\n        graph = make_graph(\n            [seq_split_node, seq_at_node],\n            [tensor_shape, []],  # type: ignore\n            [out_shape],  # type: ignore\n            [\'X\', \'pos_at\'],\n            [\'out\'],\n            [onnx.TensorProto.DOUBLE, onnx.TensorProto.INT64],\n            [onnx.TensorProto.DOUBLE],\n            [pos_at])\n        model = onnx.helper.make_model(graph, producer_name=\'backend-test\')\n        expect(model, inputs=[x], outputs=[out], name=""test_sequence_model7"")\n\n        #8th testcase - split zero length\n        seq_split_node = onnx.helper.make_node(\'SplitToSequence\', [\'X\', \'Splits\'], [\'seq_1\'])\n        seq_len_node = onnx.helper.make_node(\'SequenceLength\', [\'seq_1\'], [\'len\'])\n\n        tensor_shape = [\'n\']  # type: ignore\n        splits_shape = [3]  # type: ignore\n\n        x = np.array([]).astype(np.float32)\n        splits = np.array([0, 0, 0]).astype(np.int64)\n        out_len = np.int64(3)\n\n        graph = onnx.helper.make_graph(\n            nodes=[seq_split_node, seq_len_node],\n            name=\'Sequence\',\n            inputs=[\n                onnx.helper.make_tensor_value_info(\n                    \'X\',\n                    onnx.TensorProto.FLOAT,\n                    tensor_shape),  # type: ignore\n                onnx.helper.make_tensor_value_info(\n                    \'Splits\',\n                    onnx.TensorProto.INT64,\n                    splits_shape)],  # type: ignore\n            outputs=[\n                onnx.helper.make_tensor_value_info(\n                    \'len\',\n                    onnx.TensorProto.INT64,\n                    len_shape)])  # type: ignore\n\n        model = onnx.helper.make_model(graph, producer_name=\'backend-test\')\n        expect(model, inputs=[x, splits], outputs=[out_len], name=""test_sequence_model8"")\n'"
onnx/backend/test/case/model/shrink.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass ShrinkTest(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n\n        node = onnx.helper.make_node(\n            'Shrink', ['x'], ['y'], lambd=1.5, bias=1.5,)\n        graph = onnx.helper.make_graph(\n            nodes=[node],\n            name='Shrink',\n            inputs=[onnx.helper.make_tensor_value_info(\n                'x', onnx.TensorProto.FLOAT, [5])],\n            outputs=[onnx.helper.make_tensor_value_info(\n                'y', onnx.TensorProto.FLOAT, [5])])\n        model = onnx.helper.make_model(graph,\n                                       producer_name='backend-test')\n\n        x = np.array([-2.0, -1.0, 0.0, 1.0, 2.0], dtype=np.float32)\n        y = np.array([-0.5, 0.0, 0.0, 0.0, 0.5], dtype=np.float32)\n\n        expect(model, inputs=[x], outputs=[y],\n               name='test_shrink')\n"""
onnx/backend/test/case/model/sign.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\nfrom typing import Sequence\n\n\nclass SingleSign(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Sign', ['x'], ['y'], name='test')\n\n        x = np.array([-1.0, 4.5, -4.5, 3.1, 0.0, 2.4, -5.5]).astype(np.float32)\n        y = np.array([-1.0, 1.0, -1.0, 1.0, 0.0, 1.0, -1.0]).astype(np.float32)\n\n        graph = onnx.helper.make_graph(\n            nodes=[node],\n            name='SingleSign',\n            inputs=[onnx.helper.make_tensor_value_info('x',\n                                                       onnx.TensorProto.FLOAT,\n                                                       [7])],\n            outputs=[onnx.helper.make_tensor_value_info('y',\n                                                        onnx.TensorProto.FLOAT,\n                                                        [7])])\n        model = onnx.helper.make_model(graph, producer_name='backend-test')\n        expect(model, inputs=[x], outputs=[y],\n               name='test_sign_model')\n"""
onnx/backend/test/case/model/single-relu.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass SingleRelu(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n\n        node = onnx.helper.make_node(\n            'Relu', ['x'], ['y'], name='test')\n        graph = onnx.helper.make_graph(\n            nodes=[node],\n            name='SingleRelu',\n            inputs=[onnx.helper.make_tensor_value_info(\n                'x', onnx.TensorProto.FLOAT, [1, 2])],\n            outputs=[onnx.helper.make_tensor_value_info(\n                'y', onnx.TensorProto.FLOAT, [1, 2])])\n        model = onnx.helper.make_model(graph, producer_name='backend-test')\n\n        x = np.random.randn(1, 2).astype(np.float32)\n        y = np.maximum(x, 0)\n\n        expect(model, inputs=[x], outputs=[y],\n               name='test_single_relu_model')\n"""
onnx/backend/test/case/model/stringnormalizer.py,0,"b'# coding: utf-8\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\nfrom typing import Sequence\n\n\nclass NormalizeStrings(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        def make_graph(node, input_shape, output_shape):  # type: (onnx.helper.NodeProto, Sequence[int], Sequence[int]) -> onnx.helper.GraphProto\n            graph = onnx.helper.make_graph(\n                nodes=[node],\n                name=\'StringNormalizer\',\n                inputs=[onnx.helper.make_tensor_value_info(\'x\',\n                                                            onnx.TensorProto.STRING,\n                                                            input_shape)],\n                outputs=[onnx.helper.make_tensor_value_info(\'y\',\n                                                             onnx.TensorProto.STRING,\n                                                             output_shape)])\n            return graph\n\n        #1st model_monday_casesensintive_nochangecase\n        stopwords = [u\'monday\']\n        node = onnx.helper.make_node(\n            \'StringNormalizer\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            is_case_sensitive=1,\n            stopwords=stopwords\n        )\n\n        x = np.array([u\'monday\', u\'tuesday\', u\'wednesday\', u\'thursday\']).astype(np.object)\n        y = np.array([u\'tuesday\', u\'wednesday\', u\'thursday\']).astype(np.object)\n\n        graph = make_graph(node, [4], [3])\n        model = onnx.helper.make_model(graph, producer_name=\'backend-test\')\n        expect(model, inputs=[x], outputs=[y], name=""test_strnorm_model_monday_casesensintive_nochangecase"")\n\n        #2nd model_nostopwords_nochangecase\n        node = onnx.helper.make_node(\n            \'StringNormalizer\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            is_case_sensitive=1\n        )\n\n        x = np.array([u\'monday\', u\'tuesday\']).astype(np.object)\n        y = x\n\n        graph = make_graph(node, [2], [2])\n        model = onnx.helper.make_model(graph, producer_name=\'backend-test\')\n        expect(model, inputs=[x], outputs=[y], name=""test_strnorm_model_nostopwords_nochangecase"")\n\n        # 3rd model_monday_casesensintive_lower\n        stopwords = [u\'monday\']\n        node = onnx.helper.make_node(\n            \'StringNormalizer\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            case_change_action=\'LOWER\',\n            is_case_sensitive=1,\n            stopwords=stopwords\n        )\n\n        x = np.array([u\'monday\', u\'tuesday\', u\'wednesday\', u\'thursday\']).astype(np.object)\n        y = np.array([u\'tuesday\', u\'wednesday\', u\'thursday\']).astype(np.object)\n\n        graph = make_graph(node, [4], [3])\n        model = onnx.helper.make_model(graph, producer_name=\'backend-test\')\n        expect(model, inputs=[x], outputs=[y], name=""test_strnorm_model_monday_casesensintive_lower"")\n\n        #4 model_monday_casesensintive_upper\n        stopwords = [u\'monday\']\n        node = onnx.helper.make_node(\n            \'StringNormalizer\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            case_change_action=\'UPPER\',\n            is_case_sensitive=1,\n            stopwords=stopwords\n        )\n\n        x = np.array([u\'monday\', u\'tuesday\', u\'wednesday\', u\'thursday\']).astype(np.object)\n        y = np.array([u\'TUESDAY\', u\'WEDNESDAY\', u\'THURSDAY\']).astype(np.object)\n\n        graph = make_graph(node, [4], [3])\n        model = onnx.helper.make_model(graph, producer_name=\'backend-test\')\n        expect(model, inputs=[x], outputs=[y], name=""test_strnorm_model_monday_casesensintive_upper"")\n\n        #5 monday_insensintive_upper_twodim\n        stopwords = [u\'monday\']\n        node = onnx.helper.make_node(\n            \'StringNormalizer\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            case_change_action=\'UPPER\',\n            stopwords=stopwords\n        )\n\n        input_shape = [1, 6]\n        output_shape = [1, 4]\n        x = np.array([u\'Monday\', u\'tuesday\', u\'wednesday\', u\'Monday\', u\'tuesday\', u\'wednesday\']).astype(np.object).reshape(input_shape)\n        y = np.array([u\'TUESDAY\', u\'WEDNESDAY\', u\'TUESDAY\', u\'WEDNESDAY\']).astype(np.object).reshape(output_shape)\n\n        graph = make_graph(node, input_shape, output_shape)\n        model = onnx.helper.make_model(graph, producer_name=\'backend-test\')\n        expect(model, inputs=[x], outputs=[y], name=""test_strnorm_model_monday_insensintive_upper_twodim"")\n\n        #6 monday_empty_output\n        stopwords = [u\'monday\']\n        node = onnx.helper.make_node(\n            \'StringNormalizer\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            case_change_action=\'UPPER\',\n            is_case_sensitive=0,\n            stopwords=stopwords\n        )\n\n        x = np.array([u\'monday\', u\'monday\']).astype(np.object)\n        y = np.array([u\'\']).astype(np.object)\n\n        graph = make_graph(node, [2], [1])\n        model = onnx.helper.make_model(graph, producer_name=\'backend-test\')\n        expect(model, inputs=[x], outputs=[y], name=""test_strnorm_model_monday_empty_output"")\n'"
onnx/backend/test/case/node/__init__.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport sys\nimport re\n\nfrom typing import List, Text, Sequence, Any\nimport numpy as np  # type: ignore\n\nimport onnx\nimport onnx.mapping\n\nfrom ..utils import import_recursive\nfrom ..test_case import TestCase\n\n\n_NodeTestCases = []\n\nfrom onnx.onnx_pb import NodeProto, AttributeProto\nfrom onnx.onnx_operators_pb import FunctionProto\n\n\n# FIXME(TMVector): Any reason we can\'t get rid of this and use the C++ helper directly?\ndef function_expand_helper(node,  # type: NodeProto\n                           function_proto,  # type: FunctionProto\n                           op_prefix  # type:  Text\n                           ):  # type:  (...) -> List[NodeProto]\n    node_list = []\n    io_names_map = dict()\n    attribute_map = dict((a.name, a) for a in node.attribute)\n\n    for idx in range(len(function_proto.input)):\n        io_names_map[function_proto.input[idx]] = node.input[idx] \\\n            if idx in range(len(node.input)) else """"\n\n    for idx in range(len(function_proto.output)):\n        # Even if the node has been created with optional outputs missing, we\n        # can\'t assume that the function body handles this correctly, such as in\n        # the case that output is also an intermediate value.\n        # So we only add a name mapping if the output is present. An internal\n        # name will be generated if the missing output is used, the same as any\n        # other internal tensor.\n        if idx in range(len(node.output)) and node.output[idx] != """":\n            io_names_map[function_proto.output[idx]] = node.output[idx]\n\n    for internal_node in function_proto.node:\n        new_node = NodeProto()\n        new_node.CopyFrom(internal_node)\n        new_node.ClearField(""input"")\n        new_node.ClearField(""output"")\n        new_node.ClearField(""attribute"")\n        for internal_name in internal_node.input:\n            if internal_name in io_names_map:\n                new_node.input.append(io_names_map[internal_name])\n            else:\n                new_node.input.append(op_prefix + internal_name)\n        for internal_name in internal_node.output:\n            if internal_name in io_names_map:\n                new_node.output.append(io_names_map[internal_name])\n            else:\n                new_node.output.append(op_prefix + internal_name)\n        for attr in internal_node.attribute:\n            if attr.HasField(""ref_attr_name""):\n                if attr.ref_attr_name in attribute_map:\n                    new_attr = AttributeProto()\n                    new_attr.CopyFrom(attribute_map[attr.ref_attr_name])  # type: ignore\n                    new_attr.name = attr.name\n                    new_node.attribute.extend([new_attr])\n            else:\n                new_attr = AttributeProto()\n                new_attr.CopyFrom(attr)\n                new_node.attribute.extend([new_attr])\n        node_list.append(new_node)\n    return node_list\n\n\ndef function_testcase_helper(node, name):  # type: (NodeProto, Text) -> List[NodeProto]\n    test_op = node.op_type\n    op_prefix = test_op + ""_"" + name + ""_expanded_function""\n    schema = onnx.defs.get_schema(test_op, node.domain)\n\n    if schema.has_function:    # type: ignore\n        function_proto = schema.function_body  # type: ignore\n    elif schema.has_context_dependent_function:    # type: ignore\n        function_proto_str = schema.get_context_dependent_function(node.SerializeToString())  # type: ignore\n        function_proto = FunctionProto()\n        function_proto.ParseFromString(function_proto_str)\n    else:\n        return []\n\n    for attr in schema.attributes:\n        if attr in [a.name for a in node.attribute]:\n            continue\n        if schema.attributes[attr].default_value:\n            node.attribute.extend([schema.attributes[attr].default_value])\n\n    # function_proto.attributes\n    node_list = function_expand_helper(node, function_proto, op_prefix)\n    return node_list\n\n\ndef _extract_value_info(arr, name, ele_type=None):  # type: (np.ndarray, Text, np.dtype) -> onnx.ValueInfoProto\n    return onnx.helper.make_tensor_value_info(\n        name=name,\n        elem_type=ele_type if ele_type else onnx.mapping.NP_TYPE_TO_TENSOR_TYPE[arr.dtype],\n        shape=arr.shape)\n\n\ndef expect(node,  # type: onnx.NodeProto\n           inputs,  # type: Sequence[np.ndarray]\n           outputs,  # type: Sequence[np.ndarray]\n           name,  # type: Text\n           **kwargs  # type: Any\n           ):  # type: (...) -> None\n    present_inputs = [x for x in node.input if (x != \'\')]\n    present_outputs = [x for x in node.output if (x != \'\')]\n    input_types = [None] * len(inputs)\n    if \'input_types\' in kwargs:\n        input_types = kwargs[str(\'input_types\')]\n        del kwargs[str(\'input_types\')]\n    output_types = [None] * len(outputs)\n    if \'output_types\' in kwargs:\n        output_types = kwargs[str(\'output_types\')]\n        del kwargs[str(\'output_types\')]\n    inputs_vi = [_extract_value_info(arr, arr_name, input_type)\n                 for arr, arr_name, input_type in zip(inputs, present_inputs, input_types)]\n    outputs_vi = [_extract_value_info(arr, arr_name, output_type)\n                  for arr, arr_name, output_type in zip(outputs, present_outputs, output_types)]\n    graph = onnx.helper.make_graph(\n        nodes=[node],\n        name=name,\n        inputs=inputs_vi,\n        outputs=outputs_vi)\n    kwargs[str(\'producer_name\')] = \'backend-test\'\n    model = onnx.helper.make_model(graph, **kwargs)\n\n    _NodeTestCases.append(TestCase(\n        name=name,\n        model_name=name,\n        url=None,\n        model_dir=None,\n        model=model,\n        data_sets=[(inputs, outputs)],\n        kind=\'node\',\n        rtol=1e-3,\n        atol=1e-7,\n    ))\n\n    expanded_function_nodes = function_testcase_helper(node, name)\n    if expanded_function_nodes:\n        function_test_name = name + \'_expanded\'\n        graph = onnx.helper.make_graph(\n            nodes=expanded_function_nodes,\n            name=function_test_name,\n            inputs=inputs_vi,\n            outputs=outputs_vi)\n        kwargs[str(\'producer_name\')] = \'backend-test\'\n        model = onnx.helper.make_model(graph, **kwargs)\n        _NodeTestCases.append(TestCase(\n            name=function_test_name,\n            model_name=function_test_name,\n            url=None,\n            model_dir=None,\n            model=model,\n            data_sets=[(inputs, outputs)],\n            kind=\'node\',\n            rtol=1e-3,\n            atol=1e-7,\n        ))\n\n\ndef collect_testcases():  # type: () -> List[TestCase]\n    \'\'\'Collect node test cases defined in python/numpy code.\n    \'\'\'\n    import_recursive(sys.modules[__name__])\n    return _NodeTestCases\n'"
onnx/backend/test/case/node/abs.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\nfrom onnx.backend.sample.ops.abs import abs\n\n\nclass Abs(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Abs',\n            inputs=['x'],\n            outputs=['y'],\n        )\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = abs(x)\n\n        expect(node, inputs=[x], outputs=[y],\n               name='test_abs')\n"""
onnx/backend/test/case/node/acos.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Acos(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Acos',\n            inputs=['x'],\n            outputs=['y'],\n        )\n\n        x = np.array([-0.5, 0, 0.5]).astype(np.float32)\n        y = np.arccos(x)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_acos_example')\n\n        x = np.random.rand(3, 4, 5).astype(np.float32)\n        y = np.arccos(x)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_acos')\n"""
onnx/backend/test/case/node/acosh.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Acosh(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Acosh',\n            inputs=['x'],\n            outputs=['y'],\n        )\n\n        x = np.array([10, np.e, 1]).astype(np.float32)\n        y = np.arccosh(x)  # expected output [2.99322295,  1.65745449,  0.]\n        expect(node, inputs=[x], outputs=[y],\n               name='test_acosh_example')\n\n        x = np.random.uniform(1.0, 10.0, (3, 4, 5)).astype(np.float32)\n        y = np.arccosh(x)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_acosh')\n"""
onnx/backend/test/case/node/adagrad.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom onnx.defs import ONNX_DOMAIN, AI_ONNX_PREVIEW_TRAINING_DOMAIN\nfrom ..base import Base\nfrom . import expect\n\n\ndef apply_adagrad(r, t, x, g, h, norm_coefficient, epsilon, decay_factor):  # type: ignore\n    # Compute adjusted learning-rate.\n    r_ = r / (1 + t * decay_factor)\n    # Add gradient of regularization term.\n    g_regularized = norm_coefficient * x + g\n    # Update squared accumulated gradient.\n    h_new = h + g_regularized * g_regularized\n    # Compute ADAGRAD's gradient scaling factors\n    h_sqrt = np.sqrt(h_new) + epsilon\n    # Apply ADAGRAD update rule.\n    x_new = x - r_ * g_regularized / h_sqrt\n    return (x_new, h_new)\n\n\nclass Adagrad(Base):\n\n    @staticmethod\n    def export_adagrad():  # type: () -> None\n        # Define operator attributes.\n        norm_coefficient = 0.001\n        epsilon = 1e-5\n        decay_factor = 0.1\n\n        # Create operator.\n        node = onnx.helper.make_node('Adagrad',\n                                     inputs=['R', 'T', 'X', 'G', 'H'],\n                                     outputs=['X_new', 'H_new'],\n                                     norm_coefficient=norm_coefficient,\n                                     epsilon=epsilon,\n                                     decay_factor=decay_factor,\n                                     domain=AI_ONNX_PREVIEW_TRAINING_DOMAIN\n                                     )\n\n        # Define operator inputs.\n        r = np.array(0.1, dtype=np.float32)  # scalar\n        t = np.array(0, dtype=np.int64)  # scalar\n        x = np.array([1.0], dtype=np.float32)\n        g = np.array([-1.0], dtype=np.float32)\n        h = np.array([2.0], dtype=np.float32)\n\n        # Compute expected outputs of Adagrad.\n        x_new, h_new = apply_adagrad(r, t, x, g, h,\n                                     norm_coefficient, epsilon, decay_factor)\n\n        # Check results.\n        expect(node, inputs=[r, t, x, g, h],\n               outputs=[x_new, h_new], name='test_adagrad',\n               opset_imports=[onnx.helper.make_opsetid(AI_ONNX_PREVIEW_TRAINING_DOMAIN, 1)])\n\n    @staticmethod\n    def export_adagrad_multiple():  # type: () -> None\n        # Define operator attributes.\n        norm_coefficient = 0.001\n        epsilon = 1e-5\n        decay_factor = 0.1\n\n        node = onnx.helper.make_node('Adagrad',\n                                     inputs=['R', 'T', 'X1', 'X2',\n                                             'G1', 'G2', 'H1', 'H2'],\n                                     outputs=['X1_new', 'X2_new',\n                                              'H1_new', 'H2_new'],\n                                     norm_coefficient=norm_coefficient,\n                                     epsilon=epsilon,\n                                     decay_factor=decay_factor,\n                                     domain=AI_ONNX_PREVIEW_TRAINING_DOMAIN\n                                     )\n\n        # Define operator inputs.\n        r = np.array(0.1, dtype=np.float32)  # scalar\n        t = np.array(0, dtype=np.int64)  # scalar\n\n        x1 = np.array([1.0], dtype=np.float32)\n        g1 = np.array([-1.0], dtype=np.float32)\n        h1 = np.array([2.0], dtype=np.float32)\n\n        x2 = np.array([1.0, 2.0], dtype=np.float32)\n        g2 = np.array([-1.0, -3.0], dtype=np.float32)\n        h2 = np.array([4.0, 1.0], dtype=np.float32)\n\n        # Compute expected outputs of Adagrad.\n        x1_new, h1_new = apply_adagrad(r, t, x1, g1, h1,\n                                       norm_coefficient, epsilon, decay_factor)\n        x2_new, h2_new = apply_adagrad(r, t, x2, g2, h2,\n                                       norm_coefficient, epsilon, decay_factor)\n\n        # Check results.\n        expect(node, inputs=[r, t, x1, x2, g1, g2, h1, h2],\n               outputs=[x1_new, x2_new, h1_new, h2_new], name='test_adagrad_multiple',\n               opset_imports=[onnx.helper.make_opsetid(AI_ONNX_PREVIEW_TRAINING_DOMAIN, 1)])\n"""
onnx/backend/test/case/node/adam.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom onnx.defs import ONNX_DOMAIN, AI_ONNX_PREVIEW_TRAINING_DOMAIN\nfrom ..base import Base\nfrom . import expect\n\n\ndef apply_adam(r, t, x, g, v, h, norm_coefficient, norm_coefficient_post, alpha, beta, epsilon):  # type: ignore\n    # Add gradient of regularization term.\n    g_regularized = norm_coefficient * x + g\n    # Update momentum.\n    v_new = alpha * v + (1 - alpha) * g_regularized\n    # Update second-order momentum.\n    h_new = beta * h + (1 - beta) * (g_regularized * g_regularized)\n    # Compute element-wise square root.\n    h_sqrt = np.sqrt(h_new) + epsilon\n    # Adjust learning rate.\n    r_adjusted = None\n    if t > 0:\n        # Consider bias correction on momentums.\n        r_adjusted = r * np.sqrt(1 - beta**t) / (1 - alpha**t)\n    else:\n        # No bias correction on momentums.\n        r_adjusted = r\n    # Apply Adam update rule.\n    x_new = x - r_adjusted * (v_new / h_sqrt)\n    # It's possible to apply regularization in the end.\n    x_final = (1 - norm_coefficient_post) * x_new\n    return x_final, v_new, h_new\n\n\nclass Adam(Base):\n\n    @staticmethod\n    def export_adam():  # type: () -> None\n        # Define operator attributes.\n        norm_coefficient = 0.001\n        alpha = 0.95\n        beta = 0.1\n        epsilon = 1e-7\n\n        # Create operator.\n        node = onnx.helper.make_node('Adam',\n                                     inputs=['R', 'T', 'X', 'G', 'V', 'H'],\n                                     outputs=['X_new', 'V_new', 'H_new'],\n                                     norm_coefficient=norm_coefficient,\n                                     alpha=alpha,\n                                     beta=beta,\n                                     epsilon=epsilon,\n                                     domain=AI_ONNX_PREVIEW_TRAINING_DOMAIN\n                                     )\n\n        # Define operator inputs.\n        r = np.array(0.1, dtype=np.float32)  # scalar\n        t = np.array(0, dtype=np.int64)  # scalar\n        x = np.array([1.2, 2.8], dtype=np.float32)\n        g = np.array([-0.94, -2.5], dtype=np.float32)\n        v = np.array([1.7, 3.6], dtype=np.float32)\n        h = np.array([0.1, 0.1], dtype=np.float32)\n\n        # Compute expected outputs of Adam.\n        x_new, v_new, h_new = apply_adam(r, t, x, g, v, h,\n                                         norm_coefficient, 0.0, alpha, beta,\n                                         epsilon)\n\n        # Check results.\n        expect(node, inputs=[r, t, x, g, v, h],\n               outputs=[x_new, v_new, h_new], name='test_adam',\n               opset_imports=[onnx.helper.make_opsetid(AI_ONNX_PREVIEW_TRAINING_DOMAIN, 1)])\n\n    @staticmethod\n    def export_adam_multiple():  # type: () -> None\n        # Define operator attributes.\n        norm_coefficient = 0.001\n        alpha = 0.95\n        beta = 0.85\n        epsilon = 1e-2\n\n        node = onnx.helper.make_node('Adam',\n                                     inputs=['R', 'T', 'X1', 'X2',\n                                             'G1', 'G2', 'V1', 'V2',\n                                             'H1', 'H2'],\n                                     outputs=['X1_new', 'X2_new',\n                                              'V1_new', 'V2_new',\n                                              'H1_new', 'H2_new'],\n                                     norm_coefficient=norm_coefficient,\n                                     alpha=alpha,\n                                     beta=beta,\n                                     domain=AI_ONNX_PREVIEW_TRAINING_DOMAIN\n                                     )\n\n        # Define operator inputs.\n        r = np.array(0.1, dtype=np.float32)  # scalar\n        t = np.array(0, dtype=np.int64)  # scalar\n\n        x1 = np.array([1.0], dtype=np.float32)\n        g1 = np.array([-1.0], dtype=np.float32)\n        v1 = np.array([2.0], dtype=np.float32)\n        h1 = np.array([0.5], dtype=np.float32)\n\n        x2 = np.array([1.0, 2.0], dtype=np.float32)\n        g2 = np.array([-1.0, -3.0], dtype=np.float32)\n        v2 = np.array([4.0, 1.0], dtype=np.float32)\n        h2 = np.array([1.0, 10.0], dtype=np.float32)\n\n        # Compute expected outputs of Adam.\n        x1_new, v1_new, h1_new = apply_adam(r, t, x1, g1, v1, h1,\n                                    norm_coefficient, 0.0, alpha, beta,\n                                    epsilon)\n        x2_new, v2_new, h2_new = apply_adam(r, t, x2, g2, v2, h2,\n                                    norm_coefficient, 0.0, alpha, beta,\n                                    epsilon)\n\n        # Check results.\n        expect(node, inputs=[r, t, x1, x2, g1, g2, v1, v2, h1, h2],\n               outputs=[x1_new, x2_new, v1_new, v2_new, h1_new, h2_new],\n               name='test_adam_multiple',\n               opset_imports=[onnx.helper.make_opsetid(AI_ONNX_PREVIEW_TRAINING_DOMAIN, 1)])\n"""
onnx/backend/test/case/node/add.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Add(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Add',\n            inputs=['x', 'y'],\n            outputs=['sum'],\n        )\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.random.randn(3, 4, 5).astype(np.float32)\n        expect(node, inputs=[x, y], outputs=[x + y],\n               name='test_add')\n\n    @staticmethod\n    def export_add_broadcast():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Add',\n            inputs=['x', 'y'],\n            outputs=['sum'],\n        )\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.random.randn(5).astype(np.float32)\n        expect(node, inputs=[x, y], outputs=[x + y],\n               name='test_add_bcast')\n"""
onnx/backend/test/case/node/and.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass And(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'And',\n            inputs=['x', 'y'],\n            outputs=['and'],\n        )\n\n        # 2d\n        x = (np.random.randn(3, 4) > 0).astype(np.bool)\n        y = (np.random.randn(3, 4) > 0).astype(np.bool)\n        z = np.logical_and(x, y)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_and2d')\n\n        # 3d\n        x = (np.random.randn(3, 4, 5) > 0).astype(np.bool)\n        y = (np.random.randn(3, 4, 5) > 0).astype(np.bool)\n        z = np.logical_and(x, y)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_and3d')\n\n        # 4d\n        x = (np.random.randn(3, 4, 5, 6) > 0).astype(np.bool)\n        y = (np.random.randn(3, 4, 5, 6) > 0).astype(np.bool)\n        z = np.logical_and(x, y)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_and4d')\n\n    @staticmethod\n    def export_and_broadcast():  # type: () -> None\n        node = onnx.helper.make_node(\n            'And',\n            inputs=['x', 'y'],\n            outputs=['and'],\n        )\n\n        # 3d vs 1d\n        x = (np.random.randn(3, 4, 5) > 0).astype(np.bool)\n        y = (np.random.randn(5) > 0).astype(np.bool)\n        z = np.logical_and(x, y)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_and_bcast3v1d')\n\n        # 3d vs 2d\n        x = (np.random.randn(3, 4, 5) > 0).astype(np.bool)\n        y = (np.random.randn(4, 5) > 0).astype(np.bool)\n        z = np.logical_and(x, y)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_and_bcast3v2d')\n\n        # 4d vs 2d\n        x = (np.random.randn(3, 4, 5, 6) > 0).astype(np.bool)\n        y = (np.random.randn(5, 6) > 0).astype(np.bool)\n        z = np.logical_and(x, y)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_and_bcast4v2d')\n\n        # 4d vs 3d\n        x = (np.random.randn(3, 4, 5, 6) > 0).astype(np.bool)\n        y = (np.random.randn(4, 5, 6) > 0).astype(np.bool)\n        z = np.logical_and(x, y)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_and_bcast4v3d')\n\n        # 4d vs 4d\n        x = (np.random.randn(1, 4, 1, 6) > 0).astype(np.bool)\n        y = (np.random.randn(3, 1, 5, 6) > 0).astype(np.bool)\n        z = np.logical_and(x, y)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_and_bcast4v4d')\n"""
onnx/backend/test/case/node/argmax.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\ndef argmax_use_numpy(data, axis=0, keepdims=1):  # type: (np.ndarray, int, int) -> (np.ndarray)\n    result = np.argmax(data, axis=axis)\n    if (keepdims == 1):\n        result = np.expand_dims(result, axis)\n    return result.astype(np.int64)\n\n\ndef argmax_use_numpy_select_last_index(data, axis=0, keepdims=True):  # type: (np.ndarray, int, int) -> (np.ndarray)\n    data = np.flip(data, axis)\n    result = np.argmax(data, axis=axis)\n    result = data.shape[axis] - result - 1\n    if keepdims:\n        result = np.expand_dims(result, axis)\n    return result.astype(np.int64)\n\n\nclass ArgMax(Base):\n\n    @staticmethod\n    def export_no_keepdims():  # type: () -> None\n        data = np.array([[2, 1], [3, 10]], dtype=np.float32)\n        axis = 1\n        keepdims = 0\n        node = onnx.helper.make_node(\n            'ArgMax',\n            inputs=['data'],\n            outputs=['result'],\n            axis=axis,\n            keepdims=keepdims)\n        # result: [[0, 1]]\n        result = argmax_use_numpy(data, axis=axis, keepdims=keepdims)\n        expect(node, inputs=[data], outputs=[result], name='test_argmax_no_keepdims_example')\n\n        data = np.random.uniform(-10, 10, [2, 3, 4]).astype(np.float32)\n        # result's shape: [2, 4]\n        result = argmax_use_numpy(data, axis=axis, keepdims=keepdims)\n        expect(node, inputs=[data], outputs=[result], name='test_argmax_no_keepdims_random')\n\n    @staticmethod\n    def export_keepdims():  # type: () -> None\n        data = np.array([[2, 1], [3, 10]], dtype=np.float32)\n        axis = 1\n        keepdims = 1\n        node = onnx.helper.make_node(\n            'ArgMax',\n            inputs=['data'],\n            outputs=['result'],\n            axis=axis,\n            keepdims=keepdims)\n        # result: [[0], [1]]\n        result = argmax_use_numpy(data, axis=axis, keepdims=keepdims)\n        expect(node, inputs=[data], outputs=[result], name='test_argmax_keepdims_example')\n\n        data = np.random.uniform(-10, 10, [2, 3, 4]).astype(np.float32)\n        # result's shape: [2, 1, 4]\n        result = argmax_use_numpy(data, axis=axis, keepdims=keepdims)\n        expect(node, inputs=[data], outputs=[result], name='test_argmax_keepdims_random')\n\n    @staticmethod\n    def export_default_axes_keepdims():  # type: () -> None\n        data = np.array([[2, 1], [3, 10]], dtype=np.float32)\n        keepdims = 1\n        node = onnx.helper.make_node(\n            'ArgMax',\n            inputs=['data'],\n            outputs=['result'],\n            keepdims=keepdims)\n\n        # result: [[1], [1]]\n        result = argmax_use_numpy(data, keepdims=keepdims)\n        expect(node, inputs=[data], outputs=[result], name='test_argmax_default_axis_example')\n\n        data = np.random.uniform(-10, 10, [2, 3, 4]).astype(np.float32)\n        # result's shape: [1, 3, 4]\n        result = argmax_use_numpy(data, keepdims=keepdims)\n        expect(node, inputs=[data], outputs=[result], name='test_argmax_default_axis_random')\n\n    @staticmethod\n    def export_negative_axis_keepdims():  # type: () -> None\n        data = np.array([[2, 1], [3, 10]], dtype=np.float32)\n        axis = -1\n        keepdims = 1\n        node = onnx.helper.make_node(\n            'ArgMax',\n            inputs=['data'],\n            outputs=['result'],\n            axis=axis,\n            keepdims=keepdims)\n        # result: [[0], [1]]\n        result = argmax_use_numpy(data, axis=axis, keepdims=keepdims)\n        expect(node, inputs=[data], outputs=[result], name='test_argmax_negative_axis_keepdims_example')\n\n        data = np.random.uniform(-10, 10, [2, 3, 4]).astype(np.float32)\n        # result's shape: [2, 3, 1]\n        result = argmax_use_numpy(data, axis=axis, keepdims=keepdims)\n        expect(node, inputs=[data], outputs=[result], name='test_argmax_negative_axis_keepdims_random')\n\n    @staticmethod\n    def export_no_keepdims_select_last_index():  # type: () -> None\n        data = np.array([[2, 2], [3, 10]], dtype=np.float32)\n        axis = 1\n        keepdims = 0\n        node = onnx.helper.make_node(\n            'ArgMax',\n            inputs=['data'],\n            outputs=['result'],\n            axis=axis,\n            keepdims=keepdims,\n            select_last_index=True)\n        # result: [[1, 1]]\n        result = argmax_use_numpy_select_last_index(data, axis=axis, keepdims=keepdims)\n        expect(node, inputs=[data], outputs=[result], name='test_argmax_no_keepdims_example_select_last_index')\n\n        data = np.random.uniform(-10, 10, [2, 3, 4]).astype(np.float32)\n        # result's shape: [2, 4]\n        result = argmax_use_numpy_select_last_index(data, axis=axis, keepdims=keepdims)\n        expect(node, inputs=[data], outputs=[result], name='test_argmax_no_keepdims_random_select_last_index')\n\n    @staticmethod\n    def export_keepdims_select_last_index():  # type: () -> None\n        data = np.array([[2, 2], [3, 10]], dtype=np.float32)\n        axis = 1\n        keepdims = 1\n        node = onnx.helper.make_node(\n            'ArgMax',\n            inputs=['data'],\n            outputs=['result'],\n            axis=axis,\n            keepdims=keepdims,\n            select_last_index=True)\n        # result: [[1], [1]]\n        result = argmax_use_numpy_select_last_index(data, axis=axis, keepdims=keepdims)\n        expect(node, inputs=[data], outputs=[result], name='test_argmax_keepdims_example_select_last_index')\n\n        data = np.random.uniform(-10, 10, [2, 3, 4]).astype(np.float32)\n        # result's shape: [2, 1, 4]\n        result = argmax_use_numpy_select_last_index(data, axis=axis, keepdims=keepdims)\n        expect(node, inputs=[data], outputs=[result], name='test_argmax_keepdims_random_select_last_index')\n\n    @staticmethod\n    def export_default_axes_keepdims_select_last_index():  # type: () -> None\n        data = np.array([[2, 2], [3, 10]], dtype=np.float32)\n        keepdims = 1\n        node = onnx.helper.make_node(\n            'ArgMax',\n            inputs=['data'],\n            outputs=['result'],\n            keepdims=keepdims,\n            select_last_index=True)\n\n        # result: [[1, 1]]\n        result = argmax_use_numpy_select_last_index(data, keepdims=keepdims)\n        expect(node, inputs=[data], outputs=[result], name='test_argmax_default_axis_example_select_last_index')\n\n        data = np.random.uniform(-10, 10, [2, 3, 4]).astype(np.float32)\n        # result's shape: [1, 3, 4]\n        result = argmax_use_numpy_select_last_index(data, keepdims=keepdims)\n        expect(node, inputs=[data], outputs=[result], name='test_argmax_default_axis_random_select_last_index')\n\n    @staticmethod\n    def export_negative_axis_keepdims_select_last_index():  # type: () -> None\n        data = np.array([[2, 2], [3, 10]], dtype=np.float32)\n        axis = -1\n        keepdims = 1\n        node = onnx.helper.make_node(\n            'ArgMax',\n            inputs=['data'],\n            outputs=['result'],\n            axis=axis,\n            keepdims=keepdims,\n            select_last_index=True)\n        # result: [[1], [1]]\n        result = argmax_use_numpy_select_last_index(data, axis=axis, keepdims=keepdims)\n        expect(node, inputs=[data], outputs=[result], name='test_argmax_negative_axis_keepdims_example_select_last_index')\n\n        data = np.random.uniform(-10, 10, [2, 3, 4]).astype(np.float32)\n        # result's shape: [2, 3, 1]\n        result = argmax_use_numpy_select_last_index(data, axis=axis, keepdims=keepdims)\n        expect(node, inputs=[data], outputs=[result], name='test_argmax_negative_axis_keepdims_random_select_last_index')\n"""
onnx/backend/test/case/node/argmin.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\ndef argmin_use_numpy(data, axis=0, keepdims=1):  # type: (np.ndarray, int, int) -> (np.ndarray)\n    result = np.argmin(data, axis=axis)\n    if (keepdims == 1):\n        result = np.expand_dims(result, axis)\n    return result.astype(np.int64)\n\n\ndef argmin_use_numpy_select_last_index(data, axis=0, keepdims=True):  # type: (np.ndarray, int, int) -> (np.ndarray)\n    data = np.flip(data, axis)\n    result = np.argmin(data, axis=axis)\n    result = data.shape[axis] - result - 1\n    if keepdims:\n        result = np.expand_dims(result, axis)\n    return result.astype(np.int64)\n\n\nclass ArgMin(Base):\n\n    @staticmethod\n    def export_no_keepdims():  # type: () -> None\n        data = np.array([[2, 1], [3, 10]], dtype=np.float32)\n        axis = 1\n        keepdims = 0\n        node = onnx.helper.make_node(\n            'ArgMin',\n            inputs=['data'],\n            outputs=['result'],\n            axis=axis,\n            keepdims=keepdims)\n        # The content of result is : [[1, 0]]\n        result = argmin_use_numpy(data, axis=axis, keepdims=keepdims)\n        expect(node, inputs=[data], outputs=[result], name='test_argmin_no_keepdims_example')\n\n        data = np.random.uniform(-10, 10, [2, 3, 4]).astype(np.float32)\n        # result's shape: [2, 4]\n        result = argmin_use_numpy(data, axis=axis, keepdims=keepdims)\n        expect(node, inputs=[data], outputs=[result], name='test_argmin_no_keepdims_random')\n\n    @staticmethod\n    def export_keepdims():  # type: () -> None\n        data = np.array([[2, 1], [3, 10]], dtype=np.float32)\n        axis = 1\n        keepdims = 1\n        node = onnx.helper.make_node(\n            'ArgMin',\n            inputs=['data'],\n            outputs=['result'],\n            axis=axis,\n            keepdims=keepdims)\n        # The content of result is : [[1], [0]]\n        result = argmin_use_numpy(data, axis=axis, keepdims=keepdims)\n        expect(node, inputs=[data], outputs=[result], name='test_argmin_keepdims_example')\n\n        data = np.random.uniform(-10, 10, [2, 3, 4]).astype(np.float32)\n        # result's shape: [2, 1, 4]\n        result = argmin_use_numpy(data, axis=axis, keepdims=keepdims)\n        expect(node, inputs=[data], outputs=[result], name='test_argmin_keepdims_random')\n\n    @staticmethod\n    def export_default_axes_keepdims():  # type: () -> None\n        data = np.array([[2, 1], [3, 10]], dtype=np.float32)\n        keepdims = 1\n        node = onnx.helper.make_node(\n            'ArgMin',\n            inputs=['data'],\n            outputs=['result'],\n            keepdims=keepdims)\n\n        # The content of result is : [[0], [0]]\n        result = argmin_use_numpy(data, keepdims=keepdims)\n        expect(node, inputs=[data], outputs=[result], name='test_argmin_default_axis_example')\n\n        data = np.random.uniform(-10, 10, [2, 3, 4]).astype(np.float32)\n        # result's shape: [1, 3, 4]\n        result = argmin_use_numpy(data, keepdims=keepdims)\n        expect(node, inputs=[data], outputs=[result], name='test_argmin_default_axis_random')\n\n    @staticmethod\n    def export_negative_axis_keepdims():  # type: () -> None\n        data = np.array([[2, 1], [3, 10]], dtype=np.float32)\n        axis = -1\n        keepdims = 1\n        node = onnx.helper.make_node(\n            'ArgMin',\n            inputs=['data'],\n            outputs=['result'],\n            axis=axis,\n            keepdims=keepdims)\n        # The content of result is : [[1], [0]]\n        result = argmin_use_numpy(data, axis=axis, keepdims=keepdims)\n        expect(node, inputs=[data], outputs=[result], name='test_argmin_negative_axis_keepdims_example')\n\n        data = np.random.uniform(-10, 10, [2, 3, 4]).astype(np.float32)\n        # result's shape: [2, 3, 1]\n        result = argmin_use_numpy(data, axis=axis, keepdims=keepdims)\n        expect(node, inputs=[data], outputs=[result], name='test_argmin_negative_axis_keepdims_random')\n\n    @staticmethod\n    def export_no_keepdims_select_last_index():  # type: () -> None\n        data = np.array([[2, 2], [3, 10]], dtype=np.float32)\n        axis = 1\n        keepdims = 0\n        node = onnx.helper.make_node(\n            'ArgMin',\n            inputs=['data'],\n            outputs=['result'],\n            axis=axis,\n            keepdims=keepdims,\n            select_last_index=True)\n        # result: [[1, 0]]\n        result = argmin_use_numpy_select_last_index(data, axis=axis, keepdims=keepdims)\n        expect(node, inputs=[data], outputs=[result], name='test_argmin_no_keepdims_example_select_last_index')\n\n        data = np.random.uniform(-10, 10, [2, 3, 4]).astype(np.float32)\n        # result's shape: [2, 4]\n        result = argmin_use_numpy_select_last_index(data, axis=axis, keepdims=keepdims)\n        expect(node, inputs=[data], outputs=[result], name='test_argmin_no_keepdims_random_select_last_index')\n\n    @staticmethod\n    def export_keepdims_select_last_index():  # type: () -> None\n        data = np.array([[2, 2], [3, 10]], dtype=np.float32)\n        axis = 1\n        keepdims = 1\n        node = onnx.helper.make_node(\n            'ArgMin',\n            inputs=['data'],\n            outputs=['result'],\n            axis=axis,\n            keepdims=keepdims,\n            select_last_index=True)\n        # result: [[1], [0]]\n        result = argmin_use_numpy_select_last_index(data, axis=axis, keepdims=keepdims)\n        expect(node, inputs=[data], outputs=[result], name='test_argmin_keepdims_example_select_last_index')\n\n        data = np.random.uniform(-10, 10, [2, 3, 4]).astype(np.float32)\n        # result's shape: [2, 1, 4]\n        result = argmin_use_numpy_select_last_index(data, axis=axis, keepdims=keepdims)\n        expect(node, inputs=[data], outputs=[result], name='test_argmin_keepdims_random_select_last_index')\n\n    @staticmethod\n    def export_default_axes_keepdims_select_last_index():  # type: () -> None\n        data = np.array([[2, 2], [3, 10]], dtype=np.float32)\n        keepdims = 1\n        node = onnx.helper.make_node(\n            'ArgMin',\n            inputs=['data'],\n            outputs=['result'],\n            keepdims=keepdims,\n            select_last_index=True)\n\n        # result: [[0, 0]]\n        result = argmin_use_numpy_select_last_index(data, keepdims=keepdims)\n        expect(node, inputs=[data], outputs=[result], name='test_argmin_default_axis_example_select_last_index')\n\n        data = np.random.uniform(-10, 10, [2, 3, 4]).astype(np.float32)\n        # result's shape: [1, 3, 4]\n        result = argmin_use_numpy_select_last_index(data, keepdims=keepdims)\n        expect(node, inputs=[data], outputs=[result], name='test_argmin_default_axis_random_select_last_index')\n\n    @staticmethod\n    def export_negative_axis_keepdims_select_last_index():  # type: () -> None\n        data = np.array([[2, 2], [3, 10]], dtype=np.float32)\n        axis = -1\n        keepdims = 1\n        node = onnx.helper.make_node(\n            'ArgMin',\n            inputs=['data'],\n            outputs=['result'],\n            axis=axis,\n            keepdims=keepdims,\n            select_last_index=True)\n        # result: [[1], [0]]\n        result = argmin_use_numpy_select_last_index(data, axis=axis, keepdims=keepdims)\n        expect(node, inputs=[data], outputs=[result], name='test_argmin_negative_axis_keepdims_example_select_last_index')\n\n        data = np.random.uniform(-10, 10, [2, 3, 4]).astype(np.float32)\n        # result's shape: [2, 3, 1]\n        result = argmin_use_numpy_select_last_index(data, axis=axis, keepdims=keepdims)\n        expect(node, inputs=[data], outputs=[result], name='test_argmin_negative_axis_keepdims_random_select_last_index')\n"""
onnx/backend/test/case/node/asin.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Asin(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Asin',\n            inputs=['x'],\n            outputs=['y'],\n        )\n\n        x = np.array([-0.5, 0, 0.5]).astype(np.float32)\n        y = np.arcsin(x)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_asin_example')\n\n        x = np.random.rand(3, 4, 5).astype(np.float32)\n        y = np.arcsin(x)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_asin')\n"""
onnx/backend/test/case/node/asinh.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Asinh(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Asinh',\n            inputs=['x'],\n            outputs=['y'],\n        )\n\n        x = np.array([-1, 0, 1]).astype(np.float32)\n        y = np.arcsinh(x)  # expected output [-0.88137358,  0.,  0.88137358]\n        expect(node, inputs=[x], outputs=[y],\n               name='test_asinh_example')\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.arcsinh(x)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_asinh')\n"""
onnx/backend/test/case/node/atan.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Atan(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Atan',\n            inputs=['x'],\n            outputs=['y'],\n        )\n\n        x = np.array([-1, 0, 1]).astype(np.float32)\n        y = np.arctan(x)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_atan_example')\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.arctan(x)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_atan')\n"""
onnx/backend/test/case/node/atanh.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Atanh(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Atanh',\n            inputs=['x'],\n            outputs=['y'],\n        )\n\n        x = np.array([-0.5, 0, 0.5]).astype(np.float32)\n        y = np.arctanh(x)  # expected output [-0.54930615,  0.,  0.54930615]\n        expect(node, inputs=[x], outputs=[y],\n               name='test_atanh_example')\n\n        x = np.random.uniform(0.0, 1.0, (3, 4, 5)).astype(np.float32)\n        y = np.arctanh(x)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_atanh')\n"""
onnx/backend/test/case/node/averagepool.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\nfrom .pool_op_common import get_pad_shape, get_output_shape, pool\n\n\nclass AveragePool(Base):\n\n    @staticmethod\n    def export_averagepool_2d_precomputed_pads():  # type: () -> None\n        """"""\n        input_shape: [1, 1, 5, 5]\n        output_shape: [1, 1, 5, 5]\n        pad_shape: [4, 4] -> [2, 2, 2, 2] by axis\n        """"""\n        node = onnx.helper.make_node(\n            \'AveragePool\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            kernel_shape=[5, 5],\n            pads=[2, 2, 2, 2]\n\n        )\n        x = np.array([[[\n            [1, 2, 3, 4, 5],\n            [6, 7, 8, 9, 10],\n            [11, 12, 13, 14, 15],\n            [16, 17, 18, 19, 20],\n            [21, 22, 23, 24, 25],\n        ]]]).astype(np.float32)\n        y = np.array([[[[7, 7.5, 8, 8.5, 9],\n                        [9.5, 10, 10.5, 11, 11.5],\n                        [12, 12.5, 13, 13.5, 14],\n                        [14.5, 15, 15.5, 16, 16.5],\n                        [17, 17.5, 18, 18.5, 19]]]]).astype(np.float32)\n\n        expect(node, inputs=[x], outputs=[y], name=\'test_averagepool_2d_precomputed_pads\')\n\n    @staticmethod\n    def export_averagepool_2d_precomputed_pads_count_include_pad():  # type: () -> None\n        """"""\n        input_shape: [1, 1, 5, 5]\n        output_shape: [1, 1, 5, 5]\n        pad_shape: [4, 4] -> [2, 2, 2, 2] by axis\n        """"""\n        node = onnx.helper.make_node(\n            \'AveragePool\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            kernel_shape=[5, 5],\n            pads=[2, 2, 2, 2],\n            count_include_pad=1\n        )\n        x = np.array([[[\n            [1, 2, 3, 4, 5],\n            [6, 7, 8, 9, 10],\n            [11, 12, 13, 14, 15],\n            [16, 17, 18, 19, 20],\n            [21, 22, 23, 24, 25],\n        ]]]).astype(np.float32)\n        y = np.array([[[[2.5200, 3.6000, 4.8000, 4.0800, 3.2400],\n                        [4.5600, 6.4000, 8.4000, 7.0400, 5.5200],\n                        [7.2000, 10.0000, 13.0000, 10.8000, 8.4000],\n                        [6.9600, 9.6000, 12.4000, 10.2400, 7.9200],\n                        [6.1200, 8.4000, 10.8000, 8.8800, 6.8400]]]]).astype(np.float32)\n\n        expect(node, inputs=[x], outputs=[y], name=\'test_averagepool_2d_precomputed_pads_count_include_pad\')\n\n    @staticmethod\n    def export_averagepool_2d_precomputed_strides():  # type: () -> None\n        """"""\n        input_shape: [1, 1, 5, 5]\n        output_shape: [1, 1, 2, 2]\n        """"""\n        node = onnx.helper.make_node(\n            \'AveragePool\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            kernel_shape=[2, 2],\n            strides=[2, 2]\n        )\n        x = np.array([[[\n            [1, 2, 3, 4, 5],\n            [6, 7, 8, 9, 10],\n            [11, 12, 13, 14, 15],\n            [16, 17, 18, 19, 20],\n            [21, 22, 23, 24, 25],\n        ]]]).astype(np.float32)\n        y = np.array([[[[4, 6],\n                        [14, 16]]]]).astype(np.float32)\n\n        expect(node, inputs=[x], outputs=[y], name=\'test_averagepool_2d_precomputed_strides\')\n\n    @staticmethod\n    def export_averagepool_2d_precomputed_same_upper():  # type: () -> None\n        """"""\n        input_shape: [1, 1, 5, 5]\n        output_shape: [1, 1, 3, 3]\n        pad_shape: [2, 2] -> [1, 1, 1, 1] by axis\n        """"""\n        node = onnx.helper.make_node(\n            \'AveragePool\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            kernel_shape=[3, 3],\n            strides=[2, 2],\n            auto_pad=\'SAME_UPPER\'\n        )\n        x = np.array([[[\n            [1, 2, 3, 4, 5],\n            [6, 7, 8, 9, 10],\n            [11, 12, 13, 14, 15],\n            [16, 17, 18, 19, 20],\n            [21, 22, 23, 24, 25],\n        ]]]).astype(np.float32)\n        y = np.array([[[[4, 5.5, 7],\n                        [11.5, 13, 14.5],\n                        [19, 20.5, 22]]]]).astype(np.float32)\n\n        expect(node, inputs=[x], outputs=[y], name=\'test_averagepool_2d_precomputed_same_upper\')\n\n    @staticmethod\n    def export_averagepool_1d_default():  # type: () -> None\n        """"""\n        input_shape: [1, 3, 32]\n        output_shape: [1, 3, 31]\n        """"""\n        node = onnx.helper.make_node(\n            \'AveragePool\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            kernel_shape=[2],\n        )\n        x = np.random.randn(1, 3, 32).astype(np.float32)\n        x_shape = np.shape(x)\n        kernel_shape = [2]\n        strides = [1]\n        out_shape = get_output_shape(\'VALID\', x_shape[2:], kernel_shape, strides)\n        padded = x\n        y = pool(padded, x_shape, kernel_shape, strides, out_shape, [0], \'AVG\')\n\n        expect(node, inputs=[x], outputs=[y], name=\'test_averagepool_1d_default\')\n\n    @staticmethod\n    def export_averagepool_2d_default():  # type: () -> None\n        """"""\n        input_shape: [1, 3, 32, 32]\n        output_shape: [1, 3, 31, 31]\n        """"""\n        node = onnx.helper.make_node(\n            \'AveragePool\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            kernel_shape=[2, 2],\n        )\n        x = np.random.randn(1, 3, 32, 32).astype(np.float32)\n        x_shape = np.shape(x)\n        kernel_shape = (2, 2)\n        strides = (1, 1)\n        out_shape = get_output_shape(\'VALID\', x_shape[2:], kernel_shape, strides)\n        padded = x\n        y = pool(padded, x_shape, kernel_shape, strides, out_shape, (0, 0), \'AVG\')\n\n        expect(node, inputs=[x], outputs=[y], name=\'test_averagepool_2d_default\')\n\n    @staticmethod\n    def export_averagepool_3d_default():  # type: () -> None\n        """"""\n        input_shape: [1, 3, 32, 32, 32]\n        output_shape: [1, 3, 31, 31, 31]\n        """"""\n        node = onnx.helper.make_node(\n            \'AveragePool\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            kernel_shape=[2, 2, 2],\n        )\n        x = np.random.randn(1, 3, 32, 32, 32).astype(np.float32)\n        x_shape = np.shape(x)\n        kernel_shape = [2, 2, 2]\n        strides = [1, 1, 1]\n        out_shape = get_output_shape(\'VALID\', x_shape[2:], kernel_shape, strides)\n        padded = x\n        y = pool(padded, x_shape, kernel_shape, strides, out_shape, [0, 0, 0], \'AVG\')\n\n        expect(node, inputs=[x], outputs=[y], name=\'test_averagepool_3d_default\')\n\n    @staticmethod\n    def export_averagepool_2d_same_upper():  # type: () -> None\n        """"""\n        input_shape: [1, 3, 32, 32]\n        output_shape: [1, 3, 32, 32]\n        pad_shape: [1, 1] -> [0, 1, 0, 1] by axis\n        """"""\n        node = onnx.helper.make_node(\n            \'AveragePool\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            kernel_shape=[2, 2],\n            auto_pad=\'SAME_UPPER\'\n        )\n        x = np.random.randn(1, 3, 32, 32).astype(np.float32)\n        x_shape = np.shape(x)\n        kernel_shape = (2, 2)\n        strides = (1, 1)\n        out_shape = get_output_shape(\'SAME_UPPER\', x_shape[2:], kernel_shape, strides)\n        pad_shape = get_pad_shape(\'SAME_UPPER\', x_shape[2:], kernel_shape, strides, out_shape)\n        pad_top = pad_shape[0] // 2\n        pad_bottom = pad_shape[0] - pad_top\n        pad_left = pad_shape[1] // 2\n        pad_right = pad_shape[1] - pad_left\n        padded = np.pad(x, ((0, 0), (0, 0), (pad_top, pad_bottom), (pad_left, pad_right)), mode=\'constant\',\n                        constant_values=np.nan)\n        y = pool(padded, x_shape, kernel_shape, strides, out_shape, pad_shape, \'AVG\')\n\n        expect(node, inputs=[x], outputs=[y], name=\'test_averagepool_2d_same_upper\')\n\n    @staticmethod\n    def export_averagepool_2d_same_lower():  # type: () -> None\n        """"""\n        input_shape: [1, 3, 32, 32]\n        output_shape: [1, 3, 32, 32]\n        pad_shape: [1, 1] -> [1, 0, 1, 0] by axis\n        """"""\n        node = onnx.helper.make_node(\n            \'AveragePool\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            kernel_shape=[2, 2],\n            auto_pad=\'SAME_LOWER\'\n        )\n        x = np.random.randn(1, 3, 32, 32).astype(np.float32)\n        x_shape = np.shape(x)\n        kernel_shape = (2, 2)\n        strides = (1, 1)\n        out_shape = get_output_shape(\'SAME_LOWER\', x_shape[2:], kernel_shape, strides)\n        pad_shape = get_pad_shape(\'SAME_LOWER\', x_shape[2:], kernel_shape, strides, out_shape)\n        pad_bottom = pad_shape[0] // 2\n        pad_top = pad_shape[0] - pad_bottom\n        pad_right = pad_shape[1] // 2\n        pad_left = pad_shape[1] - pad_right\n        padded = np.pad(x, ((0, 0), (0, 0), (pad_top, pad_bottom), (pad_left, pad_right)), mode=\'constant\',\n                        constant_values=np.nan)\n        y = pool(padded, x_shape, kernel_shape, strides, out_shape, pad_shape, \'AVG\')\n\n        expect(node, inputs=[x], outputs=[y], name=\'test_averagepool_2d_same_lower\')\n\n    @staticmethod\n    def export_averagepool_2d_pads():  # type: () -> None\n        """"""\n        input_shape: [1, 3, 28, 28]\n        output_shape: [1, 3, 30, 30]\n        pad_shape: [4, 4] -> [2, 2, 2, 2] by axis\n        """"""\n        node = onnx.helper.make_node(\n            \'AveragePool\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            kernel_shape=[3, 3],\n            pads=[2, 2, 2, 2]\n        )\n        x = np.random.randn(1, 3, 28, 28).astype(np.float32)\n        x_shape = np.shape(x)\n        kernel_shape = (3, 3)\n        strides = (1, 1)\n        pad_bottom = 2\n        pad_top = 2\n        pad_right = 2\n        pad_left = 2\n        pad_shape = [pad_top + pad_bottom, pad_left + pad_right]\n        out_shape = get_output_shape(\'VALID\', np.add(x_shape[2:], pad_shape), kernel_shape, strides)\n        padded = np.pad(x, ((0, 0), (0, 0), (pad_top, pad_bottom), (pad_left, pad_right)), mode=\'constant\',\n                        constant_values=np.nan)\n        y = pool(padded, x_shape, kernel_shape, strides, out_shape, pad_shape, \'AVG\')\n\n        expect(node, inputs=[x], outputs=[y], name=\'test_averagepool_2d_pads\')\n\n    @staticmethod\n    def export_averagepool_2d_pads_count_include_pad():  # type: () -> None\n        """"""\n        input_shape: [1, 3, 28, 28]\n        output_shape: [1, 3, 30, 30]\n        pad_shape: [4, 4] -> [2, 2, 2, 2] by axis\n        """"""\n        node = onnx.helper.make_node(\n            \'AveragePool\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            kernel_shape=[3, 3],\n            pads=[2, 2, 2, 2],\n            count_include_pad=1,\n        )\n        x = np.random.randn(1, 3, 28, 28).astype(np.float32)\n        x_shape = np.shape(x)\n        kernel_shape = (3, 3)\n        strides = (1, 1)\n        pad_bottom = 2\n        pad_top = 2\n        pad_right = 2\n        pad_left = 2\n        pad_shape = [pad_top + pad_bottom, pad_left + pad_right]\n        out_shape = get_output_shape(\'VALID\', np.add(x_shape[2:], pad_shape), kernel_shape, strides)\n        padded = np.pad(x, ((0, 0), (0, 0), (pad_top, pad_bottom), (pad_left, pad_right)), mode=\'constant\',\n                        constant_values=0)\n        y = pool(padded, x_shape, kernel_shape, strides, out_shape, pad_shape, \'AVG\', count_include_pad=1)\n\n        expect(node, inputs=[x], outputs=[y], name=\'test_averagepool_2d_pads_count_include_pad\')\n\n    @staticmethod\n    def export_averagepool_2d_strides():  # type: () -> None\n        """"""\n        input_shape: [1, 3, 32, 32]\n        output_shape: [1, 3, 10, 10]\n        """"""\n        node = onnx.helper.make_node(\n            \'AveragePool\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            kernel_shape=[5, 5],\n            strides=[3, 3]\n        )\n        x = np.random.randn(1, 3, 32, 32).astype(np.float32)\n        x_shape = np.shape(x)\n        kernel_shape = (5, 5)\n        strides = (3, 3)\n        out_shape = get_output_shape(\'VALID\', x_shape[2:], kernel_shape, strides)\n        padded = x\n        y = pool(padded, x_shape, kernel_shape, strides, out_shape, (0, 0), \'AVG\')\n\n        expect(node, inputs=[x], outputs=[y], name=\'test_averagepool_2d_strides\')\n\n    @staticmethod\n    def export_averagepool_2d_ceil():  # type: () -> None\n        """"""\n        input_shape: [1, 1, 4, 4]\n        output_shape: [1, 1, 2, 2]\n        """"""\n        node = onnx.helper.make_node(\n            \'AveragePool\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            kernel_shape=[3, 3],\n            strides=[2, 2],\n            ceil_mode=True\n        )\n        x = np.array([[[\n            [1, 2, 3, 4],\n            [5, 6, 7, 8],\n            [9, 10, 11, 12],\n            [13, 14, 15, 16],\n        ]]]).astype(np.float32)\n        y = np.array([[[\n            [6, 7.5],\n            [12, 13.5]]]]).astype(np.float32)\n\n        expect(node, inputs=[x], outputs=[y], name=\'test_averagepool_2d_ceil\')\n'"
onnx/backend/test/case/node/batchnorm.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass BatchNormalization(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        def _batchnorm_test_mode(x, s, bias, mean, var, epsilon=1e-5):  # type: ignore\n            dims_x = len(x.shape)\n            dim_ones = (1,) * (dims_x - 2)\n            s = s.reshape(-1, *dim_ones)\n            bias = bias.reshape(-1, *dim_ones)\n            mean = mean.reshape(-1, *dim_ones)\n            var = var.reshape(-1, *dim_ones)\n            return s * (x - mean) / np.sqrt(var + epsilon) + bias\n\n        # input size: (1, 2, 1, 3)\n        x = np.array([[[[-1, 0, 1]], [[2, 3, 4]]]]).astype(np.float32)\n        s = np.array([1.0, 1.5]).astype(np.float32)\n        bias = np.array([0, 1]).astype(np.float32)\n        mean = np.array([0, 3]).astype(np.float32)\n        var = np.array([1, 1.5]).astype(np.float32)\n        y = _batchnorm_test_mode(x, s, bias, mean, var).astype(np.float32)\n\n        node = onnx.helper.make_node(\n            'BatchNormalization',\n            inputs=['x', 's', 'bias', 'mean', 'var'],\n            outputs=['y'],\n        )\n\n        # output size: (1, 2, 1, 3)\n        expect(node, inputs=[x, s, bias, mean, var], outputs=[y],\n               name='test_batchnorm_example')\n\n        # input size: (2, 3, 4, 5)\n        x = np.random.randn(2, 3, 4, 5).astype(np.float32)\n        s = np.random.randn(3).astype(np.float32)\n        bias = np.random.randn(3).astype(np.float32)\n        mean = np.random.randn(3).astype(np.float32)\n        var = np.random.rand(3).astype(np.float32)\n        epsilon = 1e-2\n        y = _batchnorm_test_mode(x, s, bias, mean, var, epsilon).astype(np.float32)\n\n        node = onnx.helper.make_node(\n            'BatchNormalization',\n            inputs=['x', 's', 'bias', 'mean', 'var'],\n            outputs=['y'],\n            epsilon=epsilon,\n        )\n\n        # output size: (2, 3, 4, 5)\n        expect(node, inputs=[x, s, bias, mean, var], outputs=[y],\n               name='test_batchnorm_epsilon')\n"""
onnx/backend/test/case/node/bitshift.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass BitShift(Base):\n\n    @staticmethod\n    def export_right_unit8():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'BitShift\',\n            inputs=[\'x\', \'y\'],\n            outputs=[\'z\'],\n            direction=""RIGHT""\n        )\n\n        x = np.array([16, 4, 1]).astype(np.uint8)\n        y = np.array([1, 2, 3]).astype(np.uint8)\n        z = x >> y  # expected output [8, 1, 0]\n        expect(node, inputs=[x, y], outputs=[z],\n               name=\'test_bitshift_right_uint8\')\n\n    @staticmethod\n    def export_right_unit16():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'BitShift\',\n            inputs=[\'x\', \'y\'],\n            outputs=[\'z\'],\n            direction=""RIGHT""\n        )\n\n        x = np.array([16, 4, 1]).astype(np.uint16)\n        y = np.array([1, 2, 3]).astype(np.uint16)\n        z = x >> y  # expected output [8, 1, 0]\n        expect(node, inputs=[x, y], outputs=[z],\n               name=\'test_bitshift_right_uint16\')\n\n    @staticmethod\n    def export_right_unit32():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'BitShift\',\n            inputs=[\'x\', \'y\'],\n            outputs=[\'z\'],\n            direction=""RIGHT""\n        )\n\n        x = np.array([16, 4, 1]).astype(np.uint32)\n        y = np.array([1, 2, 3]).astype(np.uint32)\n        z = x >> y  # expected output [8, 1, 0]\n        expect(node, inputs=[x, y], outputs=[z],\n               name=\'test_bitshift_right_uint32\')\n\n    @staticmethod\n    def export_right_unit64():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'BitShift\',\n            inputs=[\'x\', \'y\'],\n            outputs=[\'z\'],\n            direction=""RIGHT""\n        )\n\n        x = np.array([16, 4, 1]).astype(np.uint64)\n        y = np.array([1, 2, 3]).astype(np.uint64)\n        z = x >> y  # expected output [8, 1, 0]\n        expect(node, inputs=[x, y], outputs=[z],\n               name=\'test_bitshift_right_uint64\')\n\n    @staticmethod\n    def export_left_unit8():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'BitShift\',\n            inputs=[\'x\', \'y\'],\n            outputs=[\'z\'],\n            direction=""LEFT""\n        )\n\n        x = np.array([16, 4, 1]).astype(np.uint8)\n        y = np.array([1, 2, 3]).astype(np.uint8)\n        z = x << y  # expected output [32, 16, 8]\n        expect(node, inputs=[x, y], outputs=[z],\n               name=\'test_bitshift_left_uint8\')\n\n    @staticmethod\n    def export_left_unit16():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'BitShift\',\n            inputs=[\'x\', \'y\'],\n            outputs=[\'z\'],\n            direction=""LEFT""\n        )\n\n        x = np.array([16, 4, 1]).astype(np.uint16)\n        y = np.array([1, 2, 3]).astype(np.uint16)\n        z = x << y  # expected output [32, 16, 8]\n        expect(node, inputs=[x, y], outputs=[z],\n               name=\'test_bitshift_left_uint16\')\n\n    @staticmethod\n    def export_left_unit32():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'BitShift\',\n            inputs=[\'x\', \'y\'],\n            outputs=[\'z\'],\n            direction=""LEFT""\n        )\n\n        x = np.array([16, 4, 1]).astype(np.uint32)\n        y = np.array([1, 2, 3]).astype(np.uint32)\n        z = x << y  # expected output [32, 16, 8]\n        expect(node, inputs=[x, y], outputs=[z],\n               name=\'test_bitshift_left_uint32\')\n\n    @staticmethod\n    def export_left_unit64():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'BitShift\',\n            inputs=[\'x\', \'y\'],\n            outputs=[\'z\'],\n            direction=""LEFT""\n        )\n\n        x = np.array([16, 4, 1]).astype(np.uint64)\n        y = np.array([1, 2, 3]).astype(np.uint64)\n        z = x << y  # expected output [32, 16, 8]\n        expect(node, inputs=[x, y], outputs=[z],\n               name=\'test_bitshift_left_uint64\')\n'"
onnx/backend/test/case/node/cast.py,0,"b""# coding: utf-8\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom onnx import TensorProto\nfrom onnx.mapping import TENSOR_TYPE_TO_NP_TYPE\n\nfrom ..base import Base\nfrom . import expect\nimport sys\n\n\nclass Cast(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        shape = (3, 4)\n        test_cases = [\n            ('FLOAT', 'FLOAT16'),\n            ('FLOAT', 'DOUBLE'),\n            ('FLOAT16', 'FLOAT'),\n            ('FLOAT16', 'DOUBLE'),\n            ('DOUBLE', 'FLOAT'),\n            ('DOUBLE', 'FLOAT16'),\n            ('FLOAT', 'STRING'),\n            ('STRING', 'FLOAT'),\n            ('FLOAT', 'BFLOAT16'),\n            ('BFLOAT16', 'FLOAT'),\n        ]\n\n        for from_type, to_type in test_cases:\n            input_type = None\n            output_type = None\n            if 'BFLOAT16' == from_type or 'BFLOAT16' == to_type:\n                np_fp32 = np.array([u'0.47892547', u'0.48033667', u'0.49968487', u'0.81910545',\n                    u'0.47031248', u'0.816468', u'0.21087195', u'0.7229038',\n                    u'NaN', u'INF', u'+INF', u'-INF'], dtype=np.float32)\n                little_endisan = sys.byteorder == 'little'\n                np_uint16_view = np_fp32.view(dtype=np.uint16)\n                np_bfp16 = np_uint16_view[1::2] if little_endisan else np_uint16_view[0::2]\n                if 'BFLOAT16' == to_type:\n                    assert from_type == 'FLOAT'\n                    input = np_fp32.reshape([3, 4])\n                    output = np_bfp16.reshape([3, 4])\n                    input_type = int(TensorProto.FLOAT)\n                    output_type = int(TensorProto.BFLOAT16)\n                else:\n                    assert to_type == 'FLOAT'\n                    input = np_bfp16.reshape([3, 4])\n                    #convert bfloat to FLOAT\n                    np_fp32_zeros = np.zeros((len(np_bfp16) * 2,), dtype=np.uint16)\n                    if little_endisan:\n                        np_fp32_zeros[1::2] = np_bfp16\n                    else:\n                        np_fp32_zeros[0::2] = np_bfp16\n                    np_fp32_from_bfloat = np_fp32_zeros.view(dtype=np.float32)\n                    output = np_fp32_from_bfloat.reshape([3, 4])\n                    input_type = int(TensorProto.BFLOAT16)\n                    output_type = int(TensorProto.FLOAT)\n            elif 'STRING' != from_type:\n                input = np.random.random_sample(shape).astype(\n                    TENSOR_TYPE_TO_NP_TYPE[getattr(TensorProto, from_type)])\n                if ('STRING' == to_type):\n                    # Converting input to str, then give it np.object dtype for generating script\n                    ss = []\n                    for i in input.flatten():\n                        s = str(i).encode('utf-8')\n                        su = s.decode('utf-8')\n                        ss.append(su)\n\n                    output = np.array(ss).astype(np.object).reshape([3, 4])\n                else:\n                    output = input.astype(TENSOR_TYPE_TO_NP_TYPE[getattr(TensorProto, to_type)])\n            else:\n                input = np.array([u'0.47892547', u'0.48033667', u'0.49968487', u'0.81910545',\n                    u'0.47031248', u'0.816468', u'0.21087195', u'0.7229038',\n                    u'NaN', u'INF', u'+INF', u'-INF'], dtype=np.dtype(np.object)).reshape([3, 4])\n                output = input.astype(TENSOR_TYPE_TO_NP_TYPE[getattr(TensorProto, to_type)])\n            node = onnx.helper.make_node(\n                'Cast',\n                inputs=['input'],\n                outputs=['output'],\n                to=getattr(TensorProto, to_type),\n            )\n            if input_type and output_type:\n                expect(node, inputs=[input], outputs=[output],\n                           name='test_cast_' + from_type + '_to_' + to_type,\n                           input_types=[input_type],\n                           output_types=[output_type])\n            else:\n                expect(node, inputs=[input], outputs=[output],\n                           name='test_cast_' + from_type + '_to_' + to_type)\n"""
onnx/backend/test/case/node/ceil.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Ceil(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Ceil',\n            inputs=['x'],\n            outputs=['y'],\n        )\n\n        x = np.array([-1.5, 1.2]).astype(np.float32)\n        y = np.ceil(x)  # expected output [-1., 2.]\n        expect(node, inputs=[x], outputs=[y],\n               name='test_ceil_example')\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.ceil(x)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_ceil')\n"""
onnx/backend/test/case/node/celu.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Celu(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        alpha = 2.0\n        node = onnx.helper.make_node(\n            'Celu',\n            inputs=['X'],\n            outputs=['Y'],\n            alpha=alpha,\n        )\n\n        input_data = np.array([[[[0.8439683], [0.5665144], [0.05836735]],\n            [[0.02916367], [0.12964272], [0.5060197]],\n            [[0.79538304], [0.9411346], [0.9546573]]],\n            [[[0.17730942], [0.46192095], [0.26480448]],\n            [[0.6746842], [0.01665257], [0.62473077]],\n            [[0.9240844], [0.9722341], [0.11965699]]],\n            [[[0.41356155], [0.9129373], [0.59330076]],\n            [[0.81929934], [0.7862604], [0.11799799]],\n            [[0.69248444], [0.54119414], [0.07513223]]]], dtype=np.float32)\n\n        # Calculate expected output data\n        positive_input = np.maximum(0, input_data)\n        negative_input = np.minimum(0, alpha * (np.exp(input_data / alpha) - 1))\n        expected_output = positive_input + negative_input\n\n        expect(node, inputs=[input_data], outputs=[expected_output],\n               name='test_celu')\n"""
onnx/backend/test/case/node/clip.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Clip(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'Clip\',\n            inputs=[\'x\', \'min\', \'max\'],\n            outputs=[\'y\'],\n        )\n\n        x = np.array([-2, 0, 2]).astype(np.float32)\n        min_val = np.float32(-1)\n        max_val = np.float32(1)\n        y = np.clip(x, min_val, max_val)  # expected output [-1., 0., 1.]\n        expect(node, inputs=[x, min_val, max_val], outputs=[y],\n               name=\'test_clip_example\')\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.clip(x, min_val, max_val)\n        expect(node, inputs=[x, min_val, max_val], outputs=[y],\n               name=\'test_clip\')\n        node = onnx.helper.make_node(\n            \'Clip\',\n            inputs=[\'x\', \'min\', \'max\'],\n            outputs=[\'y\'],\n        )\n\n        min_val = np.float32(-5)\n        max_val = np.float32(5)\n\n        x = np.array([-1, 0, 1]).astype(np.float32)\n        y = np.array([-1, 0, 1]).astype(np.float32)\n        expect(node, inputs=[x, min_val, max_val], outputs=[y],\n               name=\'test_clip_inbounds\')\n\n        x = np.array([-6, 0, 6]).astype(np.float32)\n        y = np.array([-5, 0, 5]).astype(np.float32)\n        expect(node, inputs=[x, min_val, max_val], outputs=[y],\n               name=\'test_clip_outbounds\')\n\n        x = np.array([-1, 0, 6]).astype(np.float32)\n        y = np.array([-1, 0, 5]).astype(np.float32)\n        expect(node, inputs=[x, min_val, max_val], outputs=[y],\n               name=\'test_clip_splitbounds\')\n\n    @staticmethod\n    def export_clip_default():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'Clip\',\n            inputs=[\'x\', \'min\'],\n            outputs=[\'y\'],\n        )\n        min_val = np.float32(0)\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.clip(x, min_val, np.inf)\n        expect(node, inputs=[x, min_val], outputs=[y],\n               name=\'test_clip_default_min\')\n\n        no_min = """"  # optional input, not supplied\n        node = onnx.helper.make_node(\n            \'Clip\',\n            inputs=[\'x\', no_min, \'max\'],\n            outputs=[\'y\'],\n        )\n        max_val = np.float32(0)\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.clip(x, -np.inf, max_val)\n        expect(node, inputs=[x, max_val], outputs=[y],\n               name=\'test_clip_default_max\')\n\n        no_max = """"  # optional input, not supplied\n        node = onnx.helper.make_node(\n            \'Clip\',\n            inputs=[\'x\', no_min, no_max],\n            outputs=[\'y\'],\n        )\n\n        x = np.array([-1, 0, 1]).astype(np.float32)\n        y = np.array([-1, 0, 1]).astype(np.float32)\n        expect(node, inputs=[x], outputs=[y],\n               name=\'test_clip_default_inbounds\')\n\n    @staticmethod\n    def export_clip_default_int8():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'Clip\',\n            inputs=[\'x\', \'min\'],\n            outputs=[\'y\'],\n        )\n        min_val = np.int8(0)\n        x = np.random.randn(3, 4, 5).astype(np.int8)\n        y = np.clip(x, min_val, np.iinfo(np.int8).max)\n        expect(node, inputs=[x, min_val], outputs=[y],\n               name=\'test_clip_default_int8_min\')\n\n        no_min = """"  # optional input, not supplied\n        node = onnx.helper.make_node(\n            \'Clip\',\n            inputs=[\'x\', no_min, \'max\'],\n            outputs=[\'y\'],\n        )\n        max_val = np.int8(0)\n        x = np.random.randn(3, 4, 5).astype(np.int8)\n        y = np.clip(x, np.iinfo(np.int8).min, max_val)\n        expect(node, inputs=[x, max_val], outputs=[y],\n               name=\'test_clip_default_int8_max\')\n\n        no_max = """"  # optional input, not supplied\n        node = onnx.helper.make_node(\n            \'Clip\',\n            inputs=[\'x\', no_min, no_max],\n            outputs=[\'y\'],\n        )\n\n        x = np.array([-1, 0, 1]).astype(np.int8)\n        y = np.array([-1, 0, 1]).astype(np.int8)\n        expect(node, inputs=[x], outputs=[y],\n               name=\'test_clip_default_int8_inbounds\')\n'"
onnx/backend/test/case/node/compress.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Compress(Base):\n\n    @staticmethod\n    def export_compress_0():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Compress',\n            inputs=['input', 'condition'],\n            outputs=['output'],\n            axis=0,\n        )\n        input = np.array([[1, 2], [3, 4], [5, 6]]).astype(np.float32)\n        condition = np.array([0, 1, 1])\n        output = np.compress(condition, input, axis=0)\n        #print(output)\n        #[[ 3.  4.]\n        # [ 5.  6.]]\n\n        expect(node, inputs=[input, condition.astype(np.bool)], outputs=[output],\n               name='test_compress_0')\n\n    @staticmethod\n    def export_compress_1():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Compress',\n            inputs=['input', 'condition'],\n            outputs=['output'],\n            axis=1,\n        )\n        input = np.array([[1, 2], [3, 4], [5, 6]]).astype(np.float32)\n        condition = np.array([0, 1])\n        output = np.compress(condition, input, axis=1)\n        #print(output)\n        #[[ 2.]\n        # [ 4.]\n        # [ 6.]]\n\n        expect(node, inputs=[input, condition.astype(np.bool)], outputs=[output],\n               name='test_compress_1')\n\n    @staticmethod\n    def export_compress_default_axis():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Compress',\n            inputs=['input', 'condition'],\n            outputs=['output'],\n        )\n        input = np.array([[1, 2], [3, 4], [5, 6]]).astype(np.float32)\n        condition = np.array([0, 1, 0, 0, 1])\n        output = np.compress(condition, input)\n        #print(output)\n        #[ 2., 5.]\n\n        expect(node, inputs=[input, condition.astype(np.bool)], outputs=[output],\n               name='test_compress_default_axis')\n\n    @staticmethod\n    def export_compress_negative_axis():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Compress',\n            inputs=['input', 'condition'],\n            outputs=['output'],\n            axis=-1,\n        )\n        input = np.array([[1, 2], [3, 4], [5, 6]]).astype(np.float32)\n        condition = np.array([0, 1])\n        output = np.compress(condition, input, axis=-1)\n        # print(output)\n        #[[ 2.]\n        # [ 4.]\n        # [ 6.]]\n        expect(node, inputs=[input, condition.astype(np.bool)], outputs=[output],\n               name='test_compress_negative_axis')\n"""
onnx/backend/test/case/node/concat.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\nfrom typing import Dict, Sequence, Text, Any\n\n\nclass Concat(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        test_cases = {\n            '1d': ([1, 2],\n                   [3, 4]),\n            '2d': ([[1, 2], [3, 4]],\n                   [[5, 6], [7, 8]]),\n            '3d': ([[[1, 2], [3, 4]], [[5, 6], [7, 8]]],\n                   [[[9, 10], [11, 12]], [[13, 14], [15, 16]]])\n        }  # type: Dict[Text, Sequence[Any]]\n\n        for test_case, values_ in test_cases.items():\n            values = [np.asarray(v, dtype=np.float32) for v in values_]\n            for i in range(len(values[0].shape)):\n                in_args = ['value' + str(k) for k in range(len(values))]\n                node = onnx.helper.make_node(\n                    'Concat',\n                    inputs=[s for s in in_args],\n                    outputs=['output'],\n                    axis=i\n                )\n                output = np.concatenate(values, i)\n                expect(node, inputs=[v for v in values], outputs=[output],\n                       name='test_concat_' + test_case + '_axis_' + str(i))\n\n            for i in range(-len(values[0].shape), 0):\n                in_args = ['value' + str(k) for k in range(len(values))]\n                node = onnx.helper.make_node(\n                    'Concat',\n                    inputs=[s for s in in_args],\n                    outputs=['output'],\n                    axis=i\n                )\n                output = np.concatenate(values, i)\n                expect(node, inputs=[v for v in values], outputs=[output],\n                       name='test_concat_' + test_case + '_axis_negative_' + str(abs(i)))\n"""
onnx/backend/test/case/node/constant.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Constant(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        values = np.random.randn(5, 5).astype(np.float32)\n        node = onnx.helper.make_node(\n            'Constant',\n            inputs=[],\n            outputs=['values'],\n            value=onnx.helper.make_tensor(\n                name='const_tensor',\n                data_type=onnx.TensorProto.FLOAT,\n                dims=values.shape,\n                vals=values.flatten().astype(float),\n            ),\n        )\n\n        expect(node, inputs=[], outputs=[values],\n               name='test_constant')\n"""
onnx/backend/test/case/node/constantofshape.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass ConstantOfShape(Base):\n\n    @staticmethod\n    def export_float_ones():  # type: () -> None\n        x = np.array([4, 3, 2]).astype(np.int64)\n        tensor_value = onnx.helper.make_tensor(""value"", onnx.TensorProto.FLOAT,\n                                               [1], [1])\n        node = onnx.helper.make_node(\n            \'ConstantOfShape\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            value=tensor_value,\n        )\n\n        y = np.ones(x, dtype=np.float32)\n        expect(node, inputs=[x], outputs=[y],\n               name=\'test_constantofshape_float_ones\')\n\n    @staticmethod\n    def export_int32_zeros():  # type: () -> None\n        x = np.array([10, 6]).astype(np.int64)\n        tensor_value = onnx.helper.make_tensor(""value"", onnx.TensorProto.INT32,\n                                               [1], [0])\n        node = onnx.helper.make_node(\n            \'ConstantOfShape\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            value=tensor_value,\n        )\n        y = np.zeros(x, dtype=np.int32)\n        expect(node, inputs=[x], outputs=[y],\n               name=\'test_constantofshape_int_zeros\')\n\n    @staticmethod\n    def export_int32_shape_zero():  # type: () -> None\n        x = np.array([0, ]).astype(np.int64)\n        tensor_value = onnx.helper.make_tensor(""value"", onnx.TensorProto.INT32,\n                                               [1], [0])\n        node = onnx.helper.make_node(\n            \'ConstantOfShape\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            value=tensor_value,\n        )\n        y = np.zeros(x, dtype=np.int32)\n        expect(node, inputs=[x], outputs=[y],\n               name=\'test_constantofshape_int_shape_zero\')\n'"
onnx/backend/test/case/node/conv.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Conv(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n\n        x = np.array([[[[0., 1., 2., 3., 4.],  # (1, 1, 5, 5) input tensor\n                        [5., 6., 7., 8., 9.],\n                        [10., 11., 12., 13., 14.],\n                        [15., 16., 17., 18., 19.],\n                        [20., 21., 22., 23., 24.]]]]).astype(np.float32)\n        W = np.array([[[[1., 1., 1.],  # (1, 1, 3, 3) tensor for convolution weights\n                        [1., 1., 1.],\n                        [1., 1., 1.]]]]).astype(np.float32)\n\n        # Convolution with padding\n        node_with_padding = onnx.helper.make_node(\n            'Conv',\n            inputs=['x', 'W'],\n            outputs=['y'],\n            kernel_shape=[3, 3],\n            # Default values for other attributes: strides=[1, 1], dilations=[1, 1], groups=1\n            pads=[1, 1, 1, 1],\n        )\n        y_with_padding = np.array([[[[12., 21., 27., 33., 24.],  # (1, 1, 5, 5) output tensor\n                                     [33., 54., 63., 72., 51.],\n                                     [63., 99., 108., 117., 81.],\n                                     [93., 144., 153., 162., 111.],\n                                     [72., 111., 117., 123., 84.]]]]).astype(np.float32)\n        expect(node_with_padding, inputs=[x, W], outputs=[y_with_padding],\n               name='test_basic_conv_with_padding')\n\n        # Convolution without padding\n        node_without_padding = onnx.helper.make_node(\n            'Conv',\n            inputs=['x', 'W'],\n            outputs=['y'],\n            kernel_shape=[3, 3],\n            # Default values for other attributes: strides=[1, 1], dilations=[1, 1], groups=1\n            pads=[0, 0, 0, 0],\n        )\n        y_without_padding = np.array([[[[54., 63., 72.],  # (1, 1, 3, 3) output tensor\n                                        [99., 108., 117.],\n                                        [144., 153., 162.]]]]).astype(np.float32)\n        expect(node_without_padding, inputs=[x, W], outputs=[y_without_padding],\n               name='test_basic_conv_without_padding')\n\n    @staticmethod\n    def export_conv_with_strides():  # type: () -> None\n\n        x = np.array([[[[0., 1., 2., 3., 4.],  # (1, 1, 7, 5) input tensor\n                        [5., 6., 7., 8., 9.],\n                        [10., 11., 12., 13., 14.],\n                        [15., 16., 17., 18., 19.],\n                        [20., 21., 22., 23., 24.],\n                        [25., 26., 27., 28., 29.],\n                        [30., 31., 32., 33., 34.]]]]).astype(np.float32)\n        W = np.array([[[[1., 1., 1.],  # (1, 1, 3, 3) tensor for convolution weights\n                        [1., 1., 1.],\n                        [1., 1., 1.]]]]).astype(np.float32)\n\n        # Convolution with strides=2 and padding\n        node_with_padding = onnx.helper.make_node(\n            'Conv',\n            inputs=['x', 'W'],\n            outputs=['y'],\n            kernel_shape=[3, 3],\n            pads=[1, 1, 1, 1],\n            strides=[2, 2],  # Default values for other attributes: dilations=[1, 1], groups=1\n        )\n        y_with_padding = np.array([[[[12., 27., 24.],  # (1, 1, 4, 3) output tensor\n                                     [63., 108., 81.],\n                                     [123., 198., 141.],\n                                     [112., 177., 124.]]]]).astype(np.float32)\n        expect(node_with_padding, inputs=[x, W], outputs=[y_with_padding],\n               name='test_conv_with_strides_padding')\n\n        # Convolution with strides=2 and no padding\n        node_without_padding = onnx.helper.make_node(\n            'Conv',\n            inputs=['x', 'W'],\n            outputs=['y'],\n            kernel_shape=[3, 3],\n            pads=[0, 0, 0, 0],\n            strides=[2, 2],  # Default values for other attributes: dilations=[1, 1], groups=1\n        )\n        y_without_padding = np.array([[[[54., 72.],  # (1, 1, 3, 2) output tensor\n                                        [144., 162.],\n                                        [234., 252.]]]]).astype(np.float32)\n        expect(node_without_padding, inputs=[x, W], outputs=[y_without_padding],\n               name='test_conv_with_strides_no_padding')\n\n        # Convolution with strides=2 and padding only along one dimension (the H dimension in NxCxHxW tensor)\n        node_with_asymmetric_padding = onnx.helper.make_node(\n            'Conv',\n            inputs=['x', 'W'],\n            outputs=['y'],\n            kernel_shape=[3, 3],\n            pads=[1, 0, 1, 0],\n            strides=[2, 2],  # Default values for other attributes: dilations=[1, 1], groups=1\n        )\n        y_with_asymmetric_padding = np.array([[[[21., 33.],  # (1, 1, 4, 2) output tensor\n                                                [99., 117.],\n                                                [189., 207.],\n                                                [171., 183.]]]]).astype(np.float32)\n        expect(node_with_asymmetric_padding, inputs=[x, W], outputs=[y_with_asymmetric_padding],\n               name='test_conv_with_strides_and_asymmetric_padding')\n"""
onnx/backend/test/case/node/convinteger.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass ConvInteger(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n\n        x = np.array([2, 3, 4, 5, 6, 7, 8, 9, 10]).astype(np.uint8).reshape((1, 1, 3, 3))\n        x_zero_point = np.uint8(1)\n        w = np.array([1, 1, 1, 1]).astype(np.uint8).reshape((1, 1, 2, 2))\n\n        y = np.array([12, 16, 24, 28]).astype(np.int32).reshape(1, 1, 2, 2)\n\n        # ConvInteger without padding\n        convinteger_node = onnx.helper.make_node('ConvInteger',\n            inputs=['x', 'w', 'x_zero_point'],\n            outputs=['y'])\n\n        expect(convinteger_node, inputs=[x, w, x_zero_point], outputs=[y],\n               name='test_basic_convinteger')\n\n        # ConvInteger with padding\n        y_with_padding = np.array([1, 3, 5, 3, 5, 12, 16, 9, 11, 24, 28, 15, 7, 15, 17, 9]).astype(np.int32).reshape((1, 1, 4, 4))\n\n        convinteger_node_with_padding = onnx.helper.make_node('ConvInteger',\n            inputs=['x', 'w', 'x_zero_point'],\n            outputs=['y'],\n            pads=[1, 1, 1, 1],)\n\n        expect(convinteger_node_with_padding, inputs=[x, w, x_zero_point], outputs=[y_with_padding],\n               name='test_convinteger_with_padding')\n"""
onnx/backend/test/case/node/convtranspose.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass ConvTranspose(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        x = np.array([[[[0., 1., 2.],  # (1, 1, 3, 3)\n                        [3., 4., 5.],\n                        [6., 7., 8.]]]]).astype(np.float32)\n\n        W = np.array([[[[1., 1., 1.],  # (1, 2, 3, 3)\n                        [1., 1., 1.],\n                        [1., 1., 1.]],\n                       [[1., 1., 1.],\n                        [1., 1., 1.],\n                        [1., 1., 1.]]]]).astype(np.float32)\n\n        node = onnx.helper.make_node(""ConvTranspose"", [""X"", ""W""], [""Y""])\n\n        y = np.array([[[[0., 1., 3., 3., 2.],  # (1, 2, 5, 5)\n                        [3., 8., 15., 12., 7.],\n                        [9., 21., 36., 27., 15.],\n                        [9., 20., 33., 24., 13.],\n                        [6., 13., 21., 15., 8.]],\n\n                       [[0., 1., 3., 3., 2.],\n                        [3., 8., 15., 12., 7.],\n                        [9., 21., 36., 27., 15.],\n                        [9., 20., 33., 24., 13.],\n                        [6., 13., 21., 15., 8.]]]]).astype(np.float32)\n\n        expect(node, inputs=[x, W], outputs=[y], name=\'test_convtranspose\')\n\n    @staticmethod\n    def export_convtranspose_1d():  # type: () -> None\n        x = np.array([[[0., 1., 2.]]]).astype(np.float32)  # (1, 1, 3)\n\n        W = np.array([[[1., 1., 1.],  # (1, 2, 3)\n                       [1., 1., 1.]]]).astype(np.float32)\n\n        node = onnx.helper.make_node(""ConvTranspose"", [""X"", ""W""], [""Y""])\n\n        y = np.array([[[0., 1., 3., 3., 2.],  # (1, 2, 5)\n                       [0., 1., 3., 3., 2.]]]).astype(np.float32)\n\n        expect(node, inputs=[x, W], outputs=[y], name=\'test_convtranspose_1d\')\n\n    @staticmethod\n    def export_convtranspose_3d():  # type: () -> None\n        x = np.array([[[[[0., 1., 2., 3., 4.],  # (1, 1, 3, 4, 5)\n                         [5., 6., 7., 8., 9.],\n                         [10., 11., 12., 13., 14.],\n                         [15., 16., 17., 18., 19.]],\n                        [[20., 21., 22., 23., 24.],\n                         [25., 26., 27., 28., 29.],\n                         [30., 31., 32., 33., 34.],\n                         [35., 36., 37., 38., 39.]],\n                        [[40., 41., 42., 43., 44.],\n                         [45., 46., 47., 48., 49.],\n                         [50., 51., 52., 53., 54.],\n                         [55., 56., 57., 58., 59.]]]]]).astype(np.float32)\n\n        W = np.array([[[[[1., 1., 1.],  # (1, 2, 3, 3, 3)\n                         [1., 1., 1.],\n                         [1., 1., 1.]],\n                        [[1., 1., 1.],\n                         [1., 1., 1.],\n                         [1., 1., 1.]],\n                        [[1., 1., 1.],\n                         [1., 1., 1.],\n                         [1., 1., 1.]]],\n                       [[[1., 1., 1.],\n                         [1., 1., 1.],\n                         [1., 1., 1.]],\n                        [[1., 1., 1.],\n                         [1., 1., 1.],\n                         [1., 1., 1.]],\n                        [[1., 1., 1.],\n                         [1., 1., 1.],\n                         [1., 1., 1.]]]]]).astype(np.float32)\n\n        node = onnx.helper.make_node(""ConvTranspose"", [""X"", ""W""], [""Y""])\n\n        y = np.array([[[[[0., 1., 3., 6., 9., 7., 4.],  # (1, 2, 5, 6, 7)\n                         [5., 12., 21., 27., 33., 24., 13.],\n                         [15., 33., 54., 63., 72., 51., 27.],\n                         [30., 63., 99., 108., 117., 81., 42.],\n                         [25., 52., 81., 87., 93., 64., 33.],\n                         [15., 31., 48., 51., 54., 37., 19.]],\n\n                        [[20., 42., 66., 72., 78., 54., 28.],\n                         [50., 104., 162., 174., 186., 128., 66.],\n                         [90., 186., 288., 306., 324., 222., 114.],\n                         [120., 246., 378., 396., 414., 282., 144.],\n                         [90., 184., 282., 294., 306., 208., 106.],\n                         [50., 102., 156., 162., 168., 114., 58.]],\n\n                        [[60., 123., 189., 198., 207., 141., 72.],\n                         [135., 276., 423., 441., 459., 312., 159.],\n                         [225., 459., 702., 729., 756., 513., 261.],\n                         [270., 549., 837., 864., 891., 603., 306.],\n                         [195., 396., 603., 621., 639., 432., 219.],\n                         [105., 213., 324., 333., 342., 231., 117.]],\n\n                        [[60., 122., 186., 192., 198., 134., 68.],\n                         [130., 264., 402., 414., 426., 288., 146.],\n                         [210., 426., 648., 666., 684., 462., 234.],\n                         [240., 486., 738., 756., 774., 522., 264.],\n                         [170., 344., 522., 534., 546., 368., 186.],\n                         [90., 182., 276., 282., 288., 194., 98.]],\n\n                        [[40., 81., 123., 126., 129., 87., 44.],\n                         [85., 172., 261., 267., 273., 184., 93.],\n                         [135., 273., 414., 423., 432., 291., 147.],\n                         [150., 303., 459., 468., 477., 321., 162.],\n                         [105., 212., 321., 327., 333., 224., 113.],\n                         [55., 111., 168., 171., 174., 117., 59.]]],\n\n                       [[[0., 1., 3., 6., 9., 7., 4.],\n                         [5., 12., 21., 27., 33., 24., 13.],\n                         [15., 33., 54., 63., 72., 51., 27.],\n                         [30., 63., 99., 108., 117., 81., 42.],\n                         [25., 52., 81., 87., 93., 64., 33.],\n                         [15., 31., 48., 51., 54., 37., 19.]],\n\n                        [[20., 42., 66., 72., 78., 54., 28.],\n                         [50., 104., 162., 174., 186., 128., 66.],\n                         [90., 186., 288., 306., 324., 222., 114.],\n                         [120., 246., 378., 396., 414., 282., 144.],\n                         [90., 184., 282., 294., 306., 208., 106.],\n                         [50., 102., 156., 162., 168., 114., 58.]],\n\n                        [[60., 123., 189., 198., 207., 141., 72.],\n                         [135., 276., 423., 441., 459., 312., 159.],\n                         [225., 459., 702., 729., 756., 513., 261.],\n                         [270., 549., 837., 864., 891., 603., 306.],\n                         [195., 396., 603., 621., 639., 432., 219.],\n                         [105., 213., 324., 333., 342., 231., 117.]],\n\n                        [[60., 122., 186., 192., 198., 134., 68.],\n                         [130., 264., 402., 414., 426., 288., 146.],\n                         [210., 426., 648., 666., 684., 462., 234.],\n                         [240., 486., 738., 756., 774., 522., 264.],\n                         [170., 344., 522., 534., 546., 368., 186.],\n                         [90., 182., 276., 282., 288., 194., 98.]],\n\n                        [[40., 81., 123., 126., 129., 87., 44.],\n                         [85., 172., 261., 267., 273., 184., 93.],\n                         [135., 273., 414., 423., 432., 291., 147.],\n                         [150., 303., 459., 468., 477., 321., 162.],\n                         [105., 212., 321., 327., 333., 224., 113.],\n                         [55., 111., 168., 171., 174., 117., 59.]]]]]).astype(np.float32)\n\n        expect(node, inputs=[x, W], outputs=[y], name=\'test_convtranspose_3d\')\n\n    @staticmethod\n    def export_convtranspose_attributes():  # type: () -> None\n        x = np.array([[[[0., 1., 2.],  # (1, 1, 3, 3)\n                        [3., 4., 5.],\n                        [6., 7., 8.]]]]).astype(np.float32)\n\n        W = np.array([[[[1., 1., 1.],  # (1, 2, 3, 3)\n                        [1., 1., 1.],\n                        [1., 1., 1.]],\n                       [[1., 1., 1.],\n                        [1., 1., 1.],\n                        [1., 1., 1.]]]]).astype(np.float32)\n\n        y = np.array([[[[0., 0., 1., 1., 3., 2., 2., 0.],  # (1, 2, 10, 8)\n                        [0., 0., 1., 1., 3., 2., 2., 0.],\n                        [0., 0., 1., 1., 3., 2., 2., 0.],\n                        [3., 3., 7., 4., 9., 5., 5., 0.],\n                        [3., 3., 7., 4., 9., 5., 5., 0.],\n                        [3., 3., 7., 4., 9., 5., 5., 0.],\n                        [6., 6., 13., 7., 15., 8., 8., 0.],\n                        [6., 6., 13., 7., 15., 8., 8., 0.],\n                        [6., 6., 13., 7., 15., 8., 8., 0.],\n                        [0., 0., 0., 0., 0., 0., 0., 0.]],\n\n                       [[0., 0., 1., 1., 3., 2., 2., 0.],\n                        [0., 0., 1., 1., 3., 2., 2., 0.],\n                        [0., 0., 1., 1., 3., 2., 2., 0.],\n                        [3., 3., 7., 4., 9., 5., 5., 0.],\n                        [3., 3., 7., 4., 9., 5., 5., 0.],\n                        [3., 3., 7., 4., 9., 5., 5., 0.],\n                        [6., 6., 13., 7., 15., 8., 8., 0.],\n                        [6., 6., 13., 7., 15., 8., 8., 0.],\n                        [6., 6., 13., 7., 15., 8., 8., 0.],\n                        [0., 0., 0., 0., 0., 0., 0., 0.]]]]).astype(np.float32)\n\n        node = onnx.helper.make_node(""ConvTranspose"", [""X"", ""W""], [""Y""],\n                                     strides=[3, 2],\n                                     output_shape=[10, 8])\n        expect(node, inputs=[x, W], outputs=[y], name=\'test_convtranspose_output_shape\')\n\n        node = onnx.helper.make_node(""ConvTranspose"", [""X"", ""W""], [""Y""],\n                                     strides=[3, 2],\n                                     output_padding=[1, 1])\n        expect(node, inputs=[x, W], outputs=[y], name=\'test_convtranspose_pad\')\n\n        node = onnx.helper.make_node(\n            \'ConvTranspose\', [\'X\', \'W\'], [\'Y\'],\n            name=\'test\',\n            strides=[3, 2],\n            output_shape=[10, 8],\n            kernel_shape=[3, 3],\n            output_padding=[1, 1]\n        )\n        expect(node, inputs=[x, W], outputs=[y],\n               name=\'test_convtranspose_kernel_shape\')\n\n    @staticmethod\n    def export_convtranspose_pads():  # type: () -> None\n        x = np.array([[[[0., 1., 2.],  # (1, 1, 3, 3)\n                        [3., 4., 5.],\n                        [6., 7., 8.]]]]).astype(np.float32)\n\n        W = np.array([[[[1., 1., 1.],  # (1, 2, 3, 3)\n                        [1., 1., 1.],\n                        [1., 1., 1.]],\n                       [[1., 1., 1.],\n                        [1., 1., 1.],\n                        [1., 1., 1.]]]]).astype(np.float32)\n\n        node = onnx.helper.make_node(""ConvTranspose"", [""X"", ""W""], [""Y""],\n                                     strides=[3, 2],\n                                     pads=[1, 2, 1, 2])\n\n        y = np.array([[[[1., 1., 3.],  # (1, 2, 7, 3)\n                        [1., 1., 3.],\n                        [7., 4., 9.],\n                        [7., 4., 9.],\n                        [7., 4., 9.],\n                        [13., 7., 15.],\n                        [13., 7., 15.]],\n\n                       [[1., 1., 3.],\n                        [1., 1., 3.],\n                        [7., 4., 9.],\n                        [7., 4., 9.],\n                        [7., 4., 9.],\n                        [13., 7., 15.],\n                        [13., 7., 15.]]]]).astype(np.float32)\n\n        expect(node, inputs=[x, W], outputs=[y], name=\'test_convtranspose_pads\')\n\n    @staticmethod\n    def export_convtranspose_dilations():  # type: () -> None\n        x = np.array([[[[3., 8., 1.],  # (1, 1, 3, 3)\n                        [9., 5., 7.],\n                        [3., 2., 6.]]]]).astype(np.float32)\n        W = np.array([[[[7., 2.],  # (1, 1, 2, 2)\n                        [1., 9.]]]]).astype(np.float32)\n\n        node = onnx.helper.make_node(""ConvTranspose"", [""X"", ""W""], [""Y""], dilations=[2, 2])\n\n        y = np.array([[[[21., 56., 13., 16., 2.],  # [1, 1, 5, 5]\n                        [63., 35., 67., 10., 14.],\n                        [24., 22., 76., 76., 21.],\n                        [9., 5., 88., 45., 63.],\n                        [3., 2., 33., 18., 54.]]]]).astype(np.float32)\n\n        expect(node, inputs=[x, W], outputs=[y], name=\'test_convtranspose_dilations\')\n'"
onnx/backend/test/case/node/cos.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Cos(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Cos',\n            inputs=['x'],\n            outputs=['y'],\n        )\n\n        x = np.array([-1, 0, 1]).astype(np.float32)\n        y = np.cos(x)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_cos_example')\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.cos(x)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_cos')\n"""
onnx/backend/test/case/node/cosh.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Cosh(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Cosh',\n            inputs=['x'],\n            outputs=['y'],\n        )\n\n        x = np.array([-1, 0, 1]).astype(np.float32)\n        y = np.cosh(x)  # expected output [1.54308069,  1.,  1.54308069]\n        expect(node, inputs=[x], outputs=[y],\n               name='test_cosh_example')\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.cosh(x)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_cosh')\n"""
onnx/backend/test/case/node/cumsum.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass CumSum(Base):\n\n    @staticmethod\n    def export_cumsum_1d():  # type: () -> None\n        node = onnx.helper.make_node(\n            'CumSum',\n            inputs=['x', 'axis'],\n            outputs=['y']\n        )\n        x = np.array([1., 2., 3., 4., 5.]).astype(np.float64)\n        axis = np.array([0]).astype(np.int32)\n        y = np.array([1., 3., 6., 10., 15.]).astype(np.float64)\n        expect(node, inputs=[x, axis], outputs=[y],\n               name='test_cumsum_1d')\n\n    @staticmethod\n    def export_cumsum_1d_exclusive():  # type: () -> None\n        node = onnx.helper.make_node(\n            'CumSum',\n            inputs=['x', 'axis'],\n            outputs=['y'],\n            exclusive=1\n        )\n        x = np.array([1., 2., 3., 4., 5.]).astype(np.float64)\n        axis = np.array([0]).astype(np.int32)\n        y = np.array([0., 1., 3., 6., 10.]).astype(np.float64)\n        expect(node, inputs=[x, axis], outputs=[y],\n               name='test_cumsum_1d_exclusive')\n\n    @staticmethod\n    def export_cumsum_1d_reverse():  # type: () -> None\n        node = onnx.helper.make_node(\n            'CumSum',\n            inputs=['x', 'axis'],\n            outputs=['y'],\n            reverse=1\n        )\n        x = np.array([1., 2., 3., 4., 5.]).astype(np.float64)\n        axis = np.array([0]).astype(np.int32)\n        y = np.array([15., 14., 12., 9., 5.]).astype(np.float64)\n        expect(node, inputs=[x, axis], outputs=[y],\n               name='test_cumsum_1d_reverse')\n\n    @staticmethod\n    def export_cumsum_1d_reverse_exclusive():  # type: () -> None\n        node = onnx.helper.make_node(\n            'CumSum',\n            inputs=['x', 'axis'],\n            outputs=['y'],\n            reverse=1\n        )\n        x = np.array([1., 2., 3., 4., 5.]).astype(np.float64)\n        axis = np.array([0]).astype(np.int32)\n        y = np.array([14., 12., 9., 5., 0.]).astype(np.float64)\n        expect(node, inputs=[x, axis], outputs=[y],\n               name='test_cumsum_1d_reverse_exclusive')\n\n    @staticmethod\n    def export_cumsum_2d_axis_0():  # type: () -> None\n        node = onnx.helper.make_node(\n            'CumSum',\n            inputs=['x', 'axis'],\n            outputs=['y'],\n        )\n        x = np.array([1., 2., 3., 4., 5., 6.]).astype(np.float64).reshape((2, 3))\n        axis = np.array([0]).astype(np.int32)\n        y = np.array([1., 2., 3., 5., 7., 9.]).astype(np.float64).reshape((2, 3))\n        expect(node, inputs=[x, axis], outputs=[y],\n               name='test_cumsum_2d_axis_0')\n\n    @staticmethod\n    def export_cumsum_2d_axis_1():  # type: () -> None\n        node = onnx.helper.make_node(\n            'CumSum',\n            inputs=['x', 'axis'],\n            outputs=['y'],\n        )\n        x = np.array([1., 2., 3., 4., 5., 6.]).astype(np.float64).reshape((2, 3))\n        axis = np.array([1]).astype(np.int32)\n        y = np.array([1., 3., 6., 4., 9., 15.]).astype(np.float64).reshape((2, 3))\n        expect(node, inputs=[x, axis], outputs=[y],\n               name='test_cumsum_2d_axis_1')\n\n    @staticmethod\n    def export_cumsum_2d_negative_axis():  # type: () -> None\n        node = onnx.helper.make_node(\n            'CumSum',\n            inputs=['x', 'axis'],\n            outputs=['y'],\n        )\n        x = np.array([1., 2., 3., 4., 5., 6.]).astype(np.float64).reshape((2, 3))\n        axis = np.array([-1]).astype(np.int32)\n        y = np.array([1., 3., 6., 4., 9., 15.]).astype(np.float64).reshape((2, 3))\n        expect(node, inputs=[x, axis], outputs=[y],\n               name='test_cumsum_2d_negative_axis')\n"""
onnx/backend/test/case/node/depthtospace.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass DepthToSpace(Base):\n\n    @staticmethod\n    def export_default_mode_example():  # type: () -> None\n        node = onnx.helper.make_node(\n            'DepthToSpace',\n            inputs=['x'],\n            outputs=['y'],\n            blocksize=2,\n            mode='DCR'\n        )\n\n        # (1, 8, 2, 3) input tensor\n        x = np.array([[[[0., 1., 2.],\n                        [3., 4., 5.]],\n                       [[9., 10., 11.],\n                        [12., 13., 14.]],\n                       [[18., 19., 20.],\n                        [21., 22., 23.]],\n                       [[27., 28., 29.],\n                        [30., 31., 32.]],\n                       [[36., 37., 38.],\n                        [39., 40., 41.]],\n                       [[45., 46., 47.],\n                        [48., 49., 50.]],\n                       [[54., 55., 56.],\n                        [57., 58., 59.]],\n                       [[63., 64., 65.],\n                        [66., 67., 68.]]]]).astype(np.float32)\n\n        # (1, 2, 4, 6) output tensor\n        y = np.array([[[[0., 18., 1., 19., 2., 20.],\n                        [36., 54., 37., 55., 38., 56.],\n                        [3., 21., 4., 22., 5., 23.],\n                        [39., 57., 40., 58., 41., 59.]],\n                       [[9., 27., 10., 28., 11., 29.],\n                        [45., 63., 46., 64., 47., 65.],\n                        [12., 30., 13., 31., 14., 32.],\n                        [48., 66., 49., 67., 50., 68.]]]]).astype(np.float32)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_depthtospace_example')\n\n    @staticmethod\n    def export_crd_mode_example():  # type: () -> None\n        node = onnx.helper.make_node(\n            'DepthToSpace',\n            inputs=['x'],\n            outputs=['y'],\n            blocksize=2,\n            mode='CRD'\n        )\n\n        # (1, 8, 2, 3) input tensor\n        x = np.array([[[[0., 1., 2.],\n                        [3., 4., 5.]],\n                       [[9., 10., 11.],\n                        [12., 13., 14.]],\n                       [[18., 19., 20.],\n                        [21., 22., 23.]],\n                       [[27., 28., 29.],\n                        [30., 31., 32.]],\n                       [[36., 37., 38.],\n                        [39., 40., 41.]],\n                       [[45., 46., 47.],\n                        [48., 49., 50.]],\n                       [[54., 55., 56.],\n                        [57., 58., 59.]],\n                       [[63., 64., 65.],\n                        [66., 67., 68.]]]]).astype(np.float32)\n\n        # (1, 2, 4, 6) output tensor\n        y = np.array([[[[0., 9., 1., 10., 2., 11.],\n                        [18., 27., 19., 28., 20., 29.],\n                        [3., 12., 4., 13., 5., 14.],\n                        [21., 30., 22., 31., 23., 32.]],\n                       [[36., 45., 37., 46., 38., 47.],\n                        [54., 63., 55., 64., 56., 65.],\n                        [39., 48., 40., 49., 41., 50.],\n                        [57., 66., 58., 67., 59., 68.]]]]).astype(np.float32)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_depthtospace_crd_mode_example')\n"""
onnx/backend/test/case/node/dequantizelinear.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass DequantizeLinear(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node('DequantizeLinear',\n            inputs=['x', 'x_scale', 'x_zero_point'],\n            outputs=['y'],)\n\n        # scalar zero point and scale\n        x = np.array([0, 3, 128, 255]).astype(np.uint8)\n        x_scale = np.float32(2)\n        x_zero_point = np.uint8(128)\n        y = np.array([-256, -250, 0, 254], dtype=np.float32)\n\n        expect(node, inputs=[x, x_scale, x_zero_point], outputs=[y],\n               name='test_dequantizelinear')\n"""
onnx/backend/test/case/node/det.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport math\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Det(Base):\n\n    @staticmethod\n    def export_2d():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Det',\n            inputs=['x'],\n            outputs=['y'],\n        )\n\n        x = np.arange(4).reshape(2, 2).astype(np.float32)\n        y = np.linalg.det(x)  # expect -2\n        expect(node, inputs=[x], outputs=[y],\n               name='test_det_2d')\n\n    @staticmethod\n    def export_nd():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Det',\n            inputs=['x'],\n            outputs=['y'],\n        )\n\n        x = np.array([[[1, 2], [3, 4]], [[1, 2], [2, 1]], [[1, 3], [3, 1]]]).astype(np.float32)\n        y = np.linalg.det(x)  # expect array([-2., -3., -8.])\n        expect(node, inputs=[x], outputs=[y],\n               name='test_det_nd')\n"""
onnx/backend/test/case/node/div.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Div(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Div',\n            inputs=['x', 'y'],\n            outputs=['z'],\n        )\n\n        x = np.array([3, 4]).astype(np.float32)\n        y = np.array([1, 2]).astype(np.float32)\n        z = x / y  # expected output [3., 2.]\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_div_example')\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.random.rand(3, 4, 5).astype(np.float32) + 1.0\n        z = x / y\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_div')\n\n    @staticmethod\n    def export_div_broadcast():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Div',\n            inputs=['x', 'y'],\n            outputs=['z'],\n        )\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.random.rand(5).astype(np.float32) + 1.0\n        z = x / y\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_div_bcast')\n"""
onnx/backend/test/case/node/dropout.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\nimport random\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\nfrom onnx import helper\n\n\ndef dropout(X, drop_probability=0.5, seed=0, training_mode=False, return_mask=False):  # type: ignore\n    if drop_probability == 0 or training_mode is False:\n        if return_mask is True:\n            return X, np.ones(X.shape, dtype=bool)\n        else:\n            return X\n\n    np.random.seed(seed)\n    mask = np.random.uniform(0, 1.0, X.shape) >= drop_probability\n    scale = (1 / (1 - drop_probability))\n    if return_mask is True:\n        return mask * X * scale, mask.astype(bool)\n    else:\n        return mask * X * scale\n\n\nclass Dropout(Base):\n\n    # Inferencing tests.\n    @staticmethod\n    def export_default():  # type: () -> None\n        seed = np.int64(0)\n        node = onnx.helper.make_node(\n            \'Dropout\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            seed=seed\n        )\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = dropout(x)\n        expect(node, inputs=[x], outputs=[y], name=\'test_dropout_default\')\n\n    @staticmethod\n    def export_default_ratio():  # type: () -> None\n        seed = np.int64(0)\n        node = onnx.helper.make_node(\n            \'Dropout\',\n            inputs=[\'x\', \'r\'],\n            outputs=[\'y\'],\n            seed=seed\n        )\n\n        r = np.float32(0.1)\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = dropout(x, r)\n        expect(node, inputs=[x, r], outputs=[y], name=\'test_dropout_default_ratio\')\n\n    @staticmethod\n    def export_default_mask():  # type: () -> None\n        seed = np.int64(0)\n        node = onnx.helper.make_node(\n            \'Dropout\',\n            inputs=[\'x\'],\n            outputs=[\'y\', \'z\'],\n            seed=seed\n        )\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y, z = dropout(x, return_mask=True)\n        expect(node, inputs=[x], outputs=[y, z], name=\'test_dropout_default_mask\')\n\n    @staticmethod\n    def export_default_mask_ratio():  # type: () -> None\n        seed = np.int64(0)\n        node = onnx.helper.make_node(\n            \'Dropout\',\n            inputs=[\'x\', \'r\'],\n            outputs=[\'y\', \'z\'],\n            seed=seed\n        )\n\n        r = np.float32(0.1)\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y, z = dropout(x, r, return_mask=True)\n        expect(node, inputs=[x, r], outputs=[y, z], name=\'test_dropout_default_mask_ratio\')\n\n    # Training tests.\n\n    @staticmethod\n    def export_training_default():  # type: () -> None\n        seed = np.int64(0)\n        node = onnx.helper.make_node(\n            \'Dropout\',\n            inputs=[\'x\', \'r\', \'t\'],\n            outputs=[\'y\'],\n            seed=seed\n        )\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        r = np.float32(0.5)\n        t = np.bool_(True)\n        y = dropout(x, r, training_mode=t)\n        expect(node, inputs=[x, r, t], outputs=[y], name=\'test_training_dropout_default\')\n\n    @staticmethod\n    def export_training_default_ratio_mask():  # type: () -> None\n        seed = np.int64(0)\n        node = onnx.helper.make_node(\n            \'Dropout\',\n            inputs=[\'x\', \'r\', \'t\'],\n            outputs=[\'y\', \'z\'],\n            seed=seed\n        )\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        r = np.float32(0.5)\n        t = np.bool_(True)\n        y, z = dropout(x, r, training_mode=t, return_mask=True)\n        expect(node, inputs=[x, r, t], outputs=[y, z], name=\'test_training_dropout_default_mask\')\n\n    @staticmethod\n    def export_training():  # type: () -> None\n        seed = np.int64(0)\n        node = onnx.helper.make_node(\n            \'Dropout\',\n            inputs=[\'x\', \'r\', \'t\'],\n            outputs=[\'y\'],\n            seed=seed\n        )\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        r = np.float32(0.75)\n        t = np.bool_(True)\n        y = dropout(x, r, training_mode=t)\n        expect(node, inputs=[x, r, t], outputs=[y], name=\'test_training_dropout\')\n\n    @staticmethod\n    def export_training_ratio_mask():  # type: () -> None\n        seed = np.int64(0)\n        node = onnx.helper.make_node(\n            \'Dropout\',\n            inputs=[\'x\', \'r\', \'t\'],\n            outputs=[\'y\', \'z\'],\n            seed=seed\n        )\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        r = np.float32(0.75)\n        t = np.bool_(True)\n        y, z = dropout(x, r, training_mode=t, return_mask=True)\n        expect(node, inputs=[x, r, t], outputs=[y, z], name=\'test_training_dropout_mask\')\n\n    @staticmethod\n    def export_training_default_zero_ratio():  # type: () -> None\n        seed = np.int64(0)\n        node = onnx.helper.make_node(\n            \'Dropout\',\n            inputs=[\'x\', \'r\', \'t\'],\n            outputs=[\'y\'],\n            seed=seed\n        )\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        r = np.float32(0.0)\n        t = np.bool_(True)\n        y = dropout(x, r, training_mode=t)\n        expect(node, inputs=[x, r, t], outputs=[y], name=\'test_training_dropout_zero_ratio\')\n\n    @staticmethod\n    def export_training_default_zero_ratio_mask():  # type: () -> None\n        seed = np.int64(0)\n        node = onnx.helper.make_node(\n            \'Dropout\',\n            inputs=[\'x\', \'r\', \'t\'],\n            outputs=[\'y\', \'z\'],\n            seed=seed\n        )\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        r = np.float32(0.0)\n        t = np.bool_(True)\n        y, z = dropout(x, r, training_mode=t, return_mask=True)\n        expect(node, inputs=[x, r, t], outputs=[y, z], name=\'test_training_dropout_zero_ratio_mask\')\n\n    # Old dropout tests\n\n    @staticmethod\n    def export_default_old():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'Dropout\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n        )\n\n        x = np.array([-1, 0, 1]).astype(np.float32)\n        y = x\n        expect(node, inputs=[x], outputs=[y],\n               name=\'test_dropout_default_old\', opset_imports=[helper.make_opsetid("""", 11)])\n\n    @staticmethod\n    def export_random_old():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'Dropout\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            ratio=.2,\n        )\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = x\n        expect(node, inputs=[x], outputs=[y],\n               name=\'test_dropout_random_old\', opset_imports=[helper.make_opsetid("""", 11)])\n'"
onnx/backend/test/case/node/dynamicquantizelinear.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom onnx import TensorProto\nfrom ..base import Base\nfrom . import expect\n\n\nclass DynamicQuantizeLinear(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node('DynamicQuantizeLinear',\n            inputs=['x'],\n            outputs=['y', 'y_scale', 'y_zero_point'],\n        )\n\n        # expected scale 0.0196078438 and zero point 153\n        X = np.array([0, 2, -3, -2.5, 1.34, 0.5]).astype(np.float32)\n        x_min = np.minimum(0, np.min(X))\n        x_max = np.maximum(0, np.max(X))\n        Y_Scale = np.float32((x_max - x_min) / (255 - 0))  # uint8 -> [0, 255]\n        Y_ZeroPoint = np.clip(round((0 - x_min) / Y_Scale), 0, 255).astype(np.uint8)\n        Y = np.clip(np.round(X / Y_Scale) + Y_ZeroPoint, 0, 255).astype(np.uint8)\n\n        expect(node, inputs=[X], outputs=[Y, Y_Scale, Y_ZeroPoint],\n               name='test_dynamicquantizelinear')\n\n        # expected scale 0.0156862754 and zero point 255\n        X = np.array([-1.0, -2.1, -1.3, -2.5, -3.34, -4.0]).astype(np.float32)\n        x_min = np.minimum(0, np.min(X))\n        x_max = np.maximum(0, np.max(X))\n        Y_Scale = np.float32((x_max - x_min) / (255 - 0))  # uint8 -> [0, 255]\n        Y_ZeroPoint = np.clip(round((0 - x_min) / Y_Scale), 0, 255).astype(np.uint8)\n        Y = np.clip(np.round(X / Y_Scale) + Y_ZeroPoint, 0, 255).astype(np.uint8)\n\n        expect(node, inputs=[X], outputs=[Y, Y_Scale, Y_ZeroPoint],\n               name='test_dynamicquantizelinear_max_adjusted')\n\n        X = np.array([1, 2.1, 1.3, 2.5,\n                      3.34, 4.0, 1.5, 2.6,\n                      3.9, 4.0, 3.0, 2.345]).astype(np.float32).reshape((3, 4))\n\n        # expected scale 0.0156862754 and zero point 0\n        x_min = np.minimum(0, np.min(X))\n        x_max = np.maximum(0, np.max(X))\n        Y_Scale = np.float32((x_max - x_min) / (255 - 0))  # uint8 -> [0, 255]\n        Y_ZeroPoint = np.clip(round((0 - x_min) / Y_Scale), 0, 255).astype(np.uint8)\n        Y = np.clip(np.round(X / Y_Scale) + Y_ZeroPoint, 0, 255).astype(np.uint8)\n\n        expect(node, inputs=[X], outputs=[Y, Y_Scale, Y_ZeroPoint],\n               name='test_dynamicquantizelinear_min_adjusted')\n"""
onnx/backend/test/case/node/einsum.py,0,"b""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\nfrom typing import Tuple, Text\n\n\ndef einsum_reference_implementation(Eqn, Operands):  # type: (Text, Tuple[np.ndarray, ...]) -> np.ndarray\n    Z = np.einsum(Eqn, *Operands)\n    return Z\n\n\nclass Einsum(Base):\n\n    @staticmethod\n    def export_einsum_transpose():  # type: () -> None\n        Eqn = 'ij->ji'\n        node = onnx.helper.make_node(\n            'Einsum',\n            inputs=['x'],\n            outputs=['y'],\n            equation=Eqn\n        )\n\n        X = np.random.randn(3, 4)\n        Y = einsum_reference_implementation(Eqn, (X,))\n\n        expect(node, inputs=[X], outputs=[Y], name='test_einsum_transpose')\n\n    @staticmethod\n    def export_einsum_sum():  # type: () -> None\n        Eqn = 'ij->i'\n        node = onnx.helper.make_node(\n            'Einsum',\n            inputs=['x'],\n            outputs=['y'],\n            equation=Eqn\n        )\n\n        X = np.random.randn(3, 4)\n        Z = einsum_reference_implementation(Eqn, (X,))\n\n        expect(node, inputs=[X], outputs=[Z], name='test_einsum_sum')\n\n    @staticmethod\n    def export_einsum_batch_diagonal():  # type: () -> None\n        Eqn = '...ii ->...i'\n        node = onnx.helper.make_node(\n            'Einsum',\n            inputs=['x'],\n            outputs=['y'],\n            equation=Eqn\n        )\n\n        X = np.random.randn(3, 5, 5)\n        Z = einsum_reference_implementation(Eqn, (X,))\n\n        expect(node, inputs=[X], outputs=[Z], name='test_einsum_batch_diagonal')\n\n    @staticmethod\n    def export_einsum_inner_prod():  # type: () -> None\n        Eqn = 'i,i'\n        node = onnx.helper.make_node(\n            'Einsum',\n            inputs=['x', 'y'],\n            outputs=['z'],\n            equation=Eqn\n        )\n\n        X = np.random.randn(5)\n        Y = np.random.randn(5)\n        Z = einsum_reference_implementation(Eqn, (X, Y))\n\n        expect(node, inputs=[X, Y], outputs=[Z], name='test_einsum_inner_prod')\n\n    @staticmethod\n    def export_einsum_batch_matmul():  # type: () -> None\n        Eqn = 'bij, bjk -> bik'\n        node = onnx.helper.make_node(\n            'Einsum',\n            inputs=['x', 'y'],\n            outputs=['z'],\n            equation=Eqn\n        )\n\n        X = np.random.randn(5, 2, 3)\n        Y = np.random.randn(5, 3, 4)\n        Z = einsum_reference_implementation(Eqn, (X, Y))\n\n        expect(node, inputs=[X, Y], outputs=[Z], name='test_einsum_batch_matmul')\n"""
onnx/backend/test/case/node/elu.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Elu(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Elu',\n            inputs=['x'],\n            outputs=['y'],\n            alpha=2.0\n        )\n\n        x = np.array([-1, 0, 1]).astype(np.float32)\n        # expected output [-1.2642411, 0., 1.]\n        y = np.clip(x, 0, np.inf) + (np.exp(np.clip(x, -np.inf, 0)) - 1) * 2.0\n        expect(node, inputs=[x], outputs=[y],\n               name='test_elu_example')\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.clip(x, 0, np.inf) + (np.exp(np.clip(x, -np.inf, 0)) - 1) * 2.0\n        expect(node, inputs=[x], outputs=[y],\n               name='test_elu')\n\n    @staticmethod\n    def export_elu_default():  # type: () -> None\n        default_alpha = 1.0\n        node = onnx.helper.make_node(\n            'Elu',\n            inputs=['x'],\n            outputs=['y'],\n        )\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.clip(x, 0, np.inf) + (np.exp(np.clip(x, -np.inf, 0)) - 1) * default_alpha\n        expect(node, inputs=[x], outputs=[y],\n               name='test_elu_default')\n"""
onnx/backend/test/case/node/equal.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Equal(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Equal',\n            inputs=['x', 'y'],\n            outputs=['z'],\n        )\n\n        x = (np.random.randn(3, 4, 5) * 10).astype(np.int32)\n        y = (np.random.randn(3, 4, 5) * 10).astype(np.int32)\n        z = np.equal(x, y)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_equal')\n\n    @staticmethod\n    def export_equal_broadcast():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Equal',\n            inputs=['x', 'y'],\n            outputs=['z'],\n        )\n\n        x = (np.random.randn(3, 4, 5) * 10).astype(np.int32)\n        y = (np.random.randn(5) * 10).astype(np.int32)\n        z = np.equal(x, y)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_equal_bcast')\n"""
onnx/backend/test/case/node/erf.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport math\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Erf(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Erf',\n            inputs=['x'],\n            outputs=['y'],\n        )\n\n        x = np.random.randn(1, 3, 32, 32).astype(np.float32)\n        y = np.vectorize(math.erf)(x).astype(np.float32)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_erf')\n"""
onnx/backend/test/case/node/exp.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Exp(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Exp',\n            inputs=['x'],\n            outputs=['y'],\n        )\n\n        x = np.array([-1, 0, 1]).astype(np.float32)\n        y = np.exp(x)  # expected output [0.36787945, 1., 2.71828175]\n        expect(node, inputs=[x], outputs=[y],\n               name='test_exp_example')\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.exp(x)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_exp')\n"""
onnx/backend/test/case/node/expand.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Expand(Base):\n\n    @staticmethod\n    def export_dim_changed():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Expand',\n            inputs=['data', 'new_shape'],\n            outputs=['expanded'],\n        )\n        shape = [3, 1]\n        data = np.reshape(np.arange(1, np.prod(shape) + 1, dtype=np.float32), shape)\n        #print(data)\n        #[[1.], [2.], [3.]]\n        new_shape = [2, 1, 6]\n        expanded = data * np.ones(new_shape, dtype=np.float32)\n        #print(expanded)\n        #[[[1., 1., 1., 1., 1., 1.],\n        #  [2., 2., 2., 2., 2., 2.],\n        #  [3., 3., 3., 3., 3., 3.]],\n        #\n        # [[1., 1., 1., 1., 1., 1.],\n        #  [2., 2., 2., 2., 2., 2.],\n        #  [3., 3., 3., 3., 3., 3.]]]\n        new_shape = np.array(new_shape, dtype=np.int64)\n        expect(node, inputs=[data, new_shape], outputs=[expanded],\n               name='test_expand_dim_changed')\n\n    @staticmethod\n    def export_dim_unchanged():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Expand',\n            inputs=['data', 'new_shape'],\n            outputs=['expanded'],\n        )\n        shape = [3, 1]\n        new_shape = [3, 4]\n        data = np.reshape(np.arange(1, np.prod(shape) + 1, dtype=np.float32), shape)\n        #print(data)\n        #[[1.], [2.], [3.]]\n        expanded = np.tile(data, 4)\n        #print(expanded)\n        #[[1., 1., 1., 1.],\n        # [2., 2., 2., 2.],\n        # [3., 3., 3., 3.]]\n        new_shape = np.array(new_shape, dtype=np.int64)\n        expect(node, inputs=[data, new_shape], outputs=[expanded],\n               name='test_expand_dim_unchanged')\n"""
onnx/backend/test/case/node/eyelike.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass EyeLike(Base):\n\n    @staticmethod\n    def export_without_dtype():  # type: () -> None\n        shape = (4, 4)\n        node = onnx.helper.make_node(\n            'EyeLike',\n            inputs=['x'],\n            outputs=['y'],\n        )\n\n        x = np.random.randint(0, 100, size=shape, dtype=np.int32)\n        y = np.eye(shape[0], shape[1], dtype=np.int32)\n        expect(node, inputs=[x], outputs=[y], name='test_eyelike_without_dtype')\n\n    @staticmethod\n    def export_with_dtype():  # type: () -> None\n        shape = (3, 4)\n        node = onnx.helper.make_node(\n            'EyeLike',\n            inputs=['x'],\n            outputs=['y'],\n            dtype=onnx.TensorProto.DOUBLE,\n        )\n\n        x = np.random.randint(0, 100, size=shape, dtype=np.int32)\n        y = np.eye(shape[0], shape[1], dtype=np.float64)\n        expect(node, inputs=[x], outputs=[y], name='test_eyelike_with_dtype')\n\n    @staticmethod\n    def export_populate_off_main_diagonal():  # type: () -> None\n        shape = (4, 5)\n        off_diagonal_offset = 1\n        node = onnx.helper.make_node(\n            'EyeLike',\n            inputs=['x'],\n            outputs=['y'],\n            k=off_diagonal_offset,\n            dtype=onnx.TensorProto.FLOAT,\n        )\n\n        x = np.random.randint(0, 100, size=shape, dtype=np.int32)\n        y = np.eye(shape[0], shape[1], k=off_diagonal_offset, dtype=np.float32)\n        expect(node, inputs=[x], outputs=[y], name='test_eyelike_populate_off_main_diagonal')\n"""
onnx/backend/test/case/node/flatten.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Flatten(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        shape = (2, 3, 4, 5)\n        a = np.random.random_sample(shape).astype(np.float32)\n\n        for i in range(len(shape)):\n            node = onnx.helper.make_node(\n                'Flatten',\n                inputs=['a'],\n                outputs=['b'],\n                axis=i,\n            )\n\n            new_shape = (1, -1) if i == 0 else (np.prod(shape[0:i]).astype(int), -1)\n            b = np.reshape(a, new_shape)\n            expect(node, inputs=[a], outputs=[b],\n                   name='test_flatten_axis' + str(i))\n\n    @staticmethod\n    def export_flatten_with_default_axis():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Flatten',\n            inputs=['a'],\n            outputs=['b'],  # Default value for axis: axis=1\n        )\n\n        shape = (5, 4, 3, 2)\n        a = np.random.random_sample(shape).astype(np.float32)\n        new_shape = (5, 24)\n        b = np.reshape(a, new_shape)\n        expect(node, inputs=[a], outputs=[b],\n               name='test_flatten_default_axis')\n\n    @staticmethod\n    def export_flatten_negative_axis():  # type: () -> None\n        shape = (2, 3, 4, 5)\n        a = np.random.random_sample(shape).astype(np.float32)\n\n        for i in range(-len(shape), 0):\n            node = onnx.helper.make_node(\n                'Flatten',\n                inputs=['a'],\n                outputs=['b'],\n                axis=i,\n            )\n\n            new_shape = (np.prod(shape[0:i]).astype(int), -1)\n            b = np.reshape(a, new_shape)\n            expect(node, inputs=[a], outputs=[b],\n                   name='test_flatten_negative_axis' + str(abs(i)))\n"""
onnx/backend/test/case/node/floor.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Floor(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Floor',\n            inputs=['x'],\n            outputs=['y'],\n        )\n\n        x = np.array([-1.5, 1.2, 2]).astype(np.float32)\n        y = np.floor(x)  # expected output [-2., 1., 2.]\n        expect(node, inputs=[x], outputs=[y],\n               name='test_floor_example')\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.floor(x)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_floor')\n"""
onnx/backend/test/case/node/gather.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Gather(Base):\n\n    @staticmethod\n    def export_gather_0():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Gather',\n            inputs=['data', 'indices'],\n            outputs=['y'],\n            axis=0,\n        )\n        data = np.random.randn(5, 4, 3, 2).astype(np.float32)\n        indices = np.array([0, 1, 3])\n        y = np.take(data, indices, axis=0)\n\n        expect(node, inputs=[data, indices.astype(np.int64)], outputs=[y],\n               name='test_gather_0')\n\n    @staticmethod\n    def export_gather_1():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Gather',\n            inputs=['data', 'indices'],\n            outputs=['y'],\n            axis=1,\n        )\n        data = np.random.randn(5, 4, 3, 2).astype(np.float32)\n        indices = np.array([0, 1, 3])\n        y = np.take(data, indices, axis=1)\n\n        expect(node, inputs=[data, indices.astype(np.int64)], outputs=[y],\n               name='test_gather_1')\n\n    @staticmethod\n    def export_gather_negative_indices():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Gather',\n            inputs=['data', 'indices'],\n            outputs=['y'],\n            axis=0,\n        )\n        data = np.arange(10).astype(np.float32)\n        indices = np.array([0, -9, -10])\n        y = np.take(data, indices, axis=0)\n\n        expect(node, inputs=[data, indices.astype(np.int64)], outputs=[y],\n               name='test_gather_negative_indices')\n\n        # print(y)\n        # [0. 1. 0.]\n"""
onnx/backend/test/case/node/gatherelements.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\n# The below GatherElements' numpy implementation is from https://stackoverflow.com/a/46204790/11767360\ndef gather_elements(data, indices, axis=0):  # type: ignore\n    data_swaped = np.swapaxes(data, 0, axis)\n    index_swaped = np.swapaxes(indices, 0, axis)\n    gathered = np.choose(index_swaped, data_swaped, mode='wrap')\n    y = np.swapaxes(gathered, 0, axis)\n    return y\n\n\nclass GatherElements(Base):\n\n    @staticmethod\n    def export_gather_elements_0():  # type: () -> None\n        axis = 1\n        node = onnx.helper.make_node(\n            'GatherElements',\n            inputs=['data', 'indices'],\n            outputs=['y'],\n            axis=axis,\n        )\n        data = np.array([[1, 2],\n                         [3, 4]], dtype=np.float32)\n        indices = np.array([[0, 0],\n                            [1, 0]], dtype=np.int32)\n\n        y = gather_elements(data, indices, axis)\n        # print(y) produces\n        # [[1, 1],\n        #  [4, 3]]\n\n        expect(node, inputs=[data, indices.astype(np.int64)], outputs=[y],\n               name='test_gather_elements_0')\n\n    @staticmethod\n    def export_gather_elements_1():  # type: () -> None\n        axis = 0\n        node = onnx.helper.make_node(\n            'GatherElements',\n            inputs=['data', 'indices'],\n            outputs=['y'],\n            axis=axis,\n        )\n        data = np.array([[1, 2, 3],\n                         [4, 5, 6],\n                         [7, 8, 9]], dtype=np.float32)\n        indices = np.array([[1, 2, 0],\n                            [2, 0, 0]], dtype=np.int32)\n\n        y = gather_elements(data, indices, axis)\n        # print(y) produces\n        # [[4, 8, 3],\n        #  [7, 2, 3]]\n\n        expect(node, inputs=[data, indices.astype(np.int64)], outputs=[y],\n               name='test_gather_elements_1')\n\n    @staticmethod\n    def export_gather_elements_negative_indices():  # type: () -> None\n        axis = 0\n        node = onnx.helper.make_node(\n            'GatherElements',\n            inputs=['data', 'indices'],\n            outputs=['y'],\n            axis=axis,\n        )\n        data = np.array([[1, 2, 3],\n                         [4, 5, 6],\n                         [7, 8, 9]], dtype=np.float32)\n        indices = np.array([[-1, -2, 0],\n                            [-2, 0, 0]], dtype=np.int32)\n\n        y = gather_elements(data, indices, axis)\n        # print(y) produces\n        # [[7, 5, 3],\n        #  [4, 2, 3]]\n\n        expect(node, inputs=[data, indices.astype(np.int64)], outputs=[y],\n               name='test_gather_elements_negative_indices')\n"""
onnx/backend/test/case/node/gathernd.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\ndef gather_nd_impl(data, indices, batch_dims):\n    # type: (np.ndarray, np.ndarray, int) -> np.ndarray\n    # Note the data rank - will be reused multiple times later\n    data_rank = len(data.shape)\n\n    # Check input tensors' shape/rank condition\n    assert indices.shape[-1] <= data_rank\n\n    #The list of data/indice shape of batch_dims\n    batch_dims_shape = []\n\n    #The number of elements in the batch_dims for data/indice array\n    batch_dims_size = 1\n\n    # Check the shape of indice and data are identicial for batch dims.\n    for i in range(batch_dims):\n        batch_dims_shape.append(indices.shape[i])\n        batch_dims_size *= indices.shape[i]\n\n    # Compute output of the op as below\n\n    # Compute shape of output array\n    output_shape = batch_dims_shape + list(indices.shape)[batch_dims:-1] if (indices.shape[-1] == data_rank - batch_dims) \\\n     else batch_dims_shape + list(indices.shape)[batch_dims:-1] + list(data.shape)[batch_dims + indices.shape[-1]:]\n\n    # Placeholder for output data\n    output_data_buffer = []\n\n    # Flatten 'indices' to 2D array\n    reshaped_indices = indices.reshape(batch_dims_size, -1, indices.shape[-1])\n\n    # Flatten 'data' to array of shape (batch_dim_size, data.shape[batch_dimes:])\n    reshaped_data = data.reshape((batch_dims_size, ) + data.shape[batch_dims:])\n\n    # gather each scalar value from 'data'\n    for batch_dim in range(reshaped_indices.shape[0]):\n        for outer_dim in range(reshaped_indices.shape[1]):\n            gather_index = tuple(reshaped_indices[batch_dim][outer_dim])\n            output_data_buffer.append(reshaped_data[(batch_dim,) + gather_index])\n    return np.asarray(output_data_buffer, dtype=data.dtype).reshape(output_shape)\n\n\nclass GatherND(Base):\n\n    @staticmethod\n    def export_int32():  # type: () -> None\n        node = onnx.helper.make_node(\n            'GatherND',\n            inputs=['data', 'indices'],\n            outputs=['output'],\n        )\n\n        data = np.array([[0, 1], [2, 3]], dtype=np.int32)\n        indices = np.array([[0, 0], [1, 1]], dtype=np.int64)\n        output = gather_nd_impl(data, indices, 0)\n        expected_output = np.array([0, 3], dtype=np.int32)\n        assert (np.array_equal(output, expected_output))\n        expect(node, inputs=[data, indices], outputs=[output],\n               name='test_gathernd_example_int32')\n\n    @staticmethod\n    def export_float32():  # type: () -> None\n        node = onnx.helper.make_node(\n            'GatherND',\n            inputs=['data', 'indices'],\n            outputs=['output'],\n        )\n\n        data = np.array([[[0, 1], [2, 3]], [[4, 5], [6, 7]]], dtype=np.float32)\n        indices = np.array([[[0, 1]], [[1, 0]]], dtype=np.int64)\n        output = gather_nd_impl(data, indices, 0)\n        expected_output = np.array([[[2, 3]], [[4, 5]]], dtype=np.float32)\n        assert (np.array_equal(output, expected_output))\n        expect(node, inputs=[data, indices], outputs=[output],\n               name='test_gathernd_example_float32')\n\n    @staticmethod\n    def export_int32_batchdim_1():  # type: () -> None\n        node = onnx.helper.make_node(\n            'GatherND',\n            inputs=['data', 'indices'],\n            outputs=['output'],\n            batch_dims=1,\n        )\n\n        data = np.array([[[0, 1], [2, 3]], [[4, 5], [6, 7]]], dtype=np.int32)\n        indices = np.array([[1], [0]], dtype=np.int64)\n        output = gather_nd_impl(data, indices, 1)\n        expected_output = np.array([[2, 3], [4, 5]], dtype=np.int32)\n        assert (np.array_equal(output, expected_output))\n        expect(node, inputs=[data, indices], outputs=[output],\n               name='test_gathernd_example_int32_batch_dim1')\n"""
onnx/backend/test/case/node/gemm.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\nfrom typing import Optional\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\ndef gemm_reference_implementation(A, B, C=None, alpha=1., beta=1., transA=0,\n                                  transB=0):  # type: (np.ndarray, np.ndarray, Optional[np.ndarray], float, float, int, int) -> np.ndarray\n    A = A if transA == 0 else A.T\n    B = B if transB == 0 else B.T\n    C = C if C is not None else np.array(0)\n\n    Y = alpha * np.dot(A, B) + beta * C\n\n    return Y\n\n\nclass Gemm(Base):\n\n    @staticmethod\n    def export_default_zero_bias():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Gemm',\n            inputs=['a', 'b', 'c'],\n            outputs=['y']\n        )\n        a = np.random.ranf([3, 5]).astype(np.float32)\n        b = np.random.ranf([5, 4]).astype(np.float32)\n        c = np.zeros([1, 4]).astype(np.float32)\n        y = gemm_reference_implementation(a, b, c)\n        expect(node, inputs=[a, b, c], outputs=[y],\n               name='test_gemm_default_zero_bias')\n\n    @staticmethod\n    def export_default_no_bias():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Gemm',\n            inputs=['a', 'b'],\n            outputs=['y']\n        )\n        a = np.random.ranf([2, 10]).astype(np.float32)\n        b = np.random.ranf([10, 3]).astype(np.float32)\n        y = gemm_reference_implementation(a, b)\n        expect(node, inputs=[a, b], outputs=[y],\n               name='test_gemm_default_no_bias')\n\n    @staticmethod\n    def export_default_scalar_bias():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Gemm',\n            inputs=['a', 'b', 'c'],\n            outputs=['y']\n        )\n        a = np.random.ranf([2, 3]).astype(np.float32)\n        b = np.random.ranf([3, 4]).astype(np.float32)\n        c = np.array(3.14).astype(np.float32)\n        y = gemm_reference_implementation(a, b, c)\n        expect(node, inputs=[a, b, c], outputs=[y],\n               name='test_gemm_default_scalar_bias')\n\n    @staticmethod\n    def export_default_single_elem_vector_bias():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Gemm',\n            inputs=['a', 'b', 'c'],\n            outputs=['y']\n        )\n        a = np.random.ranf([3, 7]).astype(np.float32)\n        b = np.random.ranf([7, 3]).astype(np.float32)\n        c = np.random.ranf([1]).astype(np.float32)\n        y = gemm_reference_implementation(a, b, c)\n        expect(node, inputs=[a, b, c], outputs=[y],\n               name='test_gemm_default_single_elem_vector_bias')\n\n    @staticmethod\n    def export_default_vector_bias():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Gemm',\n            inputs=['a', 'b', 'c'],\n            outputs=['y']\n        )\n        a = np.random.ranf([2, 7]).astype(np.float32)\n        b = np.random.ranf([7, 4]).astype(np.float32)\n        c = np.random.ranf([1, 4]).astype(np.float32)\n        y = gemm_reference_implementation(a, b, c)\n        expect(node, inputs=[a, b, c], outputs=[y],\n               name='test_gemm_default_vector_bias')\n\n    @staticmethod\n    def export_default_matrix_bias():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Gemm',\n            inputs=['a', 'b', 'c'],\n            outputs=['y']\n        )\n        a = np.random.ranf([3, 6]).astype(np.float32)\n        b = np.random.ranf([6, 4]).astype(np.float32)\n        c = np.random.ranf([3, 4]).astype(np.float32)\n        y = gemm_reference_implementation(a, b, c)\n        expect(node, inputs=[a, b, c], outputs=[y],\n               name='test_gemm_default_matrix_bias')\n\n    @staticmethod\n    def export_transposeA():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Gemm',\n            inputs=['a', 'b', 'c'],\n            outputs=['y'],\n            transA=1\n        )\n        a = np.random.ranf([6, 3]).astype(np.float32)\n        b = np.random.ranf([6, 4]).astype(np.float32)\n        c = np.zeros([1, 4]).astype(np.float32)\n        y = gemm_reference_implementation(a, b, c, transA=1)\n        expect(node, inputs=[a, b, c], outputs=[y],\n               name='test_gemm_transposeA')\n\n    @staticmethod\n    def export_transposeB():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Gemm',\n            inputs=['a', 'b', 'c'],\n            outputs=['y'],\n            transB=1\n        )\n        a = np.random.ranf([3, 6]).astype(np.float32)\n        b = np.random.ranf([4, 6]).astype(np.float32)\n        c = np.zeros([1, 4]).astype(np.float32)\n        y = gemm_reference_implementation(a, b, c, transB=1)\n        expect(node, inputs=[a, b, c], outputs=[y],\n               name='test_gemm_transposeB')\n\n    @staticmethod\n    def export_alpha():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Gemm',\n            inputs=['a', 'b', 'c'],\n            outputs=['y'],\n            alpha=0.5\n        )\n        a = np.random.ranf([3, 5]).astype(np.float32)\n        b = np.random.ranf([5, 4]).astype(np.float32)\n        c = np.zeros([1, 4]).astype(np.float32)\n        y = gemm_reference_implementation(a, b, c, alpha=0.5)\n        expect(node, inputs=[a, b, c], outputs=[y],\n               name='test_gemm_alpha')\n\n    @staticmethod\n    def export_beta():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Gemm',\n            inputs=['a', 'b', 'c'],\n            outputs=['y'],\n            beta=0.5\n        )\n        a = np.random.ranf([2, 7]).astype(np.float32)\n        b = np.random.ranf([7, 4]).astype(np.float32)\n        c = np.random.ranf([1, 4]).astype(np.float32)\n        y = gemm_reference_implementation(a, b, c, beta=0.5)\n        expect(node, inputs=[a, b, c], outputs=[y],\n               name='test_gemm_beta')\n\n    @staticmethod\n    def export_all_attributes():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Gemm',\n            inputs=['a', 'b', 'c'],\n            outputs=['y'],\n            alpha=0.25,\n            beta=0.35,\n            transA=1,\n            transB=1\n        )\n        a = np.random.ranf([4, 3]).astype(np.float32)\n        b = np.random.ranf([5, 4]).astype(np.float32)\n        c = np.random.ranf([1, 5]).astype(np.float32)\n        y = gemm_reference_implementation(a, b, c, transA=1, transB=1, alpha=0.25, beta=0.35)\n        expect(node, inputs=[a, b, c], outputs=[y],\n               name='test_gemm_all_attributes')\n"""
onnx/backend/test/case/node/globalaveragepool.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass GlobalAveragePool(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'GlobalAveragePool',\n            inputs=['x'],\n            outputs=['y'],\n        )\n        x = np.random.randn(1, 3, 5, 5).astype(np.float32)\n        spatial_shape = np.ndim(x) - 2\n        y = np.average(x, axis=tuple(range(spatial_shape, spatial_shape + 2)))\n        for _ in range(spatial_shape):\n            y = np.expand_dims(y, -1)\n        expect(node, inputs=[x], outputs=[y], name='test_globalaveragepool')\n\n    @staticmethod\n    def export_globalaveragepool_precomputed():  # type: () -> None\n\n        node = onnx.helper.make_node(\n            'GlobalAveragePool',\n            inputs=['x'],\n            outputs=['y'],\n        )\n        x = np.array([[[\n            [1, 2, 3],\n            [4, 5, 6],\n            [7, 8, 9],\n        ]]]).astype(np.float32)\n        y = np.array([[[[5]]]]).astype(np.float32)\n        expect(node, inputs=[x], outputs=[y], name='test_globalaveragepool_precomputed')\n"""
onnx/backend/test/case/node/globalmaxpool.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass GlobalMaxPool(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n\n        node = onnx.helper.make_node(\n            'GlobalMaxPool',\n            inputs=['x'],\n            outputs=['y'],\n        )\n        x = np.random.randn(1, 3, 5, 5).astype(np.float32)\n        spatial_shape = np.ndim(x) - 2\n        y = np.max(x, axis=tuple(range(spatial_shape, spatial_shape + 2)))\n        for _ in range(spatial_shape):\n            y = np.expand_dims(y, -1)\n        expect(node, inputs=[x], outputs=[y], name='test_globalmaxpool')\n\n    @staticmethod\n    def export_globalmaxpool_precomputed():  # type: () -> None\n\n        node = onnx.helper.make_node(\n            'GlobalMaxPool',\n            inputs=['x'],\n            outputs=['y'],\n        )\n        x = np.array([[[\n            [1, 2, 3],\n            [4, 5, 6],\n            [7, 8, 9],\n        ]]]).astype(np.float32)\n        y = np.array([[[[9]]]]).astype(np.float32)\n        expect(node, inputs=[x], outputs=[y], name='test_globalmaxpool_precomputed')\n"""
onnx/backend/test/case/node/greater.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Greater(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Greater',\n            inputs=['x', 'y'],\n            outputs=['greater'],\n        )\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.random.randn(3, 4, 5).astype(np.float32)\n        z = np.greater(x, y)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_greater')\n\n    @staticmethod\n    def export_greater_broadcast():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Greater',\n            inputs=['x', 'y'],\n            outputs=['greater'],\n        )\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.random.randn(5).astype(np.float32)\n        z = np.greater(x, y)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_greater_bcast')\n"""
onnx/backend/test/case/node/greater_equal.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Greater(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'GreaterOrEqual',\n            inputs=['x', 'y'],\n            outputs=['greater_equal'],\n        )\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.random.randn(3, 4, 5).astype(np.float32)\n        z = np.greater_equal(x, y)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_greater_equal')\n\n    @staticmethod\n    def export_greater_broadcast():  # type: () -> None\n        node = onnx.helper.make_node(\n            'GreaterOrEqual',\n            inputs=['x', 'y'],\n            outputs=['greater_equal'],\n        )\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.random.randn(5).astype(np.float32)\n        z = np.greater_equal(x, y)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_greater_equal_bcast')\n"""
onnx/backend/test/case/node/gru.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\nfrom typing import Any, Tuple\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass GRU_Helper():\n    def __init__(self, **params):  # type: (*Any) -> None\n        # GRU Input Names\n        X = str(\'X\')\n        W = str(\'W\')\n        R = str(\'R\')\n        B = str(\'B\')\n        H_0 = str(\'initial_h\')\n        LBR = str(\'linear_before_reset\')\n        number_of_gates = 3\n\n        required_inputs = [X, W, R]\n        for i in required_inputs:\n            assert i in params, ""Missing Required Input: {0}"".format(i)\n\n        self.num_directions = params[W].shape[0]\n\n        if self.num_directions == 1:\n            for k in params.keys():\n                if k != X:\n                    params[k] = np.squeeze(params[k], axis=0)\n\n            hidden_size = params[R].shape[-1]\n            batch_size = params[X].shape[1]\n\n            b = params[B] if B in params else np.zeros(2 * number_of_gates * hidden_size)\n            h_0 = params[H_0] if H_0 in params else np.zeros((batch_size, hidden_size))\n            lbr = params[LBR] if LBR in params else 0\n\n            self.X = params[X]\n            self.W = params[W]\n            self.R = params[R]\n            self.B = b\n            self.H_0 = h_0\n            self.LBR = lbr\n\n        else:\n            raise NotImplementedError()\n\n    def f(self, x):  # type: (np.ndarray) -> np.ndarray\n        return 1 / (1 + np.exp(-x))\n\n    def g(self, x):  # type: (np.ndarray) -> np.ndarray\n        return np.tanh(x)\n\n    def step(self):  # type: () -> Tuple[np.ndarray, np.ndarray]\n        h_list = []\n        [w_z, w_r, w_h] = np.split(self.W, 3)\n        [r_z, r_r, r_h] = np.split(self.R, 3)\n        [w_bz, w_br, w_bh, r_bz, r_br, r_bh] = np.split(self.B, 6)\n        gates_w = np.transpose(np.concatenate((w_z, w_r)))\n        gates_r = np.transpose(np.concatenate((r_z, r_r)))\n        gates_b = np.add(np.concatenate((w_bz, w_br)), np.concatenate((r_bz, r_br)))\n\n        H_t = self.H_0\n        for x in np.split(self.X, self.X.shape[0], axis=0):\n            gates = np.dot(x, gates_w) + np.dot(H_t, gates_r) + gates_b\n            z, r = np.split(gates, 2, -1)\n            z = self.f(z)\n            r = self.f(r)\n            h_default = self.g(np.dot(x, np.transpose(w_h)) + np.dot(r * H_t, np.transpose(r_h)) + w_bh + r_bh)\n            h_linear = self.g(np.dot(x, np.transpose(w_h)) + r * (np.dot(H_t, np.transpose(r_h)) + r_bh) + w_bh)\n            h = h_linear if self.LBR else h_default\n            H = (1 - z) * h + z * H_t\n            h_list.append(H)\n            H_t = H\n        concatenated = np.concatenate(h_list)\n        if self.num_directions == 1:\n            output = np.expand_dims(concatenated, 1)\n        return output, h_list[-1]\n\n\nclass GRU(Base):\n\n    @staticmethod\n    def export_defaults():  # type: () -> None\n        input = np.array([[[1., 2.], [3., 4.], [5., 6.]]]).astype(np.float32)\n\n        input_size = 2\n        hidden_size = 5\n        weight_scale = 0.1\n        number_of_gates = 3\n\n        node = onnx.helper.make_node(\n            \'GRU\',\n            inputs=[\'X\', \'W\', \'R\'],\n            outputs=[\'\', \'Y\'],\n            hidden_size=hidden_size\n        )\n\n        W = weight_scale * np.ones((1, number_of_gates * hidden_size, input_size)).astype(np.float32)\n        R = weight_scale * np.ones((1, number_of_gates * hidden_size, hidden_size)).astype(np.float32)\n\n        gru = GRU_Helper(X=input, W=W, R=R)\n        _, Y_h = gru.step()\n        expect(node, inputs=[input, W, R], outputs=[Y_h.astype(np.float32)], name=\'test_gru_defaults\')\n\n    @staticmethod\n    def export_initial_bias():  # type: () -> None\n        input = np.array([[[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]]]).astype(np.float32)\n\n        input_size = 3\n        hidden_size = 3\n        weight_scale = 0.1\n        custom_bias = 0.1\n        number_of_gates = 3\n\n        node = onnx.helper.make_node(\n            \'GRU\',\n            inputs=[\'X\', \'W\', \'R\', \'B\'],\n            outputs=[\'\', \'Y\'],\n            hidden_size=hidden_size\n        )\n\n        W = weight_scale * np.ones((1, number_of_gates * hidden_size, input_size)).astype(np.float32)\n        R = weight_scale * np.ones((1, number_of_gates * hidden_size, hidden_size)).astype(np.float32)\n\n        # Adding custom bias\n        W_B = custom_bias * np.ones((1, number_of_gates * hidden_size)).astype(np.float32)\n        R_B = np.zeros((1, number_of_gates * hidden_size)).astype(np.float32)\n        B = np.concatenate((W_B, R_B), axis=1)\n\n        gru = GRU_Helper(X=input, W=W, R=R, B=B)\n        _, Y_h = gru.step()\n        expect(node, inputs=[input, W, R, B], outputs=[Y_h.astype(np.float32)], name=\'test_gru_with_initial_bias\')\n\n    @staticmethod\n    def export_seq_length():  # type: () -> None\n        input = np.array([[[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]],\n                          [[10., 11., 12.], [13., 14., 15.], [16., 17., 18.]]]).astype(np.float32)\n\n        input_size = 3\n        hidden_size = 5\n        number_of_gates = 3\n\n        node = onnx.helper.make_node(\n            \'GRU\',\n            inputs=[\'X\', \'W\', \'R\', \'B\'],\n            outputs=[\'\', \'Y\'],\n            hidden_size=hidden_size\n        )\n\n        W = np.random.randn(1, number_of_gates * hidden_size, input_size).astype(np.float32)\n        R = np.random.randn(1, number_of_gates * hidden_size, hidden_size).astype(np.float32)\n\n        # Adding custom bias\n        W_B = np.random.randn(1, number_of_gates * hidden_size).astype(np.float32)\n        R_B = np.random.randn(1, number_of_gates * hidden_size).astype(np.float32)\n        B = np.concatenate((W_B, R_B), axis=1)\n\n        gru = GRU_Helper(X=input, W=W, R=R, B=B)\n        _, Y_h = gru.step()\n        expect(node, inputs=[input, W, R, B], outputs=[Y_h.astype(np.float32)], name=\'test_gru_seq_length\')\n'"
onnx/backend/test/case/node/hardmax.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Hardmax(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Hardmax',\n            inputs=['x'],\n            outputs=['y'],\n        )\n\n        x = np.array([[3, 0, 1, 2], [2, 5, 1, 0], [0, 1, 3, 2], [0, 1, 2, 3]]).astype(np.float32)\n        y = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]).astype(np.float32)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_hardmax_example')\n\n        # For multiple occurrences of the maximal values, the first occurrence is selected for one-hot output\n        x = np.array([[3, 3, 3, 1]]).astype(np.float32)\n        y = np.array([[1, 0, 0, 0]]).astype(np.float32)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_hardmax_one_hot')\n\n    @staticmethod\n    def export_hardmax_axis():  # type: () -> None\n        def hardmax_2d(x):  # type: (np.ndarray) -> np.ndarray\n            return np.eye(x.shape[1], dtype=x.dtype)[np.argmax(x, axis=1)]\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        node = onnx.helper.make_node(\n            'Hardmax',\n            inputs=['x'],\n            outputs=['y'],\n            axis=0,\n        )\n        y = hardmax_2d(x.reshape(1, 60)).reshape(3, 4, 5)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_hardmax_axis_0')\n\n        node = onnx.helper.make_node(\n            'Hardmax',\n            inputs=['x'],\n            outputs=['y'],\n            axis=1,\n        )\n        y = hardmax_2d(x.reshape(3, 20)).reshape(3, 4, 5)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_hardmax_axis_1')\n\n        # default axis is 1\n        node = onnx.helper.make_node(\n            'Hardmax',\n            inputs=['x'],\n            outputs=['y'],\n        )\n        expect(node, inputs=[x], outputs=[y],\n               name='test_hardmax_default_axis')\n\n        node = onnx.helper.make_node(\n            'Hardmax',\n            inputs=['x'],\n            outputs=['y'],\n            axis=2,\n        )\n        y = hardmax_2d(x.reshape(12, 5)).reshape(3, 4, 5)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_hardmax_axis_2')\n\n        node = onnx.helper.make_node(\n            'Hardmax',\n            inputs=['x'],\n            outputs=['y'],\n            axis=-1,\n        )\n        y = hardmax_2d(x.reshape(12, 5)).reshape(3, 4, 5)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_hardmax_negative_axis')\n"""
onnx/backend/test/case/node/hardsigmoid.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass HardSigmoid(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'HardSigmoid',\n            inputs=['x'],\n            outputs=['y'],\n            alpha=0.5,\n            beta=0.6\n        )\n\n        x = np.array([-1, 0, 1]).astype(np.float32)\n        y = np.clip(x * 0.5 + 0.6, 0, 1)  # expected output [0.1, 0.6, 1.]\n        expect(node, inputs=[x], outputs=[y],\n               name='test_hardsigmoid_example')\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.clip(x * 0.5 + 0.6, 0, 1)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_hardsigmoid')\n\n    @staticmethod\n    def export_hardsigmoid_default():  # type: () -> None\n        default_alpha = 0.2\n        default_beta = 0.5\n        node = onnx.helper.make_node(\n            'HardSigmoid',\n            inputs=['x'],\n            outputs=['y'],\n        )\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.clip(x * default_alpha + default_beta, 0, 1)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_hardsigmoid_default')\n"""
onnx/backend/test/case/node/identity.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Identity(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Identity',\n            inputs=['x'],\n            outputs=['y'],\n        )\n\n        data = np.array([[[\n            [1, 2],\n            [3, 4],\n        ]]], dtype=np.float32)\n\n        expect(node, inputs=[data], outputs=[data],\n               name='test_identity')\n"""
onnx/backend/test/case/node/instancenorm.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass InstanceNormalization(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        def _instancenorm_test_mode(x, s, bias, epsilon=1e-5):  # type: ignore\n            dims_x = len(x.shape)\n            axis = tuple(range(2, dims_x))\n            mean = np.mean(x, axis=axis, keepdims=True)\n            var = np.var(x, axis=axis, keepdims=True)\n            dim_ones = (1,) * (dims_x - 2)\n            s = s.reshape(-1, *dim_ones)\n            bias = bias.reshape(-1, *dim_ones)\n            return s * (x - mean) / np.sqrt(var + epsilon) + bias\n\n        # input size: (1, 2, 1, 3)\n        x = np.array([[[[-1, 0, 1]], [[2, 3, 4]]]]).astype(np.float32)\n        s = np.array([1.0, 1.5]).astype(np.float32)\n        bias = np.array([0, 1]).astype(np.float32)\n        y = _instancenorm_test_mode(x, s, bias).astype(np.float32)\n\n        node = onnx.helper.make_node(\n            'InstanceNormalization',\n            inputs=['x', 's', 'bias'],\n            outputs=['y'],\n        )\n\n        # output size: (1, 2, 1, 3)\n        expect(node, inputs=[x, s, bias], outputs=[y],\n               name='test_instancenorm_example')\n\n        # input size: (2, 3, 4, 5)\n        x = np.random.randn(2, 3, 4, 5).astype(np.float32)\n        s = np.random.randn(3).astype(np.float32)\n        bias = np.random.randn(3).astype(np.float32)\n        epsilon = 1e-2\n        y = _instancenorm_test_mode(x, s, bias, epsilon).astype(np.float32)\n\n        node = onnx.helper.make_node(\n            'InstanceNormalization',\n            inputs=['x', 's', 'bias'],\n            outputs=['y'],\n            epsilon=epsilon,\n        )\n\n        # output size: (2, 3, 4, 5)\n        expect(node, inputs=[x, s, bias], outputs=[y],\n               name='test_instancenorm_epsilon')\n"""
onnx/backend/test/case/node/isinf.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass IsInf(Base):\n\n    @staticmethod\n    def export_infinity():  # type: () -> None\n        node = onnx.helper.make_node('IsInf',\n                                     inputs=['x'],\n                                     outputs=['y'],\n                                     )\n\n        x = np.array([-1.2, np.nan, np.inf, 2.8, np.NINF, np.inf],\n                     dtype=np.float32)\n        y = np.isinf(x)\n        expect(node, inputs=[x], outputs=[y], name='test_isinf')\n\n    @staticmethod\n    def export_positive_infinity_only():  # type: () -> None\n        node = onnx.helper.make_node('IsInf',\n                                     inputs=['x'],\n                                     outputs=['y'],\n                                     detect_negative=0\n                                     )\n\n        x = np.array([-1.7, np.nan, np.inf, 3.6, np.NINF, np.inf],\n                     dtype=np.float32)\n        y = np.isposinf(x)\n        expect(node, inputs=[x], outputs=[y], name='test_isinf_positive')\n\n    @staticmethod\n    def export_negative_infinity_only():  # type: () -> None\n        node = onnx.helper.make_node('IsInf',\n                                     inputs=['x'],\n                                     outputs=['y'],\n                                     detect_positive=0\n                                     )\n\n        x = np.array([-1.7, np.nan, np.inf, -3.6, np.NINF, np.inf],\n                     dtype=np.float32)\n        y = np.isneginf(x)\n        expect(node, inputs=[x], outputs=[y], name='test_isinf_negative')\n"""
onnx/backend/test/case/node/isnan.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass IsNaN(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'IsNaN',\n            inputs=['x'],\n            outputs=['y'],\n        )\n\n        x = np.array([3.0, np.nan, 4.0, np.nan], dtype=np.float32)\n        y = np.isnan(x)\n        expect(node, inputs=[x], outputs=[y], name='test_isnan')\n"""
onnx/backend/test/case/node/leakyrelu.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass LeakyRelu(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'LeakyRelu',\n            inputs=['x'],\n            outputs=['y'],\n            alpha=0.1\n        )\n\n        x = np.array([-1, 0, 1]).astype(np.float32)\n        # expected output [-0.1, 0., 1.]\n        y = np.clip(x, 0, np.inf) + np.clip(x, -np.inf, 0) * 0.1\n        expect(node, inputs=[x], outputs=[y],\n               name='test_leakyrelu_example')\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.clip(x, 0, np.inf) + np.clip(x, -np.inf, 0) * 0.1\n        expect(node, inputs=[x], outputs=[y],\n               name='test_leakyrelu')\n\n    @staticmethod\n    def export_leakyrelu_default():  # type: () -> None\n        default_alpha = 0.01\n        node = onnx.helper.make_node(\n            'LeakyRelu',\n            inputs=['x'],\n            outputs=['y'],\n        )\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.clip(x, 0, np.inf) + np.clip(x, -np.inf, 0) * default_alpha\n        expect(node, inputs=[x], outputs=[y],\n               name='test_leakyrelu_default')\n"""
onnx/backend/test/case/node/less.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Less(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Less',\n            inputs=['x', 'y'],\n            outputs=['less'],\n        )\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.random.randn(3, 4, 5).astype(np.float32)\n        z = np.less(x, y)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_less')\n\n    @staticmethod\n    def export_less_broadcast():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Less',\n            inputs=['x', 'y'],\n            outputs=['less'],\n        )\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.random.randn(5).astype(np.float32)\n        z = np.less(x, y)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_less_bcast')\n"""
onnx/backend/test/case/node/less_equal.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Less(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'LessOrEqual',\n            inputs=['x', 'y'],\n            outputs=['less_equal'],\n        )\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.random.randn(3, 4, 5).astype(np.float32)\n        z = np.less_equal(x, y)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_less_equal')\n\n    @staticmethod\n    def export_less_broadcast():  # type: () -> None\n        node = onnx.helper.make_node(\n            'LessOrEqual',\n            inputs=['x', 'y'],\n            outputs=['less_equal'],\n        )\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.random.randn(5).astype(np.float32)\n        z = np.less_equal(x, y)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_less_equal_bcast')\n"""
onnx/backend/test/case/node/log.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Log(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Log',\n            inputs=['x'],\n            outputs=['y'],\n        )\n\n        x = np.array([1, 10]).astype(np.float32)\n        y = np.log(x)  # expected output [0., 2.30258512]\n        expect(node, inputs=[x], outputs=[y],\n               name='test_log_example')\n\n        x = np.exp(np.random.randn(3, 4, 5).astype(np.float32))\n        y = np.log(x)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_log')\n"""
onnx/backend/test/case/node/logsoftmax.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass LogSoftmax(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'LogSoftmax',\n            inputs=['x'],\n            outputs=['y'],\n        )\n        x = np.array([[-1, 0, 1]]).astype(np.float32)\n        # expected output [[-2.40760589, -1.40760589, -0.40760589]]\n        y = x - np.log(np.sum(np.exp(x), axis=1))\n        expect(node, inputs=[x], outputs=[y],\n               name='test_logsoftmax_example_1')\n\n    @staticmethod\n    def export_logsoftmax_axis():  # type: () -> None\n        def logsoftmax_2d(x):  # type: (np.ndarray) -> np.ndarray\n            max_x = np.max(x, axis=1).reshape((-1, 1))\n            exp_x = np.exp(x - max_x)\n            return x - max_x - np.log(np.sum(exp_x, axis=1).reshape((-1, 1)))\n\n        x = np.array([[0, 1, 2, 3], [10000, 10001, 10002, 10003]]).astype(np.float32)\n        # expected output [[-3.4401896, -2.4401896, -1.44018972, -0.44018969],\n        #                 [-3.4401896, -2.4401896, -1.44018972, -0.44018969]]\n        y = logsoftmax_2d(x)\n\n        node = onnx.helper.make_node(\n            'LogSoftmax',\n            inputs=['x'],\n            outputs=['y'],\n        )\n        expect(node, inputs=[x], outputs=[y],\n               name='test_logsoftmax_large_number')\n\n        x = np.abs(np.random.randn(3, 4, 5).astype(np.float32))\n        node = onnx.helper.make_node(\n            'LogSoftmax',\n            inputs=['x'],\n            outputs=['y'],\n            axis=0,\n        )\n        y = logsoftmax_2d(x.reshape(1, 60)).reshape(3, 4, 5)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_logsoftmax_axis_0')\n\n        node = onnx.helper.make_node(\n            'LogSoftmax',\n            inputs=['x'],\n            outputs=['y'],\n            axis=1,\n        )\n        y = logsoftmax_2d(x.reshape(3, 20)).reshape(3, 4, 5)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_logsoftmax_axis_1')\n\n        # default axis is 1\n        node = onnx.helper.make_node(\n            'LogSoftmax',\n            inputs=['x'],\n            outputs=['y'],\n        )\n        expect(node, inputs=[x], outputs=[y],\n               name='test_logsoftmax_default_axis')\n\n        node = onnx.helper.make_node(\n            'LogSoftmax',\n            inputs=['x'],\n            outputs=['y'],\n            axis=2,\n        )\n        y = logsoftmax_2d(x.reshape(12, 5)).reshape(3, 4, 5)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_logsoftmax_axis_2')\n\n        node = onnx.helper.make_node(\n            'LogSoftmax',\n            inputs=['x'],\n            outputs=['y'],\n            axis=-1,\n        )\n        y = logsoftmax_2d(x.reshape(12, 5)).reshape(3, 4, 5)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_logsoftmax_negative_axis')\n"""
onnx/backend/test/case/node/lrn.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport math\nimport numpy as np  # type: ignore\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass LRN(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        alpha = 0.0002\n        beta = 0.5\n        bias = 2.0\n        nsize = 3\n        node = onnx.helper.make_node(\n            'LRN',\n            inputs=['x'],\n            outputs=['y'],\n            alpha=alpha,\n            beta=beta,\n            bias=bias,\n            size=nsize\n        )\n        x = np.random.randn(5, 5, 5, 5).astype(np.float32)\n        square_sum = np.zeros((5, 5, 5, 5)).astype(np.float32)\n        for n, c, h, w in np.ndindex(x.shape):\n            square_sum[n, c, h, w] = sum(x[n,\n                                           max(0, c - int(math.floor((nsize - 1) / 2))):min(5, c + int(math.ceil((nsize - 1) / 2)) + 1),\n                                           h,\n                                           w] ** 2)\n        y = x / ((bias + (alpha / nsize) * square_sum) ** beta)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_lrn')\n\n    @staticmethod\n    def export_default():  # type: () -> None\n        alpha = 0.0001\n        beta = 0.75\n        bias = 1.0\n        nsize = 3\n        node = onnx.helper.make_node(\n            'LRN',\n            inputs=['x'],\n            outputs=['y'],\n            size=3\n        )\n        x = np.random.randn(5, 5, 5, 5).astype(np.float32)\n        square_sum = np.zeros((5, 5, 5, 5)).astype(np.float32)\n        for n, c, h, w in np.ndindex(x.shape):\n            square_sum[n, c, h, w] = sum(x[n,\n                                           max(0, c - int(math.floor((nsize - 1) / 2))):min(5, c + int(math.ceil((nsize - 1) / 2)) + 1),\n                                           h,\n                                           w] ** 2)\n        y = x / ((bias + (alpha / nsize) * square_sum) ** beta)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_lrn_default')\n"""
onnx/backend/test/case/node/lstm.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\nfrom typing import Any, Tuple\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass LSTM_Helper():\n    def __init__(self, **params):  # type: (*Any) -> None\n        # LSTM Input Names\n        X = str(\'X\')\n        W = str(\'W\')\n        R = str(\'R\')\n        B = str(\'B\')\n        H_0 = str(\'initial_h\')\n        C_0 = str(\'initial_c\')\n        P = str(\'P\')\n        number_of_gates = 4\n        number_of_peepholes = 3\n\n        required_inputs = [X, W, R]\n        for i in required_inputs:\n            assert i in params, ""Missing Required Input: {0}"".format(i)\n\n        self.num_directions = params[W].shape[0]\n\n        if self.num_directions == 1:\n            for k in params.keys():\n                if k != X:\n                    params[k] = np.squeeze(params[k], axis=0)\n\n            hidden_size = params[R].shape[-1]\n            batch_size = params[X].shape[1]\n\n            b = params[B] if B in params else np.zeros(2 * number_of_gates * hidden_size, dtype=np.float32)\n            p = params[P] if P in params else np.zeros(number_of_peepholes * hidden_size, dtype=np.float32)\n            h_0 = params[H_0] if H_0 in params else np.zeros((batch_size, hidden_size), dtype=np.float32)\n            c_0 = params[C_0] if C_0 in params else np.zeros((batch_size, hidden_size), dtype=np.float32)\n\n            self.X = params[X]\n            self.W = params[W]\n            self.R = params[R]\n            self.B = b\n            self.P = p\n            self.H_0 = h_0\n            self.C_0 = c_0\n        else:\n            raise NotImplementedError()\n\n    def f(self, x):  # type: (np.ndarray) -> np.ndarray\n        return 1 / (1 + np.exp(-x))\n\n    def g(self, x):  # type: (np.ndarray) -> np.ndarray\n        return np.tanh(x)\n\n    def h(self, x):  # type: (np.ndarray) -> np.ndarray\n        return np.tanh(x)\n\n    def step(self):  # type: () -> Tuple[np.ndarray, np.ndarray]\n        [p_i, p_o, p_f] = np.split(self.P, 3)\n        h_list = []\n        H_t = self.H_0\n        C_t = self.C_0\n        for x in np.split(self.X, self.X.shape[0], axis=0):\n            gates = np.dot(x, np.transpose(self.W)) + np.dot(H_t, np.transpose(self.R)) + np.add(\n                *np.split(self.B, 2))\n            i, o, f, c = np.split(gates, 4, -1)\n            i = self.f(i + p_i * C_t)\n            f = self.f(f + p_f * C_t)\n            c = self.g(c)\n            C = f * C_t + i * c\n            o = self.f(o + p_o * C)\n            H = o * self.h(C)\n            h_list.append(H)\n            H_t = H\n            C_t = C\n        concatenated = np.concatenate(h_list)\n        if self.num_directions == 1:\n            output = np.expand_dims(concatenated, 1)\n        return output, h_list[-1]\n\n\nclass LSTM(Base):\n\n    @staticmethod\n    def export_defaults():  # type: () -> None\n        input = np.array([[[1., 2.], [3., 4.], [5., 6.]]]).astype(np.float32)\n\n        input_size = 2\n        hidden_size = 3\n        weight_scale = 0.1\n        number_of_gates = 4\n\n        node = onnx.helper.make_node(\n            \'LSTM\',\n            inputs=[\'X\', \'W\', \'R\'],\n            outputs=[\'\', \'Y\'],\n            hidden_size=hidden_size\n        )\n\n        W = weight_scale * np.ones((1, number_of_gates * hidden_size, input_size)).astype(np.float32)\n        R = weight_scale * np.ones((1, number_of_gates * hidden_size, hidden_size)).astype(np.float32)\n\n        lstm = LSTM_Helper(X=input, W=W, R=R)\n        _, Y_h = lstm.step()\n        expect(node, inputs=[input, W, R], outputs=[Y_h.astype(np.float32)], name=\'test_lstm_defaults\')\n\n    @staticmethod\n    def export_initial_bias():  # type: () -> None\n        input = np.array([[[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]]]).astype(np.float32)\n\n        input_size = 3\n        hidden_size = 4\n        weight_scale = 0.1\n        custom_bias = 0.1\n        number_of_gates = 4\n\n        node = onnx.helper.make_node(\n            \'LSTM\',\n            inputs=[\'X\', \'W\', \'R\', \'B\'],\n            outputs=[\'\', \'Y\'],\n            hidden_size=hidden_size\n        )\n\n        W = weight_scale * np.ones((1, number_of_gates * hidden_size, input_size)).astype(np.float32)\n        R = weight_scale * np.ones((1, number_of_gates * hidden_size, hidden_size)).astype(np.float32)\n\n        # Adding custom bias\n        W_B = custom_bias * np.ones((1, number_of_gates * hidden_size)).astype(np.float32)\n        R_B = np.zeros((1, number_of_gates * hidden_size)).astype(np.float32)\n        B = np.concatenate((W_B, R_B), 1)\n\n        lstm = LSTM_Helper(X=input, W=W, R=R, B=B)\n        _, Y_h = lstm.step()\n        expect(node, inputs=[input, W, R, B], outputs=[Y_h.astype(np.float32)], name=\'test_lstm_with_initial_bias\')\n\n    @staticmethod\n    def export_peepholes():  # type: () -> None\n        input = np.array([[[1., 2., 3., 4.], [5., 6., 7., 8.]]]).astype(np.float32)\n\n        input_size = 4\n        hidden_size = 3\n        weight_scale = 0.1\n        number_of_gates = 4\n        number_of_peepholes = 3\n\n        node = onnx.helper.make_node(\n            \'LSTM\',\n            inputs=[\'X\', \'W\', \'R\', \'B\', \'sequence_lens\', \'initial_h\', \'initial_c\', \'P\'],\n            outputs=[\'\', \'Y\'],\n            hidden_size=hidden_size\n        )\n\n        # Initializing Inputs\n        W = weight_scale * np.ones((1, number_of_gates * hidden_size, input_size)).astype(np.float32)\n        R = weight_scale * np.ones((1, number_of_gates * hidden_size, hidden_size)).astype(np.float32)\n        B = np.zeros((1, 2 * number_of_gates * hidden_size)).astype(np.float32)\n        seq_lens = np.repeat(input.shape[0], input.shape[1]).astype(np.int32)\n        init_h = np.zeros((1, input.shape[1], hidden_size)).astype(np.float32)\n        init_c = np.zeros((1, input.shape[1], hidden_size)).astype(np.float32)\n        P = weight_scale * np.ones((1, number_of_peepholes * hidden_size)).astype(np.float32)\n\n        lstm = LSTM_Helper(X=input, W=W, R=R, B=B, P=P, initial_c=init_c, initial_h=init_h)\n        _, Y_h = lstm.step()\n        expect(node, inputs=[input, W, R, B, seq_lens, init_h, init_c, P], outputs=[Y_h.astype(np.float32)],\n               name=\'test_lstm_with_peepholes\')\n'"
onnx/backend/test/case/node/matmul.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass MatMul(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'MatMul',\n            inputs=['a', 'b'],\n            outputs=['c'],\n        )\n\n        # 2d\n        a = np.random.randn(3, 4).astype(np.float32)\n        b = np.random.randn(4, 3).astype(np.float32)\n        c = np.matmul(a, b)\n        expect(node, inputs=[a, b], outputs=[c],\n               name='test_matmul_2d')\n\n        # 3d\n        a = np.random.randn(2, 3, 4).astype(np.float32)\n        b = np.random.randn(2, 4, 3).astype(np.float32)\n        c = np.matmul(a, b)\n        expect(node, inputs=[a, b], outputs=[c],\n               name='test_matmul_3d')\n\n        # 4d\n        a = np.random.randn(1, 2, 3, 4).astype(np.float32)\n        b = np.random.randn(1, 2, 4, 3).astype(np.float32)\n        c = np.matmul(a, b)\n        expect(node, inputs=[a, b], outputs=[c],\n               name='test_matmul_4d')\n"""
onnx/backend/test/case/node/matmulinteger.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass MatMulInteger(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node('MatMulInteger',\n            inputs=['A', 'B', 'a_zero_point', 'b_zero_point'],\n            outputs=['Y'],)\n\n        A = np.array([[11, 7, 3],\n            [10, 6, 2],\n            [9, 5, 1],\n            [8, 4, 0], ], dtype=np.uint8)\n\n        a_zero_point = np.array([12], dtype=np.uint8)\n\n        B = np.array([[1, 4],\n            [2, 5],\n            [3, 6], ], dtype=np.uint8)\n\n        b_zero_point = np.array([0], dtype=np.uint8)\n\n        output = np.array([[-38, -83],\n            [-44, -98],\n            [-50, -113],\n            [-56, -128], ], dtype=np.int32)\n\n        expect(node, inputs=[A, B, a_zero_point, b_zero_point], outputs=[output],\n               name='test_matmulinteger')\n"""
onnx/backend/test/case/node/max.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\nfrom ..utils import all_numeric_dtypes\n\n\nclass Max(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        data_0 = np.array([3, 2, 1]).astype(np.float32)\n        data_1 = np.array([1, 4, 4]).astype(np.float32)\n        data_2 = np.array([2, 5, 3]).astype(np.float32)\n        result = np.array([3, 5, 4]).astype(np.float32)\n        node = onnx.helper.make_node(\n            'Max',\n            inputs=['data_0', 'data_1', 'data_2'],\n            outputs=['result'],\n        )\n        expect(node, inputs=[data_0, data_1, data_2], outputs=[result],\n               name='test_max_example')\n\n        node = onnx.helper.make_node(\n            'Max',\n            inputs=['data_0'],\n            outputs=['result'],\n        )\n        expect(node, inputs=[data_0], outputs=[data_0],\n               name='test_max_one_input')\n\n        result = np.maximum(data_0, data_1)\n        node = onnx.helper.make_node(\n            'Max',\n            inputs=['data_0', 'data_1'],\n            outputs=['result'],\n        )\n        expect(node, inputs=[data_0, data_1], outputs=[result],\n               name='test_max_two_inputs')\n\n    @staticmethod\n    def export_max_all_numeric_types():  # type: () -> None\n        for op_dtype in all_numeric_dtypes:\n            data_0 = np.array([3, 2, 1]).astype(op_dtype)\n            data_1 = np.array([1, 4, 4]).astype(op_dtype)\n            result = np.array([3, 4, 4]).astype(op_dtype)\n            node = onnx.helper.make_node(\n                'Max',\n                inputs=['data_0', 'data_1'],\n                outputs=['result'],\n            )\n            expect(node, inputs=[data_0, data_1], outputs=[result],\n                   name='test_max_{0}'.format(np.dtype(op_dtype).name))\n"""
onnx/backend/test/case/node/maxpool.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\nfrom .pool_op_common import get_output_shape, get_pad_shape, pool\n\n\nclass MaxPool(Base):\n\n    @staticmethod\n    def export_maxpool_2d_uint8():  # type: () -> None\n        """"""\n        input_shape: [1, 1, 5, 5]\n        output_shape: [1, 1, 5, 5]\n        pad_shape: [4, 4] -> [2, 2, 2, 2] by axis\n        """"""\n        node = onnx.helper.make_node(\n            \'MaxPool\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            kernel_shape=[5, 5],\n            pads=[2, 2, 2, 2]\n        )\n        x = np.array([[[\n            [1, 2, 3, 4, 5],\n            [6, 7, 8, 9, 10],\n            [11, 12, 13, 14, 15],\n            [16, 17, 18, 19, 20],\n            [21, 22, 23, 24, 25],\n        ]]]).astype(np.uint8)\n        y = np.array([[[\n            [13, 14, 15, 15, 15],\n            [18, 19, 20, 20, 20],\n            [23, 24, 25, 25, 25],\n            [23, 24, 25, 25, 25],\n            [23, 24, 25, 25, 25]]]]).astype(np.uint8)\n\n        expect(node, inputs=[x], outputs=[y], name=\'test_maxpool_2d_uint8\')\n\n    @staticmethod\n    def export_maxpool_2d_precomputed_pads():  # type: () -> None\n        """"""\n        input_shape: [1, 1, 5, 5]\n        output_shape: [1, 1, 5, 5]\n        pad_shape: [4, 4] -> [2, 2, 2, 2] by axis\n        """"""\n        node = onnx.helper.make_node(\n            \'MaxPool\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            kernel_shape=[5, 5],\n            pads=[2, 2, 2, 2]\n\n        )\n        x = np.array([[[\n            [1, 2, 3, 4, 5],\n            [6, 7, 8, 9, 10],\n            [11, 12, 13, 14, 15],\n            [16, 17, 18, 19, 20],\n            [21, 22, 23, 24, 25],\n        ]]]).astype(np.float32)\n        y = np.array([[[\n            [13, 14, 15, 15, 15],\n            [18, 19, 20, 20, 20],\n            [23, 24, 25, 25, 25],\n            [23, 24, 25, 25, 25],\n            [23, 24, 25, 25, 25]]]]).astype(np.float32)\n\n        expect(node, inputs=[x], outputs=[y], name=\'test_maxpool_2d_precomputed_pads\')\n\n    @staticmethod\n    def export_maxpool_with_argmax_2d_precomputed_pads():  # type: () -> None\n        """"""\n        input_shape: [1, 1, 5, 5]\n        output_shape: [1, 1, 5, 5]\n        pad_shape: [4, 4] -> [2, 2, 2, 2] by axis\n        """"""\n        node = onnx.helper.make_node(\n            \'MaxPool\',\n            inputs=[\'x\'],\n            outputs=[\'y\', \'z\'],\n            kernel_shape=[5, 5],\n            pads=[2, 2, 2, 2]\n        )\n        x = np.array([[[\n            [1, 2, 3, 4, 5],\n            [6, 7, 8, 9, 10],\n            [11, 12, 13, 14, 15],\n            [16, 17, 18, 19, 20],\n            [21, 22, 23, 24, 25],\n        ]]]).astype(np.float32)\n        y = np.array([[[\n            [13, 14, 15, 15, 15],\n            [18, 19, 20, 20, 20],\n            [23, 24, 25, 25, 25],\n            [23, 24, 25, 25, 25],\n            [23, 24, 25, 25, 25]]]]).astype(np.float32)\n        z = np.array([[[\n            [12, 13, 14, 14, 14],\n            [17, 18, 19, 19, 19],\n            [22, 23, 24, 24, 24],\n            [22, 23, 24, 24, 24],\n            [22, 23, 24, 24, 24]]]]).astype(np.int64)\n\n        expect(node, inputs=[x], outputs=[y, z], name=\'test_maxpool_with_argmax_2d_precomputed_pads\')\n\n    @staticmethod\n    def export_maxpool_2d_precomputed_strides():  # type: () -> None\n        """"""\n        input_shape: [1, 1, 5, 5]\n        output_shape: [1, 1, 2, 2]\n        """"""\n        node = onnx.helper.make_node(\n            \'MaxPool\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            kernel_shape=[2, 2],\n            strides=[2, 2]\n        )\n        x = np.array([[[\n            [1, 2, 3, 4, 5],\n            [6, 7, 8, 9, 10],\n            [11, 12, 13, 14, 15],\n            [16, 17, 18, 19, 20],\n            [21, 22, 23, 24, 25],\n        ]]]).astype(np.float32)\n        y = np.array([[[[7, 9],\n                        [17, 19]]]]).astype(np.float32)\n\n        expect(node, inputs=[x], outputs=[y], name=\'test_maxpool_2d_precomputed_strides\')\n\n    @staticmethod\n    def export_maxpool_with_argmax_2d_precomputed_strides():  # type: () -> None\n        """"""\n        input_shape: [1, 1, 5, 5]\n        output_shape: [1, 1, 2, 2]\n        """"""\n        node = onnx.helper.make_node(\n            \'MaxPool\',\n            inputs=[\'x\'],\n            outputs=[\'y\', \'z\'],\n            kernel_shape=[2, 2],\n            strides=[2, 2],\n            storage_order=1\n        )\n        x = np.array([[[\n            [1, 2, 3, 4, 5],\n            [6, 7, 8, 9, 10],\n            [11, 12, 13, 14, 15],\n            [16, 17, 18, 19, 20],\n            [21, 22, 23, 24, 25],\n        ]]]).astype(np.float32)\n        y = np.array([[[[7, 9],\n                        [17, 19]]]]).astype(np.float32)\n        z = np.array([[[[6, 16],\n                        [8, 18]]]]).astype(np.int64)\n\n        expect(node, inputs=[x], outputs=[y, z], name=\'test_maxpool_with_argmax_2d_precomputed_strides\')\n\n    @staticmethod\n    def export_maxpool_2d_precomputed_same_upper():  # type: () -> None\n        """"""\n        input_shape: [1, 1, 5, 5]\n        output_shape: [1, 1, 3, 3]\n        pad_shape: [2, 2] -> [1, 1, 1, 1] by axis\n        """"""\n        node = onnx.helper.make_node(\n            \'MaxPool\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            kernel_shape=[3, 3],\n            strides=[2, 2],\n            auto_pad=\'SAME_UPPER\'\n        )\n        x = np.array([[[\n            [1, 2, 3, 4, 5],\n            [6, 7, 8, 9, 10],\n            [11, 12, 13, 14, 15],\n            [16, 17, 18, 19, 20],\n            [21, 22, 23, 24, 25],\n        ]]]).astype(np.float32)\n        y = np.array([[[[7, 9, 10],\n                        [17, 19, 20],\n                        [22, 24, 25]]]]).astype(np.float32)\n\n        expect(node, inputs=[x], outputs=[y], name=\'test_maxpool_2d_precomputed_same_upper\')\n\n    @staticmethod\n    def export_maxpool_1d_default():  # type: () -> None\n        """"""\n        input_shape: [1, 3, 32]\n        output_shape: [1, 3, 31]\n        """"""\n        node = onnx.helper.make_node(\n            \'MaxPool\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            kernel_shape=[2],\n        )\n        x = np.random.randn(1, 3, 32).astype(np.float32)\n        x_shape = np.shape(x)\n        kernel_shape = [2]\n        strides = [1]\n        out_shape = get_output_shape(\'VALID\', x_shape[2:], kernel_shape, strides)\n        padded = x\n        y = pool(padded, x_shape, kernel_shape, strides, out_shape, [0], \'MAX\')\n\n        expect(node, inputs=[x], outputs=[y], name=\'test_maxpool_1d_default\')\n\n    @staticmethod\n    def export_maxpool_2d_default():  # type: () -> None\n        """"""\n        input_shape: [1, 3, 32, 32]\n        output_shape: [1, 3, 31, 31]\n        """"""\n        node = onnx.helper.make_node(\n            \'MaxPool\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            kernel_shape=[2, 2],\n        )\n        x = np.random.randn(1, 3, 32, 32).astype(np.float32)\n        x_shape = np.shape(x)\n        kernel_shape = (2, 2)\n        strides = (1, 1)\n        out_shape = get_output_shape(\'VALID\', x_shape[2:], kernel_shape, strides)\n        padded = x\n        y = pool(padded, x_shape, kernel_shape, strides, out_shape, (0, 0), \'MAX\')\n\n        expect(node, inputs=[x], outputs=[y], name=\'test_maxpool_2d_default\')\n\n    @staticmethod\n    def export_maxpool_3d_default():  # type: () -> None\n        """"""\n        input_shape: [1, 3, 32, 32, 32]\n        output_shape: [1, 3, 31, 31, 31]\n        """"""\n        node = onnx.helper.make_node(\n            \'MaxPool\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            kernel_shape=[2, 2, 2],\n        )\n        x = np.random.randn(1, 3, 32, 32, 32).astype(np.float32)\n        x_shape = np.shape(x)\n        kernel_shape = [2, 2, 2]\n        strides = [1, 1, 1]\n        out_shape = get_output_shape(\'VALID\', x_shape[2:], kernel_shape, strides)\n        padded = x\n        y = pool(padded, x_shape, kernel_shape, strides, out_shape, [0, 0, 0], \'MAX\')\n\n        expect(node, inputs=[x], outputs=[y], name=\'test_maxpool_3d_default\')\n\n    @staticmethod\n    def export_maxpool_2d_same_upper():  # type: () -> None\n        """"""\n        input_shape: [1, 3, 32, 32]\n        output_shape: [1, 3, 32, 32]\n        pad_shape: [1, 1] -> [0, 1, 0, 1] by axis\n        """"""\n        node = onnx.helper.make_node(\n            \'MaxPool\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            kernel_shape=[2, 2],\n            auto_pad=\'SAME_UPPER\'\n        )\n        x = np.random.randn(1, 3, 32, 32).astype(np.float32)\n        x_shape = np.shape(x)\n        kernel_shape = (2, 2)\n        strides = (1, 1)\n        out_shape = get_output_shape(\'SAME_UPPER\', x_shape[2:], kernel_shape, strides)\n        pad_shape = get_pad_shape(\'SAME_UPPER\', x_shape[2:], kernel_shape, strides, out_shape)\n        pad_top = pad_shape[0] // 2\n        pad_bottom = pad_shape[0] - pad_top\n        pad_left = pad_shape[1] // 2\n        pad_right = pad_shape[1] - pad_left\n        padded = np.pad(x, ((0, 0), (0, 0), (pad_top, pad_bottom), (pad_left, pad_right)), mode=\'constant\',\n                        constant_values=np.nan)\n        y = pool(padded, x_shape, kernel_shape, strides, out_shape, pad_shape, \'MAX\')\n\n        expect(node, inputs=[x], outputs=[y], name=\'test_maxpool_2d_same_upper\')\n\n    @staticmethod\n    def export_maxpool_2d_same_lower():  # type: () -> None\n        """"""\n        input_shape: [1, 3, 32, 32]\n        output_shape: [1, 3, 32, 32]\n        pad_shape: [1, 1] -> [1, 0, 1, 0] by axis\n        """"""\n        node = onnx.helper.make_node(\n            \'MaxPool\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            kernel_shape=[2, 2],\n            auto_pad=\'SAME_LOWER\'\n        )\n        x = np.random.randn(1, 3, 32, 32).astype(np.float32)\n        x_shape = np.shape(x)\n        kernel_shape = (2, 2)\n        strides = (1, 1)\n        out_shape = get_output_shape(\'SAME_LOWER\', x_shape[2:], kernel_shape, strides)\n        pad_shape = get_pad_shape(\'SAME_LOWER\', x_shape[2:], kernel_shape, strides, out_shape)\n        pad_bottom = pad_shape[0] // 2\n        pad_top = pad_shape[0] - pad_bottom\n        pad_right = pad_shape[1] // 2\n        pad_left = pad_shape[1] - pad_right\n        padded = np.pad(x, ((0, 0), (0, 0), (pad_top, pad_bottom), (pad_left, pad_right)), mode=\'constant\',\n                        constant_values=np.nan)\n        y = pool(padded, x_shape, kernel_shape, strides, out_shape, pad_shape, \'MAX\')\n\n        expect(node, inputs=[x], outputs=[y], name=\'test_maxpool_2d_same_lower\')\n\n    @staticmethod\n    def export_maxpool_2d_pads():  # type: () -> None\n        """"""\n        input_shape: [1, 3, 28, 28]\n        output_shape: [1, 3, 30, 30]\n        pad_shape: [4, 4] -> [2, 2, 2, 2] by axis\n        """"""\n        node = onnx.helper.make_node(\n            \'MaxPool\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            kernel_shape=[3, 3],\n            pads=[2, 2, 2, 2]\n        )\n        x = np.random.randn(1, 3, 28, 28).astype(np.float32)\n        x_shape = np.shape(x)\n        kernel_shape = (3, 3)\n        strides = (1, 1)\n        pad_bottom = pad_top = pad_right = pad_left = 2\n        pad_shape = [pad_top + pad_bottom, pad_left + pad_right]\n        out_shape = get_output_shape(\'VALID\', np.add(x_shape[2:], pad_shape), kernel_shape, strides)\n        padded = np.pad(x, ((0, 0), (0, 0), (pad_top, pad_bottom), (pad_left, pad_right)), mode=\'constant\',\n                        constant_values=np.nan)\n        y = pool(padded, x_shape, kernel_shape, strides, out_shape, pad_shape, \'MAX\')\n\n        expect(node, inputs=[x], outputs=[y], name=\'test_maxpool_2d_pads\')\n\n    @staticmethod\n    def export_maxpool_2d_strides():  # type: () -> None\n        """"""\n        input_shape: [1, 3, 32, 32]\n        output_shape: [1, 3, 10, 10]\n        """"""\n        node = onnx.helper.make_node(\n            \'MaxPool\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            kernel_shape=[5, 5],\n            strides=[3, 3]\n        )\n        x = np.random.randn(1, 3, 32, 32).astype(np.float32)\n        x_shape = np.shape(x)\n        kernel_shape = (5, 5)\n        strides = (3, 3)\n        out_shape = get_output_shape(\'VALID\', x_shape[2:], kernel_shape, strides)\n        padded = x\n        y = pool(padded, x_shape, kernel_shape, strides, out_shape, (0, 0), \'MAX\')\n\n        expect(node, inputs=[x], outputs=[y], name=\'test_maxpool_2d_strides\')\n\n    @staticmethod\n    def export_maxpool_2d_ceil():  # type: () -> None\n        """"""\n        input_shape: [1, 1, 4, 4]\n        output_shape: [1, 1, 2, 2]\n        """"""\n        node = onnx.helper.make_node(\n            \'MaxPool\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            kernel_shape=[3, 3],\n            strides=[2, 2],\n            ceil_mode=True\n        )\n        x = np.array([[[\n            [1, 2, 3, 4],\n            [5, 6, 7, 8],\n            [9, 10, 11, 12],\n            [13, 14, 15, 16],\n        ]]]).astype(np.float32)\n        y = np.array([[[\n            [11, 12],\n            [15, 16]]]]).astype(np.float32)\n\n        expect(node, inputs=[x], outputs=[y], name=\'test_maxpool_2d_ceil\')\n\n    @staticmethod\n    def export_maxpool_2d_dilations():  # type: () -> None\n        """"""\n        input_shape: [1, 1, 4, 4]\n        output_shape: [1, 1, 2, 2]\n        """"""\n        node = onnx.helper.make_node(\n            \'MaxPool\',\n            inputs=[\'x\'],\n            outputs=[\'y\'],\n            kernel_shape=[2, 2],\n            strides=[1, 1],\n            dilations=[2, 2]\n        )\n        x = np.array([[[\n            [1, 2, 3, 4],\n            [5, 6, 7, 8],\n            [9, 10, 11, 12],\n            [13, 14, 15, 16],\n        ]]]).astype(np.float32)\n        y = np.array([[[\n            [11, 12],\n            [15, 16]]]]).astype(np.float32)\n\n        expect(node, inputs=[x], outputs=[y], name=\'test_maxpool_2d_dilations\')\n'"
onnx/backend/test/case/node/maxunpool.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass MaxUnpool(Base):\n\n    @staticmethod\n    def export_without_output_shape():  # type: () -> None\n        node = onnx.helper.make_node(\n            'MaxUnpool',\n            inputs=['xT', 'xI'],\n            outputs=['y'],\n            kernel_shape=[2, 2],\n            strides=[2, 2]\n        )\n        xT = np.array([[[[1, 2],\n                         [3, 4]]]], dtype=np.float32)\n        xI = np.array([[[[5, 7],\n                         [13, 15]]]], dtype=np.int64)\n        y = np.array([[[[0, 0, 0, 0],\n                        [0, 1, 0, 2],\n                        [0, 0, 0, 0],\n                        [0, 3, 0, 4]]]], dtype=np.float32)\n        expect(node, inputs=[xT, xI], outputs=[y], name='test_maxunpool_export_without_output_shape')\n\n    @staticmethod\n    def export_with_output_shape():  # type: () -> None\n        node = onnx.helper.make_node(\n            'MaxUnpool',\n            inputs=['xT', 'xI', 'output_shape'],\n            outputs=['y'],\n            kernel_shape=[2, 2],\n            strides=[2, 2]\n        )\n        xT = np.array([[[[5, 6],\n                         [7, 8]]]], dtype=np.float32)\n        xI = np.array([[[[5, 7],\n                         [13, 15]]]], dtype=np.int64)\n        output_shape = np.array((1, 1, 5, 5), dtype=np.int64)\n        y = np.array([[[[0, 0, 0, 0, 0],\n                        [0, 5, 0, 6, 0],\n                        [0, 0, 0, 0, 0],\n                        [0, 7, 0, 8, 0],\n                        [0, 0, 0, 0, 0]]]], dtype=np.float32)\n        expect(node, inputs=[xT, xI, output_shape], outputs=[y], name='test_maxunpool_export_with_output_shape')\n"""
onnx/backend/test/case/node/mean.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Mean(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        data_0 = np.array([3, 0, 2]).astype(np.float32)\n        data_1 = np.array([1, 3, 4]).astype(np.float32)\n        data_2 = np.array([2, 6, 6]).astype(np.float32)\n        result = np.array([2, 3, 4]).astype(np.float32)\n        node = onnx.helper.make_node(\n            'Mean',\n            inputs=['data_0', 'data_1', 'data_2'],\n            outputs=['result'],\n        )\n        expect(node, inputs=[data_0, data_1, data_2], outputs=[result],\n               name='test_mean_example')\n\n        node = onnx.helper.make_node(\n            'Mean',\n            inputs=['data_0'],\n            outputs=['result'],\n        )\n        expect(node, inputs=[data_0], outputs=[data_0],\n               name='test_mean_one_input')\n\n        result = np.divide(np.add(data_0, data_1), 2.)\n        node = onnx.helper.make_node(\n            'Mean',\n            inputs=['data_0', 'data_1'],\n            outputs=['result'],\n        )\n        expect(node, inputs=[data_0, data_1], outputs=[result],\n               name='test_mean_two_inputs')\n"""
onnx/backend/test/case/node/meanvariancenormalization.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass MeanVarianceNormalization(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'MeanVarianceNormalization',\n            inputs=['X'],\n            outputs=['Y']\n        )\n\n        input_data = np.array([[[[0.8439683], [0.5665144], [0.05836735]],\n            [[0.02916367], [0.12964272], [0.5060197]],\n            [[0.79538304], [0.9411346], [0.9546573]]],\n            [[[0.17730942], [0.46192095], [0.26480448]],\n            [[0.6746842], [0.01665257], [0.62473077]],\n            [[0.9240844], [0.9722341], [0.11965699]]],\n            [[[0.41356155], [0.9129373], [0.59330076]],\n            [[0.81929934], [0.7862604], [0.11799799]],\n            [[0.69248444], [0.54119414], [0.07513223]]]], dtype=np.float32)\n\n        # Calculate expected output data\n        data_mean = np.mean(input_data, axis=(0, 2, 3), keepdims=1)\n        data_mean_squared = np.power(data_mean, 2)\n        data_squared = np.power(input_data, 2)\n        data_squared_mean = np.mean(data_squared, axis=(0, 2, 3), keepdims=1)\n        std = np.sqrt(data_squared_mean - data_mean_squared)\n        expected_output = (input_data - data_mean) / (std + 1e-9)\n\n        expect(node, inputs=[input_data], outputs=[expected_output],\n               name='test_mvn')\n"""
onnx/backend/test/case/node/min.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\nfrom ..utils import all_numeric_dtypes\n\n\nclass Min(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        data_0 = np.array([3, 2, 1]).astype(np.float32)\n        data_1 = np.array([1, 4, 4]).astype(np.float32)\n        data_2 = np.array([2, 5, 0]).astype(np.float32)\n        result = np.array([1, 2, 0]).astype(np.float32)\n        node = onnx.helper.make_node(\n            'Min',\n            inputs=['data_0', 'data_1', 'data_2'],\n            outputs=['result'],\n        )\n        expect(node, inputs=[data_0, data_1, data_2], outputs=[result],\n               name='test_min_example')\n\n        node = onnx.helper.make_node(\n            'Min',\n            inputs=['data_0'],\n            outputs=['result'],\n        )\n        expect(node, inputs=[data_0], outputs=[data_0],\n               name='test_min_one_input')\n\n        result = np.minimum(data_0, data_1)\n        node = onnx.helper.make_node(\n            'Min',\n            inputs=['data_0', 'data_1'],\n            outputs=['result'],\n        )\n        expect(node, inputs=[data_0, data_1], outputs=[result],\n               name='test_min_two_inputs')\n\n    @staticmethod\n    def export_min_all_numeric_types():  # type: () -> None\n        for op_dtype in all_numeric_dtypes:\n            data_0 = np.array([3, 2, 1]).astype(op_dtype)\n            data_1 = np.array([1, 4, 4]).astype(op_dtype)\n            result = np.array([1, 2, 1]).astype(op_dtype)\n            node = onnx.helper.make_node(\n                'Min',\n                inputs=['data_0', 'data_1'],\n                outputs=['result'],\n            )\n            expect(node, inputs=[data_0, data_1], outputs=[result],\n                   name='test_min_{0}'.format(np.dtype(op_dtype).name))\n"""
onnx/backend/test/case/node/mod.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Mod(Base):\n\n    @staticmethod\n    def export_mod_mixed_sign_float64():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Mod',\n            inputs=['x', 'y'],\n            outputs=['z'],\n            fmod=1\n        )\n\n        x = np.array([-4.3, 7.2, 5.0, 4.3, -7.2, 8.0]).astype(np.float64)\n        y = np.array([2.1, -3.4, 8.0, -2.1, 3.4, 5.0]).astype(np.float64)\n        z = np.fmod(x, y)  # expected output [-0.1,  0.4,  5. ,  0.1, -0.4,  3.]\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_mod_mixed_sign_float64')\n\n    @staticmethod\n    def export_mod_mixed_sign_float32():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Mod',\n            inputs=['x', 'y'],\n            outputs=['z'],\n            fmod=1\n        )\n\n        x = np.array([-4.3, 7.2, 5.0, 4.3, -7.2, 8.0]).astype(np.float32)\n        y = np.array([2.1, -3.4, 8.0, -2.1, 3.4, 5.0]).astype(np.float32)\n        z = np.fmod(x, y)  # expected output [-0.10000038, 0.39999962, 5. , 0.10000038, -0.39999962, 3.]\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_mod_mixed_sign_float32')\n\n    @staticmethod\n    def export_mod_mixed_sign_float16():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Mod',\n            inputs=['x', 'y'],\n            outputs=['z'],\n            fmod=1\n        )\n\n        x = np.array([-4.3, 7.2, 5.0, 4.3, -7.2, 8.0]).astype(np.float16)\n        y = np.array([2.1, -3.4, 8.0, -2.1, 3.4, 5.0]).astype(np.float16)\n        z = np.fmod(x, y)  # expected output [-0.10156, 0.3984 , 5. , 0.10156, -0.3984 ,  3.]\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_mod_mixed_sign_float16')\n\n    @staticmethod\n    def export_mod_mixed_sign_int64():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Mod',\n            inputs=['x', 'y'],\n            outputs=['z'],\n        )\n\n        x = np.array([-4, 7, 5, 4, -7, 8]).astype(np.int64)\n        y = np.array([2, -3, 8, -2, 3, 5]).astype(np.int64)\n        z = np.mod(x, y)  # expected output [ 0, -2,  5,  0,  2,  3]\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_mod_mixed_sign_int64')\n\n    @staticmethod\n    def export_mod_mixed_sign_int32():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Mod',\n            inputs=['x', 'y'],\n            outputs=['z'],\n        )\n\n        x = np.array([-4, 7, 5, 4, -7, 8]).astype(np.int32)\n        y = np.array([2, -3, 8, -2, 3, 5]).astype(np.int32)\n        z = np.mod(x, y)  # expected output [ 0, -2,  5,  0,  2,  3]\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_mod_mixed_sign_int32')\n\n    @staticmethod\n    def export_mod_mixed_sign_int16():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Mod',\n            inputs=['x', 'y'],\n            outputs=['z'],\n        )\n\n        x = np.array([-4, 7, 5, 4, -7, 8]).astype(np.int16)\n        y = np.array([2, -3, 8, -2, 3, 5]).astype(np.int16)\n        z = np.mod(x, y)  # expected output [ 0, -2,  5,  0,  2,  3]\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_mod_mixed_sign_int16')\n\n    @staticmethod\n    def export_mod_mixed_sign_int8():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Mod',\n            inputs=['x', 'y'],\n            outputs=['z'],\n        )\n\n        x = np.array([-4, 7, 5, 4, -7, 8]).astype(np.int8)\n        y = np.array([2, -3, 8, -2, 3, 5]).astype(np.int8)\n        z = np.mod(x, y)  # expected output [ 0, -2,  5,  0,  2,  3]\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_mod_mixed_sign_int8')\n\n    @staticmethod\n    def export_mod_uint8():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Mod',\n            inputs=['x', 'y'],\n            outputs=['z'],\n        )\n\n        x = np.array([4, 7, 5]).astype(np.uint8)\n        y = np.array([2, 3, 8]).astype(np.uint8)\n        z = np.mod(x, y)  # expected output [0, 1, 5]\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_mod_uint8')\n\n    @staticmethod\n    def export_mod_uint16():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Mod',\n            inputs=['x', 'y'],\n            outputs=['z'],\n        )\n\n        x = np.array([4, 7, 5]).astype(np.uint16)\n        y = np.array([2, 3, 8]).astype(np.uint16)\n        z = np.mod(x, y)  # expected output [0, 1, 5]\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_mod_uint16')\n\n    @staticmethod\n    def export_mod_uint32():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Mod',\n            inputs=['x', 'y'],\n            outputs=['z'],\n        )\n\n        x = np.array([4, 7, 5]).astype(np.uint32)\n        y = np.array([2, 3, 8]).astype(np.uint32)\n        z = np.mod(x, y)  # expected output [0, 1, 5]\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_mod_uint32')\n\n    @staticmethod\n    def export_mod_uint64():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Mod',\n            inputs=['x', 'y'],\n            outputs=['z'],\n        )\n\n        x = np.array([4, 7, 5]).astype(np.uint64)\n        y = np.array([2, 3, 8]).astype(np.uint64)\n        z = np.mod(x, y)  # expected output [0, 1, 5]\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_mod_uint64')\n\n    @staticmethod\n    def export_mod_int64_fmod():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Mod',\n            inputs=['x', 'y'],\n            outputs=['z'],\n            fmod=1\n        )\n\n        x = np.array([-4, 7, 5, 4, -7, 8]).astype(np.int64)\n        y = np.array([2, -3, 8, -2, 3, 5]).astype(np.int64)\n        z = np.fmod(x, y)  # expected output [ 0,  1,  5,  0, -1,  3]\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_mod_int64_fmod')\n\n    @staticmethod\n    def export_mod_broadcast():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Mod',\n            inputs=['x', 'y'],\n            outputs=['z'],\n        )\n\n        x = np.arange(0, 30).reshape([3, 2, 5])\n        y = np.array([7])\n        z = np.mod(x, y)\n        z\n        #   array([[[0, 1, 2, 3, 4],\n        #     [5, 6, 0, 1, 2]],\n\n        #    [[3, 4, 5, 6, 0],\n        #     [1, 2, 3, 4, 5]],\n\n        #    [[6, 0, 1, 2, 3],\n        #     [4, 5, 6, 0, 1]]], dtype=int32)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_mod_broadcast')\n"""
onnx/backend/test/case/node/momentum.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom onnx.defs import ONNX_DOMAIN, AI_ONNX_PREVIEW_TRAINING_DOMAIN\nfrom ..base import Base\nfrom . import expect\n\n\ndef apply_momentum(r, t, x, g, v, norm_coefficient, alpha, beta):  # type: ignore\n    # Add gradient of regularization term.\n    g_regularized = norm_coefficient * x + g\n    # Coefficient of gradient should be 1 at the first iteration.\n    beta_adjusted = beta if t > 0 else 1\n    # Update momentum.\n    v_new = alpha * v + beta_adjusted * g_regularized\n    # Apply SG with momentum update rule.\n    x_new = x - r * v_new\n    return x_new, v_new\n\n\ndef apply_nesterov(r, t, x, g, v, norm_coefficient, alpha, beta):  # type: ignore\n    # Add gradient of regularization term.\n    g_regularized = norm_coefficient * x + g\n    # Coefficient of gradient should be 1 at the first iteration.\n    beta_adjusted = beta if t > 0 else 1\n    # Update momentum.\n    v_new = alpha * v + beta_adjusted * g_regularized\n    # Apply Nesterov with momentum update rule.\n    x_new = x - r * (g_regularized + alpha * v_new)\n    return x_new, v_new\n\n\nclass Momentum(Base):\n\n    @staticmethod\n    def export_momentum():  # type: () -> None\n        # Define operator attributes.\n        norm_coefficient = 0.001\n        alpha = 0.95\n        beta = 0.1\n\n        # Create operator.\n        node = onnx.helper.make_node('Momentum',\n                                     inputs=['R', 'T', 'X', 'G', 'V'],\n                                     outputs=['X_new', 'V_new'],\n                                     norm_coefficient=norm_coefficient,\n                                     alpha=alpha,\n                                     beta=beta,\n                                     mode='standard',\n                                     domain=AI_ONNX_PREVIEW_TRAINING_DOMAIN\n                                     )\n\n        # Define operator inputs.\n        r = np.array(0.1, dtype=np.float32)  # scalar\n        t = np.array(0, dtype=np.int64)  # scalar\n        x = np.array([1.2, 2.8], dtype=np.float32)\n        g = np.array([-0.94, -2.5], dtype=np.float32)\n        v = np.array([1.7, 3.6], dtype=np.float32)\n\n        # Compute expected outputs of Momentum.\n        x_new, v_new = apply_momentum(r, t, x, g, v,\n                                      norm_coefficient, alpha, beta)\n\n        # Check results.\n        expect(node, inputs=[r, t, x, g, v],\n               outputs=[x_new, v_new], name='test_momentum',\n               opset_imports=[onnx.helper.make_opsetid(AI_ONNX_PREVIEW_TRAINING_DOMAIN, 1)])\n\n    @staticmethod\n    def export_nesterov_momentum():  # type: () -> None\n        # Define operator attributes.\n        norm_coefficient = 0.01\n        alpha = 0.95\n        beta = 1.0\n\n        # Create operator.\n        node = onnx.helper.make_node('Momentum',\n                                     inputs=['R', 'T', 'X', 'G', 'V'],\n                                     outputs=['X_new', 'V_new'],\n                                     norm_coefficient=norm_coefficient,\n                                     alpha=alpha,\n                                     beta=beta,\n                                     mode='nesterov',\n                                     domain=AI_ONNX_PREVIEW_TRAINING_DOMAIN\n                                     )\n\n        # Define operator inputs.\n        r = np.array(0.1, dtype=np.float32)  # scalar\n        t = np.array(0, dtype=np.int64)  # scalar\n        x = np.array([1.2, 2.8], dtype=np.float32)\n        g = np.array([-0.94, -2.5], dtype=np.float32)\n        v = np.array([1.7, 3.6], dtype=np.float32)\n\n        # Compute expected outputs of Momentum.\n        x_new, v_new = apply_nesterov(r, t, x, g, v,\n                                      norm_coefficient, alpha, beta)\n\n        # Check results.\n        expect(node, inputs=[r, t, x, g, v],\n               outputs=[x_new, v_new], name='test_nesterov_momentum',\n               opset_imports=[onnx.helper.make_opsetid(AI_ONNX_PREVIEW_TRAINING_DOMAIN, 1)])\n\n    @staticmethod\n    def export_momentum_multiple():  # type: () -> None\n        # Define operator attributes.\n        norm_coefficient = 0.001\n        alpha = 0.95\n        beta = 0.85\n\n        node = onnx.helper.make_node('Momentum',\n                                     inputs=['R', 'T', 'X1', 'X2',\n                                             'G1', 'G2', 'H1', 'H2'],\n                                     outputs=['X1_new', 'X2_new',\n                                              'V1_new', 'V2_new'],\n                                     norm_coefficient=norm_coefficient,\n                                     alpha=alpha,\n                                     beta=beta,\n                                     mode='standard',\n                                     domain=AI_ONNX_PREVIEW_TRAINING_DOMAIN\n                                     )\n\n        # Define operator inputs.\n        r = np.array(0.1, dtype=np.float32)  # scalar\n        t = np.array(0, dtype=np.int64)  # scalar\n\n        x1 = np.array([1.0], dtype=np.float32)\n        g1 = np.array([-1.0], dtype=np.float32)\n        v1 = np.array([2.0], dtype=np.float32)\n\n        x2 = np.array([1.0, 2.0], dtype=np.float32)\n        g2 = np.array([-1.0, -3.0], dtype=np.float32)\n        v2 = np.array([4.0, 1.0], dtype=np.float32)\n\n        # Compute expected outputs of Momentum.\n        x1_new, v1_new = apply_momentum(r, t, x1, g1, v1,\n                                        norm_coefficient, alpha, beta)\n        x2_new, v2_new = apply_momentum(r, t, x2, g2, v2,\n                                        norm_coefficient, alpha, beta)\n\n        # Check results.\n        expect(node, inputs=[r, t, x1, x2, g1, g2, v1, v2],\n               outputs=[x1_new, x2_new, v1_new, v2_new], name='test_momentum_multiple',\n               opset_imports=[onnx.helper.make_opsetid(AI_ONNX_PREVIEW_TRAINING_DOMAIN, 1)])\n"""
onnx/backend/test/case/node/mul.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Mul(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Mul',\n            inputs=['x', 'y'],\n            outputs=['z'],\n        )\n\n        x = np.array([1, 2, 3]).astype(np.float32)\n        y = np.array([4, 5, 6]).astype(np.float32)\n        z = x * y  # expected output [4., 10., 18.]\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_mul_example')\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.random.randn(3, 4, 5).astype(np.float32)\n        z = x * y\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_mul')\n\n    @staticmethod\n    def export_mul_broadcast():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Mul',\n            inputs=['x', 'y'],\n            outputs=['z'],\n        )\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.random.randn(5).astype(np.float32)\n        z = x * y\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_mul_bcast')\n"""
onnx/backend/test/case/node/neg.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Neg(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Neg',\n            inputs=['x'],\n            outputs=['y'],\n        )\n\n        x = np.array([-4, 2]).astype(np.float32)\n        y = np.negative(x)  # expected output [4., -2.],\n        expect(node, inputs=[x], outputs=[y],\n               name='test_neg_example')\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.negative(x)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_neg')\n"""
onnx/backend/test/case/node/negativeloglikelihoodloss.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\ndef compute_negative_log_likelihood_loss(input, target, weight=None, reduction=\'mean\', ignore_index=None):  # type: ignore\n    input_shape = input.shape\n    if len(input_shape) == 1:\n        raise RuntimeError(""Unsupported shape"")\n\n    target_shape = target.shape\n    N = input_shape[0]\n    C = input_shape[1]\n\n    # initialize the positional weights when required\n    gather_weight = None\n    if weight is not None:\n        # setting mode=\'clip\' to deal with ignore_index > C or < 0 cases.\n        # when the target value is > C or < 0, it doesn\'t matter which value we are\n        # taking in gather_weight, since it will be set to 0 in the following if-block\n        gather_weight = np.take(weight, target, mode=\'clip\')\n        # set `ignore_index`\'s loss weight to 0.\n        # The loss tensor will be multiplied by this weight tensor,\n        # so `ingore_index`\'s loss value will be eliminated.\n        if ignore_index is not None:\n            gather_weight = np.where(target == ignore_index, 0, gather_weight).astype(dtype=np.float32)\n    elif ignore_index is not None:\n        gather_weight = np.where(target == ignore_index, 0, 1).astype(dtype=np.float32)\n\n    # if input is 4-d and above, make it 3-d\n    if len(input_shape) != 3:\n        input = input.reshape((N, C, -1))\n        target = target.reshape((N, -1))\n\n    # Get a dimension from the reshaped input.\n    # If the original input shape is [N, C, H, W],\n    # the D here should be H * W because we reshape\n    # [N, C, H, W] to [N, C, H * W].\n    D = input.shape[2]\n    neg_gather_element_input = np.zeros((N, D), dtype=np.float32)\n    for i in range(N):\n        for d in range(D):\n            if target[i][d] != ignore_index:\n                neg_gather_element_input[i][d] = -input[i][target[i][d]][d]\n\n    loss = neg_gather_element_input\n\n    # if the input was 4-d or above reshape to the right shape\n    if len(input_shape) != 3:\n        loss = loss.reshape(target_shape)\n\n    # apply the weights when required\n    if gather_weight is not None:\n        loss = gather_weight * loss\n        if reduction == \'mean\':\n            loss = loss.sum() / gather_weight.sum()\n            return loss\n\n    if reduction == \'mean\':\n        loss = np.mean(loss)\n    elif reduction == \'sum\':\n        loss = np.sum(loss)\n    return loss\n\n\nclass NegativeLogLikelihoodLoss(Base):\n\n    @staticmethod\n    def export_input_shape_is_NC():  # type: () -> None\n        reduction = \'none\'\n        node = onnx.helper.make_node(\n            \'NegativeLogLikelihoodLoss\',\n            inputs=[\'input\', \'target\'],\n            outputs=[\'loss\'],\n            reduction=reduction\n        )\n\n        N, C = 3, 5\n        np.random.seed(0)\n        input = np.random.rand(N, C).astype(np.float32)\n        target = np.random.randint(0, high=C, size=(N, ))\n\n        negative_log_likelihood_loss = compute_negative_log_likelihood_loss(input, target, weight=None, reduction=reduction)\n\n        expect(node, inputs=[input, target], outputs=[negative_log_likelihood_loss],\n            name=\'test_nllloss_NC\')\n\n    @staticmethod\n    def export_input_shape_is_NCd1d2():  # type: () -> None\n        reduction = \'none\'\n        node = onnx.helper.make_node(\n            \'NegativeLogLikelihoodLoss\',\n            inputs=[\'input\', \'target\'],\n            outputs=[\'loss\'],\n            reduction=reduction\n        )\n\n        N, C, dim1, dim2 = 3, 5, 6, 6\n        np.random.seed(0)\n        input = np.random.rand(N, C, dim1, dim2).astype(np.float32)\n        target = np.random.randint(0, high=C, size=(N, dim1, dim2))\n\n        negative_log_likelihood_loss = compute_negative_log_likelihood_loss(input, target, weight=None, reduction=reduction)\n\n        expect(node, inputs=[input, target], outputs=[negative_log_likelihood_loss],\n            name=\'test_nllloss_NCd1d2\')\n\n    @staticmethod\n    def export_input_shape_is_NCd1d2_reduction_mean():  # type: () -> None\n        reduction = \'mean\'\n        node = onnx.helper.make_node(\n            \'NegativeLogLikelihoodLoss\',\n            inputs=[\'input\', \'target\'],\n            outputs=[\'loss\'],\n            reduction=reduction\n        )\n\n        N, C, dim1, dim2 = 3, 5, 6, 6\n        np.random.seed(0)\n        input = np.random.rand(N, C, dim1, dim2).astype(np.float32)\n        target = np.random.randint(0, high=C, size=(N, dim1, dim2))\n\n        negative_log_likelihood_loss = compute_negative_log_likelihood_loss(input, target, weight=None, reduction=reduction)\n\n        expect(node, inputs=[input, target], outputs=[negative_log_likelihood_loss],\n            name=\'test_nllloss_NCd1d2_reduction_mean\')\n\n    @staticmethod\n    def export_input_shape_is_NCd1d2_reduction_sum():  # type: () -> None\n        reduction = \'sum\'\n        node = onnx.helper.make_node(\n            \'NegativeLogLikelihoodLoss\',\n            inputs=[\'input\', \'target\'],\n            outputs=[\'loss\'],\n            reduction=reduction\n        )\n\n        N, C, dim1, dim2 = 3, 5, 6, 6\n        np.random.seed(0)\n        input = np.random.rand(N, C, dim1, dim2).astype(np.float32)\n        target = np.random.randint(0, high=C, size=(N, dim1, dim2))\n\n        negative_log_likelihood_loss = compute_negative_log_likelihood_loss(input, target, weight=None, reduction=reduction)\n\n        expect(node, inputs=[input, target], outputs=[negative_log_likelihood_loss],\n            name=\'test_nllloss_NCd1d2_reduction_sum\')\n\n    @staticmethod\n    def export_input_shape_is_NCd1d2_with_weight():  # type: () -> None\n        reduction = \'none\'\n        node = onnx.helper.make_node(\n            \'NegativeLogLikelihoodLoss\',\n            inputs=[\'input\', \'target\', \'weight\'],\n            outputs=[\'loss\'],\n            reduction=reduction\n        )\n\n        N, C, dim1, dim2 = 3, 5, 6, 6\n        np.random.seed(0)\n        input = np.random.rand(N, C, dim1, dim2).astype(np.float32)\n        target = np.random.randint(0, high=C, size=(N, dim1, dim2))\n        weight = np.random.rand(C).astype(np.float32)\n\n        negative_log_likelihood_loss = compute_negative_log_likelihood_loss(input, target, weight=weight, reduction=reduction)\n\n        expect(node, inputs=[input, target, weight], outputs=[negative_log_likelihood_loss],\n            name=\'test_nllloss_NCd1d2_with_weight\')\n\n    @staticmethod\n    def export_input_shape_is_NCd1d2_with_weight_reduction_mean():  # type: () -> None\n        reduction = \'mean\'\n        node = onnx.helper.make_node(\n            \'NegativeLogLikelihoodLoss\',\n            inputs=[\'input\', \'target\', \'weight\'],\n            outputs=[\'loss\'],\n            reduction=reduction\n        )\n\n        N, C, dim1, dim2 = 3, 5, 6, 6\n        np.random.seed(0)\n        input = np.random.rand(N, C, dim1, dim2).astype(np.float32)\n        target = np.random.randint(0, high=C, size=(N, dim1, dim2))\n        weight = np.random.rand(C).astype(np.float32)\n\n        negative_log_likelihood_loss = compute_negative_log_likelihood_loss(input, target, weight=weight, reduction=reduction)\n\n        expect(node, inputs=[input, target, weight], outputs=[negative_log_likelihood_loss],\n            name=\'test_nllloss_NCd1d2_with_weight_reduction_mean\')\n\n    @staticmethod\n    def export_input_shape_is_NCd1d2_with_weight_reduction_sum():  # type: () -> None\n        reduction = \'sum\'\n        node = onnx.helper.make_node(\n            \'NegativeLogLikelihoodLoss\',\n            inputs=[\'input\', \'target\', \'weight\'],\n            outputs=[\'loss\'],\n            reduction=reduction\n        )\n\n        N, C, dim1, dim2 = 3, 5, 6, 6\n        np.random.seed(0)\n        input = np.random.rand(N, C, dim1, dim2).astype(np.float32)\n        target = np.random.randint(0, high=C, size=(N, dim1, dim2))\n        weight = np.random.rand(C).astype(np.float32)\n\n        negative_log_likelihood_loss = compute_negative_log_likelihood_loss(input, target, weight=weight, reduction=reduction)\n\n        expect(node, inputs=[input, target, weight], outputs=[negative_log_likelihood_loss],\n            name=\'test_nllloss_NCd1d2_with_weight_reduction_sum\')\n\n    @staticmethod\n    def export_input_shape_is_NCd1d2_with_weight_reduction_sum_ignore_index():  # type: () -> None\n        reduction = \'sum\'\n        ignore_index = np.int64(0)\n        node = onnx.helper.make_node(\n            \'NegativeLogLikelihoodLoss\',\n            inputs=[\'input\', \'target\', \'weight\'],\n            outputs=[\'loss\'],\n            reduction=reduction,\n            ignore_index=ignore_index\n        )\n\n        N, C, dim1, dim2 = 3, 5, 6, 6\n        np.random.seed(0)\n        input = np.random.rand(N, C, dim1, dim2).astype(np.float32)\n        target = np.random.randint(0, high=C, size=(N, dim1, dim2))\n        target[0][0][0] = np.int64(0)\n        weight = np.random.rand(C).astype(np.float32)\n\n        negative_log_likelihood_loss = compute_negative_log_likelihood_loss(input, target, weight=weight, reduction=reduction, ignore_index=ignore_index)\n\n        expect(node, inputs=[input, target, weight], outputs=[negative_log_likelihood_loss],\n            name=\'test_nllloss_NCd1d2_with_weight_reduction_sum_ignore_index\')\n\n    @staticmethod\n    def export_input_shape_is_NCd1d2_no_weight_reduction_mean_ignore_index():  # type: () -> None\n        reduction = \'mean\'\n        ignore_index = np.int64(1)\n        node = onnx.helper.make_node(\n            \'NegativeLogLikelihoodLoss\',\n            inputs=[\'input\', \'target\'],\n            outputs=[\'loss\'],\n            reduction=reduction,\n            ignore_index=ignore_index\n        )\n\n        N, C, dim1, dim2 = 3, 5, 6, 6\n        np.random.seed(0)\n        input = np.random.rand(N, C, dim1, dim2).astype(np.float32)\n        target = np.random.randint(0, high=C, size=(N, dim1, dim2))\n        target[0][0][0] = np.int64(1)\n\n        negative_log_likelihood_loss = compute_negative_log_likelihood_loss(input, target, reduction=reduction, ignore_index=ignore_index)\n\n        expect(node, inputs=[input, target], outputs=[negative_log_likelihood_loss],\n            name=\'test_nllloss_NCd1d2_no_weight_reduction_mean_ignore_index\')\n\n    @staticmethod\n    def export_input_shape_is_NCd1():  # type: () -> None\n        reduction = \'mean\'\n        node = onnx.helper.make_node(\n            \'NegativeLogLikelihoodLoss\',\n            inputs=[\'input\', \'target\'],\n            outputs=[\'loss\'],\n            reduction=reduction\n        )\n\n        N, C, d1 = 3, 5, 2\n        np.random.seed(0)\n        input = np.random.rand(N, C, d1).astype(np.float32)\n        target = np.random.randint(0, high=C, size=(N, d1))\n\n        negative_log_likelihood_loss = compute_negative_log_likelihood_loss(input, target, weight=None, reduction=reduction)\n\n        expect(node, inputs=[input, target], outputs=[negative_log_likelihood_loss],\n            name=\'test_nllloss_NCd1\')\n\n    @staticmethod\n    def export_input_shape_is_NCd1_weight():  # type: () -> None\n        reduction = \'mean\'\n        node = onnx.helper.make_node(\n            \'NegativeLogLikelihoodLoss\',\n            inputs=[\'input\', \'target\', \'weight\'],\n            outputs=[\'loss\'],\n            reduction=reduction\n        )\n\n        N, C, d1 = 3, 5, 2\n        np.random.seed(0)\n        input = np.random.rand(N, C, d1).astype(np.float32)\n        target = np.random.randint(0, high=C, size=(N, d1))\n        weight = np.random.rand(C).astype(np.float32)\n\n        negative_log_likelihood_loss = compute_negative_log_likelihood_loss(input, target, weight=weight, reduction=reduction)\n\n        expect(node, inputs=[input, target, weight], outputs=[negative_log_likelihood_loss],\n            name=\'test_nllloss_NCd1_weight\')\n\n    @staticmethod\n    def export_input_shape_is_NCd1_ignore_index():  # type: () -> None\n        reduction = \'mean\'\n        ignore_index = np.int64(1)\n        node = onnx.helper.make_node(\n            \'NegativeLogLikelihoodLoss\',\n            inputs=[\'input\', \'target\'],\n            outputs=[\'loss\'],\n            reduction=reduction,\n            ignore_index=ignore_index\n        )\n\n        N, C, d1 = 3, 5, 2\n        np.random.seed(0)\n        input = np.random.rand(N, C, d1).astype(np.float32)\n        target = np.random.randint(0, high=C, size=(N, d1))\n        target[0][0] = np.int64(1)\n\n        negative_log_likelihood_loss = compute_negative_log_likelihood_loss(input, target, weight=None, reduction=reduction, ignore_index=ignore_index)\n\n        expect(node, inputs=[input, target], outputs=[negative_log_likelihood_loss],\n            name=\'test_nllloss_NCd1_ignore_index\')\n\n    @staticmethod\n    def export_input_shape_is_NCd1_weight_ignore_index():  # type: () -> None\n        reduction = \'mean\'\n        ignore_index = np.int64(1)\n        node = onnx.helper.make_node(\n            \'NegativeLogLikelihoodLoss\',\n            inputs=[\'input\', \'target\', \'weight\'],\n            outputs=[\'loss\'],\n            reduction=reduction,\n            ignore_index=ignore_index\n        )\n\n        N, C, d1 = 3, 5, 2\n        np.random.seed(0)\n        input = np.random.rand(N, C, d1).astype(np.float32)\n        target = np.random.randint(0, high=C, size=(N, d1))\n        target[0][0] = np.int64(1)\n        weight = np.random.rand(C).astype(np.float32)\n\n        negative_log_likelihood_loss = compute_negative_log_likelihood_loss(input, target, weight=weight, reduction=reduction, ignore_index=ignore_index)\n\n        expect(node, inputs=[input, target, weight], outputs=[negative_log_likelihood_loss],\n            name=\'test_nllloss_NCd1_weight_ignore_index\')\n\n    @staticmethod\n    def export_input_shape_is_NCd1d2d3d4d5_mean_weight():  # type: () -> None\n        reduction = \'mean\'\n\n        node = onnx.helper.make_node(\n            \'NegativeLogLikelihoodLoss\',\n            inputs=[\'input\', \'target\', \'weight\'],\n            outputs=[\'loss\'],\n            reduction=reduction)\n\n        N, C, dim1, dim2, dim3, dim4, dim5 = 3, 5, 6, 6, 5, 3, 4\n        np.random.seed(0)\n        input = np.random.rand(N, C, dim1, dim2, dim3, dim4, dim5).astype(np.float32)\n        target = np.random.randint(0, high=C, size=(N, dim1, dim2, dim3, dim4, dim5))\n        weight = np.random.rand(C).astype(np.float32)\n\n        negative_log_likelihood_loss = compute_negative_log_likelihood_loss(input,\n                                                                            target,\n                                                                            weight=weight,\n                                                                            reduction=reduction)\n\n        expect(node, inputs=[input, target, weight], outputs=[negative_log_likelihood_loss],\n            name=\'test_nllloss_NCd1d2d3d4d5_mean_weight\')\n\n    @staticmethod\n    def export_input_shape_is_NCd1d2d3d4d5_none_no_weight():  # type: () -> None\n        reduction = \'none\'\n\n        node = onnx.helper.make_node(\n            \'NegativeLogLikelihoodLoss\',\n            inputs=[\'input\', \'target\'],\n            outputs=[\'loss\'],\n            reduction=reduction)\n\n        N, C, dim1, dim2, dim3, dim4, dim5 = 3, 5, 6, 6, 5, 3, 4\n        np.random.seed(0)\n        input = np.random.rand(N, C, dim1, dim2, dim3, dim4, dim5).astype(np.float32)\n        target = np.random.randint(0, high=C, size=(N, dim1, dim2, dim3, dim4, dim5))\n\n        negative_log_likelihood_loss = compute_negative_log_likelihood_loss(input,\n                                                                            target,\n                                                                            reduction=reduction)\n\n        expect(node, inputs=[input, target], outputs=[negative_log_likelihood_loss],\n            name=\'test_nllloss_NCd1d2d3d4d5_none_no_weight\')\n\n    @staticmethod\n    def export_input_shape_is_NCd1_mean_weight_negative_ignore_index():  # type: () -> None\n        reduction = \'mean\'\n        ignore_index = np.int64(-1)\n\n        node = onnx.helper.make_node(\n            \'NegativeLogLikelihoodLoss\',\n            inputs=[\'input\', \'target\', \'weight\'],\n            outputs=[\'loss\'],\n            reduction=reduction,\n            ignore_index=ignore_index)\n\n        N, C, dim1 = 3, 5, 6\n        np.random.seed(0)\n        input = np.random.rand(N, C, dim1).astype(np.float32)\n        target = np.random.randint(0, high=C, size=(N, dim1))\n        target[0][0] = -1\n        weight = np.random.rand(C).astype(np.float32)\n\n        negative_log_likelihood_loss = compute_negative_log_likelihood_loss(input,\n                                                                            target,\n                                                                            weight=weight,\n                                                                            reduction=reduction,\n                                                                            ignore_index=ignore_index)\n\n        expect(node, inputs=[input, target, weight], outputs=[negative_log_likelihood_loss],\n            name=\'test_nllloss_NCd1_mean_weight_negative_ignore_index\')\n\n    @staticmethod\n    def export_input_shape_is_NCd1d2d3_none_no_weight_negative_ignore_index():  # type: () -> None\n        reduction = \'none\'\n        ignore_index = np.int64(-5)\n\n        node = onnx.helper.make_node(\n            \'NegativeLogLikelihoodLoss\',\n            inputs=[\'input\', \'target\'],\n            outputs=[\'loss\'],\n            reduction=reduction,\n            ignore_index=ignore_index)\n\n        N, C, dim1, dim2, dim3 = 3, 5, 6, 6, 5\n        np.random.seed(0)\n        input = np.random.rand(N, C, dim1, dim2, dim3).astype(np.float32)\n        target = np.random.randint(0, high=C, size=(N, dim1, dim2, dim3))\n        target[0][0][0][0] = -5\n\n        negative_log_likelihood_loss = compute_negative_log_likelihood_loss(input,\n                                                                            target,\n                                                                            reduction=reduction,\n                                                                            ignore_index=ignore_index)\n\n        expect(node, inputs=[input, target], outputs=[negative_log_likelihood_loss],\n            name=\'test_nllloss_NCd1d2d3_none_no_weight_negative_ignore_index\')\n\n    @staticmethod\n    def export_input_shape_is_NCd1d2d3_sum_weight_high_ignore_index():  # type: () -> None\n        reduction = \'sum\'\n        ignore_index = np.int64(10)\n\n        node = onnx.helper.make_node(\n            \'NegativeLogLikelihoodLoss\',\n            inputs=[\'input\', \'target\', \'weight\'],\n            outputs=[\'loss\'],\n            reduction=reduction,\n            ignore_index=ignore_index)\n\n        N, C = 3, 5\n        np.random.seed(0)\n        input = np.random.rand(N, C).astype(np.float32)\n        target = np.random.randint(0, high=C, size=(N))\n        target[0] = 10\n        weight = np.random.rand(C).astype(np.float32)\n\n        negative_log_likelihood_loss = compute_negative_log_likelihood_loss(input,\n                                                                            target,\n                                                                            weight=weight,\n                                                                            reduction=reduction,\n                                                                            ignore_index=ignore_index)\n\n        expect(node, inputs=[input, target, weight], outputs=[negative_log_likelihood_loss],\n            name=\'test_nllloss_NCd1d2d3_sum_weight_high_ignore_index\')\n'"
onnx/backend/test/case/node/nonmaxsuppression.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass NonMaxSuppression(Base):\n\n    @staticmethod\n    def export_nonmaxsuppression_suppress_by_IOU():  # type: () -> None\n        node = onnx.helper.make_node(\n            'NonMaxSuppression',\n            inputs=['boxes', 'scores', 'max_output_boxes_per_class', 'iou_threshold', 'score_threshold'],\n            outputs=['selected_indices']\n        )\n        boxes = np.array([[\n            [0.0, 0.0, 1.0, 1.0],\n            [0.0, 0.1, 1.0, 1.1],\n            [0.0, -0.1, 1.0, 0.9],\n            [0.0, 10.0, 1.0, 11.0],\n            [0.0, 10.1, 1.0, 11.1],\n            [0.0, 100.0, 1.0, 101.0]\n        ]]).astype(np.float32)\n        scores = np.array([[[0.9, 0.75, 0.6, 0.95, 0.5, 0.3]]]).astype(np.float32)\n        max_output_boxes_per_class = np.array([3]).astype(np.int64)\n        iou_threshold = np.array([0.5]).astype(np.float32)\n        score_threshold = np.array([0.0]).astype(np.float32)\n        selected_indices = np.array([[0, 0, 3], [0, 0, 0], [0, 0, 5]]).astype(np.int64)\n\n        expect(node, inputs=[boxes, scores, max_output_boxes_per_class, iou_threshold, score_threshold], outputs=[selected_indices], name='test_nonmaxsuppression_suppress_by_IOU')\n\n    @staticmethod\n    def export_nonmaxsuppression_suppress_by_IOU_and_scores():  # type: () -> None\n        node = onnx.helper.make_node(\n            'NonMaxSuppression',\n            inputs=['boxes', 'scores', 'max_output_boxes_per_class', 'iou_threshold', 'score_threshold'],\n            outputs=['selected_indices']\n        )\n        boxes = np.array([[\n            [0.0, 0.0, 1.0, 1.0],\n            [0.0, 0.1, 1.0, 1.1],\n            [0.0, -0.1, 1.0, 0.9],\n            [0.0, 10.0, 1.0, 11.0],\n            [0.0, 10.1, 1.0, 11.1],\n            [0.0, 100.0, 1.0, 101.0]\n        ]]).astype(np.float32)\n        scores = np.array([[[0.9, 0.75, 0.6, 0.95, 0.5, 0.3]]]).astype(np.float32)\n        max_output_boxes_per_class = np.array([3]).astype(np.int64)\n        iou_threshold = np.array([0.5]).astype(np.float32)\n        score_threshold = np.array([0.4]).astype(np.float32)\n        selected_indices = np.array([[0, 0, 3], [0, 0, 0]]).astype(np.int64)\n\n        expect(node, inputs=[boxes, scores, max_output_boxes_per_class, iou_threshold, score_threshold], outputs=[selected_indices], name='test_nonmaxsuppression_suppress_by_IOU_and_scores')\n\n    @staticmethod\n    def export_nonmaxsuppression_flipped_coordinates():  # type: () -> None\n        node = onnx.helper.make_node(\n            'NonMaxSuppression',\n            inputs=['boxes', 'scores', 'max_output_boxes_per_class', 'iou_threshold', 'score_threshold'],\n            outputs=['selected_indices']\n        )\n        boxes = np.array([[\n            [1.0, 1.0, 0.0, 0.0],\n            [0.0, 0.1, 1.0, 1.1],\n            [0.0, 0.9, 1.0, -0.1],\n            [0.0, 10.0, 1.0, 11.0],\n            [1.0, 10.1, 0.0, 11.1],\n            [1.0, 101.0, 0.0, 100.0]\n        ]]).astype(np.float32)\n        scores = np.array([[[0.9, 0.75, 0.6, 0.95, 0.5, 0.3]]]).astype(np.float32)\n        max_output_boxes_per_class = np.array([3]).astype(np.int64)\n        iou_threshold = np.array([0.5]).astype(np.float32)\n        score_threshold = np.array([0.0]).astype(np.float32)\n        selected_indices = np.array([[0, 0, 3], [0, 0, 0], [0, 0, 5]]).astype(np.int64)\n\n        expect(node, inputs=[boxes, scores, max_output_boxes_per_class, iou_threshold, score_threshold], outputs=[selected_indices], name='test_nonmaxsuppression_flipped_coordinates')\n\n    @staticmethod\n    def export_nonmaxsuppression_limit_output_size():  # type: () -> None\n        node = onnx.helper.make_node(\n            'NonMaxSuppression',\n            inputs=['boxes', 'scores', 'max_output_boxes_per_class', 'iou_threshold', 'score_threshold'],\n            outputs=['selected_indices']\n        )\n        boxes = np.array([[\n            [0.0, 0.0, 1.0, 1.0],\n            [0.0, 0.1, 1.0, 1.1],\n            [0.0, -0.1, 1.0, 0.9],\n            [0.0, 10.0, 1.0, 11.0],\n            [0.0, 10.1, 1.0, 11.1],\n            [0.0, 100.0, 1.0, 101.0]\n        ]]).astype(np.float32)\n        scores = np.array([[[0.9, 0.75, 0.6, 0.95, 0.5, 0.3]]]).astype(np.float32)\n        max_output_boxes_per_class = np.array([2]).astype(np.int64)\n        iou_threshold = np.array([0.5]).astype(np.float32)\n        score_threshold = np.array([0.0]).astype(np.float32)\n        selected_indices = np.array([[0, 0, 3], [0, 0, 0]]).astype(np.int64)\n\n        expect(node, inputs=[boxes, scores, max_output_boxes_per_class, iou_threshold, score_threshold], outputs=[selected_indices], name='test_nonmaxsuppression_limit_output_size')\n\n    @staticmethod\n    def export_nonmaxsuppression_single_box():  # type: () -> None\n        node = onnx.helper.make_node(\n            'NonMaxSuppression',\n            inputs=['boxes', 'scores', 'max_output_boxes_per_class', 'iou_threshold', 'score_threshold'],\n            outputs=['selected_indices']\n        )\n        boxes = np.array([[\n            [0.0, 0.0, 1.0, 1.0]\n        ]]).astype(np.float32)\n        scores = np.array([[[0.9]]]).astype(np.float32)\n        max_output_boxes_per_class = np.array([3]).astype(np.int64)\n        iou_threshold = np.array([0.5]).astype(np.float32)\n        score_threshold = np.array([0.0]).astype(np.float32)\n        selected_indices = np.array([[0, 0, 0]]).astype(np.int64)\n\n        expect(node, inputs=[boxes, scores, max_output_boxes_per_class, iou_threshold, score_threshold], outputs=[selected_indices], name='test_nonmaxsuppression_single_box')\n\n    @staticmethod\n    def export_nonmaxsuppression_identical_boxes():  # type: () -> None\n        node = onnx.helper.make_node(\n            'NonMaxSuppression',\n            inputs=['boxes', 'scores', 'max_output_boxes_per_class', 'iou_threshold', 'score_threshold'],\n            outputs=['selected_indices']\n        )\n        boxes = np.array([[\n            [0.0, 0.0, 1.0, 1.0],\n            [0.0, 0.0, 1.0, 1.0],\n            [0.0, 0.0, 1.0, 1.0],\n            [0.0, 0.0, 1.0, 1.0],\n            [0.0, 0.0, 1.0, 1.0],\n\n            [0.0, 0.0, 1.0, 1.0],\n            [0.0, 0.0, 1.0, 1.0],\n            [0.0, 0.0, 1.0, 1.0],\n            [0.0, 0.0, 1.0, 1.0],\n            [0.0, 0.0, 1.0, 1.0]\n        ]]).astype(np.float32)\n        scores = np.array([[[0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9]]]).astype(np.float32)\n        max_output_boxes_per_class = np.array([3]).astype(np.int64)\n        iou_threshold = np.array([0.5]).astype(np.float32)\n        score_threshold = np.array([0.0]).astype(np.float32)\n        selected_indices = np.array([[0, 0, 0]]).astype(np.int64)\n\n        expect(node, inputs=[boxes, scores, max_output_boxes_per_class, iou_threshold, score_threshold], outputs=[selected_indices], name='test_nonmaxsuppression_identical_boxes')\n\n    @staticmethod\n    def export_nonmaxsuppression_center_point_box_format():  # type: () -> None\n        node = onnx.helper.make_node(\n            'NonMaxSuppression',\n            inputs=['boxes', 'scores', 'max_output_boxes_per_class', 'iou_threshold', 'score_threshold'],\n            outputs=['selected_indices'],\n            center_point_box=1\n        )\n        boxes = np.array([[\n            [0.5, 0.5, 1.0, 1.0],\n            [0.5, 0.6, 1.0, 1.0],\n            [0.5, 0.4, 1.0, 1.0],\n            [0.5, 10.5, 1.0, 1.0],\n            [0.5, 10.6, 1.0, 1.0],\n            [0.5, 100.5, 1.0, 1.0]\n        ]]).astype(np.float32)\n        scores = np.array([[[0.9, 0.75, 0.6, 0.95, 0.5, 0.3]]]).astype(np.float32)\n        max_output_boxes_per_class = np.array([3]).astype(np.int64)\n        iou_threshold = np.array([0.5]).astype(np.float32)\n        score_threshold = np.array([0.0]).astype(np.float32)\n        selected_indices = np.array([[0, 0, 3], [0, 0, 0], [0, 0, 5]]).astype(np.int64)\n\n        expect(node, inputs=[boxes, scores, max_output_boxes_per_class, iou_threshold, score_threshold], outputs=[selected_indices], name='test_nonmaxsuppression_center_point_box_format')\n\n    @staticmethod\n    def export_nonmaxsuppression_two_classes():  # type: () -> None\n        node = onnx.helper.make_node(\n            'NonMaxSuppression',\n            inputs=['boxes', 'scores', 'max_output_boxes_per_class', 'iou_threshold', 'score_threshold'],\n            outputs=['selected_indices']\n        )\n        boxes = np.array([[\n            [0.0, 0.0, 1.0, 1.0],\n            [0.0, 0.1, 1.0, 1.1],\n            [0.0, -0.1, 1.0, 0.9],\n            [0.0, 10.0, 1.0, 11.0],\n            [0.0, 10.1, 1.0, 11.1],\n            [0.0, 100.0, 1.0, 101.0]\n        ]]).astype(np.float32)\n        scores = np.array([[[0.9, 0.75, 0.6, 0.95, 0.5, 0.3],\n                            [0.9, 0.75, 0.6, 0.95, 0.5, 0.3]]]).astype(np.float32)\n        max_output_boxes_per_class = np.array([2]).astype(np.int64)\n        iou_threshold = np.array([0.5]).astype(np.float32)\n        score_threshold = np.array([0.0]).astype(np.float32)\n        selected_indices = np.array([[0, 0, 3], [0, 0, 0], [0, 1, 3], [0, 1, 0]]).astype(np.int64)\n\n        expect(node, inputs=[boxes, scores, max_output_boxes_per_class, iou_threshold, score_threshold], outputs=[selected_indices], name='test_nonmaxsuppression_two_classes')\n\n    @staticmethod\n    def export_nonmaxsuppression_two_batches():  # type: () -> None\n        node = onnx.helper.make_node(\n            'NonMaxSuppression',\n            inputs=['boxes', 'scores', 'max_output_boxes_per_class', 'iou_threshold', 'score_threshold'],\n            outputs=['selected_indices']\n        )\n        boxes = np.array([[[0.0, 0.0, 1.0, 1.0],\n                           [0.0, 0.1, 1.0, 1.1],\n                           [0.0, -0.1, 1.0, 0.9],\n                           [0.0, 10.0, 1.0, 11.0],\n                           [0.0, 10.1, 1.0, 11.1],\n                           [0.0, 100.0, 1.0, 101.0]],\n                          [[0.0, 0.0, 1.0, 1.0],\n                           [0.0, 0.1, 1.0, 1.1],\n                           [0.0, -0.1, 1.0, 0.9],\n                           [0.0, 10.0, 1.0, 11.0],\n                           [0.0, 10.1, 1.0, 11.1],\n                           [0.0, 100.0, 1.0, 101.0]]]).astype(np.float32)\n        scores = np.array([[[0.9, 0.75, 0.6, 0.95, 0.5, 0.3]],\n                           [[0.9, 0.75, 0.6, 0.95, 0.5, 0.3]]]).astype(np.float32)\n        max_output_boxes_per_class = np.array([2]).astype(np.int64)\n        iou_threshold = np.array([0.5]).astype(np.float32)\n        score_threshold = np.array([0.0]).astype(np.float32)\n        selected_indices = np.array([[0, 0, 3], [0, 0, 0], [1, 0, 3], [1, 0, 0]]).astype(np.int64)\n\n        expect(node, inputs=[boxes, scores, max_output_boxes_per_class, iou_threshold, score_threshold], outputs=[selected_indices], name='test_nonmaxsuppression_two_batches')\n"""
onnx/backend/test/case/node/nonzero.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass NonZero(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'NonZero',\n            inputs=['condition'],\n            outputs=['result'],\n        )\n\n        condition = np.array([[1, 0], [1, 1]], dtype=np.bool)\n        result = np.array((np.nonzero(condition)))  # expected output [[0, 1, 1], [0, 0, 1]]\n        expect(node, inputs=[condition], outputs=[result],\n               name='test_nonzero_example')\n"""
onnx/backend/test/case/node/not.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Not(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Not',\n            inputs=['x'],\n            outputs=['not'],\n        )\n\n        # 2d\n        x = (np.random.randn(3, 4) > 0).astype(np.bool)\n        expect(node, inputs=[x], outputs=[np.logical_not(x)],\n               name='test_not_2d')\n\n        # 3d\n        x = (np.random.randn(3, 4, 5) > 0).astype(np.bool)\n        expect(node, inputs=[x], outputs=[np.logical_not(x)],\n               name='test_not_3d')\n\n        # 4d\n        x = (np.random.randn(3, 4, 5, 6) > 0).astype(np.bool)\n        expect(node, inputs=[x], outputs=[np.logical_not(x)],\n               name='test_not_4d')\n"""
onnx/backend/test/case/node/onehot.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\ndef one_hot(indices, depth, axis=-1, dtype=np.float32):  # type: ignore\n    ''' Compute one hot from indices at a specific axis '''\n    values = np.asarray(indices)\n    rank = len(values.shape)\n    depth_range = np.arange(depth)\n    if axis < 0:\n        axis += (rank + 1)\n    ls = values.shape[0:axis]\n    rs = values.shape[axis:rank]\n    targets = np.reshape(depth_range, (1,) * len(ls) + depth_range.shape + (1,) * len(rs))\n    values = np.reshape(np.mod(values, depth), ls + (1,) + rs)\n    return np.asarray(targets == values, dtype=dtype)\n\n\nclass OneHot(Base):\n\n    @staticmethod\n    def export_without_axis():  # type: () -> None\n        on_value = 5\n        off_value = 2\n        output_type = np.int32\n        node = onnx.helper.make_node(\n            'OneHot',\n            inputs=['indices', 'depth', 'values'],\n            outputs=['y']\n        )\n        indices = np.array([0, 7, 8], dtype=np.int64)\n        depth = np.float32(12)\n        values = np.array([off_value, on_value], dtype=output_type)\n        y = one_hot(indices, depth, dtype=output_type)\n        y = y * (on_value - off_value) + off_value\n        expect(node, inputs=[indices, depth, values], outputs=[y], name='test_onehot_without_axis')\n\n    @staticmethod\n    def export_with_axis():  # type: () -> None\n        axisValue = 1\n        on_value = 3\n        off_value = 1\n        output_type = np.float32\n        node = onnx.helper.make_node(\n            'OneHot',\n            inputs=['indices', 'depth', 'values'],\n            outputs=['y'],\n            axis=axisValue\n        )\n        indices = np.array([[1, 9],\n                            [2, 4]], dtype=np.float32)\n        depth = np.array([10], dtype=np.float32)\n        values = np.array([off_value, on_value], dtype=output_type)\n        y = one_hot(indices, depth, axis=axisValue, dtype=output_type)\n        y = y * (on_value - off_value) + off_value\n        expect(node, inputs=[indices, depth, values], outputs=[y], name='test_onehot_with_axis')\n\n    @staticmethod\n    def export_with_negative_indices():  # type: () -> None\n        axisValue = 1\n        on_value = 3\n        off_value = 1\n        output_type = np.float32\n        node = onnx.helper.make_node(\n            'OneHot',\n            inputs=['indices', 'depth', 'values'],\n            outputs=['y'],\n            axis=axisValue\n        )\n        indices = np.array([0, -7, -8], dtype=np.int64)\n\n        depth = np.array([10], dtype=np.float32)\n        values = np.array([off_value, on_value], dtype=output_type)\n        y = one_hot(indices, depth, axis=axisValue, dtype=output_type)\n        y = y * (on_value - off_value) + off_value\n        expect(node, inputs=[indices, depth, values], outputs=[y], name='test_onehot_negative_indices')\n\n        # print(y)\n        # [[3. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n        #  [1. 1. 1. 3. 1. 1. 1. 1. 1. 1.]\n        #  [1. 1. 3. 1. 1. 1. 1. 1. 1. 1.]]\n\n    @staticmethod\n    def export_with_negative_axis():  # type: () -> None\n        axisValue = -2\n        on_value = 3\n        off_value = 1\n        output_type = np.float32\n        node = onnx.helper.make_node(\n            'OneHot',\n            inputs=['indices', 'depth', 'values'],\n            outputs=['y'],\n            axis=axisValue\n        )\n        indices = np.array([[1, 9],\n                            [2, 4]], dtype=np.float32)\n        depth = np.array([10], dtype=np.float32)\n        values = np.array([off_value, on_value], dtype=output_type)\n        y = one_hot(indices, depth, axis=axisValue, dtype=output_type)\n        y = y * (on_value - off_value) + off_value\n        expect(node, inputs=[indices, depth, values], outputs=[y], name='test_onehot_with_negative_axis')\n"""
onnx/backend/test/case/node/or.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Or(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Or',\n            inputs=['x', 'y'],\n            outputs=['or'],\n        )\n\n        # 2d\n        x = (np.random.randn(3, 4) > 0).astype(np.bool)\n        y = (np.random.randn(3, 4) > 0).astype(np.bool)\n        z = np.logical_or(x, y)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_or2d')\n\n        # 3d\n        x = (np.random.randn(3, 4, 5) > 0).astype(np.bool)\n        y = (np.random.randn(3, 4, 5) > 0).astype(np.bool)\n        z = np.logical_or(x, y)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_or3d')\n\n        # 4d\n        x = (np.random.randn(3, 4, 5, 6) > 0).astype(np.bool)\n        y = (np.random.randn(3, 4, 5, 6) > 0).astype(np.bool)\n        z = np.logical_or(x, y)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_or4d')\n\n    @staticmethod\n    def export_or_broadcast():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Or',\n            inputs=['x', 'y'],\n            outputs=['or'],\n        )\n\n        # 3d vs 1d\n        x = (np.random.randn(3, 4, 5) > 0).astype(np.bool)\n        y = (np.random.randn(5) > 0).astype(np.bool)\n        z = np.logical_or(x, y)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_or_bcast3v1d')\n\n        # 3d vs 2d\n        x = (np.random.randn(3, 4, 5) > 0).astype(np.bool)\n        y = (np.random.randn(4, 5) > 0).astype(np.bool)\n        z = np.logical_or(x, y)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_or_bcast3v2d')\n\n        # 4d vs 2d\n        x = (np.random.randn(3, 4, 5, 6) > 0).astype(np.bool)\n        y = (np.random.randn(5, 6) > 0).astype(np.bool)\n        z = np.logical_or(x, y)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_or_bcast4v2d')\n\n        # 4d vs 3d\n        x = (np.random.randn(3, 4, 5, 6) > 0).astype(np.bool)\n        y = (np.random.randn(4, 5, 6) > 0).astype(np.bool)\n        z = np.logical_or(x, y)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_or_bcast4v3d')\n\n        # 4d vs 4d\n        x = (np.random.randn(1, 4, 1, 6) > 0).astype(np.bool)\n        y = (np.random.randn(3, 1, 5, 6) > 0).astype(np.bool)\n        z = np.logical_or(x, y)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_or_bcast4v4d')\n"""
onnx/backend/test/case/node/pad.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\ndef pad_impl(data, raw_pads, mode, constant_values=0.0):  # type: ignore\n\n    input_rank = data.ndim\n    if input_rank * 2 != raw_pads.size:\n        raise Exception('The number of elements in raw_pads should be 2 * data_rank')\n\n    # re-order to np.pad accepted order ((x1_begin, x1_end), (x2_begin, x2_end), ...)\n    pad_width = ()\n    for i in range(int(raw_pads.size / 2)):\n        pad_width += ((raw_pads[i], raw_pads[i + input_rank])),  # type: ignore\n\n    if mode == 'constant':\n        y = np.pad(\n            data,\n            pad_width=pad_width,\n            mode=mode,\n            constant_values=constant_values,\n        )\n        return y\n\n    y = np.pad(\n        data,\n        pad_width=pad_width,\n        mode=mode,\n    )\n\n    return y\n\n\nclass Pad(Base):\n\n    @staticmethod\n    def export_constant_pad():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Pad',\n            inputs=['x', 'pads', 'value'],\n            outputs=['y'],\n            mode='constant'\n        )\n        x = np.random.randn(1, 3, 4, 5).astype(np.float32)\n        pads = np.array([0, 0, 1, 3, 0, 0, 2, 4]).astype(np.int64)  # pad order [x1_begin, x2_begin, ..., x1_end, x2_end, ...]\n        value = np.float32(1.2)\n        y = pad_impl(\n            x,\n            pads,\n            'constant',\n            1.2\n        )\n\n        expect(node, inputs=[x, pads, value], outputs=[y],\n               name='test_constant_pad')\n\n    @staticmethod\n    def export_reflection_and_edge_pad():  # type: () -> None\n        for mode in ['edge', 'reflect']:\n            node = onnx.helper.make_node(\n                'Pad',\n                inputs=['x', 'pads'],\n                outputs=['y'],\n                mode=mode\n            )\n            x = np.random.randn(1, 3, 4, 5).astype(np.int32)\n            pads = np.array([0, 0, 1, 1, 0, 0, 1, 1]).astype(np.int64)  # pad order [x1_begin, x2_begin, ..., x1_end, x2_end, ...]\n            y = pad_impl(\n                x,\n                pads,\n                mode\n            )\n\n            expect(node, inputs=[x, pads], outputs=[y],\n                   name='test_{}_pad'.format(mode))\n"""
onnx/backend/test/case/node/pool_op_common.py,0,"b""import numpy as np  # type: ignore\nimport itertools\nfrom typing import Text, Sequence\n\n\ndef get_pad_shape(auto_pad,  # type: Text\n                  input_spatial_shape,  # type: Sequence[int]\n                  kernel_spatial_shape,  # type: Sequence[int]\n                  strides_spatial,  # type: Sequence[int]\n                  output_spatial_shape  # type: Sequence[int]\n                  ):  # type: (...) -> Sequence[int]\n    pad_shape = [0] * len(input_spatial_shape)\n    if auto_pad in ('SAME_UPPER', 'SAME_LOWER'):\n        for i in range(len(input_spatial_shape)):\n            pad_shape[i] = (output_spatial_shape[i] - 1) * strides_spatial[i] + \\\n                kernel_spatial_shape[i] - input_spatial_shape[i]\n    elif auto_pad == 'VALID':\n        pass\n    return pad_shape\n\n\ndef get_output_shape(auto_pad,  # type: Text\n                     input_spatial_shape,  # type: Sequence[int]\n                     kernel_spatial_shape,  # type: Sequence[int]\n                     strides_spatial  # type: Sequence[int]\n                     ):  # type: (...) -> Sequence[int]\n    out_shape = [0] * len(input_spatial_shape)\n    if auto_pad in ('SAME_UPPER', 'SAME_LOWER'):\n        for i in range(len(input_spatial_shape)):\n            out_shape[i] = int(\n                np.ceil(\n                    float(\n                        input_spatial_shape[i])\n                    / float(\n                        strides_spatial[i])))\n    elif auto_pad == 'VALID':\n        for i in range(len(input_spatial_shape)):\n            out_shape[i] = int(np.ceil(float(input_spatial_shape[i] - (kernel_spatial_shape[i] - 1)) / float(strides_spatial[i])))\n    return out_shape\n\n\ndef pool(padded,  # type: np.ndarray\n         x_shape,  # type: Sequence[int]\n         kernel_shape,  # type: Sequence[int]\n         strides_shape,  # type: Sequence[int]\n         out_shape,  # type: Sequence[int]\n         pad_shape,  # type: Sequence[int]\n         pooling_type,  # type: Text\n         count_include_pad=0  # type: int\n         ):  # type: (...) -> np.ndarray\n    spatial_size = len(x_shape) - 2\n    y = np.zeros([x_shape[0], x_shape[1]] + list(out_shape))\n\n    for shape in itertools.product(range(x_shape[0]), range(x_shape[1]), *[range(int(\n            (x_shape[i + 2] + pad_shape[i] - kernel_shape[i]) / strides_shape[i] + 1)) for i in range(spatial_size)]):\n        window = padded[shape[0], shape[1]]\n        window_vals = np.array([window[i] for i in list(\n            itertools.product(\n                *[range(strides_shape[i] * shape[i + 2], strides_shape[i] * shape[i + 2] + kernel_shape[i]) for i in\n                  range(spatial_size)])\n        )])\n        if pooling_type == 'AVG':\n            f = np.average\n        elif pooling_type == 'MAX':\n            f = np.max\n        else:\n            raise NotImplementedError(\n                'Pooling type {} does not support. Should be AVG, MAX'.format(pooling_type))\n\n        if count_include_pad == 1 and pooling_type == 'AVG':\n            y[shape] = f(window_vals)\n        else:\n            y[shape] = f(window_vals[np.where(~np.isnan(window_vals))])\n    return y.astype(np.float32)\n"""
onnx/backend/test/case/node/pow.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\ndef pow(x, y):  # type: ignore\n    z = np.power(x, y).astype(x.dtype)\n    return z\n\n\nclass Pow(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Pow',\n            inputs=['x', 'y'],\n            outputs=['z'],\n        )\n\n        x = np.array([1, 2, 3]).astype(np.float32)\n        y = np.array([4, 5, 6]).astype(np.float32)\n        z = pow(x, y)  # expected output [1., 32., 729.]\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_pow_example')\n\n        x = np.arange(60).reshape(3, 4, 5).astype(np.float32)\n        y = np.random.randn(3, 4, 5).astype(np.float32)\n        z = pow(x, y)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_pow')\n\n    @staticmethod\n    def export_pow_broadcast():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Pow',\n            inputs=['x', 'y'],\n            outputs=['z'],\n        )\n\n        x = np.array([1, 2, 3]).astype(np.float32)\n        y = np.array(2).astype(np.float32)\n        z = pow(x, y)  # expected output [1., 4., 9.]\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_pow_bcast_scalar')\n\n        node = onnx.helper.make_node(\n            'Pow',\n            inputs=['x', 'y'],\n            outputs=['z'],\n        )\n        x = np.array([[1, 2, 3], [4, 5, 6]]).astype(np.float32)\n        y = np.array([1, 2, 3]).astype(np.float32)\n        # expected output [[1, 4, 27], [4, 25, 216]]\n        z = pow(x, y)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_pow_bcast_array')\n\n    @staticmethod\n    def export_types():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Pow',\n            inputs=['x', 'y'],\n            outputs=['z'],\n        )\n\n        x = np.array([1, 2, 3]).astype(np.float32)\n        y = np.array([4, 5, 6]).astype(np.int64)\n        z = pow(x, y)  # expected output [1., 32., 729.]\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_pow_types_float32_int64')\n\n        x = np.array([1, 2, 3]).astype(np.int64)\n        y = np.array([4, 5, 6]).astype(np.float32)\n        z = pow(x, y)  # expected output [1, 32, 729]\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_pow_types_int64_float32')\n\n        x = np.array([1, 2, 3]).astype(np.float32)\n        y = np.array([4, 5, 6]).astype(np.int32)\n        z = pow(x, y)  # expected output [1., 32., 729.]\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_pow_types_float32_int32')\n\n        x = np.array([1, 2, 3]).astype(np.int32)\n        y = np.array([4, 5, 6]).astype(np.float32)\n        z = pow(x, y)  # expected output [1, 32, 729]\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_pow_types_int32_float32')\n\n        x = np.array([1, 2, 3]).astype(np.float32)\n        y = np.array([4, 5, 6]).astype(np.uint64)\n        z = pow(x, y)  # expected output [1., 32., 729.]\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_pow_types_float32_uint64')\n\n        x = np.array([1, 2, 3]).astype(np.float32)\n        y = np.array([4, 5, 6]).astype(np.uint32)\n        z = pow(x, y)  # expected output [1., 32., 729.]\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_pow_types_float32_uint32')\n\n        x = np.array([1, 2, 3]).astype(np.int64)\n        y = np.array([4, 5, 6]).astype(np.int64)\n        z = pow(x, y)  # expected output [1, 32, 729]\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_pow_types_int64_int64')\n\n        x = np.array([1, 2, 3]).astype(np.int32)\n        y = np.array([4, 5, 6]).astype(np.int32)\n        z = pow(x, y)  # expected output [1, 32, 729]\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_pow_types_int32_int32')\n"""
onnx/backend/test/case/node/prelu.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass PRelu(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'PRelu',\n            inputs=['x', 'slope'],\n            outputs=['y'],\n        )\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        slope = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.clip(x, 0, np.inf) + np.clip(x, -np.inf, 0) * slope\n\n        expect(node, inputs=[x, slope], outputs=[y],\n               name='test_prelu_example')\n\n    @staticmethod\n    def export_prelu_broadcast():  # type: () -> None\n        node = onnx.helper.make_node(\n            'PRelu',\n            inputs=['x', 'slope'],\n            outputs=['y'],\n        )\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        slope = np.random.randn(5).astype(np.float32)\n        y = np.clip(x, 0, np.inf) + np.clip(x, -np.inf, 0) * slope\n\n        expect(node, inputs=[x, slope], outputs=[y],\n               name='test_prelu_broadcast')\n"""
onnx/backend/test/case/node/qlinearconv.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass QLinearConv(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node('QLinearConv',\n            inputs=['x', 'x_scale', 'x_zero_point', 'w', 'w_scale', 'w_zero_point', 'y_scale', 'y_zero_point'],\n            outputs=['y'],)\n\n        x = np.array([[255, 174, 162, 25, 203, 168, 58],\n            [15, 59, 237, 95, 129, 0, 64],\n            [56, 242, 153, 221, 168, 12, 166],\n            [232, 178, 186, 195, 237, 162, 237],\n            [188, 39, 124, 77, 80, 102, 43],\n            [127, 230, 21, 83, 41, 40, 134],\n            [255, 154, 92, 141, 42, 148, 247], ], dtype=np.uint8).reshape((1, 1, 7, 7))\n\n        x_scale = np.float32(0.00369204697)\n        x_zero_point = np.uint8(132)\n\n        w = np.array([0], dtype=np.uint8).reshape((1, 1, 1, 1))\n\n        w_scale = np.array([0.00172794575], dtype=np.float32)\n        w_zero_point = np.array([255], dtype=np.uint8)\n\n        y_scale = np.float32(0.00162681262)\n        y_zero_point = np.uint8(123)\n\n        output = np.array([[0, 81, 93, 230, 52, 87, 197],\n            [240, 196, 18, 160, 126, 255, 191],\n            [199, 13, 102, 34, 87, 243, 89],\n            [23, 77, 69, 60, 18, 93, 18],\n            [67, 216, 131, 178, 175, 153, 212],\n            [128, 25, 234, 172, 214, 215, 121],\n            [0, 101, 163, 114, 213, 107, 8], ], dtype=np.uint8).reshape((1, 1, 7, 7))\n\n        expect(node, inputs=[x, x_scale, x_zero_point, w, w_scale, w_zero_point, y_scale, y_zero_point], outputs=[output],\n               name='test_qlinearconv')\n"""
onnx/backend/test/case/node/qlinearmatmul.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass QLinearMatMul(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node('QLinearMatMul',\n            inputs=['a', 'a_scale', 'a_zero_point', 'b', 'b_scale', 'b_zero_point', 'y_scale', 'y_zero_point'],\n            outputs=['y'],)\n\n        #2D\n        a = np.array([[208, 236, 0, 238],\n            [3, 214, 255, 29], ], dtype=np.uint8)\n\n        a_scale = np.array([0.0066], dtype=np.float32)\n        a_zero_point = np.array([113], dtype=np.uint8)\n\n        b = np.array([[152, 51, 244],\n            [60, 26, 255],\n            [0, 127, 246],\n            [127, 254, 247]], dtype=np.uint8)\n\n        b_scale = np.array([0.00705], dtype=np.float32)\n        b_zero_point = np.array([114], dtype=np.uint8)\n\n        y_scale = np.array([0.0107], dtype=np.float32)\n        y_zero_point = np.array([118], dtype=np.uint8)\n\n        output = np.array([[168, 115, 255],\n            [1, 66, 151], ], dtype=np.uint8)\n\n        expect(node, inputs=[a, a_scale, a_zero_point, b, b_scale, b_zero_point, y_scale, y_zero_point], outputs=[output],\n               name='test_qlinearmatmul_2D')\n\n        #3D\n        a = np.array([[[208, 236, 0, 238],\n            [3, 214, 255, 29]],\n            [[208, 236, 0, 238],\n            [3, 214, 255, 29]]], dtype=np.uint8)\n\n        a_scale = np.array([0.0066], dtype=np.float32)\n        a_zero_point = np.array([113], dtype=np.uint8)\n\n        b = np.array([[[152, 51, 244],\n            [60, 26, 255],\n            [0, 127, 246],\n            [127, 254, 247]],\n            [[152, 51, 244],\n            [60, 26, 255],\n            [0, 127, 246],\n            [127, 254, 247]]], dtype=np.uint8)\n\n        b_scale = np.array([0.00705], dtype=np.float32)\n        b_zero_point = np.array([114], dtype=np.uint8)\n\n        y_scale = np.array([0.0107], dtype=np.float32)\n        y_zero_point = np.array([118], dtype=np.uint8)\n\n        output = np.array([[[168, 115, 255],\n            [1, 66, 151]],\n            [[168, 115, 255],\n            [1, 66, 151]]], dtype=np.uint8)\n\n        expect(node, inputs=[a, a_scale, a_zero_point, b, b_scale, b_zero_point, y_scale, y_zero_point], outputs=[output],\n               name='test_qlinearmatmul_3D')\n"""
onnx/backend/test/case/node/quantizelinear.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass QuantizeLinear(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node('QuantizeLinear',\n            inputs=['x', 'y_scale', 'y_zero_point'],\n            outputs=['y'],)\n\n        x = np.array([0, 2, 3, 1000, -254, -1000]).astype(np.float32)\n        y_scale = np.float32(2)\n        y_zero_point = np.uint8(128)\n        y = np.array([128, 129, 130, 255, 1, 0]).astype(np.uint8)\n\n        expect(node, inputs=[x, y_scale, y_zero_point], outputs=[y],\n               name='test_quantizelinear')\n"""
onnx/backend/test/case/node/rangeop.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Range(Base):\n\n    @staticmethod\n    def export_range_float_type_positive_delta():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Range',\n            inputs=['start', 'limit', 'delta'],\n            outputs=['output'],\n        )\n\n        start = np.float32(1)\n        limit = np.float32(5)\n        delta = np.float32(2)\n\n        output = np.arange(start, limit, delta, dtype=np.float32)  # expected output [1.0, 3.0]\n        expect(node, inputs=[start, limit, delta], outputs=[output],\n               name='test_range_float_type_positive_delta')\n\n    @staticmethod\n    def export_range_int32_type_negative_delta():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Range',\n            inputs=['start', 'limit', 'delta'],\n            outputs=['output'],\n        )\n\n        start = np.int32(10)\n        limit = np.int32(6)\n        delta = np.int32(-3)\n\n        output = np.arange(start, limit, delta, dtype=np.int32)  # expected output [10, 7]\n        expect(node, inputs=[start, limit, delta], outputs=[output],\n               name='test_range_int32_type_negative_delta')\n"""
onnx/backend/test/case/node/reciprocal.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Reciprocal(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Reciprocal',\n            inputs=['x'],\n            outputs=['y'],\n        )\n\n        x = np.array([-4, 2]).astype(np.float32)\n        y = np.reciprocal(x)  # expected output [-0.25, 0.5],\n        expect(node, inputs=[x], outputs=[y],\n               name='test_reciprocal_example')\n\n        x = np.random.rand(3, 4, 5).astype(np.float32) + 0.5\n        y = np.reciprocal(x)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_reciprocal')\n"""
onnx/backend/test/case/node/reduce_log_sum.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass ReduceLogSum(Base):\n\n    @staticmethod\n    def export_nokeepdims():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'ReduceLogSum\',\n            inputs=[\'data\'],\n            outputs=[""reduced""],\n            axes=[2, 1],\n            keepdims=0\n        )\n        data = np.random.ranf([3, 4, 5]).astype(np.float32)\n        reduced = np.log(np.sum(data, axis=(2, 1), keepdims=False))\n        expect(node, inputs=[data], outputs=[reduced],\n               name=\'test_reduce_log_sum_desc_axes\')\n\n        node = onnx.helper.make_node(\n            \'ReduceLogSum\',\n            inputs=[\'data\'],\n            outputs=[""reduced""],\n            axes=[0, 1],\n            keepdims=0\n        )\n        data = np.random.ranf([3, 4, 5]).astype(np.float32)\n        reduced = np.log(np.sum(data, axis=(0, 1), keepdims=False))\n        expect(node, inputs=[data], outputs=[reduced],\n               name=\'test_reduce_log_sum_asc_axes\')\n\n    @staticmethod\n    def export_keepdims():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'ReduceLogSum\',\n            inputs=[\'data\'],\n            outputs=[""reduced""]\n        )\n        data = np.random.ranf([3, 4, 5]).astype(np.float32)\n        reduced = np.log(np.sum(data, keepdims=True))\n        expect(node, inputs=[data], outputs=[reduced],\n               name=\'test_reduce_log_sum_default\')\n\n    @staticmethod\n    def export_negative_axes_keepdims():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'ReduceLogSum\',\n            inputs=[\'data\'],\n            outputs=[""reduced""],\n            axes=[-2]\n        )\n        data = np.random.ranf([3, 4, 5]).astype(np.float32)\n        reduced = np.log(np.sum(data, axis=(-2), keepdims=True))\n        # print(reduced)\n        expect(node, inputs=[data], outputs=[reduced],\n               name=\'test_reduce_log_sum_negative_axes\')\n'"
onnx/backend/test/case/node/reduce_log_sum_exp.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass ReduceLogSumExp(Base):\n\n    @staticmethod\n    def export_do_not_keepdims():  # type: () -> None\n        shape = [3, 2, 2]\n        axes = [1]\n        keepdims = 0\n        node = onnx.helper.make_node(\n            'ReduceLogSumExp',\n            inputs=['data'],\n            outputs=['reduced'],\n            axes=axes,\n            keepdims=keepdims\n        )\n\n        data = np.array(\n            [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n            dtype=np.float32)\n        reduced = np.log(np.sum(\n            np.exp(data), axis=tuple(axes), keepdims=keepdims == 1))\n        # print(reduced)\n        #[[20., 2.31326175]\n        # [40.00004578, 2.31326175]\n        # [60.00671387, 2.31326175]]\n\n        expect(node, inputs=[data], outputs=[reduced],\n              name='test_reduce_log_sum_exp_do_not_keepdims_example')\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.log(np.sum(\n            np.exp(data), axis=tuple(axes), keepdims=keepdims == 1))\n\n        expect(node, inputs=[data], outputs=[reduced],\n            name='test_reduce_log_sum_exp_do_not_keepdims_random')\n\n    @staticmethod\n    def export_keepdims():  # type: () -> None\n        shape = [3, 2, 2]\n        axes = [1]\n        keepdims = 1\n        node = onnx.helper.make_node(\n            'ReduceLogSumExp',\n            inputs=['data'],\n            outputs=['reduced'],\n            axes=axes,\n            keepdims=keepdims\n        )\n\n        data = np.array(\n            [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n            dtype=np.float32)\n        reduced = np.log(np.sum(np.exp(data),\n                                axis=tuple(axes),\n                                keepdims=keepdims == 1))\n        # print(reduced)\n        # [[[20., 2.31326175]]\n        # [[40.00004578, 2.31326175]]\n        # [[60.00671387, 2.31326175]]]\n\n        expect(node, inputs=[data], outputs=[reduced],\n              name='test_reduce_log_sum_exp_keepdims_example')\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.log(np.sum(np.exp(data),\n                                axis=tuple(axes),\n                                keepdims=keepdims == 1))\n\n        expect(node, inputs=[data], outputs=[reduced],\n              name='test_reduce_log_sum_exp_keepdims_random')\n\n    @staticmethod\n    def export_default_axes_keepdims():  # type: () -> None\n        shape = [3, 2, 2]\n        axes = None\n        keepdims = 1\n\n        node = onnx.helper.make_node(\n            'ReduceLogSumExp',\n            inputs=['data'],\n            outputs=['reduced'],\n            keepdims=keepdims\n        )\n\n        data = np.array(\n            [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n            dtype=np.float32)\n        reduced = np.log(np.sum(np.exp(data),\n                                axis=axes,\n                                keepdims=keepdims == 1))\n        # print(reduced)\n        # [[[60.00671387]]]\n\n        expect(node, inputs=[data], outputs=[reduced],\n              name='test_reduce_log_sum_exp_default_axes_keepdims_example')\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.log(np.sum(np.exp(data),\n                                axis=axes,\n                                keepdims=keepdims == 1))\n        expect(node, inputs=[data], outputs=[reduced],\n              name='test_reduce_log_sum_exp_default_axes_keepdims_random')\n\n    @staticmethod\n    def export_negative_axes_keepdims():  # type: () -> None\n        shape = [3, 2, 2]\n        axes = [-2]\n        keepdims = 1\n        node = onnx.helper.make_node(\n            'ReduceLogSumExp',\n            inputs=['data'],\n            outputs=['reduced'],\n            axes=axes,\n            keepdims=keepdims\n        )\n\n        data = np.array(\n            [[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]],\n            dtype=np.float32)\n        reduced = np.log(np.sum(np.exp(data),\n                                axis=tuple(axes),\n                                keepdims=keepdims == 1))\n        # print(reduced)\n        # [[[20., 2.31326175]]\n        # [[40.00004578, 2.31326175]]\n        # [[60.00671387, 2.31326175]]]\n\n        expect(node, inputs=[data], outputs=[reduced],\n              name='test_reduce_log_sum_exp_negative_axes_keepdims_example')\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.log(np.sum(np.exp(data),\n                                axis=tuple(axes),\n                                keepdims=keepdims == 1))\n\n        expect(node, inputs=[data], outputs=[reduced],\n              name='test_reduce_log_sum_exp_negative_axes_keepdims_random')\n"""
onnx/backend/test/case/node/reducel1.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass ReduceL1(Base):\n\n    @staticmethod\n    def export_do_not_keepdims():  # type: () -> None\n        shape = [3, 2, 2]\n        axes = [2]\n        keepdims = 0\n\n        node = onnx.helper.make_node(\n            'ReduceL1',\n            inputs=['data'],\n            outputs=['reduced'],\n            axes=axes,\n            keepdims=keepdims\n        )\n\n        data = np.reshape(np.arange(1, np.prod(shape) + 1, dtype=np.float32), shape)\n        #print(data)\n        #[[[1., 2.], [3., 4.]], [[5., 6.], [7., 8.]], [[9., 10.], [11., 12.]]]\n\n        reduced = np.sum(a=np.abs(data), axis=tuple(axes), keepdims=keepdims == 1)\n        #print(reduced)\n        #[[3., 7.], [11., 15.], [19., 23.]]\n\n        expect(node, inputs=[data], outputs=[reduced],\n            name='test_reduce_l1_do_not_keepdims_example')\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.sum(a=np.abs(data), axis=tuple(axes), keepdims=keepdims == 1)\n\n        expect(node, inputs=[data], outputs=[reduced],\n            name='test_reduce_l1_do_not_keepdims_random')\n\n    @staticmethod\n    def export_keepdims():  # type: () -> None\n        shape = [3, 2, 2]\n        axes = [2]\n        keepdims = 1\n\n        node = onnx.helper.make_node(\n            'ReduceL1',\n            inputs=['data'],\n            outputs=['reduced'],\n            axes=axes,\n            keepdims=keepdims\n        )\n\n        data = np.reshape(np.arange(1, np.prod(shape) + 1, dtype=np.float32), shape)\n        #print(data)\n        #[[[1., 2.], [3., 4.]], [[5., 6.], [7., 8.]], [[9., 10.], [11., 12.]]]\n\n        reduced = np.sum(a=np.abs(data), axis=tuple(axes), keepdims=keepdims == 1)\n        #print(reduced)\n        #[[[3.], [7.]], [[11.], [15.]], [[19.], [23.]]]\n\n        expect(node, inputs=[data], outputs=[reduced],\n            name='test_reduce_l1_keep_dims_example')\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.sum(a=np.abs(data), axis=tuple(axes), keepdims=keepdims == 1)\n\n        expect(node, inputs=[data], outputs=[reduced],\n            name='test_reduce_l1_keep_dims_random')\n\n    @staticmethod\n    def export_default_axes_keepdims():  # type: () -> None\n        shape = [3, 2, 2]\n        axes = None\n        keepdims = 1\n\n        node = onnx.helper.make_node(\n            'ReduceL1',\n            inputs=['data'],\n            outputs=['reduced'],\n            keepdims=keepdims\n        )\n\n        data = np.reshape(np.arange(1, np.prod(shape) + 1, dtype=np.float32), shape)\n        #print(data)\n        #[[[1., 2.], [3., 4.]], [[5., 6.], [7., 8.]], [[9., 10.], [11., 12.]]]\n\n        reduced = np.sum(a=np.abs(data), axis=axes, keepdims=keepdims == 1)\n        #print(reduced)\n        #[[[78.]]]\n\n        expect(node, inputs=[data], outputs=[reduced],\n            name='test_reduce_l1_default_axes_keepdims_example')\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.sum(a=np.abs(data), axis=axes, keepdims=keepdims == 1)\n\n        expect(node, inputs=[data], outputs=[reduced],\n            name='test_reduce_l1_default_axes_keepdims_random')\n\n    @staticmethod\n    def export_negative_axes_keepdims():  # type: () -> None\n        shape = [3, 2, 2]\n        axes = [-1]\n        keepdims = 1\n\n        node = onnx.helper.make_node(\n            'ReduceL1',\n            inputs=['data'],\n            outputs=['reduced'],\n            axes=axes,\n            keepdims=keepdims\n        )\n\n        data = np.reshape(np.arange(1, np.prod(shape) + 1, dtype=np.float32), shape)\n        # print(data)\n        #[[[1., 2.], [3., 4.]], [[5., 6.], [7., 8.]], [[9., 10.], [11., 12.]]]\n\n        reduced = np.sum(a=np.abs(data), axis=tuple(axes), keepdims=keepdims == 1)\n        # print(reduced)\n        #[[[3.], [7.]], [[11.], [15.]], [[19.], [23.]]]\n\n        expect(node, inputs=[data], outputs=[reduced],\n            name='test_reduce_l1_negative_axes_keep_dims_example')\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.sum(a=np.abs(data), axis=tuple(axes), keepdims=keepdims == 1)\n\n        expect(node, inputs=[data], outputs=[reduced],\n            name='test_reduce_l1_negative_axes_keep_dims_random')\n"""
onnx/backend/test/case/node/reducel2.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass ReduceL2(Base):\n\n    @staticmethod\n    def export_do_not_keepdims():  # type: () -> None\n        shape = [3, 2, 2]\n        axes = [2]\n        keepdims = 0\n\n        node = onnx.helper.make_node(\n            'ReduceL2',\n            inputs=['data'],\n            outputs=['reduced'],\n            axes=axes,\n            keepdims=keepdims\n        )\n\n        data = np.reshape(np.arange(1, np.prod(shape) + 1, dtype=np.float32), shape)\n        #print(data)\n        #[[[1., 2.], [3., 4.]], [[5., 6.], [7., 8.]], [[9., 10.], [11., 12.]]]\n\n        reduced = np.sqrt(np.sum(\n            a=np.square(data), axis=tuple(axes), keepdims=keepdims == 1))\n        #print(reduced)\n        #[[2.23606798, 5.],\n        # [7.81024968, 10.63014581],\n        # [13.45362405, 16.2788206]]\n\n        expect(node, inputs=[data], outputs=[reduced],\n            name='test_reduce_l2_do_not_keepdims_example')\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.sqrt(np.sum(\n            a=np.square(data), axis=tuple(axes), keepdims=keepdims == 1))\n\n        expect(node, inputs=[data], outputs=[reduced],\n            name='test_reduce_l2_do_not_keepdims_random')\n\n    @staticmethod\n    def export_keepdims():  # type: () -> None\n        shape = [3, 2, 2]\n        axes = [2]\n        keepdims = 1\n\n        node = onnx.helper.make_node(\n            'ReduceL2',\n            inputs=['data'],\n            outputs=['reduced'],\n            axes=axes,\n            keepdims=keepdims\n        )\n\n        data = np.reshape(np.arange(1, np.prod(shape) + 1, dtype=np.float32), shape)\n        #print(data)\n        #[[[1., 2.], [3., 4.]], [[5., 6.], [7., 8.]], [[9., 10.], [11., 12.]]]\n\n        reduced = np.sqrt(np.sum(\n            a=np.square(data), axis=tuple(axes), keepdims=keepdims == 1))\n        #print(reduced)\n        #[[[2.23606798], [5.]]\n        # [[7.81024968], [10.63014581]]\n        # [[13.45362405], [16.2788206 ]]]\n\n        expect(node, inputs=[data], outputs=[reduced],\n            name='test_reduce_l2_keep_dims_example')\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.sqrt(np.sum(\n            a=np.square(data), axis=tuple(axes), keepdims=keepdims == 1))\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_l2_keep_dims_random')\n\n    @staticmethod\n    def export_default_axes_keepdims():  # type: () -> None\n        shape = [3, 2, 2]\n        axes = None\n        keepdims = 1\n\n        node = onnx.helper.make_node(\n            'ReduceL2',\n            inputs=['data'],\n            outputs=['reduced'],\n            keepdims=keepdims\n        )\n\n        data = np.reshape(np.arange(1, np.prod(shape) + 1, dtype=np.float32), shape)\n        #print(data)\n        #[[[1., 2.], [3., 4.]], [[5., 6.], [7., 8.]], [[9., 10.], [11., 12.]]]\n\n        reduced = np.sqrt(np.sum(\n            a=np.square(data), axis=axes, keepdims=keepdims == 1))\n        #print(reduced)\n        #[[[25.49509757]]]\n\n        expect(node, inputs=[data], outputs=[reduced],\n            name='test_reduce_l2_default_axes_keepdims_example')\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.sqrt(np.sum(\n            a=np.square(data), axis=axes, keepdims=keepdims == 1))\n\n        expect(node, inputs=[data], outputs=[reduced],\n            name='test_reduce_l2_default_axes_keepdims_random')\n\n    @staticmethod\n    def export_negative_axes_keepdims():  # type: () -> None\n        shape = [3, 2, 2]\n        axes = [-1]\n        keepdims = 1\n\n        node = onnx.helper.make_node(\n            'ReduceL2',\n            inputs=['data'],\n            outputs=['reduced'],\n            axes=axes,\n            keepdims=keepdims\n        )\n\n        data = np.reshape(np.arange(1, np.prod(shape) + 1, dtype=np.float32), shape)\n        # print(data)\n        #[[[1., 2.], [3., 4.]], [[5., 6.], [7., 8.]], [[9., 10.], [11., 12.]]]\n\n        reduced = np.sqrt(np.sum(\n            a=np.square(data), axis=tuple(axes), keepdims=keepdims == 1))\n        # print(reduced)\n        #[[[2.23606798], [5.]]\n        # [[7.81024968], [10.63014581]]\n        # [[13.45362405], [16.2788206 ]]]\n\n        expect(node, inputs=[data], outputs=[reduced],\n            name='test_reduce_l2_negative_axes_keep_dims_example')\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.sqrt(np.sum(\n            a=np.square(data), axis=tuple(axes), keepdims=keepdims == 1))\n\n        expect(node, inputs=[data], outputs=[reduced],\n            name='test_reduce_l2_negative_axes_keep_dims_random')\n"""
onnx/backend/test/case/node/reducemax.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass ReduceMax(Base):\n\n    @staticmethod\n    def export_do_not_keepdims():  # type: () -> None\n        shape = [3, 2, 2]\n        axes = [1]\n        keepdims = 0\n\n        node = onnx.helper.make_node(\n            'ReduceMax',\n            inputs=['data'],\n            outputs=['reduced'],\n            axes=axes,\n            keepdims=keepdims)\n\n        data = np.array([[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]], dtype=np.float32)\n        reduced = np.maximum.reduce(data, axis=tuple(axes), keepdims=keepdims == 1)\n        #print(reduced)\n        #[[20., 2.]\n        # [40., 2.]\n        # [60., 2.]]\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_max_do_not_keepdims_example')\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.maximum.reduce(data, axis=tuple(axes), keepdims=keepdims == 1)\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_max_do_not_keepdims_random')\n\n    @staticmethod\n    def export_keepdims():  # type: () -> None\n        shape = [3, 2, 2]\n        axes = [1]\n        keepdims = 1\n\n        node = onnx.helper.make_node(\n            'ReduceMax',\n            inputs=['data'],\n            outputs=['reduced'],\n            axes=axes,\n            keepdims=keepdims)\n\n        data = np.array([[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]], dtype=np.float32)\n        reduced = np.maximum.reduce(data, axis=tuple(axes), keepdims=keepdims == 1)\n        #print(reduced)\n        #[[[20., 2.]]\n        # [[40., 2.]]\n        # [[60., 2.]]]\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_max_keepdims_example')\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.maximum.reduce(data, axis=tuple(axes), keepdims=keepdims == 1)\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_max_keepdims_random')\n\n    @staticmethod\n    def export_default_axes_keepdims():  # type: () -> None\n        shape = [3, 2, 2]\n        axes = None\n        keepdims = 1\n        node = onnx.helper.make_node(\n            'ReduceMax',\n            inputs=['data'],\n            outputs=['reduced'],\n            keepdims=keepdims)\n\n        data = np.array([[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]], dtype=np.float32)\n        reduced = np.maximum.reduce(data, axis=axes, keepdims=keepdims == 1)\n        #print(reduced)\n        [[[60.]]]\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_max_default_axes_keepdim_example')\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.maximum.reduce(data, axis=axes, keepdims=keepdims == 1)\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_max_default_axes_keepdims_random')\n\n    @staticmethod\n    def export_negative_axes_keepdims():  # type: () -> None\n        shape = [3, 2, 2]\n        axes = [-2]\n        keepdims = 1\n\n        node = onnx.helper.make_node(\n            'ReduceMax',\n            inputs=['data'],\n            outputs=['reduced'],\n            axes=axes,\n            keepdims=keepdims)\n\n        data = np.array([[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]], dtype=np.float32)\n        reduced = np.maximum.reduce(data, axis=tuple(axes), keepdims=keepdims == 1)\n        # print(reduced)\n        #[[[20., 2.]]\n        # [[40., 2.]]\n        # [[60., 2.]]]\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_max_negative_axes_keepdims_example')\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.maximum.reduce(data, axis=tuple(axes), keepdims=keepdims == 1)\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_max_negative_axes_keepdims_random')\n"""
onnx/backend/test/case/node/reducemean.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass ReduceMean(Base):\n\n    @staticmethod\n    def export_do_not_keepdims():  # type: () -> None\n        shape = [3, 2, 2]\n        axes = [1]\n        keepdims = 0\n\n        node = onnx.helper.make_node(\n            'ReduceMean',\n            inputs=['data'],\n            outputs=['reduced'],\n            axes=axes,\n            keepdims=keepdims)\n\n        data = np.array([[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]], dtype=np.float32)\n        reduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n        #print(reduced)\n        #[[12.5, 1.5]\n        # [35., 1.5]\n        # [57.5, 1.5]]\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_mean_do_not_keepdims_example')\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_mean_do_not_keepdims_random')\n\n    @staticmethod\n    def export_keepdims():  # type: () -> None\n        shape = [3, 2, 2]\n        axes = [1]\n        keepdims = 1\n\n        node = onnx.helper.make_node(\n            'ReduceMean',\n            inputs=['data'],\n            outputs=['reduced'],\n            axes=axes,\n            keepdims=keepdims)\n\n        data = np.array([[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]], dtype=np.float32)\n        reduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n        #print(reduced)\n        #[[[12.5, 1.5]]\n        # [[35., 1.5]]\n        # [[57.5, 1.5]]]\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_mean_keepdims_example')\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_mean_keepdims_random')\n\n    @staticmethod\n    def export_default_axes_keepdims():  # type: () -> None\n        shape = [3, 2, 2]\n        axes = None\n        keepdims = 1\n\n        node = onnx.helper.make_node(\n            'ReduceMean',\n            inputs=['data'],\n            outputs=['reduced'],\n            keepdims=keepdims)\n\n        data = np.array([[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]], dtype=np.float32)\n        reduced = np.mean(data, axis=axes, keepdims=keepdims == 1)\n        #print(reduced)\n        #[[[18.25]]]\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_mean_default_axes_keepdims_example')\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.mean(data, axis=axes, keepdims=keepdims == 1)\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_mean_default_axes_keepdims_random')\n\n    @staticmethod\n    def export_negative_axes_keepdims():  # type: () -> None\n        shape = [3, 2, 2]\n        axes = [-2]\n        keepdims = 1\n\n        node = onnx.helper.make_node(\n            'ReduceMean',\n            inputs=['data'],\n            outputs=['reduced'],\n            axes=axes,\n            keepdims=keepdims)\n\n        data = np.array([[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]], dtype=np.float32)\n        reduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n        # print(reduced)\n        # [[[12.5, 1.5]]\n        # [[35., 1.5]]\n        # [[57.5, 1.5]]]\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_mean_negative_axes_keepdims_example')\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.mean(data, axis=tuple(axes), keepdims=keepdims == 1)\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_mean_negative_axes_keepdims_random')\n"""
onnx/backend/test/case/node/reducemin.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass ReduceMin(Base):\n\n    @staticmethod\n    def export_do_not_keepdims():  # type: () -> None\n        shape = [3, 2, 2]\n        axes = [1]\n        keepdims = 0\n\n        node = onnx.helper.make_node(\n            'ReduceMin',\n            inputs=['data'],\n            outputs=['reduced'],\n            axes=axes,\n            keepdims=keepdims)\n\n        data = np.array([[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]], dtype=np.float32)\n        reduced = np.minimum.reduce(data, axis=tuple(axes), keepdims=keepdims == 1)\n        #print(reduced)\n        #[[5., 1.]\n        # [30., 1.]\n        # [55., 1.]]\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_min_do_not_keepdims_example')\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.minimum.reduce(data, axis=tuple(axes), keepdims=keepdims == 1)\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_min_do_not_keepdims_random')\n\n    @staticmethod\n    def export_keepdims():  # type: () -> None\n        shape = [3, 2, 2]\n        axes = [1]\n        keepdims = 1\n\n        node = onnx.helper.make_node(\n            'ReduceMin', inputs=['data'],\n            outputs=['reduced'],\n            axes=axes,\n            keepdims=keepdims)\n\n        data = np.array([[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]], dtype=np.float32)\n        reduced = np.minimum.reduce(data, axis=tuple(axes), keepdims=keepdims == 1)\n        #print(reduced)\n        #[[[5., 1.]]\n        # [[30., 1.]]\n        # [[55., 1.]]]\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_min_keepdims_example')\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.minimum.reduce(data, axis=tuple(axes), keepdims=keepdims == 1)\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_min_keepdims_random')\n\n    @staticmethod\n    def export_default_axes_keepdims():  # type: () -> None\n        shape = [3, 2, 2]\n        axes = None\n        keepdims = 1\n\n        node = onnx.helper.make_node(\n            'ReduceMin',\n            inputs=['data'],\n            outputs=['reduced'],\n            keepdims=keepdims)\n\n        data = np.array([[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]], dtype=np.float32)\n        reduced = np.minimum.reduce(data, axis=axes, keepdims=keepdims == 1)\n        #print(reduced)\n        #[[[1.]]]\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_min_default_axes_keepdims_example')\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.minimum.reduce(data, axis=axes, keepdims=keepdims == 1)\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_min_default_axes_keepdims_random')\n\n    @staticmethod\n    def export_negative_axes_keepdims():  # type: () -> None\n        shape = [3, 2, 2]\n        axes = [-2]\n        keepdims = 1\n\n        node = onnx.helper.make_node(\n            'ReduceMin', inputs=['data'],\n            outputs=['reduced'],\n            axes=axes,\n            keepdims=keepdims)\n\n        data = np.array([[[5, 1], [20, 2]], [[30, 1], [40, 2]], [[55, 1], [60, 2]]], dtype=np.float32)\n        reduced = np.minimum.reduce(data, axis=tuple(axes), keepdims=keepdims == 1)\n        # print(reduced)\n        #[[[5., 1.]]\n        # [[30., 1.]]\n        # [[55., 1.]]]\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_min_negative_axes_keepdims_example')\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.minimum.reduce(data, axis=tuple(axes), keepdims=keepdims == 1)\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_min_negative_axes_keepdims_random')\n"""
onnx/backend/test/case/node/reduceprod.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass ReduceProd(Base):\n\n    @staticmethod\n    def export_do_not_keepdims():  # type: () -> None\n        shape = [3, 2, 2]\n        axes = [1]\n        keepdims = 0\n\n        node = onnx.helper.make_node(\n            'ReduceProd',\n            inputs=['data'],\n            outputs=['reduced'],\n            axes=axes,\n            keepdims=keepdims)\n\n        data = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]], dtype=np.float32)\n        reduced = np.prod(data, axis=tuple(axes), keepdims=keepdims == 1)\n        #print(reduced)\n        #[[3., 8.]\n        # [35., 48.]\n        # [99., 120.]]\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_prod_do_not_keepdims_example')\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.prod(data, axis=tuple(axes), keepdims=keepdims == 1)\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_prod_do_not_keepdims_random')\n\n    @staticmethod\n    def export_keepdims():  # type: () -> None\n        shape = [3, 2, 2]\n        axes = [1]\n        keepdims = 1\n\n        node = onnx.helper.make_node(\n            'ReduceProd',\n            inputs=['data'],\n            outputs=['reduced'],\n            axes=axes,\n            keepdims=keepdims)\n\n        data = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]], dtype=np.float32)\n        reduced = np.prod(data, axis=tuple(axes), keepdims=keepdims == 1)\n        #print(reduced)\n        #[[[3., 8.]]\n        # [[35., 48.]]\n        # [[99., 120.]]]\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_prod_keepdims_example')\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.prod(data, axis=tuple(axes), keepdims=keepdims == 1)\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_prod_keepdims_random')\n\n    @staticmethod\n    def export_default_axes_keepdims():  # type: () -> None\n        shape = [3, 2, 2]\n        axes = None\n        keepdims = 1\n\n        node = onnx.helper.make_node(\n            'ReduceProd',\n            inputs=['data'],\n            outputs=['reduced'],\n            keepdims=keepdims)\n\n        data = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]], dtype=np.float32)\n        reduced = np.prod(data, axis=axes, keepdims=keepdims == 1)\n        #print(reduced)\n        #[[[4.790016e+08]]]\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_prod_default_axes_keepdims_example')\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.prod(data, axis=axes, keepdims=keepdims == 1)\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_prod_default_axes_keepdims_random')\n\n    @staticmethod\n    def export_negative_axes_keepdims():  # type: () -> None\n        shape = [3, 2, 2]\n        axes = [-2]\n        keepdims = 1\n\n        node = onnx.helper.make_node(\n            'ReduceProd',\n            inputs=['data'],\n            outputs=['reduced'],\n            axes=axes,\n            keepdims=keepdims)\n\n        data = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]], dtype=np.float32)\n        reduced = np.prod(data, axis=tuple(axes), keepdims=keepdims == 1)\n        # print(reduced)\n        #[[[3., 8.]]\n        # [[35., 48.]]\n        # [[99., 120.]]]\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_prod_negative_axes_keepdims_example')\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.prod(data, axis=tuple(axes), keepdims=keepdims == 1)\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_prod_negative_axes_keepdims_random')\n"""
onnx/backend/test/case/node/reducesum.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass ReduceSum(Base):\n\n    @staticmethod\n    def export_do_not_keepdims():  # type: () -> None\n        shape = [3, 2, 2]\n        axes = [1]\n        keepdims = 0\n\n        node = onnx.helper.make_node(\n            'ReduceSum',\n            inputs=['data'],\n            outputs=['reduced'],\n            axes=axes,\n            keepdims=keepdims)\n\n        data = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]], dtype=np.float32)\n        reduced = np.sum(data, axis=tuple(axes), keepdims=keepdims == 1)\n        #print(reduced)\n        #[[4., 6.]\n        # [12., 14.]\n        # [20., 22.]]\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_sum_do_not_keepdims_example')\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.sum(data, axis=tuple(axes), keepdims=keepdims == 1)\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_sum_do_not_keepdims_random')\n\n    @staticmethod\n    def export_keepdims():  # type: () -> None\n        shape = [3, 2, 2]\n        axes = [1]\n        keepdims = 1\n\n        node = onnx.helper.make_node(\n            'ReduceSum',\n            inputs=['data'],\n            outputs=['reduced'],\n            axes=axes,\n            keepdims=keepdims)\n\n        data = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]], dtype=np.float32)\n        reduced = np.sum(data, axis=tuple(axes), keepdims=keepdims == 1)\n        #print(reduced)\n        #[[[4., 6.]]\n        # [[12., 14.]]\n        # [[20., 22.]]]\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_sum_keepdims_example')\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.sum(data, axis=tuple(axes), keepdims=keepdims == 1)\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_sum_keepdims_random')\n\n    @staticmethod\n    def export_default_axes_keepdims():  # type: () -> None\n        shape = [3, 2, 2]\n        axes = None\n        keepdims = 1\n\n        node = onnx.helper.make_node(\n            'ReduceSum',\n            inputs=['data'],\n            outputs=['reduced'],\n            keepdims=keepdims)\n\n        data = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]], dtype=np.float32)\n        reduced = np.sum(data, axis=axes, keepdims=keepdims == 1)\n        #print(reduced)\n        #[[[78.]]]\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_sum_default_axes_keepdims_example')\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.sum(data, axis=axes, keepdims=keepdims == 1)\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_sum_default_axes_keepdims_random')\n\n    @staticmethod\n    def export_negative_axes_keepdims():  # type: () -> None\n        shape = [3, 2, 2]\n        axes = [-2]\n        keepdims = 1\n\n        node = onnx.helper.make_node(\n            'ReduceSum',\n            inputs=['data'],\n            outputs=['reduced'],\n            axes=axes,\n            keepdims=keepdims)\n\n        data = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]], dtype=np.float32)\n        reduced = np.sum(data, axis=tuple(axes), keepdims=keepdims == 1)\n        # print(reduced)\n        #[[[4., 6.]]\n        # [[12., 14.]]\n        # [[20., 22.]]]\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_sum_negative_axes_keepdims_example')\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.sum(data, axis=tuple(axes), keepdims=keepdims == 1)\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_sum_negative_axes_keepdims_random')\n"""
onnx/backend/test/case/node/reducesumsquare.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass ReduceSumSquare(Base):\n\n    @staticmethod\n    def export_do_not_keepdims():  # type: () -> None\n        shape = [3, 2, 2]\n        axes = [1]\n        keepdims = 0\n\n        node = onnx.helper.make_node(\n            'ReduceSumSquare',\n            inputs=['data'],\n            outputs=['reduced'],\n            axes=axes,\n            keepdims=keepdims)\n\n        data = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]], dtype=np.float32)\n        reduced = np.sum(np.square(data), axis=tuple(axes), keepdims=keepdims == 1)\n        #print(reduced)\n        #[[10., 20.]\n        # [74., 100.]\n        # [202., 244.]]\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_sum_square_do_not_keepdims_example')\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.sum(np.square(data), axis=tuple(axes), keepdims=keepdims == 1)\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_sum_square_do_not_keepdims_random')\n\n    @staticmethod\n    def export_keepdims():  # type: () -> None\n        shape = [3, 2, 2]\n        axes = [1]\n        keepdims = 1\n\n        node = onnx.helper.make_node(\n            'ReduceSumSquare',\n            inputs=['data'],\n            outputs=['reduced'],\n            axes=axes,\n            keepdims=keepdims)\n\n        data = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]], dtype=np.float32)\n        reduced = np.sum(np.square(data), axis=tuple(axes), keepdims=keepdims == 1)\n        #print(reduced)\n        #[[[10., 20.]]\n        # [[74., 100.]]\n        # [[202., 244.]]]\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_sum_square_keepdims_example')\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.sum(np.square(data), axis=tuple(axes), keepdims=keepdims == 1)\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_sum_square_keepdims_random')\n\n    @staticmethod\n    def export_default_axes_keepdims():  # type: () -> None\n        shape = [3, 2, 2]\n        axes = None\n        keepdims = 1\n\n        node = onnx.helper.make_node(\n            'ReduceSumSquare',\n            inputs=['data'],\n            outputs=['reduced'],\n            keepdims=keepdims)\n\n        data = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]], dtype=np.float32)\n        reduced = np.sum(np.square(data), axis=axes, keepdims=keepdims == 1)\n        #print(reduced)\n        #[[[650.]]]\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_sum_square_default_axes_keepdims_example')\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.sum(np.square(data), axis=axes, keepdims=keepdims == 1)\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_sum_square_default_axes_keepdims_random')\n\n    @staticmethod\n    def export_negative_axes_keepdims():  # type: () -> None\n        shape = [3, 2, 2]\n        axes = [-2]\n        keepdims = 1\n\n        node = onnx.helper.make_node(\n            'ReduceSumSquare',\n            inputs=['data'],\n            outputs=['reduced'],\n            axes=axes,\n            keepdims=keepdims)\n\n        data = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]], dtype=np.float32)\n        reduced = np.sum(np.square(data), axis=tuple(axes), keepdims=keepdims == 1)\n        # print(reduced)\n        #[[[10., 20.s]]\n        # [[74., 100.]]\n        # [[202., 244.]]]\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_sum_square_negative_axes_keepdims_example')\n\n        np.random.seed(0)\n        data = np.random.uniform(-10, 10, shape).astype(np.float32)\n        reduced = np.sum(np.square(data), axis=tuple(axes), keepdims=keepdims == 1)\n\n        expect(node, inputs=[data], outputs=[reduced], name='test_reduce_sum_square_negative_axes_keepdims_random')\n"""
onnx/backend/test/case/node/relu.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Relu(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Relu',\n            inputs=['x'],\n            outputs=['y'],\n        )\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.clip(x, 0, np.inf)\n\n        expect(node, inputs=[x], outputs=[y],\n               name='test_relu')\n"""
onnx/backend/test/case/node/reshape.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\ndef reshape_reference_implementation(data, shape):  # type: (np.ndarray, np.ndarray) -> np.ndarray\n    # replace zeros with corresponding dim size\n    # we need to do this because np.reshape doesn't support 0\n    new_shape = np.copy(shape)\n    zeros_index = np.where(shape == 0)\n    new_shape[zeros_index] = np.array(data.shape)[zeros_index]\n    reshaped = np.reshape(data, new_shape)\n    return reshaped\n\n\nclass Reshape(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        original_shape = [2, 3, 4]\n        test_cases = {\n            'reordered_all_dims': np.array([4, 2, 3], dtype=np.int64),\n            'reordered_last_dims': np.array([2, 4, 3], dtype=np.int64),\n            'reduced_dims': np.array([2, 12], dtype=np.int64),\n            'extended_dims': np.array([2, 3, 2, 2], dtype=np.int64),\n            'one_dim': np.array([24], dtype=np.int64),\n            'negative_dim': np.array([2, -1, 2], dtype=np.int64),\n            'negative_extended_dims': np.array([-1, 2, 3, 4], dtype=np.int64),\n            'zero_dim': np.array([2, 0, 4, 1], dtype=np.int64),\n            'zero_and_negative_dim': np.array([2, 0, 1, -1], dtype=np.int64),\n        }\n        data = np.random.random_sample(original_shape).astype(np.float32)\n\n        for test_name, shape in test_cases.items():\n            node = onnx.helper.make_node(\n                'Reshape',\n                inputs=['data', 'shape'],\n                outputs=['reshaped'],\n            )\n\n            reshaped = reshape_reference_implementation(data, shape)\n\n            expect(node, inputs=[data, shape], outputs=[reshaped],\n                   name='test_reshape_' + test_name)\n"""
onnx/backend/test/case/node/resize.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\nfrom typing import Any, List, Callable, Union, Optional, Text\n\n\ndef cartesian(arrays, out=None):\n    # type: (List[np.ndarray], np.ndarray) -> np.ndarray\n    """"""\n    From https://stackoverflow.com/a/1235363\n    Generate a cartesian product of input arrays.\n    Parameters\n    ----------\n    arrays : list of array-like\n        1-D arrays to form the cartesian product of.\n    out : ndarray\n        Array to place the cartesian product in.\n    Returns\n    -------\n    out : ndarray\n        2-D array of shape (M, len(arrays)) containing cartesian products\n        formed of input arrays.\n    Examples\n    --------\n    >>> cartesian(([1, 2, 3], [4, 5], [6, 7]))\n    array([[1, 4, 6],\n           [1, 4, 7],\n           [1, 5, 6],\n           [1, 5, 7],\n           [2, 4, 6],\n           [2, 4, 7],\n           [2, 5, 6],\n           [2, 5, 7],\n           [3, 4, 6],\n           [3, 4, 7],\n           [3, 5, 6],\n           [3, 5, 7]])\n    """"""\n\n    arrays = [np.asarray(x) for x in arrays]\n    dtype = arrays[0].dtype\n\n    n = np.prod([x.size for x in arrays])\n    if out is None:\n        out = np.zeros([n, len(arrays)], dtype=dtype)\n\n    m = n // arrays[0].size\n    out[:, 0] = np.repeat(arrays[0], m)\n    if arrays[1:]:\n        cartesian(arrays[1:], out=out[0:m, 1:])\n        for j in range(1, arrays[0].size):\n            out[j * m:(j + 1) * m, 1:] = out[0:m, 1:]\n    return out\n\n\ndef interpolate_1d_with_x(data,                                             # type: np.ndarray\n                          scale_factor,                                     # type: float\n                          x,                                                # type: float\n                          get_coeffs,                                       # type: Callable[[float], np.ndarray]\n                          roi=None,                                         # type: np.ndarray\n                          extrapolation_value=0.0,                          # type: float\n                          coordinate_transformation_mode=\'half_pixel\',      # type: Text\n                          exclude_outside=False,                            # type: bool\n                          ):                                                # type: (...) -> np.ndarray\n    def get_neighbor_idxes(x, n, limit):  # type: (float, int, int) -> np.ndarray\n        """"""\n        Return the n nearest indexes to x among [0, limit), prefer the indexes smaller than x.\n        As a result, the ratio must be in (0, 1]\n        Examples:\n        get_neighbor_idxes(4, 2, 10) == [3, 4]\n        get_neighbor_idxes(4, 3, 10) == [3, 4, 5]\n        get_neighbor_idxes(4.4, 3, 10) == [3, 4, 5]\n        get_neighbor_idxes(4.5, 3, 10) == [3, 4, 5]\n        get_neighbor_idxes(4.6, 3, 10) == [4, 5, 6]\n        get_neighbor_idxes(4.4, 1, 10) == [4]\n        get_neighbor_idxes(4.6, 1, 10) == [5]\n        :param x:\n        :param n: the number of the wanted indexes\n        :param limit: the maximum value of index\n        :return: An np.array containing n nearest indexes in ascending order\n        """"""\n        idxes = sorted(range(limit), key=lambda idx: (abs(x - idx), idx))[:n]\n        idxes = sorted(idxes)\n        return np.array(idxes)\n\n    def get_neighbor(x, n, data):  # type: (float, int, np.ndarray) -> np.ndarray\n        """"""\n        Pad `data` in \'edge\' mode, and get n nearest elements in the padded array and their indexes in the original\n        array\n        :param x: center index (in the unpadded coordinate system) of the found nearest elements.\n        :param n: the number of neighbors.\n        :param data: the array\n        :return: A tuple containing the indexes of neighbor elements (the index can be smaller than 0 or higher than\n        len(data)) and the value of these elements\n        """"""\n        pad_width = np.ceil(n / 2).astype(np.int)\n        padded = np.pad(data, pad_width, mode=\'edge\')\n        x += pad_width\n\n        idxes = get_neighbor_idxes(x, n, len(padded))\n        ret = padded[idxes]\n        return idxes - pad_width, ret\n\n    input_width = len(data)\n    output_width = scale_factor * input_width\n    if coordinate_transformation_mode == \'align_corners\':\n        if output_width == 1:\n            x_ori = 0.\n        else:\n            x_ori = x * (input_width - 1) / (output_width - 1)\n    elif coordinate_transformation_mode == \'asymmetric\':\n        x_ori = x / scale_factor\n    elif coordinate_transformation_mode == \'tf_crop_and_resize\':\n        if output_width == 1:\n            x_ori = (roi[1] - roi[0]) * (input_width - 1) / 2\n        else:\n            x_ori = x * (roi[1] - roi[0]) * \\\n                (input_width - 1) / (output_width - 1)\n        x_ori += (roi[0] * (input_width - 1))\n        # Return extrapolation_value directly as what TF CropAndResize does\n        if x_ori < 0 or x_ori > input_width - 1:\n            return extrapolation_value\n    elif coordinate_transformation_mode == \'tf_half_pixel_for_nn\':\n        x_ori = (x + 0.5) / scale_factor\n    elif coordinate_transformation_mode == \'pytorch_half_pixel\':\n        if output_width == 1:\n            x_ori = -0.5\n        else:\n            x_ori = (x + 0.5) / scale_factor - 0.5\n    else:  # coordinate_transformation_mode == \'half_pixel\'\n        x_ori = (x + 0.5) / scale_factor - 0.5\n    x_ori_int = np.floor(x_ori).astype(np.int).item()\n\n    # ratio must be in (0, 1] since we prefer the pixel on the left of `x_ori`\n    if x_ori.is_integer():\n        ratio = 1\n    else:\n        ratio = x_ori - x_ori_int\n\n    coeffs = get_coeffs(ratio)\n    n = len(coeffs)\n\n    idxes, points = get_neighbor(x_ori, n, data)\n\n    if exclude_outside:\n        for i, idx in enumerate(idxes):\n            if idx < 0 or idx >= input_width:\n                coeffs[i] = 0\n        coeffs /= sum(coeffs)\n\n    return np.dot(coeffs, points).item()\n\n\ndef interpolate_nd_with_x(data,                      # type: np.ndarray\n                          n,                         # type: int\n                          scale_factors,             # type: List[float]\n                          x,                         # type: List[float]\n                          get_coeffs,                # type: Callable[[float], np.ndarray]\n                          roi=None,                  # type: np.ndarray\n                          **kwargs                   # type: Any\n                          ):                         # type: (...) -> np.ndarray\n    if n == 1:\n        return interpolate_1d_with_x(data, scale_factors[0], x[0], get_coeffs, roi=roi,\n                                     **kwargs)\n    return interpolate_1d_with_x(\n        [interpolate_nd_with_x(data[i], n - 1, scale_factors[1:], x[1:], get_coeffs,\n                               roi=None if roi is None else np.concatenate(\n                                   [roi[1:n], roi[n + 1:]]),\n                               **kwargs)\n         for i in range(data.shape[0])], scale_factors[0], x[0], get_coeffs,\n        roi=None if roi is None else [roi[0], roi[n]], **kwargs)\n\n\ndef interpolate_nd(data,                      # type: np.ndarray\n                   get_coeffs,                # type: Callable[[float], np.ndarray]\n                   output_size=None,          # type: Optional[List[int]]\n                   scale_factors=None,        # type: Optional[List[float]]\n                   roi=None,                  # type: np.ndarray\n                   **kwargs                   # type: Any\n                   ):                         # type: (...) -> np.ndarray\n    def get_all_coords(data):   # type: (np.ndarray) -> np.ndarray\n        return cartesian([list(range(data.shape[i])) for i in range(len(data.shape))])\n\n    assert output_size is not None or scale_factors is not None\n    if output_size is not None:\n        scale_factors = np.array(output_size) / np.array(data.shape)\n    else:\n        output_size = (scale_factors * np.array(data.shape)).astype(np.int)\n    assert scale_factors is not None\n\n    ret = np.zeros(output_size)\n    for x in get_all_coords(ret):\n        ret[tuple(x)] = interpolate_nd_with_x(data, len(data.shape), scale_factors, x, get_coeffs, roi=roi,\n                                              **kwargs)\n    return ret\n\n\ndef cubic_coeffs(ratio, A=-0.75):   # type: (float, float) -> np.ndarray\n    coeffs = [((A * (ratio + 1) - 5 * A) * (ratio + 1) + 8 * A) * (ratio + 1) - 4 * A,\n              ((A + 2) * ratio - (A + 3)) * ratio * ratio + 1,\n              ((A + 2) * (1 - ratio) - (A + 3)) * (1 - ratio) * (1 - ratio) + 1,\n              ((A * ((1 - ratio) + 1) - 5 * A) * ((1 - ratio) + 1) + 8 * A) * ((1 - ratio) + 1) - 4 * A]\n\n    return np.array(coeffs)\n\n\ndef linear_coeffs(ratio):           # type: (float) -> np.ndarray\n    return np.array([1 - ratio, ratio])\n\n\ndef nearest_coeffs(ratio, mode=\'round_prefer_floor\'):          # type: (float, Text) -> np.ndarray\n    if type(ratio) == int or ratio.is_integer():\n        return np.array([0, 1])\n    elif mode == \'round_prefer_floor\':\n        return np.array([ratio <= 0.5, ratio > 0.5])\n    elif mode == \'round_prefer_ceil\':\n        return np.array([ratio < 0.5, ratio >= 0.5])\n    elif mode == \'floor\':\n        return np.array([1, 0])\n    elif mode == \'ceil\':\n        return np.array([0, 1])\n\n\nclass Resize(Base):\n\n    @staticmethod\n    def export_resize_upsample_scales_nearest():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'Resize\',\n            inputs=[\'X\', \'roi\', \'scales\'],\n            outputs=[\'Y\'],\n            mode=\'nearest\',\n        )\n\n        data = np.array([[[\n            [1, 2],\n            [3, 4],\n        ]]], dtype=np.float32)\n\n        roi = np.array([], dtype=np.float32)\n        scales = np.array([1.0, 1.0, 2.0, 3.0], dtype=np.float32)\n\n        # [[[[1. 1. 1. 2. 2. 2.]\n        #    [1. 1. 1. 2. 2. 2.]\n        #    [3. 3. 3. 4. 4. 4.]\n        #    [3. 3. 3. 4. 4. 4.]]]]\n        output = interpolate_nd(\n            data, nearest_coeffs, scale_factors=scales).astype(np.float32)\n\n        expect(node, inputs=[data, roi, scales], outputs=[output],\n               name=\'test_resize_upsample_scales_nearest\')\n\n    @staticmethod\n    def export_resize_downsample_scales_nearest():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'Resize\',\n            inputs=[\'X\', \'roi\', \'scales\'],\n            outputs=[\'Y\'],\n            mode=\'nearest\',\n        )\n\n        data = np.array([[[\n            [1, 2, 3, 4],\n            [5, 6, 7, 8],\n        ]]], dtype=np.float32)\n\n        roi = np.array([], dtype=np.float32)\n        scales = np.array([1.0, 1.0, 0.6, 0.6], dtype=np.float32)\n\n        # [[[[1. 3.]]]]\n        output = interpolate_nd(\n            data, nearest_coeffs, scale_factors=scales).astype(np.float32)\n\n        expect(node, inputs=[data, roi, scales], outputs=[output],\n               name=\'test_resize_downsample_scales_nearest\')\n\n    @staticmethod\n    def export_resize_upsample_sizes_nearest():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'Resize\',\n            inputs=[\'X\', \'roi\', \'scales\', \'sizes\'],\n            outputs=[\'Y\'],\n            mode=\'nearest\',\n        )\n\n        data = np.array([[[\n            [1, 2],\n            [3, 4],\n        ]]], dtype=np.float32)\n\n        roi = np.array([], dtype=np.float32)\n        scales = np.array([], dtype=np.float32)\n        sizes = np.array([1, 1, 7, 8], dtype=np.int64)\n\n        # [[[[1. 1. 1. 1. 2. 2. 2. 2.]\n        #    [1. 1. 1. 1. 2. 2. 2. 2.]\n        #    [1. 1. 1. 1. 2. 2. 2. 2.]\n        #    [1. 1. 1. 1. 2. 2. 2. 2.]\n        #    [3. 3. 3. 3. 4. 4. 4. 4.]\n        #    [3. 3. 3. 3. 4. 4. 4. 4.]\n        #    [3. 3. 3. 3. 4. 4. 4. 4.]]]]\n        output = interpolate_nd(\n            data, nearest_coeffs, output_size=sizes).astype(np.float32)\n\n        expect(node, inputs=[data, roi, scales, sizes], outputs=[output],\n               name=\'test_resize_upsample_sizes_nearest\')\n\n    @staticmethod\n    def export_resize_downsample_sizes_nearest():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'Resize\',\n            inputs=[\'X\', \'roi\', \'scales\', \'sizes\'],\n            outputs=[\'Y\'],\n            mode=\'nearest\',\n        )\n\n        data = np.array([[[\n            [1, 2, 3, 4],\n            [5, 6, 7, 8],\n        ]]], dtype=np.float32)\n\n        roi = np.array([], dtype=np.float32)\n        scales = np.array([], dtype=np.float32)\n        sizes = np.array([1, 1, 1, 3], dtype=np.int64)\n\n        # [[[[1. 3.]]]]\n        output = interpolate_nd(\n            data, nearest_coeffs, output_size=sizes).astype(np.float32)\n\n        expect(node, inputs=[data, roi, scales, sizes], outputs=[output],\n               name=\'test_resize_downsample_sizes_nearest\')\n\n    @staticmethod\n    def export_resize_upsample_scales_linear():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'Resize\',\n            inputs=[\'X\', \'roi\', \'scales\'],\n            outputs=[\'Y\'],\n            mode=\'linear\',\n        )\n\n        data = np.array([[[\n            [1, 2],\n            [3, 4],\n        ]]], dtype=np.float32)\n\n        roi = np.array([], dtype=np.float32)\n        scales = np.array([1.0, 1.0, 2.0, 2.0], dtype=np.float32)\n\n        # [[[[1.   1.25 1.75 2.  ]\n        #    [1.5  1.75 2.25 2.5 ]\n        #    [2.5  2.75 3.25 3.5 ]\n        #    [3.   3.25 3.75 4.  ]]]]\n        output = interpolate_nd(\n            data, linear_coeffs, scale_factors=scales).astype(np.float32)\n\n        expect(node, inputs=[data, roi, scales], outputs=[output],\n               name=\'test_resize_upsample_scales_linear\')\n\n    @staticmethod\n    def export_resize_upsample_scales_linear_align_corners():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'Resize\',\n            inputs=[\'X\', \'roi\', \'scales\'],\n            outputs=[\'Y\'],\n            mode=\'linear\',\n            coordinate_transformation_mode=\'align_corners\'\n        )\n\n        data = np.array([[[\n            [1, 2],\n            [3, 4],\n        ]]], dtype=np.float32)\n\n        roi = np.array([], dtype=np.float32)\n        scales = np.array([1.0, 1.0, 2.0, 2.0], dtype=np.float32)\n\n        # [[[[1.         1.33333333 1.66666667 2.        ]\n        #    [1.66666667 2.         2.33333333 2.66666667]\n        #    [2.33333333 2.66666667 3.         3.33333333]\n        #    [3.         3.33333333 3.66666667 4.        ]]]]\n        output = interpolate_nd(\n            data, linear_coeffs, scale_factors=scales, coordinate_transformation_mode=\'align_corners\').astype(np.float32)\n\n        expect(node, inputs=[data, roi, scales], outputs=[output],\n               name=\'test_resize_upsample_scales_linear_align_corners\')\n\n    @staticmethod\n    def export_resize_downsample_scales_linear():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'Resize\',\n            inputs=[\'X\', \'roi\', \'scales\'],\n            outputs=[\'Y\'],\n            mode=\'linear\',\n        )\n\n        data = np.array([[[\n            [1, 2, 3, 4],\n            [5, 6, 7, 8],\n        ]]], dtype=np.float32)\n\n        roi = np.array([], dtype=np.float32)\n        scales = np.array([1.0, 1.0, 0.6, 0.6], dtype=np.float32)\n\n        # [[[[2.6666665 4.3333331]]]]\n        output = interpolate_nd(\n            data, linear_coeffs, scale_factors=scales).astype(np.float32)\n\n        expect(node, inputs=[data, roi, scales], outputs=[output],\n               name=\'test_resize_downsample_scales_linear\')\n\n    @staticmethod\n    def export_resize_downsample_scales_linear_align_corners():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'Resize\',\n            inputs=[\'X\', \'roi\', \'scales\'],\n            outputs=[\'Y\'],\n            mode=\'linear\',\n            coordinate_transformation_mode=\'align_corners\'\n        )\n\n        data = np.array([[[\n            [1, 2, 3, 4],\n            [5, 6, 7, 8],\n        ]]], dtype=np.float32)\n\n        roi = np.array([], dtype=np.float32)\n        scales = np.array([1.0, 1.0, 0.6, 0.6], dtype=np.float32)\n\n        # [[[[1.       3.142857]]]]\n        output = interpolate_nd(\n            data, linear_coeffs, scale_factors=scales, coordinate_transformation_mode=\'align_corners\').astype(np.float32)\n\n        expect(node, inputs=[data, roi, scales], outputs=[output],\n               name=\'test_resize_downsample_scales_linear_align_corners\')\n\n    @staticmethod\n    def export_resize_upsample_scales_cubic():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'Resize\',\n            inputs=[\'X\', \'roi\', \'scales\'],\n            outputs=[\'Y\'],\n            mode=\'cubic\',\n        )\n\n        data = np.array([[[\n            [1, 2, 3, 4],\n            [5, 6, 7, 8],\n            [9, 10, 11, 12],\n            [13, 14, 15, 16],\n        ]]], dtype=np.float32)\n\n        roi = np.array([], dtype=np.float32)\n        scales = np.array([1.0, 1.0, 2.0, 2.0], dtype=np.float32)\n\n        # [[[[ 0.47265625  0.76953125  1.24609375  1.875       2.28125\n        #      2.91015625  3.38671875  3.68359375]\n        #    [ 1.66015625  1.95703125  2.43359375  3.0625      3.46875\n        #      4.09765625  4.57421875  4.87109375]\n        #    [ 3.56640625  3.86328125  4.33984375  4.96875     5.375\n        #      6.00390625  6.48046875  6.77734375]\n        #    [ 6.08203125  6.37890625  6.85546875  7.484375    7.890625\n        #      8.51953125  8.99609375  9.29296875]\n        #    [ 7.70703125  8.00390625  8.48046875  9.109375    9.515625\n        #     10.14453125 10.62109375 10.91796875]\n        #    [10.22265625 10.51953125 10.99609375 11.625      12.03125\n        #     12.66015625 13.13671875 13.43359375]\n        #    [12.12890625 12.42578125 12.90234375 13.53125    13.9375\n        #     14.56640625 15.04296875 15.33984375]\n        #    [13.31640625 13.61328125 14.08984375 14.71875    15.125\n        #     15.75390625 16.23046875 16.52734375]]]]\n        output = interpolate_nd(\n            data, cubic_coeffs, scale_factors=scales).astype(np.float32)\n\n        expect(node, inputs=[data, roi, scales], outputs=[output],\n               name=\'test_resize_upsample_scales_cubic\')\n\n    @staticmethod\n    def export_resize_upsample_scales_cubic_align_corners():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'Resize\',\n            inputs=[\'X\', \'roi\', \'scales\'],\n            outputs=[\'Y\'],\n            mode=\'cubic\',\n            coordinate_transformation_mode=\'align_corners\'\n        )\n\n        data = np.array([[[\n            [1, 2, 3, 4],\n            [5, 6, 7, 8],\n            [9, 10, 11, 12],\n            [13, 14, 15, 16],\n        ]]], dtype=np.float32)\n\n        roi = np.array([], dtype=np.float32)\n        scales = np.array([1.0, 1.0, 2.0, 2.0], dtype=np.float32)\n\n        # [[[[ 1.          1.34110787  1.80029155  2.32944606  2.67055394\n        #      3.19970845  3.65889213  4.        ]\n        #    [ 2.36443149  2.70553936  3.16472303  3.69387755  4.03498542\n        #      4.56413994  5.02332362  5.36443149]\n        #    [ 4.20116618  4.54227405  5.00145773  5.53061224  5.87172012\n        #      6.40087464  6.86005831  7.20116618]\n        #    [ 6.31778426  6.65889213  7.1180758   7.64723032  7.98833819\n        #      8.51749271  8.97667638  9.31778426]\n        #    [ 7.68221574  8.02332362  8.48250729  9.01166181  9.35276968\n        #      9.8819242  10.34110787 10.68221574]\n        #    [ 9.79883382 10.13994169 10.59912536 11.12827988 11.46938776\n        #     11.99854227 12.45772595 12.79883382]\n        #    [11.63556851 11.97667638 12.43586006 12.96501458 13.30612245\n        #     13.83527697 14.29446064 14.63556851]\n        #    [13.         13.34110787 13.80029155 14.32944606 14.67055394\n        #     15.19970845 15.65889213 16.        ]]]]\n        output = interpolate_nd(\n            data, cubic_coeffs, scale_factors=scales, coordinate_transformation_mode=\'align_corners\').astype(np.float32)\n\n        expect(node, inputs=[data, roi, scales], outputs=[output],\n               name=\'test_resize_upsample_scales_cubic_align_corners\')\n\n    @staticmethod\n    def export_resize_downsample_scales_cubic():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'Resize\',\n            inputs=[\'X\', \'roi\', \'scales\'],\n            outputs=[\'Y\'],\n            mode=\'cubic\',\n        )\n\n        data = np.array([[[\n            [1, 2, 3, 4],\n            [5, 6, 7, 8],\n            [9, 10, 11, 12],\n            [13, 14, 15, 16],\n        ]]], dtype=np.float32)\n\n        roi = np.array([], dtype=np.float32)\n        scales = np.array([1.0, 1.0, 0.8, 0.8], dtype=np.float32)\n\n        # [[[[ 1.47119141  2.78125     4.08251953]\n        #    [ 6.71142578  8.02148438  9.32275391]\n        #    [11.91650391 13.2265625  14.52783203]]]]\n        output = interpolate_nd(\n            data, cubic_coeffs, scale_factors=scales).astype(np.float32)\n\n        expect(node, inputs=[data, roi, scales], outputs=[output],\n               name=\'test_resize_downsample_scales_cubic\')\n\n    @staticmethod\n    def export_resize_downsample_scales_cubic_align_corners():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'Resize\',\n            inputs=[\'X\', \'roi\', \'scales\'],\n            outputs=[\'Y\'],\n            mode=\'cubic\',\n            coordinate_transformation_mode=\'align_corners\'\n        )\n\n        data = np.array([[[\n            [1, 2, 3, 4],\n            [5, 6, 7, 8],\n            [9, 10, 11, 12],\n            [13, 14, 15, 16],\n        ]]], dtype=np.float32)\n\n        roi = np.array([], dtype=np.float32)\n        scales = np.array([1.0, 1.0, 0.8, 0.8], dtype=np.float32)\n\n        # [[[[ 1.          2.39519159  3.79038317]\n        #    [ 6.58076634  7.97595793  9.37114951]\n        #    [12.16153268 13.55672427 14.95191585]]]]\n        output = interpolate_nd(\n            data, cubic_coeffs, scale_factors=scales, coordinate_transformation_mode=\'align_corners\').astype(np.float32)\n\n        expect(node, inputs=[data, roi, scales], outputs=[output],\n               name=\'test_resize_downsample_scales_cubic_align_corners\')\n\n    @staticmethod\n    def export_resize_upsample_sizes_cubic():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'Resize\',\n            inputs=[\'X\', \'roi\', \'scales\', \'sizes\'],\n            outputs=[\'Y\'],\n            mode=\'cubic\',\n        )\n\n        data = np.array([[[\n            [1, 2, 3, 4],\n            [5, 6, 7, 8],\n            [9, 10, 11, 12],\n            [13, 14, 15, 16],\n        ]]], dtype=np.float32)\n\n        roi = np.array([], dtype=np.float32)\n        scales = np.array([], dtype=np.float32)\n        sizes = np.array([1, 1, 9, 10], dtype=np.int64)\n\n        # [[[[ 0.45507922  0.64057922  0.97157922  1.42257922  1.90732922\n        #      2.22332922  2.70807922  3.15907922  3.49007922  3.67557922]\n        #    [ 1.39437963  1.57987963  1.91087963  2.36187963  2.84662963\n        #      3.16262963  3.64737963  4.09837963  4.42937963  4.61487963]\n        #    [ 2.95130693  3.13680693  3.46780693  3.91880693  4.40355693\n        #      4.71955693  5.20430693  5.65530693  5.98630693  6.17180693]\n        #    [ 5.20525069  5.39075069  5.72175069  6.17275069  6.65750069\n        #      6.97350069  7.45825069  7.90925069  8.24025069  8.42575069]\n        #    [ 6.88975     7.07525     7.40625     7.85725     8.342\n        #      8.658       9.14275     9.59375     9.92475    10.11025   ]\n        #    [ 8.57424931  8.75974931  9.09074931  9.54174931 10.02649931\n        #     10.34249931 10.82724931 11.27824931 11.60924931 11.79474931]\n        #    [10.82819307 11.01369307 11.34469307 11.79569307 12.28044307\n        #     12.59644307 13.08119307 13.53219307 13.86319307 14.04869307]\n        #    [12.38512037 12.57062037 12.90162037 13.35262037 13.83737037\n        #     14.15337037 14.63812037 15.08912037 15.42012037 15.60562037]\n        #    [13.32442078 13.50992078 13.84092078 14.29192078 14.77667078\n        #     15.09267078 15.57742078 16.02842078 16.35942078 16.54492078]]]]\n        output = interpolate_nd(\n            data, cubic_coeffs, output_size=sizes).astype(np.float32)\n\n        expect(node, inputs=[data, roi, scales, sizes], outputs=[output],\n               name=\'test_resize_upsample_sizes_cubic\')\n\n    @staticmethod\n    def export_resize_downsample_sizes_cubic():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'Resize\',\n            inputs=[\'X\', \'roi\', \'scales\', \'sizes\'],\n            outputs=[\'Y\'],\n            mode=\'cubic\',\n        )\n\n        data = np.array([[[\n            [1, 2, 3, 4],\n            [5, 6, 7, 8],\n            [9, 10, 11, 12],\n            [13, 14, 15, 16],\n        ]]], dtype=np.float32)\n\n        roi = np.array([], dtype=np.float32)\n        scales = np.array([], dtype=np.float32)\n        sizes = np.array([1, 1, 3, 3], dtype=np.int64)\n\n        # [[[[ 1.63078704  3.00462963  4.37847222]\n        #    [ 7.12615741  8.5         9.87384259]\n        #    [12.62152778 13.99537037 15.36921296]]]]\n        output = interpolate_nd(\n            data, cubic_coeffs, output_size=sizes).astype(np.float32)\n\n        expect(node, inputs=[data, roi, scales, sizes], outputs=[output],\n               name=\'test_resize_downsample_sizes_cubic\')\n\n    # TensorFlow v1 bicubic with half_pixel_centers=True\n    @staticmethod\n    def export_resize_upsample_scales_cubic_A_n0p5_exclude_outside():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'Resize\',\n            inputs=[\'X\', \'roi\', \'scales\'],\n            outputs=[\'Y\'],\n            mode=\'cubic\',\n            cubic_coeff_a=-0.5,\n            exclude_outside=True\n        )\n\n        data = np.array([[[\n            [1, 2, 3, 4],\n            [5, 6, 7, 8],\n            [9, 10, 11, 12],\n            [13, 14, 15, 16],\n        ]]], dtype=np.float32)\n\n        roi = np.array([], dtype=np.float32)\n        scales = np.array([1.0, 1.0, 2.0, 2.0], dtype=np.float32)\n\n        # [[[[ 0.55882353  0.81494204  1.35698249  1.89705882  2.39705882\n        #      2.93713516  3.47917561  3.73529412]\n        #    [ 1.58329755  1.83941606  2.38145651  2.92153285  3.42153285\n        #      3.96160918  4.50364964  4.75976814]\n        #    [ 3.75145936  4.00757787  4.54961832  5.08969466  5.58969466\n        #      6.12977099  6.67181144  6.92792995]\n        #    [ 5.91176471  6.16788321  6.70992366  7.25        7.75\n        #      8.29007634  8.83211679  9.08823529]\n        #    [ 7.91176471  8.16788321  8.70992366  9.25        9.75\n        #     10.29007634 10.83211679 11.08823529]\n        #    [10.07207005 10.32818856 10.87022901 11.41030534 11.91030534\n        #     12.45038168 12.99242213 13.24854064]\n        #    [12.24023186 12.49635036 13.03839082 13.57846715 14.07846715\n        #     14.61854349 15.16058394 15.41670245]\n        #    [13.26470588 13.52082439 14.06286484 14.60294118 15.10294118\n        #     15.64301751 16.18505796 16.44117647]]]]\n        output = interpolate_nd(data, lambda x: cubic_coeffs(x, A=-0.5), scale_factors=scales,\n                                exclude_outside=True).astype(np.float32)\n\n        expect(node, inputs=[data, roi, scales], outputs=[output],\n               name=\'test_resize_upsample_scales_cubic_A_n0p5_exclude_outside\')\n\n    @staticmethod\n    def export_resize_downsample_scales_cubic_A_n0p5_exclude_outside():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'Resize\',\n            inputs=[\'X\', \'roi\', \'scales\'],\n            outputs=[\'Y\'],\n            mode=\'cubic\',\n            cubic_coeff_a=-0.5,\n            exclude_outside=True\n        )\n\n        data = np.array([[[\n            [1, 2, 3, 4],\n            [5, 6, 7, 8],\n            [9, 10, 11, 12],\n            [13, 14, 15, 16],\n        ]]], dtype=np.float32)\n\n        roi = np.array([], dtype=np.float32)\n        scales = np.array([1.0, 1.0, 0.8, 0.8], dtype=np.float32)\n\n        # [[[[ 1.36812675  2.6695014   4.0133367 ]\n        #    [ 6.57362535  7.875       9.2188353 ]\n        #    [11.94896657 13.25034122 14.59417652]]]]\n        output = interpolate_nd(data, lambda x: cubic_coeffs(x, A=-0.5), scale_factors=scales,\n                                exclude_outside=True).astype(np.float32)\n\n        expect(node, inputs=[data, roi, scales], outputs=[output],\n               name=\'test_resize_downsample_scales_cubic_A_n0p5_exclude_outside\')\n\n    # TensorFlow v1 bicubic with half_pixel_centers=False\n    @staticmethod\n    def export_resize_upsample_scales_cubic_asymmetric():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'Resize\',\n            inputs=[\'X\', \'roi\', \'scales\'],\n            outputs=[\'Y\'],\n            mode=\'cubic\',\n            coordinate_transformation_mode=\'asymmetric\'\n        )\n\n        data = np.array([[[\n            [1, 2, 3, 4],\n            [5, 6, 7, 8],\n            [9, 10, 11, 12],\n            [13, 14, 15, 16],\n        ]]], dtype=np.float32)\n\n        scales = np.array([1.0, 1.0, 2.0, 2.0], dtype=np.float32)\n        roi = np.array([], dtype=np.float32)\n\n        # [[[[ 1.       1.40625  2.       2.5      3.       3.59375  4.\n        #      4.09375]\n        #    [ 2.625    3.03125  3.625    4.125    4.625    5.21875  5.625\n        #      5.71875]\n        #    [ 5.       5.40625  6.       6.5      7.       7.59375  8.\n        #      8.09375]\n        #    [ 7.       7.40625  8.       8.5      9.       9.59375 10.\n        #     10.09375]\n        #    [ 9.       9.40625 10.      10.5     11.      11.59375 12.\n        #     12.09375]\n        #    [11.375   11.78125 12.375   12.875   13.375   13.96875 14.375\n        #     14.46875]\n        #    [13.      13.40625 14.      14.5     15.      15.59375 16.\n        #     16.09375]\n        #    [13.375   13.78125 14.375   14.875   15.375   15.96875 16.375\n        #     16.46875]]]]\n        output = interpolate_nd(data, lambda x: cubic_coeffs(x, A=-0.75), scale_factors=scales,\n                                coordinate_transformation_mode=\'asymmetric\').astype(np.float32)\n\n        expect(node, inputs=[data, roi, scales], outputs=[output],\n               name=\'test_resize_upsample_scales_cubic_asymmetric\')\n\n    @staticmethod\n    def export_resize_tf_crop_and_resize():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'Resize\',\n            inputs=[\'X\', \'roi\', \'scales\', \'sizes\'],\n            outputs=[\'Y\'],\n            mode=\'linear\',\n            coordinate_transformation_mode=\'tf_crop_and_resize\'\n        )\n\n        data = np.array([[[\n            [1, 2, 3, 4],\n            [5, 6, 7, 8],\n            [9, 10, 11, 12],\n            [13, 14, 15, 16],\n        ]]], dtype=np.float32)\n\n        # Note: for some rois, the result may be different with that of TF for inaccurate floating point\n        roi = np.array([0, 0, 0.4, 0.6, 1, 1, 0.6, 0.8], dtype=np.float32)\n        scales = np.array([], dtype=np.float32)\n        sizes = np.array([1, 1, 3, 3], dtype=np.int64)\n\n        # [[[[ 7.6000004  7.9        8.2      ]\n        #    [ 8.8        9.1        9.400001 ]\n        #    [10.        10.3       10.6      ]]]]\n        output = interpolate_nd(data, linear_coeffs, output_size=sizes, roi=roi,\n                                coordinate_transformation_mode=\'tf_crop_and_resize\').astype(np.float32)\n\n        expect(node, inputs=[data, roi, scales, sizes], outputs=[output],\n               name=\'test_resize_tf_crop_and_resize\')\n\n    @staticmethod\n    def export_resize_tf_crop_and_resize_extrapolation_value():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'Resize\',\n            inputs=[\'X\', \'roi\', \'scales\', \'sizes\'],\n            outputs=[\'Y\'],\n            mode=\'linear\',\n            coordinate_transformation_mode=\'tf_crop_and_resize\',\n            extrapolation_value=10.0\n        )\n\n        data = np.array([[[\n            [1, 2, 3, 4],\n            [5, 6, 7, 8],\n            [9, 10, 11, 12],\n            [13, 14, 15, 16],\n        ]]], dtype=np.float32)\n\n        # Note: for some rois, the result may be different with that of TF for inaccurate floating point\n        roi = np.array([0, 0, 0.4, 0.6, 1, 1, 1.2, 1.7], dtype=np.float32)\n        scales = np.array([], dtype=np.float32)\n        sizes = np.array([1, 1, 3, 3], dtype=np.int64)\n\n        # [[[[ 7.6000004 10.        10.       ]\n        #    [12.400001  10.        10.       ]\n        #    [10.        10.        10.       ]]]]\n        output = interpolate_nd(data, linear_coeffs, output_size=sizes, roi=roi,\n                                coordinate_transformation_mode=\'tf_crop_and_resize\', extrapolation_value=10.0).astype(np.float32)\n\n        expect(node, inputs=[data, roi, scales, sizes], outputs=[output],\n               name=\'test_resize_tf_crop_and_resize\')\n\n    @staticmethod\n    def export_resize_downsample_sizes_nearest_tf_half_pixel_for_nn():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'Resize\',\n            inputs=[\'X\', \'roi\', \'scales\', \'sizes\'],\n            outputs=[\'Y\'],\n            mode=\'nearest\',\n            coordinate_transformation_mode=\'tf_half_pixel_for_nn\'\n        )\n\n        data = np.array([[[\n            [1, 2, 3, 4],\n            [5, 6, 7, 8],\n            [9, 10, 11, 12],\n            [13, 14, 15, 16],\n        ]]], dtype=np.float32)\n\n        roi = np.array([], dtype=np.float32)\n        scales = np.array([], dtype=np.float32)\n        sizes = np.array([1, 1, 3, 2], dtype=np.int64)\n\n        # [[[[ 6.  8.]\n        #    [10. 12.]\n        #    [14. 16.]]]]\n        output = interpolate_nd(\n            data, nearest_coeffs, output_size=sizes, coordinate_transformation_mode=\'tf_half_pixel_for_nn\').astype(np.float32)\n\n        expect(node, inputs=[data, roi, scales, sizes], outputs=[output],\n               name=\'test_resize_downsample_sizes_nearest_tf_half_pixel_for_nn\')\n\n    @staticmethod\n    def export_resize_downsample_sizes_linear_pytorch_half_pixel():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'Resize\',\n            inputs=[\'X\', \'roi\', \'scales\', \'sizes\'],\n            outputs=[\'Y\'],\n            mode=\'linear\',\n            coordinate_transformation_mode=\'pytorch_half_pixel\'\n        )\n\n        data = np.array([[[\n            [1, 2, 3, 4],\n            [5, 6, 7, 8],\n            [9, 10, 11, 12],\n            [13, 14, 15, 16],\n        ]]], dtype=np.float32)\n\n        roi = np.array([], dtype=np.float32)\n        scales = np.array([], dtype=np.float32)\n        sizes = np.array([1, 1, 3, 1], dtype=np.int64)\n\n        # [[[[ 1.6666666]\n        #    [ 7.       ]\n        #    [12.333333 ]]]]\n        output = interpolate_nd(\n            data, linear_coeffs, output_size=sizes, coordinate_transformation_mode=\'pytorch_half_pixel\').astype(np.float32)\n\n        expect(node, inputs=[data, roi, scales, sizes], outputs=[output],\n               name=\'test_resize_downsample_sizes_linear_pytorch_half_pixel\')\n\n    @staticmethod\n    def export_resize_upsample_sizes_nearest_floor_align_corners():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'Resize\',\n            inputs=[\'X\', \'roi\', \'scales\', \'sizes\'],\n            outputs=[\'Y\'],\n            mode=\'nearest\',\n            coordinate_transformation_mode=\'align_corners\',\n            nearest_mode=\'floor\'\n        )\n\n        data = np.array([[[\n            [1, 2, 3, 4],\n            [5, 6, 7, 8],\n            [9, 10, 11, 12],\n            [13, 14, 15, 16],\n        ]]], dtype=np.float32)\n\n        roi = np.array([], dtype=np.float32)\n        scales = np.array([], dtype=np.float32)\n        sizes = np.array([1, 1, 8, 8], dtype=np.int64)\n\n        # [[[[ 1.  1.  1.  2.  2.  3.  3.  4.]\n        #    [ 1.  1.  1.  2.  2.  3.  3.  4.]\n        #    [ 1.  1.  1.  2.  2.  3.  3.  4.]\n        #    [ 5.  5.  5.  6.  6.  7.  7.  8.]\n        #    [ 5.  5.  5.  6.  6.  7.  7.  8.]\n        #    [ 9.  9.  9. 10. 10. 11. 11. 12.]\n        #    [ 9.  9.  9. 10. 10. 11. 11. 12.]\n        #    [13. 13. 13. 14. 14. 15. 15. 16.]]]]\n        output = interpolate_nd(\n            data, lambda x: nearest_coeffs(x, mode=\'floor\'), output_size=sizes, coordinate_transformation_mode=\'align_corners\').astype(np.float32)\n\n        expect(node, inputs=[data, roi, scales, sizes], outputs=[output],\n               name=\'test_resize_upsample_sizes_nearest_floor_align_corners\')\n\n    @staticmethod\n    def export_resize_upsample_sizes_nearest_round_prefer_ceil_asymmetric():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'Resize\',\n            inputs=[\'X\', \'roi\', \'scales\', \'sizes\'],\n            outputs=[\'Y\'],\n            mode=\'nearest\',\n            coordinate_transformation_mode=\'asymmetric\',\n            nearest_mode=\'round_prefer_ceil\'\n        )\n\n        data = np.array([[[\n            [1, 2, 3, 4],\n            [5, 6, 7, 8],\n            [9, 10, 11, 12],\n            [13, 14, 15, 16],\n        ]]], dtype=np.float32)\n\n        roi = np.array([], dtype=np.float32)\n        scales = np.array([], dtype=np.float32)\n        sizes = np.array([1, 1, 8, 8], dtype=np.int64)\n\n        # [[[[ 1.  2.  2.  3.  3.  4.  4.  4.]\n        #    [ 5.  6.  6.  7.  7.  8.  8.  8.]\n        #    [ 5.  6.  6.  7.  7.  8.  8.  8.]\n        #    [ 9. 10. 10. 11. 11. 12. 12. 12.]\n        #    [ 9. 10. 10. 11. 11. 12. 12. 12.]\n        #    [13. 14. 14. 15. 15. 16. 16. 16.]\n        #    [13. 14. 14. 15. 15. 16. 16. 16.]\n        #    [13. 14. 14. 15. 15. 16. 16. 16.]]]]\n        output = interpolate_nd(\n            data, lambda x: nearest_coeffs(x, mode=\'round_prefer_ceil\'),\n            output_size=sizes, coordinate_transformation_mode=\'asymmetric\').astype(np.float32)\n\n        expect(node, inputs=[data, roi, scales, sizes], outputs=[output],\n               name=\'test_resize_upsample_sizes_nearest_round_prefer_ceil_asymmetric\')\n\n    @staticmethod\n    def export_resize_upsample_sizes_nearest_ceil_half_pixel():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'Resize\',\n            inputs=[\'X\', \'roi\', \'scales\', \'sizes\'],\n            outputs=[\'Y\'],\n            mode=\'nearest\',\n            coordinate_transformation_mode=\'half_pixel\',\n            nearest_mode=\'ceil\'\n        )\n\n        data = np.array([[[\n            [1, 2, 3, 4],\n            [5, 6, 7, 8],\n            [9, 10, 11, 12],\n            [13, 14, 15, 16],\n        ]]], dtype=np.float32)\n\n        roi = np.array([], dtype=np.float32)\n        scales = np.array([], dtype=np.float32)\n        sizes = np.array([1, 1, 8, 8], dtype=np.int64)\n\n        # [[[[ 1.  2.  2.  3.  3.  4.  4.  4.]\n        #    [ 5.  6.  6.  7.  7.  8.  8.  8.]\n        #    [ 5.  6.  6.  7.  7.  8.  8.  8.]\n        #    [ 9. 10. 10. 11. 11. 12. 12. 12.]\n        #    [ 9. 10. 10. 11. 11. 12. 12. 12.]\n        #    [13. 14. 14. 15. 15. 16. 16. 16.]\n        #    [13. 14. 14. 15. 15. 16. 16. 16.]\n        #    [13. 14. 14. 15. 15. 16. 16. 16.]]]]\n        output = interpolate_nd(\n            data, lambda x: nearest_coeffs(x, mode=\'ceil\'), output_size=sizes).astype(np.float32)\n\n        expect(node, inputs=[data, roi, scales, sizes], outputs=[output],\n               name=\'test_resize_upsample_sizes_nearest_ceil_half_pixel\')\n'"
onnx/backend/test/case/node/reversesequence.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass ReverseSequence(Base):\n\n    @staticmethod\n    def export_reversesequence_time():  # type: () -> None\n        node = onnx.helper.make_node(\n            'ReverseSequence',\n            inputs=['x', 'sequence_lens'],\n            outputs=['y'],\n            time_axis=0,\n            batch_axis=1,\n        )\n        x = np.array([[0.0, 4.0, 8.0, 12.0],\n                      [1.0, 5.0, 9.0, 13.0],\n                      [2.0, 6.0, 10.0, 14.0],\n                      [3.0, 7.0, 11.0, 15.0]], dtype=np.float32)\n        sequence_lens = np.array([4, 3, 2, 1], dtype=np.int64)\n\n        y = np.array([[3.0, 6.0, 9.0, 12.0],\n                      [2.0, 5.0, 8.0, 13.0],\n                      [1.0, 4.0, 10.0, 14.0],\n                      [0.0, 7.0, 11.0, 15.0]], dtype=np.float32)\n\n        expect(node, inputs=[x, sequence_lens], outputs=[y],\n               name='test_reversesequence_time')\n\n    @staticmethod\n    def export_reversesequence_batch():  # type: () -> None\n        node = onnx.helper.make_node(\n            'ReverseSequence',\n            inputs=['x', 'sequence_lens'],\n            outputs=['y'],\n            time_axis=1,\n            batch_axis=0,\n        )\n        x = np.array([[0.0, 1.0, 2.0, 3.0],\n                      [4.0, 5.0, 6.0, 7.0],\n                      [8.0, 9.0, 10.0, 11.0],\n                      [12.0, 13.0, 14.0, 15.0]], dtype=np.float32)\n        sequence_lens = np.array([1, 2, 3, 4], dtype=np.int64)\n\n        y = np.array([[0.0, 1.0, 2.0, 3.0],\n                      [5.0, 4.0, 6.0, 7.0],\n                      [10.0, 9.0, 8.0, 11.0],\n                      [15.0, 14.0, 13.0, 12.0]], dtype=np.float32)\n\n        expect(node, inputs=[x, sequence_lens], outputs=[y],\n               name='test_reversesequence_batch')\n"""
onnx/backend/test/case/node/rnn.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\nfrom typing import Any, Tuple\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass RNN_Helper():\n    def __init__(self, **params):  # type: (**Any) -> None\n        # RNN Input Names\n        X = str(\'X\')\n        W = str(\'W\')\n        R = str(\'R\')\n        B = str(\'B\')\n        H_0 = str(\'initial_h\')\n\n        required_inputs = [X, W, R]\n        for i in required_inputs:\n            assert i in params, ""Missing Required Input: {0}"".format(i)\n\n        self.num_directions = params[str(W)].shape[0]\n\n        if self.num_directions == 1:\n            for k in params.keys():\n                if k != X:\n                    params[k] = np.squeeze(params[k], axis=0)\n\n            hidden_size = params[R].shape[-1]\n            batch_size = params[X].shape[1]\n\n            b = params[B] if B in params else np.zeros(2 * hidden_size, dtype=np.float32)\n            h_0 = params[H_0] if H_0 in params else np.zeros((batch_size, hidden_size), dtype=np.float32)\n\n            self.X = params[X]\n            self.W = params[W]\n            self.R = params[R]\n            self.B = b\n            self.H_0 = h_0\n        else:\n            raise NotImplementedError()\n\n    def f(self, x):  # type: (np.ndarray) -> np.ndarray\n        return np.tanh(x)\n\n    def step(self):  # type: () -> Tuple[np.ndarray, np.ndarray]\n        h_list = []\n        H_t = self.H_0\n        for x in np.split(self.X, self.X.shape[0], axis=0):\n            H = self.f(np.dot(x, np.transpose(self.W)) + np.dot(H_t, np.transpose(self.R)) + np.add(\n                *np.split(self.B, 2)))\n            h_list.append(H)\n            H_t = H\n        concatenated = np.concatenate(h_list)\n        if self.num_directions == 1:\n            output = np.expand_dims(concatenated, 1)\n        return output, h_list[-1]\n\n\nclass RNN(Base):\n\n    @staticmethod\n    def export_defaults():  # type: () -> None\n        input = np.array([[[1., 2.], [3., 4.], [5., 6.]]]).astype(np.float32)\n\n        input_size = 2\n        hidden_size = 4\n        weight_scale = 0.1\n\n        node = onnx.helper.make_node(\n            \'RNN\',\n            inputs=[\'X\', \'W\', \'R\'],\n            outputs=[\'\', \'Y\'],\n            hidden_size=hidden_size\n        )\n\n        W = weight_scale * np.ones((1, hidden_size, input_size)).astype(np.float32)\n        R = weight_scale * np.ones((1, hidden_size, hidden_size)).astype(np.float32)\n\n        rnn = RNN_Helper(X=input, W=W, R=R)\n        _, Y_h = rnn.step()\n        expect(node, inputs=[input, W, R], outputs=[Y_h.astype(np.float32)], name=\'test_simple_rnn_defaults\')\n\n    @staticmethod\n    def export_initial_bias():  # type: () -> None\n        input = np.array([[[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]]]).astype(np.float32)\n\n        input_size = 3\n        hidden_size = 5\n        custom_bias = 0.1\n        weight_scale = 0.1\n\n        node = onnx.helper.make_node(\n            \'RNN\',\n            inputs=[\'X\', \'W\', \'R\', \'B\'],\n            outputs=[\'\', \'Y\'],\n            hidden_size=hidden_size\n        )\n\n        W = weight_scale * np.ones((1, hidden_size, input_size)).astype(np.float32)\n        R = weight_scale * np.ones((1, hidden_size, hidden_size)).astype(np.float32)\n\n        # Adding custom bias\n        W_B = custom_bias * np.ones((1, hidden_size)).astype(np.float32)\n        R_B = np.zeros((1, hidden_size)).astype(np.float32)\n        B = np.concatenate((W_B, R_B), axis=1)\n\n        rnn = RNN_Helper(X=input, W=W, R=R, B=B)\n        _, Y_h = rnn.step()\n        expect(node, inputs=[input, W, R, B], outputs=[Y_h.astype(np.float32)],\n               name=\'test_simple_rnn_with_initial_bias\')\n\n    @staticmethod\n    def export_seq_length():  # type: () -> None\n        input = np.array([[[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]],\n                          [[10., 11., 12.], [13., 14., 15.], [16., 17., 18.]]]).astype(np.float32)\n\n        input_size = 3\n        hidden_size = 5\n\n        node = onnx.helper.make_node(\n            \'RNN\',\n            inputs=[\'X\', \'W\', \'R\', \'B\'],\n            outputs=[\'\', \'Y\'],\n            hidden_size=hidden_size\n        )\n\n        W = np.random.randn(1, hidden_size, input_size).astype(np.float32)\n        R = np.random.randn(1, hidden_size, hidden_size).astype(np.float32)\n\n        # Adding custom bias\n        W_B = np.random.randn(1, hidden_size).astype(np.float32)\n        R_B = np.random.randn(1, hidden_size).astype(np.float32)\n        B = np.concatenate((W_B, R_B), axis=1)\n\n        rnn = RNN_Helper(X=input, W=W, R=R, B=B)\n        _, Y_h = rnn.step()\n        expect(node, inputs=[input, W, R, B], outputs=[Y_h.astype(np.float32)], name=\'test_rnn_seq_length\')\n'"
onnx/backend/test/case/node/roialign.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass RoiAlign(Base):\n    @staticmethod\n    def export_roialign():  # type: () -> None\n        node = onnx.helper.make_node(\n            ""RoiAlign"",\n            inputs=[""X"", ""rois"", ""batch_indices""],\n            outputs=[""Y""],\n            spatial_scale=1.0,\n            output_height=5,\n            output_width=5,\n            sampling_ratio=2,\n        )\n\n        X = np.array(\n            [\n                [\n                    [\n                        [\n                            0.2764,\n                            0.7150,\n                            0.1958,\n                            0.3416,\n                            0.4638,\n                            0.0259,\n                            0.2963,\n                            0.6518,\n                            0.4856,\n                            0.7250,\n                        ],\n                        [\n                            0.9637,\n                            0.0895,\n                            0.2919,\n                            0.6753,\n                            0.0234,\n                            0.6132,\n                            0.8085,\n                            0.5324,\n                            0.8992,\n                            0.4467,\n                        ],\n                        [\n                            0.3265,\n                            0.8479,\n                            0.9698,\n                            0.2471,\n                            0.9336,\n                            0.1878,\n                            0.4766,\n                            0.4308,\n                            0.3400,\n                            0.2162,\n                        ],\n                        [\n                            0.0206,\n                            0.1720,\n                            0.2155,\n                            0.4394,\n                            0.0653,\n                            0.3406,\n                            0.7724,\n                            0.3921,\n                            0.2541,\n                            0.5799,\n                        ],\n                        [\n                            0.4062,\n                            0.2194,\n                            0.4473,\n                            0.4687,\n                            0.7109,\n                            0.9327,\n                            0.9815,\n                            0.6320,\n                            0.1728,\n                            0.6119,\n                        ],\n                        [\n                            0.3097,\n                            0.1283,\n                            0.4984,\n                            0.5068,\n                            0.4279,\n                            0.0173,\n                            0.4388,\n                            0.0430,\n                            0.4671,\n                            0.7119,\n                        ],\n                        [\n                            0.1011,\n                            0.8477,\n                            0.4726,\n                            0.1777,\n                            0.9923,\n                            0.4042,\n                            0.1869,\n                            0.7795,\n                            0.9946,\n                            0.9689,\n                        ],\n                        [\n                            0.1366,\n                            0.3671,\n                            0.7011,\n                            0.6234,\n                            0.9867,\n                            0.5585,\n                            0.6985,\n                            0.5609,\n                            0.8788,\n                            0.9928,\n                        ],\n                        [\n                            0.5697,\n                            0.8511,\n                            0.6711,\n                            0.9406,\n                            0.8751,\n                            0.7496,\n                            0.1650,\n                            0.1049,\n                            0.1559,\n                            0.2514,\n                        ],\n                        [\n                            0.7012,\n                            0.4056,\n                            0.7879,\n                            0.3461,\n                            0.0415,\n                            0.2998,\n                            0.5094,\n                            0.3727,\n                            0.5482,\n                            0.0502,\n                        ],\n                    ]\n                ]\n            ],\n            dtype=np.float32,\n        )\n        batch_indices = np.array([0, 0, 0], dtype=np.int64)\n        rois = np.array([[0, 0, 9, 9], [0, 5, 4, 9], [5, 5, 9, 9]], dtype=np.float32)\n        # (num_rois, C, output_height, output_width)\n        Y = np.array(\n            [\n                [\n                    [\n                        [0.4664, 0.4466, 0.3405, 0.5688, 0.6068],\n                        [0.3714, 0.4296, 0.3835, 0.5562, 0.3510],\n                        [0.2768, 0.4883, 0.5222, 0.5528, 0.4171],\n                        [0.4713, 0.4844, 0.6904, 0.4920, 0.8774],\n                        [0.6239, 0.7125, 0.6289, 0.3355, 0.3495],\n                    ]\n                ],\n                [\n                    [\n                        [0.3022, 0.4305, 0.4696, 0.3978, 0.5423],\n                        [0.3656, 0.7050, 0.5165, 0.3172, 0.7015],\n                        [0.2912, 0.5059, 0.6476, 0.6235, 0.8299],\n                        [0.5916, 0.7389, 0.7048, 0.8372, 0.8893],\n                        [0.6227, 0.6153, 0.7097, 0.6154, 0.4585],\n                    ]\n                ],\n                [\n                    [\n                        [0.2384, 0.3379, 0.3717, 0.6100, 0.7601],\n                        [0.3767, 0.3785, 0.7147, 0.9243, 0.9727],\n                        [0.5749, 0.5826, 0.5709, 0.7619, 0.8770],\n                        [0.5355, 0.2566, 0.2141, 0.2796, 0.3600],\n                        [0.4365, 0.3504, 0.2887, 0.3661, 0.2349],\n                    ]\n                ],\n            ],\n            dtype=np.float32,\n        )\n\n        expect(node, inputs=[X, rois, batch_indices], outputs=[Y], name=""test_roialign"")\n'"
onnx/backend/test/case/node/round.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Round(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Round',\n            inputs=['x'],\n            outputs=['y'],\n        )\n\n        x = np.array([0.1, 0.5, 0.9, 1.2, 1.5,\n                    1.8, 2.3, 2.5, 2.7, -1.1,\n                    -1.5, -1.9, -2.2, -2.5, -2.8]).astype(np.float32)\n        y = np.array([0., 0., 1., 1., 2.,\n                    2., 2., 2., 3., -1.,\n                    -2., -2., -2., -2., -3.]).astype(np.float32)  # expected output\n        expect(node, inputs=[x], outputs=[y],\n               name='test_round')\n"""
onnx/backend/test/case/node/scan.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Scan(Base):\n\n    @staticmethod\n    def export_scan_8():  # type: () -> None\n        # Given an input sequence [x1, ..., xN], sum up its elements using a scan\n        # returning the final state (x1+x2+...+xN) as well the scan_output\n        # [x1, x1+x2, ..., x1+x2+...+xN]\n        #\n        # create graph to represent scan body\n        sum_in = onnx.helper.make_tensor_value_info(\'sum_in\', onnx.TensorProto.FLOAT, [2])\n        next = onnx.helper.make_tensor_value_info(\'next\', onnx.TensorProto.FLOAT, [2])\n        sum_out = onnx.helper.make_tensor_value_info(\'sum_out\', onnx.TensorProto.FLOAT, [2])\n        scan_out = onnx.helper.make_tensor_value_info(\'scan_out\', onnx.TensorProto.FLOAT, [2])\n        add_node = onnx.helper.make_node(\n            \'Add\',\n            inputs=[\'sum_in\', \'next\'],\n            outputs=[\'sum_out\']\n        )\n        id_node = onnx.helper.make_node(\n            \'Identity\',\n            inputs=[\'sum_out\'],\n            outputs=[\'scan_out\']\n        )\n        scan_body = onnx.helper.make_graph(\n            [add_node, id_node],\n            \'scan_body\',\n            [sum_in, next],\n            [sum_out, scan_out]\n        )\n        # create scan op node\n        no_sequence_lens = \'\'   # optional input, not supplied\n        node = onnx.helper.make_node(\n            \'Scan\',\n            inputs=[no_sequence_lens, \'initial\', \'x\'],\n            outputs=[\'y\', \'z\'],\n            num_scan_inputs=1,\n            body=scan_body\n        )\n        # create inputs for batch-size 1, sequence-length 3, inner dimension 2\n        initial = np.array([0, 0]).astype(np.float32).reshape((1, 2))\n        x = np.array([1, 2, 3, 4, 5, 6]).astype(np.float32).reshape((1, 3, 2))\n        # final state computed = [1 + 3 + 5, 2 + 4 + 6]\n        y = np.array([9, 12]).astype(np.float32).reshape((1, 2))\n        # scan-output computed\n        z = np.array([1, 2, 4, 6, 9, 12]).astype(np.float32).reshape((1, 3, 2))\n\n        expect(node, inputs=[initial, x], outputs=[y, z],\n               name=\'test_scan_sum\', opset_imports=[onnx.helper.make_opsetid("""", 8)])\n\n    @staticmethod\n    def export_scan_9():  # type: () -> None\n        # Given an input sequence [x1, ..., xN], sum up its elements using a scan\n        # returning the final state (x1+x2+...+xN) as well the scan_output\n        # [x1, x1+x2, ..., x1+x2+...+xN]\n        #\n        # create graph to represent scan body\n        sum_in = onnx.helper.make_tensor_value_info(\'sum_in\', onnx.TensorProto.FLOAT, [2])\n        next = onnx.helper.make_tensor_value_info(\'next\', onnx.TensorProto.FLOAT, [2])\n        sum_out = onnx.helper.make_tensor_value_info(\'sum_out\', onnx.TensorProto.FLOAT, [2])\n        scan_out = onnx.helper.make_tensor_value_info(\'scan_out\', onnx.TensorProto.FLOAT, [2])\n        add_node = onnx.helper.make_node(\n            \'Add\',\n            inputs=[\'sum_in\', \'next\'],\n            outputs=[\'sum_out\']\n        )\n        id_node = onnx.helper.make_node(\n            \'Identity\',\n            inputs=[\'sum_out\'],\n            outputs=[\'scan_out\']\n        )\n        scan_body = onnx.helper.make_graph(\n            [add_node, id_node],\n            \'scan_body\',\n            [sum_in, next],\n            [sum_out, scan_out]\n        )\n        # create scan op node\n        node = onnx.helper.make_node(\n            \'Scan\',\n            inputs=[\'initial\', \'x\'],\n            outputs=[\'y\', \'z\'],\n            num_scan_inputs=1,\n            body=scan_body\n        )\n        # create inputs for sequence-length 3, inner dimension 2\n        initial = np.array([0, 0]).astype(np.float32).reshape((2,))\n        x = np.array([1, 2, 3, 4, 5, 6]).astype(np.float32).reshape((3, 2))\n        # final state computed = [1 + 3 + 5, 2 + 4 + 6]\n        y = np.array([9, 12]).astype(np.float32).reshape((2,))\n        # scan-output computed\n        z = np.array([1, 2, 4, 6, 9, 12]).astype(np.float32).reshape((3, 2))\n\n        expect(node, inputs=[initial, x], outputs=[y, z],\n               name=\'test_scan9_sum\', opset_imports=[onnx.helper.make_opsetid("""", 9)])\n'"
onnx/backend/test/case/node/scatter.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\nfrom onnx import helper\n\n\n# The below Scatter\'s numpy implementation is from https://stackoverflow.com/a/46204790/11767360\ndef scatter(data, indices, updates, axis=0):  # type: ignore\n    if axis < 0:\n        axis = data.ndim + axis\n\n    idx_xsection_shape = indices.shape[:axis] + indices.shape[axis + 1:]\n\n    def make_slice(arr, axis, i):  # type: ignore\n        slc = [slice(None)] * arr.ndim\n        slc[axis] = i\n        return slc\n\n    def unpack(packed):  # type: ignore\n        unpacked = packed[0]\n        for i in range(1, len(packed)):\n            unpacked = unpacked, packed[i]\n        return unpacked\n\n    # We use indices and axis parameters to create idx\n    # idx is in a form that can be used as a NumPy advanced indices for scattering of updates param. in data\n    idx = [[unpack(np.indices(idx_xsection_shape).reshape(indices.ndim - 1, -1)),\n            indices[tuple(make_slice(indices, axis, i))].reshape(1, -1)[0]] for i in range(indices.shape[axis])]\n    idx = list(np.concatenate(idx, axis=1))\n    idx.insert(axis, idx.pop())\n\n    # updates_idx is a NumPy advanced indices for indexing of elements in the updates\n    updates_idx = list(idx)\n    updates_idx.pop(axis)\n    updates_idx.insert(axis, np.repeat(np.arange(indices.shape[axis]), np.prod(idx_xsection_shape)))\n\n    scattered = np.copy(data)\n    scattered[tuple(idx)] = updates[tuple(updates_idx)]\n    return scattered\n\n\nclass Scatter(Base):\n\n    @staticmethod\n    def export_scatter_without_axis():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'Scatter\',\n            inputs=[\'data\', \'indices\', \'updates\'],\n            outputs=[\'y\'],\n        )\n        data = np.zeros((3, 3), dtype=np.float32)\n        indices = np.array([[1, 0, 2], [0, 2, 1]], dtype=np.int64)\n        updates = np.array([[1.0, 1.1, 1.2], [2.0, 2.1, 2.2]], dtype=np.float32)\n\n        y = scatter(data, indices, updates)\n        # print(y) produces\n        # [[2.0, 1.1, 0.0],\n        #  [1.0, 0.0, 2.2],\n        #  [0.0, 2.1, 1.2]]\n\n        expect(node, inputs=[data, indices, updates], outputs=[y],\n               name=\'test_scatter_without_axis\', opset_imports=[helper.make_opsetid("""", 10)])\n\n    @staticmethod\n    def export_scatter_with_axis():  # type: () -> None\n        axis = 1\n        node = onnx.helper.make_node(\n            \'Scatter\',\n            inputs=[\'data\', \'indices\', \'updates\'],\n            outputs=[\'y\'],\n            axis=axis,\n        )\n        data = np.array([[1.0, 2.0, 3.0, 4.0, 5.0]], dtype=np.float32)\n        indices = np.array([[1, 3]], dtype=np.int64)\n        updates = np.array([[1.1, 2.1]], dtype=np.float32)\n\n        y = scatter(data, indices, updates, axis=axis)\n        # print(y) produces\n        # [[1.0, 1.1, 3.0, 2.1, 5.0]]\n\n        expect(node, inputs=[data, indices, updates], outputs=[y],\n               name=\'test_scatter_with_axis\', opset_imports=[helper.make_opsetid("""", 10)])\n'"
onnx/backend/test/case/node/scatterelements.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\n# The below ScatterElements' numpy implementation is from https://stackoverflow.com/a/46204790/11767360\ndef scatter_elements(data, indices, updates, axis=0):  # type: ignore\n    if axis < 0:\n        axis = data.ndim + axis\n\n    idx_xsection_shape = indices.shape[:axis] + indices.shape[axis + 1:]\n\n    def make_slice(arr, axis, i):  # type: ignore\n        slc = [slice(None)] * arr.ndim\n        slc[axis] = i\n        return slc\n\n    def unpack(packed):  # type: ignore\n        unpacked = packed[0]\n        for i in range(1, len(packed)):\n            unpacked = unpacked, packed[i]\n        return unpacked\n\n    # We use indices and axis parameters to create idx\n    # idx is in a form that can be used as a NumPy advanced indices for scattering of updates param. in data\n    idx = [[unpack(np.indices(idx_xsection_shape).reshape(indices.ndim - 1, -1)),\n            indices[tuple(make_slice(indices, axis, i))].reshape(1, -1)[0]] for i in range(indices.shape[axis])]\n    idx = list(np.concatenate(idx, axis=1))\n    idx.insert(axis, idx.pop())\n\n    # updates_idx is a NumPy advanced indices for indexing of elements in the updates\n    updates_idx = list(idx)\n    updates_idx.pop(axis)\n    updates_idx.insert(axis, np.repeat(np.arange(indices.shape[axis]), np.prod(idx_xsection_shape)))\n\n    scattered = np.copy(data)\n    scattered[tuple(idx)] = updates[tuple(updates_idx)]\n    return scattered\n\n\nclass ScatterElements(Base):\n\n    @staticmethod\n    def export_scatter_elements_without_axis():  # type: () -> None\n        node = onnx.helper.make_node(\n            'ScatterElements',\n            inputs=['data', 'indices', 'updates'],\n            outputs=['y'],\n        )\n        data = np.zeros((3, 3), dtype=np.float32)\n        indices = np.array([[1, 0, 2], [0, 2, 1]], dtype=np.int64)\n        updates = np.array([[1.0, 1.1, 1.2], [2.0, 2.1, 2.2]], dtype=np.float32)\n\n        y = scatter_elements(data, indices, updates)\n        # print(y) produces\n        # [[2.0, 1.1, 0.0],\n        #  [1.0, 0.0, 2.2],\n        #  [0.0, 2.1, 1.2]]\n\n        expect(node, inputs=[data, indices, updates], outputs=[y],\n               name='test_scatter_elements_without_axis')\n\n    @staticmethod\n    def export_scatter_elements_with_axis():  # type: () -> None\n        axis = 1\n        node = onnx.helper.make_node(\n            'ScatterElements',\n            inputs=['data', 'indices', 'updates'],\n            outputs=['y'],\n            axis=axis,\n        )\n        data = np.array([[1.0, 2.0, 3.0, 4.0, 5.0]], dtype=np.float32)\n        indices = np.array([[1, 3]], dtype=np.int64)\n        updates = np.array([[1.1, 2.1]], dtype=np.float32)\n\n        y = scatter_elements(data, indices, updates, axis)\n        # print(y) produces\n        # [[1.0, 1.1, 3.0, 2.1, 5.0]]\n\n        expect(node, inputs=[data, indices, updates], outputs=[y],\n               name='test_scatter_elements_with_axis')\n\n    @staticmethod\n    def export_scatter_elements_with_negative_indices():  # type: () -> None\n        axis = 1\n        node = onnx.helper.make_node(\n            'ScatterElements',\n            inputs=['data', 'indices', 'updates'],\n            outputs=['y'],\n            axis=axis,\n        )\n        data = np.array([[1.0, 2.0, 3.0, 4.0, 5.0]], dtype=np.float32)\n        indices = np.array([[1, -3]], dtype=np.int64)\n        updates = np.array([[1.1, 2.1]], dtype=np.float32)\n\n        y = scatter_elements(data, indices, updates, axis)\n        # print(y) produces\n        # [[1.0, 1.1, 2.1, 4.0, 5.0]]\n\n        expect(node, inputs=[data, indices, updates], outputs=[y],\n               name='test_scatter_elements_with_negative_indices')\n"""
onnx/backend/test/case/node/scatternd.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\ndef scatter_nd_impl(data, indices, updates):\n    # type: (np.ndarray, np.ndarray, np.ndarray) -> np.ndarray\n\n    # Check tensor shapes\n    assert indices.shape[-1] <= len(data.shape)\n    assert updates.shape == indices.shape[:-1] + data.shape[indices.shape[-1]:]\n\n    # Compute output\n    output = np.copy(data)\n    for i in np.ndindex(indices.shape[:-1]):\n        # NOTE: The order of iteration in this loop is not specified.\n        # In particular, indices should not have duplicate entries: that is, if idx1 != idx2, then indices[idx1] != indices[idx2].\n        # This ensures that the output value does not depend on the iteration order.\n        output[indices[i]] = updates[i]\n    return output\n\n\nclass ScatterND(Base):\n\n    @staticmethod\n    def export_scatternd():  # type: () -> None\n        node = onnx.helper.make_node(\n            'ScatterND',\n            inputs=['data', 'indices', 'updates'],\n            outputs=['y'],\n        )\n        data = np.array(\n            [[[1, 2, 3, 4], [5, 6, 7, 8], [8, 7, 6, 5], [4, 3, 2, 1]],\n             [[1, 2, 3, 4], [5, 6, 7, 8], [8, 7, 6, 5], [4, 3, 2, 1]],\n             [[8, 7, 6, 5], [4, 3, 2, 1], [1, 2, 3, 4], [5, 6, 7, 8]],\n             [[8, 7, 6, 5], [4, 3, 2, 1], [1, 2, 3, 4], [5, 6, 7, 8]]], dtype=np.float32)\n        indices = np.array([[0], [2]], dtype=np.int64)\n        updates = np.array(\n            [[[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]],\n             [[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]]], dtype=np.float32)\n        # Expecting output as np.array(\n        #    [[[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]],\n        #     [[1, 2, 3, 4], [5, 6, 7, 8], [8, 7, 6, 5], [4, 3, 2, 1]],\n        #     [[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]],\n        #     [[8, 7, 6, 5], [4, 3, 2, 1], [1, 2, 3, 4], [5, 6, 7, 8]]], dtype=np.float32)\n        output = scatter_nd_impl(data, indices, updates)\n        expect(node, inputs=[data, indices, updates], outputs=[output],\n               name='test_scatternd')\n"""
onnx/backend/test/case/node/selu.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Selu(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Selu',\n            inputs=['x'],\n            outputs=['y'],\n            alpha=2.0,\n            gamma=3.0\n        )\n\n        x = np.array([-1, 0, 1]).astype(np.float32)\n        # expected output [-3.79272318, 0., 3.]\n        y = np.clip(x, 0, np.inf) * 3.0 + (np.exp(np.clip(x, -np.inf, 0)) - 1) * 2.0 * 3.0\n        expect(node, inputs=[x], outputs=[y],\n               name='test_selu_example')\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.clip(x, 0, np.inf) * 3.0 + (np.exp(np.clip(x, -np.inf, 0)) - 1) * 2.0 * 3.0\n        expect(node, inputs=[x], outputs=[y],\n               name='test_selu')\n\n    @staticmethod\n    def export_selu_default():  # type: () -> None\n        default_alpha = 1.67326319217681884765625\n        default_gamma = 1.05070102214813232421875\n        node = onnx.helper.make_node(\n            'Selu',\n            inputs=['x'],\n            outputs=['y'],\n        )\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.clip(x, 0, np.inf) * default_gamma + \\\n            (np.exp(np.clip(x, -np.inf, 0)) - 1) * default_alpha * default_gamma\n        expect(node, inputs=[x], outputs=[y],\n               name='test_selu_default')\n"""
onnx/backend/test/case/node/shape.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Shape(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Shape',\n            inputs=['x'],\n            outputs=['y'],\n        )\n\n        x = np.array([\n            [1, 2, 3],\n            [4, 5, 6],\n        ]).astype(np.float32)\n        y = np.array([\n            2, 3,\n        ]).astype(np.int64)\n\n        expect(node, inputs=[x], outputs=[y],\n               name='test_shape_example')\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.array(x.shape).astype(np.int64)\n\n        expect(node, inputs=[x], outputs=[y],\n               name='test_shape')\n"""
onnx/backend/test/case/node/shrink.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Shrink(Base):\n\n    @staticmethod\n    def export_hard_shrink():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Shrink',\n            inputs=['x'],\n            outputs=['y'],\n            lambd=1.5,\n        )\n        X = np.arange(-2.0, 2.1, dtype=np.float32)\n        Y = np.array([-2, 0, 0, 0, 2], dtype=np.float32)\n        expect(node, inputs=[X], outputs=[Y],\n               name='test_shrink_hard')\n\n    @staticmethod\n    def export_soft_shrink():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Shrink',\n            inputs=['x'],\n            outputs=['y'],\n            lambd=1.5,\n            bias=1.5,\n        )\n        X = np.arange(-2.0, 2.1, dtype=np.float32)\n        Y = np.array([-0.5, 0, 0, 0, 0.5], dtype=np.float32)\n        expect(node, inputs=[X], outputs=[Y],\n               name='test_shrink_soft')\n"""
onnx/backend/test/case/node/sigmoid.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Sigmoid(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Sigmoid',\n            inputs=['x'],\n            outputs=['y'],\n        )\n\n        x = np.array([-1, 0, 1]).astype(np.float32)\n        y = 1.0 / (1.0 + np.exp(np.negative(x)))  # expected output [0.26894143, 0.5, 0.7310586]\n        expect(node, inputs=[x], outputs=[y],\n               name='test_sigmoid_example')\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = 1.0 / (1.0 + np.exp(np.negative(x)))\n        expect(node, inputs=[x], outputs=[y],\n               name='test_sigmoid')\n"""
onnx/backend/test/case/node/sign.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Sign(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Sign',\n            inputs=['x'],\n            outputs=['y'],\n        )\n\n        x = np.array(range(-5, 6)).astype(np.float32)\n        y = np.sign(x)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_sign')\n"""
onnx/backend/test/case/node/sin.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Sin(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Sin',\n            inputs=['x'],\n            outputs=['y'],\n        )\n\n        x = np.array([-1, 0, 1]).astype(np.float32)\n        y = np.sin(x)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_sin_example')\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.sin(x)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_sin')\n"""
onnx/backend/test/case/node/sinh.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Sinh(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Sinh',\n            inputs=['x'],\n            outputs=['y'],\n        )\n\n        x = np.array([-1, 0, 1]).astype(np.float32)\n        y = np.sinh(x)  # expected output [-1.17520118,  0.,  1.17520118]\n        expect(node, inputs=[x], outputs=[y],\n               name='test_sinh_example')\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.sinh(x)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_sinh')\n"""
onnx/backend/test/case/node/size.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Size(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Size',\n            inputs=['x'],\n            outputs=['y'],\n        )\n\n        x = np.array([\n            [1, 2, 3],\n            [4, 5, 6],\n        ]).astype(np.float32)\n        y = np.array(6).astype(np.int64)\n\n        expect(node, inputs=[x], outputs=[y],\n               name='test_size_example')\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.array(x.size).astype(np.int64)\n\n        expect(node, inputs=[x], outputs=[y],\n               name='test_size')\n"""
onnx/backend/test/case/node/slice.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Slice(Base):\n\n    @staticmethod\n    def export_slice():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Slice',\n            inputs=['x', 'starts', 'ends', 'axes', 'steps'],\n            outputs=['y'],\n        )\n\n        x = np.random.randn(20, 10, 5).astype(np.float32)\n        y = x[0:3, 0:10]\n        starts = np.array([0, 0], dtype=np.int64)\n        ends = np.array([3, 10], dtype=np.int64)\n        axes = np.array([0, 1], dtype=np.int64)\n        steps = np.array([1, 1], dtype=np.int64)\n\n        expect(node, inputs=[x, starts, ends, axes, steps], outputs=[y],\n               name='test_slice')\n\n    @staticmethod\n    def export_slice_neg():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Slice',\n            inputs=['x', 'starts', 'ends', 'axes', 'steps'],\n            outputs=['y'],\n        )\n\n        x = np.random.randn(20, 10, 5).astype(np.float32)\n        starts = np.array([0], dtype=np.int64)\n        ends = np.array([-1], dtype=np.int64)\n        axes = np.array([1], dtype=np.int64)\n        steps = np.array([1], dtype=np.int64)\n        y = x[:, 0:-1]\n\n        expect(node, inputs=[x, starts, ends, axes, steps], outputs=[y],\n               name='test_slice_neg')\n\n    @staticmethod\n    def export_slice_start_out_of_bounds():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Slice',\n            inputs=['x', 'starts', 'ends', 'axes', 'steps'],\n            outputs=['y'],\n        )\n\n        x = np.random.randn(20, 10, 5).astype(np.float32)\n        starts = np.array([1000], dtype=np.int64)\n        ends = np.array([1000], dtype=np.int64)\n        axes = np.array([1], dtype=np.int64)\n        steps = np.array([1], dtype=np.int64)\n        y = x[:, 1000:1000]\n\n        expect(node, inputs=[x, starts, ends, axes, steps], outputs=[y],\n               name='test_slice_start_out_of_bounds')\n\n    @staticmethod\n    def export_slice_end_out_of_bounds():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Slice',\n            inputs=['x', 'starts', 'ends', 'axes', 'steps'],\n            outputs=['y'],\n        )\n\n        x = np.random.randn(20, 10, 5).astype(np.float32)\n        starts = np.array([1], dtype=np.int64)\n        ends = np.array([1000], dtype=np.int64)\n        axes = np.array([1], dtype=np.int64)\n        steps = np.array([1], dtype=np.int64)\n        y = x[:, 1:1000]\n\n        expect(node, inputs=[x, starts, ends, axes, steps], outputs=[y],\n               name='test_slice_end_out_of_bounds')\n\n    @staticmethod\n    def export_slice_default_axes():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Slice',\n            inputs=['x', 'starts', 'ends'],\n            outputs=['y'],\n        )\n\n        x = np.random.randn(20, 10, 5).astype(np.float32)\n        starts = np.array([0, 0, 3], dtype=np.int64)\n        ends = np.array([20, 10, 4], dtype=np.int64)\n        y = x[:, :, 3:4]\n\n        expect(node, inputs=[x, starts, ends], outputs=[y],\n               name='test_slice_default_axes')\n\n    @staticmethod\n    def export_slice_default_steps():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Slice',\n            inputs=['x', 'starts', 'ends', 'axes'],\n            outputs=['y'],\n        )\n\n        x = np.random.randn(20, 10, 5).astype(np.float32)\n        starts = np.array([0, 0, 3], dtype=np.int64)\n        ends = np.array([20, 10, 4], dtype=np.int64)\n        axes = np.array([0, 1, 2], dtype=np.int64)\n        y = x[:, :, 3:4]\n\n        expect(node, inputs=[x, starts, ends, axes], outputs=[y],\n               name='test_slice_default_steps')\n\n    @staticmethod\n    def export_slice_neg_steps():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Slice',\n            inputs=['x', 'starts', 'ends', 'axes', 'steps'],\n            outputs=['y'],\n        )\n\n        x = np.random.randn(20, 10, 5).astype(np.float32)\n        starts = np.array([20, 10, 4], dtype=np.int64)\n        ends = np.array([0, 0, 1], dtype=np.int64)\n        axes = np.array([0, 1, 2], dtype=np.int64)\n        steps = np.array([-1, -3, -2])\n        y = x[20:0:-1, 10:0:-3, 4:1:-2]\n\n        expect(node, inputs=[x, starts, ends, axes, steps], outputs=[y],\n               name='test_slice_neg_steps')\n\n    @staticmethod\n    def export_slice_negative_axes():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Slice',\n            inputs=['x', 'starts', 'ends', 'axes'],\n            outputs=['y'],\n        )\n\n        x = np.random.randn(20, 10, 5).astype(np.float32)\n        starts = np.array([0, 0, 3], dtype=np.int64)\n        ends = np.array([20, 10, 4], dtype=np.int64)\n        axes = np.array([0, -2, -1], dtype=np.int64)\n        y = x[:, :, 3:4]\n\n        expect(node, inputs=[x, starts, ends, axes], outputs=[y],\n               name='test_slice_negative_axes')\n"""
onnx/backend/test/case/node/softmax.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Softmax(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Softmax',\n            inputs=['x'],\n            outputs=['y'],\n        )\n        x = np.array([[-1, 0, 1]]).astype(np.float32)\n        # expected output [[0.09003058, 0.24472848, 0.66524094]]\n        y = np.exp(x) / np.sum(np.exp(x), axis=1)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_softmax_example')\n\n    @staticmethod\n    def export_softmax_axis():  # type: () -> None\n        def softmax_2d(x):  # type: (np.ndarray) -> np.ndarray\n            max_x = np.max(x, axis=1).reshape((-1, 1))\n            exp_x = np.exp(x - max_x)\n            return exp_x / np.sum(exp_x, axis=1).reshape((-1, 1))\n\n        x = np.array([[0, 1, 2, 3], [10000, 10001, 10002, 10003]]).astype(np.float32)\n        # expected output [[0.0320586, 0.08714432, 0.23688284, 0.64391428],\n        #                 [0.0320586, 0.08714432, 0.23688284, 0.64391428]]\n        y = softmax_2d(x)\n\n        node = onnx.helper.make_node(\n            'Softmax',\n            inputs=['x'],\n            outputs=['y'],\n        )\n        expect(node, inputs=[x], outputs=[y],\n               name='test_softmax_large_number')\n\n        x = np.abs(np.random.randn(3, 4, 5).astype(np.float32))\n        node = onnx.helper.make_node(\n            'Softmax',\n            inputs=['x'],\n            outputs=['y'],\n            axis=0,\n        )\n        y = softmax_2d(x.reshape(1, 60)).reshape(3, 4, 5)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_softmax_axis_0')\n\n        node = onnx.helper.make_node(\n            'Softmax',\n            inputs=['x'],\n            outputs=['y'],\n            axis=1,\n        )\n        y = softmax_2d(x.reshape(3, 20)).reshape(3, 4, 5)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_softmax_axis_1')\n\n        # default axis is 1\n        node = onnx.helper.make_node(\n            'Softmax',\n            inputs=['x'],\n            outputs=['y'],\n        )\n        expect(node, inputs=[x], outputs=[y],\n               name='test_softmax_default_axis')\n\n        node = onnx.helper.make_node(\n            'Softmax',\n            inputs=['x'],\n            outputs=['y'],\n            axis=2,\n        )\n        y = softmax_2d(x.reshape(12, 5)).reshape(3, 4, 5)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_softmax_axis_2')\n\n        node = onnx.helper.make_node(\n            'Softmax',\n            inputs=['x'],\n            outputs=['y'],\n            axis=-1,\n        )\n        y = softmax_2d(x.reshape(12, 5)).reshape(3, 4, 5)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_softmax_negative_axis')\n"""
onnx/backend/test/case/node/softmaxcrossentropy.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\ndef softmaxcrossentropy(x, target, weight=None, reduction=\'mean\', ignore_index=None, get_log_prob=None):  # type: ignore\n    input_shape = x.shape\n    if len(input_shape) == 1:\n        raise RuntimeError(""Unsupported shape"")\n\n    target_shape = target.shape\n    N = input_shape[0]\n    C = input_shape[1]\n\n    # compute log_softmax\n    max_x = np.max(x, axis=1, keepdims=True)\n    exp_x = np.exp(x - max_x)\n    p = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n    inp = np.log(p)\n    log_prob = None\n    if get_log_prob is True:\n        log_prob = np.copy(inp)\n\n    # initialize the positional weights when required\n    gather_weight = None\n    if weight is not None:\n        # setting mode=\'clip\' to deal with ignore_index > C or < 0 cases.\n        # when the target value is > C or < 0, it doesn\'t matter which value we are\n        # taking in gather_weight, since it will be set to 0 in the following if-block\n        gather_weight = np.take(weight, target, mode=\'clip\')\n        # set `ignore_index`\'s loss weight to 0.\n        # The loss tensor will be multiplied by this weight tensor,\n        # so `ingore_index`\'s loss value will be eliminated.\n        if ignore_index is not None:\n            gather_weight = np.where(target == ignore_index, 0, gather_weight).astype(dtype=np.float32)\n    elif ignore_index is not None:\n        gather_weight = np.where(target == ignore_index, 0, 1).astype(dtype=np.float32)\n\n    # if input is 4-d and above, make it 3-d\n    if len(input_shape) != 3:\n        inp = inp.reshape((N, C, -1))\n        target = target.reshape((N, -1))\n\n    # Get a dimension from the reshaped input.\n    # If the original input shape is [N, C, H, W],\n    # the D here should be H * W because we reshape\n    # [N, C, H, W] to [N, C, H * W].\n    D = inp.shape[2]\n    neg_gather_element_input = np.zeros((N, D), dtype=np.float32)\n    for i in range(N):\n        for d in range(D):\n            if target[i][d] != ignore_index:\n                neg_gather_element_input[i][d] = -inp[i][target[i][d]][d]\n\n    loss = neg_gather_element_input\n\n    # if the input was 4-d or above reshape to the right shape\n    if len(input_shape) != 3:\n        loss = loss.reshape(target_shape)\n\n    # apply the weights when required\n    if gather_weight is not None:\n        loss = gather_weight * loss\n        if reduction == \'mean\':\n            loss = loss.sum() / gather_weight.sum()\n            if get_log_prob is True:\n                return loss, log_prob\n            else:\n                return loss\n\n    if reduction == \'mean\':\n        loss = np.mean(loss)\n    elif reduction == \'sum\':\n        loss = np.sum(loss)\n\n    if get_log_prob is True:\n        return loss, log_prob\n    else:\n        return loss\n\n\nclass SoftmaxCrossEntropyLoss(Base):\n\n    @staticmethod\n    def export_softmaxcrossentropy_none():  # type: () -> None\n        # Define operator attributes.\n        reduction = \'none\'\n\n        # Create operator.\n        node = onnx.helper.make_node(\'SoftmaxCrossEntropyLoss\',\n                                     inputs=[\'x\', \'y\'],\n                                     outputs=[\'z\'],\n                                     reduction=reduction)\n\n        # Define operator inputs.\n        np.random.seed(0)\n        x = np.random.rand(3, 5).astype(np.float32)\n        labels = np.random.randint(0, high=5, size=(3, ))\n\n        # Compute SoftmaxCrossEntropyLoss\n        sce = softmaxcrossentropy(x, labels, reduction=\'none\')\n\n        # Check results\n        expect(node, inputs=[x, labels], outputs=[sce], name=\'test_sce_none\')\n\n    @staticmethod\n    def export_softmaxcrossentropy_none_log_prob():  # type: () -> None\n        # Define operator attributes.\n        reduction = \'none\'\n\n        # Create operator.\n        node = onnx.helper.make_node(\'SoftmaxCrossEntropyLoss\',\n                                     inputs=[\'x\', \'y\'],\n                                     outputs=[\'z\', \'log_prob\'],\n                                     reduction=reduction)\n\n        # Define operator inputs.\n        np.random.seed(0)\n        x = np.random.rand(3, 5).astype(np.float32)\n        labels = np.random.randint(0, high=5, size=(3, ))\n\n        # Compute SoftmaxCrossEntropyLoss\n        loss, log_prob = softmaxcrossentropy(x, labels, reduction=\'none\', get_log_prob=True)\n\n        # Check results\n        expect(node, inputs=[x, labels], outputs=[loss, log_prob], name=\'test_sce_none_log_prob\')\n\n    @staticmethod\n    def export_softmaxcrossentropy_none_weights():  # type: () -> None\n        # Define operator attributes.\n        reduction = \'none\'\n\n        # Create operator.\n        node = onnx.helper.make_node(\'SoftmaxCrossEntropyLoss\',\n                                     inputs=[\'x\', \'y\', \'w\'],\n                                     outputs=[\'z\'],\n                                     reduction=reduction)\n\n        # Define operator inputs.\n        np.random.seed(0)\n        x = np.random.rand(3, 5).astype(np.float32)\n        labels = np.random.randint(0, high=5, size=(3, ))\n        weights = np.array([0.9, 0.7, 0.8, 0.9, 0.9], dtype=np.float32)\n\n        # Compute SoftmaxCrossEntropyLoss\n        sce = softmaxcrossentropy(x, labels, weight=weights, reduction=\'none\')\n\n        # Check results\n        expect(node, inputs=[x, labels, weights], outputs=[sce], name=\'test_sce_none_weights\')\n\n    @staticmethod\n    def export_softmaxcrossentropy_none_weights_log_prob():  # type: () -> None\n        # Define operator attributes.\n        reduction = \'none\'\n\n        # Create operator.\n        node = onnx.helper.make_node(\'SoftmaxCrossEntropyLoss\',\n                                     inputs=[\'x\', \'y\', \'w\'],\n                                     outputs=[\'z\', \'log_prob\'],\n                                     reduction=reduction)\n\n        # Define operator inputs.\n        np.random.seed(0)\n        x = np.random.rand(3, 5).astype(np.float32)\n        labels = np.random.randint(0, high=5, size=(3, ))\n        weights = np.array([0.9, 0.7, 0.8, 0.9, 0.9], dtype=np.float32)\n\n        # Compute SoftmaxCrossEntropyLoss\n        loss, log_prob = softmaxcrossentropy(x, labels, weight=weights, reduction=\'none\', get_log_prob=True)\n\n        # Check results\n        expect(node, inputs=[x, labels, weights], outputs=[loss, log_prob], name=\'test_sce_none_weights_log_prob\')\n\n    @staticmethod\n    def export_softmaxcrossentropy_sum():  # type: () -> None\n        # Define operator attributes.\n        reduction = \'sum\'\n\n        # Create operator.\n        node = onnx.helper.make_node(\'SoftmaxCrossEntropyLoss\',\n                                     inputs=[\'x\', \'y\'],\n                                     outputs=[\'z\'],\n                                     reduction=reduction)\n\n        # Define operator inputs.\n        np.random.seed(0)\n        x = np.random.rand(3, 5).astype(np.float32)\n        labels = np.random.randint(0, high=5, size=(3, ))\n\n        # Compute SoftmaxCrossEntropyLoss\n        sce = softmaxcrossentropy(x, labels, reduction=\'sum\')\n\n        # Check results\n        expect(node, inputs=[x, labels], outputs=[sce], name=\'test_sce_sum\')\n\n    @staticmethod\n    def export_softmaxcrossentropy_sum_log_prob():  # type: () -> None\n        # Define operator attributes.\n        reduction = \'sum\'\n\n        # Create operator.\n        node = onnx.helper.make_node(\'SoftmaxCrossEntropyLoss\',\n                                     inputs=[\'x\', \'y\'],\n                                     outputs=[\'z\', \'log_prob\'],\n                                     reduction=reduction)\n\n        # Define operator inputs.\n        np.random.seed(0)\n        x = np.random.rand(3, 5).astype(np.float32)\n        labels = np.random.randint(0, high=5, size=(3, ))\n\n        # Compute SoftmaxCrossEntropyLoss\n        loss, log_prob = softmaxcrossentropy(x, labels, reduction=\'sum\', get_log_prob=True)\n\n        # Check results\n        expect(node, inputs=[x, labels], outputs=[loss, log_prob], name=\'test_sce_sum_log_prob\')\n\n    @staticmethod\n    def export_softmaxcrossentropy_mean():  # type: () -> None\n        # Define operator attributes.\n        reduction = \'mean\'\n\n        # Create operator.\n        node = onnx.helper.make_node(\'SoftmaxCrossEntropyLoss\',\n                                     inputs=[\'x\', \'y\'],\n                                     outputs=[\'z\'],\n                                     reduction=reduction)\n\n        # Define operator inputs.\n        np.random.seed(0)\n        x = np.random.rand(3, 5).astype(np.float32)\n        labels = np.random.randint(0, high=5, size=(3, ))\n\n        # Compute SoftmaxCrossEntropyLoss\n        sce = softmaxcrossentropy(x, labels)\n\n        # Check results\n        expect(node, inputs=[x, labels], outputs=[sce], name=\'test_sce_mean\')\n\n    @staticmethod\n    def export_softmaxcrossentropy_mean_log_prob():  # type: () -> None\n        # Define operator attributes.\n        reduction = \'mean\'\n\n        # Create operator.\n        node = onnx.helper.make_node(\'SoftmaxCrossEntropyLoss\',\n                                     inputs=[\'x\', \'y\'],\n                                     outputs=[\'z\', \'log_prob\'],\n                                     reduction=reduction)\n\n        # Define operator inputs.\n        np.random.seed(0)\n        x = np.random.rand(3, 5).astype(np.float32)\n        labels = np.random.randint(0, high=5, size=(3, ))\n\n        # Compute SoftmaxCrossEntropyLoss\n        loss, log_prob = softmaxcrossentropy(x, labels, get_log_prob=True)\n\n        # Check results\n        expect(node, inputs=[x, labels], outputs=[loss, log_prob], name=\'test_sce_mean_log_prob\')\n\n    @staticmethod\n    def export_softmaxcrossentropy_mean_3d():  # type: () -> None\n        # Define operator attributes.\n        reduction = \'mean\'\n\n        # Create operator.\n        node = onnx.helper.make_node(\'SoftmaxCrossEntropyLoss\',\n                                     inputs=[\'x\', \'y\'],\n                                     outputs=[\'z\'],\n                                     reduction=reduction)\n\n        # Define operator inputs.\n        np.random.seed(0)\n        x = np.random.rand(3, 5, 2).astype(np.float32)\n        y = np.random.randint(0, high=5, size=(3, 2))\n\n        # Compute SoftmaxCrossEntropyLoss\n        sce = softmaxcrossentropy(x, y)\n\n        # Check results\n        expect(node, inputs=[x, y], outputs=[sce], name=\'test_sce_mean_3d\')\n\n    @staticmethod\n    def export_softmaxcrossentropy_mean_3d_log_prob():  # type: () -> None\n        # Define operator attributes.\n        reduction = \'mean\'\n\n        # Create operator.\n        node = onnx.helper.make_node(\'SoftmaxCrossEntropyLoss\',\n                                     inputs=[\'x\', \'y\'],\n                                     outputs=[\'z\', \'log_prob\'],\n                                     reduction=reduction)\n\n        # Define operator inputs.\n        np.random.seed(0)\n        x = np.random.rand(3, 5, 2).astype(np.float32)\n        y = np.random.randint(0, high=5, size=(3, 2))\n\n        # Compute SoftmaxCrossEntropyLoss\n        loss, log_prob = softmaxcrossentropy(x, y, get_log_prob=True)\n\n        # Check results\n        expect(node, inputs=[x, y], outputs=[loss, log_prob], name=\'test_sce_mean_3d_log_prob\')\n\n    @staticmethod\n    def export_softmaxcrossentropy_mean_weights():  # type: () -> None\n        # Define operator attributes.\n        reduction = \'mean\'\n\n        # Create operator.\n        node = onnx.helper.make_node(\'SoftmaxCrossEntropyLoss\',\n                                     inputs=[\'x\', \'y\', \'w\'],\n                                     outputs=[\'z\'],\n                                     reduction=reduction)\n\n        # Define operator inputs.\n        np.random.seed(0)\n        x = np.random.rand(3, 5).astype(np.float32)\n        labels = np.random.randint(0, high=5, size=(3, ))\n        weights = np.array([0.9, 0.7, 0.8, 0.9, 0.9], dtype=np.float32)\n\n        # Compute SoftmaxCrossEntropyLoss\n        sce = softmaxcrossentropy(x, labels, weight=weights)\n\n        # Check results\n        expect(node, inputs=[x, labels, weights], outputs=[sce], name=\'test_sce_mean_weight\')\n\n    @staticmethod\n    def export_softmaxcrossentropy_mean_weights_log_prob():  # type: () -> None\n        # Define operator attributes.\n        reduction = \'mean\'\n\n        # Create operator.\n        node = onnx.helper.make_node(\'SoftmaxCrossEntropyLoss\',\n                                     inputs=[\'x\', \'y\', \'w\'],\n                                     outputs=[\'z\', \'log_prob\'],\n                                     reduction=reduction)\n\n        # Define operator inputs.\n        np.random.seed(0)\n        x = np.random.rand(3, 5).astype(np.float32)\n        labels = np.random.randint(0, high=5, size=(3, ))\n        weights = np.array([0.9, 0.7, 0.8, 0.9, 0.9], dtype=np.float32)\n\n        # Compute SoftmaxCrossEntropyLoss\n        loss, log_prob = softmaxcrossentropy(x, labels, weight=weights, get_log_prob=True)\n\n        # Check results\n        expect(node, inputs=[x, labels, weights], outputs=[loss, log_prob], name=\'test_sce_mean_weight_log_prob\')\n\n    @staticmethod\n    def export_softmaxcrossentropy_mean_weights_ignore_index():  # type: () -> None\n        # Define operator attributes.\n        reduction = \'mean\'\n        ignore_index = np.int64(0)\n\n        # Create operator.\n        node = onnx.helper.make_node(\'SoftmaxCrossEntropyLoss\',\n                                     inputs=[\'x\', \'y\', \'w\'],\n                                     outputs=[\'z\'],\n                                     reduction=reduction,\n                                     ignore_index=ignore_index)\n\n        # Define operator inputs.\n        np.random.seed(0)\n        x = np.random.rand(3, 5).astype(np.float32)\n        labels = np.random.randint(0, high=5, size=(3, ))\n        labels[0] = np.int64(0)\n        weights = np.array([0.9, 0.7, 0.8, 0.9, 0.9], dtype=np.float32)\n\n        # Compute SoftmaxCrossEntropyLoss\n        sce = softmaxcrossentropy(x, labels, weight=weights, ignore_index=ignore_index)\n\n        # Check results\n        expect(node, inputs=[x, labels, weights], outputs=[sce], name=\'test_sce_mean_weight_ignore_index\')\n\n    @staticmethod\n    def export_softmaxcrossentropy_mean_weights_ignore_index_log_prob():  # type: () -> None\n        # Define operator attributes.\n        reduction = \'mean\'\n        ignore_index = np.int64(0)\n\n        # Create operator.\n        node = onnx.helper.make_node(\'SoftmaxCrossEntropyLoss\',\n                                     inputs=[\'x\', \'y\', \'w\'],\n                                     outputs=[\'z\', \'log_prob\'],\n                                     reduction=reduction,\n                                     ignore_index=ignore_index)\n\n        # Define operator inputs.\n        np.random.seed(0)\n        x = np.random.rand(3, 5).astype(np.float32)\n        labels = np.random.randint(0, high=5, size=(3, ))\n        labels[0] = np.int64(0)\n        weights = np.array([0.9, 0.7, 0.8, 0.9, 0.9], dtype=np.float32)\n\n        # Compute SoftmaxCrossEntropyLoss\n        loss, log_prob = softmaxcrossentropy(x, labels, weight=weights, ignore_index=ignore_index, get_log_prob=True)\n\n        # Check results\n        expect(node, inputs=[x, labels, weights], outputs=[loss, log_prob], name=\'test_sce_mean_weight_ignore_index_log_prob\')\n\n    @staticmethod\n    def export_softmaxcrossentropy_mean_no_weights_ignore_index():  # type: () -> None\n        # Define operator attributes.\n        reduction = \'mean\'\n        ignore_index = np.int64(2)\n\n        # Create operator.\n        node = onnx.helper.make_node(\'SoftmaxCrossEntropyLoss\',\n                                    inputs=[\'x\', \'y\'],\n                                    outputs=[\'z\'],\n                                    reduction=reduction,\n                                    ignore_index=ignore_index)\n\n        # Define operator inputs.\n        np.random.seed(0)\n        x = np.random.rand(3, 5).astype(np.float32)\n        labels = np.random.randint(0, high=5, size=(3, ))\n        labels[0] = np.int64(2)\n\n        # Compute SoftmaxCrossEntropyLoss\n        sce = softmaxcrossentropy(x, labels, ignore_index=ignore_index)\n\n        # Check results\n        expect(node, inputs=[x, labels], outputs=[sce], name=\'test_sce_mean_no_weight_ignore_index\')\n\n    @staticmethod\n    def export_softmaxcrossentropy_mean_no_weights_ignore_index_log_prob():  # type: () -> None\n        # Define operator attributes.\n        reduction = \'mean\'\n        ignore_index = np.int64(2)\n\n        # Create operator.\n        node = onnx.helper.make_node(\'SoftmaxCrossEntropyLoss\',\n                                    inputs=[\'x\', \'y\'],\n                                    outputs=[\'z\', \'log_prob\'],\n                                    reduction=reduction,\n                                    ignore_index=ignore_index)\n\n        # Define operator inputs.\n        np.random.seed(0)\n        x = np.random.rand(3, 5).astype(np.float32)\n        labels = np.random.randint(0, high=5, size=(3, ))\n        labels[0] = np.int64(2)\n\n        # Compute SoftmaxCrossEntropyLoss\n        loss, log_prob = softmaxcrossentropy(x, labels, ignore_index=ignore_index, get_log_prob=True)\n\n        # Check results\n        expect(node, inputs=[x, labels], outputs=[loss, log_prob], name=\'test_sce_mean_no_weight_ignore_index_log_prob\')\n\n    @staticmethod\n    def export_softmaxcrossentropy_mean_weights_ignore_index_3d():  # type: () -> None\n        # Define operator attributes.\n        reduction = \'mean\'\n        ignore_index = np.int64(1)\n\n        # Create operator.\n        node = onnx.helper.make_node(\'SoftmaxCrossEntropyLoss\',\n                                    inputs=[\'x\', \'y\', \'w\'],\n                                    outputs=[\'z\'],\n                                    reduction=reduction,\n                                    ignore_index=ignore_index)\n\n        # Define operator inputs.\n        np.random.seed(0)\n        x = np.random.rand(3, 5, 2).astype(np.float32)\n        labels = np.random.randint(0, high=5, size=(3, 2))\n        labels[0][0] = np.int64(1)\n        weights = np.array([0.2, 0.3, 0.6, 0.1, 0.5], dtype=np.float32)\n\n        # Compute SoftmaxCrossEntropyLoss\n        sce = softmaxcrossentropy(x, labels, weight=weights, ignore_index=ignore_index)\n\n        # Check results\n        expect(node, inputs=[x, labels, weights], outputs=[sce], name=\'test_sce_mean_weight_ignore_index_3d\')\n\n    @staticmethod\n    def export_softmaxcrossentropy_mean_weights_ignore_index_3d_log_prob():  # type: () -> None\n        # Define operator attributes.\n        reduction = \'mean\'\n        ignore_index = np.int64(1)\n\n        # Create operator.\n        node = onnx.helper.make_node(\'SoftmaxCrossEntropyLoss\',\n                                    inputs=[\'x\', \'y\', \'w\'],\n                                    outputs=[\'z\', \'log_prob\'],\n                                    reduction=reduction,\n                                    ignore_index=ignore_index)\n\n        # Define operator inputs.\n        np.random.seed(0)\n        x = np.random.rand(3, 5, 2).astype(np.float32)\n        labels = np.random.randint(0, high=5, size=(3, 2))\n        labels[0][0] = np.int64(1)\n        weights = np.array([0.2, 0.3, 0.6, 0.1, 0.5], dtype=np.float32)\n\n        # Compute SoftmaxCrossEntropyLoss\n        loss, log_prob = softmaxcrossentropy(x, labels, weight=weights, ignore_index=ignore_index, get_log_prob=True)\n\n        # Check results\n        expect(node, inputs=[x, labels, weights], outputs=[loss, log_prob], name=\'test_sce_mean_weight_ignore_index_3d_log_prob\')\n\n    @staticmethod\n    def export_softmaxcrossentropy_mean_no_weights_ignore_index_3d():  # type: () -> None\n        # Define operator attributes.\n        reduction = \'mean\'\n        ignore_index = np.int64(2)\n\n        # Create operator.\n        node = onnx.helper.make_node(\'SoftmaxCrossEntropyLoss\',\n                                    inputs=[\'x\', \'y\'],\n                                    outputs=[\'z\'],\n                                    reduction=reduction,\n                                    ignore_index=ignore_index)\n\n        # Define operator inputs.\n        np.random.seed(0)\n        x = np.random.rand(3, 5, 2).astype(np.float32)\n        labels = np.random.randint(0, high=5, size=(3, 2))\n        labels[0][0] = np.int64(2)\n\n        # Compute SoftmaxCrossEntropyLoss\n        sce = softmaxcrossentropy(x, labels, ignore_index=ignore_index)\n\n        # Check results\n        expect(node, inputs=[x, labels], outputs=[sce], name=\'test_sce_mean_no_weight_ignore_index_3d\')\n\n    @staticmethod\n    def export_softmaxcrossentropy_mean_no_weights_ignore_index_3d_log_prob():  # type: () -> None\n        # Define operator attributes.\n        reduction = \'mean\'\n        ignore_index = np.int64(2)\n\n        # Create operator.\n        node = onnx.helper.make_node(\'SoftmaxCrossEntropyLoss\',\n                                    inputs=[\'x\', \'y\'],\n                                    outputs=[\'z\', \'log_prob\'],\n                                    reduction=reduction,\n                                    ignore_index=ignore_index)\n\n        # Define operator inputs.\n        np.random.seed(0)\n        x = np.random.rand(3, 5, 2).astype(np.float32)\n        labels = np.random.randint(0, high=5, size=(3, 2))\n        labels[0][0] = np.int64(2)\n\n        # Compute SoftmaxCrossEntropyLoss\n        loss, log_prob = softmaxcrossentropy(x, labels, ignore_index=ignore_index, get_log_prob=True)\n\n        # Check results\n        expect(node, inputs=[x, labels], outputs=[loss, log_prob], name=\'test_sce_mean_no_weight_ignore_index_3d_log_prob\')\n\n    @staticmethod\n    def export_softmaxcrossentropy_mean_weights_ignore_index_4d():  # type: () -> None\n        # Define operator attributes.\n        reduction = \'mean\'\n        ignore_index = np.int64(2)\n\n        # Create operator.\n        node = onnx.helper.make_node(\'SoftmaxCrossEntropyLoss\',\n                                    inputs=[\'x\', \'y\', \'w\'],\n                                    outputs=[\'z\'],\n                                    reduction=reduction,\n                                    ignore_index=ignore_index)\n\n        # Define operator inputs.\n        np.random.seed(0)\n        x = np.random.rand(3, 5, 2, 7).astype(np.float32)\n        labels = np.random.randint(0, high=5, size=(3, 2, 7))\n        labels[0][0][0] = np.int64(2)\n        weights = np.array([0.2, 0.3, 0.6, 0.1, 0.5], dtype=np.float32)\n\n        # Compute SoftmaxCrossEntropyLoss\n        sce = softmaxcrossentropy(x, labels, reduction=reduction, weight=weights, ignore_index=ignore_index)\n\n        # Check results\n        expect(node, inputs=[x, labels, weights], outputs=[sce], name=\'test_sce_mean_weight_ignore_index_4d\')\n\n    @staticmethod\n    def export_softmaxcrossentropy_mean_weights_ignore_index_4d_log_prob():  # type: () -> None\n        # Define operator attributes.\n        reduction = \'mean\'\n        ignore_index = np.int64(2)\n\n        # Create operator.\n        node = onnx.helper.make_node(\'SoftmaxCrossEntropyLoss\',\n                                    inputs=[\'x\', \'y\', \'w\'],\n                                    outputs=[\'z\', \'log_prob\'],\n                                    reduction=reduction,\n                                    ignore_index=ignore_index)\n\n        # Define operator inputs.\n        np.random.seed(0)\n        x = np.random.rand(3, 5, 2, 7).astype(np.float32)\n        labels = np.random.randint(0, high=5, size=(3, 2, 7))\n        labels[0][0][0] = np.int64(2)\n        weights = np.array([0.2, 0.3, 0.6, 0.1, 0.5], dtype=np.float32)\n\n        # Compute SoftmaxCrossEntropyLoss\n        loss, log_prob = softmaxcrossentropy(x, labels, reduction=reduction, weight=weights, ignore_index=ignore_index, get_log_prob=True)\n\n        # Check results\n        expect(node, inputs=[x, labels, weights], outputs=[loss, log_prob], name=\'test_sce_mean_weight_ignore_index_4d_log_prob\')\n\n    @staticmethod\n    def export_softmaxcrossentropy_mean_no_weights_ignore_index_4d():  # type: () -> None\n        # Define operator attributes.\n        reduction = \'mean\'\n        ignore_index = np.int64(2)\n\n        # Create operator.\n        node = onnx.helper.make_node(\'SoftmaxCrossEntropyLoss\',\n                                    inputs=[\'x\', \'y\'],\n                                    outputs=[\'z\'],\n                                    reduction=reduction,\n                                    ignore_index=ignore_index)\n\n        # Define operator inputs.\n        np.random.seed(0)\n        x = np.random.rand(3, 5, 2, 7).astype(np.float32)\n        labels = np.random.randint(0, high=5, size=(3, 2, 7))\n        labels[0][0][0] = np.int64(2)\n\n        # Compute SoftmaxCrossEntropyLoss\n        sce = softmaxcrossentropy(x, labels, reduction=reduction, ignore_index=ignore_index)\n\n        # Check results\n        expect(node, inputs=[x, labels], outputs=[sce], name=\'test_sce_mean_no_weight_ignore_index_4d\')\n\n    @staticmethod\n    def export_softmaxcrossentropy_mean_no_weights_ignore_index_4d_log_prob():  # type: () -> None\n        # Define operator attributes.\n        reduction = \'mean\'\n        ignore_index = np.int64(2)\n\n        # Create operator.\n        node = onnx.helper.make_node(\'SoftmaxCrossEntropyLoss\',\n                                    inputs=[\'x\', \'y\'],\n                                    outputs=[\'z\', \'log_prob\'],\n                                    reduction=reduction,\n                                    ignore_index=ignore_index)\n\n        # Define operator inputs.\n        np.random.seed(0)\n        x = np.random.rand(3, 5, 2, 7).astype(np.float32)\n        labels = np.random.randint(0, high=5, size=(3, 2, 7))\n        labels[0][0][0] = np.int64(2)\n\n        # Compute SoftmaxCrossEntropyLoss\n        loss, log_prob = softmaxcrossentropy(x, labels, reduction=reduction, ignore_index=ignore_index, get_log_prob=True)\n\n        # Check results\n        expect(node, inputs=[x, labels], outputs=[loss, log_prob], name=\'test_sce_mean_no_weight_ignore_index_4d_log_prob\')\n\n    @staticmethod\n    def export_input_shape_is_NCd1d2d3d4d5_mean_weight():  # type: () -> None\n        reduction = \'mean\'\n\n        node = onnx.helper.make_node(\'SoftmaxCrossEntropyLoss\',\n                                     inputs=[\'x\', \'y\', \'w\'],\n                                     outputs=[\'z\'],\n                                     reduction=reduction)\n\n        N, C, dim1, dim2, dim3, dim4, dim5 = 3, 5, 6, 6, 5, 3, 4\n        np.random.seed(0)\n        x = np.random.rand(N, C, dim1, dim2, dim3, dim4, dim5).astype(np.float32)\n        labels = np.random.randint(0, high=C, size=(N, dim1, dim2, dim3, dim4, dim5))\n        weight = np.random.rand(C).astype(np.float32)\n\n        sce = softmaxcrossentropy(x,\n                                labels,\n                                weight=weight,\n                                reduction=reduction)\n\n        expect(node, inputs=[x, labels, weight], outputs=[sce], name=\'test_sce_NCd1d2d3d4d5_mean_weight\')\n\n    @staticmethod\n    def export_input_shape_is_NCd1d2d3d4d5_mean_weight_log_prob():  # type: () -> None\n        reduction = \'mean\'\n\n        node = onnx.helper.make_node(\'SoftmaxCrossEntropyLoss\',\n                                     inputs=[\'x\', \'y\', \'w\'],\n                                     outputs=[\'z\', \'log_prob\'],\n                                     reduction=reduction)\n\n        N, C, dim1, dim2, dim3, dim4, dim5 = 3, 5, 6, 6, 5, 3, 4\n        np.random.seed(0)\n        x = np.random.rand(N, C, dim1, dim2, dim3, dim4, dim5).astype(np.float32)\n        labels = np.random.randint(0, high=C, size=(N, dim1, dim2, dim3, dim4, dim5))\n        weight = np.random.rand(C).astype(np.float32)\n\n        loss, log_prob = softmaxcrossentropy(x,\n                                labels,\n                                weight=weight,\n                                reduction=reduction,\n                                get_log_prob=True)\n\n        expect(node, inputs=[x, labels, weight], outputs=[loss, log_prob], name=\'test_sce_NCd1d2d3d4d5_mean_weight_log_prob\')\n\n    @staticmethod\n    def export_input_shape_is_NCd1d2d3d4d5_none_no_weight():  # type: () -> None\n        reduction = \'none\'\n\n        node = onnx.helper.make_node(\'SoftmaxCrossEntropyLoss\',\n                                     inputs=[\'x\', \'y\'],\n                                     outputs=[\'z\'],\n                                     reduction=reduction)\n\n        N, C, dim1, dim2, dim3, dim4, dim5 = 3, 5, 6, 6, 5, 3, 4\n        np.random.seed(0)\n        x = np.random.rand(N, C, dim1, dim2, dim3, dim4, dim5).astype(np.float32)\n        labels = np.random.randint(0, high=C, size=(N, dim1, dim2, dim3, dim4, dim5))\n\n        sce = softmaxcrossentropy(x,\n                                labels,\n                                reduction=reduction)\n\n        expect(node, inputs=[x, labels], outputs=[sce], name=\'test_sce_NCd1d2d3d4d5_none_no_weight\')\n\n    @staticmethod\n    def export_input_shape_is_NCd1d2d3d4d5_none_no_weight_log_prob():  # type: () -> None\n        reduction = \'none\'\n\n        node = onnx.helper.make_node(\'SoftmaxCrossEntropyLoss\',\n                                     inputs=[\'x\', \'y\'],\n                                     outputs=[\'z\', \'log_prob\'],\n                                     reduction=reduction)\n\n        N, C, dim1, dim2, dim3, dim4, dim5 = 3, 5, 6, 6, 5, 3, 4\n        np.random.seed(0)\n        x = np.random.rand(N, C, dim1, dim2, dim3, dim4, dim5).astype(np.float32)\n        labels = np.random.randint(0, high=C, size=(N, dim1, dim2, dim3, dim4, dim5))\n\n        loss, log_prob = softmaxcrossentropy(x,\n                                labels,\n                                reduction=reduction,\n                                get_log_prob=True)\n\n        expect(node, inputs=[x, labels], outputs=[loss, log_prob], name=\'test_sce_NCd1d2d3d4d5_none_no_weight_log_prob\')\n\n    @staticmethod\n    def export_input_shape_is_NCd1_mean_weight_negative_ignore_index():  # type: () -> None\n        reduction = \'mean\'\n        ignore_index = np.int64(-1)\n\n        node = onnx.helper.make_node(\'SoftmaxCrossEntropyLoss\',\n                                     inputs=[\'x\', \'y\', \'w\'],\n                                     outputs=[\'z\'],\n                                     reduction=reduction,\n                                     ignore_index=ignore_index)\n\n        N, C, dim1 = 3, 5, 6\n        np.random.seed(0)\n        x = np.random.rand(N, C, dim1).astype(np.float32)\n        labels = np.random.randint(0, high=C, size=(N, dim1))\n        labels[0][0] = -1\n        weight = np.random.rand(C).astype(np.float32)\n\n        sce = softmaxcrossentropy(x,\n                                  labels,\n                                  weight=weight,\n                                  reduction=reduction,\n                                  ignore_index=ignore_index)\n\n        expect(node, inputs=[x, labels, weight], outputs=[sce], name=\'test_sce_NCd1_mean_weight_negative_ignore_index\')\n\n    @staticmethod\n    def export_input_shape_is_NCd1_mean_weight_negative_ignore_index_log_prob():  # type: () -> None\n        reduction = \'mean\'\n        ignore_index = np.int64(-1)\n\n        node = onnx.helper.make_node(\'SoftmaxCrossEntropyLoss\',\n                                     inputs=[\'x\', \'y\', \'w\'],\n                                     outputs=[\'z\', \'log_prob\'],\n                                     reduction=reduction,\n                                     ignore_index=ignore_index)\n\n        N, C, dim1 = 3, 5, 6\n        np.random.seed(0)\n        x = np.random.rand(N, C, dim1).astype(np.float32)\n        labels = np.random.randint(0, high=C, size=(N, dim1))\n        labels[0][0] = -1\n        weight = np.random.rand(C).astype(np.float32)\n\n        loss, log_prob = softmaxcrossentropy(x,\n                                  labels,\n                                  weight=weight,\n                                  reduction=reduction,\n                                  ignore_index=ignore_index,\n                                  get_log_prob=True)\n\n        expect(node, inputs=[x, labels, weight], outputs=[loss, log_prob], name=\'test_sce_NCd1_mean_weight_negative_ignore_index_log_prob\')\n\n    @staticmethod\n    def export_input_shape_is_NCd1d2d3_none_no_weight_negative_ignore_index():  # type: () -> None\n        reduction = \'none\'\n        ignore_index = np.int64(-5)\n\n        node = onnx.helper.make_node(\'SoftmaxCrossEntropyLoss\',\n                                     inputs=[\'x\', \'y\'],\n                                     outputs=[\'z\'],\n                                     reduction=reduction,\n                                     ignore_index=ignore_index)\n\n        N, C, dim1, dim2, dim3 = 3, 5, 6, 6, 5\n        np.random.seed(0)\n        x = np.random.rand(N, C, dim1, dim2, dim3).astype(np.float32)\n        labels = np.random.randint(0, high=C, size=(N, dim1, dim2, dim3))\n        labels[0][0][0][0] = -5\n\n        sce = softmaxcrossentropy(x,\n                                  labels,\n                                  reduction=reduction,\n                                  ignore_index=ignore_index)\n\n        expect(node, inputs=[x, labels], outputs=[sce], name=\'test_sce_NCd1d2d3_none_no_weight_negative_ignore_index\')\n\n    @staticmethod\n    def export_input_shape_is_NCd1d2d3_none_no_weight_negative_ignore_index_log_prob():  # type: () -> None\n        reduction = \'none\'\n        ignore_index = np.int64(-5)\n\n        node = onnx.helper.make_node(\'SoftmaxCrossEntropyLoss\',\n                                     inputs=[\'x\', \'y\'],\n                                     outputs=[\'z\', \'log_prob\'],\n                                     reduction=reduction,\n                                     ignore_index=ignore_index)\n\n        N, C, dim1, dim2, dim3 = 3, 5, 6, 6, 5\n        np.random.seed(0)\n        x = np.random.rand(N, C, dim1, dim2, dim3).astype(np.float32)\n        labels = np.random.randint(0, high=C, size=(N, dim1, dim2, dim3))\n        labels[0][0][0][0] = -5\n\n        loss, log_prob = softmaxcrossentropy(x,\n                                  labels,\n                                  reduction=reduction,\n                                  ignore_index=ignore_index,\n                                  get_log_prob=True)\n\n        expect(node, inputs=[x, labels], outputs=[loss, log_prob], name=\'test_sce_NCd1d2d3_none_no_weight_negative_ignore_index_log_prob\')\n\n    @staticmethod\n    def export_input_shape_is_NCd1d2d3_sum_weight_high_ignore_index():  # type: () -> None\n        reduction = \'sum\'\n        ignore_index = np.int64(10)\n\n        node = onnx.helper.make_node(\'SoftmaxCrossEntropyLoss\',\n                                     inputs=[\'x\', \'y\', \'w\'],\n                                     outputs=[\'z\'],\n                                     reduction=reduction,\n                                     ignore_index=ignore_index)\n\n        N, C = 3, 5\n        np.random.seed(0)\n        x = np.random.rand(N, C).astype(np.float32)\n        labels = np.random.randint(0, high=C, size=(N))\n        labels[0] = 10\n        weight = np.random.rand(C).astype(np.float32)\n\n        sce = softmaxcrossentropy(x,\n                                  labels,\n                                  weight=weight,\n                                  reduction=reduction,\n                                  ignore_index=ignore_index)\n\n        expect(node, inputs=[x, labels, weight], outputs=[sce], name=\'test_sce_NCd1d2d3_sum_weight_high_ignore_index\')\n\n    @staticmethod\n    def export_input_shape_is_NCd1d2d3_sum_weight_high_ignore_index_log_prob():  # type: () -> None\n        reduction = \'sum\'\n        ignore_index = np.int64(10)\n\n        node = onnx.helper.make_node(\'SoftmaxCrossEntropyLoss\',\n                                     inputs=[\'x\', \'y\', \'w\'],\n                                     outputs=[\'z\', \'log_prob\'],\n                                     reduction=reduction,\n                                     ignore_index=ignore_index)\n\n        N, C = 3, 5\n        np.random.seed(0)\n        x = np.random.rand(N, C).astype(np.float32)\n        labels = np.random.randint(0, high=C, size=(N))\n        labels[0] = 10\n        weight = np.random.rand(C).astype(np.float32)\n\n        loss, log_prob = softmaxcrossentropy(x,\n                                  labels,\n                                  weight=weight,\n                                  reduction=reduction,\n                                  ignore_index=ignore_index,\n                                  get_log_prob=True)\n\n        expect(node, inputs=[x, labels, weight], outputs=[loss, log_prob], name=\'test_sce_NCd1d2d3_sum_weight_high_ignore_index_log_prob\')\n'"
onnx/backend/test/case/node/softplus.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Softplus(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Softplus',\n            inputs=['x'],\n            outputs=['y'],\n        )\n\n        x = np.array([-1, 0, 1]).astype(np.float32)\n        y = np.log(np.exp(x) + 1)  # expected output [0.31326166, 0.69314718, 1.31326163]\n        expect(node, inputs=[x], outputs=[y],\n               name='test_softplus_example')\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.log(np.exp(x) + 1)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_softplus')\n"""
onnx/backend/test/case/node/softsign.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Softsign(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Softsign',\n            inputs=['x'],\n            outputs=['y'],\n        )\n\n        x = np.array([-1, 0, 1]).astype(np.float32)\n        y = np.array([-0.5, 0, 0.5]).astype(np.float32)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_softsign_example')\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = x / (1 + np.abs(x))\n        expect(node, inputs=[x], outputs=[y],\n               name='test_softsign')\n"""
onnx/backend/test/case/node/split.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Split(Base):\n\n    @staticmethod\n    def export_1d():  # type: () -> None\n        input = np.array([1., 2., 3., 4., 5., 6.]).astype(np.float32)\n\n        node = onnx.helper.make_node(\n            'Split',\n            inputs=['input'],\n            outputs=['output_1', 'output_2', 'output_3'],\n            axis=0\n        )\n\n        expected_outputs = [np.array([1., 2.]).astype(np.float32), np.array([3., 4.]).astype(np.float32), np.array([5., 6.]).astype(np.float32)]\n        expect(node, inputs=[input], outputs=[y for y in expected_outputs], name='test_split_equal_parts_1d')\n\n        node = onnx.helper.make_node(\n            'Split',\n            inputs=['input'],\n            outputs=['output_1', 'output_2'],\n            axis=0,\n            split=[2, 4]\n        )\n\n        expected_outputs = [np.array([1., 2.]).astype(np.float32), np.array([3., 4., 5., 6.]).astype(np.float32)]\n        expect(node, inputs=[input], outputs=[y for y in expected_outputs], name='test_split_variable_parts_1d')\n\n    @staticmethod\n    def export_2d():  # type: () -> None\n        input = np.array([[1., 2., 3., 4., 5., 6.],\n                          [7., 8., 9., 10., 11., 12.]]).astype(np.float32)\n\n        node = onnx.helper.make_node(\n            'Split',\n            inputs=['input'],\n            outputs=['output_1', 'output_2'],\n            axis=1\n        )\n\n        expected_outputs = [np.array([[1., 2., 3.], [7., 8., 9.]]).astype(np.float32),\n                            np.array([[4., 5., 6.], [10., 11., 12.]]).astype(np.float32)]\n\n        expect(node, inputs=[input], outputs=[y for y in expected_outputs], name='test_split_equal_parts_2d')\n\n        node = onnx.helper.make_node(\n            'Split',\n            inputs=['input'],\n            outputs=['output_1', 'output_2'],\n            axis=1,\n            split=[2, 4]\n        )\n\n        expected_outputs = [np.array([[1., 2.], [7., 8.]]).astype(np.float32),\n                            np.array([[3., 4., 5., 6.], [9., 10., 11., 12.]]).astype(np.float32)]\n\n        expect(node, inputs=[input], outputs=[y for y in expected_outputs], name='test_split_variable_parts_2d')\n\n    @staticmethod\n    def export_default_values():  # type: () -> None\n        input = np.array([1., 2., 3., 4., 5., 6.]).astype(np.float32)\n\n        # If axis is not specified, split is applied on default axis 0\n        node = onnx.helper.make_node(\n            'Split',\n            inputs=['input'],\n            outputs=['output_1', 'output_2', 'output_3']\n        )\n\n        expected_outputs = [np.array([1., 2.]).astype(np.float32), np.array([3., 4.]).astype(np.float32), np.array([5., 6.]).astype(np.float32)]\n        expect(node, inputs=[input], outputs=[y for y in expected_outputs], name='test_split_equal_parts_default_axis')\n\n        node = onnx.helper.make_node(\n            'Split',\n            inputs=['input'],\n            outputs=['output_1', 'output_2'],\n            split=[2, 4]\n        )\n\n        expected_outputs = [np.array([1., 2.]).astype(np.float32), np.array([3., 4., 5., 6.]).astype(np.float32)]\n        expect(node, inputs=[input], outputs=[y for y in expected_outputs], name='test_split_variable_parts_default_axis')\n\n    @staticmethod\n    def export_zero_size_splits():  # type: () -> None\n        input = np.array([]).astype(np.float32)\n\n        # Split emtpy tensor to tensors of size zero\n        node = onnx.helper.make_node(\n            'Split',\n            inputs=['input'],\n            outputs=['output_1', 'output_2', 'output_3'],\n            split=[0, 0, 0]\n        )\n\n        expected_outputs = [np.array([]).astype(np.float32), np.array([]).astype(np.float32), np.array([]).astype(np.float32)]\n        expect(node, inputs=[input], outputs=[y for y in expected_outputs], name='test_split_zero_size_splits')\n"""
onnx/backend/test/case/node/sqrt.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Sqrt(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Sqrt',\n            inputs=['x'],\n            outputs=['y'],\n        )\n\n        x = np.array([1, 4, 9]).astype(np.float32)\n        y = np.sqrt(x)  # expected output [1., 2., 3.]\n        expect(node, inputs=[x], outputs=[y],\n               name='test_sqrt_example')\n\n        x = np.abs(np.random.randn(3, 4, 5).astype(np.float32))\n        y = np.sqrt(x)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_sqrt')\n"""
onnx/backend/test/case/node/squeeze.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom onnx.backend.test.case.base import Base\nfrom onnx.backend.test.case.node import expect\n\n\nclass Squeeze(Base):\n\n    @staticmethod\n    def export_squeeze():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Squeeze',\n            inputs=['x'],\n            outputs=['y'],\n            axes=[0],\n        )\n        x = np.random.randn(1, 3, 4, 5).astype(np.float32)\n        y = np.squeeze(x, axis=0)\n\n        expect(node, inputs=[x], outputs=[y],\n               name='test_squeeze')\n\n    @staticmethod\n    def export_squeeze_negative_axes():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Squeeze',\n            inputs=['x'],\n            outputs=['y'],\n            axes=[-2],\n        )\n        x = np.random.randn(1, 3, 1, 5).astype(np.float32)\n        y = np.squeeze(x, axis=-2)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_squeeze_negative_axes')\n"""
onnx/backend/test/case/node/stringnormalizer.py,0,"b""# coding: utf-8\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass StringNormalizer(Base):\n\n    @staticmethod\n    def export_nostopwords_nochangecase():    # type: () -> None\n        input = np.array([u'monday', u'tuesday']).astype(np.object)\n        output = input\n\n        # No stopwords. This is a NOOP\n        node = onnx.helper.make_node(\n            'StringNormalizer',\n            inputs=['x'],\n            outputs=['y'],\n            is_case_sensitive=1,\n        )\n        expect(node, inputs=[input], outputs=[output], name='test_strnormalizer_nostopwords_nochangecase')\n\n    @staticmethod\n    def export_monday_casesensintive_nochangecase():    # type: () -> None\n        input = np.array([u'monday', u'tuesday', u'wednesday', u'thursday']).astype(np.object)\n        output = np.array([u'tuesday', u'wednesday', u'thursday']).astype(np.object)\n        stopwords = [u'monday']\n\n        node = onnx.helper.make_node(\n            'StringNormalizer',\n            inputs=['x'],\n            outputs=['y'],\n            is_case_sensitive=1,\n            stopwords=stopwords\n        )\n        expect(node, inputs=[input], outputs=[output], name='test_strnormalizer_export_monday_casesensintive_nochangecase')\n\n    @staticmethod\n    def export_monday_casesensintive_lower():    # type: () -> None\n        input = np.array([u'monday', u'tuesday', u'wednesday', u'thursday']).astype(np.object)\n        output = np.array([u'tuesday', u'wednesday', u'thursday']).astype(np.object)\n        stopwords = [u'monday']\n\n        node = onnx.helper.make_node(\n            'StringNormalizer',\n            inputs=['x'],\n            outputs=['y'],\n            case_change_action='LOWER',\n            is_case_sensitive=1,\n            stopwords=stopwords\n        )\n        expect(node, inputs=[input], outputs=[output], name='test_strnormalizer_export_monday_casesensintive_lower')\n\n    @staticmethod\n    def export_monday_casesensintive_upper():    # type: () -> None\n        input = np.array([u'monday', u'tuesday', u'wednesday', u'thursday']).astype(np.object)\n        output = np.array([u'TUESDAY', u'WEDNESDAY', u'THURSDAY']).astype(np.object)\n        stopwords = [u'monday']\n\n        node = onnx.helper.make_node(\n            'StringNormalizer',\n            inputs=['x'],\n            outputs=['y'],\n            case_change_action='UPPER',\n            is_case_sensitive=1,\n            stopwords=stopwords\n        )\n        expect(node, inputs=[input], outputs=[output], name='test_strnormalizer_export_monday_casesensintive_upper')\n\n    @staticmethod\n    def export_monday_empty_output():    # type: () -> None\n        input = np.array([u'monday', u'monday']).astype(np.object)\n        output = np.array([u'']).astype(np.object)\n        stopwords = [u'monday']\n\n        node = onnx.helper.make_node(\n            'StringNormalizer',\n            inputs=['x'],\n            outputs=['y'],\n            case_change_action='UPPER',\n            is_case_sensitive=1,\n            stopwords=stopwords\n        )\n        expect(node, inputs=[input], outputs=[output], name='test_strnormalizer_export_monday_empty_output')\n\n    @staticmethod\n    def export_monday_insensintive_upper_twodim():    # type: () -> None\n        input = np.array([u'Monday', u'tuesday', u'wednesday', u'Monday', u'tuesday', u'wednesday']).astype(np.object).reshape([1, 6])\n\n        # It does upper case cecedille, accented E\n        # and german umlaut but fails\n        # with german eszett\n        output = np.array([u'TUESDAY', u'WEDNESDAY', u'TUESDAY', u'WEDNESDAY']).astype(np.object).reshape([1, 4])\n        stopwords = [u'monday']\n\n        node = onnx.helper.make_node(\n            'StringNormalizer',\n            inputs=['x'],\n            outputs=['y'],\n            case_change_action='UPPER',\n            stopwords=stopwords\n        )\n        expect(node, inputs=[input], outputs=[output], name='test_strnormalizer_export_monday_insensintive_upper_twodim')\n"""
onnx/backend/test/case/node/sub.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Sub(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Sub',\n            inputs=['x', 'y'],\n            outputs=['z'],\n        )\n\n        x = np.array([1, 2, 3]).astype(np.float32)\n        y = np.array([3, 2, 1]).astype(np.float32)\n        z = x - y  # expected output [-2., 0., 2.]\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_sub_example')\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.random.randn(3, 4, 5).astype(np.float32)\n        z = x - y\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_sub')\n\n    @staticmethod\n    def export_sub_broadcast():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Sub',\n            inputs=['x', 'y'],\n            outputs=['z'],\n        )\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.random.randn(5).astype(np.float32)\n        z = x - y\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_sub_bcast')\n"""
onnx/backend/test/case/node/sum.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Sum(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        data_0 = np.array([3, 0, 2]).astype(np.float32)\n        data_1 = np.array([1, 3, 4]).astype(np.float32)\n        data_2 = np.array([2, 6, 6]).astype(np.float32)\n        result = np.array([6, 9, 12]).astype(np.float32)\n        node = onnx.helper.make_node(\n            'Sum',\n            inputs=['data_0', 'data_1', 'data_2'],\n            outputs=['result'],\n        )\n        expect(node, inputs=[data_0, data_1, data_2], outputs=[result],\n               name='test_sum_example')\n\n        node = onnx.helper.make_node(\n            'Sum',\n            inputs=['data_0'],\n            outputs=['result'],\n        )\n        expect(node, inputs=[data_0], outputs=[data_0],\n               name='test_sum_one_input')\n\n        result = np.add(data_0, data_1)\n        node = onnx.helper.make_node(\n            'Sum',\n            inputs=['data_0', 'data_1'],\n            outputs=['result'],\n        )\n        expect(node, inputs=[data_0, data_1], outputs=[result],\n               name='test_sum_two_inputs')\n"""
onnx/backend/test/case/node/tan.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Tan(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Tan',\n            inputs=['x'],\n            outputs=['y'],\n        )\n\n        x = np.array([-1, 0, 1]).astype(np.float32)\n        y = np.tan(x)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_tan_example')\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.tan(x)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_tan')\n"""
onnx/backend/test/case/node/tanh.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Tanh(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Tanh',\n            inputs=['x'],\n            outputs=['y'],\n        )\n\n        x = np.array([-1, 0, 1]).astype(np.float32)\n        y = np.tanh(x)  # expected output [-0.76159418, 0., 0.76159418]\n        expect(node, inputs=[x], outputs=[y],\n               name='test_tanh_example')\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.tanh(x)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_tanh')\n"""
onnx/backend/test/case/node/tfidfvectorizer.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\nfrom typing import Any, Sequence\n\nimport onnx\nfrom onnx import NodeProto\nfrom ..base import Base\nfrom . import expect\n\n\nclass TfIdfVectorizerHelper():\n    def __init__(self, **params):    # type: (*Any) -> None\n        # Attr names\n        mode = str(\'mode\')\n        min_gram_length = str(\'min_gram_length\')\n        max_gram_length = str(\'max_gram_length\')\n        max_skip_count = str(\'max_skip_count\')\n        ngram_counts = str(\'ngram_counts\')\n        ngram_indexes = str(\'ngram_indexes\')\n        pool_int64s = str(\'pool_int64s\')\n\n        required_attr = [mode, min_gram_length, max_gram_length, max_skip_count,\n                         ngram_counts, ngram_indexes, pool_int64s]\n\n        for i in required_attr:\n            assert i in params, ""Missing attribute: {0}"".format(i)\n\n        self.mode = params[mode]\n        self.min_gram_length = params[min_gram_length]\n        self.max_gram_length = params[max_gram_length]\n        self.max_skip_count = params[max_skip_count]\n        self.ngram_counts = params[ngram_counts]\n        self.ngram_indexes = params[ngram_indexes]\n        self.pool_int64s = params[pool_int64s]\n\n    def make_node_noweights(self):    # type: () -> NodeProto\n        return onnx.helper.make_node(\n            \'TfIdfVectorizer\',\n            inputs=[\'X\'],\n            outputs=[\'Y\'],\n            mode=self.mode,\n            min_gram_length=self.min_gram_length,\n            max_gram_length=self.max_gram_length,\n            max_skip_count=self.max_skip_count,\n            ngram_counts=self.ngram_counts,\n            ngram_indexes=self.ngram_indexes,\n            pool_int64s=self.pool_int64s\n        )\n\n\nclass TfIdfVectorizer(Base):\n\n    @staticmethod\n    def export_tf_only_bigrams_skip0():    # type: () -> None\n        input = np.array([1, 1, 3, 3, 3, 7, 8, 6, 7, 5, 6, 8]).astype(np.int32)\n        output = np.array([0., 0., 0., 0., 1., 1., 1.]).astype(np.float32)\n\n        ngram_counts = np.array([0, 4]).astype(np.int64)\n        ngram_indexes = np.array([0, 1, 2, 3, 4, 5, 6]).astype(np.int64)\n        pool_int64s = np.array([2, 3, 5, 4,    # unigrams\n                                5, 6, 7, 8, 6, 7]).astype(np.int64)    # bigrams\n\n        helper = TfIdfVectorizerHelper(\n            mode=\'TF\',\n            min_gram_length=2,\n            max_gram_length=2,\n            max_skip_count=0,\n            ngram_counts=ngram_counts,\n            ngram_indexes=ngram_indexes,\n            pool_int64s=pool_int64s\n        )\n        node = helper.make_node_noweights()\n        expect(node, inputs=[input], outputs=[output], name=\'test_tfidfvectorizer_tf_only_bigrams_skip0\')\n\n    @staticmethod\n    def export_tf_batch_onlybigrams_skip0():    # type: () -> None\n        input = np.array([[1, 1, 3, 3, 3, 7], [8, 6, 7, 5, 6, 8]]).astype(np.int32)\n        output = np.array([[0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 1., 0., 1.]]).astype(np.float32)\n\n        ngram_counts = np.array([0, 4]).astype(np.int64)\n        ngram_indexes = np.array([0, 1, 2, 3, 4, 5, 6]).astype(np.int64)\n        pool_int64s = np.array([2, 3, 5, 4,    # unigrams\n                                5, 6, 7, 8, 6, 7]).astype(np.int64)   # bigrams\n\n        helper = TfIdfVectorizerHelper(\n            mode=\'TF\',\n            min_gram_length=2,\n            max_gram_length=2,\n            max_skip_count=0,\n            ngram_counts=ngram_counts,\n            ngram_indexes=ngram_indexes,\n            pool_int64s=pool_int64s\n        )\n        node = helper.make_node_noweights()\n        expect(node, inputs=[input], outputs=[output], name=\'test_tfidfvectorizer_tf_batch_onlybigrams_skip0\')\n\n    @staticmethod\n    def export_tf_onlybigrams_levelempty():    # type: () -> None\n        input = np.array([1, 1, 3, 3, 3, 7, 8, 6, 7, 5, 6, 8]).astype(np.int32)\n        output = np.array([1., 1., 1.]).astype(np.float32)\n\n        ngram_counts = np.array([0, 0]).astype(np.int64)\n        ngram_indexes = np.array([0, 1, 2]).astype(np.int64)\n        pool_int64s = np.array([    # unigrams none\n                               5, 6, 7, 8, 6, 7]).astype(np.int64)    # bigrams\n\n        helper = TfIdfVectorizerHelper(\n            mode=\'TF\',\n            min_gram_length=2,\n            max_gram_length=2,\n            max_skip_count=0,\n            ngram_counts=ngram_counts,\n            ngram_indexes=ngram_indexes,\n            pool_int64s=pool_int64s\n        )\n        node = helper.make_node_noweights()\n        expect(node, inputs=[input], outputs=[output], name=\'test_tfidfvectorizer_tf_onlybigrams_levelempty\')\n\n    @staticmethod\n    def export_tf_onlybigrams_skip5():    # type: () -> None\n        input = np.array([1, 1, 3, 3, 3, 7, 8, 6, 7, 5, 6, 8]).astype(np.int32)\n        output = np.array([0., 0., 0., 0., 1., 3., 1.]).astype(np.float32)\n\n        ngram_counts = np.array([0, 4]).astype(np.int64)\n        ngram_indexes = np.array([0, 1, 2, 3, 4, 5, 6]).astype(np.int64)\n        pool_int64s = np.array([2, 3, 5, 4,    # unigrams\n                                5, 6, 7, 8, 6, 7]).astype(np.int64)    # bigrams\n\n        helper = TfIdfVectorizerHelper(\n            mode=\'TF\',\n            min_gram_length=2,\n            max_gram_length=2,\n            max_skip_count=5,\n            ngram_counts=ngram_counts,\n            ngram_indexes=ngram_indexes,\n            pool_int64s=pool_int64s\n        )\n        node = helper.make_node_noweights()\n        expect(node, inputs=[input], outputs=[output], name=\'test_tfidfvectorizer_tf_onlybigrams_skip5\')\n\n    @staticmethod\n    def export_tf_batch_onlybigrams_skip5():    # type: () -> None\n        input = np.array([[1, 1, 3, 3, 3, 7], [8, 6, 7, 5, 6, 8]]).astype(np.int32)\n        output = np.array([[0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 1., 1., 1.]]).astype(np.float32)\n\n        ngram_counts = np.array([0, 4]).astype(np.int64)\n        ngram_indexes = np.array([0, 1, 2, 3, 4, 5, 6]).astype(np.int64)\n        pool_int64s = np.array([2, 3, 5, 4,    # unigrams\n                                5, 6, 7, 8, 6, 7]).astype(np.int64)   # bigrams\n\n        helper = TfIdfVectorizerHelper(\n            mode=\'TF\',\n            min_gram_length=2,\n            max_gram_length=2,\n            max_skip_count=5,\n            ngram_counts=ngram_counts,\n            ngram_indexes=ngram_indexes,\n            pool_int64s=pool_int64s\n        )\n        node = helper.make_node_noweights()\n        expect(node, inputs=[input], outputs=[output], name=\'test_tfidfvectorizer_tf_batch_onlybigrams_skip5\')\n\n    @staticmethod\n    def export_tf_uniandbigrams_skip5():    # type: () -> None\n        input = np.array([1, 1, 3, 3, 3, 7, 8, 6, 7, 5, 6, 8]).astype(np.int32)\n        output = np.array([0., 3., 1., 0., 1., 3., 1.]).astype(np.float32)\n\n        ngram_counts = np.array([0, 4]).astype(np.int64)\n        ngram_indexes = np.array([0, 1, 2, 3, 4, 5, 6]).astype(np.int64)\n        pool_int64s = np.array([2, 3, 5, 4,    # unigrams\n                                5, 6, 7, 8, 6, 7]).astype(np.int64)    # bigrams\n\n        helper = TfIdfVectorizerHelper(\n            mode=\'TF\',\n            min_gram_length=1,\n            max_gram_length=2,\n            max_skip_count=5,\n            ngram_counts=ngram_counts,\n            ngram_indexes=ngram_indexes,\n            pool_int64s=pool_int64s\n        )\n        node = helper.make_node_noweights()\n        expect(node, inputs=[input], outputs=[output], name=\'test_tfidfvectorizer_tf_uniandbigrams_skip5\')\n\n    @staticmethod\n    def export_tf_batch_uniandbigrams_skip5():    # type: () -> None\n        input = np.array([[1, 1, 3, 3, 3, 7], [8, 6, 7, 5, 6, 8]]).astype(np.int32)\n        output = np.array([[0., 3., 0., 0., 0., 0., 0.], [0., 0., 1., 0., 1., 1., 1.]]).astype(np.float32)\n\n        ngram_counts = np.array([0, 4]).astype(np.int64)\n        ngram_indexes = np.array([0, 1, 2, 3, 4, 5, 6]).astype(np.int64)\n        pool_int64s = np.array([2, 3, 5, 4,    # unigrams\n                                5, 6, 7, 8, 6, 7]).astype(np.int64)   # bigrams\n\n        helper = TfIdfVectorizerHelper(\n            mode=\'TF\',\n            min_gram_length=1,\n            max_gram_length=2,\n            max_skip_count=5,\n            ngram_counts=ngram_counts,\n            ngram_indexes=ngram_indexes,\n            pool_int64s=pool_int64s\n        )\n        node = helper.make_node_noweights()\n        expect(node, inputs=[input], outputs=[output], name=\'test_tfidfvectorizer_tf_batch_uniandbigrams_skip5\')\n'"
onnx/backend/test/case/node/thresholdedrelu.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass ThresholdedRelu(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        alpha = 2.0\n        node = onnx.helper.make_node(\n            'ThresholdedRelu',\n            inputs=['x'],\n            outputs=['y'],\n            alpha=alpha\n        )\n\n        x = np.array([-1.5, 0., 1.2, 2.0, 2.2]).astype(np.float32)\n        y = np.clip(x, alpha, np.inf)  # expected output [0., 0., 0., 0., 2.2]\n        y[y == alpha] = 0\n\n        expect(node, inputs=[x], outputs=[y],\n               name='test_thresholdedrelu_example')\n\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.clip(x, alpha, np.inf)\n        y[y == alpha] = 0\n\n        expect(node, inputs=[x], outputs=[y],\n               name='test_thresholdedrelu')\n\n    @staticmethod\n    def export_default():  # type: () -> None\n        default_alpha = 1.0\n        node = onnx.helper.make_node(\n            'ThresholdedRelu',\n            inputs=['x'],\n            outputs=['y']\n        )\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n        y = np.clip(x, default_alpha, np.inf)\n        y[y == default_alpha] = 0\n\n        expect(node, inputs=[x], outputs=[y],\n               name='test_thresholdedrelu_default')\n"""
onnx/backend/test/case/node/tile.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Tile(Base):\n\n    @staticmethod\n    def export_tile():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Tile',\n            inputs=['x', 'y'],\n            outputs=['z']\n        )\n\n        x = np.random.rand(2, 3, 4, 5).astype(np.float32)\n\n        repeats = np.random.randint(low=1, high=10, size=(np.ndim(x),)).astype(np.int64)\n\n        z = np.tile(x, repeats)\n\n        expect(node,\n               inputs=[x, repeats],\n               outputs=[z],\n               name='test_tile')\n\n    @staticmethod\n    def export_tile_precomputed():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Tile',\n            inputs=['x', 'y'],\n            outputs=['z']\n        )\n\n        x = np.array([\n            [0, 1],\n            [2, 3]\n        ], dtype=np.float32)\n\n        repeats = np.array([2, 2], dtype=np.int64)\n\n        z = np.array([\n            [0, 1, 0, 1],\n            [2, 3, 2, 3],\n            [0, 1, 0, 1],\n            [2, 3, 2, 3]\n        ], dtype=np.float32)\n\n        expect(node,\n               inputs=[x, repeats],\n               outputs=[z],\n               name='test_tile_precomputed')\n"""
onnx/backend/test/case/node/topk.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\ndef topk_sorted_implementation(X, k, axis, largest):  # type: ignore\n    sorted_indices = np.argsort(X, axis=axis)\n    sorted_values = np.sort(X, axis=axis)\n    if largest:\n        sorted_indices = np.flip(sorted_indices, axis=axis)\n        sorted_values = np.flip(sorted_values, axis=axis)\n    topk_sorted_indices = np.take(sorted_indices, np.arange(k), axis=axis)\n    topk_sorted_values = np.take(sorted_values, np.arange(k), axis=axis)\n    return topk_sorted_values, topk_sorted_indices\n\n\nclass TopK(Base):\n\n    @staticmethod\n    def export_top_k():  # type: () -> None\n        axis = 1\n        largest = 1\n\n        k = 3\n        node = onnx.helper.make_node(\n            'TopK',\n            inputs=['x', 'k'],\n            outputs=['values', 'indices'],\n            axis=axis\n        )\n        X = np.array([\n            [0, 1, 2, 3],\n            [4, 5, 6, 7],\n            [8, 9, 10, 11],\n        ], dtype=np.float32)\n        K = np.array([k], dtype=np.int64)\n        values_ref, indices_ref = topk_sorted_implementation(X, k, axis, largest)\n\n        #print(values_ref)\n        #[[ 3.  2.  1.]\n        # [ 7.  6.  5.]\n        # [11. 10.  9.]]\n        #print(indices_ref)\n        #[[3 2 1]\n        # [3 2 1]\n        # [3 2 1]]\n\n        expect(node, inputs=[X, K], outputs=[values_ref, indices_ref],\n               name='test_top_k')\n\n    @staticmethod\n    def export_top_k_smallest():  # type: () -> None\n        axis = 1\n        largest = 0\n        sorted = 1\n        k = 3\n\n        node = onnx.helper.make_node(\n            'TopK',\n            inputs=['x', 'k'],\n            outputs=['values', 'indices'],\n            axis=axis,\n            largest=largest,\n            sorted=sorted\n        )\n\n        X = np.array([\n            [0, 1, 2, 3],\n            [4, 5, 6, 7],\n            [11, 10, 9, 8],\n        ], dtype=np.float32)\n        K = np.array([k], dtype=np.int64)\n        values_ref, indices_ref = topk_sorted_implementation(X, k, axis, largest)\n\n        #print(values_ref)\n        #[[ 0.  1.  2.]\n        # [ 4.  5.  6.]\n        # [ 8.  9. 10.]]\n        #print(indices_ref)\n        #[[0 1 2]\n        # [0 1 2]\n        # [3 2 1]]\n\n        expect(node, inputs=[X, K], outputs=[values_ref, indices_ref],\n               name='test_top_k_smallest')\n\n    @staticmethod\n    def export_top_k_negative_axis():  # type: () -> None\n        axis = -1\n        largest = 1\n\n        k = 3\n        node = onnx.helper.make_node(\n            'TopK',\n            inputs=['x', 'k'],\n            outputs=['values', 'indices'],\n            axis=axis\n        )\n        X = np.array([\n            [0, 1, 2, 3],\n            [4, 5, 6, 7],\n            [8, 9, 10, 11],\n        ], dtype=np.float32)\n        K = np.array([k], dtype=np.int64)\n        values_ref, indices_ref = topk_sorted_implementation(X, k, axis, largest)\n\n        # print(values_ref)\n        #[[ 3.  2.  1.]\n        # [ 7.  6.  5.]\n        # [11. 10.  9.]]\n        # print(indices_ref)\n        #[[3 2 1]\n        # [3 2 1]\n        # [3 2 1]]\n\n        expect(node, inputs=[X, K], outputs=[values_ref, indices_ref],\n               name='test_top_k_negative_axis')\n"""
onnx/backend/test/case/node/transpose.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\nimport itertools\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Transpose(Base):\n\n    @staticmethod\n    def export_default():  # type: () -> None\n        shape = (2, 3, 4)\n        data = np.random.random_sample(shape).astype(np.float32)\n\n        node = onnx.helper.make_node(\n            'Transpose',\n            inputs=['data'],\n            outputs=['transposed']\n        )\n\n        transposed = np.transpose(data)\n        expect(node, inputs=[data], outputs=[transposed],\n               name='test_transpose_default')\n\n    @staticmethod\n    def export_all_permutations():  # type: () -> None\n        shape = (2, 3, 4)\n        data = np.random.random_sample(shape).astype(np.float32)\n        permutations = list(itertools.permutations(np.arange(len(shape))))\n\n        for i in range(len(permutations)):\n            node = onnx.helper.make_node(\n                'Transpose',\n                inputs=['data'],\n                outputs=['transposed'],\n                perm=permutations[i]\n            )\n            transposed = np.transpose(data, permutations[i])\n            expect(node, inputs=[data], outputs=[transposed],\n                   name='test_transpose_all_permutations_' + str(i))\n"""
onnx/backend/test/case/node/unique.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Unique(Base):\n\n    @staticmethod\n    def export_sorted_without_axis():  # type: () -> None\n        node_sorted = onnx.helper.make_node(\n            'Unique',\n            inputs=['X'],\n            outputs=['Y', 'indices', 'inverse_indices', 'counts']\n        )\n\n        x = np.array([2.0, 1.0, 1.0, 3.0, 4.0, 3.0], dtype=np.float32)\n        y, indices, inverse_indices, counts = np.unique(x, True, True, True)\n        expect(node_sorted, inputs=[x], outputs=[y, indices, inverse_indices, counts], name='test_unique_sorted_without_axis')\n\n    @staticmethod\n    def export_not_sorted_without_axis():  # type: () -> None\n        node_not_sorted = onnx.helper.make_node(\n            'Unique',\n            inputs=['X'],\n            outputs=['Y', 'indices', 'inverse_indices', 'counts'],\n            sorted=0\n        )\n        # numpy unique does not retain original order (it sorts the output unique values)\n        # https://github.com/numpy/numpy/issues/8621\n        # we need to recover unsorted output and indices\n        x = np.array([2.0, 1.0, 1.0, 3.0, 4.0, 3.0], dtype=np.float32)\n        y, indices, inverse_indices, counts = np.unique(x, True, True, True)\n\n        # prepare index mapping from sorted to unsorted\n        argsorted_indices = np.argsort(indices)\n        inverse_indices_map = {i: si for i, si in zip(argsorted_indices, np.arange(len(argsorted_indices)))}\n\n        indices = indices[argsorted_indices]\n        y = np.take(x, indices, axis=0)\n        inverse_indices = np.asarray([inverse_indices_map[i] for i in inverse_indices], dtype=np.int64)\n        counts = counts[argsorted_indices]\n        # print(y)\n        # [2.0, 1.0, 3.0, 4.0]\n        # print(indices)\n        # [0 1 3 4]\n        # print(inverse_indices)\n        # [0, 1, 1, 2, 3, 2]\n        # print(counts)\n        # [1, 2, 2, 1]\n\n        expect(node_not_sorted, inputs=[x], outputs=[y, indices, inverse_indices, counts], name='test_unique_not_sorted_without_axis')\n\n    @staticmethod\n    def export_sorted_with_axis():  # type: () -> None\n        node_sorted = onnx.helper.make_node(\n            'Unique',\n            inputs=['X'],\n            outputs=['Y', 'indices', 'inverse_indices', 'counts'],\n            sorted=1,\n            axis=0\n        )\n\n        x = np.array([[1, 0, 0], [1, 0, 0], [2, 3, 4]], dtype=np.float32)\n        y, indices, inverse_indices, counts = np.unique(x, True, True, True, axis=0)\n        # print(y)\n        # [[1. 0. 0.]\n        #  [2. 3. 4.]]\n        # print(indices)\n        # [0 2]\n        # print(inverse_indices)\n        # [0 0 1]\n        # print(counts)\n        # [2 1]\n\n        expect(node_sorted, inputs=[x], outputs=[y, indices, inverse_indices, counts], name='test_unique_sorted_with_axis')\n\n    @staticmethod\n    def export_sorted_with_axis_3d():  # type: () -> None\n        node_sorted = onnx.helper.make_node(\n            'Unique',\n            inputs=['X'],\n            outputs=['Y', 'indices', 'inverse_indices', 'counts'],\n            sorted=1,\n            axis=1\n        )\n\n        x = np.array([[[1., 1.], [0., 1.], [2., 1.], [0., 1.]],\n                      [[1., 1.], [0., 1.], [2., 1.], [0., 1.]]], dtype=np.float32)\n        y, indices, inverse_indices, counts = np.unique(x, True, True, True, axis=1)\n        # print(y)\n        # [[[0. 1.]\n        #  [1. 1.]\n        #  [2. 1.]]\n        # [[0. 1.]\n        #  [1. 1.]\n        #  [2. 1.]]]\n        # print(indices)\n        # [1 0 2]\n        # print(inverse_indices)\n        # [1 0 2 0]\n        # print(counts)\n        # [2 1 1]\n        expect(node_sorted, inputs=[x], outputs=[y, indices, inverse_indices, counts], name='test_unique_sorted_with_axis_3d')\n\n    @staticmethod\n    def export_sorted_with_negative_axis():  # type: () -> None\n        node_sorted = onnx.helper.make_node(\n            'Unique',\n            inputs=['X'],\n            outputs=['Y', 'indices', 'inverse_indices', 'counts'],\n            sorted=1,\n            axis=-1\n        )\n\n        x = np.array([[1, 0, 0], [1, 0, 0], [2, 3, 3]], dtype=np.float32)\n        y, indices, inverse_indices, counts = np.unique(x, True, True, True, axis=-1)\n        # print(y)\n        # [[0. 1.]\n        #  [0. 1.]\n        #  [3. 2.]]\n        # print(indices)\n        # [1 0]\n        # print(inverse_indices)\n        # [1 0 0]\n        # print(counts)\n        # [2 1]\n\n        expect(node_sorted, inputs=[x], outputs=[y, indices, inverse_indices, counts], name='test_unique_sorted_with_negative_axis')\n"""
onnx/backend/test/case/node/unsqueeze.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Unsqueeze(Base):\n\n    @staticmethod\n    def export_unsqueeze_one_axis():  # type: () -> None\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n\n        for i in range(x.ndim):\n            node = onnx.helper.make_node(\n                'Unsqueeze',\n                inputs=['x'],\n                outputs=['y'],\n                axes=[i],\n            )\n            y = np.expand_dims(x, axis=i)\n\n            expect(node, inputs=[x], outputs=[y],\n                   name='test_unsqueeze_axis_' + str(i))\n\n    @staticmethod\n    def export_unsqueeze_two_axes():  # type: () -> None\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n\n        node = onnx.helper.make_node(\n            'Unsqueeze',\n            inputs=['x'],\n            outputs=['y'],\n            axes=[1, 4],\n        )\n        y = np.expand_dims(x, axis=1)\n        y = np.expand_dims(y, axis=4)\n\n        expect(node, inputs=[x], outputs=[y],\n                name='test_unsqueeze_two_axes')\n\n    @staticmethod\n    def export_unsqueeze_three_axes():  # type: () -> None\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n\n        node = onnx.helper.make_node(\n            'Unsqueeze',\n            inputs=['x'],\n            outputs=['y'],\n            axes=[2, 4, 5],\n        )\n        y = np.expand_dims(x, axis=2)\n        y = np.expand_dims(y, axis=4)\n        y = np.expand_dims(y, axis=5)\n\n        expect(node, inputs=[x], outputs=[y],\n                name='test_unsqueeze_three_axes')\n\n    @staticmethod\n    def export_unsqueeze_unsorted_axes():  # type: () -> None\n        x = np.random.randn(3, 4, 5).astype(np.float32)\n\n        node = onnx.helper.make_node(\n            'Unsqueeze',\n            inputs=['x'],\n            outputs=['y'],\n            axes=[5, 4, 2],\n        )\n        y = np.expand_dims(x, axis=2)\n        y = np.expand_dims(y, axis=4)\n        y = np.expand_dims(y, axis=5)\n\n        expect(node, inputs=[x], outputs=[y],\n                name='test_unsqueeze_unsorted_axes')\n\n    @staticmethod\n    def export_unsqueeze_negative_axes():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Unsqueeze',\n            inputs=['x'],\n            outputs=['y'],\n            axes=[-2],\n        )\n        x = np.random.randn(1, 3, 1, 5).astype(np.float32)\n        y = np.expand_dims(x, axis=-2)\n        expect(node, inputs=[x], outputs=[y],\n               name='test_unsqueeze_negative_axes')\n"""
onnx/backend/test/case/node/upsample.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\nfrom onnx import helper\n\n\nclass Upsample(Base):\n\n    @staticmethod\n    def export_nearest():  # type: () -> None\n        node = onnx.helper.make_node(\n            \'Upsample\',\n            inputs=[\'X\', \'scales\'],\n            outputs=[\'Y\'],\n            mode=\'nearest\',\n        )\n\n        data = np.array([[[\n            [1, 2],\n            [3, 4],\n        ]]], dtype=np.float32)\n\n        scales = np.array([1.0, 1.0, 2.0, 3.0], dtype=np.float32)\n\n        output = np.array([[[\n            [1, 1, 1, 2, 2, 2],\n            [1, 1, 1, 2, 2, 2],\n            [3, 3, 3, 4, 4, 4],\n            [3, 3, 3, 4, 4, 4],\n        ]]], dtype=np.float32)\n\n        expect(node, inputs=[data, scales], outputs=[output],\n               name=\'test_upsample_nearest\', opset_imports=[helper.make_opsetid("""", 9)])\n'"
onnx/backend/test/case/node/where.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Where(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Where',\n            inputs=['condition', 'x', 'y'],\n            outputs=['z'],\n        )\n\n        condition = np.array([[1, 0], [1, 1]], dtype=np.bool)\n        x = np.array([[1, 2], [3, 4]], dtype=np.float32)\n        y = np.array([[9, 8], [7, 6]], dtype=np.float32)\n        z = np.where(condition, x, y)  # expected output [[1, 8], [3, 4]]\n        expect(node, inputs=[condition, x, y], outputs=[z],\n               name='test_where_example')\n\n    @staticmethod\n    def export_long():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Where',\n            inputs=['condition', 'x', 'y'],\n            outputs=['z'],\n        )\n\n        condition = np.array([[1, 0], [1, 1]], dtype=np.bool)\n        x = np.array([[1, 2], [3, 4]], dtype=np.int64)\n        y = np.array([[9, 8], [7, 6]], dtype=np.int64)\n        z = np.where(condition, x, y)  # expected output [[1, 8], [3, 4]]\n        expect(node, inputs=[condition, x, y], outputs=[z],\n               name='test_where_long_example')\n"""
onnx/backend/test/case/node/xor.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np  # type: ignore\n\nimport onnx\nfrom ..base import Base\nfrom . import expect\n\n\nclass Xor(Base):\n\n    @staticmethod\n    def export():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Xor',\n            inputs=['x', 'y'],\n            outputs=['xor'],\n        )\n\n        # 2d\n        x = (np.random.randn(3, 4) > 0).astype(np.bool)\n        y = (np.random.randn(3, 4) > 0).astype(np.bool)\n        z = np.logical_xor(x, y)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_xor2d')\n\n        # 3d\n        x = (np.random.randn(3, 4, 5) > 0).astype(np.bool)\n        y = (np.random.randn(3, 4, 5) > 0).astype(np.bool)\n        z = np.logical_xor(x, y)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_xor3d')\n\n        # 4d\n        x = (np.random.randn(3, 4, 5, 6) > 0).astype(np.bool)\n        y = (np.random.randn(3, 4, 5, 6) > 0).astype(np.bool)\n        z = np.logical_xor(x, y)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_xor4d')\n\n    @staticmethod\n    def export_xor_broadcast():  # type: () -> None\n        node = onnx.helper.make_node(\n            'Xor',\n            inputs=['x', 'y'],\n            outputs=['xor'],\n        )\n\n        # 3d vs 1d\n        x = (np.random.randn(3, 4, 5) > 0).astype(np.bool)\n        y = (np.random.randn(5) > 0).astype(np.bool)\n        z = np.logical_xor(x, y)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_xor_bcast3v1d')\n\n        # 3d vs 2d\n        x = (np.random.randn(3, 4, 5) > 0).astype(np.bool)\n        y = (np.random.randn(4, 5) > 0).astype(np.bool)\n        z = np.logical_xor(x, y)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_xor_bcast3v2d')\n\n        # 4d vs 2d\n        x = (np.random.randn(3, 4, 5, 6) > 0).astype(np.bool)\n        y = (np.random.randn(5, 6) > 0).astype(np.bool)\n        z = np.logical_xor(x, y)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_xor_bcast4v2d')\n\n        # 4d vs 3d\n        x = (np.random.randn(3, 4, 5, 6) > 0).astype(np.bool)\n        y = (np.random.randn(4, 5, 6) > 0).astype(np.bool)\n        z = np.logical_xor(x, y)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_xor_bcast4v3d')\n\n        # 4d vs 4d\n        x = (np.random.randn(1, 4, 1, 6) > 0).astype(np.bool)\n        y = (np.random.randn(3, 1, 5, 6) > 0).astype(np.bool)\n        z = np.logical_xor(x, y)\n        expect(node, inputs=[x, y], outputs=[z],\n               name='test_xor_bcast4v4d')\n"""
