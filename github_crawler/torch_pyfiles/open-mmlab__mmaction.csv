file_path,api_count,code
setup.py,0,"b'import os\nimport subprocess\nimport time\nfrom setuptools import find_packages, setup\n\n\ndef readme():\n    with open(\'README.md\', encoding=\'utf-8\') as f:\n        content = f.read()\n    return content\n\n\nMAJOR = 0\nMINOR = 1\nPATCH = \'rc0\'\nSUFFIX = \'\'\nSHORT_VERSION = \'{}.{}.{}{}\'.format(MAJOR, MINOR, PATCH, SUFFIX)\n\nversion_file = \'mmaction/version.py\'\n\n\ndef get_git_hash():\n\n    def _minimal_ext_cmd(cmd):\n        # construct minimal environment\n        env = {}\n        for k in [\'SYSTEMROOT\', \'PATH\', \'HOME\']:\n            v = os.environ.get(k)\n            if v is not None:\n                env[k] = v\n        # LANGUAGE is used on win32\n        env[\'LANGUAGE\'] = \'C\'\n        env[\'LANG\'] = \'C\'\n        env[\'LC_ALL\'] = \'C\'\n        out = subprocess.Popen(\n            cmd, stdout=subprocess.PIPE, env=env).communicate()[0]\n        return out\n\n    try:\n        out = _minimal_ext_cmd([\'git\', \'rev-parse\', \'HEAD\'])\n        sha = out.strip().decode(\'ascii\')\n    except OSError:\n        sha = \'unknown\'\n\n    return sha\n\n\ndef get_hash():\n    if os.path.exists(\'.git\'):\n        sha = get_git_hash()[:7]\n    elif os.path.exists(version_file):\n        try:\n            from mmaction.version import __version__\n            sha = __version__.split(\'+\')[-1]\n        except ImportError:\n            raise ImportError(\'Unable to get git version\')\n    else:\n        sha = \'unknown\'\n\n    return sha\n\n\ndef write_version_py():\n    content = """"""# GENERATED VERSION FILE\n# TIME: {}\n\n__version__ = \'{}\'\nshort_version = \'{}\'\n""""""\n    sha = get_hash()\n    VERSION = SHORT_VERSION + \'+\' + sha\n\n    with open(version_file, \'w\') as f:\n        f.write(content.format(time.asctime(), VERSION, SHORT_VERSION))\n\n\ndef get_version():\n    with open(version_file, \'r\') as f:\n        exec(compile(f.read(), version_file, \'exec\'))\n    return locals()[\'__version__\']\n\n\nif __name__ == \'__main__\':\n    write_version_py()\n    setup(\n        name=\'mmaction\',\n        version=get_version(),\n        description=\'Open MMLab Action Toolbox\',\n        long_description=readme(),\n        keywords=\'computer vision, action recognition\',\n        url=\'https://github.com/open-mmlab/mmaction\',\n        packages=find_packages(exclude=(\'configs\', \'tools\', \'demo\')),\n        package_data={\'mmaction.ops\': [\'*/*.so\']},\n        classifiers=[\n            \'Development Status :: 4 - Beta\',\n            \'License :: OSI Approved :: Apache Software License\',\n            \'Operating System :: OS Independent\',\n            \'Programming Language :: Python :: 2\',\n            \'Programming Language :: Python :: 2.7\',\n            \'Programming Language :: Python :: 3\',\n            \'Programming Language :: Python :: 3.4\',\n            \'Programming Language :: Python :: 3.5\',\n            \'Programming Language :: Python :: 3.6\',\n        ],\n        license=\'Apache License 2.0\',\n        setup_requires=[\'pytest-runner\'],\n        tests_require=[\'pytest\'],\n        install_requires=[\n            \'mmcv\', \'numpy\', \'scipy\', \'scikit-learn\', \'terminaltables\', \'lmdb\', \'joblib\'\n        ],\n        zip_safe=False)\n'"
data_tools/build_file_list.py,0,"b'import argparse\nimport os.path as osp\nimport glob\nfrom mmaction.datasets.utils import (parse_directory,\n                                     parse_hmdb51_splits,\n                                     parse_ucf101_splits,\n                                     parse_kinetics_splits,\n                                     build_split_list)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'Build file list\')\n    parser.add_argument(\'dataset\', type=str, choices=[\n                        \'hmdb51\', \'ucf101\', \'kinetics400\'])\n    parser.add_argument(\'frame_path\', type=str,\n                        help=\'root directory for the frames\')\n    parser.add_argument(\'--rgb_prefix\', type=str, default=\'img_\')\n    parser.add_argument(\'--flow_x_prefix\', type=str, default=\'flow_x_\')\n    parser.add_argument(\'--flow_y_prefix\', type=str, default=\'flow_y_\')\n    parser.add_argument(\'--num_split\', type=int, default=3)\n    parser.add_argument(\'--subset\', type=str, default=\'train\',\n                        choices=[\'train\', \'val\', \'test\'])\n    parser.add_argument(\'--level\', type=int, default=2, choices=[1, 2])\n    parser.add_argument(\'--format\', type=str,\n                        default=\'rawframes\', choices=[\'rawframes\', \'videos\'])\n    parser.add_argument(\'--out_list_path\', type=str, default=\'data/\')\n    parser.add_argument(\'--shuffle\', action=\'store_true\', default=False)\n    args = parser.parse_args()\n\n    return args\n\n\ndef main():\n    args = parse_args()\n\n    if args.level == 2:\n        def key_func(x): return \'/\'.join(x.split(\'/\')[-2:])\n    else:\n        def key_func(x): return x.split(\'/\')[-1]\n\n    if args.format == \'rawframes\':\n        frame_info = parse_directory(args.frame_path,\n                                     key_func=key_func,\n                                     rgb_prefix=args.rgb_prefix,\n                                     flow_x_prefix=args.flow_x_prefix,\n                                     flow_y_prefix=args.flow_y_prefix,\n                                     level=args.level)\n    else:\n        if args.level == 1:\n            video_list = glob.glob(osp.join(args.frame_path, \'*\'))\n        elif args.level == 2:\n            video_list = glob.glob(osp.join(args.frame_path, \'*\', \'*\'))\n        frame_info = {osp.relpath(\n            x.split(\'.\')[0], args.frame_path): (x, -1, -1) for x in video_list}\n\n    if args.dataset == \'hmdb51\':\n        split_tp = parse_hmdb51_splits(args.level)\n    if args.dataset == \'ucf101\':\n        split_tp = parse_ucf101_splits(args.level)\n    elif args.dataset == \'kinetics400\':\n        split_tp = parse_kinetics_splits(args.level)\n    assert len(split_tp) == args.num_split\n\n    out_path = args.out_list_path + args.dataset\n    if len(split_tp) > 1:\n        for i, split in enumerate(split_tp):\n            lists = build_split_list(split_tp[i], frame_info,\n                                     shuffle=args.shuffle)\n            filename = \'{}_train_split_{}_{}.txt\'.format(args.dataset,\n                                                         i + 1, args.format)\n            with open(osp.join(out_path, filename), \'w\') as f:\n                f.writelines(lists[0][0])\n            filename = \'{}_val_split_{}_{}.txt\'.format(args.dataset,\n                                                       i + 1, args.format)\n            with open(osp.join(out_path, filename), \'w\') as f:\n                f.writelines(lists[0][1])\n    else:\n        lists = build_split_list(split_tp[0], frame_info,\n                                 shuffle=args.shuffle)\n        filename = \'{}_{}_list_{}.txt\'.format(args.dataset,\n                                              args.subset,\n                                              args.format)\n        if args.subset == \'train\':\n            ind = 0\n        elif args.subset == \'val\':\n            ind = 1\n        elif args.subset == \'test\':\n            ind = 2\n        with open(osp.join(out_path, filename), \'w\') as f:\n            f.writelines(lists[0][ind])\n\n\nif __name__ == ""__main__"":\n    main()\n'"
data_tools/build_rawframes.py,0,"b'import argparse\nimport sys\nimport os\nimport os.path as osp\nimport glob\nfrom pipes import quote\nfrom multiprocessing import Pool, current_process\n\nimport mmcv\n\n\ndef dump_frames(vid_item):\n    full_path, vid_path, vid_id = vid_item\n    vid_name = vid_path.split(\'.\')[0]\n    out_full_path = osp.join(args.out_dir, vid_name)\n    try:\n        os.mkdir(out_full_path)\n    except OSError:\n        pass\n    vr = mmcv.VideoReader(full_path)\n    for i in range(len(vr)):\n        if vr[i] is not None:\n            mmcv.imwrite(\n                vr[i], \'{}/img_{:05d}.jpg\'.format(out_full_path, i + 1))\n        else:\n            print(\'[Warning] length inconsistent!\'\n                  \'Early stop with {} out of {} frames\'.format(i + 1, len(vr)))\n            break\n    print(\'{} done with {} frames\'.format(vid_name, len(vr)))\n    sys.stdout.flush()\n    return True\n\n\ndef run_optical_flow(vid_item, dev_id=0):\n    full_path, vid_path, vid_id = vid_item\n    vid_name = vid_path.split(\'.\')[0]\n    out_full_path = osp.join(args.out_dir, vid_name)\n    try:\n        os.mkdir(out_full_path)\n    except OSError:\n        pass\n\n    current = current_process()\n    dev_id = (int(current._identity[0]) - 1) % args.num_gpu\n    image_path = \'{}/img\'.format(out_full_path)\n    flow_x_path = \'{}/flow_x\'.format(out_full_path)\n    flow_y_path = \'{}/flow_y\'.format(out_full_path)\n\n    cmd = osp.join(args.df_path, \'build/extract_gpu\') + \\\n        \' -f={} -x={} -y={} -i={} -b=20 -t=1 -d={} -s=1 -o={} -w={} -h={}\' \\\n        .format(\n        quote(full_path),\n        quote(flow_x_path), quote(flow_y_path), quote(image_path),\n        dev_id, args.out_format, args.new_width, args.new_height)\n\n    os.system(cmd)\n    print(\'{} {} done\'.format(vid_id, vid_name))\n    sys.stdout.flush()\n    return True\n\n\ndef run_warp_optical_flow(vid_item, dev_id=0):\n    full_path, vid_path, vid_id = vid_item\n    vid_name = vid_path.split(\'.\')[0]\n    out_full_path = osp.join(args.out_dir, vid_name)\n    try:\n        os.mkdir(out_full_path)\n    except OSError:\n        pass\n\n    current = current_process()\n    dev_id = (int(current._identity[0]) - 1) % args.num_gpu\n    flow_x_path = \'{}/flow_x\'.format(out_full_path)\n    flow_y_path = \'{}/flow_y\'.format(out_full_path)\n\n    cmd = osp.join(args.df_path + \'build/extract_warp_gpu\') + \\\n        \' -f={} -x={} -y={} -b=20 -t=1 -d={} -s=1 -o={}\'.format(\n            quote(full_path), quote(flow_x_path), quote(flow_y_path),\n            dev_id, args.out_format)\n\n    os.system(cmd)\n    print(\'warp on {} {} done\'.format(vid_id, vid_name))\n    sys.stdout.flush()\n    return True\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'extract optical flows\')\n    parser.add_argument(\'src_dir\', type=str)\n    parser.add_argument(\'out_dir\', type=str)\n    parser.add_argument(\'--level\', type=int,\n                        choices=[1, 2],\n                        default=2)\n    parser.add_argument(\'--num_worker\', type=int, default=8)\n    parser.add_argument(\'--flow_type\', type=str,\n                        default=None, choices=[None, \'tvl1\', \'warp_tvl1\'])\n    parser.add_argument(\'--df_path\', type=str,\n                        default=\'../../mmaction/third_party/dense_flow\')\n    parser.add_argument(""--out_format"", type=str, default=\'dir\',\n                        choices=[\'dir\', \'zip\'], help=\'output format\')\n    parser.add_argument(""--ext"", type=str, default=\'avi\',\n                        choices=[\'avi\', \'mp4\'], help=\'video file extensions\')\n    parser.add_argument(""--new_width"", type=int, default=0,\n                        help=\'resize image width\')\n    parser.add_argument(""--new_height"", type=int,\n                        default=0, help=\'resize image height\')\n    parser.add_argument(""--num_gpu"", type=int, default=8, help=\'number of GPU\')\n    parser.add_argument(""--resume"", action=\'store_true\', default=False,\n                        help=\'resume optical flow extraction \'\n                        \'instead of overwriting\')\n    args = parser.parse_args()\n\n    return args\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n\n    if not osp.isdir(args.out_dir):\n        print(\'Creating folder: {}\'.format(args.out_dir))\n        os.makedirs(args.out_dir)\n    if args.level == 2:\n        classes = os.listdir(args.src_dir)\n        for classname in classes:\n            new_dir = osp.join(args.out_dir, classname)\n            if not osp.isdir(new_dir):\n                print(\'Creating folder: {}\'.format(new_dir))\n                os.makedirs(new_dir)\n\n    print(\'Reading videos from folder: \', args.src_dir)\n    print(\'Extension of videos: \', args.ext)\n    if args.level == 2:\n        fullpath_list = glob.glob(args.src_dir + \'/*/*.\' + args.ext)\n        done_fullpath_list = glob.glob(args.out_dir + \'/*/*\')\n    elif args.level == 1:\n        fullpath_list = glob.glob(args.src_dir + \'/*.\' + args.ext)\n        done_fullpath_list = glob.glob(args.out_dir + \'/*\')\n    print(\'Total number of videos found: \', len(fullpath_list))\n    if args.resume:\n        fullpath_list = set(fullpath_list).difference(set(done_fullpath_list))\n        fullpath_list = list(fullpath_list)\n        print(\'Resuming. number of videos to be done: \', len(fullpath_list))\n\n    if args.level == 2:\n        vid_list = list(map(lambda p: osp.join(\n            \'/\'.join(p.split(\'/\')[-2:])), fullpath_list))\n    elif args.level == 1:\n        vid_list = list(map(lambda p: p.split(\'/\')[-1], fullpath_list))\n\n    pool = Pool(args.num_worker)\n    if args.flow_type == \'tvl1\':\n        pool.map(run_optical_flow, zip(\n            fullpath_list, vid_list, range(len(vid_list))))\n    elif args.flow_type == \'warp_tvl1\':\n        pool.map(run_warp_optical_flow, zip(\n            fullpath_list, vid_list, range(len(vid_list))))\n    else:\n        pool.map(dump_frames, zip(\n            fullpath_list, vid_list, range(len(vid_list))))\n'"
mmaction/__init__.py,0,"b""from .version import __version__, short_version\n\n__all__ = ['__version__', 'short_version']\n"""
tools/eval_localize_results.py,0,"b'import argparse\n\nimport mmcv\nimport numpy as np\nfrom mmcv.runner import obj_from_dict\n\nfrom mmaction import datasets\nfrom mmaction.core.evaluation.localize_utils import (results2det,\n                                                     perform_regression,\n                                                     temporal_nms,\n                                                     eval_ap_parallel,\n                                                     det2df)\n\nimport pandas as pd\nfrom terminaltables import AsciiTable\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'Test an action detector\')\n    parser.add_argument(\'config\', help=\'test config file path\')\n    parser.add_argument(\'outputs\', nargs=\'+\')\n    parser.add_argument(\'--eval\', type=str,\n                        choices=[\'activitynet\', \'thumos14\'], help=\'eval types\')\n    parser.add_argument(\'--no_regression\', default=False, action=\'store_true\')\n    parser.add_argument(\'--score_weights\', default=None, type=float, nargs=\'+\')\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n\n    cfg = mmcv.Config.fromfile(args.config)\n\n    dataset = obj_from_dict(cfg.data.test, datasets, dict(test_mode=True))\n\n    output_list = []\n    for out in args.outputs:\n        output_list.append(mmcv.load(out))\n\n    if args.score_weights:\n        weights = np.array(args.score_weights) / sum(args.score_weights)\n    else:\n        weights = [1. / len(output_list) for _ in output_list]\n\n    def merge_scores(idx):\n        def merge_part(arrs, index, weights):\n            if arrs[0][index] is not None:\n                return np.sum([a[index] * w for a, w in zip(arrs, weights)],\n                              axis=0)\n            else:\n                return None\n\n        results = [output[idx] for output in output_list]\n        rel_props = output_list[0][idx][0]\n        return (rel_props, merge_part(results, 1, weights),\n                merge_part(results, 2, weights),\n                merge_part(results, 3, weights))\n\n    print(\'Merge detection scores from {} sources\'.format(len(output_list)))\n    outputs = [merge_scores(idx) for idx in range(len(dataset))]\n    print(\'Merge finished\')\n\n    eval_type = args.eval\n    if eval_type:\n        print(\'Starting evaluate {}\'.format(eval_type))\n\n        detections = results2det(\n            dataset, outputs, **cfg.test_cfg.ssn.evaluater)\n\n        if not args.no_regression:\n            print(""Performing location regression"")\n            for cls in range(len(detections)):\n                detections[cls] = {\n                    k: perform_regression(v)\n                    for k, v in detections[cls].items()\n                }\n            print(""Regression finished"")\n\n        print(""Performing NMS"")\n        for cls in range(len(detections)):\n            detections[cls] = {\n                k: temporal_nms(v, cfg.test_cfg.ssn.evaluater.nms)\n                for k, v in detections[cls].items()\n            }\n        print(""NMS finished"")\n\n        if eval_type == \'activitynet\':\n            iou_range = np.arange(0.5, 1.0, 0.05)\n        elif eval_type == \'thumos14\':\n            iou_range = np.arange(0.1, 1.0, .1)\n            # iou_range = [0.5]\n\n        # get gt\n        all_gt = pd.DataFrame(dataset.get_all_gt(), columns=[\n                              \'video-id\', \'cls\', \'t-start\', \'t-end\'])\n        gt_by_cls = [all_gt[all_gt.cls == cls].reset_index(\n            drop=True).drop(\'cls\', 1)\n            for cls in range(len(detections))]\n        plain_detections = [det2df(detections, cls)\n                            for cls in range(len(detections))]\n\n        ap_values = eval_ap_parallel(plain_detections, gt_by_cls, iou_range)\n        map_iou = ap_values.mean(axis=0)\n        print(""Evaluation finished"")\n\n        # display\n        display_title = \'Temporal detection performance ({})\'.format(args.eval)\n        display_data = [[\'IoU thresh\'], [\'mean AP\']]\n\n        for i in range(len(iou_range)):\n            display_data[0].append(\'{:.02f}\'.format(iou_range[i]))\n            display_data[1].append(\'{:.04f}\'.format(map_iou[i]))\n        table = AsciiTable(display_data, display_title)\n        table.justify_columns[-1] = \'right\'\n        table.inner_footing_row_border = True\n        print(table.table)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tools/generate_lmdb.py,0,"b""import argparse\nimport glob\nimport os.path as osp\nfrom mmcv.lmdb.io import create_rawimage_dataset\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description='generate lmdb datasets from raw frames')\n    parser.add_argument(\n        'root_dir', help='root directory to store the raw frames')\n    parser.add_argument(\n        'target_dir', help='target directory to stored the generated lmdbs')\n    parser.add_argument('--image_format', nargs='+',\n                        help='format of the images to be stored',\n                        default=['img*.jpg'])\n    parser.add_argument('--lmdb_tmpl', type=str,\n                        help='template for the lmdb to be generated',\n                        default='{}_img_lmdb')\n    parser.add_argument('--image_tmpl', type=str,\n                        help='template for the lmdb key', default=None)\n    parser.add_argument('--modality', type=str, help='modality',\n                        choices=['RGB', 'Flow'], default='RGB')\n    args = parser.parse_args()\n\n    return args\n\n\ndef main():\n    args = parse_args()\n    video_path_list = glob.glob(osp.join(args.root_dir, '*'))\n    for i, vv in enumerate(video_path_list):\n        if not osp.isdir(vv):\n            continue\n        image_file_list = []\n        for image_format in args.image_format:\n            image_file_list += glob.glob(osp.join(vv, image_format))\n        vid = vv.split('/')[-1]\n        output_path = osp.join(args.target_dir, args.lmdb_tmpl.format(vid))\n        create_rawimage_dataset(output_path, image_file_list,\n                                image_tmpl=args.image_tmpl,\n                                flag='color' if args.modality == 'RGB'\n                                else 'grayscale',\n                                check_valid=True)\n\n\nif __name__ == '__main__':\n    main()\n"""
tools/test_detector.py,2,"b""import argparse\n\nimport torch\nimport mmcv\nfrom mmcv.runner import load_checkpoint, parallel_test, obj_from_dict\nfrom mmcv.parallel import scatter, collate, MMDataParallel\n\nfrom mmaction import datasets\nfrom mmaction.datasets import build_dataloader\nfrom mmaction.models import build_detector, detectors\nfrom mmaction.core.evaluation.ava_utils import results2csv, ava_eval\n\nimport os.path as osp\n\n\ndef single_test(model, data_loader):\n    model.eval()\n    results = []\n    dataset = data_loader.dataset\n    prog_bar = mmcv.ProgressBar(len(dataset))\n    for data in data_loader:\n        with torch.no_grad():\n            result = model(return_loss=False, **data)\n        results.append(result)\n\n        batch_size = data['img_group_0'][0].size(0)\n        for _ in range(batch_size):\n            prog_bar.update()\n    return results\n\n\ndef _data_func(data, device_id):\n    data = scatter(collate([data], samples_per_gpu=1), [device_id])[0]\n    return dict(return_loss=False, rescale=True, **data)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Test an action detector')\n    parser.add_argument('config', help='test config file path')\n    parser.add_argument('checkpoint', help='checkpoint file')\n    parser.add_argument(\n        '--gpus', default=1, type=int, help='GPU number used for testing')\n    parser.add_argument(\n        '--proc_per_gpu',\n        default=1,\n        type=int,\n        help='Number of processes per GPU')\n    parser.add_argument('--out', help='output result file')\n    parser.add_argument('--eval', type=str,\n                        choices=['proposal', 'bbox'], help='eval types')\n    parser.add_argument('--ann_file', type=str,\n                        default='data/ava/ava_val_v2.1.csv')\n    parser.add_argument('--label_file', type=str,\n                        default='data/ava/'\n                        'ava_action_list_v2.1_for_activitynet_2018.pbtxt')\n    parser.add_argument('--exclude_file', type=str,\n                        default='data/ava/'\n                        'ava_val_excluded_timestamps_v2.1.csv')\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n\n    if args.out is not None and not args.out.endswith(('.pkl', '.pickle')):\n        raise ValueError('The output file must be a pkl file.')\n\n    cfg = mmcv.Config.fromfile(args.config)\n    # set cudnn_benchmark\n    if cfg.get('cudnn_benchmark', False):\n        torch.backends.cudnn.benchmark = True\n    cfg.data.test.test_mode = True\n\n    dataset = obj_from_dict(cfg.data.test, datasets, dict(test_mode=True))\n    if args.gpus == 1:\n        model = build_detector(\n            cfg.model, train_cfg=None, test_cfg=cfg.test_cfg)\n        load_checkpoint(model, args.checkpoint, strict=True)\n        model = MMDataParallel(model, device_ids=[0])\n\n        data_loader = build_dataloader(\n            dataset,\n            imgs_per_gpu=1,\n            workers_per_gpu=cfg.data.workers_per_gpu,\n            num_gpus=1,\n            dist=False,\n            shuffle=False)\n        outputs = single_test(model, data_loader)\n    else:\n        model_args = cfg.model.copy()\n        model_args.update(train_cfg=None, test_cfg=cfg.test_cfg)\n        model_type = getattr(detectors, model_args.pop('type'))\n        outputs = parallel_test(\n            model_type,\n            model_args,\n            args.checkpoint,\n            dataset,\n            _data_func,\n            range(args.gpus),\n            workers_per_gpu=args.proc_per_gpu)\n\n    if args.out:\n        print('writing results to {}'.format(args.out))\n        mmcv.dump(outputs, args.out)\n\n    eval_type = args.eval\n    if eval_type:\n        print('Starting evaluate {}'.format(eval_type))\n\n        result_file = osp.join(args.out + '.csv')\n        results2csv(dataset, outputs, result_file)\n\n        ava_eval(result_file, eval_type,\n                 args.label_file, args.ann_file, args.exclude_file)\n\n\nif __name__ == '__main__':\n    main()\n"""
tools/test_localizer.py,2,"b'import argparse\nfrom terminaltables import AsciiTable\nimport numpy as np\nimport pandas as pd\nimport torch\n\nimport mmcv\nfrom mmcv.runner import load_checkpoint, parallel_test, obj_from_dict\nfrom mmcv.parallel import scatter, collate, MMDataParallel\n\nfrom mmaction import datasets\nfrom mmaction.datasets import build_dataloader\nfrom mmaction.models import build_localizer, localizers\nfrom mmaction.models.tenons.segmental_consensuses import parse_stage_config\nfrom mmaction.core.evaluation.localize_utils import (results2det,\n                                                     perform_regression,\n                                                     temporal_nms,\n                                                     eval_ap_parallel,\n                                                     det2df)\n\n\ndef single_test(model, data_loader):\n    model.eval()\n    results = []\n    dataset = data_loader.dataset\n    prog_bar = mmcv.ProgressBar(len(dataset))\n    for data in data_loader:\n        with torch.no_grad():\n            result = model(return_loss=False, **data)\n        results.append(result)\n\n        batch_size = data[\'img_group_0\'].data[0].size(0)\n        for _ in range(batch_size):\n            prog_bar.update()\n    return results\n\n\ndef _data_func(data, device_id):\n    data = scatter(collate([data], samples_per_gpu=1), [device_id])[0]\n    return dict(return_loss=False, rescale=True, **data)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'Test an action detector\')\n    parser.add_argument(\'config\', help=\'test config file path\')\n    parser.add_argument(\'checkpoint\', help=\'checkpoint file\')\n    parser.add_argument(\n        \'--gpus\', default=1, type=int, help=\'GPU number used for testing\')\n    parser.add_argument(\n        \'--proc_per_gpu\',\n        default=1,\n        type=int,\n        help=\'Number of processes per GPU\')\n    parser.add_argument(\'--out\', help=\'output result file\')\n    parser.add_argument(\'--eval\', type=str,\n                        choices=[\'activitynet\', \'thumos14\'], help=\'eval types\')\n    parser.add_argument(\'--no_regression\', default=False, action=\'store_true\')\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n\n    if args.out is not None and not args.out.endswith((\'.pkl\', \'.pickle\')):\n        raise ValueError(\'The output file must be a pkl file.\')\n\n    cfg = mmcv.Config.fromfile(args.config)\n    # set cudnn_benchmark\n    if cfg.get(\'cudnn_benchmark\', False):\n        torch.backends.cudnn.benchmark = True\n    cfg.data.test.test_mode = True\n\n    # reorganize stpp\n    num_classes = (cfg.model.cls_head.num_classes -\n                   1 if cfg.model.cls_head.with_bg\n                   else cfg.model.cls_head.num_classes)\n    stpp_feat_multiplier = 0\n    for stpp_subcfg in cfg.model.segmental_consensus.stpp_cfg:\n        _, mult = parse_stage_config(stpp_subcfg)\n        stpp_feat_multiplier += mult\n    cfg.model.segmental_consensus = dict(\n        type=""STPPReorganized"",\n        standalong_classifier=cfg.model.\n        segmental_consensus.standalong_classifier,\n        feat_dim=num_classes + 1 + num_classes * 3 * stpp_feat_multiplier,\n        act_score_len=num_classes + 1,\n        comp_score_len=num_classes,\n        reg_score_len=num_classes * 2,\n        stpp_cfg=cfg.model.segmental_consensus.stpp_cfg)\n\n    dataset = obj_from_dict(cfg.data.test, datasets, dict(test_mode=True))\n    if args.gpus == 1:\n        model = build_localizer(\n            cfg.model, train_cfg=None, test_cfg=cfg.test_cfg)\n        load_checkpoint(model, args.checkpoint, strict=True)\n        model = MMDataParallel(model, device_ids=[0])\n\n        data_loader = build_dataloader(\n            dataset,\n            imgs_per_gpu=1,\n            workers_per_gpu=cfg.data.workers_per_gpu,\n            num_gpus=1,\n            dist=False,\n            shuffle=False)\n        outputs = single_test(model, data_loader)\n    else:\n        model_args = cfg.model.copy()\n        model_args.update(train_cfg=None, test_cfg=cfg.test_cfg)\n        model_type = getattr(localizers, model_args.pop(\'type\'))\n        outputs = parallel_test(\n            model_type,\n            model_args,\n            args.checkpoint,\n            dataset,\n            _data_func,\n            range(args.gpus),\n            workers_per_gpu=args.proc_per_gpu)\n\n    if args.out:\n        print(\'writing results to {}\'.format(args.out))\n        mmcv.dump(outputs, args.out)\n\n    eval_type = args.eval\n    if eval_type:\n        print(\'Starting evaluate {}\'.format(eval_type))\n\n        detections = results2det(\n            dataset, outputs, **cfg.test_cfg.ssn.evaluater)\n\n        if not args.no_regression:\n            print(""Performing location regression"")\n            for cls in range(len(detections)):\n                detections[cls] = {\n                    k: perform_regression(v)\n                    for k, v in detections[cls].items()\n                }\n            print(""Regression finished"")\n\n        print(""Performing NMS"")\n        for cls in range(len(detections)):\n            detections[cls] = {\n                k: temporal_nms(v, cfg.test_cfg.ssn.evaluater.nms)\n                for k, v in detections[cls].items()}\n        print(""NMS finished"")\n\n        if eval_type == \'activitynet\':\n            iou_range = np.arange(0.5, 1.0, 0.05)\n        elif eval_type == \'thumos14\':\n            iou_range = np.arange(0.1, 1.0, .1)\n\n        # get gt\n        all_gt = pd.DataFrame(dataset.get_all_gt(), columns=[\n                              \'video-id\', \'cls\', \'t-start\', \'t-end\'])\n        gt_by_cls = [all_gt[all_gt.cls == cls].reset_index(\n            drop=True).drop(\'cls\', 1)\n            for cls in range(len(detections))]\n        plain_detections = [det2df(detections, cls)\n                            for cls in range(len(detections))]\n        ap_values = eval_ap_parallel(plain_detections, gt_by_cls, iou_range)\n        map_iou = ap_values.mean(axis=0)\n        print(""Evaluation finished"")\n\n        # display\n        display_title = \'Temporal detection performance ({})\'.format(args.eval)\n        display_data = [[\'IoU thresh\'], [\'mean AP\']]\n\n        for i in range(len(iou_range)):\n            display_data[0].append(\'{:.02f}\'.format(iou_range[i]))\n            display_data[1].append(\'{:.04f}\'.format(map_iou[i]))\n        table = AsciiTable(display_data, display_title)\n        table.justify_columns[-1] = \'right\'\n        table.inner_footing_row_border = True\n        print(table.table)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tools/test_recognizer.py,7,"b'import argparse\n\nimport torch\nimport time\nimport torch.distributed as dist\nimport mmcv\nimport os.path as osp\nimport tempfile\nfrom mmcv.runner import load_checkpoint, obj_from_dict\nfrom mmcv.runner import get_dist_info\nfrom mmcv.parallel.distributed_deprecated import MMDistributedDataParallel\n\nfrom mmaction import datasets\nfrom mmaction.apis import init_dist\nfrom mmaction.datasets import build_dataloader\nfrom mmaction.models import build_recognizer\nfrom mmaction.core.evaluation.accuracy import (softmax, top_k_accuracy,\n                                               mean_class_accuracy)\nimport warnings\nwarnings.filterwarnings(""ignore"", category=UserWarning)\nargs = None\n\n\ndef multi_test(model, data_loader, tmpdir=\'./tmp\'):\n    global args\n    model.eval()\n    results = []\n    rank, world_size = get_dist_info()\n    count = 0\n    data_time_pool = 0\n    proc_time_pool = 0\n    tic = time.time()\n    for i, data in enumerate(data_loader):\n        if i % 100 == 0:\n            print(\'rank {}, data_batch {}\'.format(rank, i))\n        count = count + 1\n        tac = time.time()\n        data_time_pool = data_time_pool + tac - tic\n\n        with torch.no_grad():\n            result = model(return_loss=False, **data)\n        results.append(result)\n\n        toc = time.time()\n        proc_time_pool = proc_time_pool + toc - tac\n\n        tic = toc\n    print(\'rank {}, begin collect results\'.format(rank), flush=True)\n    results = collect_results(results, len(data_loader.dataset), tmpdir)\n    return results\n\n\ndef collect_results(result_part, size, tmpdir=None):\n    global args\n\n    rank, world_size = get_dist_info()\n    if tmpdir is None:\n        MAX_LEN = 512\n        # 32 is whitespace\n        dir_tensor = torch.full((MAX_LEN, ),\n                                32,\n                                dtype=torch.uint8,\n                                device=\'cuda\')\n        if rank == 0:\n            tmpdir = tempfile.mkdtemp()\n            tmpdir = torch.tensor(\n                bytearray(tmpdir.encode()), dtype=torch.uint8, device=\'cuda\')\n            dir_tensor[:len(tmpdir)] = tmpdir\n        dist.broadcast(dir_tensor, 0)\n        tmpdir = dir_tensor.cpu().numpy().tobytes().decode().rstrip()\n    else:\n        tmpdir = osp.join(tmpdir, args.out.split(\'.\')[0])\n        mmcv.mkdir_or_exist(tmpdir)\n    # dump the part result to the dir\n\n    print(\'rank {} begin dump\'.format(rank), flush=True)\n    mmcv.dump(result_part, osp.join(tmpdir, \'part_{}.pkl\'.format(rank)))\n    print(\'rank {} finished dump\'.format(rank), flush=True)\n    dist.barrier()\n    if rank != 0:\n        return None\n    else:\n        part_list = []\n        for i in range(world_size):\n            part_file = osp.join(tmpdir, \'part_{}.pkl\'.format(i))\n            part_list.append(mmcv.load(part_file))\n        ordered_results = []\n        for res in zip(*part_list):\n            ordered_results.extend(list(res))\n        ordered_results = ordered_results[:size]\n        return ordered_results\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'Test an action recognizer\')\n    parser.add_argument(\'config\', help=\'test config file path\')\n    parser.add_argument(\'checkpoint\', help=\'checkpoint file\')\n    parser.add_argument(\n        \'--launcher\',\n        choices=[\'none\', \'pytorch\', \'mpi\', \'slurm\'],\n        default=\'none\',\n        help=\'job launcher\')\n    parser.add_argument(\'--out\', help=\'output result file\', default=\'default.pkl\')\n    parser.add_argument(\'--use_softmax\', action=\'store_true\',\n                        help=\'whether to use softmax score\')\n    # only for TSN3D\n    parser.add_argument(\'--fcn_testing\', action=\'store_true\',\n                        help=\'use fcn testing for 3D convnet\')\n    parser.add_argument(\'--local_rank\', type=int, default=0)\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    global args\n    args = parse_args()\n\n    if args.out is not None and not args.out.endswith((\'.pkl\', \'.pickle\')):\n        raise ValueError(\'The output file must be a pkl file.\')\n\n    cfg = mmcv.Config.fromfile(args.config)\n    # set cudnn_benchmark\n    if cfg.get(\'cudnn_benchmark\', False):\n        torch.backends.cudnn.benchmark = True\n    cfg.data.test.test_mode = True\n\n    # pass arg of fcn testing\n    if args.fcn_testing:\n        cfg.model.update({\'fcn_testing\': True})\n        cfg.model[\'cls_head\'].update({\'fcn_testing\': True})\n\n    # for regular testing\n    if cfg.data.test.oversample == \'three_crop\':\n        cfg.model.spatial_temporal_module.spatial_size = 8\n\n    dataset = obj_from_dict(cfg.data.test, datasets, dict(test_mode=True))\n\n    if args.launcher == \'none\':\n        raise NotImplementedError(""By default, we use distributed testing, so that launcher should be pytorch"")\n    else:\n        distributed = True\n        init_dist(args.launcher, **cfg.dist_params)\n\n    model = build_recognizer(cfg.model, train_cfg=None, test_cfg=cfg.test_cfg)\n    data_loader = build_dataloader(\n        dataset,\n        imgs_per_gpu=1,\n        workers_per_gpu=1,\n        dist=distributed,\n        shuffle=False)\n\n    load_checkpoint(model, args.checkpoint, map_location=\'cpu\')\n    model = MMDistributedDataParallel(model.cuda())\n    outputs = multi_test(model, data_loader)\n\n    rank, _ = get_dist_info()\n    if args.out and rank == 0:\n        print(\'writing results to {}\'.format(args.out))\n        mmcv.dump(outputs, args.out)\n\n        gt_labels = []\n        for i in range(len(dataset)):\n            ann = dataset.get_ann_info(i)\n            gt_labels.append(ann[\'label\'])\n\n        if args.use_softmax:\n            print(""Averaging score over {} clips with softmax"".format(outputs[0].shape[0]))\n            results = [softmax(res, dim=1).mean(axis=0) for res in outputs]\n        else:\n            print(""Averaging score over {} clips without softmax (ie, raw)"".format(outputs[0].shape[0]))\n            results = [res.mean(axis=0) for res in outputs]\n        top1, top5 = top_k_accuracy(results, gt_labels, k=(1, 5))\n        mean_acc = mean_class_accuracy(results, gt_labels)\n        print(""Mean Class Accuracy = {:.02f}"".format(mean_acc * 100))\n        print(""Top-1 Accuracy = {:.02f}"".format(top1 * 100))\n        print(""Top-5 Accuracy = {:.02f}"".format(top5 * 100))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tools/test_recognizer_heavy.py,7,"b'import argparse\nimport time\nimport torch\nimport torch.distributed as dist\nimport mmcv\nimport os\nimport os.path as osp\nfrom mmcv.runner import load_checkpoint, obj_from_dict\nfrom mmcv.runner import get_dist_info\nfrom mmcv.parallel.distributed_deprecated import MMDistributedDataParallel\nimport tempfile\nfrom mmaction import datasets\nfrom mmaction.apis import init_dist\nfrom mmaction.datasets import build_dataloader\nfrom mmaction.models import build_recognizer\nfrom mmaction.core.evaluation.accuracy import softmax, top_k_accuracy\nfrom mmaction.core.evaluation.accuracy import mean_class_accuracy\nimport warnings\nfrom functools import reduce\nwarnings.filterwarnings(""ignore"", category=UserWarning)\n\nargs = None\n\n\ndef multi_test(model, data_loader, tmpdir=\'./tmp\'):\n    global args\n    model.eval()\n    results = []\n    rank, world_size = get_dist_info()\n    count = 0\n    data_time_pool = 0\n    proc_time_pool = 0\n    tic = time.time()\n    for i, data in enumerate(data_loader):\n        if i % 100 == 0:\n            print(\'rank {}, data_batch {}\'.format(rank, i))\n        count = count + 1\n        tac = time.time()\n        data_time_pool = data_time_pool + tac - tic\n\n        img_group = data[\'img_group_0\'].data[0]\n\n        totlen = img_group.size()[1]\n        batch_data = []\n        st = 0\n        scores = []\n\n        assert args.batch_size == -1 or totlen % args.batch_size == 0\n        while st < totlen:\n            if args.batch_size == -1:\n                sz = totlen\n            else:\n                sz = args.batch_size if st + args.batch_size <= totlen else totlen - st\n\n            ed = st + sz\n            batch_data = img_group[:, st: ed].cuda()\n            fake_data = {}\n            fake_data[\'img_group_0\'] = batch_data\n            fake_data[\'num_modalities\'] = data[\'num_modalities\']\n            fake_data[\'img_meta\'] = data[\'img_meta\']\n            with torch.no_grad():\n                score = model(return_loss=False, **fake_data)\n\n            scores.append(score)\n\n            st = ed\n\n        result = reduce(lambda x, y: x+y, scores) / len(scores)\n        results.append(result)\n\n        toc = time.time()\n        proc_time_pool = proc_time_pool + toc - tac\n\n        tic = toc\n    print(\'rank {}, begin collect results\'.format(rank), flush=True)\n    results = collect_results(results, len(data_loader.dataset), tmpdir)\n\n    return results\n\n\ndef collect_results(result_part, size, tmpdir=None):\n    rank, world_size = get_dist_info()\n    if tmpdir is None:\n        MAX_LEN = 512\n        # 32 is whitespace\n        dir_tensor = torch.full((MAX_LEN, ),\n                                32,\n                                dtype=torch.uint8,\n                                device=\'cuda\')\n        if rank == 0:\n            tmpdir = tempfile.mkdtemp()\n            tmpdir = torch.tensor(\n                bytearray(tmpdir.encode()), dtype=torch.uint8, device=\'cuda\')\n            dir_tensor[:len(tmpdir)] = tmpdir\n        dist.broadcast(dir_tensor, 0)\n        tmpdir = dir_tensor.cpu().numpy().tobytes().decode().rstrip()\n    else:\n        tmpdir = osp.join(tmpdir, args.out.split(\'.\')[0])\n        mmcv.mkdir_or_exist(tmpdir)\n    # dump the part result to the dir\n    print(\'rank {} begin dump\'.format(rank), flush=True)\n    mmcv.dump(result_part, osp.join(tmpdir, \'part_{}.pkl\'.format(rank)))\n    print(\'rank {} finished dump\'.format(rank), flush=True)\n    dist.barrier()\n    # collect all parts\n    if rank != 0:\n        return None\n    else:\n        # load results of all parts from tmp dir\n        part_list = []\n        for i in range(world_size):\n            part_file = osp.join(tmpdir, \'part_{}.pkl\'.format(i))\n            part_list.append(mmcv.load(part_file))\n        # sort the results\n        ordered_results = []\n        for res in zip(*part_list):\n            ordered_results.extend(list(res))\n        # the dataloader may pad some samples\n        ordered_results = ordered_results[:size]\n        return ordered_results\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'Test an action recognizer\')\n    parser.add_argument(\'config\', help=\'test config file path\')\n    parser.add_argument(\'checkpoint\', help=\'checkpoint file\')\n    parser.add_argument(\'--testfile\', help=\'checkpoint file\', type=str, default=\'\')\n    parser.add_argument(\n        \'--launcher\',\n        choices=[\'none\', \'pytorch\', \'slurm\', \'mpi\'],\n        default=\'none\',\n        help=\'job launcher\')\n    parser.add_argument(\'--out\', help=\'output result file\')\n    parser.add_argument(\'--use_softmax\', action=\'store_true\', help=\'whether to use softmax score\')\n    parser.add_argument(\'--local_rank\', type=int, default=0)\n    # batch_size should be divided by total crops\n    parser.add_argument(\'--batch_size\', type=int, default=-1)\n    args = parser.parse_args()\n    if \'LOCAL_RANK\' not in os.environ:\n        os.environ[\'LOCAL_RANK\'] = str(args.local_rank)\n    return args\n\n\ndef main():\n    global args\n    args = parse_args()\n\n    if args.out is not None and not args.out.endswith((\'.pkl\', \'.pickle\')):\n        raise ValueError(\'The output file must be a pkl file.\')\n\n    cfg = mmcv.Config.fromfile(args.config)\n    # must use fcn testing\n    cfg.model.update({\'fcn_testing\': True})\n    cfg.model[\'cls_head\'].update({\'fcn_testing\': True})\n\n    # set cudnn_benchmark\n    if cfg.get(\'cudnn_benchmark\', False):\n        torch.backends.cudnn.benchmark = True\n    cfg.data.test.test_mode = True\n\n    if args.testfile != \'\':\n        cfg.data.test.ann_file = args.testfile\n\n    dataset = obj_from_dict(cfg.data.test, datasets, dict(test_mode=True))\n\n    if args.launcher == \'none\':\n        distributed = False\n    else:\n        distributed = True\n        init_dist(args.launcher, **cfg.dist_params)\n\n    assert distributed, ""We only support distributed testing""\n\n    model = build_recognizer(cfg.model, train_cfg=None, test_cfg=cfg.test_cfg)\n    data_loader = build_dataloader(\n        dataset,\n        imgs_per_gpu=1,\n        workers_per_gpu=1,\n        dist=distributed,\n        shuffle=False)\n\n    load_checkpoint(model, args.checkpoint, map_location=\'cpu\')\n\n    model = MMDistributedDataParallel(model.cuda())\n    outputs = multi_test(model, data_loader)\n\n    rank, _ = get_dist_info()\n    if args.out and rank == 0:\n        print(\'writing results to {}\'.format(args.out))\n        mmcv.dump(outputs, args.out)\n\n        gt_labels = []\n        for i in range(len(dataset)):\n            ann = dataset.get_ann_info(i)\n            gt_labels.append(ann[\'label\'])\n\n        if args.use_softmax:\n            print(""Averaging score over {} clips with softmax"".format(outputs[0].shape[0]))\n            results = [softmax(res, dim=1).mean(axis=0) for res in outputs]\n        else:\n            print(""Averaging score over {} clips without softmax (ie, raw)"".format(outputs[0].shape[0]))\n            results = [res.mean(axis=0) for res in outputs]\n        top1, top5 = top_k_accuracy(results, gt_labels, k=(1, 5))\n        mean_acc = mean_class_accuracy(results, gt_labels)\n        print(""Mean Class Accuracy = {:.02f}"".format(mean_acc * 100))\n        print(""Top-1 Accuracy = {:.02f}"".format(top1 * 100))\n        print(""Top-5 Accuracy = {:.02f}"".format(top5 * 100))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tools/train_detector.py,1,"b""from __future__ import division\n\nimport argparse\nfrom mmcv import Config\n\nfrom mmaction import __version__\nfrom mmaction.datasets import get_trimmed_dataset\nfrom mmaction.apis import (train_network, init_dist, get_root_logger,\n                           set_random_seed)\nfrom mmaction.models import build_detector\nimport torch\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Train an action recognizer')\n    parser.add_argument('config', help='train config file path')\n    parser.add_argument('--work_dir', help='the dir to save logs and models')\n    parser.add_argument(\n        '--resume_from', help='the checkpoint file to resume from')\n    parser.add_argument(\n        '--validate',\n        action='store_true',\n        help='whether to evaluate the checkpoint during training')\n    parser.add_argument(\n        '--gpus',\n        type=int,\n        default=1,\n        help='number of gpus to use '\n        '(only applicable to non-distributed training)')\n    parser.add_argument('--seed', type=int, default=None, help='random seed')\n    parser.add_argument(\n        '--launcher',\n        choices=['none', 'pytorch', 'slurm', 'mpi'],\n        default='none',\n        help='job launcher')\n    parser.add_argument('--local_rank', type=int, default=0)\n    args = parser.parse_args()\n\n    return args\n\n\ndef main():\n    args = parse_args()\n\n    cfg = Config.fromfile(args.config)\n    # set cudnn_benchmark\n    if cfg.get('cudnn_benchmark', False):\n        torch.backends.cudnn.benchmark = True\n    # update configs according to CLI args\n    if args.work_dir is not None:\n        cfg.work_dir = args.work_dir\n    if args.resume_from is not None:\n        cfg.resume_from = args.resume_from\n    cfg.gpus = args.gpus\n    if cfg.checkpoint_config is not None:\n        # save mmaction version in checkpoints as meta data\n        cfg.checkpoint_config.meta = dict(\n            mmact_version=__version__, config=cfg.text)\n\n    # init distributed env first, since logger depends on the dist info.\n    if args.launcher == 'none':\n        distributed = False\n    else:\n        distributed = True\n        init_dist(args.launcher, **cfg.dist_params)\n\n    # init logger before other steps\n    logger = get_root_logger(cfg.log_level)\n    logger.info('Distributed training: {}'.format(distributed))\n\n    # set random seeds\n    if args.seed is not None:\n        logger.info('Set random seed to {}'.format(args.seed))\n        set_random_seed(args.seed)\n\n    model = build_detector(\n        cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n\n    train_dataset = get_trimmed_dataset(cfg.data.train)\n    train_network(\n        model,\n        train_dataset,\n        cfg,\n        distributed=distributed,\n        validate=args.validate,\n        logger=logger)\n\n\nif __name__ == '__main__':\n    main()\n"""
tools/train_localizer.py,1,"b""from __future__ import division\n\nimport argparse\nfrom mmcv import Config\n\nfrom mmaction import __version__\nfrom mmaction.datasets import get_trimmed_dataset\nfrom mmaction.apis import (train_network, init_dist, get_root_logger,\n                           set_random_seed)\nfrom mmaction.models import build_localizer\nimport torch\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Train an action localizer')\n    parser.add_argument('config', help='train config file path')\n    parser.add_argument('--work_dir', help='the dir to save logs and models')\n    parser.add_argument(\n        '--resume_from', help='the checkpoint file to resume from')\n    parser.add_argument(\n        '--validate',\n        action='store_true',\n        help='whether to evaluate the checkpoint during training')\n    parser.add_argument(\n        '--gpus',\n        type=int,\n        default=1,\n        help='number of gpus to use '\n        '(only applicable to non-distributed training)')\n    parser.add_argument('--seed', type=int, default=None, help='random seed')\n    parser.add_argument(\n        '--launcher',\n        choices=['none', 'pytorch', 'slurm', 'mpi'],\n        default='none',\n        help='job launcher')\n    parser.add_argument('--local_rank', type=int, default=0)\n    args = parser.parse_args()\n\n    return args\n\n\ndef main():\n    args = parse_args()\n\n    cfg = Config.fromfile(args.config)\n    # set cudnn_benchmark\n    if cfg.get('cudnn_benchmark', False):\n        torch.backends.cudnn.benchmark = True\n    # update configs according to CLI args\n    if args.work_dir is not None:\n        cfg.work_dir = args.work_dir\n    if args.resume_from is not None:\n        cfg.resume_from = args.resume_from\n    cfg.gpus = args.gpus\n    if cfg.checkpoint_config is not None:\n        # save mmaction version in checkpoints as meta data\n        cfg.checkpoint_config.meta = dict(\n            mmact_version=__version__, config=cfg.text)\n\n    # init distributed env first, since logger depends on the dist info.\n    if args.launcher == 'none':\n        distributed = False\n    else:\n        distributed = True\n        init_dist(args.launcher, **cfg.dist_params)\n\n    # init logger before other steps\n    logger = get_root_logger(cfg.log_level)\n    logger.info('Distributed training: {}'.format(distributed))\n\n    # set random seeds\n    if args.seed is not None:\n        logger.info('Set random seed to {}'.format(args.seed))\n        set_random_seed(args.seed)\n\n    model = build_localizer(\n        cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n\n    train_dataset = get_trimmed_dataset(cfg.data.train)\n    train_network(\n        model,\n        train_dataset,\n        cfg,\n        distributed=distributed,\n        validate=args.validate,\n        logger=logger)\n\n\nif __name__ == '__main__':\n    main()\n"""
tools/train_recognizer.py,1,"b""from __future__ import division\n\nimport argparse\nfrom mmcv import Config\n\nfrom mmaction import __version__\nfrom mmaction.datasets import get_trimmed_dataset\nfrom mmaction.apis import (train_network, init_dist, get_root_logger,\n                           set_random_seed)\nfrom mmaction.models import build_recognizer\nimport torch\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Train an action recognizer')\n    parser.add_argument('config', help='train config file path')\n    parser.add_argument('--work_dir', help='the dir to save logs and models')\n    parser.add_argument(\n        '--resume_from', help='the checkpoint file to resume from')\n    parser.add_argument(\n        '--validate',\n        action='store_true',\n        help='whether to evaluate the checkpoint during training')\n    parser.add_argument(\n        '--gpus',\n        type=int,\n        default=1,\n        help='number of gpus to use '\n        '(only applicable to non-distributed training)')\n    parser.add_argument('--seed', type=int, default=None, help='random seed')\n    parser.add_argument(\n        '--launcher',\n        choices=['none', 'pytorch', 'slurm', 'mpi'],\n        default='none',\n        help='job launcher')\n    parser.add_argument('--local_rank', type=int, default=0)\n    args = parser.parse_args()\n\n    return args\n\n\ndef main():\n    args = parse_args()\n\n    cfg = Config.fromfile(args.config)\n    # set cudnn_benchmark\n    if cfg.get('cudnn_benchmark', False):\n        torch.backends.cudnn.benchmark = True\n    # update configs according to CLI args\n    if args.work_dir is not None:\n        cfg.work_dir = args.work_dir\n    if args.resume_from is not None:\n        cfg.resume_from = args.resume_from\n    cfg.gpus = args.gpus\n    if cfg.checkpoint_config is not None:\n        # save mmaction version in checkpoints as meta data\n        cfg.checkpoint_config.meta = dict(\n            mmact_version=__version__, config=cfg.text)\n\n    # init distributed env first, since logger depends on the dist info.\n    if args.launcher == 'none':\n        distributed = False\n    else:\n        distributed = True\n        init_dist(args.launcher, **cfg.dist_params)\n\n    # init logger before other steps\n    logger = get_root_logger(cfg.log_level)\n    logger.info('Distributed training: {}'.format(distributed))\n\n    # set random seeds\n    if args.seed is not None:\n        logger.info('Set random seed to {}'.format(args.seed))\n        set_random_seed(args.seed)\n\n    model = build_recognizer(\n        cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n\n    train_dataset = get_trimmed_dataset(cfg.data.train)\n    train_network(\n        model,\n        train_dataset,\n        cfg,\n        distributed=distributed,\n        validate=args.validate,\n        logger=logger)\n\n\nif __name__ == '__main__':\n    main()\n"""
configs/I3D_RGB/i3d_kinetics400_3d_rgb_r50_c3d_inflate3x1x1_seg1_f32s2.py,0,"b'# model settings\nmodel = dict(\n    type=\'TSN3D\',\n    backbone=dict(\n        type=\'ResNet_I3D\',\n        pretrained=\'modelzoo://resnet50\',\n        depth=50,\n        num_stages=4,\n        out_indices=[3],\n        frozen_stages=-1,\n        inflate_freq=((1,1,1), (1,0,1,0), (1,0,1,0,1,0), (0,1,0)),\n        inflate_style=\'3x1x1\',\n        conv1_kernel_t=5,\n        conv1_stride_t=2,\n        pool1_kernel_t=1,\n        pool1_stride_t=2,\n        bn_eval=False,\n        partial_bn=False,\n        style=\'pytorch\'),\n    spatial_temporal_module=dict(\n        type=\'SimpleSpatialTemporalModule\',\n        spatial_type=\'avg\',\n        temporal_size=4,\n        spatial_size=7),\n    segmental_consensus=dict(\n        type=\'SimpleConsensus\',\n        consensus_type=\'avg\'),\n    cls_head=dict(\n        type=\'ClsHead\',\n        with_avg_pool=False,\n        temporal_feature_size=1,\n        spatial_feature_size=1,\n        dropout_ratio=0.5,\n        in_channels=2048,\n        num_classes=400))\ntrain_cfg = None\ntest_cfg = None\n# dataset settings\ndataset_type = \'RawFramesDataset\'\ndata_root = \'data/kinetics400/rawframes_train/\'\ndata_root_val = \'data/kinetics400/rawframes_val/\'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    videos_per_gpu=8,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=\'data/kinetics400/kinetics400_train_list_rawframes.txt\',\n        img_prefix=data_root,\n        img_norm_cfg=img_norm_cfg,\n        input_format=""NCTHW"",\n        num_segments=1,\n        new_length=32,\n        new_step=2,\n        random_shift=True,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=224,\n        div_255=False,\n        flip_ratio=0.5,\n        resize_keep_ratio=True,\n        oversample=None,\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=True,\n        scales=[1, 0.8],\n        max_distort=0,\n        test_mode=False),\n    val=dict(\n        type=dataset_type,\n        ann_file=\'data/kinetics400/kinetics400_val_list_rawframes.txt\',\n        img_prefix=data_root_val,\n        img_norm_cfg=img_norm_cfg,\n        input_format=""NCTHW"",\n        num_segments=1,\n        new_length=32,\n        new_step=2,\n        random_shift=True,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=224,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample=None,\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=False),\n    test=dict(\n        type=dataset_type,\n        ann_file=\'data/kinetics400/kinetics400_val_list_rawframes.txt\',\n        img_prefix=data_root_val,\n        img_norm_cfg=img_norm_cfg,\n        input_format=""NCTHW"",\n        num_segments=10,\n        new_length=32,\n        new_step=2,\n        random_shift=True,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=256,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample=\'three_crop\',\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type=\'SGD\', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy=\'step\',\n    step=[40, 80])\ncheckpoint_config = dict(interval=1)\n# workflow = [(\'train\', 5), (\'val\', 1)]\nworkflow = [(\'train\', 1)]\n# yapf:disable\nlog_config = dict(\n    interval=20,\n    hooks=[\n        dict(type=\'TextLoggerHook\'),\n        # dict(type=\'TensorboardLoggerHook\')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 100\ndist_params = dict(backend=\'nccl\')\nlog_level = \'INFO\'\nwork_dir = \'./work_dirs/i3d_kinetics_3d_rgb_r50_c3d_inflate3x1x1_seg1_f32s2_b8_g8_imagenet\'\nload_from = None\nresume_from = None\n\n\n\n'"
configs/I3D_RGB/i3d_kinetics400_3d_rgb_r50_c3d_inflate3x1x1_seg1_f32s2_video.py,0,"b'# model settings\nmodel = dict(\n    type=\'TSN3D\',\n    backbone=dict(\n        type=\'ResNet_I3D\',\n        pretrained=\'modelzoo://resnet50\',\n        depth=50,\n        num_stages=4,\n        out_indices=[3],\n        frozen_stages=-1,\n        inflate_freq=((1,1,1), (1,0,1,0), (1,0,1,0,1,0), (0,1,0)),\n        inflate_style=\'3x1x1\',\n        conv1_kernel_t=5,\n        conv1_stride_t=2,\n        pool1_kernel_t=1,\n        pool1_stride_t=2,\n        bn_eval=False,\n        partial_bn=False,\n        style=\'pytorch\'),\n    spatial_temporal_module=dict(\n        type=\'SimpleSpatialTemporalModule\',\n        spatial_type=\'avg\',\n        temporal_size=4,\n        spatial_size=7),\n    segmental_consensus=dict(\n        type=\'SimpleConsensus\',\n        consensus_type=\'avg\'),\n    cls_head=dict(\n        type=\'ClsHead\',\n        with_avg_pool=False,\n        temporal_feature_size=1,\n        spatial_feature_size=1,\n        dropout_ratio=0.5,\n        in_channels=2048,\n        num_classes=400))\ntrain_cfg = None\ntest_cfg = None\n# dataset settings\ndataset_type = \'VideoDataset\'\ndata_root = \'data/kinetics400/videos_train/\'\ndata_root_val = \'data/kinetics400/videos_val/\'\nuse_decord = False\nvideo_ext = \'mp4\'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    videos_per_gpu=8,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=\'data/kinetics400/kinetics400_train_list_videos.txt\',\n        img_prefix=data_root,\n        img_norm_cfg=img_norm_cfg,\n        input_format=""NCTHW"",\n        num_segments=1,\n        new_length=32,\n        new_step=2,\n        random_shift=True,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=224,\n        div_255=False,\n        flip_ratio=0.5,\n        resize_keep_ratio=True,\n        oversample=None,\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=True,\n        scales=[1, 0.8],\n        max_distort=0,\n        test_mode=False,\n        use_decord=use_decord,\n        video_ext=video_ext),\n    val=dict(\n        type=dataset_type,\n        ann_file=\'data/kinetics400/kinetics400_val_list_videos.txt\',\n        img_prefix=data_root_val,\n        img_norm_cfg=img_norm_cfg,\n        input_format=""NCTHW"",\n        num_segments=1,\n        new_length=32,\n        new_step=2,\n        random_shift=True,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=224,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample=None,\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=False,\n        use_decord=use_decord,\n        video_ext=video_ext),\n    test=dict(\n        type=dataset_type,\n        ann_file=\'data/kinetics400/kinetics400_val_list_videos.txt\',\n        img_prefix=data_root_val,\n        img_norm_cfg=img_norm_cfg,\n        input_format=""NCTHW"",\n        num_segments=10,\n        new_length=32,\n        new_step=2,\n        random_shift=True,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=256,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample=\'three_crop\',\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=True,\n        use_decord=use_decord,\n        video_ext=video_ext))\n# optimizer\noptimizer = dict(type=\'SGD\', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy=\'step\',\n    step=[40, 80])\ncheckpoint_config = dict(interval=1)\n# workflow = [(\'train\', 5), (\'val\', 1)]\nworkflow = [(\'train\', 1)]\n# yapf:disable\nlog_config = dict(\n    interval=20,\n    hooks=[\n        dict(type=\'TextLoggerHook\'),\n        # dict(type=\'TensorboardLoggerHook\')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 100\ndist_params = dict(backend=\'nccl\')\nlog_level = \'INFO\'\nwork_dir = \'./work_dirs/i3d_kinetics_3d_rgb_r50_c3d_inflate3x1x1_seg1_f32s2_b8_g8_imagenet\'\nload_from = None\nresume_from = None\n\n\n\n'"
configs/SlowOnly/slowonly_kinetics400_se_rgb_r50_seg1_4x16_finetune.py,0,"b'model = dict(\n    type=\'TSN3D\',\n    backbone=dict(\n        type=\'ResNet_I3D\',\n        pretrained=\'modelzoo://resnet50\',\n        depth=50,\n        num_stages=4,\n        out_indices=[3],\n        frozen_stages=-1,\n        inflate_freq=(0, 0, 1, 1),\n        conv1_kernel_t=1,\n        conv1_stride_t=1,\n        pool1_kernel_t=1,\n        pool1_stride_t=1,\n        inflate_style=\'3x1x1\',\n        bn_eval=False,\n        no_pool2=True,\n        partial_bn=False,\n        style=\'pytorch\'),\n    spatial_temporal_module=dict(\n        type=\'SimpleSpatialTemporalModule\',\n        spatial_type=\'avg\',\n        temporal_size=4,\n        spatial_size=7),\n    segmental_consensus=dict(\n        type=\'SimpleConsensus\',\n        consensus_type=\'avg\'),\n    cls_head=dict(\n        type=\'ClsHead\',\n        with_avg_pool=False,\n        temporal_feature_size=1,\n        spatial_feature_size=1,\n        dropout_ratio=0.5,\n        in_channels=2048,\n        num_classes=400))\ntrain_cfg = None\ntest_cfg = None\n# dataset settings\ndataset_type = \'RawFramesDataset\'\ndata_root = \'data/kinetics400/rawframes_train/\'\ndata_root_val = \'data/kinetics400/rawframes_val/\'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    videos_per_gpu=16,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=\'data/kinetics400/kinetics400_train_list_rawframes.txt\',\n        img_prefix=data_root,\n        img_norm_cfg=img_norm_cfg,\n        input_format=""NCTHW"",\n        num_segments=1,\n        new_length=4,\n        new_step=16,\n        random_shift=True,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=224,\n        div_255=False,\n        flip_ratio=0.5,\n        resize_crop=True,\n        resize_keep_ratio=True,\n        oversample=None,\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=True,\n        test_mode=False),\n    val=dict(\n        type=dataset_type,\n        ann_file=\'data/kinetics400/kinetics400_val_list_rawframes.txt\',\n        img_prefix=data_root_val,\n        img_norm_cfg=img_norm_cfg,\n        input_format=""NCTHW"",\n        num_segments=1,\n        new_length=4,\n        new_step=16,\n        random_shift=False,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=224,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample=None,\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=True),\n   test=dict(\n        type=dataset_type,\n        ann_file=\'data/kinetics400/kinetics400_val_list_rawframes.txt\',\n        img_prefix=data_root_val,\n        img_norm_cfg=img_norm_cfg,\n        input_format=""NCTHW"",\n        num_segments=10,\n        new_length=4,\n        new_step=16,\n        random_shift=False,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=256,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample=\'three_crop\',\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=True))\n\noptimizer = dict(type=\'SGD\', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy=\'step\',\n    step=[90, 130],\n    warmup_ratio=0.01,\n    warmup=\'linear\',\n    warmup_iters=18800)\n\ncheckpoint_config = dict(interval=5)\nworkflow = [(\'train\', 1)]\n# yapf:disable\nlog_config = dict(\n    interval=20,\n    hooks=[\n        dict(type=\'TextLoggerHook\'),\n        dict(type=\'TensorboardLoggerHook\')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 150\ndist_params = dict(backend=\'nccl\')\nlog_level = \'INFO\'\nwork_dir = \'./work_dirs/slowonly_kinetics400_se_rgb_r50_seg1_4x16_finetune_b16_g8\'\nload_from = None\nresume_from = None\n'"
configs/SlowOnly/slowonly_kinetics400_se_rgb_r50_seg1_4x16_scratch.py,0,"b'model = dict(\n    type=\'TSN3D\',\n    backbone=dict(\n        type=\'ResNet_I3D\',\n        pretrained=None,\n        depth=50,\n        num_stages=4,\n        out_indices=[3],\n        frozen_stages=-1,\n        inflate_freq=(0, 0, 1, 1),\n        conv1_kernel_t=1,\n        conv1_stride_t=1,\n        pool1_kernel_t=1,\n        pool1_stride_t=1,\n        inflate_style=\'3x1x1\',\n        bn_eval=False,\n        no_pool2=True,\n        partial_bn=False,\n        style=\'pytorch\'),\n    spatial_temporal_module=dict(\n        type=\'SimpleSpatialTemporalModule\',\n        spatial_type=\'avg\',\n        temporal_size=4,\n        spatial_size=7),\n    segmental_consensus=dict(\n        type=\'SimpleConsensus\',\n        consensus_type=\'avg\'),\n    cls_head=dict(\n        type=\'ClsHead\',\n        with_avg_pool=False,\n        temporal_feature_size=1,\n        spatial_feature_size=1,\n        dropout_ratio=0.5,\n        in_channels=2048,\n        num_classes=400))\ntrain_cfg = None\ntest_cfg = None\n# dataset settings\ndataset_type = \'RawFramesDataset\'\ndata_root = \'data/kinetics400/rawframes_train/\'\ndata_root_val = \'data/kinetics400/rawframes_val/\'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    videos_per_gpu=16,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=\'data/kinetics400/kinetics400_train_list_rawframes.txt\',\n        img_prefix=data_root,\n        img_norm_cfg=img_norm_cfg,\n        input_format=""NCTHW"",\n        num_segments=1,\n        new_length=4,\n        new_step=16,\n        random_shift=True,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=224,\n        div_255=False,\n        flip_ratio=0.5,\n        resize_crop=True,\n        resize_keep_ratio=True,\n        oversample=None,\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=True,\n        test_mode=False),\n    val=dict(\n        type=dataset_type,\n        ann_file=\'data/kinetics400/kinetics400_val_list_rawframes.txt\',\n        img_prefix=data_root_val,\n        img_norm_cfg=img_norm_cfg,\n        input_format=""NCTHW"",\n        num_segments=1,\n        new_length=4,\n        new_step=16,\n        random_shift=False,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=224,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample=None,\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=True),\n   test=dict(\n        type=dataset_type,\n        ann_file=\'data/kinetics400/kinetics400_val_list_rawframes.txt\',\n        img_prefix=data_root_val,\n        img_norm_cfg=img_norm_cfg,\n        input_format=""NCTHW"",\n        num_segments=10,\n        new_length=4,\n        new_step=16,\n        random_shift=False,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=256,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample=\'three_crop\',\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=True))\n\noptimizer = dict(type=\'SGD\', lr=0.2, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy=\'cosine\',\n    warmup_ratio=0.01,\n    warmup=\'linear\',\n    warmup_iters=63920)\n\ncheckpoint_config = dict(interval=5)\nworkflow = [(\'train\', 1)]\n# yapf:disable\nlog_config = dict(\n    interval=20,\n    hooks=[\n        dict(type=\'TextLoggerHook\'),\n        dict(type=\'TensorboardLoggerHook\')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 256\ndist_params = dict(backend=\'nccl\')\nlog_level = \'INFO\'\nwork_dir = \'./work_dirs/slowonly_kinetics400_se_rgb_r50_seg1_4x16_scratch_b16_g8\'\nload_from = None\nresume_from = None\n'"
configs/SlowOnly/slowonly_kinetics400_se_rgb_r50_seg1_8x8_finetune.py,0,"b'\nmodel = dict(\n    type=\'TSN3D\',\n    backbone=dict(\n        type=\'ResNet_I3D\',\n        pretrained=\'modelzoo://resnet50\',\n        depth=50,\n        num_stages=4,\n        out_indices=[3],\n        frozen_stages=-1,\n        inflate_freq=(0, 0, 1, 1),\n        conv1_kernel_t=1,\n        conv1_stride_t=1,\n        pool1_kernel_t=1,\n        pool1_stride_t=1,\n        inflate_style=\'3x1x1\',\n        bn_eval=False,\n        no_pool2=True,\n        partial_bn=False,\n        style=\'pytorch\'),\n    spatial_temporal_module=dict(\n        type=\'SimpleSpatialTemporalModule\',\n        spatial_type=\'avg\',\n        temporal_size=8,\n        spatial_size=7),\n    segmental_consensus=dict(\n        type=\'SimpleConsensus\',\n        consensus_type=\'avg\'),\n    cls_head=dict(\n        type=\'ClsHead\',\n        with_avg_pool=False,\n        temporal_feature_size=1,\n        spatial_feature_size=1,\n        dropout_ratio=0.5,\n        in_channels=2048,\n        num_classes=400))\ntrain_cfg = None\ntest_cfg = None\n# dataset settings\ndataset_type = \'RawFramesDataset\'\ndata_root = \'data/kinetics400/rawframes_train/\'\ndata_root_val = \'data/kinetics400/rawframes_val/\'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    videos_per_gpu=8,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=\'data/kinetics400/kinetics400_train_list_rawframes.txt\',\n        img_prefix=data_root,\n        img_norm_cfg=img_norm_cfg,\n        input_format=""NCTHW"",\n        num_segments=1,\n        new_length=8,\n        new_step=8,\n        random_shift=True,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=224,\n        div_255=False,\n        flip_ratio=0.5,\n        resize_crop=True,\n        resize_keep_ratio=True,\n        oversample=None,\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=True,\n        test_mode=False),\n    val=dict(\n        type=dataset_type,\n        ann_file=\'data/kinetics400/kinetics400_val_list_rawframes.txt\',\n        img_prefix=data_root_val,\n        img_norm_cfg=img_norm_cfg,\n        input_format=""NCTHW"",\n        num_segments=1,\n        new_length=8,\n        new_step=8,\n        random_shift=False,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=224,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample=None,\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=True),\n   test=dict(\n        type=dataset_type,\n        ann_file=\'data/kinetics400/kinetics400_val_list_rawframes.txt\',\n        img_prefix=data_root_val,\n        img_norm_cfg=img_norm_cfg,\n        input_format=""NCTHW"",\n        num_segments=10,\n        new_length=8,\n        new_step=8,\n        random_shift=True,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=256,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample=\'three_crop\',\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=True))\n\noptimizer = dict(type=\'SGD\', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy=\'step\',\n    step=[90, 130],\n    warmup_ratio=0.01,\n    warmup=\'linear\',\n    warmup_iters=37600)\n\ncheckpoint_config = dict(interval=5)\nworkflow = [(\'train\', 1)]\n# yapf:disable\nlog_config = dict(\n    interval=20,\n    hooks=[\n        dict(type=\'TextLoggerHook\'),\n        dict(type=\'TensorboardLoggerHook\')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 150\ndist_params = dict(backend=\'nccl\')\nlog_level = \'INFO\'\nwork_dir = \'./work_dirs/slowonly_kinetics400_se_rgb_r50_seg1_8x8_finetune_b8_g8\'\nload_from = None\nresume_from = None\n'"
configs/SlowOnly/slowonly_kinetics400_se_rgb_r50_seg1_8x8_scratch.py,0,"b'model = dict(\n    type=\'TSN3D\',\n    backbone=dict(\n        type=\'ResNet_I3D\',\n        pretrained=None,\n        depth=50,\n        num_stages=4,\n        out_indices=[3],\n        frozen_stages=-1,\n        inflate_freq=(0, 0, 1, 1),\n        conv1_kernel_t=1,\n        conv1_stride_t=1,\n        pool1_kernel_t=1,\n        pool1_stride_t=1,\n        inflate_style=\'3x1x1\',\n        bn_eval=False,\n        no_pool2=True,\n        partial_bn=False,\n        style=\'pytorch\'),\n    spatial_temporal_module=dict(\n        type=\'SimpleSpatialTemporalModule\',\n        spatial_type=\'avg\',\n        temporal_size=8,\n        spatial_size=7),\n    segmental_consensus=dict(\n        type=\'SimpleConsensus\',\n        consensus_type=\'avg\'),\n    cls_head=dict(\n        type=\'ClsHead\',\n        with_avg_pool=False,\n        temporal_feature_size=1,\n        spatial_feature_size=1,\n        dropout_ratio=0.5,\n        in_channels=2048,\n        num_classes=400))\ntrain_cfg = None\ntest_cfg = None\n# dataset settings\ndataset_type = \'RawFramesDataset\'\ndata_root = \'data/kinetics400/rawframes_train/\'\ndata_root_val = \'data/kinetics400/rawframes_val/\'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    videos_per_gpu=8,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=\'data/kinetics400/kinetics400_train_list_rawframes.txt\',\n        img_prefix=data_root,\n        img_norm_cfg=img_norm_cfg,\n        input_format=""NCTHW"",\n        num_segments=1,\n        new_length=8,\n        new_step=8,\n        random_shift=True,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=224,\n        div_255=False,\n        flip_ratio=0.5,\n        resize_crop=True,\n        resize_keep_ratio=True,\n        oversample=None,\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=True,\n        test_mode=False),\n    val=dict(\n        type=dataset_type,\n        ann_file=\'data/kinetics400/kinetics400_val_list_rawframes.txt\',\n        img_prefix=data_root_val,\n        img_norm_cfg=img_norm_cfg,\n        input_format=""NCTHW"",\n        num_segments=1,\n        new_length=8,\n        new_step=8,\n        random_shift=False,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=224,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample=None,\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=True),\n   test=dict(\n        type=dataset_type,\n        ann_file=\'data/kinetics400/kinetics400_val_list_rawframes.txt\',\n        img_prefix=data_root_val,\n        img_norm_cfg=img_norm_cfg,\n        input_format=""NCTHW"",\n        num_segments=10,\n        new_length=8,\n        new_step=8,\n        random_shift=True,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=256,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample=\'three_crop\',\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=True))\n\noptimizer = dict(type=\'SGD\', lr=0.1, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy=\'cosine\',\n    warmup_ratio=0.01,\n    warmup=\'linear\',\n    warmup_iters=127840)\n\ncheckpoint_config = dict(interval=5)\nworkflow = [(\'train\', 1)]\n# yapf:disable\nlog_config = dict(\n    interval=20,\n    hooks=[\n        dict(type=\'TextLoggerHook\'),\n        dict(type=\'TensorboardLoggerHook\')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 196\ndist_params = dict(backend=\'nccl\')\nlog_level = \'INFO\'\nwork_dir = \'./work_dirs/slowonly_kinetics400_se_rgb_r50_seg1_8x8_scratch_b8_g8\'\nload_from = None\nresume_from = None\n'"
configs/TSN/tsn_kinetics400_2d_rgb_r50_seg3_f1s1.py,0,"b'model = dict(\n    type=\'TSN2D\',\n    backbone=dict(\n        type=\'ResNet\',\n        pretrained=\'modelzoo://resnet50\',\n        depth=50,\n        out_indices=(3,),\n        bn_eval=False,\n        partial_bn=False),\n    spatial_temporal_module=dict(\n        type=\'SimpleSpatialModule\',\n        spatial_type=\'avg\',\n        spatial_size=7),\n    segmental_consensus=dict(\n        type=\'SimpleConsensus\',\n        consensus_type=\'avg\'),\n    cls_head=dict(\n        type=\'ClsHead\',\n        with_avg_pool=False,\n        temporal_feature_size=1,\n        spatial_feature_size=1,\n        dropout_ratio=0.4,\n        in_channels=2048,\n        num_classes=400))\ntrain_cfg = None\ntest_cfg = None\n# dataset settings\ndataset_type = \'RawFramesDataset\'\ndata_root = \'data/kinetics400/rawframes_train\'\ndata_root_val = \'data/kinetics400/rawframes_val\'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n\ndata = dict(\n    workers_per_gpu=2,\n    videos_per_gpu=32,\n    train=dict(\n        type=dataset_type,\n        ann_file=\'data/kinetics400/kinetics400_train_list_rawframes.txt\',\n        img_prefix=data_root,\n        img_norm_cfg=img_norm_cfg,\n        num_segments=3,\n        new_length=1,\n        new_step=1,\n        random_shift=True,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=224,\n        div_255=False,\n        flip_ratio=0.5,\n        resize_keep_ratio=True,\n        oversample=None,\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=True,\n        scales=[1, 0.875, 0.75, 0.66],\n        max_distort=1,\n        test_mode=False),\n    val=dict(\n        type=dataset_type,\n        ann_file=\'data/kinetics400/kinetics400_val_list_rawframes.txt\',\n        img_prefix=data_root_val,\n        img_norm_cfg=img_norm_cfg,\n        num_segments=3,\n        new_length=1,\n        new_step=1,\n        random_shift=False,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=224,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample=None,\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=False),\n    test=dict(\n        type=dataset_type,\n        ann_file=\'data/kinetics400/kinetics400_val_list_rawframes.txt\',\n        img_prefix=data_root_val,\n        img_norm_cfg=img_norm_cfg,\n        num_segments=25,\n        new_length=1,\n        new_step=1,\n        random_shift=False,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=224,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample=""ten_crop"",\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type=\'SGD\', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy=\'step\',\n    step=[40, 80])\ncheckpoint_config = dict(interval=5)\nworkflow = [(\'train\', 5), (\'val\', 1)]\n# workflow = [(\'train\', 1)]\n# yapf:disable\nlog_config = dict(\n    interval=20,\n    hooks=[\n        dict(type=\'TextLoggerHook\'),\n        dict(type=\'TensorboardLoggerHook\')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 100\ndist_params = dict(backend=\'nccl\')\nlog_level = \'INFO\'\nwork_dir = \'./work_dirs/tsn_kinetics400_2d_rgb_r50_seg3_f1s1_b32_g8_imagenet\'\nload_from = None\nresume_from = None\n'"
configs/ava/ava_fast_rcnn_nl_r50_c4_1x_kinetics_pretrain_crop.py,0,"b'# model settings\nmodel = dict(\n    type=\'FastRCNN\',\n    backbone=dict(\n        type=\'ResNet_I3D\',\n        pretrained=\'open-mmlab://kin400/nl3d_r50_f32s2_k400\',\n        pretrained2d=False,\n        depth=50,\n        num_stages=3,\n        spatial_strides=(1, 2, 2),\n        temporal_strides=(1, 1, 1),\n        dilations=(1, 1, 1),\n        out_indices=(2,),\n        frozen_stages=-1,\n        inflate_freq=((1,1,1), (1,0,1,0), (1,0,1,0,1,0)),\n        inflate_style=\'3x1x1\',\n        nonlocal_stages=(1, 2),\n        # nonlocal_freq=((0,0,0), (0,1,0,1), (0,1,0,1,0,1)),\n        nonlocal_cfg=dict(nonlocal_type=""gaussian""),\n        nonlocal_freq=((0,0,0), (0,1,0,1), (0,1,0,1,0,1), (0,0,0)),\n        conv1_kernel_t=5,\n        conv1_stride_t=1,\n        pool1_kernel_t=1,\n        pool1_stride_t=1,\n        bn_eval=False,\n        partial_bn=False,\n        bn_frozen=True,\n        style=\'pytorch\'),\n    shared_head=dict(\n        type=\'ResI3DLayer\',\n        pretrained=\'open-mmlab://kin400/nl3d_r50_f32s2_k400\',\n        pretrained2d=False,\n        depth=50,\n        stage=3,\n        spatial_stride=2,\n        temporal_stride=1,\n        dilation=1,\n        style=\'pytorch\',\n        inflate_freq=(0, 1, 0),\n        inflate_style=\'3x1x1\',\n        bn_eval=False,\n        bn_frozen=True),\n    bbox_roi_extractor=dict(\n        type=\'SingleRoIStraight3DExtractor\',\n        roi_layer=dict(type=\'RoIAlign\', out_size=16, sample_num=2),\n        out_channels=1024,\n        featmap_strides=[16],\n        with_temporal_pool=True),\n    dropout_ratio=0.3,\n    bbox_head=dict(\n        type=\'BBoxHead\',\n        with_reg=False,\n        with_temporal_pool=False,\n        with_spatial_pool=True,\n        spatial_pool_type=\'max\',\n        roi_feat_size=(1, 8, 8),\n        in_channels=2048,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        multilabel_classification=True,\n        reg_class_agnostic=True,\n        nms_class_agnostic=True))\n# model training and testing settings\ntrain_cfg = dict(\n    train_detector=False,\n    person_det_score_thr=0.9,\n    rcnn=dict(\n        assigner=dict(\n            type=\'MaxIoUAssigner\',\n            pos_iou_thr=0.9,\n            neg_iou_thr=0.9,\n            min_pos_iou=0.9,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type=\'RandomSampler\',\n            num=32, # 512,\n            pos_fraction=1, #0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_weight=-1,\n        cls_weight=1,\n        debug=False))\ntest_cfg = dict(\n    train_detector=False,\n    person_det_score_thr=0.85,\n    rcnn=dict(\n        score_thr=0.00, nms=dict(type=\'nms\', iou_thr=1.0), max_per_img=100,\n        action_thr=0.00))\n# dataset settings\ndataset_type = \'AVADataset\'\ndata_root = \'data/ava/rawframes/\'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    videos_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=\'data/ava/annotations/ava_train_v2.1.csv\',\n        exclude_file=\'data/ava/annotations/ava_train_excluded_timestamps_v2.1.csv\',\n        label_file=\'data/ava/annotations/ava_action_list_v2.1_for_activitynet_2018.pbtxt\',\n        video_stat_file=\'data/ava/ava_video_resolution_stats.csv\',\n        proposal_file=\'data/ava/ava_dense_proposals_train.FAIR.recall_93.9.pkl\',\n        img_prefix=data_root,\n        img_norm_cfg=img_norm_cfg,\n        input_format=""NCTHW"",\n        new_length=32,\n        new_step=2,\n        random_shift=False,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=[(800, 256), (1000, 320)],\n        input_size=256, # do no crop\n        div_255=False,\n        size_divisor=32,\n        flip_ratio=0.5,\n        resize_keep_ratio=True,\n        test_mode=False,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=\'data/ava/annotations/ava_val_v2.1.csv\',\n        exclude_file=\'data/ava/annotations/ava_val_excluded_timestamps_v2.1.csv\',\n        label_file=\'data/ava/annotations/ava_action_list_v2.1_for_activitynet_2018.pbtxt\',\n        video_stat_file=\'data/ava/ava_video_resolution_stats.csv\',\n        proposal_file=\'data/ava/ava_dense_proposals_val.FAIR.recall_93.9.pkl\',\n        img_prefix=data_root,\n        img_norm_cfg=img_norm_cfg,\n        input_format=""NCTHW"",\n        new_length=32,\n        new_step=2,\n        random_shift=False,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=[(800, 256), ],\n        input_size=256,\n        div_255=False,\n        size_divisor=32,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=\'data/ava/annotations/ava_val_v2.1.csv\',\n        exclude_file=\'data/ava/annotations/ava_val_excluded_timestamps_v2.1.csv\',\n        label_file=\'data/ava/annotations/ava_action_list_v2.1_for_activitynet_2018.pbtxt\',\n        video_stat_file=\'data/ava/ava_video_resolution_stats.csv\',\n        proposal_file=\'data/ava/ava_dense_proposals_val.FAIR.recall_93.9.pkl\',\n        img_prefix=data_root,\n        img_norm_cfg=img_norm_cfg,\n        input_format=\'NCTHW\',\n        new_length=32,\n        new_step=2,\n        random_shift=False,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=[(800, 256), ],\n        input_size=None,\n        div_255=False,\n        size_divisor=32,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type=\'SGD\', lr=0.04, momentum=0.9, weight_decay=1e-6)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy=\'step\',\n    warmup=\'linear\',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 4,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type=\'TextLoggerHook\'),\n        # dict(type=\'TensorboardLoggerHook\')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend=\'nccl\')\nlog_level = \'INFO\'\nwork_dir = \'./work_dirs/ava_fast_rcnn_nl_r50_c4_1x_f32s2_kinetics_pretrain_crop_multiscale\'\nload_from = None\nresume_from = None\nworkflow = [(\'train\', 1)]\n'"
configs/hmdb51/tsn_flow_bninception.py,0,"b""# model settings\nmodel = dict(\n    type='TSN2D',\n    modality='Flow',\n    in_channels=10,\n    backbone=dict(\n        type='BNInception',\n        pretrained='open-mmlab://bninception_caffe',\n        bn_eval=False,\n        partial_bn=True),\n    spatial_temporal_module=dict(\n        type='SimpleSpatialModule',\n        spatial_type='avg',\n        spatial_size=7),\n    segmental_consensus=dict(\n        type='SimpleConsensus',\n        consensus_type='avg'),\n    cls_head=dict(\n        type='ClsHead',\n        with_avg_pool=False,\n        temporal_feature_size=1,\n        spatial_feature_size=1,\n        dropout_ratio=0.7,\n        in_channels=1024,\n        num_classes=51))\ntrain_cfg = None\ntest_cfg = None\n# dataset settings\ndataset_type = 'RawFramesDataset'\ndata_root = 'data/hmdb51/rawframes'\nimg_norm_cfg = dict(\n    mean=[128], std=[1], to_rgb=False)\ndata = dict(\n    videos_per_gpu=32,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file='data/hmdb51/hmdb51_train_split_1_rawframes.txt',\n        img_prefix=data_root,\n        img_norm_cfg=img_norm_cfg,\n        num_segments=3,\n        new_length=5,\n        new_step=1,\n        random_shift=True,\n        modality='Flow',\n        image_tmpl='flow_{}_{:05d}.jpg',\n        img_scale=256,\n        input_size=224,\n        div_255=False,\n        flip_ratio=0.5,\n        resize_keep_ratio=True,\n        oversample=None,\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=True,\n        scales=[1, 0.875, 0.75, 0.66],\n        max_distort=1,\n        test_mode=False),\n    val=dict(\n        type=dataset_type,\n        ann_file='data/hmdb51/hmdb51_val_split_1_rawframes.txt',\n        img_prefix=data_root,\n        img_norm_cfg=img_norm_cfg,\n        num_segments=3,\n        new_length=5,\n        new_step=1,\n        random_shift=False,\n        modality='Flow',\n        image_tmpl='flow_{}_{:05d}.jpg',\n        img_scale=256,\n        input_size=224,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample=None,\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=False),\n    test=dict(\n        type=dataset_type,\n        ann_file='data/hmdb51/hmdb51_val_split_1_rawframes.txt',\n        img_prefix=data_root,\n        img_norm_cfg=img_norm_cfg,\n        num_segments=25,\n        new_length=5,\n        new_step=1,\n        random_shift=False,\n        modality='Flow',\n        image_tmpl='flow_{}_{:05d}.jpg',\n        img_scale=256,\n        input_size=224,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample='ten_crop',\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.005, momentum=0.9, weight_decay=0.0005)\noptimizer_config = dict(grad_clip=dict(max_norm=20, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    step=[190, 300])\ncheckpoint_config = dict(interval=1)\n# workflow = [('train', 5), ('val', 1)]\nworkflow = [('train', 1)]\n# yapf:disable\nlog_config = dict(\n    interval=20,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 340\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/tsn_2d_flow_bninception_seg_3_f1s1_b32_g8_lr_0.005'\nload_from = None\nresume_from = None\n"""
configs/hmdb51/tsn_rgb_bninception.py,0,"b""# model settings\nmodel = dict(\n    type='TSN2D',\n    backbone=dict(\n        type='BNInception',\n        pretrained='open-mmlab://bninception_caffe',\n        bn_eval=False,\n        partial_bn=True),\n    spatial_temporal_module=dict(\n        type='SimpleSpatialModule',\n        spatial_type='avg',\n        spatial_size=7),\n    segmental_consensus=dict(\n        type='SimpleConsensus',\n        consensus_type='avg'),\n    cls_head=dict(\n        type='ClsHead',\n        with_avg_pool=False,\n        temporal_feature_size=1,\n        spatial_feature_size=1,\n        dropout_ratio=0.8,\n        in_channels=1024,\n        init_std=0.001,\n        num_classes=51))\ntrain_cfg = None\ntest_cfg = None\n# dataset settings\ndataset_type = 'RawFramesDataset'\ndata_root = 'data/hmdb51/rawframes'\nimg_norm_cfg = dict(\n   mean=[104, 117, 128], std=[1, 1, 1], to_rgb=False)\n\ndata = dict(\n    videos_per_gpu=32,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file='data/hmdb51/hmdb51_train_split_1_rawframes.txt',\n        img_prefix=data_root,\n        img_norm_cfg=img_norm_cfg,\n        num_segments=3,\n        new_length=1,\n        new_step=1,\n        random_shift=True,\n        modality='RGB',\n        image_tmpl='img_{:05d}.jpg',\n        img_scale=256,\n        input_size=224,\n        div_255=False,\n        flip_ratio=0.5,\n        resize_keep_ratio=True,\n        oversample=None,\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=True,\n        scales=[1, 0.875, 0.75, 0.66],\n        max_distort=1,\n        test_mode=False),\n    val=dict(\n        type=dataset_type,\n        ann_file='data/hmdb51/hmdb51_val_split_1_rawframes.txt',\n        img_prefix=data_root,\n        img_norm_cfg=img_norm_cfg,\n        num_segments=3,\n        new_length=1,\n        new_step=1,\n        random_shift=False,\n        modality='RGB',\n        image_tmpl='img_{:05d}.jpg',\n        img_scale=256,\n        input_size=224,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample=None,\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=False),\n    test=dict(\n        type=dataset_type,\n        ann_file='data/hmdb51/hmdb51_val_split_1_rawframes.txt',\n        img_prefix=data_root,\n        img_norm_cfg=img_norm_cfg,\n        num_segments=25,\n        new_length=1,\n        new_step=1,\n        random_shift=False,\n        modality='RGB',\n        image_tmpl='img_{:05d}.jpg',\n        img_scale=256,\n        input_size=224,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample='ten_crop',\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.001, momentum=0.9, weight_decay=0.0005)\noptimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    step=[30, 60])\ncheckpoint_config = dict(interval=1)\n# workflow = [('train', 5), ('val', 1)]\nworkflow = [('train', 1)]\n# yapf:disable\nlog_config = dict(\n    interval=20,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 80\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/tsn_2d_rgb_bninception_seg_3_f1s1_b32_g8'\nload_from = None\nresume_from = None\n"""
configs/thumos14/ssn_thumos14_rgb_bn_inception.py,0,"b'# model settings\nmodel = dict(\n    type=\'SSN2D\',\n    backbone=dict(\n        type=\'BNInception\',\n        pretrained=\'open-mmlab://bninception_caffe\',\n        bn_eval=False,\n        partial_bn=True),\n    spatial_temporal_module=dict(\n        type=\'SimpleSpatialModule\',\n        spatial_type=\'avg\',\n        spatial_size=7),\n    dropout_ratio=0.8,\n    segmental_consensus=dict(\n        type=\'StructuredTemporalPyramidPooling\',\n        standalong_classifier=True,\n        stpp_cfg=(1, 1, 1),\n        num_seg=(2, 5, 2)),\n    cls_head=dict(\n        type=\'SSNHead\',\n        dropout_ratio=0.,\n        in_channels_activity=1024,\n        in_channels_complete=3072,\n        num_classes=20,\n        with_bg=False,\n        with_reg=True))\n# model training and testing settings\ntrain_cfg = dict(\n    ssn=dict(\n        assigner=dict(\n            fg_iou_thr=0.7,\n            bg_iou_thr=0.01,\n            incomplete_iou_thr=0.3,\n            bg_coverage_thr=0.02,\n            incomplete_overlap_thr=0.01),\n        sampler=dict(\n            num_per_video=8,\n            fg_ratio=1,\n            bg_ratio=1,\n            incomplete_ratio=6,\n            add_gt_as_proposals=True),\n        loss_weight=dict(\n            comp_loss_weight=0.1,\n            reg_loss_weight=0.1),\n        debug=False))\ntest_cfg=dict(\n    ssn=dict(\n        sampler=dict(\n            test_interval=6,\n            batch_size=16),\n        evaluater=dict(\n            top_k=2000,\n            nms=0.2,\n            softmax_before_filter=True,\n            cls_score_dict=None,\n            cls_top_k=2)))\n# dataset settings\ndataset_type = \'SSNDataset\'\ndata_root = \'./data/thumos14/rawframes/\'\nimg_norm_cfg = dict(\n    mean=[104, 117, 128], std=[1, 1, 1], to_rgb=False)\ndata = dict(\n    videos_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=\'data/thumos14/thumos14_tag_val_normalized_proposal_list.txt\',\n        img_prefix=data_root,\n        img_norm_cfg=img_norm_cfg,\n        train_cfg=train_cfg,\n        test_cfg=test_cfg,\n        input_format=""NCHW"",\n        body_seg=model[\'segmental_consensus\'][\'num_seg\'][1],\n        aug_seg=(model[\'segmental_consensus\'][\'num_seg\'][0],\n                 model[\'segmental_consensus\'][\'num_seg\'][2]),\n        aug_ratio=0.5,\n        new_length=1,\n        new_step=1,\n        random_shift=True,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=(340, 256),\n        input_size=224,\n        div_255=False,\n        size_divisor=32,\n        flip_ratio=0.5,\n        resize_keep_ratio=True,\n        test_mode=False,\n        verbose=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=\'data/thumos14/thumos14_tag_test_normalized_proposal_list.txt\',\n        img_prefix=data_root,\n        img_norm_cfg=img_norm_cfg,\n        train_cfg=train_cfg,\n        test_cfg=test_cfg,\n        input_format=""NCHW"",\n        body_seg=model[\'segmental_consensus\'][\'num_seg\'][1],\n        aug_seg=(model[\'segmental_consensus\'][\'num_seg\'][0],\n                 model[\'segmental_consensus\'][\'num_seg\'][2]),\n        aug_ratio=0.5,\n        new_length=1,\n        new_step=1,\n        random_shift=False,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=(340, 256),\n        input_size=224,\n        div_255=False,\n        size_divisor=32,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        test_mode=False),\n    test=dict(\n        type=dataset_type,\n        ann_file=\'data/thumos14/thumos14_tag_test_normalized_proposal_list.txt\',\n        img_prefix=data_root,\n        img_norm_cfg=img_norm_cfg,\n        train_cfg=train_cfg,\n        test_cfg=test_cfg,\n        input_format=\'NCHW\',\n        aug_ratio=0.5,\n        new_length=1,\n        new_step=1,\n        random_shift=False,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=(340, 256),\n        input_size=224,\n        oversample=None,\n        div_255=False,\n        size_divisor=32,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        test_mode=True))\n# optimizer\noptimizer = dict(type=\'SGD\', lr=0.001, momentum=0.9, weight_decay=1e-6)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy=\'step\',\n    # warmup=\'linear\',\n    # warmup_iters=500,\n    # warmup_ratio=1.0 / 3,\n    step=[200, 400])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=5,\n    hooks=[\n        dict(type=\'TextLoggerHook\'),\n        # dict(type=\'TensorboardLoggerHook\')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 450 \ndist_params = dict(backend=\'nccl\')\nlog_level = \'INFO\'\nwork_dir = \'./work_dirs/ssn_thumos14_2d_rgb_bn_inception\'\nload_from = None\nresume_from = None\nworkflow = [(\'train\', 1)]\n'"
mmaction/apis/__init__.py,0,"b""from .env import init_dist, get_root_logger, set_random_seed\nfrom .train import train_network\n\n__all__ = [\n    'init_dist', 'get_root_logger', 'set_random_seed',\n    'train_network',\n]\n"""
mmaction/apis/env.py,6,"b""import logging\nimport os\nimport random\n\nimport numpy as np\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom mmcv.runner import get_dist_info\n\n\ndef init_dist(launcher, backend='nccl', **kwargs):\n    if mp.get_start_method(allow_none=True) is None:\n        mp.set_start_method('spawn')\n    if launcher == 'pytorch':\n        _init_dist_pytorch(backend, **kwargs)\n    elif launcher == 'mpi':\n        _init_dist_mpi(backend, **kwargs)\n    elif launcher == 'slurm':\n        _init_dist_slurm(backend, **kwargs)\n    else:\n        raise ValueError('Invalid launcher type: {}'.format(launcher))\n\n\ndef _init_dist_pytorch(backend, **kwargs):\n    # TODO: use local_rank instead of rank % num_gpus\n    rank = int(os.environ['RANK'])\n    num_gpus = torch.cuda.device_count()\n    torch.cuda.set_device(rank % num_gpus)\n    dist.init_process_group(backend=backend, **kwargs)\n\n\ndef _init_dist_mpi(backend, **kwargs):\n    raise NotImplementedError\n\n\ndef _init_dist_slurm(backend, **kwargs):\n    raise NotImplementedError\n\n\ndef set_random_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef get_root_logger(log_level=logging.INFO):\n    logger = logging.getLogger()\n    if not logger.hasHandlers():\n        logging.basicConfig(\n            format='%(asctime)s - %(levelname)s - %(message)s',\n            level=log_level)\n    rank, _ = get_dist_info()\n    if rank != 0:\n        logger.setLevel('ERROR')\n    return logger\n"""
mmaction/apis/train.py,1,"b""from __future__ import division\n\nfrom collections import OrderedDict\n\nimport torch\nfrom mmcv.runner import Runner, DistSamplerSeedHook\nfrom mmcv.parallel import MMDataParallel\nfrom mmcv.parallel.distributed_deprecated import MMDistributedDataParallel\n\nfrom mmaction.core import (DistOptimizerHook, DistEvalTopKAccuracyHook,\n                           AVADistEvalmAPHook)\nfrom mmaction.datasets import build_dataloader\nfrom .env import get_root_logger\n\n\ndef parse_losses(losses):\n    log_vars = OrderedDict()\n    for loss_name, loss_value in losses.items():\n        if isinstance(loss_value, torch.Tensor):\n            log_vars[loss_name] = loss_value.mean()\n        elif isinstance(loss_value, list):\n            log_vars[loss_name] = sum(_loss.mean() for _loss in loss_value)\n        else:\n            raise TypeError(\n                '{} is not a tensor or list of tensors'.format(loss_name))\n\n    loss = sum(_value for _key, _value in log_vars.items() if 'loss' in _key)\n\n    log_vars['loss'] = loss\n    for name in log_vars:\n        log_vars[name] = log_vars[name].item()\n\n    return loss, log_vars\n\n\ndef batch_processor(model, data, train_mode):\n    losses = model(**data)\n    loss, log_vars = parse_losses(losses)\n\n    outputs = dict(\n        loss=loss, log_vars=log_vars,\n        num_samples=len(data['img_group_0'].data))\n\n    return outputs\n\n\ndef train_network(model,\n                  dataset,\n                  cfg,\n                  distributed=False,\n                  validate=False,\n                  logger=None):\n    if logger is None:\n        logger = get_root_logger(cfg.log_level)\n\n    # start training\n    if distributed:\n        _dist_train(model, dataset, cfg, validate=validate)\n    else:\n        _non_dist_train(model, dataset, cfg, validate=validate)\n\n\ndef _dist_train(model, dataset, cfg, validate=False):\n    # prepare data loaders\n    data_loaders = [\n        build_dataloader(\n            dataset,\n            cfg.data.videos_per_gpu,\n            cfg.data.workers_per_gpu,\n            dist=True)\n    ]\n    # put model on gpus\n    model = MMDistributedDataParallel(model.cuda())\n    # build runner\n    runner = Runner(model, batch_processor, cfg.optimizer, cfg.work_dir,\n                    cfg.log_level)\n    # register hooks\n    optimizer_config = DistOptimizerHook(**cfg.optimizer_config)\n    runner.register_training_hooks(cfg.lr_config, optimizer_config,\n                                   cfg.checkpoint_config, cfg.log_config)\n    runner.register_hook(DistSamplerSeedHook())\n    # register eval hooks\n    if validate:\n        if cfg.data.val.type in ['RawFramesDataset', 'VideoDataset']:\n            runner.register_hook(\n                DistEvalTopKAccuracyHook(cfg.data.val, k=(1, 5)))\n        if cfg.data.val.type == 'AVADataset':\n            runner.register_hook(AVADistEvalmAPHook(cfg.data.val))\n    # if validate:\n    #     if isinstance(model.module, RPN):\n    #         # TODO: implement recall hooks for other datasets\n    #         runner.register_hook(CocoDistEvalRecallHook(cfg.data.val))\n    #     else:\n    #         if cfg.data.val.type == 'CocoDataset':\n    #             runner.register_hook(CocoDistEvalmAPHook(cfg.data.val))\n    #         else:\n    #             runner.register_hook(DistEvalmAPHook(cfg.data.val))\n\n    if cfg.resume_from:\n        runner.resume(cfg.resume_from)\n    elif cfg.load_from:\n        runner.load_checkpoint(cfg.load_from)\n    runner.run(data_loaders, cfg.workflow, cfg.total_epochs)\n\n\ndef _non_dist_train(model, dataset, cfg, validate=False):\n    # prepare data loaders\n    data_loaders = [\n        build_dataloader(\n            dataset,\n            cfg.data.videos_per_gpu,\n            cfg.data.workers_per_gpu,\n            cfg.gpus,\n            dist=False)\n    ]\n    # put model on gpus\n    model = MMDataParallel(model, device_ids=range(cfg.gpus)).cuda()\n    # build runner\n    runner = Runner(model, batch_processor, cfg.optimizer, cfg.work_dir,\n                    cfg.log_level)\n    runner.register_training_hooks(cfg.lr_config, cfg.optimizer_config,\n                                   cfg.checkpoint_config, cfg.log_config)\n\n    if cfg.resume_from:\n        runner.resume(cfg.resume_from)\n    elif cfg.load_from:\n        runner.load_checkpoint(cfg.load_from)\n    runner.run(data_loaders, cfg.workflow, cfg.total_epochs)\n"""
mmaction/core/__init__.py,0,b'from .evaluation import *\nfrom .utils import *\n'
mmaction/datasets/__init__.py,0,"b""from .rawframes_dataset import RawFramesDataset\nfrom .lmdbframes_dataset import LMDBFramesDataset\nfrom .video_dataset import VideoDataset\nfrom .ssn_dataset import SSNDataset\nfrom .ava_dataset import AVADataset\nfrom .utils import get_untrimmed_dataset, get_trimmed_dataset\nfrom .loader import GroupSampler, DistributedGroupSampler, build_dataloader\n\n__all__ = [\n    'RawFramesDataset', 'LMDBFramesDataset',\n    'VideoDataset', 'SSNDataset', 'AVADataset',\n    'get_trimmed_dataset', 'get_untrimmed_dataset',\n    'GroupSampler', 'DistributedGroupSampler', 'build_dataloader'\n]\n"""
mmaction/datasets/ava_dataset.py,1,"b'import mmcv\nimport numpy as np\nimport os.path as osp\nfrom mmcv.parallel import DataContainer as DC\nfrom torch.utils.data import Dataset\n\nfrom .transforms import (GroupImageTransform, BboxTransform)\nfrom .utils import (to_tensor, random_scale)\n\n_TIMESTAMP_BIAS = 600\n_TIMESTAMP_START = 840  # 60*14min\n_TIMESTAMP_END = 1860  # 60*31min\n_FPS = 30\n\n\nclass RawFramesRecord(object):\n    def __init__(self, row):\n        self._data = row\n\n    @property\n    def video_id(self):\n        return self._data[0]\n\n    @property\n    def timestamp(self):\n        return int(self._data[1])\n\n    @property\n    def entity_box(self):\n        x1 = float(self._data[2])\n        y1 = float(self._data[3])\n        x2 = float(self._data[4])\n        y2 = float(self._data[5])\n        return np.array([x1, y1, x2, y2])\n\n    @property\n    def label(self):\n        return int(self._data[6])\n\n    @property\n    def entity_id(self):\n        return int(self._data[7])\n\n\nclass AVADataset(Dataset):\n    def __init__(self,\n                 ann_file,\n                 exclude_file,\n                 label_file,\n                 video_stat_file,\n                 img_prefix,\n                 img_norm_cfg,\n                 new_length=1,\n                 new_step=1,\n                 random_shift=True,\n                 modality=\'RGB\',\n                 image_tmpl=\'img_{}.jpg\',\n                 img_scale=(340, 256),\n                 input_size=None,\n                 div_255=False,\n                 size_divisor=None,\n                 multiscale_mode=\'value\',\n                 proposal_file=None,\n                 num_max_proposals=1000,\n                 flip_ratio=0.5,\n                 resize_keep_ratio=True,\n                 resize_ratio=[1, 0.875, 0.75, 0.66],\n                 with_label=False,\n                 test_mode=False,\n                 oversample=None,\n                 random_crop=False,\n                 more_fix_crop=False,\n                 multiscale_crop=False,\n                 scales=None,\n                 max_distort=1,\n                 input_format=\'NCHW\'):\n        # prefix of images path\n        self.img_prefix = img_prefix\n\n        # load annotations\n        self.video_infos = self.load_annotations(\n            ann_file, video_stat_file, exclude_file)\n        self.ann_file = ann_file\n        self.exclude_file = exclude_file\n        self.label_file = label_file\n        if proposal_file is not None:\n            self.proposals = self.load_proposals(proposal_file)\n        else:\n            self.proposals = None\n\n        # filter videos with no annotation during training\n        if not test_mode:\n            valid_inds = self._filter_records(exclude_file=exclude_file)\n            print(""{} out of {} frames are valid."".format(\n                len(valid_inds), len(self.video_infos)))\n            self.video_infos = [self.video_infos[i] for i in valid_inds]\n\n        # normalization config\n        self.img_norm_cfg = img_norm_cfg\n\n        # max proposals per image\n        self.num_max_proposals = num_max_proposals\n\n        # parameters for frame fetching\n        # number of consecutive frames\n        self.old_length = new_length * new_step\n        self.new_length = new_length\n        # number of steps (sparse sampling for efficiency of io)\n        self.new_step = new_step\n        # whether to temporally random shift when training\n        self.random_shift = random_shift\n\n        # parameters for modalities\n        if isinstance(modality, (list, tuple)):\n            self.modalities = modality\n            num_modality = len(modality)\n        else:\n            self.modalities = [modality]\n            num_modality = 1\n        if isinstance(image_tmpl, (list, tuple)):\n            self.image_tmpls = image_tmpl\n        else:\n            self.image_tmpls = [image_tmpl]\n        assert len(self.image_tmpls) == num_modality\n\n        # parameters for image preprocessing\n        # img_scale\n        if isinstance(img_scale, int):\n            img_scale = (np.Inf, img_scale)\n        self.img_scales = img_scale\n        # network input size\n        if isinstance(input_size, int):\n            input_size = (input_size, input_size)\n        self.input_size = input_size\n\n        # parameters for specification from pre-trained networks (lecacy issue)\n        self.div_255 = div_255\n\n        # parameters for data augmentation\n        # multi-scale mode (only applicable for multi-scale training)\n        self.multiscale_mode = multiscale_mode\n        assert multiscale_mode in [\'value\', \'range\']\n        # flip ratio\n        self.flip_ratio = flip_ratio\n        self.resize_keep_ratio = resize_keep_ratio\n\n        # with_label is False for RPN\n        self.with_label = with_label\n\n        # test mode or not\n        self.test_mode = test_mode\n\n        # set group flag for the sampler\n        if not self.test_mode:\n            self._set_group_flag()\n\n        # transforms\n        self.img_group_transform = GroupImageTransform(\n            size_divisor=None,\n            crop_size=self.input_size,\n            oversample=oversample,\n            random_crop=random_crop, more_fix_crop=more_fix_crop,\n            multiscale_crop=multiscale_crop, scales=scales,\n            max_distort=max_distort,\n            **self.img_norm_cfg)\n\n        # input format\n        assert input_format in [\'NCHW\', \'NCTHW\']\n        self.input_format = input_format\n\n        self.bbox_transform = BboxTransform()\n\n    def __len__(self):\n        return len(self.video_infos)\n\n    def arrange_annotations(self, records, video_stats):\n        """""" Rearrange the frame-level to annotation format similar to COCO\n        [\n            {\n                \'video_id\': \'xxxxxxxxxxx\',\n                \'timestamp\': 902, ## in sec\n                \'width\': 340,\n                \'height\': 256,\n                \'fps\': 30,\n                \'shot_info\': (ss, tt),     ## in frame\n                \'ann\': {\n                    \'bboxes\': <np.ndarray> (n, 4),\n                    \'labels\': <np.ndarray> (n, ), ==> <np.ndarray> (n, m=80)\n                    \'pids\': <np.ndarray> (n, ),\n                    \'tracklets\': <np.ndarray> (n, t, 4) (TODO: To be added)\n                }\n            },\n            ...\n        ]\n        The \'ann\' field is optional for testing\n        """"""\n\n        record_dict_by_image = dict()\n        for record in records:\n            image_key = ""{},{:04d}"".format(record.video_id, record.timestamp)\n            if image_key not in record_dict_by_image:\n                record_dict_by_image[image_key] = [record]\n            else:\n                record_dict_by_image[image_key].append(record)\n\n        def merge(records):\n            bboxes = []\n            labels = []\n            pids = []\n            while len(records) > 0:\n                r = records[0]\n                rs = list(filter(lambda x: np.array_equal(\n                    x.entity_box, r.entity_box), records))\n                records = list(filter(lambda x: not np.array_equal(\n                    x.entity_box, r.entity_box), records))\n                bboxes.append(\n                    r.entity_box * np.array([width, height, width, height]))\n                valid_labels = np.stack([r.label for r in rs])\n                padded_labels = np.pad(\n                    valid_labels, (0, 81 - valid_labels.shape[0]),\n                    \'constant\', constant_values=-1)\n                labels.append(padded_labels)\n                pids.append(r.entity_id)\n            bboxes = np.stack(bboxes)\n            labels = np.stack(labels)\n            # print(bboxes)\n            # print(labels)\n            pids = np.stack(pids)\n            return bboxes, labels, pids\n\n        new_records = []\n        for image_key in record_dict_by_image:\n            video_id, timestamp = image_key.split(\',\')\n            width = int(video_stats[video_id].split(\'x\')[0])\n            height = int(video_stats[video_id].split(\'x\')[1])\n            shot_info = (0, (_TIMESTAMP_END - _TIMESTAMP_START) * _FPS)\n\n            bboxes, labels, pids = merge(record_dict_by_image[image_key])\n\n            ann = dict(bboxes=bboxes,\n                       labels=labels,\n                       pids=pids)\n            new_record = dict(video_id=video_id,\n                              timestamp=int(timestamp),\n                              width=width,\n                              height=height,\n                              shot_info=shot_info,\n                              fps=_FPS,\n                              ann=ann)\n            new_records.append(new_record)\n\n        return new_records\n\n    def load_annotations(self, ann_file, video_stat_file, exclude_file=None):\n        rawframe_records = [RawFramesRecord(\n            x.strip().split(\',\')) for x in open(ann_file)]\n        video_stats = [tuple(x.strip().split(\' \'))\n                       for x in open(video_stat_file)]\n        video_stats = {item[0]: item[1] for item in video_stats}\n        return self.arrange_annotations(rawframe_records, video_stats)\n        # return mmcv.load(ann_file)\n\n    def load_proposals(self, proposal_file):\n        return mmcv.load(proposal_file)\n\n    def get_ann_info(self, idx):\n        return self.video_infos[idx][\'ann\']\n\n    def _filter_records(self, exclude_file=None):\n        valid_inds = []\n        if exclude_file is not None:\n            exclude_records = [x.strip().split(\',\')\n                               for x in open(exclude_file)]\n            for i, video_info in enumerate(self.video_infos):\n                valid = True\n                for vv, tt in exclude_records:\n                    if (video_info[\'video_id\'] == vv\n                            and video_info[\'timestamp\'] == int(tt)):\n                        valid = False\n                        break\n                if valid:\n                    valid_inds.append(i)\n        else:\n            for i, _ in enumerate(self.video_infos):\n                valid_inds.append(i)\n        return valid_inds\n\n    def _set_group_flag(self):\n        """"""Set flag according to image aspect ratio.\n\n        Images with aspect ratio greater than 1 will be set as group 1,\n        otherwise group 0.\n        """"""\n        self.flag = np.zeros(len(self), dtype=np.uint8)\n        for i in range(len(self)):\n            # img_info = self.img_infos[i]\n            # if img_info[\'width\'] / img_info[\'height\'] > 1:\n            self.flag[i] = 1\n\n    def _load_image(self, directory, image_tmpl, modality, idx):\n        if modality in [\'RGB\', \'RGBDiff\']:\n            return [mmcv.imread(osp.join(directory, image_tmpl.format(idx)))]\n        elif modality == \'Flow\':\n            x_imgs = mmcv.imread(\n                osp.join(directory, image_tmpl.format(\'x\', idx)),\n                flag=\'grayscale\')\n            y_imgs = mmcv.imread(\n                osp.join(directory, image_tmpl.format(\'y\', idx)),\n                flag=\'grayscale\')\n            return [x_imgs, y_imgs]\n        else:\n            raise ValueError(\n                \'Not implemented yet; modality should be \'\n                \'[""RGB"", ""RGBDiff"", ""Flow""]\')\n\n    def _get_frames(self, record, image_tmpl, modality, indice, skip_offsets):\n        images = list()\n        #\n        p = indice - self.new_step\n        for i, ind in enumerate(\n                range(-2, -(self.old_length+1) // 2, -self.new_step)):\n            seg_imgs = self._load_image(osp.join(\n                self.img_prefix, record[\'video_id\']),\n                image_tmpl, modality, p + skip_offsets[i])\n            images = seg_imgs + images\n            if p - self.new_step >= record[\'shot_info\'][0]:\n                p -= self.new_step\n        p = indice\n        for i, ind in enumerate(\n                range(0, (self.old_length+1) // 2, self.new_step)):\n            seg_imgs = self._load_image(osp.join(\n                self.img_prefix, record[\'video_id\']),\n                image_tmpl, modality, p + skip_offsets[i])\n            images.extend(seg_imgs)\n            if p + self.new_step < record[\'shot_info\'][1]:\n                p += self.new_step\n        return images\n\n    def _rand_another(self, idx):\n        pool = np.where(self.flag == self.flag[idx])[0]\n        return np.random.choice(pool)\n\n    def __getitem__(self, idx):\n        if self.test_mode:\n            return self.prepare_test_imgs(idx)\n        while True:\n            data = self.prepare_train_imgs(idx)\n            if data is None:\n                idx = self._rand_another(idx)\n                continue\n            return data\n\n    def prepare_train_imgs(self, idx):\n        video_info = self.video_infos[idx]\n\n        # load proposals if necessary\n        if self.proposals is not None:\n            image_key = ""{},{:04d}"".format(\n                video_info[\'video_id\'], video_info[\'timestamp\'])\n            if image_key not in self.proposals:\n                return None\n            proposals = self.proposals[image_key][: self.num_max_proposals]\n            if len(proposals) == 0:\n                return None\n            if not (proposals.shape[1] == 4 or proposals.shape[1] == 5):\n                raise AssertionError(\n                    \'proposals should have shapes (n, 4) or (n,5), \'\n                    \'but found {}\'.format(proposals.shape))\n            if proposals.shape[1] == 4:\n                proposals = proposals * np.array(\n                    [video_info[\'width\'], video_info[\'height\'],\n                     video_info[\'width\'], video_info[\'height\']])\n            else:\n                proposals = proposals * np.array(\n                    [video_info[\'width\'], video_info[\'height\'],\n                     video_info[\'width\'], video_info[\'height\'], 1.0])\n            proposals = proposals.astype(np.float32)\n            if proposals.shape[1] == 5:\n                scores = proposals[:, 4, None]\n                proposals = proposals[:, :4]\n            else:\n                scores = None\n\n        ann = self.get_ann_info(idx)\n        gt_bboxes = ann[\'bboxes\']\n        gt_labels = ann[\'labels\']\n\n        # skip the record if there is no valid gt bbox\n        if len(gt_bboxes) == 0:\n            return None\n\n        gt_bboxes = gt_bboxes.astype(np.float32)\n\n        indice = video_info[\'fps\'] * \\\n            (video_info[\'timestamp\'] - _TIMESTAMP_START) + 1\n        skip_offsets = np.random.randint(\n            self.new_step, size=self.old_length // self.new_step)\n\n        data = dict(num_modalities=DC(to_tensor(len(self.modalities))))\n\n        # handle the first modality\n        modality = self.modalities[0]\n        image_tmpl = self.image_tmpls[0]\n        img_group = self._get_frames(\n            video_info, image_tmpl, modality, indice, skip_offsets)\n\n        # TODO: add extra augmentation ...\n\n        flip = True if np.random.rand() < self.flip_ratio else False\n        img_scale = random_scale(self.img_scales, self.multiscale_mode)\n        (img_group, img_shape, pad_shape,\n         scale_factor, crop_quadruple) = self.img_group_transform(\n            img_group, img_scale,\n            crop_history=None,\n            flip=flip, keep_ratio=self.resize_keep_ratio,\n            div_255=self.div_255,\n            is_flow=True if modality == \'Flow\' else False)\n        ori_shape = (video_info[\'height\'], video_info[\'width\'], 3)\n        img_meta = dict(\n            ori_shape=ori_shape,\n            img_shape=img_shape,\n            pad_shape=pad_shape,\n            scale_factor=scale_factor,\n            crop_quadruple=crop_quadruple,\n            flip=flip)\n\n        # [L x C x H x W]\n        if self.input_format == ""NCTHW"":\n            img_group = np.transpose(img_group, (1, 0, 2, 3))\n            # img_group = img_group[None, :]\n\n        data.update(dict(\n            img_group_0=DC(to_tensor(img_group), stack=True, pad_dims=2),\n            img_meta=DC(img_meta, cpu_only=True)\n        ))\n\n        # handle the rest modalities using the same\n        for i, (modality, image_tmpl) in enumerate(\n                zip(self.modalities[1:], self.image_tmpls[1:])):\n            img_group = self._get_frames(\n                video_info, image_tmpl, modality, indice, skip_offsets)\n\n            # TODO: add extra augmentation ...\n\n            # apply transforms\n            flip = True if np.random.rand() < self.flip_ratio else False\n            (img_group, img_shape, pad_shape,\n             scale_factor, crop_quadruple) = self.img_group_transform(\n                img_group, img_scale,\n                crop_history=data[\'img_meta\'][\'crop_quadruple\'],\n                flip=data[\'img_meta\'][\'flip\'],\n                keep_ratio=self.resize_keep_ratio,\n                div_255=self.div_255,\n                is_flow=True if modality == \'Flow\' else False)\n\n            if self.input_format == ""NCTHW"":\n                # Convert [L x C x H x W] to [C x L x H x W]\n                img_group = np.transpose(img_group, (1, 0, 2, 3))\n                # img_group = img_group[None, :]\n\n            else:\n                data.update({\n                    \'img_group_{}\'.format(i+1): DC(\n                        to_tensor(img_group), stack=True, pad_dims=2),\n                })\n\n        if self.proposals is not None:\n            proposals = self.bbox_transform(\n                proposals, img_shape, scale_factor, flip, crop=crop_quadruple)\n            proposals = np.hstack(\n                [proposals, scores]) if scores is not None else proposals\n            data[\'proposals\'] = DC(to_tensor(proposals))\n\n        gt_bboxes = self.bbox_transform(\n            gt_bboxes, img_shape, scale_factor, flip, crop=crop_quadruple)\n        data[\'gt_bboxes\'] = DC(to_tensor(gt_bboxes))\n\n        if self.with_label:\n            data[\'gt_labels\'] = DC(to_tensor(gt_labels))\n\n        return data\n\n    def prepare_test_imgs(self, idx):\n        video_info = self.video_infos[idx]\n\n        # load proposals if necessary\n        if self.proposals is not None:\n            image_key = ""{},{:04d}"".format(\n                video_info[\'video_id\'], video_info[\'timestamp\'])\n            if image_key not in self.proposals:\n                proposal = np.array([[0, 0, 1, 1, 1]], dtype=float)\n            else:\n                proposal = self.proposals[image_key][: self.num_max_proposals]\n            if not (proposal.shape[1] == 4 or proposal.shape[1] == 5):\n                raise AssertionError(\n                    \'proposals should have shapes (n, 4) or (n,5), \'\n                    \'but found {}\'.format(proposal.shape))\n            if proposal.shape[1] == 4:\n                proposal = proposal * np.array(\n                    [video_info[\'width\'], video_info[\'height\'],\n                     video_info[\'width\'], video_info[\'height\']])\n            else:\n                proposal = proposal * np.array(\n                    [video_info[\'width\'], video_info[\'height\'],\n                     video_info[\'width\'], video_info[\'height\'], 1.0])\n            proposal = proposal.astype(np.float32)\n        else:\n            proposal = None\n\n        def prepare_single(img_group, scale,\n                           crop_quadruple, flip, proposal=None):\n            (_img_group, img_shape, pad_shape,\n             scale_factor, crop_quadruple) = self.img_group_transform(\n                img_group, scale,\n                crop_history=crop_quadruple,\n                 flip=flip,\n                 keep_ratio=self.resize_keep_ratio)\n            _img_group = to_tensor(_img_group)\n            _img_meta = dict(\n                ori_shape=(video_info[\'height\'], video_info[\'width\'], 3),\n                img_shape=img_shape,\n                pad_shape=pad_shape,\n                scale_factor=scale_factor,\n                crop_quadruple=crop_quadruple,\n                flip=flip)\n            if proposal is not None:\n                if proposal.shape[1] == 5:\n                    score = proposal[:, 4, None]\n                    proposal = proposal[:, :4]\n                else:\n                    score = None\n                _proposal = self.bbox_transform(\n                    proposal, img_shape,\n                    scale_factor, flip, crop=crop_quadruple)\n                _proposal = np.hstack(\n                    [_proposal, score] if score is not None else _proposal)\n                _proposal = to_tensor(_proposal)\n            else:\n                _proposal = None\n            return _img_group, _img_meta, _proposal\n\n        indice = video_info[\'fps\'] * \\\n            (video_info[\'timestamp\'] - _TIMESTAMP_START) + 1\n        skip_offsets = np.random.randint(\n            self.new_step, size=self.old_length // self.new_step)\n\n        data = dict(num_modalities=DC(to_tensor(len(self.modalities))))\n\n        for i, (modality, image_tmpl) in enumerate(\n                zip(self.modalities, self.image_tmpls)):\n            img_group = self._get_frames(\n                video_info, image_tmpl, modality, indice, skip_offsets)\n            img_groups = []\n            img_metas = []\n            proposals = []\n            for scale in self.img_scales:\n                _img_group, _img_meta, _proposal = prepare_single(\n                    img_group, scale, None, False, proposal)\n                if self.input_format == ""NCTHW"":\n                    # Convert [L x C x H x W] to [C x L x H x W]\n                    _img_group = np.transpose(_img_group, (1, 0, 2, 3))\n                img_groups.append(_img_group)\n                img_metas.append(DC(_img_meta, cpu_only=True))\n                proposals.append(_proposal)\n                if self.flip_ratio > 0:\n                    _img_group, _img_meta, _proposal = prepare_single(\n                        img_group, scale, None, True, proposal)\n                    if self.input_format == ""NCTHW"":\n                        # Convert [L x C x H x W] to [C x L x H x W]\n                        _img_group = np.transpose(_img_group, (1, 0, 2, 3))\n                    img_groups.append(_img_group)\n                    img_metas.append(DC(_img_meta, cpu_only=True))\n                    proposals.append(_proposal)\n            data[\'img_group_{}\'.format(i)] = img_groups\n            if i == 0:\n                data[\'img_meta\'] = img_metas\n            if self.proposals is not None:\n                data[\'proposals\'] = proposals\n\n        return data\n'"
mmaction/datasets/feature_dataset.py,0,b''
mmaction/datasets/lmdbframes_dataset.py,1,"b'import mmcv\nimport numpy as np\nimport os.path as osp\nfrom mmcv.parallel import DataContainer as DC\nfrom torch.utils.data import Dataset\n\nfrom .transforms import (GroupImageTransform)\nfrom .utils import to_tensor\n\nimport lmdb\n\n\nclass RawFramesRecord(object):\n    def __init__(self, row):\n        self._data = row\n\n    @property\n    def path(self):\n        return self._data[0]\n\n    @property\n    def num_frames(self):\n        return int(self._data[1])\n\n    @property\n    def label(self):\n        return int(self._data[2])\n\n\nclass LMDBFramesDataset(Dataset):\n    def __init__(self,\n                 ann_file,\n                 lmdb_prefix,\n                 img_norm_cfg,\n                 num_segments=3,\n                 new_length=1,\n                 new_step=1,\n                 random_shift=True,\n                 modality=\'RGB\',\n                 lmdb_tmpl=\'{}_img_lmdb\',\n                 key_tmpl=\'img_{:05d}\',\n                 img_scale=256,\n                 input_size=224,\n                 div_255=False,\n                 size_divisor=None,\n                 proposal_file=None,\n                 num_max_proposals=1000,\n                 flip_ratio=0.5,\n                 resize_keep_ratio=True,\n                 resize_ratio=[1, 0.875, 0.75, 0.66],\n                 test_mode=False,\n                 oversample=False,\n                 random_crop=False,\n                 more_fix_crop=False,\n                 multiscale_crop=False,\n                 scales=None,\n                 max_distort=1,\n                 input_format=\'NCHW\'):\n        # prefix of lmdb path\n        self.lmdb_prefix = lmdb_prefix\n\n        # load annotations\n        self.video_infos = self.load_annotations(ann_file)\n\n        # normalization config\n        self.img_norm_cfg = img_norm_cfg\n\n        # parameters for frame fetching\n        # number of segments\n        self.num_segments = num_segments\n        # number of consecutive frames\n        self.old_length = new_length * new_step\n        self.new_length = new_length\n        # number of steps (sparse sampling for efficiency of io)\n        self.new_step = new_step\n        # whether to temporally random shift when training\n        self.random_shift = random_shift\n\n        # parameters for modalities\n        if isinstance(modality, (list, tuple)):\n            self.modalities = modality\n            num_modality = len(modality)\n        else:\n            self.modalities = [modality]\n            num_modality = 1\n        self.lmdb_tmpl = lmdb_tmpl\n        if isinstance(key_tmpl, (list, tuple)):\n            self.key_tmpls = key_tmpl\n        else:\n            self.key_tmpls = [key_tmpl]\n        assert len(self.key_tmpls) == num_modality\n\n        # parameters for image preprocessing\n        # img_scale\n        if isinstance(img_scale, int):\n            img_scale = (np.Inf, img_scale)\n        self.img_scale = img_scale\n        # network input size\n        if isinstance(input_size, int):\n            input_size = (input_size, input_size)\n        self.input_size = input_size\n\n        # parameters for specification from pre-trained networks (lecacy issue)\n        self.div_255 = div_255\n\n        # parameters for data augmentation\n        # flip ratio\n        self.flip_ratio = flip_ratio\n        self.resize_keep_ratio = resize_keep_ratio\n\n        # test mode or not\n        self.test_mode = test_mode\n\n        # set group flag for the sampler\n        if not self.test_mode:\n            self._set_group_flag()\n\n        # transforms\n        self.img_group_transform = GroupImageTransform(\n            size_divisor=None, crop_size=self.input_size,\n            oversample=oversample, random_crop=random_crop,\n            more_fix_crop=more_fix_crop,\n            multiscale_crop=multiscale_crop, scales=scales,\n            max_distort=max_distort,\n            **self.img_norm_cfg)\n\n        # input format\n        assert input_format in [\'NCHW\', \'NCTHW\']\n        self.input_format = input_format\n        \'\'\'\n        self.bbox_transform = Bbox_transform()\n        \'\'\'\n\n    def __len__(self):\n        return len(self.video_infos)\n\n    def load_annotations(self, ann_file):\n        return [RawFramesRecord(x.strip().split(\' \')) for x in open(ann_file)]\n        # return mmcv.load(ann_file)\n\n    def load_proposals(self, proposal_file):\n        return mmcv.load(proposal_file)\n\n    def get_ann_info(self, idx):\n        return {\'path\': self.video_infos[idx].path,\n                \'num_frames\': self.video_infos[idx].num_frames,\n                \'label\': self.video_infos[idx].label}\n        # return self.video_infos[idx][\'ann\']\n\n    def _set_group_flag(self):\n        """"""Set flag according to image aspect ratio.\n\n        Images with aspect ratio greater than 1 will be set as group 1,\n        otherwise group 0.\n        """"""\n        self.flag = np.zeros(len(self), dtype=np.uint8)\n        for i in range(len(self)):\n            # img_info = self.img_infos[i]\n            # if img_info[\'width\'] / img_info[\'height\'] > 1:\n            self.flag[i] = 1\n\n    def _load_image(self, key_tmpl, modality, idx):\n        print(""key_tmpl.format(idx)="", key_tmpl.format(idx))\n        if modality in [\'RGB\', \'RGBDiff\']:\n            img_bytes = self.txn.get(str(key_tmpl.format(idx)).encode())\n            return [mmcv.imfrombytes(img_bytes, flag=\'color\')]\n        elif modality == \'Flow\':\n            img_bytes = self.txn.get(key_tmpl.format(\'x\', idx))\n            x_imgs = mmcv.imfrombytes(img_bytes, flag=\'grayscale\')\n            img_bytes = self.txn.get(key_tmpl.format(\'y\', idx))\n            y_imgs = mmcv.imfrombytes(img_bytes, flag=\'grayscale\')\n            return [x_imgs, y_imgs]\n        else:\n            raise ValueError(\n                \'Not implemented yet; modality\'\n                \'should be [""RGB"", ""RGBDiff"", ""Flow""]\')\n\n    def _sample_indices(self, record):\n        \'\'\'\n\n        :param record: VideoRawFramesRecord\n        :return: list, list\n        \'\'\'\n        average_duration = (record.num_frames -\n                            self.old_length + 1) // self.num_segments\n        if average_duration > 0:\n            offsets = np.multiply(\n                list(range(self.num_segments)), average_duration)\n            offsets = offsets + np.random.randint(\n                average_duration, size=self.num_segments)\n        elif record.num_frames > max(self.num_segments, self.old_length):\n            offsets = np.sort(np.random.randint(\n                record.num_frames - self.old_length + 1,\n                size=self.num_segments))\n        else:\n            offsets = np.zeros((self.num_segments,))\n        skip_offsets = np.random.randint(\n            self.new_step, size=self.old_length // self.new_step)\n        return offsets + 1, skip_offsets  # frame index starts from 1\n\n    def _get_val_indices(self, record):\n        if record.num_frames > self.num_segments + self.old_length - 1:\n            tick = (record.num_frames - self.old_length + 1) / \\\n                float(self.num_segments)\n            offsets = np.array([int(tick / 2.0 + tick * x)\n                                for x in range(self.num_segments)])\n        else:\n            offsets = np.zeros((self.num_segments,))\n        skip_offsets = np.random.randint(\n            self.new_step, size=self.old_length // self.new_step)\n        return offsets + 1, skip_offsets\n\n    def _get_test_indices(self, record):\n        if record.num_frames > self.old_length - 1:\n            tick = (record.num_frames - self.old_length + 1) / \\\n                float(self.num_segments)\n            offsets = np.array([int(tick / 2.0 + tick * x)\n                                for x in range(self.num_segments)])\n        else:\n            offsets = np.zeros((self.num_segments,))\n        skip_offsets = np.random.randint(\n            self.new_step, size=self.old_length // self.new_step)\n        return offsets + 1, skip_offsets\n\n    def _get_frames(self, record, key_tmpl, modality, indices, skip_offsets):\n\n        self.env = lmdb.open(\n            osp.join(self.lmdb_prefix, self.lmdb_tmpl.format(record.path)),\n            max_readers=1,\n            readonly=True,\n            lock=False,\n            readahead=False,\n            meminit=False)\n        self.txn = self.env.begin(write=False)\n\n        images = list()\n        for seg_ind in indices:\n            p = int(seg_ind)\n            for i, ind in enumerate(range(0, self.old_length, self.new_step)):\n                seg_imgs = self._load_image(\n                    key_tmpl, modality, p + skip_offsets[i])\n                images.extend(seg_imgs)\n                if p + self.new_step < record.num_frames:\n                    p += self.new_step\n        return images\n\n    def __getitem__(self, idx):\n        record = self.video_infos[idx]\n        if self.test_mode:\n            segment_indices, skip_offsets = self._get_test_indices(record)\n        else:\n            segment_indices, skip_offsets = self._sample_indices(\n                record) if self.random_shift else self._get_val_indices(record)\n\n        data = dict(num_modalities=DC(to_tensor(len(self.modalities))),\n                    gt_label=DC(to_tensor(record.label),\n                                stack=True, pad_dims=None))\n\n        # handle the first modality\n        modality = self.modalities[0]\n        key_tmpl = self.key_tmpls[0]\n        img_group = self._get_frames(\n            record, key_tmpl, modality, segment_indices, skip_offsets)\n\n        flip = True if np.random.rand() < self.flip_ratio else False\n        (img_group, img_shape, pad_shape,\n         scale_factor, crop_quadruple) = self.img_group_transform(\n            img_group, self.img_scale,\n            crop_history=None,\n            flip=flip, keep_ratio=self.resize_keep_ratio,\n            div_255=self.div_255,\n            is_flow=True if modality == \'Flow\' else False)\n        ori_shape = (256, 340, 3)\n        img_meta = dict(\n            ori_shape=ori_shape,\n            img_shape=img_shape,\n            pad_shape=pad_shape,\n            scale_factor=scale_factor,\n            crop_quadruple=crop_quadruple,\n            flip=flip)\n\n        # [M x C x H x W]\n        # M = 1 * N_oversample * N_seg * L\n        if self.input_format == ""NCTHW"":\n            img_group = img_group.reshape(\n                (-1, self.num_segments, self.new_length) + img_group.shape[1:])\n            # N_over x N_seg x L x C x H x W\n            img_group = np.transpose(img_group, (0, 1, 3, 2, 4, 5))\n            # N_over x N_seg x C x L x H x W\n            img_group = img_group.reshape((-1,) + img_group.shape[2:])\n            # M\' x C x L x H x W\n\n        data.update(dict(\n            img_group_0=DC(to_tensor(img_group), stack=True, pad_dims=2),\n            img_meta=DC(img_meta, cpu_only=True)\n        ))\n\n        # handle the rest modalities using the same\n        for i, (modality, key_tmpl) in enumerate(\n                zip(self.modalities[1:], self.key_tmpls[1:])):\n            img_group = self._get_frames(\n                record, key_tmpl, modality, segment_indices, skip_offsets)\n\n            # apply transforms\n            flip = True if np.random.rand() < self.flip_ratio else False\n            (img_group, img_shape, pad_shape,\n             scale_factor, crop_quadruple) = self.img_group_transform(\n                 img_group, self.img_scale,\n                crop_history=data[\'img_meta\'][\'crop_quadruple\'],\n                flip=data[\'img_meta\'][\'flip\'],\n                keep_ratio=self.resize_keep_ratio,\n                div_255=self.div_255,\n                is_flow=True if modality == \'Flow\' else False)\n\n            if self.input_format == ""NCTHW"":\n                # Convert [M x C x H x W] to [M\' x C x T x H x W]\n                # M = 1 * N_oversample * N_seg * L\n                # M\' = 1 * N_oversample * N_seg, T = L\n                img_group = img_group.reshape(\n                    (-1, self.num_segments,\n                     self.new_length) + img_group.shape[1:])\n                img_group = np.transpose(img_group, (0, 1, 3, 2, 4, 5))\n                img_group = img_group.reshape((-1,) + img_group.shape[2:])\n\n            else:\n                data.update({\n                    \'img_group_{}\'.format(i+1):\n                    DC(to_tensor(img_group), stack=True, pad_dims=2),\n                })\n\n        return data\n'"
mmaction/datasets/rawframes_dataset.py,1,"b'import mmcv\nimport numpy as np\nimport os.path as osp\nfrom mmcv.parallel import DataContainer as DC\nfrom torch.utils.data import Dataset\n\nfrom .transforms import (GroupImageTransform)\nfrom .utils import to_tensor\n\n\nclass RawFramesRecord(object):\n    def __init__(self, row):\n        self._data = row\n\n    @property\n    def path(self):\n        return self._data[0]\n\n    @property\n    def num_frames(self):\n        return int(self._data[1])\n\n    @property\n    def label(self):\n        return int(self._data[2])\n\n\nclass RawFramesDataset(Dataset):\n    def __init__(self,\n                 ann_file,\n                 img_prefix,\n                 img_norm_cfg,\n                 num_segments=3,\n                 new_length=1,\n                 new_step=1,\n                 random_shift=True,\n                 temporal_jitter=False,\n                 modality=\'RGB\',\n                 image_tmpl=\'img_{}.jpg\',\n                 img_scale=256,\n                 img_scale_file=None,\n                 input_size=224,\n                 div_255=False,\n                 size_divisor=None,\n                 proposal_file=None,\n                 num_max_proposals=1000,\n                 flip_ratio=0.5,\n                 resize_keep_ratio=True,\n                 resize_ratio=[1, 0.875, 0.75, 0.66],\n                 test_mode=False,\n                 oversample=None,\n                 random_crop=False,\n                 more_fix_crop=False,\n                 multiscale_crop=False,\n                 resize_crop=False,\n                 rescale_crop=False,\n                 scales=None,\n                 max_distort=1,\n                 input_format=\'NCHW\'):\n        # prefix of images path\n        self.img_prefix = img_prefix\n\n        # load annotations\n        self.video_infos = self.load_annotations(ann_file)\n        # normalization config\n        self.img_norm_cfg = img_norm_cfg\n\n        # parameters for frame fetching\n        # number of segments\n        self.num_segments = num_segments\n        # number of consecutive frames\n        self.old_length = new_length * new_step\n        self.new_length = new_length\n        # number of steps (sparse sampling for efficiency of io)\n        self.new_step = new_step\n        # whether to temporally random shift when training\n        self.random_shift = random_shift\n        # whether to temporally jitter if new_step > 1\n        self.temporal_jitter = temporal_jitter\n\n        # parameters for modalities\n        if isinstance(modality, (list, tuple)):\n            self.modalities = modality\n            num_modality = len(modality)\n        else:\n            self.modalities = [modality]\n            num_modality = 1\n        if isinstance(image_tmpl, (list, tuple)):\n            self.image_tmpls = image_tmpl\n        else:\n            self.image_tmpls = [image_tmpl]\n        assert len(self.image_tmpls) == num_modality\n\n        # parameters for image preprocessing\n        # img_scale\n        if isinstance(img_scale, int):\n            img_scale = (np.Inf, img_scale)\n        self.img_scale = img_scale\n        if img_scale_file is not None:\n            self.img_scale_dict = {line.split(\' \')[0]:\n                                   (int(line.split(\' \')[1]),\n                                    int(line.split(\' \')[2]))\n                                   for line in open(img_scale_file)}\n        else:\n            self.img_scale_dict = None\n        # network input size\n        if isinstance(input_size, int):\n            input_size = (input_size, input_size)\n        self.input_size = input_size\n\n        # parameters for specification from pre-trained networks (lecacy issue)\n        self.div_255 = div_255\n\n        # parameters for data augmentation\n        # flip ratio\n        self.flip_ratio = flip_ratio\n        self.resize_keep_ratio = resize_keep_ratio\n\n        # test mode or not\n        self.test_mode = test_mode\n\n        # set group flag for the sampler\n        # if not self.test_mode:\n        self._set_group_flag()\n\n        # transforms\n        assert oversample in [None, \'three_crop\', \'ten_crop\']\n        self.img_group_transform = GroupImageTransform(\n            size_divisor=None, crop_size=self.input_size,\n            oversample=oversample, random_crop=random_crop,\n            more_fix_crop=more_fix_crop,\n            multiscale_crop=multiscale_crop, scales=scales,\n            max_distort=max_distort,\n            resize_crop=resize_crop,\n            rescale_crop=rescale_crop,\n            **self.img_norm_cfg)\n\n        # input format\n        assert input_format in [\'NCHW\', \'NCTHW\']\n        self.input_format = input_format\n        \'\'\'\n        self.bbox_transform = Bbox_transform()\n        \'\'\'\n\n    def __len__(self):\n        return len(self.video_infos)\n\n    def load_annotations(self, ann_file):\n        return [RawFramesRecord(x.strip().split(\' \')) for x in open(ann_file)]\n        # return mmcv.load(ann_file)\n\n    def load_proposals(self, proposal_file):\n        return mmcv.load(proposal_file)\n\n    def get_ann_info(self, idx):\n        return {\'path\': self.video_infos[idx].path,\n                \'num_frames\': self.video_infos[idx].num_frames,\n                \'label\': self.video_infos[idx].label}\n        # return self.video_infos[idx][\'ann\']\n\n    def _set_group_flag(self):\n        """"""Set flag according to image aspect ratio.\n\n        Images with aspect ratio greater than 1 will be set as group 1,\n        otherwise group 0.\n        """"""\n        self.flag = np.zeros(len(self), dtype=np.uint8)\n        for i in range(len(self)):\n            # img_info = self.img_infos[i]\n            # if img_info[\'width\'] / img_info[\'height\'] > 1:\n            self.flag[i] = 1\n\n    def _load_image(self, directory, image_tmpl, modality, idx):\n        if modality in [\'RGB\', \'RGBDiff\']:\n            return [mmcv.imread(osp.join(directory, image_tmpl.format(idx)))]\n        elif modality == \'Flow\':\n            x_imgs = mmcv.imread(\n                osp.join(directory, image_tmpl.format(\'x\', idx)),\n                flag=\'grayscale\')\n            y_imgs = mmcv.imread(\n                osp.join(directory, image_tmpl.format(\'y\', idx)),\n                flag=\'grayscale\')\n            return [x_imgs, y_imgs]\n        else:\n            raise ValueError(\n                \'Not implemented yet; modality should be \'\n                \'[""RGB"", ""RGBDiff"", ""Flow""]\')\n\n    def _sample_indices(self, record):\n        \'\'\'\n\n        :param record: VideoRawFramesRecord\n        :return: list, list\n        \'\'\'\n        average_duration = (record.num_frames -\n                            self.old_length + 1) // self.num_segments\n        if average_duration > 0:\n            offsets = np.multiply(list(range(self.num_segments)),\n                                  average_duration)\n            offsets = offsets + np.random.randint(average_duration,\n                                                  size=self.num_segments)\n        elif record.num_frames > max(self.num_segments, self.old_length):\n            offsets = np.sort(np.random.randint(\n                record.num_frames - self.old_length + 1,\n                size=self.num_segments))\n        else:\n            offsets = np.zeros((self.num_segments,))\n        if self.temporal_jitter:\n            skip_offsets = np.random.randint(\n                self.new_step, size=self.old_length // self.new_step)\n        else:\n            skip_offsets = np.zeros(\n                self.old_length // self.new_step, dtype=int)\n        return offsets + 1, skip_offsets  # frame index starts from 1\n\n    def _get_val_indices(self, record):\n        if record.num_frames > self.num_segments + self.old_length - 1:\n            tick = (record.num_frames - self.old_length + 1) / \\\n                float(self.num_segments)\n            offsets = np.array([int(tick / 2.0 + tick * x)\n                                for x in range(self.num_segments)])\n        else:\n            offsets = np.zeros((self.num_segments,))\n        if self.temporal_jitter:\n            skip_offsets = np.random.randint(\n                self.new_step, size=self.old_length // self.new_step)\n        else:\n            skip_offsets = np.zeros(\n                self.old_length // self.new_step, dtype=int)\n        return offsets + 1, skip_offsets\n\n    def _get_test_indices(self, record):\n        if record.num_frames > self.old_length - 1:\n            tick = (record.num_frames - self.old_length + 1) / \\\n                float(self.num_segments)\n            offsets = np.array([int(tick / 2.0 + tick * x)\n                                for x in range(self.num_segments)])\n        else:\n            offsets = np.zeros((self.num_segments,))\n        if self.temporal_jitter:\n            skip_offsets = np.random.randint(\n                self.new_step, size=self.old_length // self.new_step)\n        else:\n            skip_offsets = np.zeros(\n                self.old_length // self.new_step, dtype=int)\n        return offsets + 1, skip_offsets\n\n    def _get_frames(self, record, image_tmpl, modality, indices, skip_offsets):\n        images = list()\n        for seg_ind in indices:\n            p = int(seg_ind)\n            for i, ind in enumerate(range(0, self.old_length, self.new_step)):\n                if p + skip_offsets[i] <= record.num_frames:\n                    seg_imgs = self._load_image(osp.join(\n                        self.img_prefix, record.path),\n                        image_tmpl, modality, p + skip_offsets[i])\n                else:\n                    seg_imgs = self._load_image(\n                        osp.join(self.img_prefix, record.path),\n                        image_tmpl, modality, p)\n                images.extend(seg_imgs)\n                if p + self.new_step < record.num_frames:\n                    p += self.new_step\n        return images\n\n    def __getitem__(self, idx):\n        record = self.video_infos[idx]\n        if self.test_mode:\n            segment_indices, skip_offsets = self._get_test_indices(record)\n        else:\n            segment_indices, skip_offsets = self._sample_indices(\n                record) if self.random_shift else self._get_val_indices(record)\n\n        data = dict(num_modalities=DC(to_tensor(len(self.modalities))),\n                    gt_label=DC(to_tensor(record.label), stack=True,\n                                pad_dims=None))\n\n        # handle the first modality\n        modality = self.modalities[0]\n        image_tmpl = self.image_tmpls[0]\n        img_group = self._get_frames(\n            record, image_tmpl, modality, segment_indices, skip_offsets)\n\n        flip = True if np.random.rand() < self.flip_ratio else False\n        if (self.img_scale_dict is not None\n                and record.path in self.img_scale_dict):\n            img_scale = self.img_scale_dict[record.path]\n        else:\n            img_scale = self.img_scale\n        (img_group, img_shape, pad_shape,\n         scale_factor, crop_quadruple) = self.img_group_transform(\n            img_group, img_scale,\n            crop_history=None,\n            flip=flip, keep_ratio=self.resize_keep_ratio,\n            div_255=self.div_255,\n             is_flow=True if modality == \'Flow\' else False)\n        ori_shape = (256, 340, 3)\n        img_meta = dict(\n            ori_shape=ori_shape,\n            img_shape=img_shape,\n            pad_shape=pad_shape,\n            scale_factor=scale_factor,\n            crop_quadruple=crop_quadruple,\n            flip=flip)\n        # [M x C x H x W]\n        # M = 1 * N_oversample * N_seg * L\n        if self.input_format == ""NCTHW"":\n            img_group = img_group.reshape(\n                (-1, self.num_segments, self.new_length) + img_group.shape[1:])\n            # N_over x N_seg x L x C x H x W\n            img_group = np.transpose(img_group, (0, 1, 3, 2, 4, 5))\n            # N_over x N_seg x C x L x H x W\n            img_group = img_group.reshape((-1,) + img_group.shape[2:])\n            # M\' x C x L x H x W\n\n        data.update(dict(\n            img_group_0=DC(to_tensor(img_group), stack=True, pad_dims=2),\n            img_meta=DC(img_meta, cpu_only=True)\n        ))\n\n        # handle the rest modalities using the same\n        for i, (modality, image_tmpl) in enumerate(\n                zip(self.modalities[1:], self.image_tmpls[1:])):\n            img_group = self._get_frames(\n                record, image_tmpl, modality, segment_indices, skip_offsets)\n\n            # apply transforms\n            flip = True if np.random.rand() < self.flip_ratio else False\n            (img_group, img_shape, pad_shape,\n             scale_factor, crop_quadruple) = self.img_group_transform(\n                img_group, img_scale,\n                crop_history=data[\'img_meta\'][\n                    \'crop_quadruple\'],\n                flip=data[\'img_meta\'][\n                    \'flip\'], keep_ratio=self.resize_keep_ratio,\n                div_255=self.div_255,\n                is_flow=True if modality == \'Flow\' else False)\n\n            if self.input_format == ""NCTHW"":\n                # Convert [M x C x H x W] to [M\' x C x T x H x W]\n                # M = 1 * N_oversample * N_seg * L\n                # M\' = 1 * N_oversample * N_seg, T = L\n                img_group = img_group.reshape(\n                    (-1, self.num_segments,\n                     self.new_length) + img_group.shape[1:])\n                img_group = np.transpose(img_group, (0, 1, 3, 2, 4, 5))\n                img_group = img_group.reshape((-1,) + img_group.shape[2:])\n\n            data.update({\n                \'img_group_{}\'.format(i+1):\n                DC(to_tensor(img_group), stack=True, pad_dims=2),\n            })\n\n        return data\n'"
mmaction/datasets/ssn_dataset.py,1,"b'import mmcv\nimport numpy as np\nimport math\nimport os.path as osp\nfrom mmcv.parallel import DataContainer as DC\nfrom torch.utils.data import Dataset\n\nfrom .transforms import (GroupImageTransform)\nfrom .utils import (to_tensor, parse_directory,\n                    process_localize_proposal_list,\n                    load_localize_proposal_file)\n\nfrom mmaction.core.bbox1d import temporal_iou\n\n\nclass SSNInstance(object):\n    def __init__(self, start_frame, end_frame, video_frame_count,\n                 fps=1, label=None,\n                 best_iou=None, overlap_self=None):\n        self.start_frame = start_frame\n        self.end_frame = min(end_frame, video_frame_count)\n        self._label = label\n        self.fps = fps\n\n        self.coverage = (end_frame - start_frame) / video_frame_count\n\n        self.best_iou = best_iou\n        self.overlap_self = overlap_self\n\n        self.loc_reg = None\n        self.size_reg = None\n\n    def compute_regression_targets(self, gt_list, fg_thresh):\n        if self.best_iou < fg_thresh:\n            # background proposals do not need this\n            return\n\n        # find the groundtruth instance with the highest IOU\n        ious = [temporal_iou((self.start_frame, self.end_frame),\n                             (gt.start_frame, gt.end_frame)) for gt in gt_list]\n        best_gt_id = np.argmax(ious)\n\n        best_gt = gt_list[best_gt_id]\n\n        prop_center = (self.start_frame + self.end_frame) / 2\n        gt_center = (best_gt.start_frame + best_gt.end_frame) / 2\n\n        prop_size = self.end_frame - self.start_frame + 1\n        gt_size = best_gt.end_frame - best_gt.start_frame + 1\n\n        # get regression target:\n        # (1). center shift proportional to the proposal duration\n        # (2). logairthm of the groundtruth duration over proposal duration\n\n        self.loc_reg = (gt_center - prop_center) / prop_size\n        try:\n            self.size_reg = math.log(gt_size / prop_size)\n        except ValueError:\n            print(gt_size, prop_size, self.start_frame, self.end_frame)\n            raise ValueError(""gt_size / prop_size should be valid."")\n\n    @property\n    def start_time(self):\n        return self.start_frame / self.fps\n\n    @property\n    def end_time(self):\n        return self.end_frame / self.fps\n\n    @property\n    def label(self):\n        return self._label if self._label is not None else -1\n\n    @property\n    def regression_targets(self):\n        target = ([self.loc_reg, self.size_reg] if self.loc_reg is not None\n                  else [0., 0.])\n        return target\n\n\nclass SSNVideoRecord(object):\n    def __init__(self, prop_record):\n        self._data = prop_record\n\n        frame_count = int(self._data[1])\n\n        self.gt = [\n            SSNInstance(int(x[1]), int(x[2]), frame_count, label=int(x[0]),\n                        best_iou=1.0)\n            for x in self._data[2] if int(x[2]) > int(x[1])\n        ]\n\n        self.gt = list(filter(lambda x: x.start_frame < frame_count, self.gt))\n\n        self.proposals = [\n            SSNInstance(int(x[3]), int(x[4]), frame_count, label=int(x[0]),\n                        best_iou=float(x[1]), overlap_self=float(x[2]))\n            for x in self._data[3] if int(x[4]) > int(x[3])\n        ]\n\n        self.proposals = list(\n            filter(lambda x: x.start_frame < frame_count, self.proposals))\n\n    @property\n    def video_id(self):\n        return self._data[0]\n\n    @property\n    def num_frames(self):\n        return int(self._data[1])\n\n    def get_fg(self, fg_thresh, with_gt=True):\n        fg = [p for p in self.proposals if p.best_iou > fg_thresh]\n        if with_gt:\n            fg.extend(self.gt)\n\n        for x in fg:\n            x.compute_regression_targets(self.gt, fg_thresh)\n\n        return fg\n\n    def get_negatives(self, incomplete_iou_thresh, bg_iou_thresh,\n                      bg_coverage_thresh=0.01, incomplete_overlap_thresh=0.7):\n        tag = [0] * len(self.proposals)\n\n        incomplete_props = []\n        background_props = []\n\n        for i in range(len(tag)):\n            if self.proposals[i].best_iou < incomplete_iou_thresh and \\\n                    self.proposals[i].overlap_self > incomplete_overlap_thresh:\n                tag[i] = 1  # incomplete\n                incomplete_props.append(self.proposals[i])\n\n        for i in range(len(tag)):\n            if tag[i] == 0 and \\\n                    self.proposals[i].best_iou < bg_iou_thresh and \\\n                    self.proposals[i].coverage > bg_coverage_thresh:\n                background_props.append(self.proposals[i])\n        return incomplete_props, background_props\n\n\nclass SSNDataset(Dataset):\n    def __init__(self,\n                 ann_file,\n                 img_prefix,\n                 img_norm_cfg,\n                 train_cfg,\n                 test_cfg,\n                 video_centric=True,\n                 reg_stats=None,\n                 body_seg=5,\n                 aug_seg=2,\n                 aug_ratio=(0.5, 0.5),\n                 new_length=1,\n                 new_step=1,\n                 random_shift=True,\n                 modality=\'RGB\',\n                 image_tmpl=\'img_{}.jpg\',\n                 img_scale=256,\n                 input_size=None,\n                 div_255=False,\n                 size_divisor=None,\n                 flip_ratio=0.5,\n                 resize_keep_ratio=True,\n                 resize_ratio=[1, 0.875, 0.75, 0.66],\n                 filter_gt=True,\n                 test_mode=False,\n                 oversample=None,\n                 random_crop=False,\n                 more_fix_crop=False,\n                 multiscale_crop=False,\n                 scales=None,\n                 max_distort=1,\n                 input_format=\'NCHW\',\n                 verbose=False):\n        # prefix of images path\n        self.img_prefix = img_prefix\n\n        # normalization config\n        self.img_norm_cfg = img_norm_cfg\n\n        self.train_cfg = train_cfg\n        self.test_cfg = test_cfg\n\n        self.verbose = verbose\n\n        # load annotations\n        if \'normalized_\' in ann_file:\n            self.proposal_file = ann_file.replace(\'normalized_\', \'\')\n            if not osp.exists(self.proposal_file):\n                print(\'{} does not exist. Converting from {}\'.format(\n                    self.proposal_file, ann_file))\n                frame_dict = parse_directory(\n                    self.img_prefix, key_func=lambda x: x.split(\'/\')[-1])\n                process_localize_proposal_list(\n                    ann_file, self.proposal_file, frame_dict)\n                print(\'Finished conversion.\')\n        else:\n            self.proposal_file = ann_file\n\n        proposal_infos = load_localize_proposal_file(self.proposal_file)\n        self.video_infos = [SSNVideoRecord(p) for p in proposal_infos]\n\n        # filter videos with no annotation during training\n        if filter_gt or not test_mode:\n            valid_inds = self._filter_records()\n        print(""{} out of {} videos are valid."".format(\n            len(valid_inds), len(self.video_infos)))\n        self.video_infos = [self.video_infos[i] for i in valid_inds]\n\n        self.video_dict = {\n            record.video_id: record for record in self.video_infos}\n\n        # construct three pools:\n        # 1. Foreground\n        # 2. Background\n        # 3. Incomplete\n        self.fg_pool = []\n        self.bg_pool = []\n        self.incomp_pool = []\n\n        for v in self.video_infos:\n            self.fg_pool.extend([(v.video_id, prop)\n                                 for prop in v.get_fg(\n                                     self.train_cfg.ssn.assigner.fg_iou_thr,\n                                     self.train_cfg.ssn.sampler.\n                                     add_gt_as_proposals)])\n            incomp, bg = v.get_negatives(\n                self.train_cfg.ssn.assigner.incomplete_iou_thr,\n                self.train_cfg.ssn.assigner.bg_iou_thr,\n                self.train_cfg.ssn.assigner.bg_coverage_thr,\n                self.train_cfg.ssn.assigner.incomplete_overlap_thr)\n            self.incomp_pool.extend([(v.video_id, prop) for prop in incomp])\n            self.bg_pool.extend([v.video_id, prop] for prop in bg)\n\n        if reg_stats is None:\n            self.reg_stats = self._compute_regression_stats()\n        else:\n            self.reg_stats = reg_stats\n\n        self.video_centric = video_centric\n\n        self.body_seg = body_seg\n        self.aug_seg = aug_seg\n\n        if isinstance(aug_ratio, (int, float)):\n            self.aug_ratio = (aug_ratio, aug_ratio)\n        else:\n            assert isinstance(aug_ratio, (tuple, list))\n            assert len(aug_ratio) == 2\n            self.aug_ratio = aug_ratio\n\n        denum = self.train_cfg.ssn.sampler.fg_ratio + \\\n            self.train_cfg.ssn.sampler.bg_ratio + \\\n            self.train_cfg.ssn.sampler.incomplete_ratio\n        self.fg_per_video = int(self.train_cfg.ssn.sampler.num_per_video *\n                                (self.train_cfg.ssn.sampler.fg_ratio / denum))\n        self.bg_per_video = int(self.train_cfg.ssn.sampler.num_per_video *\n                                (self.train_cfg.ssn.sampler.bg_ratio / denum))\n        self.incomplete_per_video = self.train_cfg.ssn.sampler.num_per_video -\\\n            self.fg_per_video - self.bg_per_video\n\n        self.test_interval = self.test_cfg.ssn.sampler.test_interval\n\n        # parameters for frame fetching\n        # number of consecutive frames\n        self.old_length = new_length * new_step\n        self.new_length = new_length\n        # number of steps (sparse sampling for efficiency of io)\n        self.new_step = new_step\n        # whether to temporally random shift when training\n        self.random_shift = random_shift\n\n        # parameters for modalities\n        if isinstance(modality, (list, tuple)):\n            self.modalities = modality\n            num_modality = len(modality)\n        else:\n            self.modalities = [modality]\n            num_modality = 1\n        if isinstance(image_tmpl, (list, tuple)):\n            self.image_tmpls = image_tmpl\n        else:\n            self.image_tmpls = [image_tmpl]\n        assert len(self.image_tmpls) == num_modality\n\n        # parameters for image preprocessing\n        # img_scale\n        if isinstance(img_scale, int):\n            img_scale = (np.Inf, img_scale)\n        self.img_scale = img_scale\n        # network input size\n        if isinstance(input_size, int):\n            input_size = (input_size, input_size)\n        self.input_size = input_size\n\n        # parameters for specification from pre-trained networks (lecacy issue)\n        self.div_255 = div_255\n\n        # flip ratio\n        self.flip_ratio = flip_ratio\n        self.resize_keep_ratio = resize_keep_ratio\n\n        # test mode or not\n        self.filter_gt = filter_gt\n        self.test_mode = test_mode\n\n        # set group flag for the sampler\n        if not self.test_mode:\n            self._set_group_flag()\n\n        # transforms\n        assert oversample in [None, \'three_crop\', \'ten_crop\']\n        self.oversample = oversample\n        self.img_group_transform = GroupImageTransform(\n            size_divisor=None, crop_size=self.input_size,\n            oversample=oversample, random_crop=random_crop,\n            more_fix_crop=more_fix_crop,\n            multiscale_crop=multiscale_crop, scales=scales,\n            max_distort=max_distort,\n            **self.img_norm_cfg)\n\n        # input format\n        assert input_format in [\'NCHW\', \'NCTHW\']\n        self.input_format = input_format\n\n        if self.verbose:\n            print(""""""\n            SSNDataset: proposal file {prop_file} parsed.\n\n            There are {pnum} usable proposals from {vnum} videos.\n            {fnum} foreground proposals\n            {inum} incomplete proposals\n            {bnum} background proposals\n\n            Sample config:\n            FG/BG/INC: {fr}/{br}/{ir}\n            Video Centric: {vc}\n\n            Regression Stats:\n            Location: mean {stats[0][0]:.05f} std {stats[1][0]:.05f}\n            Duration: mean {stats[0][1]:.05f} std {stats[1][1]:.05f}\n            """""".format(prop_file=self.proposal_file,\n                       pnum=len(self.fg_pool) + len(self.bg_pool) +\n                       len(self.incomp_pool),\n                       fnum=len(self.fg_pool), inum=len(self.incomp_pool),\n                       bnum=len(self.bg_pool),\n                       fr=self.fg_per_video, br=self.bg_per_video,\n                       ir=self.incomplete_per_video,\n                       vnum=len(self.video_infos), vc=self.video_centric,\n                       stats=self.reg_stats))\n        else:\n            print(""""""\n            SSNDataset: proposal file {prop_file} parsed.\n            """""".format(prop_file=self.proposal_file))\n\n    def __len__(self):\n        return len(self.video_infos)\n\n    def get_ann_info(self, idx):\n        return self.video_infos[idx]\n\n    def get_all_gt(self):\n        gt_list = []\n        for video in self.video_infos:\n            vid = video.video_id\n            gt_list.extend(\n                [[vid, x.label - 1, x.start_frame / video.num_frames,\n                  x.end_frame / video.num_frames] for x in video.gt])\n        return gt_list\n\n    def _filter_records(self):\n        valid_inds = []\n        for i, x in enumerate(self.video_infos):\n            if len(x.gt) > 0:\n                valid_inds.append(i)\n        return valid_inds\n\n    def _set_group_flag(self):\n        """"""Set flag according to image aspect ratio.\n\n        Images with aspect ratio greater than 1 will be set as group 1,\n        otherwise group 0.\n        """"""\n        self.flag = np.zeros(len(self), dtype=np.uint8)\n        for i in range(len(self)):\n            # img_info = self.img_infos[i]\n            # if img_info[\'width\'] / img_info[\'height\'] > 1:\n            self.flag[i] = 1\n\n    def _load_image(self, directory, image_tmpl, modality, idx):\n        if modality in [\'RGB\', \'RGBDiff\']:\n            return [mmcv.imread(osp.join(directory, image_tmpl.format(idx)))]\n        elif modality == \'Flow\':\n            x_imgs = mmcv.imread(\n                osp.join(directory, image_tmpl.format(\'x\', idx)),\n                flag=\'grayscale\')\n            y_imgs = mmcv.imread(\n                osp.join(directory, image_tmpl.format(\'y\', idx)),\n                flag=\'grayscale\')\n            return [x_imgs, y_imgs]\n        else:\n            raise ValueError(\n                \'Not implemented yet; modality should be \'\n                \'[""RGB"", ""RGBDiff"", ""Flow""]\')\n\n    def _video_centric_sampling(self, record):\n        fg = record.get_fg(self.train_cfg.ssn.assigner.fg_iou_thr,\n                           self.train_cfg.ssn.sampler.add_gt_as_proposals)\n        incomp, bg = record.get_negatives(\n            self.train_cfg.ssn.assigner.incomplete_iou_thr,\n            self.train_cfg.ssn.assigner.bg_iou_thr,\n            self.train_cfg.ssn.assigner.bg_coverage_thr,\n            self.train_cfg.ssn.assigner.incomplete_overlap_thr)\n\n        def sample_video_proposals(proposal_type, video_id, video_pool,\n                                   requested_num, dataset_pool):\n            if len(video_pool) == 0:\n                # if there is nothing in the video pool,\n                # go fetch from the dataset pool\n                return [(dataset_pool[x], proposal_type)\n                        for x in np.random.choice(\n                        len(dataset_pool), requested_num, replace=False)]\n            else:\n                replicate = len(video_pool) < requested_num\n                idx = np.random.choice(\n                    len(video_pool), requested_num, replace=replicate)\n                return [((video_id, video_pool[x]), proposal_type)\n                        for x in idx]\n\n        out_props = []\n        out_props.extend(sample_video_proposals(\n            0, record.video_id, fg, self.fg_per_video, self.fg_pool))\n        out_props.extend(sample_video_proposals(\n            1, record.video_id, incomp,\n            self.incomplete_per_video, self.incomp_pool))\n        out_props.extend(sample_video_proposals(\n            2, record.video_id, bg, self.bg_per_video, self.bg_pool))\n\n        return out_props\n\n    def _random_sampling(self):\n        out_props = []\n\n        out_props.extend([(x, 0) for x in np.random.choice(\n            self.fg_pool, self.fg_per_video, replace=False)])\n        out_props.extend([(x, 1) for x in np.random.choice(\n            self.incomp_pool, self.incomplete_per_video, replace=False)])\n        out_props.extend([(x, 2) for x in np.random.choice(\n            self.bg_pool, self.bg_per_video, replace=False)])\n\n        return out_props\n\n    def __getitem__(self, idx):\n        if self.test_mode:\n            return self.prepare_test_imgs(idx)\n        else:\n            return self.prepare_train_imgs(idx)\n\n    def _compute_regression_stats(self):\n        if self.verbose:\n            print(\'Computing regression target normlizing constants\')\n        targets = []\n        for video in self.video_infos:\n            fg = video.get_fg(self.train_cfg.ssn.assigner.fg_iou_thr, False)\n            for p in fg:\n                targets.append(list(p.regression_targets))\n\n        return np.array((np.mean(targets, axis=0), np.std(targets, axis=0)))\n\n    def _sample_indices(self, valid_length, num_seg):\n        average_duration = (valid_length + 1) // num_seg\n        if average_duration > 0:\n            offsets = np.multiply(list(range(num_seg)), average_duration) + \\\n                np.random.randint(average_duration, size=num_seg)\n        elif valid_length > num_seg:\n            offsets = np.sort(np.random.randint(valid_length, size=num_seg))\n        else:\n            offsets = np.zeros((num_seg, ))\n\n        return offsets\n\n    def _get_val_indices(self, valid_length, num_seg):\n        if valid_length > num_seg:\n            tick = valid_length / float(num_seg)\n            offsets = np.array([int(tick / 2.0 + tick * x)\n                                for x in range(num_seg)])\n        else:\n            offsets = np.zeros((num_seg, ))\n\n        return offsets\n\n    def _sample_ssn_indices(self, prop, frame_cnt):\n        start_frame = prop.start_frame + 1\n        end_frame = prop.end_frame\n\n        duration = end_frame - start_frame + 1\n        assert duration != 0\n        valid_length = duration - self.old_length\n\n        valid_starting = max(\n            1, start_frame - int(duration * self.aug_ratio[0]))\n        valid_ending = min(frame_cnt - self.old_length + 1,\n                           end_frame + int(duration * self.aug_ratio[1]))\n\n        valid_starting_length = start_frame - valid_starting - \\\n            self.old_length + 1\n        valid_ending_length = valid_ending - end_frame - self.old_length + 1\n\n        starting_scale = (valid_starting_length +\n                          self.old_length + 1) / (duration * self.aug_ratio[0])\n        ending_scale = (valid_ending_length + self.old_length +\n                        1) / (duration * self.aug_ratio[1])\n\n        starting_offsets = self._sample_indices(\n            valid_starting_length, self.aug_seg[0]) if self.random_shift \\\n            else self._get_val_indices(valid_starting_length, self.aug_seg[0])\n        starting_offsets += valid_starting\n        course_offsets = self._sample_indices(\n            valid_length, self.body_seg) if self.random_shift \\\n            else self._get_val_indices(valid_length, self.body_seg)\n        course_offsets += start_frame\n        ending_offsets = self._sample_indices(\n            valid_ending_length, self.aug_seg[1]) if self.random_shift \\\n            else self._get_val_indices(valid_ending_length, self.aug_seg[1])\n        ending_offsets += end_frame\n\n        offsets = np.concatenate(\n            (starting_offsets, course_offsets, ending_offsets))\n        stage_split = [self.aug_seg[0], self.aug_seg[0] + self.body_seg,\n                       self.aug_seg[0] + self.body_seg + self.aug_seg[1]]\n        return offsets, starting_scale, ending_scale, stage_split\n\n    def prepare_train_imgs(self, idx):\n        if self.video_centric:\n            video_info = self.video_infos[idx]\n            props = self._video_centric_sampling(video_info)\n        else:\n            props = self._random_sampling()\n\n        out_frames = []\n        for _ in range(len(self.modalities)):\n            out_frames.append([])\n        out_prop_scaling = []\n        out_prop_type = []\n        out_prop_labels = []\n        out_prop_reg_targets = []\n        out_img_meta = []\n\n        skip_offsets = np.random.randint(\n            self.new_step, size=self.old_length // self.new_step)\n\n        data = dict(num_modalities=DC(to_tensor(len(self.modalities))))\n\n        for idx, prop in enumerate(props):\n            frame_cnt = self.video_dict[prop[0][0]].num_frames\n\n            (prop_indices, starting_scale, ending_scale,\n             stage_split) = self._sample_ssn_indices(prop[0][1], frame_cnt)\n\n            modality = self.modalities[0]\n            image_tmpl = self.image_tmpls[0]\n\n            # handle the first modality\n            img_group = []\n            for idx, seg_ind in enumerate(prop_indices):\n                for i, x in enumerate(\n                        range(0, self.old_length, self.new_step)):\n                    img_group.extend(self._load_image(\n                        osp.join(self.img_prefix, prop[0][0]),\n                        image_tmpl, modality,\n                        min(frame_cnt, int(seg_ind) + x + skip_offsets[i])))\n\n            flip = True if np.random.rand() < self.flip_ratio else False\n            (img_group, img_shape, pad_shape,\n             scale_factor, crop_quadruple) = self.img_group_transform(\n                img_group, self.img_scale,\n                crop_history=None,\n                flip=flip, keep_ratio=self.resize_keep_ratio,\n                div_255=self.div_255,\n                is_flow=True if modality == \'Flow\' else False)\n            ori_shape = (256, 340, 3)\n            img_meta = dict(\n                ori_shape=ori_shape,\n                img_shape=img_shape,\n                pad_shape=pad_shape,\n                scale_factor=scale_factor,\n                crop_quadruple=crop_quadruple,\n                flip=flip)\n            out_img_meta.append(img_meta)\n            # [L x C x H x W]\n            if self.input_format == ""NCTHW"":\n                img_group = np.transpose(img_group, (1, 0, 2, 3))\n            out_frames[0].append(img_group)\n\n            for i, (modality, image_tmpl) in enumerate(\n                    zip(self.modalities[1:], self.image_tmpls[1:])):\n                img_group = []\n                for idx, seg_ind in enumerate(prop_indices):\n                    for x in range(0, self.old_length, self.new_step):\n                        img_group.extend(self._load_image(osp.join(\n                            self.img_prefix, prop[0][0]), image_tmpl, modality,\n                            int(seg_ind) + x + skip_offsets[i]))\n\n                flip = True if np.random.rand() < self.flip_ratio else False\n                (img_group, img_shape, pad_shape,\n                 scale_factor, crop_quadruple) = self.img_group_transform(\n                    img_group, self.img_scale,\n                    crop_history=img_meta[\'crop_quadruple\'],\n                    flip=img_meta[\'flip\'], keep_ratio=self.resize_keep_ratio,\n                    div_255=self.div_255,\n                    is_flow=True if modality == \'Flow\' else False)\n                # [L x C x H x W]\n                if self.input_format == ""NCTHW"":\n                    img_group = np.transpose(img_group, (1, 0, 2, 3))\n                out_frames[i + 1].append(img_group)\n\n            if prop[1] == 0:\n                label = prop[0][1].label\n            elif prop[1] == 1:\n                label = prop[0][1].label\n            elif prop[1] == 2:\n                label = 0\n            else:\n                raise ValueError(""proposal type should be 0, 1, or 2"")\n            out_prop_scaling.append([starting_scale, ending_scale])\n            out_prop_labels.append(label)\n            out_prop_type.append(prop[1])\n\n            if prop[1] == 0:\n                reg_targets = prop[0][1].regression_targets\n                reg_targets = ((reg_targets[0] - self.reg_stats[0][0]) / \\\n                    self.reg_stats[1][0],\n                (reg_targets[1] - self.reg_stats[0][1]) / \\\n                    self.reg_stats[1][1])\n            else:\n                reg_targets = (0., 0.)\n\n            out_prop_reg_targets.append(reg_targets)\n\n        for i in range(len(out_frames)):\n            out_frames[i] = np.array(out_frames[i])\n\n        data.update({\n            \'img_group_0\': DC(to_tensor(out_frames[0]), stack=True,\n                              pad_dims=2),\n            \'img_meta\': DC(out_img_meta, cpu_only=True)\n        })\n\n        for i, (modality, image_tmpl) in enumerate(\n                zip(self.modalities[1:], self.image_tmpls[1:])):\n            data.update({\n                \'img_group_{}\'.format(i+1): DC(to_tensor(out_frames[i+1]),\n                                               stack=True, pad_dims=2)\n            })\n\n        data[\'reg_targets\'] = DC(to_tensor(\n            np.array(out_prop_reg_targets, dtype=np.float32)),\n            stack=True, pad_dims=None)\n        data[\'prop_scaling\'] = DC(to_tensor(\n            np.array(out_prop_scaling, dtype=np.float32)),\n            stack=True, pad_dims=None)\n        data[\'prop_labels\'] = DC(\n            to_tensor(np.array(out_prop_labels)), stack=True, pad_dims=None)\n        data[\'prop_type\'] = DC(\n            to_tensor(np.array(out_prop_type)), stack=True, pad_dims=None)\n\n        return data\n\n    def prepare_test_imgs(self, idx):\n        video_info = self.video_infos[idx]\n\n        props = video_info.proposals\n        video_id = video_info.video_id\n        frame_cnt = video_info.num_frames\n        frame_ticks = np.arange(\n            0, frame_cnt - self.old_length, self.test_interval, dtype=int) + 1\n\n        num_sampled_frames = len(frame_ticks)\n\n        if len(props) == 0:\n            props.append(SSNInstance(0, frame_cnt - 1, frame_cnt))\n\n        rel_prop_list = []\n        proposal_tick_list = []\n        scaling_list = []\n        out_frames = []\n        for _ in range(len(self.modalities)):\n            out_frames.append([])\n        out_img_meta = []\n        for proposal in props:\n            rel_prop = (proposal.start_frame / frame_cnt,\n                        proposal.end_frame / frame_cnt)\n            rel_duration = rel_prop[1] - rel_prop[0]\n            rel_starting_duration = rel_duration * self.aug_ratio[0]\n            rel_ending_duration = rel_duration * self.aug_ratio[1]\n            rel_starting = rel_prop[0] - rel_starting_duration\n            rel_ending = rel_prop[1] + rel_ending_duration\n\n            real_rel_starting = max(0.0, rel_starting)\n            real_rel_ending = min(1.0, rel_ending)\n\n            starting_scaling = (\n                rel_prop[0] - real_rel_starting) / rel_starting_duration\n            ending_scaling = (real_rel_ending -\n                              rel_prop[1]) / rel_ending_duration\n\n            proposal_ticks = (int(real_rel_starting * num_sampled_frames),\n                              int(rel_prop[0] * num_sampled_frames),\n                              int(rel_prop[1] * num_sampled_frames),\n                              int(real_rel_ending * num_sampled_frames))\n\n            rel_prop_list.append(rel_prop)\n            proposal_tick_list.append(proposal_ticks)\n            scaling_list.append((starting_scaling, ending_scaling))\n\n        data = dict(num_modalities=DC(to_tensor(len(self.modalities))))\n\n        modality = self.modalities[0]\n        image_tmpl = self.image_tmpls[0]\n\n        skip_offsets = np.random.randint(\n            self.new_step, size=self.old_length // self.new_step)\n\n        # handle the first modality\n        img_group = []\n        for idx, seg_ind in enumerate(frame_ticks):\n            for x in range(0, self.old_length, self.new_step):\n                img_group.extend(self._load_image(\n                    osp.join(self.img_prefix, video_id), image_tmpl,\n                    modality, int(seg_ind) + x))\n\n        flip = True if np.random.rand() < self.flip_ratio else False\n        (img_group, img_shape, pad_shape,\n         scale_factor, crop_quadruple) = self.img_group_transform(\n            img_group, self.img_scale,\n            crop_history=None,\n            flip=flip, keep_ratio=self.resize_keep_ratio,\n            div_255=self.div_255,\n            is_flow=True if modality == \'Flow\' else False)\n        ori_shape = (256, 340, 3)\n        img_meta = dict(\n            ori_shape=ori_shape,\n            img_shape=img_shape,\n            pad_shape=pad_shape,\n            scale_factor=scale_factor,\n            crop_quadruple=crop_quadruple,\n            flip=flip)\n        out_img_meta.append(img_meta)\n        # [L x C x H x W]\n        if self.input_format == ""NCTHW"":\n            img_group = np.transpose(img_group, (1, 0, 2, 3))\n        out_frames[0].append(img_group)\n\n        for i, (modality, image_tmpl) in enumerate(\n                zip(self.modalities[1:], self.image_tmpls[1:])):\n            img_group = []\n            for idx, seg_ind in enumerate(frame_ticks):\n                for j, x in enumerate(\n                        range(0, self.old_length, self.new_step)):\n                    img_group.extend(self._load_image(osp.join(\n                        self.img_prefix, video_id), image_tmpl, modality,\n                        int(seg_ind) + x + skip_offsets[j]))\n\n            flip = True if np.random.rand() < self.flip_ratio else False\n            (img_group, img_shape, pad_shape,\n             scale_factor, crop_quadruple) = self.img_group_transform(\n                img_group, self.img_scale,\n                crop_history=img_meta[\'crop_quadruple\'],\n                flip=img_meta[\'flip\'], keep_ratio=self.resize_keep_ratio,\n                div_255=self.div_255,\n                is_flow=True if modality == \'Flow\' else False)\n            # [L x C x H x W]\n            if self.input_format == ""NCTHW"":\n                img_group = np.transpose(img_group, (1, 0, 2, 3))\n            out_frames[i + 1].append(img_group)\n\n        for i in range(len(out_frames)):\n            if self.oversample == \'ten_crop\':\n                num_crop = 10\n            elif self.oversample == \'three_crop\':\n                num_crop = 3\n            else:\n                num_crop = 1\n            out_frames[i] = np.array(out_frames[i])\n            out_frames[i] = out_frames[i].reshape(\n                (num_crop, -1) + out_frames[i].shape[2:])\n\n        data.update({\n            \'img_group_0\': DC(to_tensor(out_frames[0]), cpu_only=True),\n            \'img_meta\': DC(out_img_meta, cpu_only=True)\n        })\n\n        for i, (modality, image_tmpl) in enumerate(\n                zip(self.modalities[1:], self.image_tmpls[1:])):\n            data.update({\n                \'img_group_{}\'.format(i+1):\n                DC(to_tensor(out_frames[i+1]), cpu_only=True)\n            })\n\n        data[\'rel_prop_list\'] = DC(to_tensor(\n            np.array(rel_prop_list, dtype=np.float32)),\n            stack=True, pad_dims=None)\n        data[\'scaling_list\'] = DC(\n            to_tensor(np.array(scaling_list, dtype=np.float32)),\n            stack=True, pad_dims=None)\n        data[\'prop_tick_list\'] = DC(\n            to_tensor(np.array(proposal_tick_list)),\n            stack=True, pad_dims=None)\n        data[\'reg_stats\'] = DC(to_tensor(self.reg_stats),\n                               stack=True, pad_dims=None)\n\n        return data\n'"
mmaction/datasets/transforms.py,0,"b'import mmcv\nimport numpy as np\nimport random\nimport math\nimport cv2\nimport random as rd\n\n__all__ = [\'GroupImageTransform\', \'ImageTransform\', \'BboxTransform\']\n\nclass GroupColorJitter(object):\n    def __init__(self, color_space_aug=False, alphastd=0.1, eigval=None, eigvec=None):\n        if eigval is None:\n            # note that the data range should be [0, 255]\n            self.eigval = np.array([55.46, 4.794, 1.148])\n        if eigvec is None:\n            self.eigvec = np.array([[-0.5675, 0.7192, 0.4009],\n                           [-0.5808, -0.0045, -0.8140],\n                           [-0.5836, -0.6948, 0.4203]])\n        self.alphastd = alphastd\n        self.color_space_aug = color_space_aug\n\n    @staticmethod\n    def brightnetss(img, delta):\n        if random.uniform(0, 1) > 0.5:\n           #delta = np.random.uniform(-32, 32)\n           delta = np.array(delta).astype(np.float32)\n           img = img + delta\n           #img_group = [img + delta for img in img_group]\n        return img\n\n    @staticmethod\n    def contrast(img, alpha):\n        if random.uniform(0, 1) > 0.5:\n           #alpha = np.random.uniform(0.6,1.4)\n           alpha = np.array(alpha).astype(np.float32)\n           img = img * alpha\n           #img_group = [img * alpha for img in img_group]\n        return img\n\n    @staticmethod\n    def saturation(img, alpha):\n        if random.uniform(0, 1) > 0.5:\n           #alpha = np.random.uniform(0.6,1.4)\n           gray = img * np.array([0.299, 0.587, 0.114]).astype(np.float32)\n           gray = np.sum(gray, 2, keepdims=True)\n           gray *= (1.0 - alpha)\n           img = img * alpha\n           img = img + gray\n        return img\n\n    @staticmethod\n    def hue(img, alpha):\n        if random.uniform(0, 1) > 0.5:\n           #alpha = random.uniform(-18, 18)\n           u = np.cos(alpha * np.pi)\n           w = np.sin(alpha * np.pi)\n           bt = np.array([[1.0, 0.0, 0.0],\n                           [0.0, u, -w],\n                           [0.0, w, u]])\n           tyiq = np.array([[0.299, 0.587, 0.114],\n                             [0.596, -0.274, -0.321],\n                             [0.211, -0.523, 0.311]])\n           ityiq = np.array([[1.0, 0.956, 0.621],\n                              [1.0, -0.272, -0.647],\n                              [1.0, -1.107, 1.705]])\n           t = np.dot(np.dot(ityiq, bt), tyiq).T\n           t = np.array(t).astype(np.float32)\n           img = np.dot(img, t)\n           #img_group = [np.dot(img, t) for img in img_group]\n        return img\n\n    def __call__(self, img_group):\n        if self.color_space_aug:\n            bright_delta = np.random.uniform(-32, 32)\n            contrast_alpha = np.random.uniform(0.6,1.4)\n            saturation_alpha = np.random.uniform(0.6,1.4)\n            hue_alpha = random.uniform(-18, 18)\n            out = []\n            for img in img_group:\n                img = self.brightnetss(img, delta=bright_delta)\n                if random.uniform(0, 1) > 0.5:\n                    img = self.contrast(img, alpha=contrast_alpha)\n                    img = self.saturation(img, alpha=saturation_alpha)\n                    img = self.hue(img, alpha=hue_alpha)\n                else:\n                    img = self.saturation(img, alpha=saturation_alpha)\n                    img = self.hue(img, alpha=hue_alpha)\n                    img = self.contrast(img, alpha=contrast_alpha)\n                out.append(img)\n            img_group = out\n\n        alpha = np.random.normal(0, self.alphastd, size=(3,))\n        rgb = np.array(np.dot(self.eigvec * alpha, self.eigval)).astype(np.float32)\n        bgr = np.expand_dims(np.expand_dims(rgb[::-1], 0),0)\n        return [img + rgb for img in img_group]\n\nclass RandomResizedCrop(object):\n    def __init__(self, size, scale=(0.08, 1.0), ratio=(3. / 4., 4. / 3.)):\n        self.size = size\n        self.scale = scale\n        self.ratio = ratio\n\n    @staticmethod\n    def get_params(img, scale, ratio):\n        """"""Get parameters for ``crop`` for a random sized crop.\n        Args:\n            img (PIL Image): Image to be cropped.\n            scale (tuple): range of size of the origin size cropped\n            ratio (tuple): range of aspect ratio of the origin aspect ratio cropped\n        Returns:\n            tuple: params (i, j, h, w) to be passed to ``crop`` for a random\n                sized crop.\n        """"""\n        for attempt in range(10):\n            area = img.shape[0] * img.shape[1]\n            target_area = random.uniform(*scale) * area\n            aspect_ratio = random.uniform(*ratio)\n\n            w = int(round(math.sqrt(target_area * aspect_ratio)))\n            h = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if random.random() < 0.5:\n                w, h = h, w\n\n            if w <= img.shape[0] and h <= img.shape[1]:\n                i = random.randint(0, img.shape[1] - h)\n                j = random.randint(0, img.shape[0] - w)\n                return i, j, h, w\n\n        # Fallback\n        w = min(img.shape[0], img.shape[1])\n        i = (img.shape[1] - w) // 2\n        j = (img.shape[0] - w) // 2\n        return i, j, w, w\n\n    def __call__(self, img_group):\n        """"""\n        Args:\n            clip (list of PIL Image): list of Image to be cropped and resized.\n        Returns:\n            list of PIL Image: Randomly cropped and resized image.\n        """"""\n        x1, y1, th, tw = self.get_params(img_group[0], self.scale, self.ratio)\n        box = np.array([x1, y1, x1+tw-1, y1+th-1], dtype=np.float32)\n        return ([mmcv.imresize(mmcv.imcrop(img, box), self.size) for img in img_group], box)\n\n\n\nclass RandomRescaledCrop(object):\n    def __init__(self, size, scale=(256, 320)):\n        self.size = size\n        self.scale = scale\n\n    def __call__(self, img_group):\n        shortedge = float(random.randint(*self.scale))\n\n        w, h, _ = img_group[0].shape\n        scale = max(shortedge / w, shortedge / h)\n        img_group = [mmcv.imrescale(img, scale) for img in img_group]\n        w, h, _ = img_group[0].shape\n        w_offset = random.randint(0, w - self.size[0])\n        h_offset = random.randint(0, h - self.size[1])\n\n        box = np.array([w_offset, h_offset,\n                        w_offset + self.size[0] - 1, h_offset + self.size[1] - 1],\n                        dtype=np.float32)\n\n        return ([img[w_offset: w_offset + self.size[0], h_offset: h_offset + self.size[1]] for img in img_group], box)\n\n\nclass GroupCrop(object):\n    def __init__(self, crop_quadruple):\n        self.crop_quadruple = crop_quadruple\n\n    def __call__(self, img_group, is_flow=False):\n        return [mmcv.imcrop(img, self.crop_quadruple)\n                for img in img_group], self.crop_quadruple\n\n\nclass GroupCenterCrop(object):\n    def __init__(self, size):\n        self.size = size if not isinstance(size, int) else (size, size)\n\n    def __call__(self, img_group, is_flow=False):\n        h = img_group[0].shape[0]\n        w = img_group[0].shape[1]\n        tw, th = self.size\n        x1 = (w - tw) // 2\n        y1 = (h - th) // 2\n        box = np.array([x1, y1, x1+tw-1, y1+th-1])\n        return ([mmcv.imcrop(img, box) for img in img_group],\n                np.array([x1, y1, tw, th], dtype=np.float32))\n\n\nclass Group3CropSample(object):\n    def __init__(self, crop_size):\n        self.crop_size = crop_size if not isinstance(\n            crop_size, int) else (crop_size, crop_size)\n\n    def __call__(self, img_group, is_flow=False):\n\n        image_h = img_group[0].shape[0]\n        image_w = img_group[0].shape[1]\n        crop_w, crop_h = self.crop_size\n        assert crop_h == image_h or crop_w == image_w\n\n        if crop_h == image_h:\n            w_step = (image_w - crop_w) // 2\n            offsets = list()\n            offsets.append((0, 0))  # left\n            offsets.append((2 * w_step, 0))  # right\n            offsets.append((w_step, 0))  # middle\n        elif crop_w == image_w:\n            h_step = (image_h - crop_h) // 2\n            offsets = list()\n            offsets.append((0, 0))  # top\n            offsets.append((0, 2 * h_step))  # down\n            offsets.append((0, h_step))  # middle\n\n        oversample_group = list()\n        for o_w, o_h in offsets:\n            normal_group = list()\n            flip_group = list()\n            for i, img in enumerate(img_group):\n                crop = mmcv.imcrop(img, np.array(\n                    [o_w, o_h, o_w + crop_w-1, o_h + crop_h-1]))\n                normal_group.append(crop)\n                flip_crop = mmcv.imflip(crop)\n\n                if is_flow and i % 2 == 0:\n                    flip_group.append(mmcv.iminvert(flip_crop))\n                else:\n                    flip_group.append(flip_crop)\n\n            oversample_group.extend(normal_group)\n            # oversample_group.extend(flip_group)\n        return oversample_group, None\n\n\nclass GroupOverSample(object):\n    def __init__(self, crop_size):\n        self.crop_size = crop_size if not isinstance(\n            crop_size, int) else (crop_size, crop_size)\n\n    def __call__(self, img_group, is_flow=False):\n\n        image_h = img_group[0].shape[0]\n        image_w = img_group[0].shape[1]\n        crop_w, crop_h = self.crop_size\n\n        offsets = GroupMultiScaleCrop.fill_fix_offset(\n            False, image_w, image_h, crop_w, crop_h)\n        oversample_group = list()\n        for o_w, o_h in offsets:\n            normal_group = list()\n            flip_group = list()\n            for i, img in enumerate(img_group):\n                crop = mmcv.imcrop(img, np.array(\n                    [o_w, o_h, o_w + crop_w-1, o_h + crop_h-1]))\n                normal_group.append(crop)\n                flip_crop = mmcv.imflip(crop)\n\n                if is_flow and i % 2 == 0:\n                    flip_group.append(mmcv.iminvert(flip_crop))\n                else:\n                    flip_group.append(flip_crop)\n\n            oversample_group.extend(normal_group)\n            oversample_group.extend(flip_group)\n        return oversample_group, None\n\n\nclass GroupMultiScaleCrop(object):\n\n    def __init__(self, input_size,\n                 scales=None, max_distort=1,\n                 fix_crop=True, more_fix_crop=True):\n        self.scales = scales if scales is not None else [1, .875, .75, .66]\n        self.max_distort = max_distort\n        self.fix_crop = fix_crop\n        self.more_fix_crop = more_fix_crop\n        self.input_size = input_size if not isinstance(input_size, int) else [\n            input_size, input_size]\n        self.interpolation = \'bilinear\'\n\n    def __call__(self, img_group, is_flow=False):\n\n        im_h = img_group[0].shape[0]\n        im_w = img_group[0].shape[1]\n\n        crop_w, crop_h, offset_w, offset_h = self._sample_crop_size(\n            (im_w, im_h))\n        box = np.array([offset_w, offset_h, offset_w +\n                        crop_w - 1, offset_h + crop_h - 1])\n        crop_img_group = [mmcv.imcrop(img, box) for img in img_group]\n        ret_img_group = [mmcv.imresize(\n            img, (self.input_size[0], self.input_size[1]),\n            interpolation=self.interpolation)\n            for img in crop_img_group]\n        return (ret_img_group, np.array([offset_w, offset_h, crop_w, crop_h],\n                                        dtype=np.float32))\n\n    def _sample_crop_size(self, im_size):\n        image_w, image_h = im_size[0], im_size[1]\n\n        # find a crop size\n        base_size = min(image_w, image_h)\n        crop_sizes = [int(base_size * x) for x in self.scales]\n        crop_h = [self.input_size[1] if abs(\n            x - self.input_size[1]) < 3 else x for x in crop_sizes]\n        crop_w = [self.input_size[0] if abs(\n            x - self.input_size[0]) < 3 else x for x in crop_sizes]\n\n        pairs = []\n        for i, h in enumerate(crop_h):\n            for j, w in enumerate(crop_w):\n                if abs(i - j) <= self.max_distort:\n                    pairs.append((w, h))\n\n        crop_pair = random.choice(pairs)\n        if not self.fix_crop:\n            w_offset = random.randint(0, image_w - crop_pair[0])\n            h_offset = random.randint(0, image_h - crop_pair[1])\n        else:\n            w_offset, h_offset = self._sample_fix_offset(\n                image_w, image_h, crop_pair[0], crop_pair[1])\n\n        return crop_pair[0], crop_pair[1], w_offset, h_offset\n\n    def _sample_fix_offset(self, image_w, image_h, crop_w, crop_h):\n        offsets = self.fill_fix_offset(\n            self.more_fix_crop, image_w, image_h, crop_w, crop_h)\n        return random.choice(offsets)\n\n    @staticmethod\n    def fill_fix_offset(more_fix_crop, image_w, image_h, crop_w, crop_h):\n        w_step = (image_w - crop_w) // 4\n        h_step = (image_h - crop_h) // 4\n\n        ret = list()\n        ret.append((0, 0))  # upper left\n        ret.append((4 * w_step, 0))  # upper right\n        ret.append((0, 4 * h_step))  # lower left\n        ret.append((4 * w_step, 4 * h_step))  # lower right\n        ret.append((2 * w_step, 2 * h_step))  # center\n\n        if more_fix_crop:\n            ret.append((0, 2 * h_step))  # center left\n            ret.append((4 * w_step, 2 * h_step))  # center right\n            ret.append((2 * w_step, 4 * h_step))  # lower center\n            ret.append((2 * w_step, 0 * h_step))  # upper center\n\n            ret.append((1 * w_step, 1 * h_step))  # upper left quarter\n            ret.append((3 * w_step, 1 * h_step))  # upper right quarter\n            ret.append((1 * w_step, 3 * h_step))  # lower left quarter\n            ret.append((3 * w_step, 3 * h_step))  # lower righ quarter\n\n        return ret\n\n\nclass GroupImageTransform(object):\n    """"""Preprocess a group of images.\n    1. rescale the images to expected size\n    2. (for classification networks) crop the images with a given size\n    3. flip the images (if needed)\n    4(a) divided by 255 (0-255 => 0-1, if needed)\n    4. normalize the images\n    5. pad the images (if needed)\n    6. transpose to (c, h, w)\n    7. stack to (N, c, h, w)\n    where, N = 1 * N_oversample * N_seg * L\n    """"""\n\n    def __init__(self,\n                 mean=(0, 0, 0),\n                 std=(1, 1, 1),\n                 to_rgb=True,\n                 size_divisor=None,\n                 crop_size=None,\n                 oversample=None,\n                 random_crop=False,\n                 resize_crop=False,\n                 rescale_crop=False,\n                 more_fix_crop=False,\n                 multiscale_crop=False,\n                 scales=None,\n                 max_distort=1):\n        self.mean = np.array(mean, dtype=np.float32)\n        self.std = np.array(std, dtype=np.float32)\n        self.to_rgb = to_rgb\n        self.size_divisor = size_divisor\n        self.resize_crop = resize_crop\n        self.rescale_crop = rescale_crop\n\n        # croping parameters\n        if crop_size is not None:\n            if oversample == \'three_crop\':\n                self.op_crop = Group3CropSample(crop_size)\n            elif oversample == \'ten_crop\':\n                # oversample crop (test)\n                self.op_crop = GroupOverSample(crop_size)\n            elif resize_crop:\n                self.op_crop = RandomResizedCrop(crop_size)\n            elif rescale_crop:\n                self.op_crop = RandomRescaledCrop(crop_size)\n            elif multiscale_crop:\n                # multiscale crop (train)\n                self.op_crop = GroupMultiScaleCrop(\n                    crop_size, scales=scales, max_distort=max_distort,\n                    fix_crop=not random_crop, more_fix_crop=more_fix_crop)\n            else:\n                # center crop (val)\n                self.op_crop = GroupCenterCrop(crop_size)\n        else:\n            self.op_crop = None\n\n    def __call__(self, img_group, scale, crop_history=None, flip=False,\n                 keep_ratio=True, div_255=False, is_flow=False):\n\n        if self.resize_crop or self.rescale_crop:\n            img_group, crop_quadruple = self.op_crop(img_group)\n            img_shape = img_group[0].shape\n            scale_factor = None\n        else:\n            # 1. rescale\n            if keep_ratio:\n                tuple_list = [mmcv.imrescale(\n                    img, scale, return_scale=True) for img in img_group]\n                img_group, scale_factors = list(zip(*tuple_list))\n                scale_factor = scale_factors[0]\n            else:\n                tuple_list = [mmcv.imresize(\n                    img, scale, return_scale=True) for img in img_group]\n                img_group, w_scales, h_scales = list(zip(*tuple_list))\n                scale_factor = np.array([w_scales[0], h_scales[0],\n                                         w_scales[0], h_scales[0]],\n                                        dtype=np.float32)\n            # 2. crop (if necessary)\n            if crop_history is not None:\n                self.op_crop = GroupCrop(crop_history)\n            if self.op_crop is not None:\n                img_group, crop_quadruple = self.op_crop(\n                    img_group, is_flow=is_flow)\n            else:\n                crop_quadruple = None\n\n            img_shape = img_group[0].shape\n        # 3. flip\n        if flip:\n            img_group = [mmcv.imflip(img) for img in img_group]\n        if is_flow:\n            for i in range(0, len(img_group), 2):\n                img_group[i] = mmcv.iminvert(img_group[i])\n        # 4a. div_255\n        if div_255:\n            img_group = [mmcv.imnormalize(img, 0, 255, False)\n                         for img in img_group]\n        # 4. normalize\n        img_group = [mmcv.imnormalize(\n            img, self.mean, self.std, self.to_rgb) for img in img_group]\n        # 5. pad\n        if self.size_divisor is not None:\n            img_group = [mmcv.impad_to_multiple(\n                img, self.size_divisor) for img in img_group]\n            pad_shape = img_group[0].shape\n        else:\n            pad_shape = img_shape\n        if is_flow:\n            assert len(img_group[0].shape) == 2\n            img_group = [np.stack((flow_x, flow_y), axis=2)\n                         for flow_x, flow_y in zip(\n                             img_group[0::2], img_group[1::2])]\n        # 6. transpose\n        img_group = [img.transpose(2, 0, 1) for img in img_group]\n\n        # Stack into numpy.array\n        img_group = np.stack(img_group, axis=0)\n        return img_group, img_shape, pad_shape, scale_factor, crop_quadruple\n\n\nclass ImageTransform(object):\n    """"""Preprocess an image.\n    1. rescale the image to expected size\n    2. normalize the image\n    3. flip the image (if needed)\n    4. pad the image (if needed)\n    5. transpose to (c, h, w)\n    """"""\n\n    def __init__(self,\n                 mean=(0, 0, 0),\n                 std=(1, 1, 1),\n                 to_rgb=True,\n                 size_divisor=None):\n        self.mean = np.array(mean, dtype=np.float32)\n        self.std = np.array(std, dtype=np.float32)\n        self.to_rgb = to_rgb\n        self.size_divisor = size_divisor\n\n    def __call__(self, img, scale, flip=False, keep_ratio=True):\n        if keep_ratio:\n            img, scale_factor = mmcv.imrescale(img, scale, return_scale=True)\n        else:\n            img, w_scale, h_scale = mmcv.imresize(\n                img, scale, return_scale=True)\n            scale_factor = np.array([w_scale, h_scale, w_scale, h_scale],\n                                    dtype=np.float32)\n        img_shape = img.shape\n        img = mmcv.imnormalize(img, self.mean, self.std, self.to_rgb)\n        if flip:\n            img = mmcv.imflip(img)\n        if self.size_divisor is not None:\n            img = mmcv.impad_to_multiple(img, self.size_divisor)\n            pad_shape = img.shape\n        else:\n            pad_shape = img_shape\n        img = img.transpose(2, 0, 1)\n        return img, img_shape, pad_shape, scale_factor\n\n\ndef bbox_flip(bboxes, img_shape):\n    """"""Flip bboxes horizontally.\n    Args:\n        bboxes(ndarray): shape (..., 4*k)\n        img_shape(tuple): (height, width)\n    """"""\n    assert bboxes.shape[-1] % 4 == 0\n    w = img_shape[1]\n    flipped = bboxes.copy()\n    flipped[..., 0::4] = w - bboxes[..., 2::4] - 1\n    flipped[..., 2::4] = w - bboxes[..., 0::4] - 1\n    return flipped\n\n\ndef bbox_crop(bboxes, crop_quadruple):\n    """"""Flip bboxes horizontally.\n    Args:\n        bboxes(ndarray): shape (..., 4*k)\n        crop_quadruple(tuple): (x1, y1, tw, th)\n    """"""\n    assert bboxes.shape[-1] % 4 == 0\n    assert crop_quadruple is not None\n    cropped = bboxes.copy()\n    x1, y1, tw, th = crop_quadruple\n    cropped[..., 0::2] = bboxes[..., 0::2] - x1\n    cropped[..., 1::2] = bboxes[..., 1::2] - y1\n    return cropped\n\n\nclass BboxTransform(object):\n    """"""Preprocess gt bboxes.\n    1. rescale bboxes according to image size\n    2. flip bboxes (if needed)\n    3. pad the first dimension to `max_num_gts`\n    """"""\n\n    def __init__(self, max_num_gts=None):\n        self.max_num_gts = max_num_gts\n\n    def __call__(self, bboxes, img_shape, scale_factor, flip=False, crop=None):\n        gt_bboxes = bboxes * scale_factor\n        if crop is not None:\n            gt_bboxes = bbox_crop(gt_bboxes, crop)\n        if flip:\n            gt_bboxes = bbox_flip(gt_bboxes, img_shape)\n        gt_bboxes[:, 0::2] = np.clip(gt_bboxes[:, 0::2], 0, img_shape[1] - 1)\n        gt_bboxes[:, 1::2] = np.clip(gt_bboxes[:, 1::2], 0, img_shape[0] - 1)\n        if self.max_num_gts is None:\n            return gt_bboxes\n        else:\n            num_gts = gt_bboxes.shape[0]\n            padded_bboxes = np.zeros((self.max_num_gts, 4), dtype=np.float32)\n            padded_bboxes[:num_gts, :] = gt_bboxes\n            return padded_bboxes\n'"
mmaction/datasets/utils.py,7,"b'import copy\nfrom collections import Sequence\nimport torch\nimport numpy as np\nimport os\nimport glob\nimport fnmatch\nimport mmcv\nfrom mmcv.runner import obj_from_dict\nfrom .. import datasets\nimport csv\nimport random\n\n\ndef to_tensor(data):\n    """"""Convert objects of various python types to :obj:`torch.Tensor`.\n\n    Supported types are: :class:`numpy.ndarray`, :class:`torch.Tensor`,\n    :class:`Sequence`, :class:`int` and :class:`float`.\n    """"""\n    if isinstance(data, torch.Tensor):\n        return data\n    elif isinstance(data, np.ndarray):\n        return torch.from_numpy(data)\n    elif isinstance(data, Sequence) and not mmcv.is_str(data):\n        return torch.tensor(data)\n    elif isinstance(data, int):\n        return torch.LongTensor([data])\n    elif isinstance(data, float):\n        return torch.FloatTensor([data])\n    else:\n        raise TypeError(\'type {} cannot be converted to tensor.\'.format(\n            type(data)))\n\n\ndef get_untrimmed_dataset(data_cfg):\n    if isinstance(data_cfg[\'ann_file\'], (list, tuple)):\n        ann_files = data_cfg[\'ann_file\']\n        num_dset = len(ann_files)\n    else:\n        ann_files = [data_cfg[\'ann_file\']]\n        num_dset = 1\n\n    if \'proposal_file\' in data_cfg.keys():\n        if isinstance(data_cfg[\'proposal_file\'], (list, tuple)):\n            proposal_files = data_cfg[\'proposal_file\']\n        else:\n            proposal_files = [data_cfg[\'proposal_file\']]\n    else:\n        proposal_files = [None] * num_dset\n    assert len(proposal_files) == num_dset\n\n    if isinstance(data_cfg[\'img_prefix\'], (list, tuple)):\n        img_prefixes = data_cfg[\'img_prefix\']\n    else:\n        img_prefixes = [data_cfg[\'img_prefix\']]\n    assert len(img_prefixes) == num_dset\n\n    dsets = []\n    for i in range(num_dset):\n        data_info = copy.deepcopy(data_cfg)\n        data_info[\'ann_file\'] = ann_files[i]\n        data_info[\'proposal_file\'] = proposal_files[i]\n        data_info[\'img_prefix\'] = img_prefixes[i]\n        dset = obj_from_dict(data_info, datasets)\n        dsets.append(dset)\n\n    if len(dsets) > 1:\n        raise ValueError(""Not implemented yet"")\n    else:\n        dset = dsets[0]\n\n    return dset\n\n\ndef get_trimmed_dataset(data_cfg):\n    if isinstance(data_cfg[\'ann_file\'], (list, tuple)):\n        ann_files = data_cfg[\'ann_file\']\n        num_dset = len(ann_files)\n    else:\n        ann_files = [data_cfg[\'ann_file\']]\n        num_dset = 1\n\n    if isinstance(data_cfg[\'img_prefix\'], (list, tuple)):\n        img_prefixes = data_cfg[\'img_prefix\']\n    else:\n        img_prefixes = [data_cfg[\'img_prefix\']]\n    assert len(img_prefixes) == num_dset\n\n    dsets = []\n    for i in range(num_dset):\n        data_info = copy.deepcopy(data_cfg)\n        data_info[\'ann_file\'] = ann_files[i]\n        data_info[\'img_prefix\'] = img_prefixes[i]\n        dset = obj_from_dict(data_info, datasets)\n        dsets.append(dset)\n\n    if len(dsets) > 1:\n        raise ValueError(""Not implemented yet"")\n    else:\n        dset = dsets[0]\n\n    return dset\n\n\ndef random_scale(img_scales, mode=\'range\'):\n    """"""Randomly select a scale from a list of scales or scale ranges.\n    Args:\n        img_scales (list[tuple]): Image scale or scale range.\n        mode (str): ""range"" or ""value"".\n    Returns:\n        tuple: Sampled image scale.\n    """"""\n    num_scales = len(img_scales)\n    if num_scales == 1:  # fixed scale is specified\n        img_scale = img_scales[0]\n    elif num_scales == 2:  # randomly sample a scale\n        if mode == \'range\':\n            img_scale_long = [max(s) for s in img_scales]\n            img_scale_short = [min(s) for s in img_scales]\n            long_edge = np.random.randint(\n                min(img_scale_long),\n                max(img_scale_long) + 1)\n            short_edge = np.random.randint(\n                min(img_scale_short),\n                max(img_scale_short) + 1)\n            img_scale = (long_edge, short_edge)\n        elif mode == \'value\':\n            img_scale = img_scales[np.random.randint(num_scales)]\n    else:\n        if mode != \'value\':\n            raise ValueError(\n                \'Only ""value"" mode supports more than 2 image scales\')\n        img_scale = img_scales[np.random.randint(num_scales)]\n    return img_scale\n\n\ndef load_localize_proposal_file(filename):\n    lines = list(open(filename))\n    from itertools import groupby\n    groups = groupby(lines, lambda x: x.startswith(\'#\'))\n\n    info_list = [[x.strip() for x in list(g)] for k, g in groups if not k]\n\n    def parse_group(info):\n        offset = 0\n        vid = info[offset]\n        offset += 1\n\n        n_frame = int(float(info[1]) * float(info[2]))\n        n_gt = int(info[3])\n        offset = 4\n\n        gt_boxes = [x.split() for x in info[offset: offset + n_gt]]\n        offset += n_gt\n        n_pr = int(info[offset])\n        offset += 1\n        pr_boxes = [x.split() for x in info[offset: offset + n_pr]]\n\n        return vid, n_frame, gt_boxes, pr_boxes\n\n    return [parse_group(l) for l in info_list]\n\n\ndef process_localize_proposal_list(norm_proposal_list,\n                                   out_list_name, frame_dict):\n    norm_proposals = load_localize_proposal_file(norm_proposal_list)\n\n    processed_proposal_list = []\n    for idx, prop in enumerate(norm_proposals):\n        vid = prop[0]\n        frame_info = frame_dict[vid]\n        frame_cnt = frame_info[1]\n        frame_path = frame_info[0].split(\'/\')[-1]\n\n        gt = [[int(x[0]), int(float(x[1]) * frame_cnt),\n               int(float(x[2]) * frame_cnt)] for x in prop[2]]\n\n        prop = [[int(x[0]), float(x[1]), float(x[2]),\n                 int(float(x[3]) * frame_cnt), int(float(x[4]) * frame_cnt)]\n                for x in prop[3]]\n\n        out_tmpl = ""# {idx}\\n{path}\\n{fc}\\n1\\n{num_gt}\\n{gt}{num_prop}\\n{prop}""\n\n        gt_dump = \'\\n\'.join([\'{} {:d} {:d}\'.format(*x)\n                             for x in gt]) + (\'\\n\' if len(gt) else \'\')\n        prop_dump = \'\\n\'.join([\'{} {:.04f} {:.04f} {:d} {:d}\'.format(\n            *x) for x in prop]) + (\'\\n\' if len(prop) else \'\')\n\n        processed_proposal_list.append(out_tmpl.format(\n            idx=idx, path=frame_path, fc=frame_cnt,\n            num_gt=len(gt), gt=gt_dump,\n            num_prop=len(prop), prop=prop_dump))\n\n    open(out_list_name, \'w\').writelines(processed_proposal_list)\n\n\ndef parse_directory(path, key_func=lambda x: x[-11:],\n                    rgb_prefix=\'img_\',\n                    flow_x_prefix=\'flow_x_\',\n                    flow_y_prefix=\'flow_y_\',\n                    level=1):\n    """"""\n    Parse directories holding extracted frames from standard benchmarks\n    """"""\n    print(\'parse frames under folder {}\'.format(path))\n    if level == 1:\n        frame_folders = glob.glob(os.path.join(path, \'*\'))\n    elif level == 2:\n        frame_folders = glob.glob(os.path.join(path, \'*\', \'*\'))\n    else:\n        raise ValueError(\'level can be only 1 or 2\')\n\n    def count_files(directory, prefix_list):\n        lst = os.listdir(directory)\n        cnt_list = [len(fnmatch.filter(lst, x+\'*\')) for x in prefix_list]\n        return cnt_list\n\n    # check RGB\n    frame_dict = {}\n    for i, f in enumerate(frame_folders):\n        all_cnt = count_files(f, (rgb_prefix, flow_x_prefix, flow_y_prefix))\n        k = key_func(f)\n\n        x_cnt = all_cnt[1]\n        y_cnt = all_cnt[2]\n        if x_cnt != y_cnt:\n            raise ValueError(\n                \'x and y direction have different number \'\n                \'of flow images. video: \' + f)\n        if i % 200 == 0:\n            print(\'{} videos parsed\'.format(i))\n\n        frame_dict[k] = (f, all_cnt[0], x_cnt)\n\n    print(\'frame folder analysis done\')\n    return frame_dict\n\n\ndef build_split_list(split, frame_info, shuffle=False):\n\n    def build_set_list(set_list):\n        rgb_list, flow_list = list(), list()\n        for item in set_list:\n            if item[0] not in frame_info:\n                # print(""item:"", item)\n                continue\n            elif frame_info[item[0]][1] > 0:\n                rgb_cnt = frame_info[item[0]][1]\n                flow_cnt = frame_info[item[0]][2]\n                rgb_list.append(\'{} {} {}\\n\'.format(\n                    item[0], rgb_cnt, item[1]))\n                flow_list.append(\'{} {} {}\\n\'.format(\n                    item[0], flow_cnt, item[1]))\n            else:\n                rgb_list.append(\'{} {}\\n\'.format(\n                    item[0], item[1]))\n                flow_list.append(\'{} {}\\n\'.format(\n                    item[0], item[1]))\n        if shuffle:\n            random.shuffle(rgb_list)\n            random.shuffle(flow_list)\n        return rgb_list, flow_list\n\n    train_rgb_list, train_flow_list = build_set_list(split[0])\n    test_rgb_list, test_flow_list = build_set_list(split[1])\n    return (train_rgb_list, test_rgb_list), (train_flow_list, test_flow_list)\n\n\ndef mimic_ucf101(frame_path, anno_dir):\n    # Create classInd.txt, trainlist01.txt, testlist01.txt as in UCF101\n\n    classes_list = os.listdir(frame_path)\n    classes_list.sort()\n\n    classDict = {}\n    classIndFile = os.path.join(anno_dir, \'classInd.txt\')\n    with open(classIndFile, \'w\') as f:\n        for class_id, class_name in enumerate(classes_list):\n            classDict[class_name] = class_id\n            cur_line = str(class_id + 1) + \' \' + class_name + \'\\r\\n\'\n            f.write(cur_line)\n\n\n    for split_id in range(1, 4):\n        splitTrainFile = os.path.join(anno_dir, \'trainlist%02d.txt\' % (split_id))\n        with open(splitTrainFile, \'w\') as target_train_f:\n            for class_name in classDict.keys():\n                fname = class_name + \'_test_split%d.txt\' % (split_id)\n                fname_path = os.path.join(anno_dir, fname)\n                source_f = open(fname_path, \'r\')\n                source_info = source_f.readlines()\n                for _, source_line in enumerate(source_info):\n                    cur_info = source_line.split(\' \')\n                    video_name = cur_info[0]\n                    if cur_info[1] == \'1\':\n                        target_line = class_name + \'/\' + video_name + \' \' + str(classDict[class_name] + 1) + \'\\n\'\n                        target_train_f.write(target_line)\n\n        splitTestFile = os.path.join(anno_dir, \'testlist%02d.txt\' % (split_id))\n        with open(splitTestFile, \'w\') as target_test_f:\n            for class_name in classDict.keys():\n                fname = class_name + \'_test_split%d.txt\' % (split_id)\n                fname_path = os.path.join(anno_dir, fname)\n                source_f = open(fname_path, \'r\')\n                source_info = source_f.readlines()\n                for _, source_line in enumerate(source_info):\n                    cur_info = source_line.split(\' \')\n                    video_name = cur_info[0]\n                    if cur_info[1] == \'2\':\n                        target_line = class_name + \'/\' + video_name + \' \' + str(classDict[class_name] + 1) + \'\\n\'\n                        target_test_f.write(target_line)\n\n\ndef parse_hmdb51_splits(level):\n    \n    mimic_ucf101(\'data/hmdb51/rawframes\', \'data/hmdb51/annotations\')\n\n    class_ind = [x.strip().split()\n                 for x in open(\'data/hmdb51/annotations/classInd.txt\')]\n    class_mapping = {x[1]: int(x[0]) - 1 for x in class_ind}\n\n    def line2rec(line):\n        items = line.strip().split(\' \')\n        vid = items[0].split(\'.\')[0]\n        vid = \'/\'.join(vid.split(\'/\')[-level:])\n        label = class_mapping[items[0].split(\'/\')[0]]\n        return vid, label\n\n    splits = []\n    for i in range(1, 4):\n        train_list = [line2rec(x) for x in open(\n            \'data/hmdb51/annotations/trainlist{:02d}.txt\'.format(i))]\n        test_list = [line2rec(x) for x in open(\n            \'data/hmdb51/annotations/testlist{:02d}.txt\'.format(i))]\n        splits.append((train_list, test_list))\n    return splits\n\n\ndef parse_ucf101_splits(level):\n    class_ind = [x.strip().split()\n                 for x in open(\'data/ucf101/annotations/classInd.txt\')]\n    class_mapping = {x[1]: int(x[0]) - 1 for x in class_ind}\n\n    def line2rec(line):\n        items = line.strip().split(\' \')\n        vid = items[0].split(\'.\')[0]\n        vid = \'/\'.join(vid.split(\'/\')[-level:])\n        label = class_mapping[items[0].split(\'/\')[0]]\n        return vid, label\n\n    splits = []\n    for i in range(1, 4):\n        train_list = [line2rec(x) for x in open(\n            \'data/ucf101/annotations/trainlist{:02d}.txt\'.format(i))]\n        test_list = [line2rec(x) for x in open(\n            \'data/ucf101/annotations/testlist{:02d}.txt\'.format(i))]\n        splits.append((train_list, test_list))\n    return splits\n\n\ndef parse_kinetics_splits(level):\n    csv_reader = csv.reader(\n        open(\'data/kinetics400/annotations/kinetics_train.csv\'))\n    # skip the first line\n    next(csv_reader)\n\n    def convert_label(s):\n        return s.replace(\'""\', \'\').replace(\' \', \'_\')\n\n    labels_sorted = sorted(\n        set([convert_label(row[0]) for row in csv_reader]))\n    class_mapping = {label: i for i, label in enumerate(labels_sorted)}\n\n    def list2rec(x, test=False):\n        if test:\n            vid = \'{}_{:06d}_{:06d}\'.format(x[0], int(x[1]), int(x[2]))\n            label = -1  # label unknown\n            return vid, label\n        else:\n            vid = \'{}_{:06d}_{:06d}\'.format(x[1], int(x[2]), int(x[3]))\n            if level == 2:\n                vid = \'{}/{}\'.format(convert_label(x[0]), vid)\n            else:\n                assert level == 1\n            label = class_mapping[convert_label(x[0])]\n            return vid, label\n        \n    csv_reader = csv.reader(\n        open(\'data/kinetics400/annotations/kinetics_train.csv\'))\n    next(csv_reader)\n    train_list = [list2rec(x) for x in csv_reader]\n    csv_reader = csv.reader(\n        open(\'data/kinetics400/annotations/kinetics_val.csv\'))\n    next(csv_reader)\n    val_list = [list2rec(x) for x in csv_reader]\n    csv_reader = csv.reader(\n        open(\'data/kinetics400/annotations/kinetics_test.csv\'))\n    next(csv_reader)\n    test_list = [list2rec(x, test=True) for x in csv_reader]\n\n    return ((train_list, val_list, test_list), )\n'"
mmaction/datasets/video_dataset.py,1,"b'import mmcv\nimport numpy as np\nimport os.path as osp\nfrom mmcv.parallel import DataContainer as DC\nfrom torch.utils.data import Dataset\n\nfrom .transforms import (GroupImageTransform)\nfrom .utils import to_tensor\n\ntry:\n    import decord\nexcept ImportError:\n    pass\n\n\nclass RawFramesRecord(object):\n\n    def __init__(self, row):\n        self._data = row\n        self.num_frames = -1\n\n    @property\n    def path(self):\n        return self._data[0]\n\n    @property\n    def label(self):\n        return int(self._data[1])\n\n\nclass VideoDataset(Dataset):\n\n    def __init__(self,\n                 ann_file,\n                 img_prefix,\n                 img_norm_cfg,\n                 num_segments=3,\n                 new_length=1,\n                 new_step=1,\n                 random_shift=True,\n                 temporal_jitter=False,\n                 modality=\'RGB\',\n                 image_tmpl=\'img_{}.jpg\',\n                 img_scale=256,\n                 img_scale_file=None,\n                 input_size=224,\n                 div_255=False,\n                 size_divisor=None,\n                 proposal_file=None,\n                 num_max_proposals=1000,\n                 flip_ratio=0.5,\n                 resize_keep_ratio=True,\n                 resize_ratio=[1, 0.875, 0.75, 0.66],\n                 test_mode=False,\n                 oversample=None,\n                 random_crop=False,\n                 more_fix_crop=False,\n                 multiscale_crop=False,\n                 resize_crop=False,\n                 rescale_crop=False,\n                 scales=None,\n                 max_distort=1,\n                 input_format=\'NCHW\',\n                 use_decord=False,\n                 video_ext=\'mp4\'):\n        # prefix of images path\n        self.img_prefix = img_prefix\n\n        # load annotations\n        self.video_infos = self.load_annotations(ann_file)\n        # normalization config\n        self.img_norm_cfg = img_norm_cfg\n\n        # parameters for frame fetching\n        # number of segments\n        self.num_segments = num_segments\n        # number of consecutive frames\n        self.old_length = new_length * new_step\n        self.new_length = new_length\n        # number of steps (sparse sampling for efficiency of io)\n        self.new_step = new_step\n        # whether to temporally random shift when training\n        self.random_shift = random_shift\n        # whether to temporally jitter if new_step > 1\n        self.temporal_jitter = temporal_jitter\n\n        # parameters for modalities\n        if isinstance(modality, (list, tuple)):\n            self.modalities = modality\n            num_modality = len(modality)\n        else:\n            self.modalities = [modality]\n            num_modality = 1\n        if isinstance(image_tmpl, (list, tuple)):\n            self.image_tmpls = image_tmpl\n        else:\n            self.image_tmpls = [image_tmpl]\n        assert len(self.image_tmpls) == num_modality\n\n        # parameters for image preprocessing\n        # img_scale\n        if isinstance(img_scale, int):\n            img_scale = (np.Inf, img_scale)\n        self.img_scale = img_scale\n        if img_scale_file is not None:\n            self.img_scale_dict = {\n                line.split(\' \')[0]:\n                (int(line.split(\' \')[1]), int(line.split(\' \')[2]))\n                for line in open(img_scale_file)\n            }\n        else:\n            self.img_scale_dict = None\n        # network input size\n        if isinstance(input_size, int):\n            input_size = (input_size, input_size)\n        self.input_size = input_size\n\n        # parameters for specification from pre-trained networks (lecacy issue)\n        self.div_255 = div_255\n\n        # parameters for data augmentation\n        # flip ratio\n        self.flip_ratio = flip_ratio\n        self.resize_keep_ratio = resize_keep_ratio\n\n        # test mode or not\n        self.test_mode = test_mode\n\n        # set group flag for the sampler\n        # if not self.test_mode:\n        self._set_group_flag()\n\n        # transforms\n        assert oversample in [None, \'three_crop\', \'ten_crop\']\n        self.img_group_transform = GroupImageTransform(\n            size_divisor=None,\n            crop_size=self.input_size,\n            oversample=oversample,\n            random_crop=random_crop,\n            more_fix_crop=more_fix_crop,\n            multiscale_crop=multiscale_crop,\n            scales=scales,\n            max_distort=max_distort,\n            resize_crop=resize_crop,\n            rescale_crop=rescale_crop,\n            **self.img_norm_cfg)\n\n        # input format\n        assert input_format in [\'NCHW\', \'NCTHW\']\n        self.input_format = input_format\n        \'\'\'\n        self.bbox_transform = Bbox_transform()\n        \'\'\'\n\n        self.use_decord = use_decord\n        self.video_ext = video_ext\n\n    def __len__(self):\n        return len(self.video_infos)\n\n    def load_annotations(self, ann_file):\n        return [RawFramesRecord(x.strip().split(\' \')) for x in open(ann_file)]\n        # return mmcv.load(ann_file)\n\n    def load_proposals(self, proposal_file):\n        return mmcv.load(proposal_file)\n\n    def get_ann_info(self, idx):\n        return {\n            \'path\': self.video_infos[idx].path,\n            \'label\': self.video_infos[idx].label\n        }\n        # return self.video_infos[idx][\'ann\']\n\n    def _set_group_flag(self):\n        """"""Set flag according to image aspect ratio.\n\n        Images with aspect ratio greater than 1 will be set as group 1,\n        otherwise group 0.\n        """"""\n        self.flag = np.zeros(len(self), dtype=np.uint8)\n        for i in range(len(self)):\n            # img_info = self.img_infos[i]\n            # if img_info[\'width\'] / img_info[\'height\'] > 1:\n            self.flag[i] = 1\n\n    def _load_image(self, video_reader, directory, modality, idx):\n        if modality in [\'RGB\', \'RGBDiff\']:\n            return [video_reader[idx - 1]]\n        elif modality == \'Flow\':\n            raise NotImplementedError\n        else:\n            raise ValueError(\'Not implemented yet; modality should be \'\n                             \'[""RGB"", ""RGBDiff"", ""Flow""]\')\n\n    def _sample_indices(self, record):\n        \'\'\'\n\n        :param record: VideoRawFramesRecord\n        :return: list, list\n        \'\'\'\n        average_duration = (record.num_frames - self.old_length +\n                            1) // self.num_segments\n        if average_duration > 0:\n            offsets = np.multiply(\n                list(range(self.num_segments)), average_duration)\n            offsets = offsets + np.random.randint(\n                average_duration, size=self.num_segments)\n        elif record.num_frames > max(self.num_segments, self.old_length):\n            offsets = np.sort(\n                np.random.randint(\n                    record.num_frames - self.old_length + 1,\n                    size=self.num_segments))\n        else:\n            offsets = np.zeros((self.num_segments, ))\n        if self.temporal_jitter:\n            skip_offsets = np.random.randint(\n                self.new_step, size=self.old_length // self.new_step)\n        else:\n            skip_offsets = np.zeros(\n                self.old_length // self.new_step, dtype=int)\n        return offsets + 1, skip_offsets  # frame index starts from 1\n\n    def _get_val_indices(self, record):\n        if record.num_frames > self.num_segments + self.old_length - 1:\n            tick = (record.num_frames - self.old_length + 1) / \\\n                float(self.num_segments)\n            offsets = np.array(\n                [int(tick / 2.0 + tick * x) for x in range(self.num_segments)])\n        else:\n            offsets = np.zeros((self.num_segments, ))\n        if self.temporal_jitter:\n            skip_offsets = np.random.randint(\n                self.new_step, size=self.old_length // self.new_step)\n        else:\n            skip_offsets = np.zeros(\n                self.old_length // self.new_step, dtype=int)\n        return offsets + 1, skip_offsets\n\n    def _get_test_indices(self, record):\n        if record.num_frames > self.old_length - 1:\n            tick = (record.num_frames - self.old_length + 1) / \\\n                float(self.num_segments)\n            offsets = np.array(\n                [int(tick / 2.0 + tick * x) for x in range(self.num_segments)])\n        else:\n            offsets = np.zeros((self.num_segments, ))\n        if self.temporal_jitter:\n            skip_offsets = np.random.randint(\n                self.new_step, size=self.old_length // self.new_step)\n        else:\n            skip_offsets = np.zeros(\n                self.old_length // self.new_step, dtype=int)\n        return offsets + 1, skip_offsets\n\n    def _get_frames(self, record, video_reader, image_tmpl, modality, indices,\n                    skip_offsets):\n        if self.use_decord:\n            if modality not in [\'RGB\', \'RGBDiff\']:\n                raise NotImplementedError\n            images = list()\n            for seg_ind in indices:\n                p = int(seg_ind)\n                if p > 1:\n                    video_reader.seek(p - 1)\n                cur_content = video_reader.next().asnumpy()\n                # Cache the (p-1)-th frame first. This is to avoid decord\'s\n                # StopIteration, which may consequently affect the mmcv.runner\n                for i, ind in enumerate(\n                        range(0, self.old_length, self.new_step)):\n                    if (skip_offsets[i] > 0\n                            and p + skip_offsets[i] <= record.num_frames):\n                        if skip_offsets[i] > 1:\n                            video_reader.skip_frames(skip_offsets[i] - 1)\n                        cur_content = video_reader.next().asnumpy()\n                    seg_imgs = [cur_content]\n                    images.extend(seg_imgs)\n                    if (self.new_step > 1\n                            and p + self.new_step <= record.num_frames):\n                        video_reader.skip_frames(self.new_step - 1)\n                    p += self.new_step\n            return images\n        else:\n            images = list()\n            for seg_ind in indices:\n                p = int(seg_ind)\n                for i, ind in enumerate(\n                        range(0, self.old_length, self.new_step)):\n                    if p + skip_offsets[i] <= record.num_frames:\n                        seg_imgs = self._load_image(\n                            video_reader, osp.join(self.img_prefix,\n                                                   record.path), modality,\n                            p + skip_offsets[i])\n                    else:\n                        seg_imgs = self._load_image(\n                            video_reader, osp.join(self.img_prefix,\n                                                   record.path), modality, p)\n                    images.extend(seg_imgs)\n                    if p + self.new_step < record.num_frames:\n                        p += self.new_step\n            return images\n\n    def __getitem__(self, idx):\n        record = self.video_infos[idx]\n        if self.use_decord:\n            video_reader = decord.VideoReader(\'{}.{}\'.format(\n                osp.join(self.img_prefix, record.path), self.video_ext))\n            record.num_frames = len(video_reader)\n        else:\n            video_reader = mmcv.VideoReader(\'{}.{}\'.format(\n                osp.join(self.img_prefix, record.path), self.video_ext))\n            record.num_frames = len(video_reader)\n\n        if self.test_mode:\n            segment_indices, skip_offsets = self._get_test_indices(record)\n        else:\n            segment_indices, skip_offsets = self._sample_indices(\n                record) if self.random_shift else self._get_val_indices(record)\n\n        data = dict(\n            num_modalities=DC(to_tensor(len(self.modalities))),\n            gt_label=DC(to_tensor(record.label), stack=True, pad_dims=None))\n\n        # handle the first modality\n        modality = self.modalities[0]\n        image_tmpl = self.image_tmpls[0]\n        img_group = self._get_frames(record, video_reader, image_tmpl,\n                                     modality, segment_indices, skip_offsets)\n\n        flip = True if np.random.rand() < self.flip_ratio else False\n        if (self.img_scale_dict is not None\n                and record.path in self.img_scale_dict):\n            img_scale = self.img_scale_dict[record.path]\n        else:\n            img_scale = self.img_scale\n        (img_group, img_shape, pad_shape, scale_factor,\n         crop_quadruple) = self.img_group_transform(\n             img_group,\n             img_scale,\n             crop_history=None,\n             flip=flip,\n             keep_ratio=self.resize_keep_ratio,\n             div_255=self.div_255,\n             is_flow=True if modality == \'Flow\' else False)\n        ori_shape = (256, 340, 3)\n        img_meta = dict(\n            ori_shape=ori_shape,\n            img_shape=img_shape,\n            pad_shape=pad_shape,\n            scale_factor=scale_factor,\n            crop_quadruple=crop_quadruple,\n            flip=flip)\n        # [M x C x H x W]\n        # M = 1 * N_oversample * N_seg * L\n        if self.input_format == ""NCTHW"":\n            img_group = img_group.reshape((-1, self.num_segments,\n                                           self.new_length) +\n                                          img_group.shape[1:])\n            # N_over x N_seg x L x C x H x W\n            img_group = np.transpose(img_group, (0, 1, 3, 2, 4, 5))\n            # N_over x N_seg x C x L x H x W\n            img_group = img_group.reshape((-1, ) + img_group.shape[2:])\n            # M\' x C x L x H x W\n\n        data.update(\n            dict(\n                img_group_0=DC(to_tensor(img_group), stack=True, pad_dims=2),\n                img_meta=DC(img_meta, cpu_only=True)))\n\n        # handle the rest modalities using the same\n        for i, (modality, image_tmpl) in enumerate(\n                zip(self.modalities[1:], self.image_tmpls[1:])):\n            img_group = self._get_frames(record, video_reader, image_tmpl, modality,\n                                         segment_indices, skip_offsets)\n\n            # apply transforms\n            flip = True if np.random.rand() < self.flip_ratio else False\n            (img_group, img_shape, pad_shape, scale_factor,\n             crop_quadruple) = self.img_group_transform(\n                 img_group,\n                 img_scale,\n                 crop_history=data[\'img_meta\'][\'crop_quadruple\'],\n                 flip=data[\'img_meta\'][\'flip\'],\n                 keep_ratio=self.resize_keep_ratio,\n                 div_255=self.div_255,\n                 is_flow=True if modality == \'Flow\' else False)\n            if self.input_format == ""NCTHW"":\n                # Convert [M x C x H x W] to [M\' x C x T x H x W]\n                # M = 1 * N_oversample * N_seg * L\n                # M\' = 1 * N_oversample * N_seg, T = L\n                img_group = img_group.reshape((-1, self.num_segments,\n                                               self.new_length) +\n                                              img_group.shape[1:])\n                img_group = np.transpose(img_group, (0, 1, 3, 2, 4, 5))\n                img_group = img_group.reshape((-1, ) + img_group.shape[2:])\n\n            data.update({\n                \'img_group_{}\'.format(i + 1):\n                DC(to_tensor(img_group), stack=True, pad_dims=2),\n            })\n\n        return data\n'"
mmaction/losses/__init__.py,0,"b""from .flow_losses import charbonnier_loss, SSIM_loss\nfrom .losses import (\n    weighted_nll_loss, weighted_cross_entropy, weighted_binary_cross_entropy,\n    weighted_smoothl1, accuracy,\n    weighted_multilabel_binary_cross_entropy,\n    multilabel_accuracy)\nfrom .ssn_losses import (OHEMHingeLoss, completeness_loss,\n                         classwise_regression_loss)\n\n__all__ = [\n    'charbonnier_loss', 'SSIM_loss',\n    'weighted_nll_loss', 'weighted_cross_entropy',\n    'weighted_binary_cross_entropy',\n    'weighted_smoothl1', 'accuracy',\n    'weighted_multilabel_binary_cross_entropy',\n    'multilabel_accuracy',\n    'OHEMHingeLoss', 'completeness_loss',\n    'classwise_regression_loss'\n]\n"""
mmaction/losses/flow_losses.py,9,"b""import math\nimport torch\nimport torch.nn as nn\n\n\ndef charbonnier_loss(difference, mask, alpha=1, beta=1., epsilon=0.001):\n    '''\n    : sum( (x*beta)^2 + epsilon^2)^alpha\n    '''\n    if mask is not None:\n        assert difference.size(0) == mask.size(0)\n        assert difference.size(2) == mask.size(2)\n        assert difference.size(3) == mask.size(3)\n    res = torch.pow(torch.pow(difference * beta, 2) + epsilon ** 2, alpha)\n    if mask is not None:\n        batch_pixels = torch.sum(mask)\n        return torch.sum(res * mask) / batch_pixels\n    else:\n        batch_pixels = torch.numel(res)\n        return torch.sum(res) / batch_pixels\n\n\ndef SSIM_loss(img1, img2, kernel_size=8, stride=8, c1=0.00001, c2=0.00001):\n    num = img1.size(0)\n    channels = img1.size(1)\n\n    kernel_h = kernel_w = kernel_size\n    sigma = (kernel_w + kernel_h) / 12.\n    gauss_kernel = torch.zeros((1, 1, kernel_h, kernel_w)).type(img1.type())\n    for h in range(kernel_h):\n        for w in range(kernel_w):\n            gauss_kernel[0, 0, h, w] = math.exp(\n                -(math.pow(h - kernel_h/2.0, 2) + math.pow(- kernel_w/2.0, 2))\n                / (2.0 * sigma ** 2)) / (2 * 3.14159 * sigma ** 2)\n    gauss_kernel = gauss_kernel / torch.sum(gauss_kernel)\n    gauss_kernel = gauss_kernel.repeat(channels, 1, 1, 1)\n\n    gauss_filter = nn.Conv2d(channels, channels, kernel_size,\n                             stride=stride, padding=0,\n                             groups=channels, bias=False)\n    gauss_filter.weight.data = gauss_kernel\n    gauss_filter.weight.requires_grad = False\n\n    ux = gauss_filter(img1)\n    uy = gauss_filter(img2)\n    sx2 = gauss_filter(img1 ** 2)\n    sy2 = gauss_filter(img2 ** 2)\n    sxy = gauss_filter(img1 * img2)\n\n    ux2 = ux ** 2\n    uy2 = uy ** 2\n    sx2 = sx2 - ux2\n    sy2 = sy2 - uy2\n    sxy = sxy - ux * uy\n\n    lp = (2 * ux * uy + c1) / (ux2 + uy2 + c1)\n    sc = (2 * sxy + c2) / (sx2 + sy2 + c2)\n\n    ssim = lp * sc\n    return (lp.numel() - torch.sum(ssim)) / num\n"""
mmaction/losses/losses.py,23,"b'import torch\nimport torch.nn.functional as F\n\n\ndef weighted_nll_loss(pred, label, weight, avg_factor=None):\n    if avg_factor is None:\n        avg_factor = max(torch.sum(weight > 0).float().item(), 1.)\n    raw = F.nll_loss(pred, label, reduction=\'none\')\n    return torch.sum(raw * weight)[None] / avg_factor\n\n\ndef weighted_cross_entropy(pred, label, weight, avg_factor=None, reduce=True):\n    if avg_factor is None:\n        avg_factor = max(torch.sum(weight > 0).float().item(), 1.)\n    raw = F.cross_entropy(pred, label, reduction=\'none\')\n    if reduce:\n        return torch.sum(raw * weight)[None] / avg_factor\n    else:\n        return raw * weight / avg_factor\n\n\ndef weighted_binary_cross_entropy(pred, label, weight, avg_factor=None):\n    if pred.dim() != label.dim():\n        label, weight = _expand_binary_labels(label, weight, pred.size(-1))\n    if avg_factor is None:\n        avg_factor = max(torch.sum(weight > 0).float().item(), 1.)\n    return F.binary_cross_entropy_with_logits(\n        pred, label.float(), weight.float(),\n        reduction=\'sum\')[None] / avg_factor\n\n\ndef smooth_l1_loss(pred, target, beta=1.0, reduction=\'mean\'):\n    assert beta > 0\n    assert pred.size() == target.size() and target.numel() > 0\n    diff = torch.abs(pred - target)\n    loss = torch.where(diff < beta, 0.5 * diff * diff / beta,\n                       diff - 0.5 * beta)\n    reduction_enum = F._Reduction.get_enum(reduction)\n    # none: 0, mean: 1, sum: 2\n    if reduction_enum == 0:\n        return loss\n    elif reduction_enum == 1:\n        return loss.sum() / pred.numel()\n    elif reduction_enum == 2:\n        return loss.sum()\n\n\ndef weighted_smoothl1(pred, target, weight, beta=1.0, avg_factor=None):\n    if avg_factor is None:\n        avg_factor = torch.sum(weight > 0).float().item() / 4 + 1e-6\n    loss = smooth_l1_loss(pred, target, beta, reduction=\'none\')\n    return torch.sum(loss * weight)[None] / avg_factor\n\n\ndef accuracy(pred, target, topk=1):\n    if isinstance(topk, int):\n        topk = (topk, )\n        return_single = True\n    else:\n        return_single = False\n\n    maxk = max(topk)\n    _, pred_label = pred.topk(maxk, 1, True, True)\n    pred_label = pred_label.t()\n    correct = pred_label.eq(target.view(1, -1).expand_as(pred_label))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n        res.append(correct_k.mul_(100.0 / pred.size(0)))\n    return res[0] if return_single else res\n\n\ndef _expand_binary_labels(labels, label_weights, label_channels):\n    bin_labels = labels.new_full((labels.size(0), label_channels), 0)\n    inds = torch.nonzero(labels >= 1).squeeze()\n    if inds.numel() > 0:\n        bin_labels[inds, labels[inds] - 1] = 1\n    bin_label_weights = label_weights.view(-1, 1).expand(\n        label_weights.size(0), label_channels)\n    return bin_labels, bin_label_weights\n\n\ndef weighted_multilabel_binary_cross_entropy(\n        pred, label, weight, avg_factor=None):\n    label, weight = _expand_multilabel_binary_labels(\n        label, weight, pred.size(-1))\n    if avg_factor is None:\n        avg_factor = max(torch.sum(weight > 0).float().item(), 1.)\n    return F.binary_cross_entropy_with_logits(\n        pred, label.float(), weight.float(),\n        reduction=\'sum\')[None] / avg_factor\n\n\ndef _expand_multilabel_binary_labels(labels, label_weights, label_channels):\n    bin_labels = labels.new_full((labels.size(0), label_channels), 0)\n    inds = torch.nonzero(labels >= 1)\n    if inds.numel() > 0:\n        for ind in inds:\n            # note that labels starts from 1\n            bin_labels[ind[0], labels[ind[0], ind[1]] - 1] = 1\n            # bin_labels[ind[0], 0] = 1\n    bin_label_weights = label_weights\n    return bin_labels, bin_label_weights\n\n\ndef multilabel_accuracy(pred, target, topk=1, thr=0.5):\n    if topk is None:\n        topk = ()\n    elif isinstance(topk, int):\n        topk = (topk, )\n\n    pred = pred.sigmoid()\n    pred_bin_labels = pred.new_full((pred.size(0), ), 0, dtype=torch.long)\n    pred_vec_labels = pred.new_full(pred.size(), 0, dtype=torch.long)\n    for i in range(pred.size(0)):\n        inds = torch.nonzero(pred[i, 1:] > thr).squeeze() + 1\n        if inds.numel() > 0:\n            pred_vec_labels[i, inds] = 1\n            # pred_bin_labels[i] = 1\n        if pred[i, 0] > thr:\n            pred_bin_labels[i] = 1\n    target_bin_labels = target.new_full(\n        (target.size(0), ), 0, dtype=torch.long)\n    target_vec_labels = target.new_full(target.size(), 0, dtype=torch.long)\n    for i in range(target.size(0)):\n        inds = torch.nonzero(target[i, :] >= 1).squeeze()\n        if inds.numel() > 0:\n            target_vec_labels[i, target[i, inds]] = 1\n            target_bin_labels[i] = 1\n    # overall accuracy\n    correct = pred_bin_labels.eq(target_bin_labels)\n    acc = correct.float().sum(0, keepdim=True).mul_(100.0 / correct.size(0))\n\n    # def overlap(tensor1, tensor2):\n    #     indices = tensor1.new_zeros(tensor1).astype(torch.uint8)\n    #     for elem in tensor2:\n    #         indices = indices | (tensor1 == elem)\n    #     return tensor1[indices]\n\n    # recall@thr\n    recall_thr, prec_thr = recall_prec(pred_vec_labels, target_vec_labels)\n\n    # recall@k\n    recalls = []\n    precs = []\n    for k in topk:\n        _, pred_label = pred.topk(k, 1, True, True)\n        pred_vec_labels = pred.new_full(pred.size(), 0, dtype=torch.long)\n        for i in range(pred.size(0)):\n            pred_vec_labels[i, pred_label[i]] = 1\n        recall_k, prec_k = recall_prec(pred_vec_labels, target_vec_labels)\n        recalls.append(recall_k)\n        precs.append(prec_k)\n\n    return acc, recall_thr, prec_thr, recalls, precs\n\n\ndef recall_prec(pred_vec, target_vec):\n    """"""\n    Args:\n        pred_vec: <torch.tensor> (n, C+1), each element is either 0 or 1\n        target_vec: <torch.tensor> (n, C+1), each element is either 0 or 1\n\n    Returns:\n        recall\n        prec\n    """"""\n    recall = pred_vec.new_full((pred_vec.size(0), ), 0).float()\n    prec = pred_vec.new_full((pred_vec.size(0), ), 0).float()\n    num_pos = 0\n    for i in range(target_vec.size(0)):\n        if target_vec[i, :].float().sum(0) == 0:\n            continue\n        correct_labels = pred_vec[i, :] & target_vec[i, :]\n        recall[i] = correct_labels.float().sum(0, keepdim=True) / \\\n            target_vec[i, :].float().sum(0, keepdim=True)\n        prec[i] = correct_labels.float().sum(0, keepdim=True) / \\\n            (pred_vec[i, :].float().sum(0, keepdim=True) + 1e-6)\n        num_pos += 1\n    recall = recall.float().sum(0, keepdim=True).mul_(100. / num_pos)\n    prec = prec.float().sum(0, keepdim=True).mul_(100. / num_pos)\n    return recall, prec\n'"
mmaction/losses/ssn_losses.py,10,"b'import torch\nimport torch.nn.functional as F\n\n\nclass OHEMHingeLoss(torch.autograd.Function):\n    """"""\n    This class is the core implementation for the completeness loss in paper.\n    It compute class-wise hinge loss and performs online hard negative mining\n    (OHEM).\n    """"""\n\n    @staticmethod\n    def forward(ctx, pred, labels, is_positive, ohem_ratio, group_size):\n        n_sample = pred.size()[0]\n        assert n_sample == len(\n            labels), ""mismatch between sample size and label size""\n        losses = torch.zeros(n_sample)\n        slopes = torch.zeros(n_sample)\n        for i in range(n_sample):\n            losses[i] = max(0, 1 - is_positive * pred[i, labels[i] - 1])\n            slopes[i] = -is_positive if losses[i] != 0 else 0\n\n        losses = losses.view(-1, group_size).contiguous()\n        sorted_losses, indices = torch.sort(losses, dim=1, descending=True)\n        keep_num = int(group_size * ohem_ratio)\n        loss = torch.zeros(1).cuda()\n        for i in range(losses.size(0)):\n            loss += sorted_losses[i, :keep_num].sum()\n        ctx.loss_ind = indices[:, :keep_num]\n        ctx.labels = labels\n        ctx.slopes = slopes\n        ctx.shape = pred.size()\n        ctx.group_size = group_size\n        ctx.num_group = losses.size(0)\n        return loss\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        labels = ctx.labels\n        slopes = ctx.slopes\n\n        grad_in = torch.zeros(ctx.shape)\n        for group in range(ctx.num_group):\n            for idx in ctx.loss_ind[group]:\n                loc = idx + group * ctx.group_size\n                grad_in[loc, labels[loc] - 1] = slopes[loc] * \\\n                    grad_output.data[0]\n        return torch.autograd.Variable(grad_in.cuda()), None, None, None, None\n\n\ndef completeness_loss(pred, labels, sample_split,\n                      sample_group_size, ohem_ratio=0.17):\n    pred_dim = pred.size()[1]\n    pred = pred.view(-1, sample_group_size, pred_dim)\n    labels = labels.view(-1, sample_group_size)\n\n    pos_group_size = sample_split\n    neg_group_size = sample_group_size - sample_split\n    pos_prob = pred[:, :sample_split, :].contiguous().view(-1, pred_dim)\n    neg_prob = pred[:, sample_split:, :].contiguous().view(-1, pred_dim)\n    pos_ls = OHEMHingeLoss.apply(pos_prob,\n                                 labels[:, :sample_split].contiguous(\n                                 ).view(-1), 1,\n                                 1.0, pos_group_size)\n    neg_ls = OHEMHingeLoss.apply(neg_prob,\n                                 labels[:, sample_split:].contiguous(\n                                 ).view(-1), -1,\n                                 ohem_ratio, neg_group_size)\n    pos_cnt = pos_prob.size(0)\n    neg_cnt = int(neg_prob.size()[0] * ohem_ratio)\n\n    return pos_ls / float(pos_cnt + neg_cnt) + \\\n        neg_ls / float(pos_cnt + neg_cnt)\n\n\ndef classwise_regression_loss(pred, labels, targets):\n    indexer = labels.data - 1\n    prep = pred[:, indexer, :]\n    class_pred = torch.cat((torch.diag(prep[:, :, 0]).view(-1, 1),\n                            torch.diag(prep[:, :, 1]).view(-1, 1)),\n                           dim=1)\n    loss = F.smooth_l1_loss(class_pred.view(-1), targets.view(-1)) * 2\n    return loss\n'"
mmaction/models/__init__.py,0,"b""from .tenons.backbones import *\nfrom .tenons.spatial_temporal_modules import *\nfrom .tenons.segmental_consensuses import *\nfrom .tenons.cls_heads import * \nfrom .recognizers import *\nfrom .tenons.necks import *\nfrom .tenons.roi_extractors import *\nfrom .tenons.anchor_heads import *\nfrom .tenons.shared_heads import *\nfrom .tenons.bbox_heads import *\nfrom .detectors import *\nfrom .localizers import *\n\n\nfrom .registry import (BACKBONES, SPATIAL_TEMPORAL_MODULES, SEGMENTAL_CONSENSUSES, HEADS,\n                       RECOGNIZERS, LOCALIZERS, DETECTORS, ARCHITECTURES,\n                       NECKS, ROI_EXTRACTORS)\nfrom .builder import (build_backbone, build_spatial_temporal_module, build_segmental_consensus, \n                      build_head, build_recognizer, build_detector,\n                      build_localizer, build_architecture,\n                      build_neck, build_roi_extractor)\n\n__all__ = [\n    'BACKBONES', 'SPATIAL_TEMPORAL_MODULES', 'SEGMENTAL_CONSENSUSES', 'HEADS',\n    'RECOGNIZERS', 'LOCALIZERS', 'DETECTORS', 'ARCHITECTURES',\n    'NECKS', 'ROI_EXTRACTORS',\n    'build_backbone', 'build_spatial_temporal_module', 'build_segmental_consensus',\n    'build_head', 'build_recognizer', 'build_detector',\n    'build_localizer', 'build_architecture',\n    'build_neck', 'build_roi_extractor'\n]\n"""
mmaction/models/builder.py,0,"b""import mmcv\nfrom torch import nn\n\nfrom .registry import (BACKBONES, FLOWNETS, SPATIAL_TEMPORAL_MODULES,\n                       SEGMENTAL_CONSENSUSES, HEADS,\n                       RECOGNIZERS, DETECTORS, LOCALIZERS, ARCHITECTURES,\n                       NECKS, ROI_EXTRACTORS)\n\n\ndef _build_module(cfg, registry, default_args):\n    assert isinstance(cfg, dict) and 'type' in cfg\n    assert isinstance(default_args, dict) or default_args is None\n    args = cfg.copy()\n    obj_type = args.pop('type')\n    if mmcv.is_str(obj_type):\n        if obj_type not in registry.module_dict:\n            raise KeyError('{} is not in the {} registry'.format(\n                obj_type, registry.name))\n        obj_type = registry.module_dict[obj_type]\n    elif not isinstance(obj_type, type):\n        raise TypeError('type must be a str or valid type, but got {}'.format(\n            type(obj_type)))\n    if default_args is not None:\n        for name, value in default_args.items():\n            args.setdefault(name, value)\n    return obj_type(**args)\n\n\ndef build(cfg, registry, default_args=None):\n    if isinstance(cfg, list):\n        modules = [_build_module(cfg_, registry, default_args) for cfg_ in cfg]\n        return nn.Sequential(*modules)\n    else:\n        return _build_module(cfg, registry, default_args)\n\n\ndef build_backbone(cfg):\n    return build(cfg, BACKBONES)\n\n\ndef build_flownet(cfg):\n    return build(cfg, FLOWNETS)\n\n\ndef build_spatial_temporal_module(cfg):\n    return build(cfg, SPATIAL_TEMPORAL_MODULES)\n\n\ndef build_segmental_consensus(cfg):\n    return build(cfg, SEGMENTAL_CONSENSUSES)\n\n\ndef build_head(cfg):\n    return build(cfg, HEADS)\n\n\ndef build_recognizer(cfg, train_cfg=None, test_cfg=None):\n    return build(cfg, RECOGNIZERS,\n                 dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n\ndef build_localizer(cfg, train_cfg=None, test_cfg=None):\n    return build(cfg, LOCALIZERS, dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n\ndef build_detector(cfg, train_cfg=None, test_cfg=None):\n    return build(cfg, DETECTORS, dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n\ndef build_architecture(cfg, train_cfg=None, test_cfg=None):\n    return build(cfg, ARCHITECTURES,\n                 dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n\ndef build_neck(cfg):\n    return build(cfg, NECKS)\n\n\ndef build_roi_extractor(cfg):\n    return build(cfg, ROI_EXTRACTORS)\n"""
mmaction/models/registry.py,1,"b'import torch.nn as nn\n\n\nclass Registry(object):\n\n    def __init__(self, name):\n        self._name = name\n        self._module_dict = dict()\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def module_dict(self):\n        return self._module_dict\n\n    def _register_module(self, module_class):\n        """"""Register a module\n\n        Args:\n            module (:obj:`nn.Module`): Module to be registered.\n        """"""\n        if not issubclass(module_class, nn.Module):\n            raise TypeError(\n                \'module must be a child of nn.Module, but got {}\'.format(\n                    module_class))\n        module_name = module_class.__name__\n        if module_name in self._module_dict:\n            raise KeyError(\'{} is already registered in {}\'.format(\n                module_name, self.name))\n        self._module_dict[module_name] = module_class\n\n    def register_module(self, cls):\n        self._register_module(cls)\n        return cls\n\n\nBACKBONES = Registry(\'backbone\')\nFLOWNETS = Registry(\'flownet\')\nSPATIAL_TEMPORAL_MODULES = Registry(\'spatial_temporal_module\')\nSEGMENTAL_CONSENSUSES = Registry(\'segmental_consensus\')\nHEADS = Registry(\'head\')\nRECOGNIZERS = Registry(\'recognizer\')\nLOCALIZERS = Registry(\'localizer\')\nDETECTORS = Registry(\'detector\')\nARCHITECTURES = Registry(\'architecture\')\nNECKS = Registry(\'neck\')\nROI_EXTRACTORS = Registry(\'roi_extractor\')\n'"
mmaction/ops/__init__.py,0,"b""from .nms import nms, soft_nms\nfrom .roi_align import RoIAlign, roi_align\nfrom .roi_pool import RoIPool, roi_pool\n\n__all__ = [\n    'nms', 'soft_nms', 'RoIAlign', 'roi_align', 'RoIPool', 'roi_pool'\n]\n"""
mmaction/utils/misc.py,0,"b""import functools\nimport numpy as np\nimport mmcv\n\n\ndef rsetattr(obj, attr, val):\n    '''\n        See:\n        https://stackoverflow.com/questions/31174295/getattr-and-setattr-on-nested-objects\n    '''\n    pre, _, post = attr.rpartition('.')\n    return setattr(rgetattr(obj, pre) if pre else obj, post, val)\n\n\ndef rgetattr(obj, attr, *args):\n    def _getattr(obj, attr):\n        return getattr(obj, attr, *args)\n    return functools.reduce(_getattr, [obj] + attr.split('.'))\n\n\ndef rhasattr(obj, attr, *args):\n    def _hasattr(obj, attr):\n        if hasattr(obj, attr):\n            return getattr(obj, attr)\n        else:\n            return None\n    return functools.reduce(_hasattr, [obj] + attr.split('.')) is not None\n\n\ndef tensor2video_snaps(tensor, mean=(0, 0, 0), std=(1, 1, 1), to_rgb=True):\n    num_videos = tensor.size(0)\n    num_frames = tensor.size(2)\n    mean = np.array(mean, dtype=np.float32)\n    std = np.array(std, dtype=np.float32)\n    video_snaps = []\n    for vid_id in range(num_videos):\n        img = tensor[vid_id, :, num_frames //\n                     2, ...].cpu().numpy().transpose(1, 2, 0)\n        img = mmcv.imdenormalize(\n            img, mean, std, to_bgr=to_rgb).astype(np.uint8)\n        video_snaps.append(np.ascontiguousarray(img))\n    return video_snaps\n\n\ndef multi_apply(func, *args, **kwargs):\n    pfunc = functools.partial(func, **kwargs) if kwargs else func\n    map_results = map(pfunc, *args)\n    return tuple(map(list, zip(*map_results)))\n"""
test_configs/CSN/ipcsn_kinetics400_se_rgb_r152_seg1_32x2.py,0,"b'# model settings\nmodel = dict(\n    type=\'TSN3D\',\n    backbone=dict(\n        type=\'ResNet_3D\',\n        pretrained=None,\n        depth=152,\n        use_pool1=True,\n        block_type=\'0.3d\'),\n    spatial_temporal_module=dict(\n        type=\'SimpleSpatialTemporalModule\',\n        spatial_type=\'avg\',\n        temporal_size=-1,\n        spatial_size=-1),\n    segmental_consensus=dict(\n        type=\'SimpleConsensus\',\n        consensus_type=\'avg\'),\n    cls_head=dict(\n        type=\'ClsHead\',\n        with_avg_pool=False,\n        temporal_feature_size=1,\n        spatial_feature_size=1,\n        dropout_ratio=0.5,\n        in_channels=2048,\n        num_classes=400))\n\ntrain_cfg = None\ntest_cfg = None\n# dataset settings\ndataset_type = \'RawFramesDataset\'\ndata_root_val = \'data/kinetics400/rawframes_val/\'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n\ndata = dict(\n    test=dict(\n        type=dataset_type,\n        ann_file=\'data/kinetics400/kinetics400_train_list_rawframes.txt\',\n        img_prefix=data_root_val,\n        img_norm_cfg=img_norm_cfg,\n        input_format=""NCTHW"",\n        num_segments=10,\n        new_length=32,\n        new_step=2,\n        random_shift=True,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=256,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample=\'three_crop\',\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=True))\n\ndist_params = dict(backend=\'nccl\', port=16187)\n'"
test_configs/CSN/ircsn_kinetics400_se_rgb_r152_seg1_32x2.py,0,"b'# model settings\nmodel = dict(\n    type=\'TSN3D\',\n    backbone=dict(\n        type=\'ResNet_3D\',\n        pretrained=None,\n        depth=152,\n        use_pool1=True,\n        block_type=\'3d-sep\'),\n    spatial_temporal_module=dict(\n        type=\'SimpleSpatialTemporalModule\',\n        spatial_type=\'avg\',\n        temporal_size=-1,\n        spatial_size=-1),\n    segmental_consensus=dict(\n        type=\'SimpleConsensus\',\n        consensus_type=\'avg\'),\n    cls_head=dict(\n        type=\'ClsHead\',\n        with_avg_pool=False,\n        temporal_feature_size=1,\n        spatial_feature_size=1,\n        dropout_ratio=0.5,\n        in_channels=2048,\n        num_classes=400))\n\ntrain_cfg = None\ntest_cfg = None\n# dataset settings\ndataset_type = \'RawFramesDataset\'\ndata_root_val = \'data/kinetics400/rawframes_val/\'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n\ndata = dict(\n    test=dict(\n        type=dataset_type,\n        ann_file=\'data/kinetics400/kinetics400_train_list_rawframes.txt\',\n        img_prefix=data_root_val,\n        img_norm_cfg=img_norm_cfg,\n        input_format=""NCTHW"",\n        num_segments=10,\n        new_length=32,\n        new_step=2,\n        random_shift=True,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=256,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample=\'three_crop\',\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=True))\n\ndist_params = dict(backend=\'nccl\', port=16187)\n'"
test_configs/I3D_Flow/i3d_hmdb51_3d_tvl1_inception_v1_seg1_f64s1.py,0,"b'# model settings\nmodel = dict(\n    type=\'TSN3D\',\n    backbone=dict(\n        type=\'InceptionV1_I3D\',\n        pretrained=None,\n        modality=\'Flow\'),\n    spatial_temporal_module=dict(\n        type=\'SimpleSpatialTemporalModule\',\n        spatial_type=\'avg\',\n        temporal_size=-1,\n        spatial_size=-1),\n    segmental_consensus=dict(\n        type=\'SimpleConsensus\',\n        consensus_type=\'avg\'),\n    cls_head=dict(\n        type=\'ClsHead\',\n        with_avg_pool=False,\n        temporal_feature_size=1,\n        spatial_feature_size=1,\n        dropout_ratio=0.5,\n        in_channels=2048,\n        num_classes=51))\ntrain_cfg = None\ntest_cfg = None\n# dataset settings\ndataset_type = \'RawFramesDataset\'\ndata_root_val = \'data/hmdb51/rawframes/\'\nimg_norm_cfg = dict(\n    mean=[128, 128], std=[128, 128])\ndata = dict(\n    test=dict(\n        type=dataset_type,\n        ann_file=\'data/hmdb51/hmdb51_train_split_1_rawframes.txt\',\n        img_prefix=data_root_val,\n        img_norm_cfg=img_norm_cfg,\n        input_format=""NCTHW"",\n        num_segments=10,\n        new_length=64,\n        new_step=1,\n        random_shift=True,\n        modality=\'Flow\',\n        image_tmpl=\'{}_{:05d}.jpg\',\n        img_scale=256,\n        input_size=256,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample=\'three_crop\',\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=True))\n\ndist_params = dict(backend=\'nccl\')\n'"
test_configs/I3D_Flow/i3d_kinetics400_3d_tvl1_inception_v1_seg1_f64s1.py,0,"b'# model settings\nmodel = dict(\n    type=\'TSN3D\',\n    backbone=dict(\n        type=\'InceptionV1_I3D\',\n        pretrained=None,\n        modality=\'Flow\'),\n    spatial_temporal_module=dict(\n        type=\'SimpleSpatialTemporalModule\',\n        spatial_type=\'avg\',\n        temporal_size=-1,\n        spatial_size=-1),\n    segmental_consensus=dict(\n        type=\'SimpleConsensus\',\n        consensus_type=\'avg\'),\n    cls_head=dict(\n        type=\'ClsHead\',\n        with_avg_pool=False,\n        temporal_feature_size=1,\n        spatial_feature_size=1,\n        dropout_ratio=0.5,\n        in_channels=2048,\n        num_classes=400))\ntrain_cfg = None\ntest_cfg = None\n# dataset settings\ndataset_type = \'RawFramesDataset\'\ndata_root_val = \'data/kinetics400/rawframes_val/\'\nimg_norm_cfg = dict(\n    mean=[128, 128], std=[128, 128])\ndata = dict(\n    test=dict(\n        type=dataset_type,\n        ann_file=\'data/kinetics400/kinetics400_val_list_rawframes.txt\',\n        img_prefix=data_root_val,\n        img_norm_cfg=img_norm_cfg,\n        input_format=""NCTHW"",\n        num_segments=10,\n        new_length=64,\n        new_step=1,\n        random_shift=True,\n        modality=\'Flow\',\n        image_tmpl=\'{}_{:05d}.jpg\',\n        img_scale=256,\n        input_size=256,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample=\'three_crop\',\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=True))\n\ndist_params = dict(backend=\'nccl\')\n'"
test_configs/I3D_Flow/i3d_ucf101_3d_tvl1_inception_v1_seg1_f64s1.py,0,"b'# model settings\nmodel = dict(\n    type=\'TSN3D\',\n    backbone=dict(\n        type=\'InceptionV1_I3D\',\n        pretrained=None,\n        modality=\'Flow\'),\n    spatial_temporal_module=dict(\n        type=\'SimpleSpatialTemporalModule\',\n        spatial_type=\'avg\',\n        temporal_size=-1,\n        spatial_size=-1),\n    segmental_consensus=dict(\n        type=\'SimpleConsensus\',\n        consensus_type=\'avg\'),\n    cls_head=dict(\n        type=\'ClsHead\',\n        with_avg_pool=False,\n        temporal_feature_size=1,\n        spatial_feature_size=1,\n        dropout_ratio=0.5,\n        in_channels=2048,\n        num_classes=101))\ntrain_cfg = None\ntest_cfg = None\n# dataset settings\ndataset_type = \'RawFramesDataset\'\ndata_root_val = \'data/ucf101/rawframes/\'\nimg_norm_cfg = dict(\n    mean=[128, 128], std=[128, 128])\ndata = dict(\n    test=dict(\n        type=dataset_type,\n        ann_file=\'data/ucf101/ucf101_train_split_1_rawframes.txt\',\n        img_prefix=data_root_val,\n        img_norm_cfg=img_norm_cfg,\n        input_format=""NCTHW"",\n        num_segments=10,\n        new_length=64,\n        new_step=1,\n        random_shift=True,\n        modality=\'Flow\',\n        image_tmpl=\'{}_{:05d}.jpg\',\n        img_scale=256,\n        input_size=256,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample=\'three_crop\',\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=True))\n\ndist_params = dict(backend=\'nccl\')\n'"
test_configs/I3D_RGB/i3d_hmdb51_3d_rgb_inception_v1_seg1_f64s1.py,0,"b'# model settings\nmodel = dict(\n    type=\'TSN3D\',\n    backbone=dict(\n        type=\'InceptionV1_I3D\',\n        pretrained=None,\n        modality=\'RGB\'),\n    spatial_temporal_module=dict(\n        type=\'SimpleSpatialTemporalModule\',\n        spatial_type=\'avg\',\n        temporal_size=-1,\n        spatial_size=-1),\n    segmental_consensus=dict(\n        type=\'SimpleConsensus\',\n        consensus_type=\'avg\'),\n    cls_head=dict(\n        type=\'ClsHead\',\n        with_avg_pool=False,\n        temporal_feature_size=1,\n        spatial_feature_size=1,\n        dropout_ratio=0.5,\n        in_channels=2048,\n        num_classes=51))\ntrain_cfg = None\ntest_cfg = None\n# dataset settings\ndataset_type = \'RawFramesDataset\'\ndata_root_val = \'data/hmdb51/rawframes/\'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    test=dict(\n        type=dataset_type,\n        ann_file=\'data/hmdb51/hmdb51_train_split_1_rawframes.txt\',\n        img_prefix=data_root_val,\n        img_norm_cfg=img_norm_cfg,\n        input_format=""NCTHW"",\n        num_segments=10,\n        new_length=64,\n        new_step=1,\n        random_shift=True,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=256,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample=\'three_crop\',\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=True))\n\ndist_params = dict(backend=\'nccl\')\n'"
test_configs/I3D_RGB/i3d_kinetics400_3d_rgb_inception_v1_seg1_f64s1.py,0,"b'# model settings\nmodel = dict(\n    type=\'TSN3D\',\n    backbone=dict(\n        type=\'InceptionV1_I3D\',\n        pretrained=None,\n        modality=\'RGB\'),\n    spatial_temporal_module=dict(\n        type=\'SimpleSpatialTemporalModule\',\n        spatial_type=\'avg\',\n        temporal_size=-1,\n        spatial_size=-1),\n    segmental_consensus=dict(\n        type=\'SimpleConsensus\',\n        consensus_type=\'avg\'),\n    cls_head=dict(\n        type=\'ClsHead\',\n        with_avg_pool=False,\n        temporal_feature_size=1,\n        spatial_feature_size=1,\n        dropout_ratio=0.5,\n        in_channels=2048,\n        num_classes=400))\ntrain_cfg = None\ntest_cfg = None\n# dataset settings\ndataset_type = \'RawFramesDataset\'\ndata_root_val = \'data/kinetics400/rawframes_val/\'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    test=dict(\n        type=dataset_type,\n        ann_file=\'data/kinetics400/kinetics400_val_list_rawframes.txt\',\n        img_prefix=data_root_val,\n        img_norm_cfg=img_norm_cfg,\n        input_format=""NCTHW"",\n        num_segments=10,\n        new_length=64,\n        new_step=1,\n        random_shift=True,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=256,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample=\'three_crop\',\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=True))\n\ndist_params = dict(backend=\'nccl\')\n'"
test_configs/I3D_RGB/i3d_kinetics400_3d_rgb_r50_c3d_inflate3x1x1_seg1_f32s2.py,0,"b'# model settings\nmodel = dict(\n    type=\'TSN3D\',\n    backbone=dict(\n        type=\'ResNet_I3D\',\n        pretrained=None,\n        depth=50,\n        num_stages=4,\n        out_indices=[3],\n        frozen_stages=-1,\n        inflate_freq=((1,1,1), (1,0,1,0), (1,0,1,0,1,0), (0,1,0)),\n        inflate_style=\'3x1x1\',\n        conv1_kernel_t=5,\n        conv1_stride_t=2,\n        pool1_kernel_t=1,\n        pool1_stride_t=2,\n        bn_eval=False,\n        partial_bn=False,\n        style=\'pytorch\'),\n    spatial_temporal_module=dict(\n        type=\'SimpleSpatialTemporalModule\',\n        spatial_type=\'avg\',\n        temporal_size=4,\n        spatial_size=7),\n    segmental_consensus=dict(\n        type=\'SimpleConsensus\',\n        consensus_type=\'avg\'),\n    cls_head=dict(\n        type=\'ClsHead\',\n        with_avg_pool=False,\n        temporal_feature_size=1,\n        spatial_feature_size=1,\n        dropout_ratio=0.5,\n        in_channels=2048,\n        num_classes=400))\ntrain_cfg = None\ntest_cfg = None\n# dataset settings\ndataset_type = \'RawFramesDataset\'\ndata_root_val = \'data/kinetics400/rawframes_val/\'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    test=dict(\n        type=dataset_type,\n        ann_file=\'data/kinetics400/kinetics400_val_list_rawframes.txt\',\n        img_prefix=data_root_val,\n        img_norm_cfg=img_norm_cfg,\n        input_format=""NCTHW"",\n        num_segments=10,\n        new_length=32,\n        new_step=2,\n        random_shift=True,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=256,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample=\'three_crop\',\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=True))\n\ndist_params = dict(backend=\'nccl\')\n'"
test_configs/I3D_RGB/i3d_ucf101_3d_rgb_inception_v1_seg1_f64s1.py,0,"b'# model settings\nmodel = dict(\n    type=\'TSN3D\',\n    backbone=dict(\n        type=\'InceptionV1_I3D\',\n        pretrained=None,\n        modality=\'RGB\'),\n    spatial_temporal_module=dict(\n        type=\'SimpleSpatialTemporalModule\',\n        spatial_type=\'avg\',\n        temporal_size=-1,\n        spatial_size=-1),\n    segmental_consensus=dict(\n        type=\'SimpleConsensus\',\n        consensus_type=\'avg\'),\n    cls_head=dict(\n        type=\'ClsHead\',\n        with_avg_pool=False,\n        temporal_feature_size=1,\n        spatial_feature_size=1,\n        dropout_ratio=0.5,\n        in_channels=2048,\n        num_classes=101))\ntrain_cfg = None\ntest_cfg = None\n# dataset settings\ndataset_type = \'RawFramesDataset\'\ndata_root_val = \'data/ucf101/rawframes/\'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    test=dict(\n        type=dataset_type,\n        ann_file=\'data/ucf101/ucf101_train_split_1_rawframes.txt\',\n        img_prefix=data_root_val,\n        img_norm_cfg=img_norm_cfg,\n        input_format=""NCTHW"",\n        num_segments=10,\n        new_length=64,\n        new_step=1,\n        random_shift=True,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=256,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample=\'three_crop\',\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=True))\n\ndist_params = dict(backend=\'nccl\')\n'"
test_configs/R2plus1D/r2plus1d_kinetics400_se_rgb_r34_seg1_32x2.py,0,"b'# model settings\nmodel = dict(\n    type=\'TSN3D\',\n    backbone=dict(\n        type=\'ResNet_3D\',\n        pretrained=None,\n        depth=34,\n        use_pool1=True,\n        block_type=\'2.5d\',\n        use_syncbn=True),\n    spatial_temporal_module=dict(\n        type=\'SimpleSpatialTemporalModule\',\n        spatial_type=\'avg\',\n        temporal_size=-1,\n        spatial_size=-1),\n    segmental_consensus=dict(\n        type=\'SimpleConsensus\',\n        consensus_type=\'avg\'),\n    cls_head=dict(\n        type=\'ClsHead\',\n        with_avg_pool=False,\n        temporal_feature_size=1,\n        spatial_feature_size=1,\n        dropout_ratio=0.5,\n        in_channels=512,\n        num_classes=400))\ntrain_cfg = None\ntest_cfg = None\n# dataset settings\ndataset_type = \'RawFramesDataset\'\ndata_root_val = \'data/kinetics400/rawframes_val/\'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    test=dict(\n        type=dataset_type,\n        ann_file=\'data/kinetics400/kinetics400_train_list_rawframes.txt\',\n        img_prefix=data_root_val,\n        img_norm_cfg=img_norm_cfg,\n        input_format=""NCTHW"",\n        num_segments=10,\n        new_length=32,\n        new_step=2,\n        random_shift=True,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=256,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample=\'three_crop\',\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=True))\n\ndist_params = dict(backend=\'nccl\', port=16187)\n'"
test_configs/R2plus1D/r2plus1d_kinetics400_se_rgb_r34_seg1_8x8.py,0,"b'# model settings\nmodel = dict(\n    type=\'TSN3D\',\n    backbone=dict(\n        type=\'ResNet_3D\',\n        pretrained=None,\n        depth=34,\n        use_pool1=True,\n        block_type=\'2.5d\',\n        use_syncbn=True),\n    spatial_temporal_module=dict(\n        type=\'SimpleSpatialTemporalModule\',\n        spatial_type=\'avg\',\n        temporal_size=-1,\n        spatial_size=-1),\n    segmental_consensus=dict(\n        type=\'SimpleConsensus\',\n        consensus_type=\'avg\'),\n    cls_head=dict(\n        type=\'ClsHead\',\n        with_avg_pool=False,\n        temporal_feature_size=1,\n        spatial_feature_size=1,\n        dropout_ratio=0.5,\n        in_channels=512,\n        num_classes=400))\ntrain_cfg = None\ntest_cfg = None\n# dataset settings\ndataset_type = \'RawFramesDataset\'\ndata_root_val = \'data/kinetics400/rawframes_val/\'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    test=dict(\n        type=dataset_type,\n        ann_file=\'data/kinetics400/kinetics400_train_list_rawframes.txt\',\n        img_prefix=data_root_val,\n        img_norm_cfg=img_norm_cfg,\n        input_format=""NCTHW"",\n        num_segments=10,\n        new_length=8,\n        new_step=8,\n        random_shift=True,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=256,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample=\'three_crop\',\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=True))\n\ndist_params = dict(backend=\'nccl\', port=16187)\n'"
test_configs/SlowFast/slowfast_kinetics400_se_rgb_r50_seg1_4x16.py,0,"b'# model settings\nmodel = dict(\n    type=\'TSN3D\',\n    backbone=dict(\n        type=\'ResNet_I3D_SlowFast\',\n        pretrained_slow=None,\n        pretrained_fast=None,\n        depth=50,\n        tau=8,   # f32s2, tau=8 <=> f64s1, tau=16\n        alpha=8,\n        beta_inv=8,\n        num_stages=4,\n        out_indices=[3],\n        frozen_stages=-1,\n        slow_inflate_freq=(0, 0, 1, 1),\n        fast_inflate_freq=(1, 1, 1, 1),\n        inflate_style=\'3x1x1\',\n        bn_eval=False,\n        partial_bn=False,\n        style=\'pytorch\'),\n    spatial_temporal_module=dict(\n        type=\'SlowFastSpatialTemporalModule\',\n        adaptive_pool=True,\n        spatial_type=\'avg\',\n        temporal_size=1,\n        spatial_size=1),\n    segmental_consensus=dict(\n        type=\'SimpleConsensus\',\n        consensus_type=\'avg\'),\n    cls_head=dict(\n        type=\'ClsHead\',\n        with_avg_pool=False,\n        temporal_feature_size=1,\n        spatial_feature_size=1,\n        dropout_ratio=0.5,\n        in_channels=2304,  # 2048+256\n        num_classes=400))\ntrain_cfg = None\ntest_cfg = None\n# dataset settings\ndataset_type = \'RawFramesDataset\'\ndata_root_val = \'data/kinetics400/rawframes_val/\'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n\ndata = dict(\n    test=dict(\n        type=dataset_type,\n        ann_file=\'data/kinetics400/kinetics400_val_list_rawframes.txt\',\n        img_prefix=data_root_val,\n        img_norm_cfg=img_norm_cfg,\n        input_format=""NCTHW"",\n        num_segments=10,\n        new_length=32,\n        new_step=2,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=256,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample=\'three_crop\',\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=True)\n)\n\ndist_params = dict(backend=\'nccl\', port=16187)\n'"
test_configs/SlowOnly/slowonly_kinetics400_se_rgb_r101_seg1_8x8.py,0,"b'model = dict(\n    type=\'TSN3D\',\n    backbone=dict(\n        type=\'ResNet_I3D\',\n        pretrained=None,\n        depth=101,\n        num_stages=4,\n        out_indices=[3],\n        frozen_stages=-1,\n        inflate_freq=(0, 0, 1, 1),\n        conv1_kernel_t=1,\n        conv1_stride_t=1,\n        pool1_kernel_t=1,\n        pool1_stride_t=1,\n        inflate_style=\'3x1x1\',\n        bn_eval=False,\n        no_pool2=True,\n        partial_bn=False,\n        style=\'pytorch\'),\n    spatial_temporal_module=dict(\n        type=\'SimpleSpatialTemporalModule\',\n        spatial_type=\'avg\',\n        temporal_size=-1,\n        spatial_size=-1),\n    segmental_consensus=dict(\n        type=\'SimpleConsensus\',\n        consensus_type=\'avg\'),\n    cls_head=dict(\n        type=\'ClsHead\',\n        with_avg_pool=False,\n        temporal_feature_size=1,\n        spatial_feature_size=1,\n        dropout_ratio=0.5,\n        in_channels=2048,\n        num_classes=400))\ntrain_cfg = None\ntest_cfg = None\n# dataset settings\ndataset_type = \'RawFramesDataset\'\ndata_root_val = \'data/kinetics400/rawframes_val/\'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n   test=dict(\n        type=dataset_type,\n        ann_file=\'data/kinetics400/kinetics400_val_list_rawframes.txt\',\n        img_prefix=data_root_val,\n        img_norm_cfg=img_norm_cfg,\n        input_format=""NCTHW"",\n        num_segments=10,\n        new_length=8,\n        new_step=8,\n        random_shift=False,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=256,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample=\'three_crop\',\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=True))\n\ndist_params = dict(backend=\'nccl\')\n'"
test_configs/SlowOnly/slowonly_kinetics400_se_rgb_r50_seg1_4x16.py,0,"b'model = dict(\n    type=\'TSN3D\',\n    backbone=dict(\n        type=\'ResNet_I3D\',\n        pretrained=None,\n        depth=50,\n        num_stages=4,\n        out_indices=[3],\n        frozen_stages=-1,\n        inflate_freq=(0, 0, 1, 1),\n        conv1_kernel_t=1,\n        conv1_stride_t=1,\n        pool1_kernel_t=1,\n        pool1_stride_t=1,\n        inflate_style=\'3x1x1\',\n        bn_eval=False,\n        no_pool2=True,\n        partial_bn=False,\n        style=\'pytorch\'),\n    spatial_temporal_module=dict(\n        type=\'SimpleSpatialTemporalModule\',\n        spatial_type=\'avg\',\n        temporal_size=-1,\n        spatial_size=-1),\n    segmental_consensus=dict(\n        type=\'SimpleConsensus\',\n        consensus_type=\'avg\'),\n    cls_head=dict(\n        type=\'ClsHead\',\n        with_avg_pool=False,\n        temporal_feature_size=1,\n        spatial_feature_size=1,\n        dropout_ratio=0.5,\n        in_channels=2048,\n        num_classes=400))\ntrain_cfg = None\ntest_cfg = None\n# dataset settings\ndataset_type = \'RawFramesDataset\'\ndata_root_val = \'data/kinetics400/rawframes_val/\'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n   test=dict(\n        type=dataset_type,\n        ann_file=\'data/kinetics400/kinetics400_val_list_rawframes.txt\',\n        img_prefix=data_root_val,\n        img_norm_cfg=img_norm_cfg,\n        input_format=""NCTHW"",\n        num_segments=10,\n        new_length=4,\n        new_step=16,\n        random_shift=False,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=256,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample=\'three_crop\',\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=True))\n\ndist_params = dict(backend=\'nccl\')\n'"
test_configs/SlowOnly/slowonly_kinetics400_se_rgb_r50_seg1_8x8.py,0,"b'model = dict(\n    type=\'TSN3D\',\n    backbone=dict(\n        type=\'ResNet_I3D\',\n        pretrained=None,\n        depth=50,\n        num_stages=4,\n        out_indices=[3],\n        frozen_stages=-1,\n        inflate_freq=(0, 0, 1, 1),\n        conv1_kernel_t=1,\n        conv1_stride_t=1,\n        pool1_kernel_t=1,\n        pool1_stride_t=1,\n        inflate_style=\'3x1x1\',\n        bn_eval=False,\n        no_pool2=True,\n        partial_bn=False,\n        style=\'pytorch\'),\n    spatial_temporal_module=dict(\n        type=\'SimpleSpatialTemporalModule\',\n        spatial_type=\'avg\',\n        temporal_size=-1,\n        spatial_size=-1),\n    segmental_consensus=dict(\n        type=\'SimpleConsensus\',\n        consensus_type=\'avg\'),\n    cls_head=dict(\n        type=\'ClsHead\',\n        with_avg_pool=False,\n        temporal_feature_size=1,\n        spatial_feature_size=1,\n        dropout_ratio=0.5,\n        in_channels=2048,\n        num_classes=400))\ntrain_cfg = None\ntest_cfg = None\n# dataset settings\ndataset_type = \'RawFramesDataset\'\ndata_root_val = \'data/kinetics400/rawframes_val/\'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n   test=dict(\n        type=dataset_type,\n        ann_file=\'data/kinetics400/kinetics400_val_list_rawframes.txt\',\n        img_prefix=data_root_val,\n        img_norm_cfg=img_norm_cfg,\n        input_format=""NCTHW"",\n        num_segments=10,\n        new_length=8,\n        new_step=8,\n        random_shift=False,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=256,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample=\'three_crop\',\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=True))\n\ndist_params = dict(backend=\'nccl\')\n'"
test_configs/TSN/tsn_kinetics400_2d_rgb_r50_seg3_f1s1.py,0,"b'model = dict(\n    type=\'TSN2D\',\n    backbone=dict(\n        type=\'ResNet\',\n        pretrained=None,\n        depth=50,\n        out_indices=(3,),\n        bn_eval=False,\n        partial_bn=False),\n    spatial_temporal_module=dict(\n        type=\'SimpleSpatialModule\',\n        spatial_type=\'avg\',\n        spatial_size=7),\n    segmental_consensus=dict(\n        type=\'SimpleConsensus\',\n        consensus_type=\'avg\'),\n    cls_head=dict(\n        type=\'ClsHead\',\n        with_avg_pool=False,\n        temporal_feature_size=1,\n        spatial_feature_size=1,\n        dropout_ratio=0.4,\n        in_channels=2048,\n        num_classes=400))\ntrain_cfg = None\ntest_cfg = None\n# dataset settings\ndataset_type = \'RawFramesDataset\'\ndata_root_val = \'data/kinetics400/rawframes_val\'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n\ndata = dict(\n    test=dict(\n        type=dataset_type,\n        ann_file=\'data/kinetics400/kinetics400_val_list_rawframes.txt\',\n        img_prefix=data_root_val,\n        img_norm_cfg=img_norm_cfg,\n        num_segments=25,\n        new_length=1,\n        new_step=1,\n        random_shift=False,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=256,\n        input_size=224,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample=""ten_crop"",\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=True))\n\ndist_params = dict(backend=\'nccl\')\n'"
test_configs/ava/ava_fast_rcnn_nl_r50_c4_1x_kinetics_pretrain_crop.py,0,"b'# model settings\nmodel = dict(\n    type=\'FastRCNN\',\n    backbone=dict(\n        type=\'ResNet_I3D\',\n        pretrained=None,\n        pretrained2d=False,\n        depth=50,\n        num_stages=3,\n        spatial_strides=(1, 2, 2),\n        temporal_strides=(1, 1, 1),\n        dilations=(1, 1, 1),\n        out_indices=(2,),\n        frozen_stages=-1,\n        inflate_freq=((1,1,1), (1,0,1,0), (1,0,1,0,1,0)),\n        inflate_style=\'3x1x1\',\n        nonlocal_stages=(1, 2),\n        # nonlocal_freq=((0,0,0), (0,1,0,1), (0,1,0,1,0,1)),\n        nonlocal_cfg=dict(nonlocal_type=""gaussian""),\n        nonlocal_freq=((0,0,0), (0,1,0,1), (0,1,0,1,0,1), (0,0,0)),\n        conv1_kernel_t=5,\n        conv1_stride_t=1,\n        pool1_kernel_t=1,\n        pool1_stride_t=1,\n        bn_eval=False,\n        partial_bn=False,\n        bn_frozen=True,\n        style=\'pytorch\'),\n    shared_head=dict(\n        type=\'ResI3DLayer\',\n        pretrained=None,\n        pretrained2d=False,\n        depth=50,\n        stage=3,\n        spatial_stride=2,\n        temporal_stride=1,\n        dilation=1,\n        style=\'pytorch\',\n        inflate_freq=(0, 1, 0),\n        inflate_style=\'3x1x1\',\n        bn_eval=False,\n        bn_frozen=True),\n    bbox_roi_extractor=dict(\n        type=\'SingleRoIStraight3DExtractor\',\n        roi_layer=dict(type=\'RoIAlign\', out_size=16, sample_num=2),\n        out_channels=1024,\n        featmap_strides=[16],\n        with_temporal_pool=True),\n    dropout_ratio=0.3,\n    bbox_head=dict(\n        type=\'BBoxHead\',\n        with_reg=False,\n        with_temporal_pool=False,\n        with_spatial_pool=True,\n        spatial_pool_type=\'max\',\n        roi_feat_size=(1, 8, 8),\n        in_channels=2048,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        multilabel_classification=True,\n        reg_class_agnostic=True,\n        nms_class_agnostic=True))\n# model training and testing settings\ntest_cfg = dict(\n    train_detector=False,\n    person_det_score_thr=0.85,\n    rcnn=dict(\n        score_thr=0.00, nms=dict(type=\'nms\', iou_thr=1.0), max_per_img=100,\n        action_thr=0.00))\n# dataset settings\ndataset_type = \'AVADataset\'\ndata_root = \'data/ava/rawframes/\'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    test=dict(\n        type=dataset_type,\n        ann_file=\'data/ava/annotations/ava_val_v2.1.csv\',\n        exclude_file=\'data/ava/annotations/ava_val_excluded_timestamps_v2.1.csv\',\n        label_file=\'data/ava/annotations/ava_action_list_v2.1_for_activitynet_2018.pbtxt\',\n        video_stat_file=\'data/ava/ava_video_resolution_stats.csv\',\n        proposal_file=\'data/ava/ava_dense_proposals_val.FAIR.recall_93.9.pkl\',\n        img_prefix=data_root,\n        img_norm_cfg=img_norm_cfg,\n        input_format=\'NCTHW\',\n        new_length=32,\n        new_step=2,\n        random_shift=False,\n        modality=\'RGB\',\n        image_tmpl=\'img_{:05d}.jpg\',\n        img_scale=[(800, 256), ],\n        input_size=None,\n        div_255=False,\n        size_divisor=32,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        with_label=False,\n        test_mode=True))\n\ndist_params = dict(backend=\'nccl\')\n'"
test_configs/thumos14/ssn_thumos14_rgb_bn_inception.py,0,"b""# model settings\nmodel = dict(\n    type='SSN2D',\n    backbone=dict(\n        type='BNInception',\n        pretrained=None,\n        bn_eval=False,\n        partial_bn=True),\n    spatial_temporal_module=dict(\n        type='SimpleSpatialModule',\n        spatial_type='avg',\n        spatial_size=7),\n    dropout_ratio=0.8,\n    segmental_consensus=dict(\n        type='StructuredTemporalPyramidPooling',\n        standalong_classifier=True,\n        stpp_cfg=(1, 1, 1),\n        num_seg=(2, 5, 2)),\n    cls_head=dict(\n        type='SSNHead',\n        dropout_ratio=0.,\n        in_channels_activity=1024,\n        in_channels_complete=3072,\n        num_classes=20,\n        with_bg=False,\n        with_reg=True))\n# model training and testing settings\ntest_cfg=dict(\n    ssn=dict(\n        sampler=dict(\n            test_interval=6,\n            batch_size=16),\n        evaluater=dict(\n            top_k=2000,\n            nms=0.2,\n            softmax_before_filter=True,\n            cls_score_dict=None,\n            cls_top_k=2)))\n# dataset settings\ndataset_type = 'SSNDataset'\ndata_root = './data/thumos14/rawframes/'\nimg_norm_cfg = dict(\n    mean=[104, 117, 128], std=[1, 1, 1], to_rgb=False)\ndata = dict(\n    test=dict(\n        type=dataset_type,\n        ann_file='data/thumos14/thumos14_tag_test_normalized_proposal_list.txt',\n        img_prefix=data_root,\n        img_norm_cfg=img_norm_cfg,\n        train_cfg=train_cfg,\n        test_cfg=test_cfg,\n        input_format='NCHW',\n        aug_ratio=0.5,\n        new_length=1,\n        new_step=1,\n        random_shift=False,\n        modality='RGB',\n        image_tmpl='img_{:05d}.jpg',\n        img_scale=(340, 256),\n        input_size=224,\n        oversample=None,\n        div_255=False,\n        size_divisor=32,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        test_mode=True))\n\ndist_params = dict(backend='nccl')\n"""
configs/TSN/ucf101/tsn_flow_bninception.py,0,"b""# model settings\nmodel = dict(\n    type='TSN2D',\n    modality='Flow',\n    in_channels=10,\n    backbone=dict(\n        type='BNInception',\n        pretrained='open-mmlab://bninception_caffe',\n        bn_eval=False,\n        partial_bn=True),\n    spatial_temporal_module=dict(\n        type='SimpleSpatialModule',\n        spatial_type='avg',\n        spatial_size=7),\n    segmental_consensus=dict(\n        type='SimpleConsensus',\n        consensus_type='avg'),\n    cls_head=dict(\n        type='ClsHead',\n        with_avg_pool=False,\n        temporal_feature_size=1,\n        spatial_feature_size=1,\n        dropout_ratio=0.7,\n        in_channels=1024,\n        num_classes=101))\ntrain_cfg = None\ntest_cfg = None\n# dataset settings\ndataset_type = 'RawFramesDataset'\ndata_root = 'data/ucf101/rawframes'\nimg_norm_cfg = dict(\n    mean=[128], std=[1], to_rgb=False)\ndata = dict(\n    videos_per_gpu=32,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file='data/ucf101/ucf101_train_split_1_rawframes.txt',\n        img_prefix=data_root,\n        img_norm_cfg=img_norm_cfg,\n        num_segments=3,\n        new_length=5,\n        new_step=1,\n        random_shift=True,\n        modality='Flow',\n        image_tmpl='flow_{}_{:05d}.jpg',\n        img_scale=256,\n        input_size=224,\n        div_255=False,\n        flip_ratio=0.5,\n        resize_keep_ratio=True,\n        oversample=None,\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=True,\n        scales=[1, 0.875, 0.75, 0.66],\n        max_distort=1,\n        test_mode=False),\n    val=dict(\n        type=dataset_type,\n        ann_file='data/ucf101/ucf101_val_split_1_rawframes.txt',\n        img_prefix=data_root,\n        img_norm_cfg=img_norm_cfg,\n        num_segments=3,\n        new_length=5,\n        new_step=1,\n        random_shift=False,\n        modality='Flow',\n        image_tmpl='flow_{}_{:05d}.jpg',\n        img_scale=256,\n        input_size=224,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample=None,\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=False),\n    test=dict(\n        type=dataset_type,\n        ann_file='data/ucf101/ucf101_val_split_1_rawframes.txt',\n        img_prefix=data_root,\n        img_norm_cfg=img_norm_cfg,\n        num_segments=25,\n        new_length=5,\n        new_step=1,\n        random_shift=False,\n        modality='Flow',\n        image_tmpl='flow_{}_{:05d}.jpg',\n        img_scale=256,\n        input_size=224,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample='ten_crop',\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.005, momentum=0.9, weight_decay=0.0005)\noptimizer_config = dict(grad_clip=dict(max_norm=20, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    step=[190, 300])\ncheckpoint_config = dict(interval=1)\n# workflow = [('train', 5), ('val', 1)]\nworkflow = [('train', 1)]\n# yapf:disable\nlog_config = dict(\n    interval=20,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 340\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/tsn_2d_flow_bninception_seg_3_f1s1_b32_g8_lr_0.005'\nload_from = None\nresume_from = None\n\n\n\n"""
configs/TSN/ucf101/tsn_rgb_bninception.py,0,"b""# model settings\nmodel = dict(\n    type='TSN2D',\n    backbone=dict(\n        type='BNInception',\n        pretrained='open-mmlab://bninception_caffe',\n        bn_eval=False,\n        partial_bn=True),\n    spatial_temporal_module=dict(\n        type='SimpleSpatialModule',\n        spatial_type='avg',\n        spatial_size=7),\n    segmental_consensus=dict(\n        type='SimpleConsensus',\n        consensus_type='avg'),\n    cls_head=dict(\n        type='ClsHead',\n        with_avg_pool=False,\n        temporal_feature_size=1,\n        spatial_feature_size=1,\n        dropout_ratio=0.8,\n        in_channels=1024,\n        init_std=0.001,\n        num_classes=101))\ntrain_cfg = None\ntest_cfg = None\n# dataset settings\ndataset_type = 'RawFramesDataset'\ndata_root = 'data/ucf101/rawframes'\nimg_norm_cfg = dict(\n   mean=[104, 117, 128], std=[1, 1, 1], to_rgb=False)\n\ndata = dict(\n    videos_per_gpu=32,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file='data/ucf101/ucf101_train_split_1_rawframes.txt',\n        img_prefix=data_root,\n        img_norm_cfg=img_norm_cfg,\n        num_segments=3,\n        new_length=1,\n        new_step=1,\n        random_shift=True,\n        modality='RGB',\n        image_tmpl='img_{:05d}.jpg',\n        img_scale=256,\n        input_size=224,\n        div_255=False,\n        flip_ratio=0.5,\n        resize_keep_ratio=True,\n        oversample=None,\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=True,\n        scales=[1, 0.875, 0.75, 0.66],\n        max_distort=1,\n        test_mode=False),\n    val=dict(\n        type=dataset_type,\n        ann_file='data/ucf101/ucf101_val_split_1_rawframes.txt',\n        img_prefix=data_root,\n        img_norm_cfg=img_norm_cfg,\n        num_segments=3,\n        new_length=1,\n        new_step=1,\n        random_shift=False,\n        modality='RGB',\n        image_tmpl='img_{:05d}.jpg',\n        img_scale=256,\n        input_size=224,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample=None,\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=False),\n    test=dict(\n        type=dataset_type,\n        ann_file='data/ucf101/ucf101_val_split_1_rawframes.txt',\n        img_prefix=data_root,\n        img_norm_cfg=img_norm_cfg,\n        num_segments=25,\n        new_length=1,\n        new_step=1,\n        random_shift=False,\n        modality='RGB',\n        image_tmpl='img_{:05d}.jpg',\n        img_scale=256,\n        input_size=224,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample='ten_crop',\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.001, momentum=0.9, weight_decay=0.0005)\noptimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    step=[30, 60])\ncheckpoint_config = dict(interval=1)\n# workflow = [('train', 5), ('val', 1)]\nworkflow = [('train', 1)]\n# yapf:disable\nlog_config = dict(\n    interval=20,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 80\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/tsn_2d_rgb_bninception_seg_3_f1s1_b32_g8'\nload_from = None\nresume_from = None\n\n\n\n"""
mmaction/core/anchor2d/__init__.py,0,"b""from .anchor_generator import AnchorGenerator\nfrom .anchor_target import anchor_target\n\n__all__ = ['AnchorGenerator', 'anchor_target']"""
mmaction/core/anchor2d/anchor_generator.py,9,"b""import torch\n\n\nclass AnchorGenerator(object):\n\n    def __init__(self, base_size, scales, ratios, scale_major=True, ctr=None):\n        self.base_size = base_size\n        self.scales = torch.Tensor(scales)\n        self.ratios = torch.Tensor(ratios)\n        self.scale_major = scale_major\n        self.ctr = ctr\n        self.base_anchors = self.gen_base_anchors()\n\n    @property\n    def num_base_anchors(self):\n        return self.base_anchors.size(0)\n\n    def gen_base_anchors(self):\n        w = self.base_size\n        h = self.base_size\n        if self.ctr is None:\n            x_ctr = 0.5 * (w - 1)\n            y_ctr = 0.5 * (h - 1)\n        else:\n            x_ctr, y_ctr = self.ctr\n\n        h_ratios = torch.sqrt(self.ratios)\n        w_ratios = 1 / h_ratios\n        if self.scale_major:\n            ws = (w * w_ratios[:, None] * self.scales[None, :]).view(-1)\n            hs = (h * h_ratios[:, None] * self.scales[None, :]).view(-1)\n        else:\n            ws = (w * self.scales[:, None] * w_ratios[None, :]).view(-1)\n            hs = (h * self.scales[:, None] * h_ratios[None, :]).view(-1)\n\n        base_anchors = torch.stack(\n            [\n                x_ctr - 0.5 * (ws - 1), y_ctr - 0.5 * (hs - 1),\n                x_ctr + 0.5 * (ws - 1), y_ctr + 0.5 * (hs - 1)\n            ],\n            dim=-1).round()\n\n        return base_anchors\n\n    def _meshgrid(self, x, y, row_major=True):\n        xx = x.repeat(len(y))\n        yy = y.view(-1, 1).repeat(1, len(x)).view(-1)\n        if row_major:\n            return xx, yy\n        else:\n            return yy, xx\n\n    def grid_anchors(self, featmap_size, stride=16, device='cuda'):\n        base_anchors = self.base_anchors.to(device)\n\n        feat_h, feat_w = featmap_size\n        shift_x = torch.arange(0, feat_w, device=device) * stride\n        shift_y = torch.arange(0, feat_h, device=device) * stride\n        shift_xx, shift_yy = self._meshgrid(shift_x, shift_y)\n        shifts = torch.stack([shift_xx, shift_yy, shift_xx, shift_yy], dim=-1)\n        shifts = shifts.type_as(base_anchors)\n        # first feat_w elements correspond to the first row of shifts\n        # add A anchors (1, A, 4) to K shifts (K, 1, 4) to get\n        # shifted anchors (K, A, 4), reshape to (K*A, 4)\n\n        all_anchors = base_anchors[None, :, :] + shifts[:, None, :]\n        all_anchors = all_anchors.view(-1, 4)\n        # first A rows correspond to A anchors of (0, 0) in feature map,\n        # then (0, 1), (0, 2), ...\n        return all_anchors\n\n    def valid_flags(self, featmap_size, valid_size, device='cuda'):\n        feat_h, feat_w = featmap_size\n        valid_h, valid_w = valid_size\n        assert valid_h <= feat_h and valid_w <= feat_w\n        valid_x = torch.zeros(feat_w, dtype=torch.uint8, device=device)\n        valid_y = torch.zeros(feat_h, dtype=torch.uint8, device=device)\n        valid_x[:valid_w] = 1\n        valid_y[:valid_h] = 1\n        valid_xx, valid_yy = self._meshgrid(valid_x, valid_y)\n        valid = valid_xx & valid_yy\n        valid = valid[:, None].expand(\n            valid.size(0), self.num_base_anchors).contiguous().view(-1)\n        return valid\n"""
mmaction/core/anchor2d/anchor_target.py,7,"b'import torch\n\nfrom ..bbox2d import assign_and_sample, build_assigner, PseudoSampler, bbox2delta\nfrom mmaction.utils.misc import multi_apply\n\ndef anchor_target(anchor_list,\n                  valid_flag_list,\n                  gt_bboxes_list,\n                  img_metas,\n                  target_means,\n                  target_stds,\n                  cfg,\n                  gt_bboxes_ignore_list=None,\n                  gt_labels_list=None,\n                  label_channels=1,\n                  sampling=True,\n                  unmap_outputs=True):\n    """"""Compute regression and classification targets for anchors.\n\n    Args:\n        anchor_list (list[list]): Multi level anchors of each image.\n        valid_flag_list (list[list]): Multi level valid flags of each image.\n        gt_bboxes_list (list[Tensor]): Ground truth bboxes of each image.\n        img_metas (list[dict]): Meta info of each image.\n        target_means (Iterable): Mean value of regression targets.\n        target_stds (Iterable): Std value of regression targets.\n        cfg (dict): RPN train configs.\n\n    Returns:\n        tuple\n    """"""\n    num_imgs = len(img_metas)\n    assert len(anchor_list) == len(valid_flag_list) == num_imgs\n\n    # anchor number of multi levels\n    num_level_anchors = [anchors.size(0) for anchors in anchor_list[0]]\n    # concat all level anchors and flags to a single tensor\n    for i in range(num_imgs):\n        assert len(anchor_list[i]) == len(valid_flag_list[i])\n        anchor_list[i] = torch.cat(anchor_list[i])\n        valid_flag_list[i] = torch.cat(valid_flag_list[i])\n\n    # compute targets for each image\n    if gt_bboxes_ignore_list is None:\n        gt_bboxes_ignore_list = [None for _ in range(num_imgs)]\n    if gt_labels_list is None:\n        gt_labels_list = [None for _ in range(num_imgs)]\n    (all_labels, all_label_weights, all_bbox_targets, all_bbox_weights,\n     pos_inds_list, neg_inds_list) = multi_apply(\n         anchor_target_single,\n         anchor_list,\n         valid_flag_list,\n         gt_bboxes_list,\n         gt_bboxes_ignore_list,\n         gt_labels_list,\n         img_metas,\n         target_means=target_means,\n         target_stds=target_stds,\n         cfg=cfg,\n         label_channels=label_channels,\n         sampling=sampling,\n         unmap_outputs=unmap_outputs)\n    # no valid anchors\n    if any([labels is None for labels in all_labels]):\n        return None\n    # sampled anchors of all images\n    num_total_pos = sum([max(inds.numel(), 1) for inds in pos_inds_list])\n    num_total_neg = sum([max(inds.numel(), 1) for inds in neg_inds_list])\n    # split targets to a list w.r.t. multiple levels\n    labels_list = images_to_levels(all_labels, num_level_anchors)\n    label_weights_list = images_to_levels(all_label_weights, num_level_anchors)\n    bbox_targets_list = images_to_levels(all_bbox_targets, num_level_anchors)\n    bbox_weights_list = images_to_levels(all_bbox_weights, num_level_anchors)\n    return (labels_list, label_weights_list, bbox_targets_list,\n            bbox_weights_list, num_total_pos, num_total_neg)\n\n\ndef images_to_levels(target, num_level_anchors):\n    """"""Convert targets by image to targets by feature level.\n\n    [target_img0, target_img1] -> [target_level0, target_level1, ...]\n    """"""\n    target = torch.stack(target, 0)\n    level_targets = []\n    start = 0\n    for n in num_level_anchors:\n        end = start + n\n        level_targets.append(target[:, start:end].squeeze(0))\n        start = end\n    return level_targets\n\n\ndef anchor_target_single(flat_anchors,\n                         valid_flags,\n                         gt_bboxes,\n                         gt_bboxes_ignore,\n                         gt_labels,\n                         img_meta,\n                         target_means,\n                         target_stds,\n                         cfg,\n                         label_channels=1,\n                         sampling=True,\n                         unmap_outputs=True):\n    inside_flags = anchor_inside_flags(flat_anchors, valid_flags,\n                                       img_meta[\'img_shape\'][:2],\n                                       cfg.allowed_border)\n    if not inside_flags.any():\n        return (None, ) * 6\n    # assign gt and sample anchors\n    anchors = flat_anchors[inside_flags, :]\n\n    if sampling:\n        assign_result, sampling_result = assign_and_sample(\n            anchors, gt_bboxes, gt_bboxes_ignore, None, cfg)\n    else:\n        bbox_assigner = build_assigner(cfg.assigner)\n        assign_result = bbox_assigner.assign(anchors, gt_bboxes,\n                                             gt_bboxes_ignore, gt_labels)\n        bbox_sampler = PseudoSampler()\n        sampling_result = bbox_sampler.sample(assign_result, anchors,\n                                              gt_bboxes)\n\n    num_valid_anchors = anchors.shape[0]\n    bbox_targets = torch.zeros_like(anchors)\n    bbox_weights = torch.zeros_like(anchors)\n    labels = anchors.new_zeros(num_valid_anchors, dtype=torch.long)\n    label_weights = anchors.new_zeros(num_valid_anchors, dtype=torch.float)\n\n    pos_inds = sampling_result.pos_inds\n    neg_inds = sampling_result.neg_inds\n    if len(pos_inds) > 0:\n        pos_bbox_targets = bbox2delta(sampling_result.pos_bboxes,\n                                      sampling_result.pos_gt_bboxes,\n                                      target_means, target_stds)\n        bbox_targets[pos_inds, :] = pos_bbox_targets\n        bbox_weights[pos_inds, :] = 1.0\n        if gt_labels is None:\n            labels[pos_inds] = 1\n        else:\n            labels[pos_inds] = gt_labels[sampling_result.pos_assigned_gt_inds]\n        if cfg.pos_weight <= 0:\n            label_weights[pos_inds] = 1.0\n        else:\n            label_weights[pos_inds] = cfg.pos_weight\n    if len(neg_inds) > 0:\n        label_weights[neg_inds] = 1.0\n\n    # map up to original set of anchors\n    if unmap_outputs:\n        num_total_anchors = flat_anchors.size(0)\n        labels = unmap(labels, num_total_anchors, inside_flags)\n        label_weights = unmap(label_weights, num_total_anchors, inside_flags)\n        bbox_targets = unmap(bbox_targets, num_total_anchors, inside_flags)\n        bbox_weights = unmap(bbox_weights, num_total_anchors, inside_flags)\n\n    return (labels, label_weights, bbox_targets, bbox_weights, pos_inds,\n            neg_inds)\n\n\ndef anchor_inside_flags(flat_anchors, valid_flags, img_shape,\n                        allowed_border=0):\n    img_h, img_w = img_shape[:2]\n    if allowed_border >= 0:\n        inside_flags = valid_flags & \\\n            (flat_anchors[:, 0] >= -allowed_border) & \\\n            (flat_anchors[:, 1] >= -allowed_border) & \\\n            (flat_anchors[:, 2] < img_w + allowed_border) & \\\n            (flat_anchors[:, 3] < img_h + allowed_border)\n    else:\n        inside_flags = valid_flags\n    return inside_flags\n\n\ndef unmap(data, count, inds, fill=0):\n    """""" Unmap a subset of item (data) back to the original set of items (of\n    size count) """"""\n    if data.dim() == 1:\n        ret = data.new_full((count, ), fill)\n        ret[inds] = data\n    else:\n        new_size = (count, ) + data.size()[1:]\n        ret = data.new_full(new_size, fill)\n        ret[inds, :] = data\n    return ret\n'"
mmaction/core/bbox1d/__init__.py,0,"b""from .geometry import temporal_iou\n\n__all__ = [\n    'temporal_iou'\n]\n"""
mmaction/core/bbox1d/geometry.py,0,"b'\ndef temporal_iou(span_A, span_B):\n    """"""\n    Calculates the intersection over union of two temporal ""bounding boxes""\n    span_A: (start, end)\n    span_B: (start, end)\n    """"""\n    union = min(span_A[0], span_B[0]), max(span_A[1], span_B[1])\n    inter = max(span_A[0], span_B[0]), min(span_A[1], span_B[1])\n\n    if inter[0] >= inter[1]:\n        return 0\n    else:\n        return float(inter[1] - inter[0]) / float(union[1] - union[0])\n'"
mmaction/core/bbox2d/__init__.py,0,"b""from .geometry import bbox_overlaps\nfrom .assigners import BaseAssigner, MaxIoUAssigner, AssignResult\nfrom .samplers import BaseSampler, PseudoSampler, RandomSampler\nfrom .assign_sampling import build_assigner, build_sampler, assign_and_sample\nfrom .transforms import (bbox2delta, delta2bbox, bbox_flip, bbox_mapping,\n                         bbox_mapping_back, bbox2roi, bbox2result)\nfrom .bbox_target import bbox_target\n\n__all__ = [\n    'bbox_overlaps', 'BaseAssigner', 'MaxIoUAssigner', 'AssignResult',\n    'BaseSampler', 'PseudoSampler', 'RandomSampler',\n    'build_assigner', 'build_sampler', 'assign_and_sample',\n    'bbox2delta', 'delta2bbox', 'bbox_flip', 'bbox_mapping',\n    'bbox_mapping_back', 'bbox2roi', 'roi2bbox', 'bbox2result', 'bbox_target'\n]\n"""
mmaction/core/bbox2d/assign_sampling.py,0,"b""import mmcv\n\nfrom . import assigners, samplers\n\n\ndef build_assigner(cfg, **kwargs):\n    if isinstance(cfg, assigners.BaseAssigner):\n        return cfg\n    elif isinstance(cfg, dict):\n        return mmcv.runner.obj_from_dict(\n            cfg, assigners, default_args=kwargs)\n    else:\n        raise TypeError('Invalid type {} for building a sampler'.format(\n            type(cfg)))\n\n\ndef build_sampler(cfg, **kwargs):\n    if isinstance(cfg, samplers.BaseSampler):\n        return cfg\n    elif isinstance(cfg, dict):\n        return mmcv.runner.obj_from_dict(\n            cfg, samplers, default_args=kwargs)\n    else:\n        raise TypeError('Invalid type {} for building a sampler'.format(\n            type(cfg)))\n\n\ndef assign_and_sample(bboxes, gt_bboxes, gt_bboxes_ignore, gt_labels, cfg):\n    bbox_assigner = build_assigner(cfg.assigner)\n    bbox_sampler = build_sampler(cfg.sampler)\n    assign_result = bbox_assigner.assign(bboxes, gt_bboxes, gt_bboxes_ignore,\n                                         gt_labels)\n    sampling_result = bbox_sampler.sample(assign_result, bboxes, gt_bboxes,\n                                          gt_labels)\n    return assign_result, sampling_result\n"""
mmaction/core/bbox2d/bbox_target.py,8,"b""import torch\n\nfrom .transforms import bbox2delta\nfrom mmaction.utils.misc import multi_apply\n\n\ndef bbox_target(pos_bboxes_list,\n                neg_bboxes_list,\n                pos_gt_bboxes_list,\n                pos_gt_labels_list,\n                cfg,\n                reg_classes=1,\n                target_means=[.0, .0, .0, .0],\n                target_stds=[1.0, 1.0, 1.0, 1.0],\n                concat=True):\n    (labels, label_weights, bbox_targets,\n     bbox_weights, class_weights) = multi_apply(\n        bbox_target_single,\n        pos_bboxes_list,\n        neg_bboxes_list,\n        pos_gt_bboxes_list,\n        pos_gt_labels_list,\n        cfg=cfg,\n        reg_classes=reg_classes,\n        target_means=target_means,\n        target_stds=target_stds)\n\n    if concat:\n        labels = torch.cat(labels, 0)\n        label_weights = torch.cat(label_weights, 0)\n        bbox_targets = torch.cat(bbox_targets, 0)\n        bbox_weights = torch.cat(bbox_weights, 0)\n        class_weights = torch.cat(class_weights, 0)\n    return labels, label_weights, bbox_targets, bbox_weights, class_weights\n\n\ndef bbox_target_single(pos_bboxes,\n                       neg_bboxes,\n                       pos_gt_bboxes,\n                       pos_gt_labels,\n                       cfg,\n                       reg_classes=1,\n                       target_means=[.0, .0, .0, .0],\n                       target_stds=[1.0, 1.0, 1.0, 1.0]):\n    num_pos = pos_bboxes.size(0)\n    num_neg = neg_bboxes.size(0)\n    num_samples = num_pos + num_neg\n    if len(pos_gt_labels[0]) == 1:\n        labels = pos_bboxes.new_zeros(num_samples, dtype=torch.long)\n    else:\n        labels = pos_bboxes.new_zeros(\n            (num_samples, len(pos_gt_labels[0])), dtype=torch.long)\n    label_weights = pos_bboxes.new_zeros(num_samples)\n    if len(pos_gt_labels[0]) == 1:\n        class_weights = pos_bboxes.new_zeros(num_samples)\n    else:\n        class_weights = pos_bboxes.new_zeros(\n            num_samples, len(pos_gt_labels[0]))\n    bbox_targets = pos_bboxes.new_zeros(num_samples, 4)\n    bbox_weights = pos_bboxes.new_zeros(num_samples, 4)\n    if num_pos > 0:\n        labels[:num_pos] = pos_gt_labels\n        pos_weight = 1.0 if cfg.pos_weight <= 0 else cfg.pos_weight\n        label_weights[:num_pos] = pos_weight\n        class_weight = 1.0 if not hasattr(\n            cfg, 'cls_weight') or cfg.cls_weight <= 0 else cfg.cls_weight\n        class_weights[:num_pos] = class_weight\n        pos_bbox_targets = bbox2delta(pos_bboxes, pos_gt_bboxes, target_means,\n                                      target_stds)\n        bbox_targets[:num_pos, :] = pos_bbox_targets\n        bbox_weights[:num_pos, :] = 1\n    if num_neg > 0:\n        label_weights[-num_neg:] = 1.0\n        class_weights[-num_neg:] = 0.0\n\n    return labels, label_weights, bbox_targets, bbox_weights, class_weights\n\n\ndef expand_target(bbox_targets, bbox_weights, labels, num_classes):\n    bbox_targets_expand = bbox_targets.new_zeros((bbox_targets.size(0),\n                                                  4 * num_classes))\n    bbox_weights_expand = bbox_weights.new_zeros((bbox_weights.size(0),\n                                                  4 * num_classes))\n    for i in torch.nonzero(labels > 0).squeeze(-1):\n        start, end = labels[i] * 4, (labels[i] + 1) * 4\n        bbox_targets_expand[i, start:end] = bbox_targets[i, :]\n        bbox_weights_expand[i, start:end] = bbox_weights[i, :]\n    return bbox_targets_expand, bbox_weights_expand\n"""
mmaction/core/bbox2d/geometry.py,4,"b'import torch\n\n\ndef bbox_overlaps(bboxes1, bboxes2, mode=\'iou\', is_aligned=False):\n    """"""Calculate overlap between two set of bboxes.\n\n    If ``is_aligned`` is ``False``, then calculate the ious between each bbox\n    of bboxes1 and bboxes2, otherwise the ious between each aligned pair of\n    bboxes1 and bboxes2.\n\n    Args:\n        bboxes1 (Tensor): shape (m, 4)\n        bboxes2 (Tensor): shape (n, 4), if is_aligned is ``True``, then m and n\n            must be equal.\n        mode (str): ""iou"" (intersection over union) or iof (intersection over\n            foreground).\n\n    Returns:\n        ious(Tensor): shape (m, n) if is_aligned == False else shape (m, 1)\n    """"""\n\n    assert mode in [\'iou\', \'iof\']\n\n    rows = bboxes1.size(0)\n    cols = bboxes2.size(0)\n    if is_aligned:\n        assert rows == cols\n\n    if rows * cols == 0:\n        return bboxes1.new(rows, 1) if is_aligned else bboxes1.new(rows, cols)\n\n    if is_aligned:\n        lt = torch.max(bboxes1[:, :2], bboxes2[:, :2])  # [rows, 2]\n        rb = torch.min(bboxes1[:, 2:], bboxes2[:, 2:])  # [rows, 2]\n\n        wh = (rb - lt + 1).clamp(min=0)  # [rows, 2]\n        overlap = wh[:, 0] * wh[:, 1]\n        area1 = (bboxes1[:, 2] - bboxes1[:, 0] + 1) * (\n            bboxes1[:, 3] - bboxes1[:, 1] + 1)\n\n        if mode == \'iou\':\n            area2 = (bboxes2[:, 2] - bboxes2[:, 0] + 1) * (\n                bboxes2[:, 3] - bboxes2[:, 1] + 1)\n            ious = overlap / (area1 + area2 - overlap)\n        else:\n            ious = overlap / area1\n    else:\n        lt = torch.max(bboxes1[:, None, :2], bboxes2[:, :2])  # [rows, cols, 2]\n        rb = torch.min(bboxes1[:, None, 2:], bboxes2[:, 2:])  # [rows, cols, 2]\n\n        wh = (rb - lt + 1).clamp(min=0)  # [rows, cols, 2]\n        overlap = wh[:, :, 0] * wh[:, :, 1]\n        area1 = (bboxes1[:, 2] - bboxes1[:, 0] + 1) * (\n            bboxes1[:, 3] - bboxes1[:, 1] + 1)\n\n        if mode == \'iou\':\n            area2 = (bboxes2[:, 2] - bboxes2[:, 0] + 1) * (\n                bboxes2[:, 3] - bboxes2[:, 1] + 1)\n            ious = overlap / (area1[:, None] + area2 - overlap)\n        else:\n            ious = overlap / (area1[:, None])\n\n    return ious\n'"
mmaction/core/bbox2d/transforms.py,10,"b'import mmcv\nimport numpy as np\nimport torch\n\n\ndef bbox2delta(proposals, gt, means=[0, 0, 0, 0], stds=[1, 1, 1, 1]):\n    assert proposals.size() == gt.size()\n\n    proposals = proposals.float()\n    gt = gt.float()\n    px = (proposals[..., 0] + proposals[..., 2]) * 0.5\n    py = (proposals[..., 1] + proposals[..., 3]) * 0.5\n    pw = (proposals[..., 2] - proposals[..., 0]) + 1.0\n    ph = (proposals[..., 3] - proposals[..., 1]) + 1.0\n\n    gx = (gt[..., 0] + gt[..., 2]) * 0.5\n    gy = (gt[..., 1] + gt[..., 3]) * 0.5\n    gw = (gt[..., 2] - gt[..., 0]) + 1.0\n    gh = (gt[..., 3] - gt[..., 1]) + 1.0\n\n    dx = (gx - px) / pw\n    dy = (gy - py) / ph\n    dw = torch.log(gw / pw)\n    dh = torch.log(gh / ph)\n    deltas = torch.stack([dx, dy, dw, dh], dim=-1)\n\n    means = deltas.new_tensor(means).unsqueeze(0)\n    stds = deltas.new_tensor(stds).unsqueeze(0)\n    deltas = deltas.sub_(means).div_(stds)\n\n    return deltas\n\n\ndef delta2bbox(rois,\n               deltas,\n               means=[0, 0, 0, 0],\n               stds=[1, 1, 1, 1],\n               max_shape=None,\n               wh_ratio_clip=16 / 1000):\n    means = deltas.new_tensor(means).repeat(1, deltas.size(1) // 4)\n    stds = deltas.new_tensor(stds).repeat(1, deltas.size(1) // 4)\n    denorm_deltas = deltas * stds + means\n    dx = denorm_deltas[:, 0::4]\n    dy = denorm_deltas[:, 1::4]\n    dw = denorm_deltas[:, 2::4]\n    dh = denorm_deltas[:, 3::4]\n    max_ratio = np.abs(np.log(wh_ratio_clip))\n    dw = dw.clamp(min=-max_ratio, max=max_ratio)\n    dh = dh.clamp(min=-max_ratio, max=max_ratio)\n    px = ((rois[:, 0] + rois[:, 2]) * 0.5).unsqueeze(1).expand_as(dx)\n    py = ((rois[:, 1] + rois[:, 3]) * 0.5).unsqueeze(1).expand_as(dy)\n    pw = ((rois[:, 2] - rois[:, 0]) + 1.0).unsqueeze(1).expand_as(dw)\n    ph = ((rois[:, 3] - rois[:, 1]) + 1.0).unsqueeze(1).expand_as(dh)\n    gw = pw * dw.exp()\n    gh = ph * dh.exp()\n    gx = torch.addcmul(px, 1, pw, dx)  # gx = px + pw * dx\n    gy = torch.addcmul(py, 1, ph, dy)  # gy = py + ph * dy\n    x1 = gx - gw * 0.5 + 0.5\n    y1 = gy - gh * 0.5 + 0.5\n    x2 = gx + gw * 0.5 - 0.5\n    y2 = gy + gh * 0.5 - 0.5\n    if max_shape is not None:\n        x1 = x1.clamp(min=0, max=max_shape[1] - 1)\n        y1 = y1.clamp(min=0, max=max_shape[0] - 1)\n        x2 = x2.clamp(min=0, max=max_shape[1] - 1)\n        y2 = y2.clamp(min=0, max=max_shape[0] - 1)\n    bboxes = torch.stack([x1, y1, x2, y2], dim=-1).view_as(deltas)\n    return bboxes\n\n\ndef bbox_flip(bboxes, img_shape):\n    """"""Flip bboxes horizontally.\n\n    Args:\n        bboxes(Tensor or ndarray): Shape (..., 4*k)\n        img_shape(tuple): Image shape.\n\n    Returns:\n        Same type as `bboxes`: Flipped bboxes.\n    """"""\n    if isinstance(bboxes, torch.Tensor):\n        assert bboxes.shape[-1] % 4 == 0\n        flipped = bboxes.clone()\n        flipped[:, 0::4] = img_shape[1] - bboxes[:, 2::4] - 1\n        flipped[:, 2::4] = img_shape[1] - bboxes[:, 0::4] - 1\n        return flipped\n    elif isinstance(bboxes, np.ndarray):\n        return mmcv.bbox_flip(bboxes, img_shape)\n\n\ndef bbox_mapping(bboxes, img_shape, scale_factor, flip):\n    """"""Map bboxes from the original image scale to testing scale""""""\n    new_bboxes = bboxes * scale_factor\n    if flip:\n        new_bboxes = bbox_flip(new_bboxes, img_shape)\n    return new_bboxes\n\n\ndef bbox_mapping_back(bboxes, img_shape, scale_factor, flip):\n    """"""Map bboxes from testing scale to original image scale""""""\n    new_bboxes = bbox_flip(bboxes, img_shape) if flip else bboxes\n    new_bboxes = new_bboxes / scale_factor\n    return new_bboxes\n\n\ndef bbox2roi(bbox_list):\n    """"""Convert a list of bboxes to roi format\n\n    Args:\n        bbox_list (list[Tensor]): a list of bboxes\n        corresponding to a batch of images\n\n    Returns:\n        Tensor: shape (n, 5), [batch_ind, x1, y1, x2, y2]\n    """"""\n    rois_list = []\n    for img_id, bboxes in enumerate(bbox_list):\n        if bboxes.size(0) > 0:\n            img_inds = bboxes.new_full((bboxes.size(0), 1), img_id)\n            rois = torch.cat([img_inds, bboxes[:, :4]], dim=-1)\n        else:\n            rois = bboxes.new_zeros((0, 5))\n        rois_list.append(rois)\n    rois = torch.cat(rois_list, 0)\n    return rois\n\n\ndef roi2bbox(rois):\n    bbox_list = []\n    img_ids = torch.unique(rois[:, 0].cpu(), sorted=True)\n    for img_id in img_ids:\n        inds = (rois[:, 0] == img_id.item())\n        bbox = rois[inds, 1:]\n        bbox_list.append(bbox)\n    return bbox_list\n\n\ndef bbox2result(bboxes, labels, num_classes, thr=0.01):\n    """"""Convert detection results to a list of numpy arrays.\n    Args:\n        bboxes (Tensor): shape (n, 5)\n        labels (Tensor): shape (n, ) or shape (n, #num_classes)\n        num_classes (int): class number, including background class\n    Returns:\n        list(ndarray): bbox results of each class\n    """"""\n    if bboxes.shape[0] == 0:\n        return [\n            np.zeros((0, 5), dtype=np.float32) for i in range(num_classes - 1)\n        ]\n    else:\n        bboxes = bboxes.cpu().numpy()\n        labels = labels.cpu().numpy()\n        if labels.ndim == 1:\n            return [bboxes[labels == i, :] for i in range(num_classes - 1)]\n        else:\n            scores = labels  # rename for clarification\n            thr = (thr, ) * num_classes if isinstance(thr, float) else thr\n            assert scores.shape[1] == num_classes\n            assert len(thr) == num_classes\n\n            result = []\n            for i in range(num_classes - 1):\n                where = scores[:, i+1] > thr[i+1]\n                result.append(np.concatenate(\n                    (bboxes[where, :4], scores[where, i+1:i+2]), axis=1))\n            return result\n'"
mmaction/core/evaluation/__init__.py,0,"b""from .class_names import (get_classes)\nfrom .eval_hooks import (DistEvalHook, DistEvalTopKAccuracyHook,\n                         AVADistEvalmAPHook)\n\n__all__ = [\n    'get_classes',\n    'DistEvalHook', 'DistEvalTopKAccuracyHook',\n    'AVADistEvalmAPHook'\n]\n"""
mmaction/core/evaluation/accuracy.py,0,"b'import numpy as np\nfrom sklearn.metrics import confusion_matrix\n\n\ndef softmax(x, dim=1):\n    """"""Compute softmax values for each sets of scores in x.""""""\n    e_x = np.exp(x - np.max(x, axis=dim, keepdims=True))\n    return e_x / e_x.sum(axis=dim, keepdims=True)\n\n\ndef mean_class_accuracy(scores, labels):\n    pred = np.argmax(scores, axis=1)\n    cf = confusion_matrix(labels, pred).astype(float)\n\n    cls_cnt = cf.sum(axis=1)\n    cls_hit = np.diag(cf)\n\n    return np.mean(cls_hit/cls_cnt)\n\n\ndef top_k_acc(score, lb_set, k=3):\n    idx = np.argsort(score)[-k:]\n    return len(lb_set.intersection(idx)), len(lb_set)\n\n\ndef top_k_hit(score, lb_set, k=3):\n    idx = np.argsort(score)[-k:]\n    return len(lb_set.intersection(idx)) > 0, 1\n\n\ndef top_k_accuracy(scores, labels, k=(1,)):\n    res = []\n    for kk in k:\n        hits = []\n        for x, y in zip(scores, labels):\n            y = [y] if isinstance(y, int) else y\n            hits.append(top_k_hit(x, set(y), k=kk)[0])\n        res.append(np.mean(hits))\n    return res\n'"
mmaction/core/evaluation/ava_utils.py,0,"b'import csv\nimport logging\nimport time\nfrom collections import defaultdict\nimport heapq\nimport numpy as np\n\nfrom .recall import eval_recalls\n\ntry:\n    import sys\n    import os.path as osp\n    sys.path.append(\n        osp.abspath(osp.join(__file__, \'../../../\',\n                             \'third_party/ActivityNet/Evaluation/ava\')))\n    from mmaction.third_party.ActivityNet.Evaluation.ava import (\n        object_detection_evaluation as det_eval)\n    import standard_fields\n\nexcept ImportError:\n    print(\'Failed to import ActivityNet evaluation toolbox. Did you clone with\'\n          \'""--recursive""?\')\n\n\ndef det2csv(dataset, results):\n    csv_results = []\n    for idx in range(len(dataset)):\n        video_id = dataset.video_infos[idx][\'video_id\']\n        timestamp = dataset.video_infos[idx][\'timestamp\']\n        width = dataset.video_infos[idx][\'width\']\n        height = dataset.video_infos[idx][\'height\']\n        result = results[idx]\n        for label in range(len(result)):\n            for bbox in result[label]:\n                _bbox = bbox.tolist()\n                x1 = _bbox[0] / width\n                y1 = _bbox[1] / height\n                x2 = _bbox[2] / width\n                y2 = _bbox[3] / height\n                csv_results.append(\n                    (video_id, timestamp, x1, y1, x2, y2, label + 1, _bbox[4]))\n    return csv_results\n\n\ndef results2csv(dataset, results, out_file):\n    if isinstance(results[0], list):\n        csv_results = det2csv(dataset, results)\n    # TODO: integrate CSVWriter into mmcv.fileio\n    # mmcv.dump(csv_results, out_file)\n    with open(out_file, \'w\') as f:\n        for csv_result in csv_results:\n            f.write(\',\'.join(map(lambda x: str(x), csv_result)))\n            f.write(\'\\n\')\n\n\ndef print_time(message, start):\n    print(""==> %g seconds to %s"" % (time.time() - start, message))\n\n\ndef make_image_key(video_id, timestamp):\n    """"""Returns a unique identifier for a video id & timestamp.""""""\n    return ""%s,%04d"" % (video_id, int(timestamp))\n\n\ndef read_csv(csv_file, class_whitelist=None, capacity=0):\n    """"""Loads boxes and class labels from a CSV file in the AVA format.\n\n    CSV file format described at https://research.google.com/ava/download.html.\n\n    Args:\n        csv_file: A file object.\n        class_whitelist: If provided, boxes corresponding to (integer) class\n        labels not in this set are skipped.\n        capacity: Maximum number of labeled boxes allowed for each example.\n        Default is 0 where there is no limit.\n\n    Returns:\n        boxes: A dictionary mapping each unique image key (string) to a list of\n        boxes, given as coordinates [y1, x1, y2, x2].\n        labels: A dictionary mapping each unique image key (string) to a list\n        of integer class lables, matching the corresponding box in `boxes`.\n        scores: A dictionary mapping each unique image key (string) to a list\n        of score values lables, matching the corresponding label in `labels`.\n        If scores are not provided in the csv, then they will default to 1.0.\n    """"""\n    start = time.time()\n    entries = defaultdict(list)\n    boxes = defaultdict(list)\n    labels = defaultdict(list)\n    scores = defaultdict(list)\n    reader = csv.reader(csv_file)\n    for row in reader:\n        assert len(row) in [7, 8], ""Wrong number of columns: "" + row\n        image_key = make_image_key(row[0], row[1])\n        x1, y1, x2, y2 = [float(n) for n in row[2:6]]\n        action_id = int(row[6])\n        if class_whitelist and action_id not in class_whitelist:\n            continue\n            score = 1.0\n        if len(row) == 8:\n            score = float(row[7])\n        if capacity < 1 or len(entries[image_key]) < capacity:\n            heapq.heappush(entries[image_key],\n                           (score, action_id, y1, x1, y2, x2))\n        elif score > entries[image_key][0][0]:\n            heapq.heapreplace(entries[image_key],\n                              (score, action_id, y1, x1, y2, x2))\n    for image_key in entries:\n        # Evaluation API assumes boxes with descending scores\n        entry = sorted(entries[image_key], key=lambda tup: -tup[0])\n        for item in entry:\n            score, action_id, y1, x1, y2, x2 = item\n            boxes[image_key].append([y1, x1, y2, x2])\n            labels[image_key].append(action_id)\n            scores[image_key].append(score)\n    print_time(""read file "" + csv_file.name, start)\n    return boxes, labels, scores\n\n\ndef read_exclusions(exclusions_file):\n    """"""Reads a CSV file of excluded timestamps.\n\n    Args:\n        exclusions_file: A file object containing a csv of video-id,timestamp.\n\n    Returns:\n        A set of strings containing excluded image keys, e.g.\n        ""aaaaaaaaaaa,0904"",\n        or an empty set if exclusions file is None.\n    """"""\n    excluded = set()\n    if exclusions_file:\n        reader = csv.reader(exclusions_file)\n    for row in reader:\n        assert len(row) == 2, ""Expected only 2 columns, got: "" + row\n        excluded.add(make_image_key(row[0], row[1]))\n    return excluded\n\n\ndef read_labelmap(labelmap_file):\n    """"""Reads a labelmap without the dependency on protocol buffers.\n\n    Args:\n        labelmap_file: A file object containing a label map protocol buffer.\n\n    Returns:\n        labelmap: The label map in the form used by the\n        object_detection_evaluation\n        module - a list of {""id"": integer, ""name"": classname } dicts.\n        class_ids: A set containing all of the valid class id integers.\n    """"""\n    labelmap = []\n    class_ids = set()\n    name = """"\n    class_id = """"\n    for line in labelmap_file:\n        if line.startswith(""  name:""):\n            name = line.split(\'""\')[1]\n        elif line.startswith(""  id:"") or line.startswith(""  label_id:""):\n            class_id = int(line.strip().split("" "")[-1])\n            labelmap.append({""id"": class_id, ""name"": name})\n            class_ids.add(class_id)\n    return labelmap, class_ids\n\n\ndef ava_eval(result_file, result_type,\n             label_file, ann_file,\n             exclude_file, max_dets=(100, 300, 1000),\n             verbose=True):\n    assert result_type in [\'proposal\', \'bbox\']\n\n    start = time.time()\n    categories, class_whitelist = read_labelmap(open(label_file))\n    gt_boxes, gt_labels, _ = read_csv(open(ann_file), class_whitelist, 0)\n    if verbose:\n        print_time(""Reading detection results"", start)\n\n    if exclude_file is not None:\n        excluded_keys = read_exclusions(open(exclude_file))\n    else:\n        excluded_keys = list()\n\n    start = time.time()\n    boxes, labels, scores = read_csv(open(result_file), class_whitelist, 0)\n    if verbose:\n        print_time(""Reading detection results"", start)\n\n    if result_type == \'proposal\':\n        gts = [np.array(gt_boxes[image_key], dtype=float)\n               for image_key in gt_boxes]\n        proposals = []\n        for image_key in gt_boxes:\n            if image_key in boxes:\n                proposals.append(\n                    np.concatenate(\n                        (np.array(boxes[image_key], dtype=float),\n                         np.array(scores[image_key], dtype=float)[:, None]),\n                        axis=1))\n            else:\n                proposals.append(np.zeros((1, 5)))\n\n        recalls = eval_recalls(gts, proposals, np.array(\n            max_dets), np.arange(0.5, 0.96, 0.05), print_summary=False)\n        ar = recalls.mean(axis=1)\n        for i, num in enumerate(max_dets):\n            print(\'Recall@0.5@{}\\t={:.4f}\'.format(num, recalls[i, 0]))\n            print(\'AR@{}\\t={:.4f}\'.format(num, ar[i]))\n\n    if result_type == \'bbox\':\n        pascal_evaluator = det_eval.PascalDetectionEvaluator(\n            categories)\n\n        start = time.time()\n        for image_key in gt_boxes:\n            if verbose and image_key in excluded_keys:\n                logging.info(""Found excluded timestamp in detections: %s.""\n                             ""It will be ignored."", image_key)\n                continue\n            pascal_evaluator.add_single_ground_truth_image_info(\n                image_key, {\n                    standard_fields.InputDataFields.groundtruth_boxes:\n                        np.array(gt_boxes[image_key], dtype=float),\n                    standard_fields.InputDataFields.groundtruth_classes:\n                        np.array(gt_labels[image_key], dtype=int),\n                    standard_fields.InputDataFields.groundtruth_difficult:\n                        np.zeros(len(gt_boxes[image_key]), dtype=bool)\n                })\n        if verbose:\n            print_time(""Convert groundtruth"", start)\n\n        start = time.time()\n        for image_key in boxes:\n            if verbose and image_key in excluded_keys:\n                logging.info(""Found excluded timestamp in detections: %s.""\n                             ""It will be ignored."", image_key)\n                continue\n            pascal_evaluator.add_single_detected_image_info(\n                image_key, {\n                    standard_fields.DetectionResultFields.detection_boxes:\n                        np.array(boxes[image_key], dtype=float),\n                    standard_fields.DetectionResultFields.detection_classes:\n                        np.array(labels[image_key], dtype=int),\n                    standard_fields.DetectionResultFields.detection_scores:\n                        np.array(scores[image_key], dtype=float)\n                })\n        if verbose:\n            print_time(""convert detections"", start)\n\n        start = time.time()\n        metrics = pascal_evaluator.evaluate()\n        if verbose:\n            print_time(""run_evaluator"", start)\n        for display_name in metrics:\n            print(\'{}=\\t{}\'.format(display_name, metrics[display_name]))\n'"
mmaction/core/evaluation/bbox_overlaps.py,0,"b'import numpy as np\n\n\ndef bbox_overlaps(bboxes1, bboxes2, mode=\'iou\'):\n    """"""Calculate the ious between each bbox of bboxes1 and bboxes2.\n    Args:\n        bboxes1(ndarray): shape (n, 4)\n        bboxes2(ndarray): shape (k, 4)\n        mode(str): iou (intersection over union) or iof (intersection\n            over foreground)\n    Returns:\n        ious(ndarray): shape (n, k)\n    """"""\n\n    assert mode in [\'iou\', \'iof\']\n\n    bboxes1 = bboxes1.astype(np.float32)\n    bboxes2 = bboxes2.astype(np.float32)\n    rows = bboxes1.shape[0]\n    cols = bboxes2.shape[0]\n    ious = np.zeros((rows, cols), dtype=np.float32)\n    if rows * cols == 0:\n        return ious\n    exchange = False\n    if bboxes1.shape[0] > bboxes2.shape[0]:\n        bboxes1, bboxes2 = bboxes2, bboxes1\n        ious = np.zeros((cols, rows), dtype=np.float32)\n        exchange = True\n    area1 = (bboxes1[:, 2] - bboxes1[:, 0] + 1) * (\n        bboxes1[:, 3] - bboxes1[:, 1] + 1)\n    area2 = (bboxes2[:, 2] - bboxes2[:, 0] + 1) * (\n        bboxes2[:, 3] - bboxes2[:, 1] + 1)\n    for i in range(bboxes1.shape[0]):\n        x_start = np.maximum(bboxes1[i, 0], bboxes2[:, 0])\n        y_start = np.maximum(bboxes1[i, 1], bboxes2[:, 1])\n        x_end = np.minimum(bboxes1[i, 2], bboxes2[:, 2])\n        y_end = np.minimum(bboxes1[i, 3], bboxes2[:, 3])\n        overlap = np.maximum(x_end - x_start + 1, 0) * np.maximum(\n            y_end - y_start + 1, 0)\n        if mode == \'iou\':\n            union = area1[i] + area2 - overlap\n        else:\n            union = area1[i] if not exchange else area2\n        ious[i, :] = overlap / union\n    if exchange:\n        ious = ious.T\n    return ious\n'"
mmaction/core/evaluation/class_names.py,0,"b'import mmcv\n\n\ndef ava_classes():\n    return [\n        \'bend/bow (at the waist)\', \'crawl\', \'crouch/kneel\', \'dance\',\n        \'fall down\', \'get up\', \'jump/leap\', \'lie/sleep\', \'martial art\',\n        \'run/jog\', \'sit\', \'stand\', \'swim\', \'walk\', \'answer phone\',\n        \'brush teeth\', \'carry/hold (an object)\', \'catch (an object)\', \'chop\',\n        \'climb (e.g., a mountain)\',\n        \'clink glass\', \'close (e.g., a door, a box)\', \'cook\', \'cut\', \'dig\',\n        \'dress/put on clothing\', \'drink\', \'driving (e.g., a car, a truck)\',\n        \'eat\', \'enter\', \'exit\', \'extract\', \'fishing\', \'hit (an object)\',\n        \'kick (an object)\', \'lift/pick up\', \'listen (e.g., to music)\',\n        \'open (e.g., a window, a car door)\', \'paint\', \'play board game\',\n        \'play musical instrument\', \'play with pets\', \'point to (an object)\',\n        \'press\', \'pull (an object)\', \'push (an object)\', \'put down\', \'read\',\n        \'ride (e.g., a bike, a car, a horse)\', \'row boat\', \'sail boat\',\n        \'shoot\', \'shovel\', \'smoke\', \'stir\', \'take a photo\',\n        \'text on/look at a cellphone\', \'throw\', \'touch (an object)\',\n        \' (e.g., a screwdriver)\', \'watch (e.g., TV)\', \'work on a computer\',\n        \'write\', \'fight/hit (a person)\',\n        \'give/serve (an object) to (a person)\',\n        \'grab (a person)\', \'hand clap\', \'hand shake\', \'hand wave\',\n        \'hug (a person)\',\n        \'kick (a person)\', \'kiss (a person)\', \'lift (a person)\',\n        \'listen to (a person)\', \'play with kids\', \'push (another person)\',\n        \'sing to (e.g., self, a person, a group)\',\n        \'take (an object) from (a person)\',\n        \'talk to (e.g., self, a person, a group)\', \'watch (a person)\'\n    ]\n\n\ndataset_aliases = {\n    \'ava\': [\'ava\', \'ava2.1\', \'ava2.2\'],\n}\n\n\ndef get_classes(dataset):\n    """"""Get class names of a dataset.""""""\n    alias2name = {}\n    for name, aliases in dataset_aliases.items():\n        for alias in aliases:\n            alias2name[alias] = name\n\n    if mmcv.is_str(dataset):\n        if dataset in alias2name:\n            labels = eval(alias2name[dataset] + \'_classes()\')\n        else:\n            raise ValueError(\'Unrecognized dataset: {}\'.format(dataset))\n    else:\n        raise TypeError(\'dataset must a str, but got {}\'.format(type(dataset)))\n    return labels\n'"
mmaction/core/evaluation/eval_hooks.py,4,"b'import os\nimport os.path as osp\nimport logging\nimport mmcv\nimport time\nimport torch\nimport numpy as np\nimport torch.distributed as dist\nfrom mmcv.runner import Hook, obj_from_dict\nfrom mmcv.parallel import scatter, collate\nfrom torch.utils.data import Dataset\n\nfrom mmaction import datasets\nfrom .accuracy import top_k_accuracy\nfrom .ava_utils import (results2csv, read_csv, read_labelmap,\n                        read_exclusions)\n\ntry:\n    import sys\n    sys.path.append(\n        osp.abspath(osp.join(__file__, \'../../../\',\n                             \'third_party/ActivityNet/Evaluation/ava\')))\n    from mmaction.third_party.ActivityNet.Evaluation.ava import (\n        object_detection_evaluation as det_eval)\n    import standard_fields\nexcept ImportError:\n    print(\'Failed to import ActivityNet evaluation toolbox. Did you clone with\'\n          \'""--recursive""?\')\n\n\nclass DistEvalHook(Hook):\n    def __init__(self, dataset, interval=1):\n        if isinstance(dataset, Dataset):\n            self.dataset = dataset\n        elif isinstance(dataset, dict):\n            self.dataset = obj_from_dict(dataset, datasets,\n                                         {\'test_mode\': True})\n        else:\n            raise TypeError(\n                \'dataset must be a Dataset object or a dict, not {}\'.format(\n                    type(dataset)))\n        self.interval = interval\n\n    def after_train_epoch(self, runner):\n        if not self.every_n_epochs(runner, self.interval):\n            return\n        runner.model.eval()\n        results = [None for _ in range(len(self.dataset))]\n        if runner.rank == 0:\n            prog_bar = mmcv.ProgressBar(len(self.dataset))\n        for idx in range(runner.rank, len(self.dataset), runner.world_size):\n            data = self.dataset[idx]\n            data_gpu = scatter(\n                collate([data], samples_per_gpu=1),\n                [torch.cuda.current_device()])[0]\n\n            # compute output\n            with torch.no_grad():\n                result = runner.model(\n                    return_loss=False, rescale=True, **data_gpu)\n            results[idx] = result\n\n            batch_size = runner.world_size\n            if runner.rank == 0:\n                for _ in range(batch_size):\n                    prog_bar.update()\n\n        if runner.rank == 0:\n            print(\'\\n\')\n            dist.barrier()\n            for i in range(1, runner.world_size):\n                tmp_file = osp.join(runner.work_dir, \'temp_{}.pkl\'.format(i))\n                tmp_results = mmcv.load(tmp_file)\n                for idx in range(i, len(results), runner.world_size):\n                    results[idx] = tmp_results[idx]\n                os.remove(tmp_file)\n            self.evaluate(runner, results)\n        else:\n            tmp_file = osp.join(runner.work_dir,\n                                \'temp_{}.pkl\'.format(runner.rank))\n            mmcv.dump(results, tmp_file)\n            dist.barrier()\n        dist.barrier()\n\n    def evaluate(self):\n        raise NotImplementedError\n\n\nclass DistEvalTopKAccuracyHook(DistEvalHook):\n\n    def __init__(self,\n                 dataset,\n                 k=(1,)):\n        super(DistEvalTopKAccuracyHook, self).__init__(dataset)\n        self.k = k\n\n    def evaluate(self, runner, results):\n        gt_labels = []\n        for i in range(len(self.dataset)):\n            ann = self.dataset.get_ann_info(i)\n            gt_labels.append(ann[\'label\'])\n\n        results = [res.squeeze() for res in results]\n        top1, top5 = top_k_accuracy(results, gt_labels, k=self.k)\n        runner.mode = \'val\'\n        runner.log_buffer.output[\'top1 acc\'] = top1\n        runner.log_buffer.output[\'top5 acc\'] = top5\n        runner.log_buffer.ready = True\n\n\nclass AVADistEvalmAPHook(DistEvalHook):\n\n    def __init__(self, dataset):\n        super(AVADistEvalmAPHook, self).__init__(dataset)\n\n    def evaluate(self, runner, results, verbose=False):\n\n        categories, class_whitelist = read_labelmap(\n            open(self.dataset.label_file))\n        if verbose:\n            logging.info(""CATEGORIES ({}):\\n"".format(len(categories)))\n\n        excluded_keys = read_exclusions(open(self.dataset.exclude_file))\n        pascal_evaluator = det_eval.PascalDetectionEvaluator(\n            categories)\n\n        def print_time(message, start):\n            logging.info(""==> %g seconds to %s"", time.time() - start, message)\n\n        # Reads the ground truth data.\n        boxes, labels, _ = read_csv(\n            open(self.dataset.ann_file), class_whitelist, 0)\n        start = time.time()\n        for image_key in boxes:\n            if verbose and image_key in excluded_keys:\n                logging.info(""Found excluded timestamp in detections: %s.""\n                             ""It will be ignored."", image_key)\n                continue\n            pascal_evaluator.add_single_ground_truth_image_info(\n                image_key, {\n                    standard_fields.InputDataFields.groundtruth_boxes:\n                        np.array(boxes[image_key], dtype=float),\n                    standard_fields.InputDataFields.groundtruth_classes:\n                        np.array(labels[image_key], dtype=int),\n                    standard_fields.InputDataFields.groundtruth_difficult:\n                        np.zeros(len(boxes[image_key]), dtype=bool)\n                })\n        if verbose:\n            print_time(""Convert groundtruth"", start)\n\n        # Read detections datas.\n        tmp_file = osp.join(runner.work_dir, \'temp_0.csv\')\n        results2csv(self.dataset, results, tmp_file)\n\n        boxes, labels, scores = read_csv(open(tmp_file), class_whitelist, 50)\n        start = time.time()\n        for image_key in boxes:\n            if verbose and image_key in excluded_keys:\n                logging.info(""Found excluded timestamp in detections: %s.""\n                             ""It will be ignored."", image_key)\n                continue\n            pascal_evaluator.add_single_detected_image_info(\n                image_key, {\n                    standard_fields.DetectionResultFields.detection_boxes:\n                        np.array(boxes[image_key], dtype=float),\n                    standard_fields.DetectionResultFields.detection_classes:\n                        np.array(labels[image_key], dtype=int),\n                    standard_fields.DetectionResultFields.detection_scores:\n                        np.array(scores[image_key], dtype=float)\n                })\n        if verbose:\n            print_time(""convert detections"", start)\n\n        start = time.time()\n        metrics = pascal_evaluator.evaluate()\n        if verbose:\n            print_time(""run_evaluator"", start)\n        for display_name in metrics:\n            runner.log_buffer.output[display_name] = metrics[display_name]\n        runner.log_buffer.ready = True\n'"
mmaction/core/evaluation/localize_utils.py,0,"b'import numpy as np\nfrom .accuracy import softmax\nimport pandas as pd\nfrom multiprocessing import Pool\nimport mmcv\n\ntry:\n    import sys\n    import os.path as osp\n    sys.path.append(\n        osp.abspath(osp.join(__file__, \'../../../\',\n                             \'third_party/ActivityNet/Evaluation\')))\n    from mmaction.third_party.ActivityNet.Evaluation.eval_detection import (\n        compute_average_precision_detection)\nexcept ImportError:\n    print(\'Failed to import ActivityNet evaluation toolbox. Did you clone with\'\n          \'""--recursive""?\')\n\n\ndef results2det(dataset, outputs,\n                top_k=2000, nms=0.2,\n                softmax_before_filter=True,\n                cls_score_dict=None,\n                cls_top_k=2):\n    num_class = outputs[0][1].shape[1] - 1\n    detections = [dict() for i in range(num_class)]\n\n    for idx in range(len(dataset)):\n        video_id = dataset.video_infos[idx].video_id\n        rel_prop = outputs[idx][0]\n        if len(rel_prop[0].shape) == 3:\n            rel_prop = np.squeeze(rel_prop, 0)\n\n        act_scores = outputs[idx][1]\n        comp_scores = outputs[idx][2]\n        reg_scores = outputs[idx][3]\n        if reg_scores is None:\n            reg_scores = np.zeros(\n                len(rel_prop), num_class, 2, dtype=np.float32)\n        reg_scores = reg_scores.reshape((-1, num_class, 2))\n\n        if top_k <= 0 and cls_score_dict is None:\n            combined_scores = softmax(act_scores[:, 1:]) * np.exp(comp_scores)\n            for i in range(num_class):\n                loc_scores = reg_scores[:, i, 0][:, None]\n                dur_scores = reg_scores[:, i, 1][:, None]\n                detections[i][video_id] = np.concatenate((\n                    rel_prop, combined_scores[:, i][:, None],\n                    loc_scores, dur_scores), axis=1)\n        elif cls_score_dict is None:\n            combined_scores = softmax(act_scores[:, 1:]) * np.exp(comp_scores)\n            keep_idx = np.argsort(combined_scores.ravel())[-top_k:]\n            for k in keep_idx:\n                cls = k % num_class\n                prop_idx = k // num_class\n                new_item = [rel_prop[prop_idx, 0], rel_prop[prop_idx, 1],\n                            combined_scores[prop_idx, cls],\n                            reg_scores[prop_idx, cls, 0],\n                            reg_scores[prop_idx, cls, 1]]\n                if video_id not in detections[cls]:\n                    detections[cls][video_id] = np.array([new_item])\n                else:\n                    detections[cls][video_id] = np.vstack(\n                        [detections[cls][video_id], new_item])\n        else:\n            cls_score_dict = mmcv.load(cls_score_dict)\n            if softmax_before_filter:\n                combined_scores = softmax(\n                    act_scores[:, 1:]) * np.exp(comp_scores)\n            else:\n                combined_scores = act_scores[:, 1:] * np.exp(comp_scores)\n            video_cls_score = cls_score_dict[video_id]\n\n            for video_cls in np.argsort(video_cls_score,)[-cls_top_k:]:\n                loc_scores = reg_scores[:, video_cls, 0][:, None]\n                dur_scores = reg_scores[:, video_cls, 1][:, None]\n                detections[video_cls][video_id] = np.concatenate((\n                    rel_prop, combined_scores[:, video_cls][:, None],\n                    loc_scores, dur_scores), axis=1)\n\n    return detections\n\n\ndef perform_regression(detections):\n    t0 = detections[:, 0]\n    t1 = detections[:, 1]\n    center = (t0 + t1) / 2\n    duration = t1 - t0\n\n    new_center = center + duration * detections[:, 3]\n    new_duration = duration * np.exp(detections[:, 4])\n\n    new_detections = np.concatenate((\n        np.clip(new_center - new_duration / 2, 0, 1)[:, None],\n        np.clip(new_center + new_duration / 2, 0, 1)[:, None],\n        detections[:, 2:]), axis=1)\n    return new_detections\n\n\ndef temporal_nms(detections, thresh):\n    t0 = detections[:, 0]\n    t1 = detections[:, 1]\n    scores = detections[:, 2]\n\n    durations = t1 - t0\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n        tt0 = np.maximum(t0[i], t0[order[1:]])\n        tt1 = np.minimum(t1[i], t1[order[1:]])\n        intersection = tt1 - tt0\n        iou = intersection / \\\n            (durations[i] + durations[order[1:]] - intersection).astype(float)\n\n        inds = np.where(iou <= thresh)[0]\n        order = order[inds + 1]\n\n    return detections[keep, :]\n\n\ndef det2df(detections, cls):\n    detection_list = []\n    for vid, dets in detections[cls].items():\n        detection_list.extend([[vid, cls] + x[:3] for x in dets.tolist()])\n    df = pd.DataFrame(detection_list, columns=[\n                      \'video-id\', \'cls\', \'t-start\', \'t-end\', \'score\'])\n    return df\n\n\ndef eval_ap(iou, iou_idx, cls, gt, prediction):\n    ap = compute_average_precision_detection(gt, prediction, iou)\n    sys.stdout.flush()\n    return cls, iou_idx, ap\n\n\ndef eval_ap_parallel(detections, gt_by_cls, iou_range, worker=32):\n    ap_values = np.zeros((len(detections), len(iou_range)))\n\n    def callback(rst):\n        sys.stdout.flush()\n        ap_values[rst[0], rst[1]] = rst[2][0]\n\n    pool = Pool(worker)\n    jobs = []\n    for iou_idx, min_overlap in enumerate(iou_range):\n        for cls in range(len(detections)):\n            jobs.append(pool.apply_async(eval_ap, args=([min_overlap], iou_idx,\n                                                        cls, gt_by_cls[cls],\n                                                        detections[cls],),\n                                         callback=callback))\n    pool.close()\n    pool.join()\n    return ap_values\n'"
mmaction/core/evaluation/recall.py,0,"b'import numpy as np\nfrom terminaltables import AsciiTable\n\nfrom .bbox_overlaps import bbox_overlaps\n\n\ndef _recalls(all_ious, proposal_nums, thrs):\n\n    img_num = all_ious.shape[0]\n    total_gt_num = sum([ious.shape[0] for ious in all_ious])\n\n    _ious = np.zeros((proposal_nums.size, total_gt_num), dtype=np.float32)\n    for k, proposal_num in enumerate(proposal_nums):\n        tmp_ious = np.zeros(0)\n        for i in range(img_num):\n            ious = all_ious[i][:, :proposal_num].copy()\n            gt_ious = np.zeros((ious.shape[0]))\n            if ious.size == 0:\n                tmp_ious = np.hstack((tmp_ious, gt_ious))\n                continue\n            for j in range(ious.shape[0]):\n                gt_max_overlaps = ious.argmax(axis=1)\n                max_ious = ious[np.arange(0, ious.shape[0]), gt_max_overlaps]\n                gt_idx = max_ious.argmax()\n                gt_ious[j] = max_ious[gt_idx]\n                box_idx = gt_max_overlaps[gt_idx]\n                ious[gt_idx, :] = -1\n                ious[:, box_idx] = -1\n            tmp_ious = np.hstack((tmp_ious, gt_ious))\n        _ious[k, :] = tmp_ious\n\n    _ious = np.fliplr(np.sort(_ious, axis=1))\n    recalls = np.zeros((proposal_nums.size, thrs.size))\n    for i, thr in enumerate(thrs):\n        recalls[:, i] = (_ious >= thr).sum(axis=1) / float(total_gt_num)\n\n    return recalls\n\n\ndef set_recall_param(proposal_nums, iou_thrs):\n    """"""Check proposal_nums and iou_thrs and set correct format.\n    """"""\n    if isinstance(proposal_nums, list):\n        _proposal_nums = np.array(proposal_nums)\n    elif isinstance(proposal_nums, int):\n        _proposal_nums = np.array([proposal_nums])\n    else:\n        _proposal_nums = proposal_nums\n\n    if iou_thrs is None:\n        _iou_thrs = np.array([0.5])\n    elif isinstance(iou_thrs, list):\n        _iou_thrs = np.array(iou_thrs)\n    elif isinstance(iou_thrs, float):\n        _iou_thrs = np.array([iou_thrs])\n    else:\n        _iou_thrs = iou_thrs\n\n    return _proposal_nums, _iou_thrs\n\n\ndef eval_recalls(gts,\n                 proposals,\n                 proposal_nums=None,\n                 iou_thrs=None,\n                 print_summary=True):\n    """"""Calculate recalls.\n    Args:\n        gts(list or ndarray): a list of arrays of shape (n, 4)\n        proposals(list or ndarray): a list of arrays of shape (k, 4) or (k, 5)\n        proposal_nums(int or list of int or ndarray): top N proposals\n        thrs(float or list or ndarray): iou thresholds\n    Returns:\n        ndarray: recalls of different ious and proposal nums\n    """"""\n\n    img_num = len(gts)\n    assert img_num == len(proposals)\n\n    proposal_nums, iou_thrs = set_recall_param(proposal_nums, iou_thrs)\n\n    all_ious = []\n    for i in range(img_num):\n        if proposals[i].ndim == 2 and proposals[i].shape[1] == 5:\n            scores = proposals[i][:, 4]\n            sort_idx = np.argsort(scores)[::-1]\n            img_proposal = proposals[i][sort_idx, :]\n        else:\n            img_proposal = proposals[i]\n        prop_num = min(img_proposal.shape[0], proposal_nums[-1])\n        if gts[i] is None or gts[i].shape[0] == 0:\n            ious = np.zeros((0, img_proposal.shape[0]), dtype=np.float32)\n        else:\n            ious = bbox_overlaps(gts[i], img_proposal[:prop_num, :4])\n        all_ious.append(ious)\n    all_ious = np.array(all_ious)\n    recalls = _recalls(all_ious, proposal_nums, iou_thrs)\n    if print_summary:\n        print_recall_summary(recalls, proposal_nums, iou_thrs)\n    return recalls\n\n\ndef print_recall_summary(recalls,\n                         proposal_nums,\n                         iou_thrs,\n                         row_idxs=None,\n                         col_idxs=None):\n    """"""Print recalls in a table.\n    Args:\n        recalls(ndarray): calculated from `bbox_recalls`\n        proposal_nums(ndarray or list): top N proposals\n        iou_thrs(ndarray or list): iou thresholds\n        row_idxs(ndarray): which rows(proposal nums) to print\n        col_idxs(ndarray): which cols(iou thresholds) to print\n    """"""\n    proposal_nums = np.array(proposal_nums, dtype=np.int32)\n    iou_thrs = np.array(iou_thrs)\n    if row_idxs is None:\n        row_idxs = np.arange(proposal_nums.size)\n    if col_idxs is None:\n        col_idxs = np.arange(iou_thrs.size)\n    row_header = [\'\'] + iou_thrs[col_idxs].tolist()\n    table_data = [row_header]\n    for i, num in enumerate(proposal_nums[row_idxs]):\n        row = [\n            \'{:.3f}\'.format(val)\n            for val in recalls[row_idxs[i], col_idxs].tolist()\n        ]\n        row.insert(0, num)\n        table_data.append(row)\n    table = AsciiTable(table_data)\n    print(table.table)\n\n\ndef plot_num_recall(recalls, proposal_nums):\n    """"""Plot Proposal_num-Recalls curve.\n    Args:\n        recalls(ndarray or list): shape (k,)\n        proposal_nums(ndarray or list): same shape as `recalls`\n    """"""\n    if isinstance(proposal_nums, np.ndarray):\n        _proposal_nums = proposal_nums.tolist()\n    else:\n        _proposal_nums = proposal_nums\n    if isinstance(recalls, np.ndarray):\n        _recalls = recalls.tolist()\n    else:\n        _recalls = recalls\n\n    import matplotlib.pyplot as plt\n    f = plt.figure()\n    plt.plot([0] + _proposal_nums, [0] + _recalls)\n    plt.xlabel(\'Proposal num\')\n    plt.ylabel(\'Recall\')\n    plt.axis([0, proposal_nums.max(), 0, 1])\n    f.show()\n\n\ndef plot_iou_recall(recalls, iou_thrs):\n    """"""Plot IoU-Recalls curve.\n    Args:\n        recalls(ndarray or list): shape (k,)\n        iou_thrs(ndarray or list): same shape as `recalls`\n    """"""\n    if isinstance(iou_thrs, np.ndarray):\n        _iou_thrs = iou_thrs.tolist()\n    else:\n        _iou_thrs = iou_thrs\n    if isinstance(recalls, np.ndarray):\n        _recalls = recalls.tolist()\n    else:\n        _recalls = recalls\n\n    import matplotlib.pyplot as plt\n    f = plt.figure()\n    plt.plot(_iou_thrs + [1.0], _recalls + [0.])\n    plt.xlabel(\'IoU\')\n    plt.ylabel(\'Recall\')\n    plt.axis([iou_thrs.min(), 1, 0, 1])\n    f.show()\n'"
mmaction/core/post_processing/__init__.py,0,"b""from .bbox_nms import multiclass_nms, singleclass_nms\nfrom .merge_augs import (merge_aug_proposals, merge_aug_bboxes,\n                         merge_aug_scores)\n\n__all__ = [\n    'multiclass_nms', 'singleclass_nms', 'merge_aug_proposals', 'merge_aug_bboxes',\n    'merge_aug_scores'\n]\n"""
mmaction/core/post_processing/bbox_nms.py,8,"b'import torch\n\nfrom mmaction.ops.nms import nms_wrapper\n\n\ndef multiclass_nms(multi_bboxes, multi_scores, score_thr, nms_cfg, max_num=-1):\n    """"""NMS for multi-class bboxes.\n\n    Args:\n        multi_bboxes (Tensor): shape (n, #class*4) or (n, 4)\n        multi_scores (Tensor): shape (n, #class)\n        score_thr (float): bbox threshold, bboxes with scores lower than it\n            will not be considered.\n        nms_thr (float): NMS IoU threshold\n        max_num (int): if there are more than max_num bboxes after NMS,\n            only top max_num will be kept.\n\n    Returns:\n        tuple: (bboxes, labels), tensors of shape (k, 5) and (k, 1). labels\n            are 0-based.\n    """"""\n    num_classes = multi_scores.shape[1]\n    bboxes, labels = [], []\n    nms_cfg_ = nms_cfg.copy()\n    nms_type = nms_cfg_.pop(\'type\', \'nms\')\n    nms_op = getattr(nms_wrapper, nms_type)\n    for i in range(1, num_classes):\n        cls_inds = multi_scores[:, i] > score_thr\n        if not cls_inds.any():\n            continue\n        # get bboxes and scores of this class\n        if multi_bboxes.shape[1] == 4:\n            _bboxes = multi_bboxes[cls_inds, :]\n        else:\n            _bboxes = multi_bboxes[cls_inds, i * 4:(i + 1) * 4]\n        _scores = multi_scores[cls_inds, i]\n        cls_dets = torch.cat([_bboxes, _scores[:, None]], dim=1)\n        cls_dets, _ = nms_op(cls_dets, **nms_cfg_)\n        cls_labels = multi_bboxes.new_full(\n            (cls_dets.shape[0], ), i - 1, dtype=torch.long)\n        bboxes.append(cls_dets)\n        labels.append(cls_labels)\n    if bboxes:\n        bboxes = torch.cat(bboxes)\n        labels = torch.cat(labels)\n        if bboxes.shape[0] > max_num:\n            _, inds = bboxes[:, -1].sort(descending=True)\n            inds = inds[:max_num]\n            bboxes = bboxes[inds]\n            labels = labels[inds]\n    else:\n        bboxes = multi_bboxes.new_zeros((0, 5))\n        labels = multi_bboxes.new_zeros((0, ), dtype=torch.long)\n\n    return bboxes, labels\n\n\ndef singleclass_nms(multi_bboxes, multi_scores, score_thr, nms_cfg,\n                    max_num=-1):\n    """"""NMS for single-class bboxes.\n\n    Args:\n        multi_bboxes (Tensor): shape (n, 4)\n        multi_scores (Tensor): shape (n, #class)\n        score_thr (float): bbox threshold, bboxes with scores lower than it\n            will not be considered.\n        nms_thr (float): NMS IoU threshold\n        max_num (int): if there are more than max_num bboxes after NMS,\n            only top max_num will be kept.\n\n    Returns:\n        tuple: (bboxes, scores), tensors of shape (k, 5) and (k, #class).\n            labels are 0-based.\n    """"""\n    bboxes, scores = [], []\n    nms_cfg_ = nms_cfg.copy()\n    nms_type = nms_cfg_.pop(\'type\', \'nms\')\n    nms_op = getattr(nms_wrapper, nms_type)\n\n    cls_inds = multi_scores[:, 0] > score_thr\n    if not cls_inds.any():\n        bboxes = multi_bboxes.new_zeros((0, 5))\n        scores = multi_bboxes.new_zeros((0, multi_scores.size(1)))\n        return bboxes, scores\n    # get bboxes and scores of this class\n    _bboxes = multi_bboxes[cls_inds, :]\n    _scores = multi_scores[cls_inds, :]\n    cls_dets = torch.cat([_bboxes, _scores[:, 0:1]], dim=1)\n    cls_dets, nms_keep = nms_op(cls_dets, **nms_cfg_)\n    cls_scores = _scores[nms_keep, :]\n    bboxes.append(cls_dets)\n    scores.append(cls_scores)\n\n    if bboxes:\n        bboxes = torch.cat(bboxes)\n        scores = torch.cat(scores)\n        if bboxes.shape[0] > max_num:\n            _, inds = bboxes[:, -1].sort(descending=True)\n            inds = inds[:max_num]\n            bboxes = bboxes[inds]\n            scores = scores[inds]\n    else:\n        bboxes = multi_bboxes.new_zeros((0, 5))\n        scores = multi_bboxes.new_zeros((0, multi_scores.size(1)))\n\n    return bboxes, scores\n'"
mmaction/core/post_processing/merge_augs.py,5,"b'import torch\n\nimport numpy as np\n\nfrom mmaction.ops import nms\nfrom ..bbox2d import bbox_mapping_back\n\n\ndef merge_aug_proposals(aug_proposals, img_metas, rpn_test_cfg):\n    """"""Merge augmented proposals (multiscale, flip, etc.)\n\n    Args:\n        aug_proposals (list[Tensor]): proposals from different testing\n            schemes, shape (n, 5). Note that they are not rescaled to the\n            original image size.\n        img_metas (list[dict]): image info including ""shape_scale"" and ""flip"".\n        rpn_test_cfg (dict): rpn test config.\n\n    Returns:\n        Tensor: shape (n, 4), proposals corresponding to original image scale.\n    """"""\n    recovered_proposals = []\n    for proposals, img_info in zip(aug_proposals, img_metas):\n        img_shape = img_info[\'img_shape\']\n        scale_factor = img_info[\'scale_factor\']\n        flip = img_info[\'flip\']\n        _proposals = proposals.clone()\n        _proposals[:, :4] = bbox_mapping_back(_proposals[:, :4], img_shape,\n                                              scale_factor, flip)\n        recovered_proposals.append(_proposals)\n    aug_proposals = torch.cat(recovered_proposals, dim=0)\n    merged_proposals, _ = nms(aug_proposals, rpn_test_cfg.nms_thr)\n    scores = merged_proposals[:, 4]\n    _, order = scores.sort(0, descending=True)\n    num = min(rpn_test_cfg.max_num, merged_proposals.shape[0])\n    order = order[:num]\n    merged_proposals = merged_proposals[order, :]\n    return merged_proposals\n\n\ndef merge_aug_bboxes(aug_bboxes, aug_scores, img_metas, rcnn_test_cfg):\n    """"""Merge augmented detection bboxes and scores.\n\n    Args:\n        aug_bboxes (list[Tensor]): shape (n, 4*#class)\n        aug_scores (list[Tensor] or None): shape (n, #class)\n        img_shapes (list[Tensor]): shape (3, ).\n        rcnn_test_cfg (dict): rcnn test config.\n\n    Returns:\n        tuple: (bboxes, scores)\n    """"""\n    recovered_bboxes = []\n    for bboxes, img_info in zip(aug_bboxes, img_metas):\n        img_shape = img_info[0][\'img_shape\']\n        scale_factor = img_info[0][\'scale_factor\']\n        flip = img_info[0][\'flip\']\n        bboxes = bbox_mapping_back(bboxes, img_shape, scale_factor, flip)\n        recovered_bboxes.append(bboxes)\n    bboxes = torch.stack(recovered_bboxes).mean(dim=0)\n    if aug_scores is None:\n        return bboxes\n    else:\n        scores = torch.stack(aug_scores).mean(dim=0)\n        return bboxes, scores\n\n\ndef merge_aug_scores(aug_scores):\n    """"""Merge augmented bbox scores.""""""\n    if isinstance(aug_scores[0], torch.Tensor):\n        return torch.mean(torch.stack(aug_scores), dim=0)\n    else:\n        return np.mean(aug_scores, axis=0)\n'"
mmaction/core/utils/__init__.py,0,"b""from .dist_utils import allreduce_grads, DistOptimizerHook\n\n__all__ = [\n    'allreduce_grads', 'DistOptimizerHook',\n]"""
mmaction/core/utils/dist_utils.py,2,"b""from collections import OrderedDict\n\nimport torch.distributed as dist\nfrom torch._utils import (_flatten_dense_tensors, _unflatten_dense_tensors,\n                          _take_tensors)\nfrom mmcv.runner import OptimizerHook\n\n\ndef _allreduce_coalesced(tensors, world_size, bucket_size_mb=-1):\n    if bucket_size_mb > 0:\n        bucket_size_bytes = bucket_size_mb * 1024 * 1024\n        buckets = _take_tensors(tensors, bucket_size_bytes)\n    else:\n        buckets = OrderedDict()\n        for tensor in tensors:\n            tp = tensor.type()\n            if tp not in buckets:\n                buckets[tp] = []\n            buckets[tp].append(tensor)\n        buckets = buckets.values()\n\n    for bucket in buckets:\n        flat_tensors = _flatten_dense_tensors(bucket)\n        dist.all_reduce(flat_tensors)\n        flat_tensors.div_(world_size)\n        for tensor, synced in zip(\n                bucket, _unflatten_dense_tensors(flat_tensors, bucket)):\n            tensor.copy_(synced)\n\n\ndef allreduce_grads(model, coalesce=True, bucket_size_mb=-1):\n    grads = [\n        param.grad.data for param in model.parameters()\n        if param.requires_grad and param.grad is not None\n    ]\n    world_size = dist.get_world_size()\n    if coalesce:\n        _allreduce_coalesced(grads, world_size, bucket_size_mb)\n    else:\n        for tensor in grads:\n            dist.all_reduce(tensor.div_(world_size))\n\n\nclass DistOptimizerHook(OptimizerHook):\n\n    def __init__(self, grad_clip=None, coalesce=True, bucket_size_mb=-1):\n        self.grad_clip = grad_clip\n        self.coalesce = coalesce\n        self.bucket_size_mb = bucket_size_mb\n\n    def after_train_iter(self, runner):\n        runner.optimizer.zero_grad()\n        runner.outputs['loss'].backward()\n        allreduce_grads(runner.model, self.coalesce, self.bucket_size_mb)\n        if self.grad_clip is not None:\n            self.clip_grads(runner.model.parameters())\n        runner.optimizer.step()\n"""
mmaction/datasets/loader/__init__.py,0,"b""from .build_loader import build_dataloader\nfrom .sampler import GroupSampler, DistributedGroupSampler\n\n__all__ = [\n    'GroupSampler', 'DistributedGroupSampler', 'build_dataloader'\n]\n"""
mmaction/datasets/loader/build_loader.py,1,"b""from functools import partial\n\nfrom mmcv.runner import get_dist_info\nfrom mmcv.parallel import collate\nfrom torch.utils.data import DataLoader\n\nfrom .sampler import GroupSampler, DistributedGroupSampler, DistributedSampler\n\n# https://github.com/pytorch/pytorch/issues/973\nimport resource\nrlimit = resource.getrlimit(resource.RLIMIT_NOFILE)\nresource.setrlimit(resource.RLIMIT_NOFILE, (4096, rlimit[1]))\n\n\ndef build_dataloader(dataset,\n                     imgs_per_gpu,\n                     workers_per_gpu,\n                     num_gpus=1,\n                     dist=True,\n                     **kwargs):\n    shuffle = kwargs.get('shuffle', True)\n    if dist:\n        rank, world_size = get_dist_info()\n        if shuffle:\n            sampler = DistributedGroupSampler(dataset, imgs_per_gpu, world_size, rank)\n        else:\n            sampler = DistributedSampler(dataset, world_size, rank, shuffle=False)\n        batch_size = imgs_per_gpu\n        num_workers = workers_per_gpu\n    else:\n        if not kwargs.get('shuffle', True):\n            sampler = None\n        else:\n            sampler = GroupSampler(dataset, imgs_per_gpu)\n        batch_size = num_gpus * imgs_per_gpu\n        num_workers = num_gpus * workers_per_gpu\n\n    data_loader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        sampler=sampler,\n        num_workers=num_workers,\n        collate_fn=partial(collate, samples_per_gpu=imgs_per_gpu),\n        pin_memory=False,\n        **kwargs)\n\n    return data_loader\n"""
mmaction/datasets/loader/sampler.py,11,"b'from __future__ import division\n\nimport math\nimport torch\nimport numpy as np\n\nfrom torch.distributed import get_world_size, get_rank\nfrom torch.utils.data.sampler import Sampler\nfrom torch.utils.data import DistributedSampler as _DistributedSampler\n\n\nclass GroupSampler(Sampler):\n\n    def __init__(self, dataset, samples_per_gpu=1):\n        assert hasattr(dataset, \'flag\')\n        self.dataset = dataset\n        self.samples_per_gpu = samples_per_gpu\n        self.flag = dataset.flag.astype(np.int64)\n        self.group_sizes = np.bincount(self.flag)\n        self.num_samples = 0\n        for i, size in enumerate(self.group_sizes):\n            self.num_samples += int(np.ceil(\n                size / self.samples_per_gpu)) * self.samples_per_gpu\n\n    def __iter__(self):\n        indices = []\n        for i, size in enumerate(self.group_sizes):\n            if size == 0:\n                continue\n            indice = np.where(self.flag == i)[0]\n            assert len(indice) == size\n            np.random.shuffle(indice)\n            num_extra = int(np.ceil(size / self.samples_per_gpu)\n                            ) * self.samples_per_gpu - len(indice)\n            indice = np.concatenate([indice, indice[:num_extra]])\n            indices.append(indice)\n        indices = np.concatenate(indices)\n        indices = [\n            indices[i * self.samples_per_gpu:(i + 1) * self.samples_per_gpu]\n            for i in np.random.permutation(\n                range(len(indices) // self.samples_per_gpu))\n        ]\n        indices = np.concatenate(indices)\n        indices = torch.from_numpy(indices).long()\n        assert len(indices) == self.num_samples\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_samples\n\n\nclass DistributedSampler(_DistributedSampler):\n\n    def __init__(self, dataset, num_replicas=None, rank=None, shuffle=True, byclass=False):\n        super().__init__(dataset, num_replicas=num_replicas, rank=rank)\n        self.shuffle = shuffle\n\n    def __iter__(self):\n        # deterministically shuffle based on epoch\n        if self.shuffle:\n            g = torch.Generator()\n            g.manual_seed(self.epoch)\n            indices = torch.randperm(len(self.dataset), generator=g).tolist()\n        else:\n            indices = torch.arange(len(self.dataset)).tolist()\n\n        # add extra samples to make it evenly divisible\n        indices += indices[:(self.total_size - len(indices))]\n        assert len(indices) == self.total_size\n\n        # subsample\n        indices = indices[self.rank:self.total_size:self.num_replicas]\n        assert len(indices) == self.num_samples\n        return iter(indices)\n\n\nclass DistributedGroupSampler(Sampler):\n    """"""Sampler that restricts data loading to a subset of the dataset.\n    It is especially useful in conjunction with\n    :class:`torch.nn.parallel.DistributedDataParallel`. In such case, each\n    process can pass a DistributedSampler instance as a DataLoader sampler,\n    and load a subset of the original dataset that is exclusive to it.\n    .. note::\n        Dataset is assumed to be of constant size.\n    Arguments:\n        dataset: Dataset used for sampling.\n        num_replicas (optional): Number of processes participating in\n            distributed training.\n        rank (optional): Rank of the current process within num_replicas.\n    """"""\n\n    def __init__(self,\n                 dataset,\n                 samples_per_gpu=1,\n                 num_replicas=None,\n                 rank=None):\n        if num_replicas is None:\n            num_replicas = get_world_size()\n        if rank is None:\n            rank = get_rank()\n        self.dataset = dataset\n        self.samples_per_gpu = samples_per_gpu\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n\n        assert hasattr(self.dataset, \'flag\')\n        self.flag = self.dataset.flag\n        self.group_sizes = np.bincount(self.flag)\n\n        self.num_samples = 0\n        for i, j in enumerate(self.group_sizes):\n            self.num_samples += int(\n                math.ceil(self.group_sizes[i] * 1.0 / self.samples_per_gpu /\n                          self.num_replicas)) * self.samples_per_gpu\n        self.total_size = self.num_samples * self.num_replicas\n\n    def __iter__(self):\n        # deterministically shuffle based on epoch\n        g = torch.Generator()\n        g.manual_seed(self.epoch)\n\n        indices = []\n        for i, size in enumerate(self.group_sizes):\n            if size > 0:\n                indice = np.where(self.flag == i)[0]\n                assert len(indice) == size\n                indice = indice[list(torch.randperm(int(size),\n                                                    generator=g))].tolist()\n                extra = int(\n                    math.ceil(\n                        size * 1.0 / self.samples_per_gpu / self.num_replicas)\n                ) * self.samples_per_gpu * self.num_replicas - len(indice)\n                indice += indice[:extra]\n                indices += indice\n\n        assert len(indices) == self.total_size\n\n        indices = [\n            indices[j] for i in list(\n                torch.randperm(\n                    len(indices) // self.samples_per_gpu, generator=g))\n            for j in range(i * self.samples_per_gpu, (i + 1) *\n                           self.samples_per_gpu)\n        ]\n\n        # subsample\n        offset = self.num_samples * self.rank\n        indices = indices[offset:offset + self.num_samples]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_samples\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch\n'"
mmaction/models/detectors/__init__.py,0,"b""from .base import BaseDetector\nfrom .two_stage import TwoStageDetector\nfrom .fast_rcnn import FastRCNN\nfrom .faster_rcnn import FasterRCNN\n\n__all__ = [\n    'BaseDetector', 'TwoStageDetector',\n    'FastRCNN', 'FasterRCNN',\n]\n"""
mmaction/models/detectors/base.py,1,"b'import logging\nfrom abc import ABCMeta, abstractmethod\n\nimport mmcv\nimport numpy as np\nimport torch.nn as nn\n\nfrom mmaction.core import get_classes\nfrom mmaction.utils.misc import tensor2video_snaps\n\n\nclass BaseDetector(nn.Module):\n    """"""Base class for detectors""""""\n\n    __metaclass__ = ABCMeta\n\n    def __init__(self):\n        super(BaseDetector, self).__init__()\n\n    @property\n    def with_neck(self):\n        return hasattr(self, \'neck\') and self.neck is not None\n\n    @property\n    def with_shared_head(self):\n        return hasattr(self, \'shared_head\') and self.shared_head is not None\n\n    @property\n    def with_bbox(self):\n        return hasattr(self, \'bbox_head\') and self.bbox_head is not None\n\n    @abstractmethod\n    def extract_feat(self, img_group):\n        pass\n\n    def extract_feats(self, img_groups):\n        assert isinstance(img_groups, list)\n        for img_group in img_groups:\n            yield self.extract_feat(img_group)\n\n    @abstractmethod\n    def forward_train(self, num_modalities, img_metas, **kwargs):\n        pass\n\n    @abstractmethod\n    def simple_test(self, num_modalities, img_metas, **kwargs):\n        pass\n\n    @abstractmethod\n    def aug_test(self, num_modalities, img_metas, **kwargs):\n        pass\n\n    def init_weights(self, pretrained=None):\n        if pretrained is not None:\n            logger = logging.getLogger()\n            logger.info(\'load model from: {}\'.format(pretrained))\n\n    def forward_test(self, num_modalities, img_metas, **kwargs):\n        if not isinstance(img_metas, list):\n            raise TypeError(\'{} must be a list, but got {}\'.format(\n                img_metas, type(img_metas)))\n\n        num_augs = len(kwargs[\'img_group_0\'])\n        if num_augs != len(img_metas):\n            raise ValueError(\n                \'num of augmentations ({}) != num of image meta ({})\'.format(\n                    num_augs, len(img_metas)))\n        # TODO: remove the restriction of videos_per_gpu == 1 when prepared\n        videos_per_gpu = kwargs[\'img_group_0\'][0].size(0)\n        assert videos_per_gpu == 1\n\n        if num_augs == 1:\n            return self.simple_test(num_modalities, img_metas, **kwargs)\n        else:\n            return self.aug_test(num_modalities, img_metas, **kwargs)\n\n    def forward(self, num_modalities, img_meta, return_loss=True, **kwargs):\n        num_modalities = int(num_modalities[0])\n        if return_loss:\n            return self.forward_train(num_modalities, img_meta, **kwargs)\n        else:\n            return self.forward_test(num_modalities, img_meta, **kwargs)\n\n    def show_result(self,\n                    data,\n                    bbox_result,\n                    img_norm_cfg,\n                    dataset=\'ava\',\n                    score_thr=0.3):\n\n        img_group_tensor = data[\'img_group_0\'][0]\n        img_metas = data[\'img_meta\'][0].data[0]\n        imgs = tensor2video_snaps(img_group_tensor, **img_norm_cfg)\n        assert len(imgs) == len(img_metas)\n\n        if isinstance(dataset, str):\n            class_names = get_classes(dataset)\n        elif isinstance(dataset, (list, tuple)) or dataset is None:\n            class_names = dataset\n        else:\n            raise TypeError(\n                \'dataset must be a valid dataset name or a sequence\'\n                \' of class names, not {}\'.format(type(dataset)))\n\n        for img, img_meta in zip(imgs, img_metas):\n            h, w, _ = img_meta[\'img_shape\']\n            img_show = img[:h, :w, :]\n\n            bboxes = np.vstack(bbox_result)\n            # draw bounding boxes\n            labels = [\n                np.full(bbox.shape[0], i, dtype=np.int32)\n                for i, bbox in enumerate(bbox_result)\n            ]\n            labels = np.concatenate(labels)\n            mmcv.imshow_det_bboxes(\n                img_show,\n                bboxes,\n                labels,\n                class_names=class_names,\n                score_thr=score_thr)\n'"
mmaction/models/detectors/fast_rcnn.py,0,"b'from .two_stage import TwoStageDetector\nfrom ..registry import DETECTORS\n\n\n@DETECTORS.register_module\nclass FastRCNN(TwoStageDetector):\n\n    def __init__(self,\n                 backbone,\n                 bbox_roi_extractor,\n                 bbox_head,\n                 train_cfg,\n                 test_cfg,\n                 dropout_ratio=0,\n                 neck=None,\n                 shared_head=None,\n                 pretrained=None):\n        super(FastRCNN, self).__init__(\n            backbone=backbone,\n            neck=neck,\n            shared_head=shared_head,\n            bbox_roi_extractor=bbox_roi_extractor,\n            dropout_ratio=dropout_ratio,\n            bbox_head=bbox_head,\n            train_cfg=train_cfg,\n            test_cfg=test_cfg,\n            pretrained=pretrained)\n'"
mmaction/models/detectors/faster_rcnn.py,0,"b'from .two_stage import TwoStageDetector\nfrom ..registry import DETECTORS\n\n\n@DETECTORS.register_module\nclass FasterRCNN(TwoStageDetector):\n\n    def __init__(self,\n                 backbone,\n                 rpn_head,\n                 bbox_roi_extractor,\n                 bbox_head,\n                 train_cfg,\n                 test_cfg,\n                 dropout_ratio=0,\n                 neck=None,\n                 shared_head=None,\n                 pretrained=None):\n        super(FasterRCNN, self).__init__(\n            backbone=backbone,\n            neck=neck,\n            shared_head=shared_head,\n            rpn_head=rpn_head,\n            bbox_roi_extractor=bbox_roi_extractor,\n            dropout_ratio=dropout_ratio,\n            bbox_head=bbox_head,\n            train_cfg=train_cfg,\n            test_cfg=test_cfg,\n            pretrained=pretrained)\n'"
mmaction/models/detectors/test_mixins.py,0,"b'from mmaction.core.bbox2d import bbox2roi, bbox_mapping\nfrom mmaction.core.post_processing import (merge_aug_proposals,\n                                           merge_aug_bboxes,\n                                           multiclass_nms)\n\n\nclass RPNTestMixin(object):\n\n    def simple_test_rpn(self, x, img_meta, rpn_test_cfg):\n        x_slice = (xx[:, :, xx.size(2) // 2, :, :] for xx in x)\n        rpn_outs = self.rpn_head(x_slice)\n        proposal_inputs = rpn_outs + (img_meta, rpn_test_cfg)\n        proposal_list = self.rpn_head.get_bboxes(*proposal_inputs)\n        return proposal_list\n\n    def aug_test_rpn(self, feats, img_metas, rpn_test_cfg):\n        imgs_per_gpu = len(img_metas[0])\n        aug_proposals = [[] for _ in range(imgs_per_gpu)]\n        for x, img_meta in zip(feats, img_metas):\n            proposal_list = self.simple_test_rpn(x, img_meta, rpn_test_cfg)\n            for i, proposals in enumerate(proposal_list):\n                aug_proposals[i].append(proposals)\n        # after merging, proposals will be rescaled to the original image size\n        merged_proposals = [\n            merge_aug_proposals(proposals, img_meta, rpn_test_cfg)\n            for proposals, img_meta in zip(aug_proposals, img_metas)\n        ]\n        return merged_proposals\n\n\nclass BBoxTestMixin(object):\n\n    def simple_test_bboxes(self,\n                           x,\n                           img_meta,\n                           proposals,\n                           rcnn_test_cfg,\n                           rescale=False):\n        """"""Test only det bboxes without augmentation.""""""\n        rois = bbox2roi(proposals)\n        roi_feats = self.bbox_roi_extractor(\n            x[:len(self.bbox_roi_extractor.featmap_strides)], rois)\n        if self.with_shared_head:\n            roi_feats = self.shared_head(roi_feats)\n        cls_score, bbox_pred = self.bbox_head(roi_feats)\n        img_shape = img_meta[0][\'img_shape\']\n        scale_factor = img_meta[0][\'scale_factor\']\n        det_bboxes, det_labels = self.bbox_head.get_det_bboxes(\n            rois,\n            cls_score,\n            bbox_pred,\n            img_shape,\n            scale_factor,\n            rescale=rescale,\n            cfg=rcnn_test_cfg,\n            crop_quadruple=img_meta[0][\'crop_quadruple\'])\n        return det_bboxes, det_labels\n\n    def aug_test_bboxes(self, feats, img_metas, proposal_list, rcnn_test_cfg):\n        aug_bboxes = []\n        aug_scores = []\n        for x, img_meta in zip(feats, img_metas):\n            # only one image in the batch\n            img_shape = img_meta[0][\'img_shape\']\n            scale_factor = img_meta[0][\'scale_factor\']\n            flip = img_meta[0][\'flip\']\n            # TODO more flexible\n            proposals = bbox_mapping(proposal_list[0][:, :4], img_shape,\n                                     scale_factor, flip)\n            rois = bbox2roi([proposals])\n            # recompute feature maps to save GPU memory\n            roi_feats = self.bbox_roi_extractor(\n                x[:len(self.bbox_roi_extractor.featmap_strides)], rois)\n            if self.with_shared_head:\n                roi_feats = self.shared_head(roi_feats)\n            cls_score, bbox_pred = self.bbox_head(roi_feats)\n            bboxes, scores = self.bbox_head.get_det_bboxes(\n                rois,\n                cls_score,\n                bbox_pred,\n                img_shape,\n                scale_factor,\n                rescale=False,\n                cfg=None)\n            aug_bboxes.append(bboxes)\n            aug_scores.append(scores)\n        # after merging, bboxes will be rescaled to the original image size\n        merged_bboxes, merged_scores = merge_aug_bboxes(\n            aug_bboxes, aug_scores, img_metas, rcnn_test_cfg)\n        det_bboxes, det_labels = multiclass_nms(\n            merged_bboxes, merged_scores, rcnn_test_cfg.score_thr,\n            rcnn_test_cfg.nms, rcnn_test_cfg.max_per_img)\n        return det_bboxes, det_labels\n'"
mmaction/models/detectors/two_stage.py,1,"b'import torch.nn as nn\n\nfrom .base import BaseDetector\nfrom .test_mixins import RPNTestMixin, BBoxTestMixin\nfrom .. import builder\nfrom ..registry import DETECTORS\nfrom mmaction.core.bbox2d import (bbox2roi, bbox2result,\n                                  build_assigner, build_sampler)\n\n\n@DETECTORS.register_module\nclass TwoStageDetector(BaseDetector, RPNTestMixin, BBoxTestMixin):\n\n    def __init__(self,\n                 backbone,\n                 neck=None,\n                 shared_head=None,\n                 rpn_head=None,\n                 bbox_roi_extractor=None,\n                 dropout_ratio=0,\n                 bbox_head=None,\n                 train_cfg=None,\n                 test_cfg=None,\n                 pretrained=None):\n\n        super(TwoStageDetector, self).__init__()\n        self.backbone = builder.build_backbone(backbone)\n\n        if neck is not None:\n            self.neck = builder.build_neck(neck)\n\n        if shared_head is not None:\n            self.shared_head = builder.build_head(shared_head)\n\n        if rpn_head is not None:\n            self.rpn_head = builder.build_head(rpn_head)\n\n        if bbox_head is not None:\n            self.bbox_roi_extractor = builder.build_roi_extractor(\n                bbox_roi_extractor)\n            self.bbox_head = builder.build_head(bbox_head)\n\n        if dropout_ratio > 0:\n            self.dropout = nn.Dropout(p=dropout_ratio)\n        else:\n            self.dropout = None\n\n        self.train_cfg = train_cfg\n        self.test_cfg = test_cfg\n\n        self.init_weights()\n\n    @property\n    def with_rpn(self):\n        return hasattr(self, \'rpn_head\') and self.rpn_head is not None\n\n    def init_weights(self):\n        super(TwoStageDetector, self).init_weights()\n        self.backbone.init_weights()\n        if self.with_neck:\n            if isinstance(self.neck, nn.Sequential):\n                for m in self.neck:\n                    m.init_weights()\n        if self.with_shared_head:\n            self.shared_head.init_weights()\n        if self.with_rpn:\n            self.rpn_head.init_weights()\n        if self.with_bbox:\n            self.bbox_roi_extractor.init_weights()\n            self.bbox_head.init_weights()\n\n    def extract_feat(self, image_group):\n        x = self.backbone(image_group)\n        if self.with_neck:\n            x = self.neck()\n        else:\n            if not isinstance(x, (list, tuple)):\n                x = (x, )\n        return x\n\n    def forward_train(self,\n                      num_modalities,\n                      img_meta,\n                      gt_bboxes,\n                      gt_labels,\n                      gt_bboxes_ignore=None,\n                      proposals=None,\n                      **kwargs):\n        assert num_modalities == 1\n        img_group = kwargs[\'img_group_0\']\n\n        x = self.extract_feat(img_group)\n\n        losses = dict()\n\n        # RPN forward and loss\n        if self.with_rpn:\n            x_slice = (xx[:, :, xx.size(2) // 2, :, :] for xx in x)\n            rpn_outs = self.rpn_head(x_slice)\n            rpn_loss_inputs = rpn_outs + \\\n                (gt_bboxes, img_meta, self.train_cfg.rpn)\n            rpn_losses = self.rpn_head.loss(\n                *rpn_loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)\n            losses.update(rpn_losses)\n\n            proposal_inputs = rpn_outs + (img_meta, self.test_cfg.rpn)\n            proposal_list = self.rpn_head.get_bboxes(*proposal_inputs)\n        else:\n            proposal_list = proposals\n\n        if not self.train_cfg.train_detector:\n            proposal_list = []\n            for proposal in proposals:\n                select_inds = proposal[:, 4] >= min(\n                    self.train_cfg.person_det_score_thr,\n                    max(proposal[:, 4]))\n                proposal_list.append(proposal[select_inds])\n\n        # assign gts and sample proposals\n        if self.with_bbox:\n            bbox_assigner = build_assigner(self.train_cfg.rcnn.assigner)\n            bbox_sampler = build_sampler(\n                self.train_cfg.rcnn.sampler, context=self)\n            num_imgs = img_group.size(0)\n            if gt_bboxes_ignore is None:\n                gt_bboxes_ignore = [None for _ in range(num_imgs)]\n            sampling_results = []\n            for i in range(num_imgs):\n                assign_result = bbox_assigner.assign(\n                    proposal_list[i], gt_bboxes[i], gt_bboxes_ignore[i],\n                    gt_labels[i])\n                sampling_result = bbox_sampler.sample(\n                    assign_result,\n                    proposal_list[i],\n                    gt_bboxes[i],\n                    gt_labels[i],\n                    feats=[lvl_feat[i][None] for lvl_feat in x])\n                sampling_results.append(sampling_result)\n\n        # bbox head forward and loss\n        if self.with_bbox:\n            rois = bbox2roi([res.bboxes for res in sampling_results])\n            # TODO: a more flexible way to decide which feature maps to use\n            bbox_feats = self.bbox_roi_extractor(\n                x[:self.bbox_roi_extractor.num_inputs], rois)\n            if self.with_shared_head:\n                bbox_feats = self.shared_head(bbox_feats)\n\n            if self.dropout is not None:\n                bbox_feats = self.dropout(bbox_feats)\n\n            cls_score, bbox_pred = self.bbox_head(bbox_feats)\n\n            bbox_targets = self.bbox_head.get_target(\n                sampling_results, gt_bboxes, gt_labels, self.train_cfg.rcnn)\n            loss_bbox = self.bbox_head.loss(cls_score, bbox_pred,\n                                            *bbox_targets)\n            if not self.train_cfg.train_detector:\n                loss_bbox.pop(\'loss_person_cls\')\n            losses.update(loss_bbox)\n\n        return losses\n\n    def simple_test(self, num_modalities, img_meta,\n                    proposals=None, rescale=False,\n                    **kwargs):\n        """"""Test without augmentation.""""""\n        assert self.with_bbox, ""Bbox head must be implemented.""\n\n        assert num_modalities == 1\n        img_group = kwargs[\'img_group_0\'][0]\n        x = self.extract_feat(img_group)\n\n        if proposals is None:\n            proposal_list = self.simple_test_rpn(\n                x, img_meta, self.test_cfg.rpn)\n        else:\n            proposal_list = []\n            for proposal in proposals:\n                proposal = proposal[0, ...]\n                if not self.test_cfg.train_detector:\n                    select_inds = proposal[:, 4] >= min(\n                        self.test_cfg.person_det_score_thr,\n                        max(proposal[:, 4]))\n                    proposal = proposal[select_inds]\n                proposal_list.append(proposal)\n\n        img_meta = img_meta[0]\n\n        det_bboxes, det_labels = self.simple_test_bboxes(\n            x, img_meta, proposal_list, self.test_cfg.rcnn, rescale=rescale)\n        bbox_results = bbox2result(det_bboxes, det_labels,\n                                   self.bbox_head.num_classes,\n                                   thr=self.test_cfg.rcnn.action_thr)\n\n        return bbox_results\n\n    def aug_test(self, num_modalities, img_metas,\n                 proposals=None, rescale=False,\n                 **kwargs):\n        """"""Test with augmentations.\n\n        If rescale is False, then returned bboxes will fit the scale\n        of imgs[0]\n        """"""\n        assert num_modalities == 1\n        img_groups = kwargs[\'img_group_0\']\n        if proposals is None:\n            proposal_list = self.aug_test_rpn(\n                self.extract_feats(img_groups), img_metas, self.test_cfg.rpn)\n        else:\n            # TODO: need check\n            proposal_list = []\n            for proposal in proposals:\n                proposal = proposal[0, ...]\n                if not self.test_cfg.train_detector:\n                    select_inds = proposal[:, 4] >= min(\n                        self.test_cfg.person_det_score_thr,\n                        max(proposal[:, 4]))\n                    proposal = proposal[select_inds]\n                proposal_list.append(proposal)\n\n        det_bboxes, det_labels = self.aug_test_bboxes(\n            self.extract_feats(img_groups), img_metas, proposal_list,\n            self.test_cfg.rcnn)\n\n        if rescale:\n            _det_bboxes = det_bboxes\n        else:\n            _det_bboxes = det_bboxes.clone()\n            _det_bboxes[:, :4] *= img_metas[0][0][\'scale_factor\']\n        bbox_results = bbox2result(_det_bboxes, det_labels,\n                                   self.bbox_head.num_classes,\n                                   thr=self.test_cfg.rcnn.action_thr)\n\n        return bbox_results\n'"
mmaction/models/localizers/SSN2D.py,2,"b'import torch\nimport torch.nn as nn\nfrom .base import BaseLocalizer\nfrom .. import builder\nfrom ..registry import LOCALIZERS\n\n\n@LOCALIZERS.register_module\nclass SSN2D(BaseLocalizer):\n\n    def __init__(self,\n                 backbone,\n                 modality=\'RGB\',\n                 in_channels=3,\n                 spatial_temporal_module=None,\n                 dropout_ratio=0.5,\n                 segmental_consensus=None,\n                 cls_head=None,\n                 train_cfg=None,\n                 test_cfg=None):\n\n        super(SSN2D, self).__init__()\n        self.backbone = builder.build_backbone(backbone)\n        self.modality = modality\n        self.in_channels = in_channels\n\n        if spatial_temporal_module is not None:\n            self.spatial_temporal_module = (\n                builder.build_spatial_temporal_module(spatial_temporal_module))\n        else:\n            raise NotImplementedError\n\n        self.dropout_ratio = dropout_ratio\n        if self.dropout_ratio != 0:\n            self.dropout = nn.Dropout(p=self.dropout_ratio)\n\n        if segmental_consensus is not None:\n            self.segmental_consensus = builder.build_segmental_consensus(\n                segmental_consensus)\n        else:\n            raise NotImplementedError\n\n        if cls_head is not None:\n            self.cls_head = builder.build_head(cls_head)\n            self.is_test_prepared = False\n        else:\n            raise NotImplementedError\n\n        self.train_cfg = train_cfg\n        self.test_cfg = test_cfg\n\n        assert modality in [\'RGB\', \'Flow\', \'RGBDiff\']\n\n        self.init_weights()\n\n        if modality == \'Flow\' or modality == \'RGBDiff\':\n            self._construct_2d_backbone_conv1(in_channels)\n\n    @property\n    def with_spatial_temporal_module(self):\n        return (hasattr(self, \'spatial_temporal_module\') and\n                self.spatial_temporal_module is not None)\n\n    @property\n    def with_segmental_consensus(self):\n        return (hasattr(self, \'segmental_consensus\') and\n                self.segmental_consensus is not None)\n\n    @property\n    def with_cls_head(self):\n        return hasattr(self, \'cls_head\') and self.cls_head is not None\n\n    def _construct_2d_backbone_conv1(self, in_channels):\n        modules = list(self.backbone.modules())\n        first_conv_idx = list(filter(lambda x: isinstance(\n            modules[x], nn.Conv2d), list(range(len(modules)))))[0]\n        conv_layer = modules[first_conv_idx]\n        container = modules[first_conv_idx - 1]\n\n        params = [x.clone() for x in conv_layer.parameters()]\n        kernel_size = params[0].size()\n        new_kernel_size = kernel_size[:1] + (in_channels, ) + kernel_size[2:]\n        new_kernel_data = params[0].data.mean(dim=1, keepdim=True).expand(\n            new_kernel_size).contiguous()  # make contiguous!\n\n        new_conv_layer = nn.Conv2d(in_channels, conv_layer.out_channels,\n                                   conv_layer.kernel_size,\n                                   conv_layer.stride, conv_layer.padding,\n                                   bias=True if len(params) == 2 else False)\n        new_conv_layer.weight.data = new_kernel_data\n        if len(params) == 2:\n            new_conv_layer.bias.data = params[1].data\n        # remove "".weight"" suffix to get the layer layer_name\n        layer_name = list(container.state_dict().keys())[0][:-7]\n        setattr(container, layer_name, new_conv_layer)\n\n    def init_weights(self):\n        super(SSN2D, self).init_weights()\n        self.backbone.init_weights()\n\n        if self.with_spatial_temporal_module:\n            self.spatial_temporal_module.init_weights()\n\n        if self.with_segmental_consensus:\n            self.segmental_consensus.init_weights()\n\n        if self.with_cls_head:\n            self.cls_head.init_weights()\n\n    def extract_feat(self, img_group):\n        x = self.backbone(img_group)\n        return x\n\n    def forward_train(self,\n                      num_modalities,\n                      img_meta,\n                      prop_scaling,\n                      prop_type,\n                      prop_labels,\n                      reg_targets,\n                      **kwargs):\n        assert num_modalities == 1\n        img_group = kwargs[\'img_group_0\']\n\n        img_group = img_group.reshape(\n            (-1, self.in_channels) + img_group.shape[4:])\n\n        x = self.extract_feat(img_group)\n        if self.with_spatial_temporal_module:\n            x = self.spatial_temporal_module(x)\n        if self.dropout is not None:\n            x = self.dropout(x)\n        activity_feat, completeness_feat = self.segmental_consensus(\n            x, prop_scaling)\n        losses = dict()\n        if self.with_cls_head:\n            activity_score, completeness_score, bbox_pred = self.cls_head(\n                (activity_feat, completeness_feat))\n            loss_cls = self.cls_head.loss(activity_score, completeness_score,\n                                          bbox_pred, prop_type, prop_labels,\n                                          reg_targets, self.train_cfg)\n            losses.update(loss_cls)\n\n        return losses\n\n    def forward_test(self,\n                     num_modalities,\n                     img_meta,\n                     rel_prop_list,\n                     scaling_list,\n                     prop_tick_list,\n                     reg_stats,\n                     **kwargs):\n        assert num_modalities == 1\n        img_group = kwargs[\'img_group_0\']\n\n        img_group = img_group[0]\n        num_crop = img_group.shape[0]\n        img_group = img_group.reshape(\n            (num_crop, -1, self.in_channels) + img_group.shape[3:])\n        num_ticks = img_group.shape[1]\n\n        output = []\n        minibatch_size = self.test_cfg.ssn.sampler.batch_size\n        for ind in range(0, num_ticks, minibatch_size):\n            chunk = img_group[:, ind:ind + minibatch_size, ...].view(\n                (-1,) + img_group.shape[2:])\n            x = self.extract_feat(chunk.cuda())\n            x = self.spatial_temporal_module(x)\n            # merge crop to save memory\n            # TODO: A smarte way of dealing with arbitary long videos\n            x = x.reshape((num_crop, x.size(0)//num_crop, -1)).mean(dim=0)\n            output.append(x)\n        output = torch.cat(output, dim=0)\n\n        if not self.is_test_prepared:\n            self.is_test_prepared = self.cls_head.prepare_test_fc(\n                self.segmental_consensus.feat_multiplier)\n        output = self.cls_head(output, test_mode=True)\n\n        rel_prop_list = rel_prop_list.squeeze(0)\n        prop_tick_list = prop_tick_list.squeeze(0)\n        scaling_list = scaling_list.squeeze(0)\n        reg_stats = reg_stats.squeeze(0)\n        (activity_scores, completeness_scores,\n         bbox_preds) = self.segmental_consensus(\n            output, prop_tick_list, scaling_list)\n        if bbox_preds is not None:\n            bbox_preds = bbox_preds.view(-1, self.cls_head.num_classes, 2)\n            bbox_preds[:, :, 0] = bbox_preds[:, :, 0] * \\\n                reg_stats[1, 0] + reg_stats[0, 0]\n            bbox_preds[:, :, 1] = bbox_preds[:, :, 1] * \\\n                reg_stats[1, 1] + reg_stats[0, 1]\n\n        return rel_prop_list.cpu().numpy(), activity_scores.cpu().numpy(), \\\n            completeness_scores.cpu().numpy(), bbox_preds.cpu().numpy()\n'"
mmaction/models/localizers/__init__.py,0,"b""from .base import BaseLocalizer\nfrom .SSN2D import SSN2D\n\n__all__ = [\n    'BaseLocalizer', 'SSN2D'\n]\n"""
mmaction/models/localizers/base.py,1,"b'import logging\nfrom abc import ABCMeta, abstractmethod\n\nimport torch.nn as nn\n\nclass BaseLocalizer(nn.Module):\n    """"""Base class for localizers""""""\n\n    __metaclass__ = ABCMeta\n\n    def __init__(self):\n        super(BaseLocalizer, self).__init__()\n\n    @abstractmethod\n    def forward_train(self, num_modalities, **kwargs):\n        pass\n\n    @abstractmethod\n    def forward_test(self, num_modalities, **kwargs):\n        pass\n    \n    def init_weights(self, pretrained=None):\n        if pretrained is not None:\n            logger = logging.getLogger()\n            logger.info(""load model from: {}"".format(pretrained))\n\n    def forward(self, num_modalities, img_meta, return_loss=True, **kwargs):\n        num_modalities = int(num_modalities[0])\n        if return_loss:\n            return self.forward_train(num_modalities, img_meta, **kwargs)\n        else:\n            return self.forward_test(num_modalities, img_meta, **kwargs)\n'"
mmaction/models/recognizers/TSN2D.py,1,"b'import torch.nn as nn\nfrom .base import BaseRecognizer\nfrom .. import builder\nfrom ..registry import RECOGNIZERS\n\n\n@RECOGNIZERS.register_module\nclass TSN2D(BaseRecognizer):\n\n    def __init__(self,\n                 backbone,\n                 modality=\'RGB\',\n                 in_channels=3,\n                 spatial_temporal_module=None,\n                 segmental_consensus=None,\n                 cls_head=None,\n                 train_cfg=None,\n                 test_cfg=None):\n\n        super(TSN2D, self).__init__()\n        self.backbone = builder.build_backbone(backbone)\n        self.modality = modality\n        self.in_channels = in_channels\n\n        if spatial_temporal_module is not None:\n            self.spatial_temporal_module = builder.build_spatial_temporal_module(\n                spatial_temporal_module)\n        else:\n            raise NotImplementedError\n\n        if segmental_consensus is not None:\n            self.segmental_consensus = builder.build_segmental_consensus(\n                segmental_consensus)\n        else:\n            raise NotImplementedError\n\n        if cls_head is not None:\n            self.cls_head = builder.build_head(cls_head)\n        else:\n            raise NotImplementedError\n\n        self.train_cfg = train_cfg\n        self.test_cfg = test_cfg\n\n        assert modality in [\'RGB\', \'Flow\', \'RGBDiff\']\n\n        self.init_weights()\n\n        if modality == \'Flow\' or modality == \'RGBDiff\':\n            self._construct_2d_backbone_conv1(in_channels)\n\n    @property\n    def with_spatial_temporal_module(self):\n        return hasattr(self, \'spatial_temporal_module\') and self.spatial_temporal_module is not None\n\n    @property\n    def with_segmental_consensus(self):\n        return hasattr(self, \'segmental_consensus\') and self.segmental_consensus is not None\n\n    @property\n    def with_cls_head(self):\n        return hasattr(self, \'cls_head\') and self.cls_head is not None\n\n    def _construct_2d_backbone_conv1(self, in_channels):\n        modules = list(self.backbone.modules())\n        first_conv_idx = list(filter(lambda x: isinstance(\n            modules[x], nn.Conv2d), list(range(len(modules)))))[0]\n        conv_layer = modules[first_conv_idx]\n        container = modules[first_conv_idx - 1]\n\n        params = [x.clone() for x in conv_layer.parameters()]\n        kernel_size = params[0].size()\n        new_kernel_size = kernel_size[:1] + (in_channels, ) + kernel_size[2:]\n        new_kernel_data = params[0].data.mean(dim=1, keepdim=True).expand(\n            new_kernel_size).contiguous()  # make contiguous!\n\n        new_conv_layer = nn.Conv2d(in_channels, conv_layer.out_channels,\n                                   conv_layer.kernel_size, conv_layer.stride, conv_layer.padding,\n                                   bias=True if len(params) == 2 else False)\n        new_conv_layer.weight.data = new_kernel_data\n        if len(params) == 2:\n            new_conv_layer.bias.data = params[1].data\n        # remove "".weight"" suffix to get the layer layer_name\n        layer_name = list(container.state_dict().keys())[0][:-7]\n        setattr(container, layer_name, new_conv_layer)\n\n    def init_weights(self):\n        super(TSN2D, self).init_weights()\n        self.backbone.init_weights()\n\n        if self.with_spatial_temporal_module:\n            self.spatial_temporal_module.init_weights()\n\n        if self.with_segmental_consensus:\n            self.segmental_consensus.init_weights()\n\n        if self.with_cls_head:\n            self.cls_head.init_weights()\n\n    def extract_feat(self, img_group):\n        x = self.backbone(img_group)\n        return x\n\n    def forward_train(self,\n                      num_modalities,\n                      img_meta,\n                      gt_label,\n                      **kwargs):\n        assert num_modalities == 1\n        img_group = kwargs[\'img_group_0\']\n\n        bs = img_group.shape[0]\n        img_group = img_group.reshape(\n            (-1, self.in_channels) + img_group.shape[3:])\n        num_seg = img_group.shape[0] // bs\n\n        x = self.extract_feat(img_group)\n        if self.with_spatial_temporal_module:\n            x = self.spatial_temporal_module(x)\n        x = x.reshape((-1, num_seg) + x.shape[1:])\n        if self.with_segmental_consensus:\n            x = self.segmental_consensus(x)\n            x = x.squeeze(1)\n        losses = dict()\n        if self.with_cls_head:\n            cls_score = self.cls_head(x)\n            gt_label = gt_label.squeeze()\n            loss_cls = self.cls_head.loss(cls_score, gt_label)\n            losses.update(loss_cls)\n\n        return losses\n\n    def forward_test(self,\n                     num_modalities,\n                     img_meta,\n                     **kwargs):\n        assert num_modalities == 1\n        img_group = kwargs[\'img_group_0\']\n\n        bs = img_group.shape[0]\n        img_group = img_group.reshape(\n            (-1, self.in_channels) + img_group.shape[3:])\n        num_seg = img_group.shape[0] // bs\n\n        x = self.extract_feat(img_group)\n        if self.with_spatial_temporal_module:\n            x = self.spatial_temporal_module(x)\n        x = x.reshape((-1, num_seg) + x.shape[1:])\n        if self.with_segmental_consensus:\n            x = self.segmental_consensus(x)\n            x = x.squeeze(1)\n        if self.with_cls_head:\n            x = self.cls_head(x)\n\n        return x.cpu().numpy()\n'"
mmaction/models/recognizers/TSN3D.py,4,"b""from .base import BaseRecognizer\nfrom .. import builder\nfrom ..registry import RECOGNIZERS\n\nimport torch\n\n\n@RECOGNIZERS.register_module\nclass TSN3D(BaseRecognizer):\n\n    def __init__(self,\n                 backbone,\n                 flownet=None,\n                 spatial_temporal_module=None,\n                 segmental_consensus=None,\n                 cls_head=None,\n                 train_cfg=None,\n                 test_cfg=None,\n                 fcn_testing=False):\n\n        super(TSN3D, self).__init__()\n        self.fcn_testing = fcn_testing\n        self.backbone = builder.build_backbone(backbone)\n\n        if flownet is not None:\n            self.flownet = builder.build_flownet(flownet)\n\n        if spatial_temporal_module is not None:\n            self.spatial_temporal_module = builder.build_spatial_temporal_module(\n                spatial_temporal_module)\n        else:\n            raise NotImplementedError\n\n        if segmental_consensus is not None:\n            self.segmental_consensus = builder.build_segmental_consensus(\n                segmental_consensus)\n        else:\n            raise NotImplementedError\n\n        if cls_head is not None:\n            self.cls_head = builder.build_head(cls_head)\n        else:\n            raise NotImplementedError\n\n        self.train_cfg = train_cfg\n        self.test_cfg = test_cfg\n\n        self.init_weights()\n\n    @property\n    def with_flownet(self):\n        return hasattr(self, 'flownet') and self.flownet is not None\n\n    @property\n    def with_spatial_temporal_module(self):\n        return hasattr(self, 'spatial_temporal_module') and self.spatial_temporal_module is not None\n\n    @property\n    def with_segmental_consensus(self):\n        return hasattr(self, 'segmental_consensus') and self.segmental_consensus is not None\n\n    @property\n    def with_cls_head(self):\n        return hasattr(self, 'cls_head') and self.cls_head is not None\n\n    def init_weights(self):\n        super(TSN3D, self).init_weights()\n        self.backbone.init_weights()\n\n        if self.with_flownet:\n            self.flownet.init_weights()\n\n        if self.with_spatial_temporal_module:\n            self.spatial_temporal_module.init_weights()\n\n        if self.with_segmental_consensus:\n            self.segmental_consensus.init_weights()\n\n        if self.with_cls_head:\n            self.cls_head.init_weights()\n\n    def extract_feat_with_flow(self, img_group,\n                               trajectory_forward=None,\n                               trajectory_backward=None):\n        x = self.backbone(img_group,\n                          trajectory_forward=trajectory_forward,\n                          trajectory_backward=trajectory_backward)\n        return x\n\n    def extract_feat(self, img_group):\n        x = self.backbone(img_group)\n        return x\n\n    def forward_train(self,\n                      num_modalities,\n                      img_meta,\n                      gt_label,\n                      **kwargs):\n        assert num_modalities == 1\n        img_group = kwargs['img_group_0']\n\n        bs = img_group.shape[0]\n        img_group = img_group.reshape((-1, ) + img_group.shape[2:])\n        num_seg = img_group.shape[0] // bs\n\n        if self.with_flownet:\n            if self.flownet.multiframe:\n                img_forward = img_group[:, :, 1:, :, :]\n                if self.flownet.flip_rgb:\n                    img_forward = img_forward.flip(1)\n                img_forward = img_forward.transpose(1, 2).contiguous().view(\n                    (img_forward.size(0), -1,\n                     img_forward.size(3), img_forward.size(4)))\n                trajectory_forward, photometric_forward, ssim_forward, smooth_forward = self.flownet(\n                    img_forward)\n                img_backward = img_group.flip(2)[:, :, 1:, :, :]\n                if self.flownet.rgb_disorder:\n                    img_backward = img_backward.flip(1)\n                img_backward = img_backward.transpose(1, 2).contiguous().view(\n                    (img_backward.size(0), -1,\n                     img_backward.size(3), img_backward.size(4)))\n                trajectory_backward, photometric_backward, ssim_backward, smooth_backward = self.flownet(\n                    img_backward)\n            else:\n                # TODO: Wrap it into a function, e.g. ImFlows2ImFlowStack\n                num_frames = img_group.size(2)\n                traj_forwards, traj_backwards = [], []\n                photometric_forwards, photometric_backwards = [], []\n                ssim_forwards, ssim_backwards = [], []\n                smooth_forwards, smooth_backwards = [], []\n                for i in range(1, num_frames - 1):\n                    img_forward = img_group[:, :, i:i+2, :, :]\n                    if self.flownet.flip_rgb:\n                        img_forward = img_forward.flip(1)\n                    img_forward = img_forward.transpose(1, 2).contiguous().view(\n                        (img_forward.size(0), -1,\n                         img_forward.size(3), img_forward.size(4)))\n                    traj_forward, photometric_forward, ssim_forward, smooth_forward = self.flownet(\n                        img_forward)\n                    traj_forwards.append(traj_forward)\n                    photometric_forwards.append(photometric_forward)\n                    ssim_forwards.append(ssim_forward)\n                    smooth_forwards.append(smooth_forward)\n                    img_backward = img_group[\n                        :, :,\n                        num_frames - i - 1: num_frames - i + 1, :, :].flip(2)\n                    if self.flownet.flip_rgb:\n                        img_backward = img_backward.flip(1)\n                    img_backward = img_backward.transpose(1, 2).contiguous().view(\n                        (img_backward.size(0), -1,\n                         img_backward.size(3), img_backward.size(4)))\n                    traj_backward, photometric_backward, ssim_backward, smooth_backward = self.flownet(\n                        img_backward)\n                    traj_backwards.append(traj_backward)\n                    photometric_backwards.append(photometric_backward)\n                    ssim_backwards.append(ssim_backward)\n                    smooth_backwards.append(smooth_backward)\n\n                def _organize_trajectories(trajectory_lvls_pairs):\n                    res = [[]] * len(trajectory_lvls_pairs[0])\n                    for trajectory_lvls in trajectory_lvls_pairs:\n                        for i, trajectory in enumerate(trajectory_lvls):\n                            res[i].append(trajectory)\n                    for i in range(len(trajectory_lvls_pairs[0])):\n                        res[i] = torch.cat(res[i], 1)\n                    return tuple(res)\n\n                def _organize_loss_outs(loss_outs_lvls_pairs):\n                    L = len(loss_outs_lvls_pairs)\n                    num_level = len(loss_outs_lvls_pairs[0])\n                    num_item = len(loss_outs_lvls_pairs[0][0])\n                    res = []\n                    for i in range(num_level):\n                        res_level = []\n                        for j in range(num_item):\n                            outs = []\n                            for k in range(L):\n                                outs.append(loss_outs_lvls_pairs[k][i][j])\n                            res_level.append(outs)\n                        res.append(res_level)\n                    for i in range(num_level):\n                        for j in range(num_item):\n                            res[i][j] = torch.cat(res[i][j], 1)\n                        res[i] = tuple(res[i])\n                    return tuple(res)\n\n                trajectory_forward = _organize_trajectories(traj_forwards)\n                trajectory_backward = _organize_trajectories(traj_backwards)\n                photometric_forward = _organize_loss_outs(photometric_forwards)\n                photometric_backward = _organize_loss_outs(\n                    photometric_backwards)\n                ssim_forward = _organize_loss_outs(ssim_forwards)\n                ssim_backward = _organize_loss_outs(ssim_backwards)\n                smooth_forward = _organize_loss_outs(smooth_forwards)\n                smooth_backward = _organize_loss_outs(smooth_backwards)\n\n            x = self.extract_feat_with_flow(\n                img_group[:, :, 1:-1, :, :],\n                trajectory_forward=trajectory_forward,\n                trajectory_backward=trajectory_backward)\n        else:\n            x = self.extract_feat(img_group)\n        if self.with_spatial_temporal_module:\n            x = self.spatial_temporal_module(x)\n        if self.with_segmental_consensus:\n            x = x.reshape((-1, num_seg) + x.shape[1:])\n            x = self.segmental_consensus(x)\n            x = x.squeeze(1)\n        losses = dict()\n        if self.with_flownet:\n            losses.update(self.flownet.loss(photometric_forward,\n                                            ssim_forward, smooth_forward,\n                                            direction='forward'))\n            losses.update(self.flownet.loss(photometric_backward,\n                                            ssim_backward, smooth_backward,\n                                            direction='backward'))\n        if self.with_cls_head:\n            cls_score = self.cls_head(x)\n            gt_label = gt_label.squeeze()\n            loss_cls = self.cls_head.loss(cls_score, gt_label)\n            losses.update(loss_cls)\n\n        return losses\n\n    def forward_test(self,\n                     num_modalities,\n                     img_meta,\n                     **kwargs):\n        assert num_modalities == 1\n        img_group = kwargs['img_group_0']\n\n        bs = img_group.shape[0]\n        img_group = img_group.reshape((-1, ) + img_group.shape[2:])\n        num_seg = img_group.shape[0] // bs\n\n        if self.with_flownet:\n            if self.flownet.multiframe:\n                img_forward = img_group[:, :, 1:, :, :]\n                if self.flownet.flip_rgb:\n                    img_forward = img_forward.flip(1)\n                img_forward = img_forward.transpose(1, 2).contiguous().view(\n                    (img_forward.size(0), -1,\n                     img_forward.size(3), img_forward.size(4)))\n                trajectory_forward, _, _, _ = self.flownet(\n                    img_forward, train=False)\n                img_backward = img_group.flip(2)[:, :, 1:, :, :]\n                if self.flownet.flip_rgb:\n                    img_backward = img_backward.flip(1)\n                img_backward = img_backward.transpose(1, 2).contiguous().view(\n                    (img_backward.size(0), -1,\n                     img_backward.size(3), img_backward.size(4)))\n                trajectory_backward, _, _, _ = self.flownet(\n                    img_backward, train=False)\n            else:\n                # TODO: Wrap it into a function, e.g. ImFlows2ImFlowStack\n                num_frames = img_group.size(2)\n                traj_forwards, traj_backwards = [], []\n                for i in range(1, num_frames - 1):\n                    img_forward = img_group[:, :, i:i+2, :, :]\n                    if self.flownet.rgb_disorder:\n                        img_forward = img_forward.flip(1)\n                    img_forward = img_forward.transpose(1, 2).contiguous().view(\n                        (img_forward.size(0), -1,\n                         img_forward.size(3), img_forward.size(4)))\n                    traj_forward, _, _, _ = self.flownet(\n                        img_forward, train=False)\n                    traj_forwards.append(traj_forward)\n                    img_backward = img_group[\n                        :, :, num_frames - i - 1:\n                        num_frames - i + 1, :, :].flip(2)\n                    if self.flownet.rgb_disorder:\n                        img_backward = img_backward.flip(1)\n                    img_backward = img_backward.transpose(1, 2).contiguous().view(\n                        (img_backward.size(0), -1,\n                         img_backward.size(3), img_backward.size(4)))\n                    traj_backward, _, _, _ = self.flownet(\n                        img_backward, train=False)\n                    traj_backwards.append(traj_backward)\n\n                def _organize_trajectories(trajectory_lvls_pairs):\n                    res = [[]] * len(trajectory_lvls_pairs[0])\n                    for trajectory_lvls in trajectory_lvls_pairs:\n                        for i, trajectory in enumerate(trajectory_lvls):\n                            res[i].append(trajectory)\n                    for i in range(len(trajectory_lvls_pairs[0])):\n                        res[i] = torch.cat(res[i], 1)\n                    return tuple(res)\n\n                trajectory_forward = _organize_trajectories(traj_forwards)\n                trajectory_backward = _organize_trajectories(traj_backwards)\n\n            x = self.extract_feat_with_flow(\n                img_group[:, :, 1:-1, :, :],\n                trajectory_forward=trajectory_forward,\n                trajectory_backward=trajectory_backward)\n        else:\n            x = self.extract_feat(img_group)\n\n        if self.fcn_testing:\n            x = self.cls_head(x)\n\n            prob1 = torch.nn.functional.softmax(x.mean([2,3,4]), 1).mean(0, keepdim=True)\n            return prob1.detach().cpu().numpy()\n\n\n        if self.with_spatial_temporal_module:\n            x = self.spatial_temporal_module(x)\n        if self.with_segmental_consensus:\n            x = x.reshape((-1, num_seg) + x.shape[1:])\n            x = self.segmental_consensus(x)\n            x = x.squeeze(1)\n        if self.with_cls_head:\n            x = self.cls_head(x)\n\n        return x.cpu().numpy()\n"""
mmaction/models/recognizers/__init__.py,0,"b""from .base import BaseRecognizer\nfrom .TSN2D import TSN2D\nfrom .TSN3D import TSN3D\n\n__all__ = [\n    'BaseRecognizer', 'TSN2D', 'TSN3D'\n]"""
mmaction/models/recognizers/base.py,1,"b'import logging\nfrom abc import ABCMeta, abstractmethod\n\nimport torch.nn as nn\n\n\nclass BaseRecognizer(nn.Module):\n    """"""Base class for recognizers""""""\n\n    __metaclass__ = ABCMeta\n\n    def __init__(self):\n        super(BaseRecognizer, self).__init__()\n\n    @property\n    def with_tenon_list(self):\n        return hasattr(self, \'tenon_list\') and self.tenon_list is not None\n\n    @property\n    def with_cls(self):\n        return hasattr(self, \'cls_head\') and self.cls_head is not None\n\n    @abstractmethod\n    def forward_train(self, num_modalities, **kwargs):\n        pass\n\n    @abstractmethod\n    def forward_test(self, num_modalities, **kwargs):\n        pass\n\n    def init_weights(self, pretrained=None):\n        if pretrained is not None:\n            logger = logging.getLogger()\n            logger.info(""load model from: {}"".format(pretrained))\n\n    def forward(self, num_modalities, img_meta, return_loss=True, **kwargs):\n        num_modalities = int(num_modalities[0])\n        if return_loss:\n            return self.forward_train(num_modalities, img_meta, **kwargs)\n        else:\n            return self.forward_test(num_modalities, img_meta, **kwargs)\n'"
mmaction/ops/nms/__init__.py,0,"b""from .nms_wrapper import nms, soft_nms\n\n__all__ = ['nms', 'soft_nms']\n"""
mmaction/ops/nms/nms_wrapper.py,6,"b'import numpy as np\nimport torch\n\nfrom . import nms_cuda, nms_cpu\nfrom .soft_nms_cpu import soft_nms_cpu\n\n\ndef nms(dets, iou_thr, device_id=None):\n    """"""Dispatch to either CPU or GPU NMS implementations.\n\n    The input can be either a torch tensor or numpy array. GPU NMS will be used\n    if the input is a gpu tensor or device_id is specified, otherwise CPU NMS\n    will be used. The returned type will always be the same as inputs.\n\n    Arguments:\n        dets (torch.Tensor or np.ndarray): bboxes with scores.\n        iou_thr (float): IoU threshold for NMS.\n        device_id (int, optional): when `dets` is a numpy array, if `device_id`\n            is None, then cpu nms is used, otherwise gpu_nms will be used.\n\n    Returns:\n        tuple: kept bboxes and indice, which is always the same data type as\n            the input.\n    """"""\n    # convert dets (tensor or numpy array) to tensor\n    if isinstance(dets, torch.Tensor):\n        is_numpy = False\n        dets_th = dets\n    elif isinstance(dets, np.ndarray):\n        is_numpy = True\n        device = \'cpu\' if device_id is None else \'cuda:{}\'.format(device_id)\n        dets_th = torch.from_numpy(dets).to(device)\n    else:\n        raise TypeError(\n            \'dets must be either a Tensor or numpy array, but got {}\'.format(\n                type(dets)))\n\n    # execute cpu or cuda nms\n    if dets_th.shape[0] == 0:\n        inds = dets_th.new_zeros(0, dtype=torch.long)\n    else:\n        if dets_th.is_cuda:\n            inds = nms_cuda.nms(dets_th, iou_thr)\n        else:\n            inds = nms_cpu.nms(dets_th, iou_thr)\n\n    if is_numpy:\n        inds = inds.cpu().numpy()\n    return dets[inds, :], inds\n\n\ndef soft_nms(dets, iou_thr, method=\'linear\', sigma=0.5, min_score=1e-3):\n    if isinstance(dets, torch.Tensor):\n        is_tensor = True\n        dets_np = dets.detach().cpu().numpy()\n    elif isinstance(dets, np.ndarray):\n        is_tensor = False\n        dets_np = dets\n    else:\n        raise TypeError(\n            \'dets must be either a Tensor or numpy array, but got {}\'.format(\n                type(dets)))\n\n    method_codes = {\'linear\': 1, \'gaussian\': 2}\n    if method not in method_codes:\n        raise ValueError(\'Invalid method for SoftNMS: {}\'.format(method))\n    new_dets, inds = soft_nms_cpu(\n        dets_np,\n        iou_thr,\n        method=method_codes[method],\n        sigma=sigma,\n        min_score=min_score)\n\n    if is_tensor:\n        return dets.new_tensor(new_dets), dets.new_tensor(\n            inds, dtype=torch.long)\n    else:\n        return new_dets.astype(np.float32), inds.astype(np.int64)\n'"
mmaction/ops/nms/setup.py,1,"b'import os.path as osp\nfrom setuptools import setup, Extension\n\nimport numpy as np\nfrom Cython.Build import cythonize\nfrom Cython.Distutils import build_ext\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\next_args = dict(\n    include_dirs=[np.get_include()],\n    language=\'c++\',\n    extra_compile_args={\n        \'cc\': [\'-Wno-unused-function\', \'-Wno-write-strings\'],\n        \'nvcc\': [\'-c\', \'--compiler-options\', \'-fPIC\'],\n    },\n)\n\nextensions = [\n    Extension(\'soft_nms_cpu\', [\'src/soft_nms_cpu.pyx\'], **ext_args),\n]\n\n\ndef customize_compiler_for_nvcc(self):\n    """"""inject deep into distutils to customize how the dispatch\n    to cc/nvcc works.\n    If you subclass UnixCCompiler, it\'s not trivial to get your subclass\n    injected in, and still have the right customizations (i.e.\n    distutils.sysconfig.customize_compiler) run on it. So instead of going\n    the OO route, I have this. Note, it\'s kindof like a wierd functional\n    subclassing going on.""""""\n\n    # tell the compiler it can processes .cu\n    self.src_extensions.append(\'.cu\')\n\n    # save references to the default compiler_so and _comple methods\n    default_compiler_so = self.compiler_so\n    super = self._compile\n\n    # now redefine the _compile method. This gets executed for each\n    # object but distutils doesn\'t have the ability to change compilers\n    # based on source extension: we add it.\n    def _compile(obj, src, ext, cc_args, extra_postargs, pp_opts):\n        if osp.splitext(src)[1] == \'.cu\':\n            # use the cuda for .cu files\n            self.set_executable(\'compiler_so\', \'nvcc\')\n            # use only a subset of the extra_postargs, which are 1-1 translated\n            # from the extra_compile_args in the Extension class\n            postargs = extra_postargs[\'nvcc\']\n        else:\n            postargs = extra_postargs[\'cc\']\n\n        super(obj, src, ext, cc_args, postargs, pp_opts)\n        # reset the default compiler_so, which we might have changed for cuda\n        self.compiler_so = default_compiler_so\n\n    # inject our redefined _compile method into the class\n    self._compile = _compile\n\n\nclass custom_build_ext(build_ext):\n\n    def build_extensions(self):\n        customize_compiler_for_nvcc(self.compiler)\n        build_ext.build_extensions(self)\n\n\nsetup(\n    name=\'soft_nms\',\n    cmdclass={\'build_ext\': custom_build_ext},\n    ext_modules=cythonize(extensions),\n)\n\nsetup(\n    name=\'nms_cuda\',\n    ext_modules=[\n        CUDAExtension(\'nms_cuda\', [\n            \'src/nms_cuda.cpp\',\n            \'src/nms_kernel.cu\',\n        ]),\n        CUDAExtension(\'nms_cpu\', [\n            \'src/nms_cpu.cpp\',\n        ]),\n    ],\n    cmdclass={\'build_ext\': BuildExtension})\n'"
mmaction/ops/resample2d_package/__init__.py,0,b'\n'
mmaction/ops/resample2d_package/resample2d.py,2,"b'from torch.nn.modules.module import Module\nfrom torch.autograd import Function, Variable\nimport resample2d_cuda\n\n\nclass Resample2dFunction(Function):\n\n    @staticmethod\n    def forward(ctx, input1, input2, kernel_size=1):\n        assert input1.is_contiguous()\n        assert input2.is_contiguous()\n\n        ctx.save_for_backward(input1, input2)\n        ctx.kernel_size = kernel_size\n\n        _, d, _, _ = input1.size()\n        b, _, h, w = input2.size()\n        output = input1.new(b, d, h, w).zero_()\n\n        resample2d_cuda.forward(input1, input2, output, kernel_size)\n\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        grad_output = grad_output.contiguous()\n        assert grad_output.is_contiguous()\n\n        input1, input2 = ctx.saved_tensors\n\n        grad_input1 = Variable(input1.new(input1.size()).zero_())\n        grad_input2 = Variable(input1.new(input2.size()).zero_())\n\n        resample2d_cuda.backward(input1, input2, grad_output.data,\n                                 grad_input1.data, grad_input2.data,\n                                 ctx.kernel_size)\n\n        return grad_input1, grad_input2, None\n\n\nclass Resample2d(Module):\n\n    def __init__(self, kernel_size=1):\n        super(Resample2d, self).__init__()\n        self.kernel_size = kernel_size\n\n    def forward(self, input1, input2):\n        input1_c = input1.contiguous()\n        return Resample2dFunction.apply(input1_c, input2, self.kernel_size)\n'"
mmaction/ops/resample2d_package/setup.py,1,"b""#!/usr/bin/env python3\nimport os\nimport torch\n\nfrom setuptools import setup\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\ncxx_args = ['-std=c++11']\n\nnvcc_args = [\n    '-gencode', 'arch=compute_50,code=sm_50',\n    '-gencode', 'arch=compute_52,code=sm_52',\n    '-gencode', 'arch=compute_60,code=sm_60',\n    '-gencode', 'arch=compute_61,code=sm_61',\n    '-gencode', 'arch=compute_70,code=sm_70',\n    '-gencode', 'arch=compute_70,code=compute_70'\n]\n\nsetup(\n    name='resample2d_cuda',\n    ext_modules=[\n        CUDAExtension('resample2d_cuda', [\n            'resample2d_cuda.cc',\n            'resample2d_kernel.cu'\n        ], extra_compile_args={'cxx': cxx_args, 'nvcc': nvcc_args})\n    ],\n    cmdclass={\n        'build_ext': BuildExtension\n    })\n"""
mmaction/ops/roi_align/__init__.py,0,"b""from .functions.roi_align import roi_align\nfrom .modules.roi_align import RoIAlign\n\n__all__ = ['roi_align', 'RoIAlign']\n"""
mmaction/ops/roi_align/gradcheck.py,3,"b""import numpy as np\nimport torch\nfrom torch.autograd import gradcheck\n\nimport os.path as osp\nimport sys\nsys.path.append(osp.abspath(osp.join(__file__, '../../')))\nfrom roi_align import RoIAlign  # noqa: E402\n\nfeat_size = 15\nspatial_scale = 1.0 / 8\nimg_size = feat_size / spatial_scale\nnum_imgs = 2\nnum_rois = 20\n\nbatch_ind = np.random.randint(num_imgs, size=(num_rois, 1))\nrois = np.random.rand(num_rois, 4) * img_size * 0.5\nrois[:, 2:] += img_size * 0.5\nrois = np.hstack((batch_ind, rois))\n\nfeat = torch.randn(\n    num_imgs, 16, feat_size, feat_size, requires_grad=True, device='cuda:0')\nrois = torch.from_numpy(rois).float().cuda()\ninputs = (feat, rois)\nprint('Gradcheck for roi align...')\ntest = gradcheck(RoIAlign(3, spatial_scale), inputs, atol=1e-3, eps=1e-3)\nprint(test)\ntest = gradcheck(RoIAlign(3, spatial_scale, 2), inputs, atol=1e-3, eps=1e-3)\nprint(test)\n"""
mmaction/ops/roi_align/setup.py,1,"b""from setuptools import setup\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\nsetup(\n    name='roi_align_cuda',\n    ext_modules=[\n        CUDAExtension('roi_align_cuda', [\n            'src/roi_align_cuda.cpp',\n            'src/roi_align_kernel.cu',\n        ]),\n    ],\n    cmdclass={'build_ext': BuildExtension})\n"""
mmaction/ops/roi_pool/__init__.py,0,"b""from .functions.roi_pool import roi_pool\nfrom .modules.roi_pool import RoIPool\n\n__all__ = ['roi_pool', 'RoIPool']\n"""
mmaction/ops/roi_pool/gradcheck.py,3,"b""import torch\nfrom torch.autograd import gradcheck\n\nimport os.path as osp\nimport sys\nsys.path.append(osp.abspath(osp.join(__file__, '../../')))\nfrom roi_pool import RoIPool  # noqa: E402\n\nfeat = torch.randn(4, 16, 15, 15, requires_grad=True).cuda()\nrois = torch.Tensor([[0, 0, 0, 50, 50], [0, 10, 30, 43, 55],\n                     [1, 67, 40, 110, 120]]).cuda()\ninputs = (feat, rois)\nprint('Gradcheck for roi pooling...')\ntest = gradcheck(RoIPool(4, 1.0 / 8), inputs, eps=1e-5, atol=1e-3)\nprint(test)\n"""
mmaction/ops/roi_pool/setup.py,1,"b""from setuptools import setup\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\nsetup(\n    name='roi_pool',\n    ext_modules=[\n        CUDAExtension('roi_pool_cuda', [\n            'src/roi_pool_cuda.cpp',\n            'src/roi_pool_kernel.cu',\n        ])\n    ],\n    cmdclass={'build_ext': BuildExtension})\n"""
mmaction/ops/trajectory_conv_package/__init__.py,0,b''
mmaction/ops/trajectory_conv_package/gradcheck.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import gradcheck\n\nfrom traj_conv import TrajConv\n \nnum_deformable_groups = 2\n\nN, inC, inT, inH, inW = 2, 8, 8, 4, 4\noutC, outT, outH, outW = 4, 8, 4, 4\nkT, kH, kW = 3, 3, 3\n\nconv = nn.Conv3d(inC, num_deformable_groups * 3 * kT * kH * kW,\n                 kernel_size=(kT, kH, kW),\n                 stride=(1,1,1),\n                 padding=(1,1,1),\n                 bias=False)\n\nconv_offset3d = TrajConv(inC, outC, (kT, kH, kW),\n                         stride=(1,1,1), padding=(1,1,1),\n                         num_deformable_groups=num_deformable_groups).double().cuda()\n \ninput = torch.randn(N, inC, inT, inH, inW, requires_grad=True).double().cuda()\noffset = torch.rand(N, num_deformable_groups * 2 * kT * kH * kW, inT, inH, inW, requires_grad=True) * 1 - 0.5\noffset = offset.double().cuda()\ntest = gradcheck(conv_offset3d, (input, offset), eps=1e-5, atol=1e-1, rtol=1e-5)\nprint(test)\n'"
mmaction/ops/trajectory_conv_package/setup.py,1,"b""#!/usr/bin/env python3\nimport os\nimport torch\n\nfrom setuptools import setup, find_packages\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\ncxx_args = ['-std=c++11']\n\nnvcc_args = [\n    '-gencode', 'arch=compute_50,code=sm_50',\n    '-gencode', 'arch=compute_52,code=sm_52',\n    '-gencode', 'arch=compute_60,code=sm_60',\n    '-gencode', 'arch=compute_61,code=sm_61',\n    '-gencode', 'arch=compute_70,code=sm_70',\n    '-gencode', 'arch=compute_70,code=compute_70',\n]\n\nsetup(\n    name='traj_conv_cuda',\n    ext_modules=[\n        CUDAExtension('traj_conv_cuda', [\n            'traj_conv_cuda.cpp',\n            'deform_3d_conv_cuda_kernel.cu',\n        ], extra_compile_args={'cxx': cxx_args, 'nvcc': nvcc_args})\n    ],\n    cmdclass={\n        'build_ext': BuildExtension\n    })\n"""
mmaction/ops/trajectory_conv_package/traj_conv.py,17,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd import Function\nfrom torch.nn.modules.module import Module\nfrom torch.nn.modules.utils import _triple\n\nimport math\n\nimport traj_conv_cuda\n\nclass TrajConvFunction(Function):\n\n    @staticmethod\n    def forward(ctx,\n                input,\n                offset,\n                weight,\n                bias,\n                stride=1,\n                padding=0,\n                dilation=1,\n                deformable_groups=1,\n                im2col_step=64):\n        if input is not None and input.dim() != 5:\n            raise ValueError(\n                ""Expected 5D tensor as input, got {}D tensor instead."".format(\n                    input.dim()))\n        ctx.stride = _triple(stride)\n        ctx.padding = _triple(padding)\n        ctx.dilation = _triple(dilation)\n        ctx.deformable_groups = deformable_groups\n        ctx.im2col_step = im2col_step\n\n        ctx.save_for_backward(input, offset, weight, bias)\n\n        output = input.new(*TrajConvFunction._output_size(\n            input, weight, ctx.padding, ctx.dilation, ctx.stride))\n\n        ctx.bufs_ = [input.new(), input.new()]  # columns, ones\n\n        if not input.is_cuda:\n            raise NotImplementedError\n        else:\n            if isinstance(input, torch.autograd.Variable):\n                if not (isinstance(input.data, torch.cuda.FloatTensor) or isinstance(input.data, torch.cuda.DoubleTensor)):\n                    raise NotImplementedError\n            else:\n                if not (isinstance(input, torch.cuda.FloatTensor) or isinstance(input, torch.cuda.DoubleTensor)):\n                    raise NotImplementedError\n\n            cur_im2col_step = min(ctx.im2col_step, input.shape[0])\n            assert (input.shape[0] %\n                    cur_im2col_step) == 0, \'im2col step must divide batchsize\'\n            traj_conv_cuda.deform_3d_conv_forward_cuda(\n                input, weight, bias, offset, output, ctx.bufs_[0], ctx.bufs_[1],\n                weight.size(2), weight.size(3), weight.size(4),\n                ctx.stride[0], ctx.stride[1], ctx.stride[2],\n                ctx.padding[0], ctx.padding[1], ctx.padding[2],\n                ctx.dilation[0], ctx.dilation[1], ctx.dilation[2], ctx.deformable_groups, cur_im2col_step)\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, offset, weight, bias = ctx.saved_tensors\n\n        grad_input = grad_offset = grad_weight = grad_bias = None\n\n        if not grad_output.is_cuda:\n            raise NotImplementedError\n        else:\n            if isinstance(grad_output, torch.autograd.Variable):\n                if not (isinstance(grad_output.data, torch.cuda.FloatTensor) or isinstance(grad_output.data, torch.cuda.DoubleTensor)):\n                    raise NotImplementedError\n            else:\n                if not (isinstance(grad_output, torch.cuda.FloatTensor) or isinstance(grad_output, torch.cuda.DoubleTensor)):\n                    raise NotImplementedError\n\n            cur_im2col_step = min(ctx.im2col_step, input.shape[0])\n            assert (input.shape[0] %\n                    cur_im2col_step) == 0, \'im2col step must divide batchsize\'\n\n            if ctx.needs_input_grad[0] or ctx.needs_input_grad[1]:\n                grad_input = torch.zeros_like(input)\n                grad_offset = torch.zeros_like(offset)\n                # print(""input.size: "", input.size())\n                # print(""offset.size: "", offset.size())\n                # print(""grad_output.size: "", grad_output.size())\n                # print(""grad_input.size: "", grad_input.size())\n                # print(""grad_offset.size: "", grad_offset.size())\n                traj_conv_cuda.deform_3d_conv_backward_input_cuda(\n                    input, offset, grad_output, grad_input,\n                    grad_offset, weight, bias, ctx.bufs_[0],\n                    weight.size(2), weight.size(3), weight.size(4),\n                    ctx.stride[0], ctx.stride[1], ctx.stride[2],\n                    ctx.padding[0], ctx.padding[1], ctx.padding[2],\n                    ctx.dilation[0], ctx.dilation[1], ctx.dilation[2],\n                    ctx.deformable_groups, cur_im2col_step)\n\n            if ctx.needs_input_grad[2]:\n                grad_weight = torch.zeros_like(weight)\n                grad_bias = torch.zeros_like(bias)\n                traj_conv_cuda.deform_3d_conv_backward_parameters_cuda(\n                    input, offset, grad_output,\n                    grad_weight, grad_bias, ctx.bufs_[0], ctx.bufs_[1], \n                    weight.size(2), weight.size(3), weight.size(4),\n                    ctx.stride[0], ctx.stride[1], ctx.stride[2],\n                    ctx.padding[0], ctx.padding[1], ctx.padding[2],\n                    ctx.dilation[0], ctx.dilation[1], ctx.dilation[2],\n                    ctx.deformable_groups, 1, cur_im2col_step)\n\n        return grad_input, grad_offset, grad_weight, grad_bias, None, None, None, None, None\n\n\n    @staticmethod\n    def _output_size(input, weight, padding, dilation, stride):\n        channels = weight.size(0)\n        output_size = (input.size(0), channels)\n        for d in range(input.dim() - 2):\n            in_size = input.size(d + 2)\n            pad = padding[d]\n            kernel = dilation[d] * (weight.size(d + 2) - 1) + 1\n            stride_ = stride[d]\n            output_size += ((in_size + (2 * pad) - kernel) // stride_ + 1, )\n        if not all(map(lambda s: s > 0, output_size)):\n            raise ValueError(\n                ""convolution input is too small (output would be {})"".format(\n                    \'x\'.join(map(str, output_size))))\n        return output_size\n\n\nclass TrajConv(Module):\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 num_deformable_groups=1,\n                 im2col_step=64,\n                 bias=True):\n        super(TrajConv, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = _triple(kernel_size)\n        self.stride = _triple(stride)\n        self.padding = _triple(padding)\n        self.dilation = _triple(dilation)\n        self.num_deformable_groups = num_deformable_groups\n        self.im2col_step = im2col_step\n\n        self.weight = nn.Parameter(\n            torch.Tensor(out_channels, in_channels, *self.kernel_size))\n        if bias:\n            self.bias = nn.Parameter(\n                torch.Tensor(out_channels,))\n        else:\n            self.bias = nn.Parameter(\n                torch.zeros(0,))\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        n = self.in_channels\n        for k in self.kernel_size:\n            n *= k\n        stdv = 1. / math.sqrt(n)\n        self.weight.data.uniform_(-stdv, stdv)\n        if self.bias.nelement() != 0:\n            self.bias.data.fill_(0.)\n\n    def forward(self, input, offset):\n        return TrajConvFunction.apply(input, offset, self.weight, self.bias, self.stride,\n                                      self.padding, self.dilation,\n                                      self.num_deformable_groups,\n                                      self.im2col_step)\n'"
test_configs/TSN/ucf101/tsn_flow_bninception.py,0,"b""# model settings\nmodel = dict(\n    type='TSN2D',\n    modality='Flow',\n    in_channels=10,\n    backbone=dict(\n        type='BNInception',\n        pretrained=None,\n        bn_eval=False,\n        partial_bn=True),\n    spatial_temporal_module=dict(\n        type='SimpleSpatialModule',\n        spatial_type='avg',\n        spatial_size=7),\n    segmental_consensus=dict(\n        type='SimpleConsensus',\n        consensus_type='avg'),\n    cls_head=dict(\n        type='ClsHead',\n        with_avg_pool=False,\n        temporal_feature_size=1,\n        spatial_feature_size=1,\n        dropout_ratio=0.7,\n        in_channels=1024,\n        num_classes=101))\ntrain_cfg = None\ntest_cfg = None\n# dataset settings\ndataset_type = 'RawFramesDataset'\ndata_root = 'data/ucf101/rawframes'\nimg_norm_cfg = dict(\n    mean=[128], std=[1], to_rgb=False)\ndata = dict(\n    test=dict(\n        type=dataset_type,\n        ann_file='data/ucf101/ucf101_val_split_1_rawframes.txt',\n        img_prefix=data_root,\n        img_norm_cfg=img_norm_cfg,\n        num_segments=25,\n        new_length=5,\n        new_step=1,\n        random_shift=False,\n        modality='Flow',\n        image_tmpl='flow_{}_{:05d}.jpg',\n        img_scale=256,\n        input_size=224,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample='ten_crop',\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=True))\n\ndist_params = dict(backend='nccl')\n"""
test_configs/TSN/ucf101/tsn_rgb_bninception.py,0,"b""# model settings\nmodel = dict(\n    type='TSN2D',\n    backbone=dict(\n        type='BNInception',\n        pretrained=None,\n        bn_eval=False,\n        partial_bn=True),\n    spatial_temporal_module=dict(\n        type='SimpleSpatialModule',\n        spatial_type='avg',\n        spatial_size=7),\n    segmental_consensus=dict(\n        type='SimpleConsensus',\n        consensus_type='avg'),\n    cls_head=dict(\n        type='ClsHead',\n        with_avg_pool=False,\n        temporal_feature_size=1,\n        spatial_feature_size=1,\n        dropout_ratio=0.8,\n        in_channels=1024,\n        init_std=0.001,\n        num_classes=101))\ntrain_cfg = None\ntest_cfg = None\n# dataset settings\ndataset_type = 'RawFramesDataset'\ndata_root = 'data/ucf101/rawframes'\nimg_norm_cfg = dict(\n   mean=[104, 117, 128], std=[1, 1, 1], to_rgb=False)\n\ndata = dict(\n    test=dict(\n        type=dataset_type,\n        ann_file='data/ucf101/ucf101_val_split_1_rawframes.txt',\n        img_prefix=data_root,\n        img_norm_cfg=img_norm_cfg,\n        num_segments=25,\n        new_length=1,\n        new_step=1,\n        random_shift=False,\n        modality='RGB',\n        image_tmpl='img_{:05d}.jpg',\n        img_scale=256,\n        input_size=224,\n        div_255=False,\n        flip_ratio=0,\n        resize_keep_ratio=True,\n        oversample='ten_crop',\n        random_crop=False,\n        more_fix_crop=False,\n        multiscale_crop=False,\n        test_mode=True))\n\ndist_params = dict(backend='nccl')\n"""
mmaction/core/bbox2d/assigners/__init__.py,0,"b""from .base_assigner import BaseAssigner\nfrom .max_iou_assigner import MaxIoUAssigner\nfrom .assign_result import AssignResult\n\n__all__ = ['BaseAssigner', 'MaxIoUAssigner', 'AssignResult']"""
mmaction/core/bbox2d/assigners/assign_result.py,5,"b'import torch\n\n\nclass AssignResult(object):\n\n    def __init__(self, num_gts, gt_inds, max_overlaps, labels=None):\n        self.num_gts = num_gts\n        self.gt_inds = gt_inds\n        self.max_overlaps = max_overlaps\n        self.labels = labels\n\n    def add_gt_(self, gt_labels):\n        self_inds = torch.arange(\n            1, len(gt_labels) + 1, dtype=torch.long, device=gt_labels.device)\n        self.gt_inds = torch.cat([self_inds, self.gt_inds])\n        self.max_overlaps = torch.cat(\n            [self.max_overlaps.new_ones(self.num_gts), self.max_overlaps])\n        if self.labels is not None:\n            self.labels = torch.cat([gt_labels, self.labels])\n'"
mmaction/core/bbox2d/assigners/base_assigner.py,0,"b'from abc import ABCMeta, abstractmethod\n\n\nclass BaseAssigner(metaclass=ABCMeta):\n\n    @abstractmethod\n    def assign(self, bboxes, gt_bboxes, gt_bboxes_ignore=None, gt_labels=None):\n        pass\n'"
mmaction/core/bbox2d/assigners/max_iou_assigner.py,2,"b'import torch\n\nfrom .base_assigner import BaseAssigner\nfrom .assign_result import AssignResult\nfrom ..geometry import bbox_overlaps\n\n\nclass MaxIoUAssigner(BaseAssigner):\n    """"""Assign a corresponding gt bbox or background to each bbox.\n\n    Each proposals will be assigned with `-1`, `0`, or a positive integer\n    indicating the ground truth index.\n\n    - -1: don\'t care\n    - 0: negative sample, no assigned gt\n    - positive integer: positive sample, index (1-based) of assigned gt\n\n    Args:\n        pos_iou_thr (float): IoU threshold for positive bboxes.\n        neg_iou_thr (float or tuple): IoU threshold for negative bboxes.\n        min_pos_iou (float): Minimum iou for a bbox to be considered as a\n            positive bbox. Positive samples can have smaller IoU than\n            pos_iou_thr due to the 4th step (assign max IoU sample to each gt).\n        gt_max_assign_all (bool): Whether to assign all bboxes with the same\n            highest overlap with some gt to that gt.\n        ignore_iof_thr (float): IoF threshold for ignoring bboxes (if\n            `gt_bboxes_ignore` is specified). Negative values mean not\n            ignoring any bboxes.\n        ignore_wrt_candidates (bool): Whether to compute the iof between\n            `bboxes` and `gt_bboxes_ignore`, or the contrary.\n    """"""\n\n    def __init__(self,\n                 pos_iou_thr,\n                 neg_iou_thr,\n                 min_pos_iou=.0,\n                 gt_max_assign_all=True,\n                 ignore_iof_thr=-1,\n                 ignore_wrt_candidates=True):\n        self.pos_iou_thr = pos_iou_thr\n        self.neg_iou_thr = neg_iou_thr\n        self.min_pos_iou = min_pos_iou\n        self.gt_max_assign_all = gt_max_assign_all\n        self.ignore_iof_thr = ignore_iof_thr\n        self.ignore_wrt_candidates = ignore_wrt_candidates\n\n    def assign(self, bboxes, gt_bboxes, gt_bboxes_ignore=None, gt_labels=None):\n        """"""Assign gt to bboxes.\n\n        This method assign a gt bbox to every bbox (proposal/anchor), each bbox\n        will be assigned with -1, 0, or a positive number. -1 means don\'t care,\n        0 means negative sample, positive number is the index (1-based) of\n        assigned gt.\n        The assignment is done in following steps, the order matters.\n\n        1. assign every bbox to -1\n        2. assign proposals whose iou with all gts < neg_iou_thr to 0\n        3. for each bbox, if the iou with its nearest gt >= pos_iou_thr,\n           assign it to that bbox\n        4. for each gt bbox, assign its nearest proposals (may be more than\n           one) to itself\n\n        Args:\n            bboxes (Tensor): Bounding boxes to be assigned, shape(n, 4).\n            gt_bboxes (Tensor): Groundtruth boxes, shape (k, 4).\n            gt_bboxes_ignore (Tensor, optional): Ground truth bboxes that are\n                labelled as `ignored`, e.g., crowd boxes in COCO.\n            gt_labels (Tensor, optional): Label of gt_bboxes, shape (k, ).\n\n        Returns:\n            :obj:`AssignResult`: The assign result.\n        """"""\n        if bboxes.shape[0] == 0 or gt_bboxes.shape[0] == 0:\n            raise ValueError(\'No gt or bboxes\')\n        bboxes = bboxes[:, :4]\n        overlaps = bbox_overlaps(gt_bboxes, bboxes)\n\n        if (self.ignore_iof_thr > 0) and (gt_bboxes_ignore is not None) and (\n                gt_bboxes_ignore.numel() > 0):\n            if self.ignore_wrt_candidates:\n                ignore_overlaps = bbox_overlaps(\n                    bboxes, gt_bboxes_ignore, mode=\'iof\')\n                ignore_max_overlaps, _ = ignore_overlaps.max(dim=1)\n            else:\n                ignore_overlaps = bbox_overlaps(\n                    gt_bboxes_ignore, bboxes, mode=\'iof\')\n                ignore_max_overlaps, _ = ignore_overlaps.max(dim=0)\n            overlaps[:, ignore_max_overlaps > self.ignore_iof_thr] = -1\n\n        assign_result = self.assign_wrt_overlaps(overlaps, gt_labels)\n        return assign_result\n\n    def assign_wrt_overlaps(self, overlaps, gt_labels=None):\n        """"""Assign w.r.t. the overlaps of bboxes with gts.\n\n        Args:\n            overlaps (Tensor): Overlaps between k gt_bboxes and n bboxes,\n                shape(k, n).\n            gt_labels (Tensor, optional): Labels of k gt_bboxes, shape (k, ).\n\n        Returns:\n            :obj:`AssignResult`: The assign result.\n        """"""\n        if overlaps.numel() == 0:\n            raise ValueError(\'No gt or proposals\')\n\n        num_gts, num_bboxes = overlaps.size(0), overlaps.size(1)\n\n        # 1. assign -1 by default\n        assigned_gt_inds = overlaps.new_full(\n            (num_bboxes, ), -1, dtype=torch.long)\n\n        # for each anchor, which gt best overlaps with it\n        # for each anchor, the max iou of all gts\n        max_overlaps, argmax_overlaps = overlaps.max(dim=0)\n        # for each gt, which anchor best overlaps with it\n        # for each gt, the max iou of all proposals\n        gt_max_overlaps, gt_argmax_overlaps = overlaps.max(dim=1)\n\n        # 2. assign negative: below\n        if isinstance(self.neg_iou_thr, float):\n            assigned_gt_inds[(max_overlaps >= 0)\n                             & (max_overlaps < self.neg_iou_thr)] = 0\n        elif isinstance(self.neg_iou_thr, tuple):\n            assert len(self.neg_iou_thr) == 2\n            assigned_gt_inds[(max_overlaps >= self.neg_iou_thr[0])\n                             & (max_overlaps < self.neg_iou_thr[1])] = 0\n\n        # 3. assign positive: above positive IoU threshold\n        pos_inds = max_overlaps >= self.pos_iou_thr\n        assigned_gt_inds[pos_inds] = argmax_overlaps[pos_inds] + 1\n\n        # 4. assign fg: for each gt, proposals with highest IoU\n        for i in range(num_gts):\n            if gt_max_overlaps[i] >= self.min_pos_iou:\n                if self.gt_max_assign_all:\n                    max_iou_inds = overlaps[i, :] == gt_max_overlaps[i]\n                    assigned_gt_inds[max_iou_inds] = i + 1\n                else:\n                    assigned_gt_inds[gt_argmax_overlaps[i]] = i + 1\n\n        if gt_labels is not None:\n            # consider multi-class case (AVA)\n            if len(gt_labels[0]) == 1:\n                assigned_labels = assigned_gt_inds.new_zeros((num_bboxes, ))\n            else:\n                assigned_labels = assigned_gt_inds.new_zeros(\n                    (num_bboxes, len(gt_labels[0])))\n            pos_inds = torch.nonzero(assigned_gt_inds > 0).squeeze()\n            if pos_inds.numel() > 0:\n                assigned_labels[pos_inds] = gt_labels[\n                    assigned_gt_inds[pos_inds] - 1]\n        else:\n            assigned_labels = None\n\n        return AssignResult(\n            num_gts, assigned_gt_inds, max_overlaps, labels=assigned_labels)\n'"
mmaction/core/bbox2d/samplers/__init__.py,0,"b""from .base_sampler import BaseSampler\nfrom .pseudo_sampler import PseudoSampler\nfrom .random_sampler import RandomSampler\nfrom .sampling_result import SamplingResult\n\n__all__ = [\n    'BaseSampler', 'PseudoSampler', 'RandomSampler'\n    'SamplingResult'\n]"""
mmaction/core/bbox2d/samplers/base_sampler.py,4,"b'from abc import ABCMeta, abstractmethod\n\nimport torch\n\nfrom .sampling_result import SamplingResult\n\n\nclass BaseSampler(metaclass=ABCMeta):\n\n    def __init__(self,\n                 num,\n                 pos_fraction,\n                 neg_pos_ub=-1,\n                 add_gt_as_proposals=True,\n                 **kwargs):\n        self.num = num\n        self.pos_fraction = pos_fraction\n        self.neg_pos_ub = neg_pos_ub\n        self.add_gt_as_proposals = add_gt_as_proposals\n        self.pos_sampler = self\n        self.neg_sampler = self\n\n    @abstractmethod\n    def _sample_pos(self, assign_result, num_expected, **kwargs):\n        pass\n\n    @abstractmethod\n    def _sample_neg(self, assign_result, num_expected, **kwargs):\n        pass\n\n    def sample(self,\n               assign_result,\n               bboxes,\n               gt_bboxes,\n               gt_labels=None,\n               **kwargs):\n        """"""Sample positive and negative bboxes.\n        This is a simple implementation of bbox sampling given candidates,\n        assigning results and ground truth bboxes.\n        Args:\n            assign_result (:obj:`AssignResult`): Bbox assigning results.\n            bboxes (Tensor): Boxes to be sampled from.\n            gt_bboxes (Tensor): Ground truth bboxes.\n            gt_labels (Tensor, optional): Class labels of ground truth bboxes.\n        Returns:\n            :obj:`SamplingResult`: Sampling result.\n        """"""\n        bboxes = bboxes[:, :4]\n\n        gt_flags = bboxes.new_zeros((bboxes.shape[0], ), dtype=torch.uint8)\n        if self.add_gt_as_proposals:\n            bboxes = torch.cat([gt_bboxes, bboxes], dim=0)\n            assign_result.add_gt_(gt_labels)\n            gt_ones = bboxes.new_ones(gt_bboxes.shape[0], dtype=torch.uint8)\n            gt_flags = torch.cat([gt_ones, gt_flags])\n\n        num_expected_pos = int(self.num * self.pos_fraction)\n        pos_inds = self.pos_sampler._sample_pos(\n            assign_result, num_expected_pos, bboxes=bboxes, **kwargs)\n        # We found that sampled indices have duplicated items occasionally.\n        # (may be a bug of PyTorch)\n        pos_inds = pos_inds.unique()\n        num_sampled_pos = pos_inds.numel()\n        num_expected_neg = self.num - num_sampled_pos\n        if self.neg_pos_ub >= 0:\n            _pos = max(1, num_sampled_pos)\n            neg_upper_bound = int(self.neg_pos_ub * _pos)\n            if num_expected_neg > neg_upper_bound:\n                num_expected_neg = neg_upper_bound\n        neg_inds = self.neg_sampler._sample_neg(\n            assign_result, num_expected_neg, bboxes=bboxes, **kwargs)\n        neg_inds = neg_inds.unique()\n\n        return SamplingResult(pos_inds, neg_inds, bboxes, gt_bboxes,\n                              assign_result, gt_flags)\n'"
mmaction/core/bbox2d/samplers/pseudo_sampler.py,3,"b'import torch\n\nfrom .base_sampler import BaseSampler\nfrom .sampling_result import SamplingResult\n\n\nclass PseudoSampler(BaseSampler):\n\n    def __init__(self, **kwargs):\n        pass\n\n    def _sample_pos(self, **kwargs):\n        raise NotImplementedError\n\n    def _sample_neg(self, **kwargs):\n        raise NotImplementedError\n\n    def sample(self, assign_result, bboxes, gt_bboxes, **kwargs):\n        pos_inds = torch.nonzero(\n            assign_result.gt_inds > 0).squeeze(-1).unique()\n        neg_inds = torch.nonzero(\n            assign_result.gt_inds == 0).squeeze(-1).unique()\n        gt_flags = bboxes.new_zeros(bboxes.shape[0], dtype=torch.uint8)\n        sampling_result = SamplingResult(pos_inds, neg_inds, bboxes, gt_bboxes,\n                                         assign_result, gt_flags)\n        return sampling_result\n'"
mmaction/core/bbox2d/samplers/random_sampler.py,3,"b'import numpy as np\nimport torch\n\nfrom .base_sampler import BaseSampler\n\n\nclass RandomSampler(BaseSampler):\n\n    def __init__(self,\n                 num,\n                 pos_fraction,\n                 neg_pos_ub=-1,\n                 add_gt_as_proposals=True,\n                 **kwargs):\n        super(RandomSampler, self).__init__(num, pos_fraction, neg_pos_ub,\n                                            add_gt_as_proposals)\n\n    @staticmethod\n    def random_choice(gallery, num):\n        """"""Random select some elements from the gallery.\n        It seems that Pytorch\'s implementation is slower than numpy so we use\n        numpy to randperm the indices.\n        """"""\n        assert len(gallery) >= num\n        if isinstance(gallery, list):\n            gallery = np.array(gallery)\n        cands = np.arange(len(gallery))\n        np.random.shuffle(cands)\n        rand_inds = cands[:num]\n        if not isinstance(gallery, np.ndarray):\n            rand_inds = torch.from_numpy(rand_inds).long().to(gallery.device)\n        return gallery[rand_inds]\n\n    def _sample_pos(self, assign_result, num_expected, **kwargs):\n        """"""Randomly sample some positive samples.""""""\n        pos_inds = torch.nonzero(assign_result.gt_inds > 0)\n        if pos_inds.numel() != 0:\n            pos_inds = pos_inds.squeeze(1)\n        if pos_inds.numel() <= num_expected:\n            return pos_inds\n        else:\n            return self.random_choice(pos_inds, num_expected)\n\n    def _sample_neg(self, assign_result, num_expected, **kwargs):\n        """"""Randomly sample some negative samples.""""""\n        neg_inds = torch.nonzero(assign_result.gt_inds == 0)\n        if neg_inds.numel() != 0:\n            neg_inds = neg_inds.squeeze(1)\n        if len(neg_inds) <= num_expected:\n            return neg_inds\n        else:\n            return self.random_choice(neg_inds, num_expected)\n'"
mmaction/core/bbox2d/samplers/sampling_result.py,1,"b'import torch\n\n\nclass SamplingResult(object):\n\n    def __init__(self, pos_inds, neg_inds, bboxes, gt_bboxes, assign_result,\n                 gt_flags):\n        self.pos_inds = pos_inds\n        self.neg_inds = neg_inds\n        self.pos_bboxes = bboxes[pos_inds]\n        self.neg_bboxes = bboxes[neg_inds]\n        self.pos_is_gt = gt_flags[pos_inds]\n\n        self.num_gts = gt_bboxes.shape[0]\n        self.pos_assigned_gt_inds = assign_result.gt_inds[pos_inds] - 1\n        self.pos_gt_bboxes = gt_bboxes[self.pos_assigned_gt_inds, :]\n        if assign_result.labels is not None:\n            self.pos_gt_labels = assign_result.labels[pos_inds]\n        else:\n            self.pos_gt_labels = None\n\n    @property\n    def bboxes(self):\n        return torch.cat([self.pos_bboxes, self.neg_bboxes])\n'"
mmaction/models/tenons/anchor_heads/__init__.py,0,"b""from .anchor_head import AnchorHead\nfrom .rpn_head import RPNHead\n\n__all__ = ['AnchorHead', 'RPNHead']"""
mmaction/models/tenons/anchor_heads/anchor_head.py,4,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nfrom mmcv.cnn import normal_init\n\n\nfrom mmaction.core.anchor2d import AnchorGenerator, anchor_target\nfrom mmaction.core.bbox2d import delta2bbox\nfrom mmaction.losses import weighted_cross_entropy, weighted_smoothl1, weighted_binary_cross_entropy\nfrom mmaction.utils.misc import multi_apply\nfrom ...registry import HEADS\n\n@HEADS.register_module\nclass AnchorHead(nn.Module):\n    """"""Anchor-based head (RPN, etc.).\n\n    """"""\n\n    def __init__(self,\n                 num_classes,\n                 in_channels,\n                 feat_channels=256,\n                 anchor_scales=[8, 16, 32],\n                 anchor_ratios=[0.5, 1.0, 2.0],\n                 anchor_strides=[4, 8, 16, 32, 64],\n                 anchor_base_sizes=None,\n                 target_means=(.0, .0, .0, .0),\n                 target_stds=(1., 1., 1., 1.),\n                 use_sigmoid_cls=False,\n                 use_focal_loss=False):\n        super(AnchorHead, self).__init__()\n        self.in_channels = in_channels\n        self.num_classes = num_classes\n        self.feat_channels = feat_channels\n        self.anchor_scales = anchor_scales\n        self.anchor_ratios = anchor_ratios\n        self.anchor_strides = anchor_strides\n        self.anchor_base_sizes = list(\n            anchor_strides) if anchor_base_sizes is None else anchor_base_sizes\n        self.target_means = target_means\n        self.target_stds = target_stds\n        self.use_sigmoid_cls = use_sigmoid_cls\n        self.use_focal_loss = use_focal_loss\n\n        self.anchor_generators = []\n        for anchor_base in self.anchor_base_sizes:\n            self.anchor_generators.append(\n                AnchorGenerator(anchor_base, anchor_scales, anchor_ratios))\n\n        self.num_anchors = len(self.anchor_ratios) * len(self.anchor_scales)\n        if self.use_sigmoid_cls:\n            self.cls_out_channels = self.num_classes - 1\n        else:\n            self.cls_out_channels = self.num_classes\n\n        self._init_layers()\n\n    def _init_layers(self):\n        self.conv_cls = nn.Conv2d(self.feat_channels,\n                                  self.num_anchors * self.cls_out_channels, 1)\n        self.conv_reg = nn.Conv2d(self.feat_channels, self.num_anchors * 4, 1)\n\n    def init_weights(self):\n        normal_init(self.conv_cls, std=0.01)\n        normal_init(self.conv_reg, std=0.01)\n\n    def forward_single(self, x):\n        cls_score = self.conv_cls(x)\n        bbox_pred = self.conv_reg(x)\n        return cls_score, bbox_pred\n\n    def forward(self, feats):\n        return multi_apply(self.forward_single, feats)\n\n    def get_anchors(self, featmap_sizes, img_metas):\n        """"""Get anchors according to feature map sizes.\n        \n        """"""\n        num_imgs = len(img_metas)\n        num_levels = len(featmap_sizes)\n\n        # since feature map sizes of all images are the same, we only compute\n        # anchors for one time\n        multi_level_anchors = []\n        for i in range(num_levels):\n            anchors = self.anchor_generators[i].grid_anchors(\n                featmap_sizes[i], self.anchor_strides[i])\n            multi_level_anchors.append(anchors)\n        anchor_list = [multi_level_anchors for _ in range(num_imgs)]\n\n        # for each image, we compute valid flags of multi level anchors\n        valid_flag_list = []\n        for img_id, img_meta in enumerate(img_metas):\n            multi_level_flags = []\n            for i in range(num_levels):\n                anchor_stride = self.anchor_strides[i]\n                feat_h, feat_w = featmap_sizes[i]\n                h, w, _ = img_meta[\'pad_shape\']\n                valid_feat_h = min(int(np.ceil(h / anchor_stride)), feat_h)\n                valid_feat_w = min(int(np.ceil(w / anchor_stride)), feat_w)\n                flags = self.anchor_generators[i].valid_flags(\n                    (feat_h, feat_w), (valid_feat_h, valid_feat_w))\n                multi_level_flags.append(flags)\n            valid_flag_list.append(multi_level_flags)\n\n        return anchor_list, valid_flag_list\n\n    def loss_single(self, cls_score, bbox_pred, labels, label_weights,\n                    bbox_targets, bbox_weights, num_total_samples, cfg):\n        # classification loss\n        labels = labels.reshape(-1)\n        label_weights = label_weights.reshape(-1)\n        cls_score = cls_score.permute(0, 2, 3, 1).reshape(\n            -1, self.cls_out_channels)\n        if self.use_sigmoid_cls:\n            if self.use_focal_loss:\n                # TODO\n                raise NotImplementedError\n            else:\n                cls_criterion = weighted_binary_cross_entropy\n        else:\n            if self.use_focal_loss:\n                raise NotImplementedError\n            else:\n                cls_criterion = weighted_cross_entropy\n        if self.use_focal_loss:\n            # TODO\n            raise NotImplementedError\n        else:\n            loss_cls = cls_criterion(\n                cls_score, labels, label_weights, avg_factor=num_total_samples)\n        # regression loss\n        bbox_targets = bbox_targets.reshape(-1, 4)\n        bbox_weights = bbox_weights.reshape(-1, 4)\n        bbox_pred = bbox_pred.permute(0, 2, 3, 1).reshape(-1, 4)\n        loss_reg = weighted_smoothl1(\n            bbox_pred,\n            bbox_targets,\n            bbox_weights,\n            beta=cfg.smoothl1_beta,\n            avg_factor=num_total_samples)\n        return loss_cls, loss_reg\n\n    def loss(self,\n             cls_scores,\n             bbox_preds,\n             gt_bboxes,\n             gt_labels,\n             img_metas,\n             cfg,\n             gt_bboxes_ignore=None):\n        featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n        assert len(featmap_sizes) == len(self.anchor_generators)\n\n        anchor_list, valid_flag_list = self.get_anchors(\n            featmap_sizes, img_metas)\n        sampling = False if self.use_focal_loss else True\n        label_channels = self.cls_out_channels if self.use_sigmoid_cls else 1\n        cls_reg_targets = anchor_target(\n            anchor_list,\n            valid_flag_list,\n            gt_bboxes,\n            img_metas,\n            self.target_means,\n            self.target_stds,\n            cfg,\n            gt_bboxes_ignore_list=gt_bboxes_ignore,\n            gt_labels_list=gt_labels,\n            label_channels=label_channels,\n            sampling=sampling)\n        if cls_reg_targets is None:\n            return None\n        (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list,\n         num_total_pos, num_total_neg) = cls_reg_targets\n        num_total_samples = (num_total_pos if self.use_focal_loss else\n                             num_total_pos + num_total_neg)\n        losses_cls, losses_reg = multi_apply(\n            self.loss_single,\n            cls_scores,\n            bbox_preds,\n            labels_list,\n            label_weights_list,\n            bbox_targets_list,\n            bbox_weights_list,\n            num_total_samples=num_total_samples,\n            cfg=cfg)\n        return dict(loss_cls=losses_cls, loss_reg=losses_reg)\n\n    def get_bboxes(self, cls_scores, bbox_preds, img_metas, cfg,\n                   rescale=False):\n        assert len(cls_scores) == len(bbox_preds)\n        num_levels = len(cls_scores)\n\n        mlvl_anchors = [\n            self.anchor_generators[i].grid_anchors(cls_scores[i].size()[-2:],\n                                                   self.anchor_strides[i])\n            for i in range(num_levels)\n        ]\n        result_list = []\n        for img_id in range(len(img_metas)):\n            cls_score_list = [\n                cls_scores[i][img_id].detach() for i in range(num_levels)\n            ]\n            bbox_pred_list = [\n                bbox_preds[i][img_id].detach() for i in range(num_levels)\n            ]\n            img_shape = img_metas[img_id][\'img_shape\']\n            scale_factor = img_metas[img_id][\'scale_factor\']\n            proposals = self.get_bboxes_single(cls_score_list, bbox_pred_list,\n                                               mlvl_anchors, img_shape,\n                                               scale_factor, cfg, rescale)\n            result_list.append(proposals)\n        return result_list\n\n    def get_bboxes_single(self,\n                          cls_scores,\n                          bbox_preds,\n                          mlvl_anchors,\n                          img_shape,\n                          scale_factor,\n                          cfg,\n                          rescale=False):\n        assert len(cls_scores) == len(bbox_preds) == len(mlvl_anchors)\n        mlvl_bboxes = []\n        mlvl_scores = []\n        for cls_score, bbox_pred, anchors in zip(cls_scores, bbox_preds,\n                                                 mlvl_anchors):\n            assert cls_score.size()[-2:] == bbox_pred.size()[-2:]\n            cls_score = cls_score.permute(1, 2, 0).reshape(\n                -1, self.cls_out_channels)\n            if self.use_sigmoid_cls:\n                scores = cls_score.sigmoid()\n            else:\n                scores = cls_score.softmax(-1)\n            bbox_pred = bbox_pred.permute(1, 2, 0).reshape(-1, 4)\n            nms_pre = cfg.get(\'nms_pre\', -1)\n            if nms_pre > 0 and scores.shape[0] > nms_pre:\n                if self.use_sigmoid_cls:\n                    max_scores, _ = scores.max(dim=1)\n                else:\n                    max_scores, _ = scores[:, 1:].max(dim=1)\n                _, topk_inds = max_scores.topk(nms_pre)\n                anchors = anchors[topk_inds, :]\n                bbox_pred = bbox_pred[topk_inds, :]\n                scores = scores[topk_inds, :]\n            bboxes = delta2bbox(anchors, bbox_pred, self.target_means,\n                                self.target_stds, img_shape)\n            mlvl_bboxes.append(bboxes)\n            mlvl_scores.append(scores)\n        mlvl_bboxes = torch.cat(mlvl_bboxes)\n        if rescale:\n            mlvl_bboxes /= mlvl_bboxes.new_tensor(scale_factor)\n        mlvl_scores = torch.cat(mlvl_scores)\n        if self.use_sigmoid_cls:\n            padding = mlvl_scores.new_zeros(mlvl_scores.shape[0], 1)\n            mlvl_scores = torch.cat([padding, mlvl_scores], dim=1)\n        det_bboxes, det_labels = multiclass_nms(\n            mlvl_bboxes, mlvl_scores, cfg.score_thr, cfg.nms, cfg.max_per_img)\n        return det_bboxes, det_labels\n'"
mmaction/models/tenons/anchor_heads/rpn_head.py,5,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import normal_init\n\nfrom mmaction.core.bbox2d import delta2bbox\nfrom mmaction.ops import nms\nfrom .anchor_head import AnchorHead\nfrom ...registry import HEADS\n\n@HEADS.register_module\nclass RPNHead(AnchorHead):\n\n    def __init__(self, in_channels, **kwargs):\n        super(RPNHead, self).__init__(2, in_channels, **kwargs)\n\n    def _init_layers(self):\n        self.rpn_conv = nn.Conv2d(\n            self.in_channels, self.feat_channels, 3, padding=1)\n        self.rpn_cls = nn.Conv2d(self.feat_channels,\n                                 self.num_anchors * self.cls_out_channels, 1)\n        self.rpn_reg = nn.Conv2d(self.feat_channels, self.num_anchors * 4, 1)\n\n    def init_weights(self):\n        normal_init(self.rpn_conv, std=0.01)\n        normal_init(self.rpn_cls, std=0.01)\n        normal_init(self.rpn_reg, std=0.01)\n\n    def forward_single(self, x):\n        x = self.rpn_conv(x)\n        x = F.relu(x, inplace=True)\n        rpn_cls_score = self.rpn_cls(x)\n        rpn_bbox_pred = self.rpn_reg(x)\n        return rpn_cls_score, rpn_bbox_pred\n\n    def loss(self,\n             cls_scores,\n             bbox_preds,\n             gt_bboxes,\n             img_metas,\n             cfg,\n             gt_bboxes_ignore=None):\n        losses = super(RPNHead, self).loss(\n            cls_scores,\n            bbox_preds,\n            gt_bboxes,\n            None,\n            img_metas,\n            cfg,\n            gt_bboxes_ignore=gt_bboxes_ignore)\n        return dict(\n            loss_rpn_cls=losses['loss_cls'], loss_rpn_reg=losses['loss_reg'])\n\n    def get_bboxes_single(self,\n                          cls_scores,\n                          bbox_preds,\n                          mlvl_anchors,\n                          img_shape,\n                          scale_factor,\n                          cfg,\n                          rescale=False):\n        mlvl_proposals = []\n        for idx in range(len(cls_scores)):\n            rpn_cls_score = cls_scores[idx]\n            rpn_bbox_pred = bbox_preds[idx]\n            assert rpn_cls_score.size()[-2:] == rpn_bbox_pred.size()[-2:]\n            anchors = mlvl_anchors[idx]\n            rpn_cls_score = rpn_cls_score.permute(1, 2, 0)\n            if self.use_sigmoid_cls:\n                rpn_cls_score = rpn_cls_score.reshape(-1)\n                scores = rpn_cls_score.sigmoid()\n            else:\n                rpn_cls_score = rpn_cls_score.reshape(-1, 2)\n                scores = rpn_cls_score.softmax(dim=1)[:, 1]\n            rpn_bbox_pred = rpn_bbox_pred.permute(1, 2, 0).reshape(-1, 4)\n            if cfg.nms_pre > 0 and scores.shape[0] > cfg.nms_pre:\n                _, topk_inds = scores.topk(cfg.nms_pre)\n                rpn_bbox_pred = rpn_bbox_pred[topk_inds, :]\n                anchors = anchors[topk_inds, :]\n                scores = scores[topk_inds]\n            proposals = delta2bbox(anchors, rpn_bbox_pred, self.target_means,\n                                   self.target_stds, img_shape)\n            if cfg.min_bbox_size > 0:\n                w = proposals[:, 2] - proposals[:, 0] + 1\n                h = proposals[:, 3] - proposals[:, 1] + 1\n                valid_inds = torch.nonzero((w >= cfg.min_bbox_size) &\n                                           (h >= cfg.min_bbox_size)).squeeze()\n                proposals = proposals[valid_inds, :]\n                scores = scores[valid_inds]\n            proposals = torch.cat([proposals, scores.unsqueeze(-1)], dim=-1)\n            proposals, _ = nms(proposals, cfg.nms_thr)\n            proposals = proposals[:cfg.nms_post, :]\n            mlvl_proposals.append(proposals)\n        proposals = torch.cat(mlvl_proposals, 0)\n        if cfg.nms_across_levels:\n            proposals, _ = nms(proposals, cfg.nms_thr)\n            proposals = proposals[:cfg.max_num, :]\n        else:\n            scores = proposals[:, 4]\n            num = min(cfg.max_num, proposals.shape[0])\n            _, topk_inds = scores.topk(num)\n            proposals = proposals[topk_inds, :]\n        return proposals\n"""
mmaction/models/tenons/backbones/__init__.py,0,"b""from .bninception import BNInception\nfrom .resnet import ResNet\n\nfrom .inception_v1_i3d import InceptionV1_I3D\nfrom .resnet_i3d import ResNet_I3D\nfrom .resnet_s3d import ResNet_S3D\nfrom .resnet_i3d_slowfast import ResNet_I3D_SlowFast\nfrom .resnet_r3d import ResNet_R3D\n\n__all__ = [\n    'BNInception',\n    'ResNet',\n    'InceptionV1_I3D',\n    'ResNet_I3D',\n    'ResNet_S3D',\n    'ResNet_I3D_SlowFast',\n    'ResNet_R3D'\n]\n"""
mmaction/models/tenons/backbones/bninception.py,11,"b""import logging\n\nimport torch\nimport torch.nn as nn\nfrom mmcv.cnn import constant_init, kaiming_init\nfrom mmcv.runner import load_checkpoint\n\nfrom ...registry import BACKBONES\n\n\n__all__ = ['BNInception']\n\n@BACKBONES.register_module\nclass BNInception(nn.Module):\n\n    def __init__(self, \n                 pretrained=None,\n                 bn_eval=True,\n                 bn_frozen=False,\n                 partial_bn=False):\n        super(BNInception, self).__init__()\n\n        self.pretrained = pretrained\n        self.bn_eval = bn_eval\n        self.bn_frozen = bn_frozen\n        self.partial_bn = partial_bn\n\n        inplace = True\n        self.conv1_7x7_s2 = nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n        self.conv1_7x7_s2_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True)\n        self.conv1_relu_7x7 = nn.ReLU (inplace)\n        self.pool1_3x3_s2 = nn.MaxPool2d ((3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=True)\n        self.conv2_3x3_reduce = nn.Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n        self.conv2_3x3_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True)\n        self.conv2_relu_3x3_reduce = nn.ReLU (inplace)\n        self.conv2_3x3 = nn.Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.conv2_3x3_bn = nn.BatchNorm2d(192, eps=1e-05, momentum=0.9, affine=True)\n        self.conv2_relu_3x3 = nn.ReLU (inplace)\n        self.pool2_3x3_s2 = nn.MaxPool2d ((3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=True)\n        self.inception_3a_1x1 = nn.Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_3a_1x1_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_3a_relu_1x1 = nn.ReLU (inplace)\n        self.inception_3a_3x3_reduce = nn.Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_3a_3x3_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_3a_relu_3x3_reduce = nn.ReLU (inplace)\n        self.inception_3a_3x3 = nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_3a_3x3_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_3a_relu_3x3 = nn.ReLU (inplace)\n        self.inception_3a_double_3x3_reduce = nn.Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_3a_double_3x3_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_3a_relu_double_3x3_reduce = nn.ReLU (inplace)\n        self.inception_3a_double_3x3_1 = nn.Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_3a_double_3x3_1_bn = nn.BatchNorm2d(96, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_3a_relu_double_3x3_1 = nn.ReLU (inplace)\n        self.inception_3a_double_3x3_2 = nn.Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_3a_double_3x3_2_bn = nn.BatchNorm2d(96, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_3a_relu_double_3x3_2 = nn.ReLU (inplace)\n        self.inception_3a_pool = nn.AvgPool2d (3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n        self.inception_3a_pool_proj = nn.Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_3a_pool_proj_bn = nn.BatchNorm2d(32, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_3a_relu_pool_proj = nn.ReLU (inplace)\n        self.inception_3b_1x1 = nn.Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_3b_1x1_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_3b_relu_1x1 = nn.ReLU (inplace)\n        self.inception_3b_3x3_reduce = nn.Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_3b_3x3_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_3b_relu_3x3_reduce = nn.ReLU (inplace)\n        self.inception_3b_3x3 = nn.Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_3b_3x3_bn = nn.BatchNorm2d(96, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_3b_relu_3x3 = nn.ReLU (inplace)\n        self.inception_3b_double_3x3_reduce = nn.Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_3b_double_3x3_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_3b_relu_double_3x3_reduce = nn.ReLU (inplace)\n        self.inception_3b_double_3x3_1 = nn.Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_3b_double_3x3_1_bn = nn.BatchNorm2d(96, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_3b_relu_double_3x3_1 = nn.ReLU (inplace)\n        self.inception_3b_double_3x3_2 = nn.Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_3b_double_3x3_2_bn = nn.BatchNorm2d(96, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_3b_relu_double_3x3_2 = nn.ReLU (inplace)\n        self.inception_3b_pool = nn.AvgPool2d (3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n        self.inception_3b_pool_proj = nn.Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_3b_pool_proj_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_3b_relu_pool_proj = nn.ReLU (inplace)\n        self.inception_3c_3x3_reduce = nn.Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_3c_3x3_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_3c_relu_3x3_reduce = nn.ReLU (inplace)\n        self.inception_3c_3x3 = nn.Conv2d(128, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.inception_3c_3x3_bn = nn.BatchNorm2d(160, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_3c_relu_3x3 = nn.ReLU (inplace)\n        self.inception_3c_double_3x3_reduce = nn.Conv2d(320, 64, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_3c_double_3x3_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_3c_relu_double_3x3_reduce = nn.ReLU (inplace)\n        self.inception_3c_double_3x3_1 = nn.Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_3c_double_3x3_1_bn = nn.BatchNorm2d(96, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_3c_relu_double_3x3_1 = nn.ReLU (inplace)\n        self.inception_3c_double_3x3_2 = nn.Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.inception_3c_double_3x3_2_bn = nn.BatchNorm2d(96, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_3c_relu_double_3x3_2 = nn.ReLU (inplace)\n        self.inception_3c_pool = nn.MaxPool2d ((3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=True)\n        self.inception_4a_1x1 = nn.Conv2d(576, 224, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4a_1x1_bn = nn.BatchNorm2d(224, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_4a_relu_1x1 = nn.ReLU (inplace)\n        self.inception_4a_3x3_reduce = nn.Conv2d(576, 64, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4a_3x3_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_4a_relu_3x3_reduce = nn.ReLU (inplace)\n        self.inception_4a_3x3 = nn.Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4a_3x3_bn = nn.BatchNorm2d(96, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_4a_relu_3x3 = nn.ReLU (inplace)\n        self.inception_4a_double_3x3_reduce = nn.Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4a_double_3x3_reduce_bn = nn.BatchNorm2d(96, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_4a_relu_double_3x3_reduce = nn.ReLU (inplace)\n        self.inception_4a_double_3x3_1 = nn.Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4a_double_3x3_1_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_4a_relu_double_3x3_1 = nn.ReLU (inplace)\n        self.inception_4a_double_3x3_2 = nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4a_double_3x3_2_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_4a_relu_double_3x3_2 = nn.ReLU (inplace)\n        self.inception_4a_pool = nn.AvgPool2d (3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n        self.inception_4a_pool_proj = nn.Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4a_pool_proj_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_4a_relu_pool_proj = nn.ReLU (inplace)\n        self.inception_4b_1x1 = nn.Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4b_1x1_bn = nn.BatchNorm2d(192, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_4b_relu_1x1 = nn.ReLU (inplace)\n        self.inception_4b_3x3_reduce = nn.Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4b_3x3_reduce_bn = nn.BatchNorm2d(96, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_4b_relu_3x3_reduce = nn.ReLU (inplace)\n        self.inception_4b_3x3 = nn.Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4b_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_4b_relu_3x3 = nn.ReLU (inplace)\n        self.inception_4b_double_3x3_reduce = nn.Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4b_double_3x3_reduce_bn = nn.BatchNorm2d(96, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_4b_relu_double_3x3_reduce = nn.ReLU (inplace)\n        self.inception_4b_double_3x3_1 = nn.Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4b_double_3x3_1_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_4b_relu_double_3x3_1 = nn.ReLU (inplace)\n        self.inception_4b_double_3x3_2 = nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4b_double_3x3_2_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_4b_relu_double_3x3_2 = nn.ReLU (inplace)\n        self.inception_4b_pool = nn.AvgPool2d (3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n        self.inception_4b_pool_proj = nn.Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4b_pool_proj_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_4b_relu_pool_proj = nn.ReLU (inplace)\n        self.inception_4c_1x1 = nn.Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4c_1x1_bn = nn.BatchNorm2d(160, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_4c_relu_1x1 = nn.ReLU (inplace)\n        self.inception_4c_3x3_reduce = nn.Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4c_3x3_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_4c_relu_3x3_reduce = nn.ReLU (inplace)\n        self.inception_4c_3x3 = nn.Conv2d(128, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4c_3x3_bn = nn.BatchNorm2d(160, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_4c_relu_3x3 = nn.ReLU (inplace)\n        self.inception_4c_double_3x3_reduce = nn.Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4c_double_3x3_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_4c_relu_double_3x3_reduce = nn.ReLU (inplace)\n        self.inception_4c_double_3x3_1 = nn.Conv2d(128, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4c_double_3x3_1_bn = nn.BatchNorm2d(160, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_4c_relu_double_3x3_1 = nn.ReLU (inplace)\n        self.inception_4c_double_3x3_2 = nn.Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4c_double_3x3_2_bn = nn.BatchNorm2d(160, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_4c_relu_double_3x3_2 = nn.ReLU (inplace)\n        self.inception_4c_pool = nn.AvgPool2d (3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n        self.inception_4c_pool_proj = nn.Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4c_pool_proj_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_4c_relu_pool_proj = nn.ReLU (inplace)\n        self.inception_4d_1x1 = nn.Conv2d(608, 96, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4d_1x1_bn = nn.BatchNorm2d(96, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_4d_relu_1x1 = nn.ReLU (inplace)\n        self.inception_4d_3x3_reduce = nn.Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4d_3x3_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_4d_relu_3x3_reduce = nn.ReLU (inplace)\n        self.inception_4d_3x3 = nn.Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4d_3x3_bn = nn.BatchNorm2d(192, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_4d_relu_3x3 = nn.ReLU (inplace)\n        self.inception_4d_double_3x3_reduce = nn.Conv2d(608, 160, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4d_double_3x3_reduce_bn = nn.BatchNorm2d(160, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_4d_relu_double_3x3_reduce = nn.ReLU (inplace)\n        self.inception_4d_double_3x3_1 = nn.Conv2d(160, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4d_double_3x3_1_bn = nn.BatchNorm2d(192, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_4d_relu_double_3x3_1 = nn.ReLU (inplace)\n        self.inception_4d_double_3x3_2 = nn.Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4d_double_3x3_2_bn = nn.BatchNorm2d(192, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_4d_relu_double_3x3_2 = nn.ReLU (inplace)\n        self.inception_4d_pool = nn.AvgPool2d (3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n        self.inception_4d_pool_proj = nn.Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4d_pool_proj_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_4d_relu_pool_proj = nn.ReLU (inplace)\n        self.inception_4e_3x3_reduce = nn.Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4e_3x3_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_4e_relu_3x3_reduce = nn.ReLU (inplace)\n        self.inception_4e_3x3 = nn.Conv2d(128, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.inception_4e_3x3_bn = nn.BatchNorm2d(192, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_4e_relu_3x3 = nn.ReLU (inplace)\n        self.inception_4e_double_3x3_reduce = nn.Conv2d(608, 192, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_4e_double_3x3_reduce_bn = nn.BatchNorm2d(192, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_4e_relu_double_3x3_reduce = nn.ReLU (inplace)\n        self.inception_4e_double_3x3_1 = nn.Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_4e_double_3x3_1_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_4e_relu_double_3x3_1 = nn.ReLU (inplace)\n        self.inception_4e_double_3x3_2 = nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.inception_4e_double_3x3_2_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_4e_relu_double_3x3_2 = nn.ReLU (inplace)\n        self.inception_4e_pool = nn.MaxPool2d ((3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=True)\n        self.inception_5a_1x1 = nn.Conv2d(1056, 352, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_5a_1x1_bn = nn.BatchNorm2d(352, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_5a_relu_1x1 = nn.ReLU (inplace)\n        self.inception_5a_3x3_reduce = nn.Conv2d(1056, 192, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_5a_3x3_reduce_bn = nn.BatchNorm2d(192, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_5a_relu_3x3_reduce = nn.ReLU (inplace)\n        self.inception_5a_3x3 = nn.Conv2d(192, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_5a_3x3_bn = nn.BatchNorm2d(320, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_5a_relu_3x3 = nn.ReLU (inplace)\n        self.inception_5a_double_3x3_reduce = nn.Conv2d(1056, 160, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_5a_double_3x3_reduce_bn = nn.BatchNorm2d(160, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_5a_relu_double_3x3_reduce = nn.ReLU (inplace)\n        self.inception_5a_double_3x3_1 = nn.Conv2d(160, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_5a_double_3x3_1_bn = nn.BatchNorm2d(224, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_5a_relu_double_3x3_1 = nn.ReLU (inplace)\n        self.inception_5a_double_3x3_2 = nn.Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_5a_double_3x3_2_bn = nn.BatchNorm2d(224, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_5a_relu_double_3x3_2 = nn.ReLU (inplace)\n        self.inception_5a_pool = nn.AvgPool2d (3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n        self.inception_5a_pool_proj = nn.Conv2d(1056, 128, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_5a_pool_proj_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_5a_relu_pool_proj = nn.ReLU (inplace)\n        self.inception_5b_1x1 = nn.Conv2d(1024, 352, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_5b_1x1_bn = nn.BatchNorm2d(352, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_5b_relu_1x1 = nn.ReLU (inplace)\n        self.inception_5b_3x3_reduce = nn.Conv2d(1024, 192, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_5b_3x3_reduce_bn = nn.BatchNorm2d(192, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_5b_relu_3x3_reduce = nn.ReLU (inplace)\n        self.inception_5b_3x3 = nn.Conv2d(192, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_5b_3x3_bn = nn.BatchNorm2d(320, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_5b_relu_3x3 = nn.ReLU (inplace)\n        self.inception_5b_double_3x3_reduce = nn.Conv2d(1024, 192, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_5b_double_3x3_reduce_bn = nn.BatchNorm2d(192, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_5b_relu_double_3x3_reduce = nn.ReLU (inplace)\n        self.inception_5b_double_3x3_1 = nn.Conv2d(192, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_5b_double_3x3_1_bn = nn.BatchNorm2d(224, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_5b_relu_double_3x3_1 = nn.ReLU (inplace)\n        self.inception_5b_double_3x3_2 = nn.Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.inception_5b_double_3x3_2_bn = nn.BatchNorm2d(224, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_5b_relu_double_3x3_2 = nn.ReLU (inplace)\n        self.inception_5b_pool = nn.MaxPool2d ((3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1), ceil_mode=True)\n        self.inception_5b_pool_proj = nn.Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1))\n        self.inception_5b_pool_proj_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True)\n        self.inception_5b_relu_pool_proj = nn.ReLU (inplace)\n\n\n    def init_weights(self):\n        if isinstance(self.pretrained, str):\n            logger = logging.getLogger()\n            load_checkpoint(self, self.pretrained, strict=False, logger=logger)\n        elif self.pretrained is None:\n            for m in self.modules():\n                if isinstance(m, nn.Conv2d):\n                    kaiming_init(m)\n                elif isinstance(m, nn.BatchNorm2d):\n                    constant_init(m, 1)\n\n\n    def conv1(self, input):\n        conv1_7x7_s2_out = self.conv1_7x7_s2(input)\n        conv1_7x7_s2_bn_out = self.conv1_7x7_s2_bn(conv1_7x7_s2_out)\n        conv1_relu_7x7_out = self.conv1_relu_7x7(conv1_7x7_s2_bn_out)\n        return conv1_7x7_s2_bn_out\n\n    def forward(self, input):\n        conv1_7x7_s2_bn_out = self.conv1(input)\n        pool1_3x3_s2_out = self.pool1_3x3_s2(conv1_7x7_s2_bn_out)\n        conv2_3x3_reduce_out = self.conv2_3x3_reduce(pool1_3x3_s2_out)\n        conv2_3x3_reduce_bn_out = self.conv2_3x3_reduce_bn(conv2_3x3_reduce_out)\n        conv2_relu_3x3_reduce_out = self.conv2_relu_3x3_reduce(conv2_3x3_reduce_bn_out)\n        conv2_3x3_out = self.conv2_3x3(conv2_3x3_reduce_bn_out)\n        conv2_3x3_bn_out = self.conv2_3x3_bn(conv2_3x3_out)\n        conv2_relu_3x3_out = self.conv2_relu_3x3(conv2_3x3_bn_out)\n        pool2_3x3_s2_out = self.pool2_3x3_s2(conv2_3x3_bn_out)\n        inception_3a_1x1_out = self.inception_3a_1x1(pool2_3x3_s2_out)\n        inception_3a_1x1_bn_out = self.inception_3a_1x1_bn(inception_3a_1x1_out)\n        inception_3a_relu_1x1_out = self.inception_3a_relu_1x1(inception_3a_1x1_bn_out)\n        inception_3a_3x3_reduce_out = self.inception_3a_3x3_reduce(pool2_3x3_s2_out)\n        inception_3a_3x3_reduce_bn_out = self.inception_3a_3x3_reduce_bn(inception_3a_3x3_reduce_out)\n        inception_3a_relu_3x3_reduce_out = self.inception_3a_relu_3x3_reduce(inception_3a_3x3_reduce_bn_out)\n        inception_3a_3x3_out = self.inception_3a_3x3(inception_3a_3x3_reduce_bn_out)\n        inception_3a_3x3_bn_out = self.inception_3a_3x3_bn(inception_3a_3x3_out)\n        inception_3a_relu_3x3_out = self.inception_3a_relu_3x3(inception_3a_3x3_bn_out)\n        inception_3a_double_3x3_reduce_out = self.inception_3a_double_3x3_reduce(pool2_3x3_s2_out)\n        inception_3a_double_3x3_reduce_bn_out = self.inception_3a_double_3x3_reduce_bn(inception_3a_double_3x3_reduce_out)\n        inception_3a_relu_double_3x3_reduce_out = self.inception_3a_relu_double_3x3_reduce(inception_3a_double_3x3_reduce_bn_out)\n        inception_3a_double_3x3_1_out = self.inception_3a_double_3x3_1(inception_3a_double_3x3_reduce_bn_out)\n        inception_3a_double_3x3_1_bn_out = self.inception_3a_double_3x3_1_bn(inception_3a_double_3x3_1_out)\n        inception_3a_relu_double_3x3_1_out = self.inception_3a_relu_double_3x3_1(inception_3a_double_3x3_1_bn_out)\n        inception_3a_double_3x3_2_out = self.inception_3a_double_3x3_2(inception_3a_double_3x3_1_bn_out)\n        inception_3a_double_3x3_2_bn_out = self.inception_3a_double_3x3_2_bn(inception_3a_double_3x3_2_out)\n        inception_3a_relu_double_3x3_2_out = self.inception_3a_relu_double_3x3_2(inception_3a_double_3x3_2_bn_out)\n        inception_3a_pool_out = self.inception_3a_pool(pool2_3x3_s2_out)\n        inception_3a_pool_proj_out = self.inception_3a_pool_proj(inception_3a_pool_out)\n        inception_3a_pool_proj_bn_out = self.inception_3a_pool_proj_bn(inception_3a_pool_proj_out)\n        inception_3a_relu_pool_proj_out = self.inception_3a_relu_pool_proj(inception_3a_pool_proj_bn_out)\n        inception_3a_output_out = torch.cat([inception_3a_1x1_bn_out,inception_3a_3x3_bn_out,inception_3a_double_3x3_2_bn_out,inception_3a_pool_proj_bn_out], 1)\n        inception_3b_1x1_out = self.inception_3b_1x1(inception_3a_output_out)\n        inception_3b_1x1_bn_out = self.inception_3b_1x1_bn(inception_3b_1x1_out)\n        inception_3b_relu_1x1_out = self.inception_3b_relu_1x1(inception_3b_1x1_bn_out)\n        inception_3b_3x3_reduce_out = self.inception_3b_3x3_reduce(inception_3a_output_out)\n        inception_3b_3x3_reduce_bn_out = self.inception_3b_3x3_reduce_bn(inception_3b_3x3_reduce_out)\n        inception_3b_relu_3x3_reduce_out = self.inception_3b_relu_3x3_reduce(inception_3b_3x3_reduce_bn_out)\n        inception_3b_3x3_out = self.inception_3b_3x3(inception_3b_3x3_reduce_bn_out)\n        inception_3b_3x3_bn_out = self.inception_3b_3x3_bn(inception_3b_3x3_out)\n        inception_3b_relu_3x3_out = self.inception_3b_relu_3x3(inception_3b_3x3_bn_out)\n        inception_3b_double_3x3_reduce_out = self.inception_3b_double_3x3_reduce(inception_3a_output_out)\n        inception_3b_double_3x3_reduce_bn_out = self.inception_3b_double_3x3_reduce_bn(inception_3b_double_3x3_reduce_out)\n        inception_3b_relu_double_3x3_reduce_out = self.inception_3b_relu_double_3x3_reduce(inception_3b_double_3x3_reduce_bn_out)\n        inception_3b_double_3x3_1_out = self.inception_3b_double_3x3_1(inception_3b_double_3x3_reduce_bn_out)\n        inception_3b_double_3x3_1_bn_out = self.inception_3b_double_3x3_1_bn(inception_3b_double_3x3_1_out)\n        inception_3b_relu_double_3x3_1_out = self.inception_3b_relu_double_3x3_1(inception_3b_double_3x3_1_bn_out)\n        inception_3b_double_3x3_2_out = self.inception_3b_double_3x3_2(inception_3b_double_3x3_1_bn_out)\n        inception_3b_double_3x3_2_bn_out = self.inception_3b_double_3x3_2_bn(inception_3b_double_3x3_2_out)\n        inception_3b_relu_double_3x3_2_out = self.inception_3b_relu_double_3x3_2(inception_3b_double_3x3_2_bn_out)\n        inception_3b_pool_out = self.inception_3b_pool(inception_3a_output_out)\n        inception_3b_pool_proj_out = self.inception_3b_pool_proj(inception_3b_pool_out)\n        inception_3b_pool_proj_bn_out = self.inception_3b_pool_proj_bn(inception_3b_pool_proj_out)\n        inception_3b_relu_pool_proj_out = self.inception_3b_relu_pool_proj(inception_3b_pool_proj_bn_out)\n        inception_3b_output_out = torch.cat([inception_3b_1x1_bn_out,inception_3b_3x3_bn_out,inception_3b_double_3x3_2_bn_out,inception_3b_pool_proj_bn_out], 1)\n        inception_3c_3x3_reduce_out = self.inception_3c_3x3_reduce(inception_3b_output_out)\n        inception_3c_3x3_reduce_bn_out = self.inception_3c_3x3_reduce_bn(inception_3c_3x3_reduce_out)\n        inception_3c_relu_3x3_reduce_out = self.inception_3c_relu_3x3_reduce(inception_3c_3x3_reduce_bn_out)\n        inception_3c_3x3_out = self.inception_3c_3x3(inception_3c_3x3_reduce_bn_out)\n        inception_3c_3x3_bn_out = self.inception_3c_3x3_bn(inception_3c_3x3_out)\n        inception_3c_relu_3x3_out = self.inception_3c_relu_3x3(inception_3c_3x3_bn_out)\n        inception_3c_double_3x3_reduce_out = self.inception_3c_double_3x3_reduce(inception_3b_output_out)\n        inception_3c_double_3x3_reduce_bn_out = self.inception_3c_double_3x3_reduce_bn(inception_3c_double_3x3_reduce_out)\n        inception_3c_relu_double_3x3_reduce_out = self.inception_3c_relu_double_3x3_reduce(inception_3c_double_3x3_reduce_bn_out)\n        inception_3c_double_3x3_1_out = self.inception_3c_double_3x3_1(inception_3c_double_3x3_reduce_bn_out)\n        inception_3c_double_3x3_1_bn_out = self.inception_3c_double_3x3_1_bn(inception_3c_double_3x3_1_out)\n        inception_3c_relu_double_3x3_1_out = self.inception_3c_relu_double_3x3_1(inception_3c_double_3x3_1_bn_out)\n        inception_3c_double_3x3_2_out = self.inception_3c_double_3x3_2(inception_3c_double_3x3_1_bn_out)\n        inception_3c_double_3x3_2_bn_out = self.inception_3c_double_3x3_2_bn(inception_3c_double_3x3_2_out)\n        inception_3c_relu_double_3x3_2_out = self.inception_3c_relu_double_3x3_2(inception_3c_double_3x3_2_bn_out)\n        inception_3c_pool_out = self.inception_3c_pool(inception_3b_output_out)\n        inception_3c_output_out = torch.cat([inception_3c_3x3_bn_out,inception_3c_double_3x3_2_bn_out,inception_3c_pool_out], 1)\n        inception_4a_1x1_out = self.inception_4a_1x1(inception_3c_output_out)\n        inception_4a_1x1_bn_out = self.inception_4a_1x1_bn(inception_4a_1x1_out)\n        inception_4a_relu_1x1_out = self.inception_4a_relu_1x1(inception_4a_1x1_bn_out)\n        inception_4a_3x3_reduce_out = self.inception_4a_3x3_reduce(inception_3c_output_out)\n        inception_4a_3x3_reduce_bn_out = self.inception_4a_3x3_reduce_bn(inception_4a_3x3_reduce_out)\n        inception_4a_relu_3x3_reduce_out = self.inception_4a_relu_3x3_reduce(inception_4a_3x3_reduce_bn_out)\n        inception_4a_3x3_out = self.inception_4a_3x3(inception_4a_3x3_reduce_bn_out)\n        inception_4a_3x3_bn_out = self.inception_4a_3x3_bn(inception_4a_3x3_out)\n        inception_4a_relu_3x3_out = self.inception_4a_relu_3x3(inception_4a_3x3_bn_out)\n        inception_4a_double_3x3_reduce_out = self.inception_4a_double_3x3_reduce(inception_3c_output_out)\n        inception_4a_double_3x3_reduce_bn_out = self.inception_4a_double_3x3_reduce_bn(inception_4a_double_3x3_reduce_out)\n        inception_4a_relu_double_3x3_reduce_out = self.inception_4a_relu_double_3x3_reduce(inception_4a_double_3x3_reduce_bn_out)\n        inception_4a_double_3x3_1_out = self.inception_4a_double_3x3_1(inception_4a_double_3x3_reduce_bn_out)\n        inception_4a_double_3x3_1_bn_out = self.inception_4a_double_3x3_1_bn(inception_4a_double_3x3_1_out)\n        inception_4a_relu_double_3x3_1_out = self.inception_4a_relu_double_3x3_1(inception_4a_double_3x3_1_bn_out)\n        inception_4a_double_3x3_2_out = self.inception_4a_double_3x3_2(inception_4a_double_3x3_1_bn_out)\n        inception_4a_double_3x3_2_bn_out = self.inception_4a_double_3x3_2_bn(inception_4a_double_3x3_2_out)\n        inception_4a_relu_double_3x3_2_out = self.inception_4a_relu_double_3x3_2(inception_4a_double_3x3_2_bn_out)\n        inception_4a_pool_out = self.inception_4a_pool(inception_3c_output_out)\n        inception_4a_pool_proj_out = self.inception_4a_pool_proj(inception_4a_pool_out)\n        inception_4a_pool_proj_bn_out = self.inception_4a_pool_proj_bn(inception_4a_pool_proj_out)\n        inception_4a_relu_pool_proj_out = self.inception_4a_relu_pool_proj(inception_4a_pool_proj_bn_out)\n        inception_4a_output_out = torch.cat([inception_4a_1x1_bn_out,inception_4a_3x3_bn_out,inception_4a_double_3x3_2_bn_out,inception_4a_pool_proj_bn_out], 1)\n        inception_4b_1x1_out = self.inception_4b_1x1(inception_4a_output_out)\n        inception_4b_1x1_bn_out = self.inception_4b_1x1_bn(inception_4b_1x1_out)\n        inception_4b_relu_1x1_out = self.inception_4b_relu_1x1(inception_4b_1x1_bn_out)\n        inception_4b_3x3_reduce_out = self.inception_4b_3x3_reduce(inception_4a_output_out)\n        inception_4b_3x3_reduce_bn_out = self.inception_4b_3x3_reduce_bn(inception_4b_3x3_reduce_out)\n        inception_4b_relu_3x3_reduce_out = self.inception_4b_relu_3x3_reduce(inception_4b_3x3_reduce_bn_out)\n        inception_4b_3x3_out = self.inception_4b_3x3(inception_4b_3x3_reduce_bn_out)\n        inception_4b_3x3_bn_out = self.inception_4b_3x3_bn(inception_4b_3x3_out)\n        inception_4b_relu_3x3_out = self.inception_4b_relu_3x3(inception_4b_3x3_bn_out)\n        inception_4b_double_3x3_reduce_out = self.inception_4b_double_3x3_reduce(inception_4a_output_out)\n        inception_4b_double_3x3_reduce_bn_out = self.inception_4b_double_3x3_reduce_bn(inception_4b_double_3x3_reduce_out)\n        inception_4b_relu_double_3x3_reduce_out = self.inception_4b_relu_double_3x3_reduce(inception_4b_double_3x3_reduce_bn_out)\n        inception_4b_double_3x3_1_out = self.inception_4b_double_3x3_1(inception_4b_double_3x3_reduce_bn_out)\n        inception_4b_double_3x3_1_bn_out = self.inception_4b_double_3x3_1_bn(inception_4b_double_3x3_1_out)\n        inception_4b_relu_double_3x3_1_out = self.inception_4b_relu_double_3x3_1(inception_4b_double_3x3_1_bn_out)\n        inception_4b_double_3x3_2_out = self.inception_4b_double_3x3_2(inception_4b_double_3x3_1_bn_out)\n        inception_4b_double_3x3_2_bn_out = self.inception_4b_double_3x3_2_bn(inception_4b_double_3x3_2_out)\n        inception_4b_relu_double_3x3_2_out = self.inception_4b_relu_double_3x3_2(inception_4b_double_3x3_2_bn_out)\n        inception_4b_pool_out = self.inception_4b_pool(inception_4a_output_out)\n        inception_4b_pool_proj_out = self.inception_4b_pool_proj(inception_4b_pool_out)\n        inception_4b_pool_proj_bn_out = self.inception_4b_pool_proj_bn(inception_4b_pool_proj_out)\n        inception_4b_relu_pool_proj_out = self.inception_4b_relu_pool_proj(inception_4b_pool_proj_bn_out)\n        inception_4b_output_out = torch.cat([inception_4b_1x1_bn_out,inception_4b_3x3_bn_out,inception_4b_double_3x3_2_bn_out,inception_4b_pool_proj_bn_out], 1)\n        inception_4c_1x1_out = self.inception_4c_1x1(inception_4b_output_out)\n        inception_4c_1x1_bn_out = self.inception_4c_1x1_bn(inception_4c_1x1_out)\n        inception_4c_relu_1x1_out = self.inception_4c_relu_1x1(inception_4c_1x1_bn_out)\n        inception_4c_3x3_reduce_out = self.inception_4c_3x3_reduce(inception_4b_output_out)\n        inception_4c_3x3_reduce_bn_out = self.inception_4c_3x3_reduce_bn(inception_4c_3x3_reduce_out)\n        inception_4c_relu_3x3_reduce_out = self.inception_4c_relu_3x3_reduce(inception_4c_3x3_reduce_bn_out)\n        inception_4c_3x3_out = self.inception_4c_3x3(inception_4c_3x3_reduce_bn_out)\n        inception_4c_3x3_bn_out = self.inception_4c_3x3_bn(inception_4c_3x3_out)\n        inception_4c_relu_3x3_out = self.inception_4c_relu_3x3(inception_4c_3x3_bn_out)\n        inception_4c_double_3x3_reduce_out = self.inception_4c_double_3x3_reduce(inception_4b_output_out)\n        inception_4c_double_3x3_reduce_bn_out = self.inception_4c_double_3x3_reduce_bn(inception_4c_double_3x3_reduce_out)\n        inception_4c_relu_double_3x3_reduce_out = self.inception_4c_relu_double_3x3_reduce(inception_4c_double_3x3_reduce_bn_out)\n        inception_4c_double_3x3_1_out = self.inception_4c_double_3x3_1(inception_4c_double_3x3_reduce_bn_out)\n        inception_4c_double_3x3_1_bn_out = self.inception_4c_double_3x3_1_bn(inception_4c_double_3x3_1_out)\n        inception_4c_relu_double_3x3_1_out = self.inception_4c_relu_double_3x3_1(inception_4c_double_3x3_1_bn_out)\n        inception_4c_double_3x3_2_out = self.inception_4c_double_3x3_2(inception_4c_double_3x3_1_bn_out)\n        inception_4c_double_3x3_2_bn_out = self.inception_4c_double_3x3_2_bn(inception_4c_double_3x3_2_out)\n        inception_4c_relu_double_3x3_2_out = self.inception_4c_relu_double_3x3_2(inception_4c_double_3x3_2_bn_out)\n        inception_4c_pool_out = self.inception_4c_pool(inception_4b_output_out)\n        inception_4c_pool_proj_out = self.inception_4c_pool_proj(inception_4c_pool_out)\n        inception_4c_pool_proj_bn_out = self.inception_4c_pool_proj_bn(inception_4c_pool_proj_out)\n        inception_4c_relu_pool_proj_out = self.inception_4c_relu_pool_proj(inception_4c_pool_proj_bn_out)\n        inception_4c_output_out = torch.cat([inception_4c_1x1_bn_out,inception_4c_3x3_bn_out,inception_4c_double_3x3_2_bn_out,inception_4c_pool_proj_bn_out], 1)\n        inception_4d_1x1_out = self.inception_4d_1x1(inception_4c_output_out)\n        inception_4d_1x1_bn_out = self.inception_4d_1x1_bn(inception_4d_1x1_out)\n        inception_4d_relu_1x1_out = self.inception_4d_relu_1x1(inception_4d_1x1_bn_out)\n        inception_4d_3x3_reduce_out = self.inception_4d_3x3_reduce(inception_4c_output_out)\n        inception_4d_3x3_reduce_bn_out = self.inception_4d_3x3_reduce_bn(inception_4d_3x3_reduce_out)\n        inception_4d_relu_3x3_reduce_out = self.inception_4d_relu_3x3_reduce(inception_4d_3x3_reduce_bn_out)\n        inception_4d_3x3_out = self.inception_4d_3x3(inception_4d_3x3_reduce_bn_out)\n        inception_4d_3x3_bn_out = self.inception_4d_3x3_bn(inception_4d_3x3_out)\n        inception_4d_relu_3x3_out = self.inception_4d_relu_3x3(inception_4d_3x3_bn_out)\n        inception_4d_double_3x3_reduce_out = self.inception_4d_double_3x3_reduce(inception_4c_output_out)\n        inception_4d_double_3x3_reduce_bn_out = self.inception_4d_double_3x3_reduce_bn(inception_4d_double_3x3_reduce_out)\n        inception_4d_relu_double_3x3_reduce_out = self.inception_4d_relu_double_3x3_reduce(inception_4d_double_3x3_reduce_bn_out)\n        inception_4d_double_3x3_1_out = self.inception_4d_double_3x3_1(inception_4d_double_3x3_reduce_bn_out)\n        inception_4d_double_3x3_1_bn_out = self.inception_4d_double_3x3_1_bn(inception_4d_double_3x3_1_out)\n        inception_4d_relu_double_3x3_1_out = self.inception_4d_relu_double_3x3_1(inception_4d_double_3x3_1_bn_out)\n        inception_4d_double_3x3_2_out = self.inception_4d_double_3x3_2(inception_4d_double_3x3_1_bn_out)\n        inception_4d_double_3x3_2_bn_out = self.inception_4d_double_3x3_2_bn(inception_4d_double_3x3_2_out)\n        inception_4d_relu_double_3x3_2_out = self.inception_4d_relu_double_3x3_2(inception_4d_double_3x3_2_bn_out)\n        inception_4d_pool_out = self.inception_4d_pool(inception_4c_output_out)\n        inception_4d_pool_proj_out = self.inception_4d_pool_proj(inception_4d_pool_out)\n        inception_4d_pool_proj_bn_out = self.inception_4d_pool_proj_bn(inception_4d_pool_proj_out)\n        inception_4d_relu_pool_proj_out = self.inception_4d_relu_pool_proj(inception_4d_pool_proj_bn_out)\n        inception_4d_output_out = torch.cat([inception_4d_1x1_bn_out,inception_4d_3x3_bn_out,inception_4d_double_3x3_2_bn_out,inception_4d_pool_proj_bn_out], 1)\n        inception_4e_3x3_reduce_out = self.inception_4e_3x3_reduce(inception_4d_output_out)\n        inception_4e_3x3_reduce_bn_out = self.inception_4e_3x3_reduce_bn(inception_4e_3x3_reduce_out)\n        inception_4e_relu_3x3_reduce_out = self.inception_4e_relu_3x3_reduce(inception_4e_3x3_reduce_bn_out)\n        inception_4e_3x3_out = self.inception_4e_3x3(inception_4e_3x3_reduce_bn_out)\n        inception_4e_3x3_bn_out = self.inception_4e_3x3_bn(inception_4e_3x3_out)\n        inception_4e_relu_3x3_out = self.inception_4e_relu_3x3(inception_4e_3x3_bn_out)\n        inception_4e_double_3x3_reduce_out = self.inception_4e_double_3x3_reduce(inception_4d_output_out)\n        inception_4e_double_3x3_reduce_bn_out = self.inception_4e_double_3x3_reduce_bn(inception_4e_double_3x3_reduce_out)\n        inception_4e_relu_double_3x3_reduce_out = self.inception_4e_relu_double_3x3_reduce(inception_4e_double_3x3_reduce_bn_out)\n        inception_4e_double_3x3_1_out = self.inception_4e_double_3x3_1(inception_4e_double_3x3_reduce_bn_out)\n        inception_4e_double_3x3_1_bn_out = self.inception_4e_double_3x3_1_bn(inception_4e_double_3x3_1_out)\n        inception_4e_relu_double_3x3_1_out = self.inception_4e_relu_double_3x3_1(inception_4e_double_3x3_1_bn_out)\n        inception_4e_double_3x3_2_out = self.inception_4e_double_3x3_2(inception_4e_double_3x3_1_bn_out)\n        inception_4e_double_3x3_2_bn_out = self.inception_4e_double_3x3_2_bn(inception_4e_double_3x3_2_out)\n        inception_4e_relu_double_3x3_2_out = self.inception_4e_relu_double_3x3_2(inception_4e_double_3x3_2_bn_out)\n        inception_4e_pool_out = self.inception_4e_pool(inception_4d_output_out)\n        inception_4e_output_out = torch.cat([inception_4e_3x3_bn_out,inception_4e_double_3x3_2_bn_out,inception_4e_pool_out], 1)\n        inception_5a_1x1_out = self.inception_5a_1x1(inception_4e_output_out)\n        inception_5a_1x1_bn_out = self.inception_5a_1x1_bn(inception_5a_1x1_out)\n        inception_5a_relu_1x1_out = self.inception_5a_relu_1x1(inception_5a_1x1_bn_out)\n        inception_5a_3x3_reduce_out = self.inception_5a_3x3_reduce(inception_4e_output_out)\n        inception_5a_3x3_reduce_bn_out = self.inception_5a_3x3_reduce_bn(inception_5a_3x3_reduce_out)\n        inception_5a_relu_3x3_reduce_out = self.inception_5a_relu_3x3_reduce(inception_5a_3x3_reduce_bn_out)\n        inception_5a_3x3_out = self.inception_5a_3x3(inception_5a_3x3_reduce_bn_out)\n        inception_5a_3x3_bn_out = self.inception_5a_3x3_bn(inception_5a_3x3_out)\n        inception_5a_relu_3x3_out = self.inception_5a_relu_3x3(inception_5a_3x3_bn_out)\n        inception_5a_double_3x3_reduce_out = self.inception_5a_double_3x3_reduce(inception_4e_output_out)\n        inception_5a_double_3x3_reduce_bn_out = self.inception_5a_double_3x3_reduce_bn(inception_5a_double_3x3_reduce_out)\n        inception_5a_relu_double_3x3_reduce_out = self.inception_5a_relu_double_3x3_reduce(inception_5a_double_3x3_reduce_bn_out)\n        inception_5a_double_3x3_1_out = self.inception_5a_double_3x3_1(inception_5a_double_3x3_reduce_bn_out)\n        inception_5a_double_3x3_1_bn_out = self.inception_5a_double_3x3_1_bn(inception_5a_double_3x3_1_out)\n        inception_5a_relu_double_3x3_1_out = self.inception_5a_relu_double_3x3_1(inception_5a_double_3x3_1_bn_out)\n        inception_5a_double_3x3_2_out = self.inception_5a_double_3x3_2(inception_5a_double_3x3_1_bn_out)\n        inception_5a_double_3x3_2_bn_out = self.inception_5a_double_3x3_2_bn(inception_5a_double_3x3_2_out)\n        inception_5a_relu_double_3x3_2_out = self.inception_5a_relu_double_3x3_2(inception_5a_double_3x3_2_bn_out)\n        inception_5a_pool_out = self.inception_5a_pool(inception_4e_output_out)\n        inception_5a_pool_proj_out = self.inception_5a_pool_proj(inception_5a_pool_out)\n        inception_5a_pool_proj_bn_out = self.inception_5a_pool_proj_bn(inception_5a_pool_proj_out)\n        inception_5a_relu_pool_proj_out = self.inception_5a_relu_pool_proj(inception_5a_pool_proj_bn_out)\n        inception_5a_output_out = torch.cat([inception_5a_1x1_bn_out,inception_5a_3x3_bn_out,inception_5a_double_3x3_2_bn_out,inception_5a_pool_proj_bn_out], 1)\n        inception_5b_1x1_out = self.inception_5b_1x1(inception_5a_output_out)\n        inception_5b_1x1_bn_out = self.inception_5b_1x1_bn(inception_5b_1x1_out)\n        inception_5b_relu_1x1_out = self.inception_5b_relu_1x1(inception_5b_1x1_bn_out)\n        inception_5b_3x3_reduce_out = self.inception_5b_3x3_reduce(inception_5a_output_out)\n        inception_5b_3x3_reduce_bn_out = self.inception_5b_3x3_reduce_bn(inception_5b_3x3_reduce_out)\n        inception_5b_relu_3x3_reduce_out = self.inception_5b_relu_3x3_reduce(inception_5b_3x3_reduce_bn_out)\n        inception_5b_3x3_out = self.inception_5b_3x3(inception_5b_3x3_reduce_bn_out)\n        inception_5b_3x3_bn_out = self.inception_5b_3x3_bn(inception_5b_3x3_out)\n        inception_5b_relu_3x3_out = self.inception_5b_relu_3x3(inception_5b_3x3_bn_out)\n        inception_5b_double_3x3_reduce_out = self.inception_5b_double_3x3_reduce(inception_5a_output_out)\n        inception_5b_double_3x3_reduce_bn_out = self.inception_5b_double_3x3_reduce_bn(inception_5b_double_3x3_reduce_out)\n        inception_5b_relu_double_3x3_reduce_out = self.inception_5b_relu_double_3x3_reduce(inception_5b_double_3x3_reduce_bn_out)\n        inception_5b_double_3x3_1_out = self.inception_5b_double_3x3_1(inception_5b_double_3x3_reduce_bn_out)\n        inception_5b_double_3x3_1_bn_out = self.inception_5b_double_3x3_1_bn(inception_5b_double_3x3_1_out)\n        inception_5b_relu_double_3x3_1_out = self.inception_5b_relu_double_3x3_1(inception_5b_double_3x3_1_bn_out)\n        inception_5b_double_3x3_2_out = self.inception_5b_double_3x3_2(inception_5b_double_3x3_1_bn_out)\n        inception_5b_double_3x3_2_bn_out = self.inception_5b_double_3x3_2_bn(inception_5b_double_3x3_2_out)\n        inception_5b_relu_double_3x3_2_out = self.inception_5b_relu_double_3x3_2(inception_5b_double_3x3_2_bn_out)\n        inception_5b_pool_out = self.inception_5b_pool(inception_5a_output_out)\n        inception_5b_pool_proj_out = self.inception_5b_pool_proj(inception_5b_pool_out)\n        inception_5b_pool_proj_bn_out = self.inception_5b_pool_proj_bn(inception_5b_pool_proj_out)\n        inception_5b_relu_pool_proj_out = self.inception_5b_relu_pool_proj(inception_5b_pool_proj_bn_out)\n        inception_5b_output_out = torch.cat([inception_5b_1x1_bn_out,inception_5b_3x3_bn_out,inception_5b_double_3x3_2_bn_out,inception_5b_pool_proj_bn_out], 1)\n        return inception_5b_output_out\n\n    def train(self, mode=True):\n        super(BNInception, self).train(mode)\n        if self.bn_eval:\n            for m in self.modules():\n                if isinstance(m, nn.BatchNorm2d):\n                    m.eval()\n                    if self.bn_frozen:\n                        for params in m.parameters():\n                            params.requires_grad = False\n        if self.partial_bn:\n            for n, m in self.named_modules():\n                if 'conv1' not in n and isinstance(m, nn.BatchNorm2d):\n                    m.eval()\n                    m.weight.requires_grad = False\n                    m.bias.requires_grad = False\n\n"""
mmaction/models/tenons/backbones/inception_v1_i3d.py,11,"b""import logging\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import constant_init, kaiming_init\nfrom mmcv.runner import load_checkpoint\n\nfrom ...registry import BACKBONES\n# from mmaction.ops.reflection_pad3d import reflection_pad3d\n\n\n__all__ = ['InceptionV1_I3D']\n\n@BACKBONES.register_module\nclass InceptionV1_I3D(nn.Module):\n\n    ## TODO:\n    ## Refactor it into a more modular way\n    ## Reference: Table 1 from https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf\n\n    def __init__(self,\n                 pretrained=None,\n                 bn_eval=True,\n                 bn_frozen=False,\n                 partial_bn=False,\n                 modality='RGB'):\n        super(InceptionV1_I3D, self).__init__()\n\n        self.pretrained = pretrained\n        self.bn_eval = bn_eval\n        self.bn_frozen = bn_frozen\n        self.partial_bn = partial_bn\n        self.modality = modality\n\n        inplace = True\n        assert modality in ['RGB', 'Flow']\n        if modality == 'RGB':\n            self.conv1_7x7_s2 = nn.Conv3d(3, 64, kernel_size=(7, 7, 7), stride=(2, 2, 2), padding=(0, 0, 0), bias=False)\n        else:\n            self.conv1_7x7_s2 = nn.Conv3d(2, 64, kernel_size=(7, 7, 7), stride=(2, 2, 2), padding=(0, 0, 0), bias=False)\n        self.conv1_7x7_s2_bn = nn.BatchNorm3d(64, eps=1e-05, affine=True)\n        self.conv1_relu_7x7 = nn.ReLU(inplace)\n        self.pool1_3x3_s2 = nn.MaxPool3d((1, 3, 3), stride=(1, 2, 2), dilation=(1, 1, 1), ceil_mode=True)\n        self.conv2_3x3_reduce = nn.Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.conv2_3x3_reduce_bn = nn.BatchNorm3d(64, eps=1e-05, affine=True)\n        self.conv2_relu_3x3_reduce = nn.ReLU(inplace)\n        self.conv2_3x3 = nn.Conv3d(64, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n        self.conv2_3x3_bn = nn.BatchNorm3d(192, eps=1e-05, affine=True)\n        self.conv2_relu_3x3 = nn.ReLU(inplace)\n        self.pool2_3x3_s2 = nn.MaxPool3d((1, 3, 3), stride=(1, 2, 2), dilation=(1, 1, 1), ceil_mode=True)\n\n        ##########\n        self.inception_3a_1x1 = nn.Conv3d(192, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.inception_3a_1x1_bn = nn.BatchNorm3d(64, eps=1e-05, affine=True)\n        self.inception_3a_relu_1x1 = nn.ReLU(inplace)\n\n        self.inception_3a_branch1_3x3_reduce = nn.Conv3d(192, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.inception_3a_branch1_3x3_reduce_bn = nn.BatchNorm3d(96, eps=1e-05, affine=True)\n        self.inception_3a_branch1_relu_3x3_reduce = nn.ReLU(inplace)\n        self.inception_3a_branch1_3x3 = nn.Conv3d(96, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n        self.inception_3a_branch1_3x3_bn = nn.BatchNorm3d(128, eps=1e-05, affine=True)\n        self.inception_3a_branch1_relu_3x3 = nn.ReLU(inplace)\n\n        self.inception_3a_branch2_3x3_reduce = nn.Conv3d(192, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.inception_3a_branch2_3x3_reduce_bn = nn.BatchNorm3d(16, eps=1e-05, affine=True)\n        self.inception_3a_branch2_relu_3x3_reduce = nn.ReLU(inplace)\n        self.inception_3a_branch2_3x3 = nn.Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n        self.inception_3a_branch2_3x3_bn = nn.BatchNorm3d(32, eps=1e-05, affine=True)\n        self.inception_3a_branch2_relu_3x3 = nn.ReLU(inplace)\n\n        # self.inception_3a_pool = nn.MaxPool3d(3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n        self.inception_3a_pool = nn.MaxPool3d(3, stride=1, padding=1, ceil_mode=True)\n        self.inception_3a_pool_proj = nn.Conv3d(192, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.inception_3a_pool_proj_bn = nn.BatchNorm3d(32, eps=1e-05, affine=True)\n        self.inception_3a_relu_pool_proj = nn.ReLU(inplace)\n\n        self.inception_3b_1x1 = nn.Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.inception_3b_1x1_bn = nn.BatchNorm3d(128, eps=1e-05, affine=True)\n        self.inception_3b_relu_1x1 = nn.ReLU(inplace)\n\n        self.inception_3b_branch1_3x3_reduce = nn.Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.inception_3b_branch1_3x3_reduce_bn = nn.BatchNorm3d(128, eps=1e-05, affine=True)\n        self.inception_3b_branch1_relu_3x3_reduce = nn.ReLU(inplace)\n        self.inception_3b_branch1_3x3 = nn.Conv3d(128, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n        self.inception_3b_branch1_3x3_bn = nn.BatchNorm3d(192, eps=1e-05, affine=True)\n        self.inception_3b_branch1_relu_3x3 = nn.ReLU(inplace)\n\n        self.inception_3b_branch2_3x3_reduce = nn.Conv3d(256, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.inception_3b_branch2_3x3_reduce_bn = nn.BatchNorm3d(32, eps=1e-05, affine=True)\n        self.inception_3b_branch2_relu_3x3_reduce = nn.ReLU(inplace)\n        self.inception_3b_branch2_3x3 = nn.Conv3d(32, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n        self.inception_3b_branch2_3x3_bn = nn.BatchNorm3d(96, eps=1e-05, affine=True)\n        self.inception_3b_branch2_relu_3x3 = nn.ReLU(inplace)\n\n        # self.inception_3b_pool = nn.MaxPool3d(3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n        self.inception_3b_pool = nn.MaxPool3d(3, stride=1, padding=1, ceil_mode=True)\n        self.inception_3b_pool_proj = nn.Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.inception_3b_pool_proj_bn = nn.BatchNorm3d(64, eps=1e-05, affine=True)\n        self.inception_3b_relu_pool_proj = nn.ReLU(inplace)\n\n        self.inception_3c_pool = nn.MaxPool3d((3, 3, 3), stride=(2, 2, 2), dilation=(1, 1, 1), ceil_mode=True)\n\n        ##########\n        self.inception_4a_1x1 = nn.Conv3d(480, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.inception_4a_1x1_bn = nn.BatchNorm3d(192, eps=1e-05, affine=True)\n        self.inception_4a_relu_1x1 = nn.ReLU(inplace)\n\n        self.inception_4a_branch1_3x3_reduce = nn.Conv3d(480, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.inception_4a_branch1_3x3_reduce_bn = nn.BatchNorm3d(96, eps=1e-05, affine=True)\n        self.inception_4a_branch1_relu_3x3_reduce = nn.ReLU(inplace)\n        self.inception_4a_branch1_3x3 = nn.Conv3d(96, 208, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n        self.inception_4a_branch1_3x3_bn = nn.BatchNorm3d(208, eps=1e-05, affine=True)\n        self.inception_4a_branch1_relu_3x3 = nn.ReLU(inplace)\n\n        self.inception_4a_branch2_3x3_reduce = nn.Conv3d(480, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.inception_4a_branch2_3x3_reduce_bn = nn.BatchNorm3d(16, eps=1e-05, affine=True)\n        self.inception_4a_branch2_relu_3x3_reduce = nn.ReLU(inplace)\n        self.inception_4a_branch2_3x3 = nn.Conv3d(16, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n        self.inception_4a_branch2_3x3_bn = nn.BatchNorm3d(48, eps=1e-05, affine=True)\n        self.inception_4a_branch2_relu_3x3 = nn.ReLU(inplace)\n\n        # self.inception_4a_pool = nn.MaxPool3d(3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n        self.inception_4a_pool = nn.MaxPool3d(3, stride=1, padding=1, ceil_mode=True)\n        self.inception_4a_pool_proj = nn.Conv3d(480, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.inception_4a_pool_proj_bn = nn.BatchNorm3d(64, eps=1e-05, affine=True)\n        self.inception_4a_relu_pool_proj = nn.ReLU(inplace)\n\n        self.inception_4b_1x1 = nn.Conv3d(512, 160, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.inception_4b_1x1_bn = nn.BatchNorm3d(160, eps=1e-05, affine=True)\n        self.inception_4b_relu_1x1 = nn.ReLU(inplace)\n\n        self.inception_4b_branch1_3x3_reduce = nn.Conv3d(512, 112, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.inception_4b_branch1_3x3_reduce_bn = nn.BatchNorm3d(112, eps=1e-05, affine=True)\n        self.inception_4b_branch1_relu_3x3_reduce = nn.ReLU(inplace)\n        self.inception_4b_branch1_3x3 = nn.Conv3d(112, 224, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n        self.inception_4b_branch1_3x3_bn = nn.BatchNorm3d(224, eps=1e-05, affine=True)\n        self.inception_4b_branch1_relu_3x3 = nn.ReLU(inplace)\n\n        self.inception_4b_branch2_3x3_reduce = nn.Conv3d(512, 24, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.inception_4b_branch2_3x3_reduce_bn = nn.BatchNorm3d(24, eps=1e-05, affine=True)\n        self.inception_4b_branch2_relu_3x3_reduce = nn.ReLU(inplace)\n        self.inception_4b_branch2_3x3 = nn.Conv3d(24, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n        self.inception_4b_branch2_3x3_bn = nn.BatchNorm3d(64, eps=1e-05, affine=True)\n        self.inception_4b_branch2_relu_3x3 = nn.ReLU(inplace)\n\n        # self.inception_4b_pool = nn.MaxPool3d(3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n        self.inception_4b_pool = nn.MaxPool3d(3, stride=1, padding=1, ceil_mode=True)\n        self.inception_4b_pool_proj = nn.Conv3d(512, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.inception_4b_pool_proj_bn = nn.BatchNorm3d(64, eps=1e-05, affine=True)\n        self.inception_4b_relu_pool_proj = nn.ReLU(inplace)\n\n        self.inception_4c_1x1 = nn.Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.inception_4c_1x1_bn = nn.BatchNorm3d(128, eps=1e-05, affine=True)\n        self.inception_4c_relu_1x1 = nn.ReLU(inplace)\n\n        self.inception_4c_branch1_3x3_reduce = nn.Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.inception_4c_branch1_3x3_reduce_bn = nn.BatchNorm3d(128, eps=1e-05, affine=True)\n        self.inception_4c_branch1_relu_3x3_reduce = nn.ReLU(inplace)\n        self.inception_4c_branch1_3x3 = nn.Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n        self.inception_4c_branch1_3x3_bn = nn.BatchNorm3d(256, eps=1e-05, affine=True)\n        self.inception_4c_branch1_relu_3x3 = nn.ReLU(inplace)\n\n        self.inception_4c_branch2_3x3_reduce = nn.Conv3d(512, 24, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.inception_4c_branch2_3x3_reduce_bn = nn.BatchNorm3d(24, eps=1e-05, affine=True)\n        self.inception_4c_branch2_relu_3x3_reduce = nn.ReLU(inplace)\n        self.inception_4c_branch2_3x3 = nn.Conv3d(24, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n        self.inception_4c_branch2_3x3_bn = nn.BatchNorm3d(64, eps=1e-05, affine=True)\n        self.inception_4c_branch2_relu_3x3 = nn.ReLU(inplace)\n\n        # self.inception_4c_pool = nn.MaxPool3d(3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n        self.inception_4c_pool = nn.MaxPool3d(3, stride=1, padding=1, ceil_mode=True)\n        self.inception_4c_pool_proj = nn.Conv3d(512, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.inception_4c_pool_proj_bn = nn.BatchNorm3d(64, eps=1e-05, affine=True)\n        self.inception_4c_relu_pool_proj = nn.ReLU(inplace)\n\n        self.inception_4d_1x1 = nn.Conv3d(512, 112, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.inception_4d_1x1_bn = nn.BatchNorm3d(112, eps=1e-05, affine=True)\n        self.inception_4d_relu_1x1 = nn.ReLU(inplace)\n\n        self.inception_4d_branch1_3x3_reduce = nn.Conv3d(512, 144, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.inception_4d_branch1_3x3_reduce_bn = nn.BatchNorm3d(144, eps=1e-05, affine=True)\n        self.inception_4d_branch1_relu_3x3_reduce = nn.ReLU(inplace)\n        self.inception_4d_branch1_3x3 = nn.Conv3d(144, 288, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n        self.inception_4d_branch1_3x3_bn = nn.BatchNorm3d(288, eps=1e-05, affine=True)\n        self.inception_4d_branch1_relu_3x3 = nn.ReLU(inplace)\n\n        self.inception_4d_branch2_3x3_reduce = nn.Conv3d(512, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.inception_4d_branch2_3x3_reduce_bn = nn.BatchNorm3d(32, eps=1e-05, affine=True)\n        self.inception_4d_branch2_relu_3x3_reduce = nn.ReLU(inplace)\n        self.inception_4d_branch2_3x3 = nn.Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n        self.inception_4d_branch2_3x3_bn = nn.BatchNorm3d(64, eps=1e-05, affine=True)\n        self.inception_4d_branch2_relu_3x3 = nn.ReLU(inplace)\n\n        # self.inception_4d_pool = nn.MaxPool3d(3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n        self.inception_4d_pool = nn.MaxPool3d(3, stride=1, padding=1, ceil_mode=True)\n        self.inception_4d_pool_proj = nn.Conv3d(512, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.inception_4d_pool_proj_bn = nn.BatchNorm3d(64, eps=1e-05, affine=True)\n        self.inception_4d_relu_pool_proj = nn.ReLU(inplace)\n\n        self.inception_4e_1x1 = nn.Conv3d(528, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.inception_4e_1x1_bn = nn.BatchNorm3d(256, eps=1e-05, affine=True)\n        self.inception_4e_relu_1x1 = nn.ReLU(inplace)\n\n        self.inception_4e_branch1_3x3_reduce = nn.Conv3d(528, 160, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.inception_4e_branch1_3x3_reduce_bn = nn.BatchNorm3d(160, eps=1e-05, affine=True)\n        self.inception_4e_branch1_relu_3x3_reduce = nn.ReLU(inplace)\n        self.inception_4e_branch1_3x3 = nn.Conv3d(160, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n        self.inception_4e_branch1_3x3_bn = nn.BatchNorm3d(320, eps=1e-05, affine=True)\n        self.inception_4e_branch1_relu_3x3 = nn.ReLU(inplace)\n\n        self.inception_4e_branch2_3x3_reduce = nn.Conv3d(528, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.inception_4e_branch2_3x3_reduce_bn = nn.BatchNorm3d(32, eps=1e-05, affine=True)\n        self.inception_4e_branch2_relu_3x3_reduce = nn.ReLU(inplace)\n        self.inception_4e_branch2_3x3 = nn.Conv3d(32, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n        self.inception_4e_branch2_3x3_bn = nn.BatchNorm3d(128, eps=1e-05, affine=True)\n        self.inception_4e_branch2_relu_3x3 = nn.ReLU(inplace)\n\n        # self.inception_4e_pool = nn.MaxPool3d(3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n        self.inception_4e_pool = nn.MaxPool3d(3, stride=1, padding=1, ceil_mode=True)\n        self.inception_4e_pool_proj = nn.Conv3d(528, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.inception_4e_pool_proj_bn = nn.BatchNorm3d(128, eps=1e-05, affine=True)\n        self.inception_4e_relu_pool_proj = nn.ReLU(inplace)\n\n        self.inception_4f_pool = nn.MaxPool3d((2, 2, 2), stride=(2, 2, 2), dilation=(1, 1, 1), ceil_mode=True)\n\n        ##########\n        self.inception_5a_1x1 = nn.Conv3d(832, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.inception_5a_1x1_bn = nn.BatchNorm3d(256, eps=1e-05, affine=True)\n        self.inception_5a_relu_1x1 = nn.ReLU(inplace)\n\n        self.inception_5a_branch1_3x3_reduce = nn.Conv3d(832, 160, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.inception_5a_branch1_3x3_reduce_bn = nn.BatchNorm3d(160, eps=1e-05, affine=True)\n        self.inception_5a_branch1_relu_3x3_reduce = nn.ReLU(inplace)\n        self.inception_5a_branch1_3x3 = nn.Conv3d(160, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n        self.inception_5a_branch1_3x3_bn = nn.BatchNorm3d(320, eps=1e-05, affine=True)\n        self.inception_5a_branch1_relu_3x3 = nn.ReLU(inplace)\n\n        self.inception_5a_branch2_3x3_reduce = nn.Conv3d(832, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.inception_5a_branch2_3x3_reduce_bn = nn.BatchNorm3d(32, eps=1e-05, affine=True)\n        self.inception_5a_branch2_relu_3x3_reduce = nn.ReLU(inplace)\n        self.inception_5a_branch2_3x3 = nn.Conv3d(32, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n        self.inception_5a_branch2_3x3_bn = nn.BatchNorm3d(128, eps=1e-05, affine=True)\n        self.inception_5a_branch2_relu_3x3 = nn.ReLU(inplace)\n\n        # self.inception_5a_pool = nn.MaxPool3d(3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n        self.inception_5a_pool = nn.MaxPool3d(3, stride=1, padding=1, ceil_mode=True)\n        self.inception_5a_pool_proj = nn.Conv3d(832, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.inception_5a_pool_proj_bn = nn.BatchNorm3d(128, eps=1e-05, affine=True)\n        self.inception_5a_relu_pool_proj = nn.ReLU(inplace)\n\n        self.inception_5b_1x1 = nn.Conv3d(832, 384, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.inception_5b_1x1_bn = nn.BatchNorm3d(384, eps=1e-05, affine=True)\n        self.inception_5b_relu_1x1 = nn.ReLU(inplace)\n\n        self.inception_5b_branch1_3x3_reduce = nn.Conv3d(832, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.inception_5b_branch1_3x3_reduce_bn = nn.BatchNorm3d(192, eps=1e-05, affine=True)\n        self.inception_5b_branch1_relu_3x3_reduce = nn.ReLU(inplace)\n        self.inception_5b_branch1_3x3 = nn.Conv3d(192, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n        self.inception_5b_branch1_3x3_bn = nn.BatchNorm3d(384, eps=1e-05, affine=True)\n        self.inception_5b_branch1_relu_3x3 = nn.ReLU(inplace)\n\n        self.inception_5b_branch2_3x3_reduce = nn.Conv3d(832, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.inception_5b_branch2_3x3_reduce_bn = nn.BatchNorm3d(48, eps=1e-05, affine=True)\n        self.inception_5b_branch2_relu_3x3_reduce = nn.ReLU(inplace)\n        self.inception_5b_branch2_3x3 = nn.Conv3d(48, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n        self.inception_5b_branch2_3x3_bn = nn.BatchNorm3d(128, eps=1e-05, affine=True)\n        self.inception_5b_branch2_relu_3x3 = nn.ReLU(inplace)\n\n        self.inception_5b_pool = nn.MaxPool3d(3, stride=1, padding=1, dilation=1, ceil_mode=True)\n        self.inception_5b_pool_proj = nn.Conv3d(832, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.inception_5b_pool_proj_bn = nn.BatchNorm3d(128, eps=1e-05,  affine=True)\n        self.inception_5b_relu_pool_proj = nn.ReLU(inplace)\n\n\n    def init_weights(self):\n        if isinstance(self.pretrained, str):\n            logger = logging.getLogger()\n            load_checkpoint(self, self.pretrained, strict=False, logger=logger)\n        elif self.pretrained is None:\n            for m in self.modules():\n                if isinstance(m, nn.Conv3d):\n                    kaiming_init(m)\n                elif isinstance(m, nn.BatchNorm3d):\n                    constant_init(m, 1)\n\n\n    def forward(self, input):\n        conv1_7x7_s2_out = self.conv1_7x7_s2(F.pad(input, (2, 4, 2, 4, 2, 4)))\n        conv1_7x7_s2_bn_out = self.conv1_7x7_s2_bn(conv1_7x7_s2_out)\n        conv1_relu_7x7_out = self.conv1_relu_7x7(conv1_7x7_s2_bn_out)\n        pool1_3x3_s2_out = self.pool1_3x3_s2(conv1_7x7_s2_bn_out)\n        conv2_3x3_reduce_out = self.conv2_3x3_reduce(pool1_3x3_s2_out)\n        conv2_3x3_reduce_bn_out = self.conv2_3x3_reduce_bn(conv2_3x3_reduce_out)\n        conv2_relu_3x3_reduce_out = self.conv2_relu_3x3_reduce(conv2_3x3_reduce_bn_out)\n        conv2_3x3_out = self.conv2_3x3(conv2_3x3_reduce_bn_out)\n        conv2_3x3_bn_out = self.conv2_3x3_bn(conv2_3x3_out)\n        conv2_relu_3x3_out = self.conv2_relu_3x3(conv2_3x3_bn_out)\n        pool2_3x3_s2_out = self.pool2_3x3_s2(conv2_3x3_bn_out)\n\n        inception_3a_1x1_out = self.inception_3a_1x1(pool2_3x3_s2_out)\n        inception_3a_1x1_bn_out = self.inception_3a_1x1_bn(inception_3a_1x1_out)\n        inception_3a_relu_1x1_out = self.inception_3a_relu_1x1(inception_3a_1x1_bn_out)\n        inception_3a_branch1_3x3_reduce_out = self.inception_3a_branch1_3x3_reduce(pool2_3x3_s2_out)\n        inception_3a_branch1_3x3_reduce_bn_out = self.inception_3a_branch1_3x3_reduce_bn(inception_3a_branch1_3x3_reduce_out)\n        inception_3a_branch1_relu_3x3_reduce_out = self.inception_3a_branch1_relu_3x3_reduce(inception_3a_branch1_3x3_reduce_bn_out)\n        inception_3a_branch1_3x3_out = self.inception_3a_branch1_3x3(inception_3a_branch1_3x3_reduce_bn_out)\n        inception_3a_branch1_3x3_bn_out = self.inception_3a_branch1_3x3_bn(inception_3a_branch1_3x3_out)\n        inception_3a_branch1_relu_3x3_out = self.inception_3a_branch1_relu_3x3(inception_3a_branch1_3x3_bn_out)\n        inception_3a_branch2_3x3_reduce_out = self.inception_3a_branch2_3x3_reduce(pool2_3x3_s2_out)\n        inception_3a_branch2_3x3_reduce_bn_out = self.inception_3a_branch2_3x3_reduce_bn(inception_3a_branch2_3x3_reduce_out)\n        inception_3a_branch2_relu_3x3_reduce_out = self.inception_3a_branch2_relu_3x3_reduce(inception_3a_branch2_3x3_reduce_bn_out)\n        inception_3a_branch2_3x3_out = self.inception_3a_branch2_3x3(inception_3a_branch2_3x3_reduce_bn_out)\n        inception_3a_branch2_3x3_bn_out = self.inception_3a_branch2_3x3_bn(inception_3a_branch2_3x3_out)\n        inception_3a_branch2_relu_3x3_out = self.inception_3a_branch2_relu_3x3(inception_3a_branch2_3x3_bn_out)\n        inception_3a_pool_out = self.inception_3a_pool(pool2_3x3_s2_out)\n        inception_3a_pool_proj_out = self.inception_3a_pool_proj(inception_3a_pool_out)\n        inception_3a_pool_proj_bn_out = self.inception_3a_pool_proj_bn(inception_3a_pool_proj_out)\n        inception_3a_relu_pool_proj_out = self.inception_3a_relu_pool_proj(inception_3a_pool_proj_bn_out)\n        inception_3a_output_out = torch.cat([inception_3a_1x1_bn_out,inception_3a_branch1_3x3_bn_out,inception_3a_branch2_3x3_bn_out,inception_3a_pool_proj_bn_out], 1)\n\n\n        inception_3b_1x1_out = self.inception_3b_1x1(inception_3a_output_out)\n        inception_3b_1x1_bn_out = self.inception_3b_1x1_bn(inception_3b_1x1_out)\n        inception_3b_relu_1x1_out = self.inception_3b_relu_1x1(inception_3b_1x1_bn_out)\n        inception_3b_branch1_3x3_reduce_out = self.inception_3b_branch1_3x3_reduce(inception_3a_output_out)\n        inception_3b_branch1_3x3_reduce_bn_out = self.inception_3b_branch1_3x3_reduce_bn(inception_3b_branch1_3x3_reduce_out)\n        inception_3b_branch1_relu_3x3_reduce_out = self.inception_3b_branch1_relu_3x3_reduce(inception_3b_branch1_3x3_reduce_bn_out)\n        inception_3b_branch1_3x3_out = self.inception_3b_branch1_3x3(inception_3b_branch1_3x3_reduce_bn_out)\n        inception_3b_branch1_3x3_bn_out = self.inception_3b_branch1_3x3_bn(inception_3b_branch1_3x3_out)\n        inception_3b_branch1_relu_3x3_out = self.inception_3b_branch1_relu_3x3(inception_3b_branch1_3x3_bn_out)\n        inception_3b_branch2_3x3_reduce_out = self.inception_3b_branch2_3x3_reduce(inception_3a_output_out)\n        inception_3b_branch2_3x3_reduce_bn_out = self.inception_3b_branch2_3x3_reduce_bn(inception_3b_branch2_3x3_reduce_out)\n        inception_3b_branch2_relu_3x3_reduce_out = self.inception_3b_branch2_relu_3x3_reduce(inception_3b_branch2_3x3_reduce_bn_out)\n        inception_3b_branch2_3x3_out = self.inception_3b_branch2_3x3(inception_3b_branch2_3x3_reduce_bn_out)\n        inception_3b_branch2_3x3_bn_out = self.inception_3b_branch2_3x3_bn(inception_3b_branch2_3x3_out)\n        inception_3b_branch2_relu_3x3_out = self.inception_3b_branch2_relu_3x3(inception_3b_branch2_3x3_bn_out)\n        inception_3b_pool_out = self.inception_3b_pool(inception_3a_output_out)\n        inception_3b_pool_proj_out = self.inception_3b_pool_proj(inception_3b_pool_out)\n        inception_3b_pool_proj_bn_out = self.inception_3b_pool_proj_bn(inception_3b_pool_proj_out)\n        inception_3b_relu_pool_proj_out = self.inception_3b_relu_pool_proj(inception_3b_pool_proj_bn_out)\n        inception_3b_output_out = torch.cat([inception_3b_1x1_bn_out,inception_3b_branch1_3x3_bn_out,inception_3b_branch2_3x3_bn_out,inception_3b_pool_proj_bn_out], 1)\n\n\n        inception_3c_pool_out = self.inception_3c_pool(inception_3b_output_out)\n\n        inception_4a_1x1_out = self.inception_4a_1x1(inception_3c_pool_out)\n        inception_4a_1x1_bn_out = self.inception_4a_1x1_bn(inception_4a_1x1_out)\n        inception_4a_relu_1x1_out = self.inception_4a_relu_1x1(inception_4a_1x1_bn_out)\n        inception_4a_branch1_3x3_reduce_out = self.inception_4a_branch1_3x3_reduce(inception_3c_pool_out)\n        inception_4a_branch1_3x3_reduce_bn_out = self.inception_4a_branch1_3x3_reduce_bn(inception_4a_branch1_3x3_reduce_out)\n        inception_4a_branch1_relu_3x3_reduce_out = self.inception_4a_branch1_relu_3x3_reduce(inception_4a_branch1_3x3_reduce_bn_out)\n        inception_4a_branch1_3x3_out = self.inception_4a_branch1_3x3(inception_4a_branch1_3x3_reduce_bn_out)\n        inception_4a_branch1_3x3_bn_out = self.inception_4a_branch1_3x3_bn(inception_4a_branch1_3x3_out)\n        inception_4a_branch1_relu_3x3_out = self.inception_4a_branch1_relu_3x3(inception_4a_branch1_3x3_bn_out)\n        inception_4a_branch2_3x3_reduce_out = self.inception_4a_branch2_3x3_reduce(inception_3c_pool_out)\n        inception_4a_branch2_3x3_reduce_bn_out = self.inception_4a_branch2_3x3_reduce_bn(inception_4a_branch2_3x3_reduce_out)\n        inception_4a_branch2_relu_3x3_reduce_out = self.inception_4a_branch2_relu_3x3_reduce(inception_4a_branch2_3x3_reduce_bn_out)\n        inception_4a_branch2_3x3_out = self.inception_4a_branch2_3x3(inception_4a_branch2_3x3_reduce_bn_out)\n        inception_4a_branch2_3x3_bn_out = self.inception_4a_branch2_3x3_bn(inception_4a_branch2_3x3_out)\n        inception_4a_branch2_relu_3x3_out = self.inception_4a_branch2_relu_3x3(inception_4a_branch2_3x3_bn_out)\n        inception_4a_pool_out = self.inception_4a_pool(inception_3c_pool_out)\n        inception_4a_pool_proj_out = self.inception_4a_pool_proj(inception_4a_pool_out)\n        inception_4a_pool_proj_bn_out = self.inception_4a_pool_proj_bn(inception_4a_pool_proj_out)\n        inception_4a_relu_pool_proj_out = self.inception_4a_relu_pool_proj(inception_4a_pool_proj_bn_out)\n        inception_4a_output_out = torch.cat([inception_4a_1x1_bn_out,inception_4a_branch1_3x3_bn_out,inception_4a_branch2_3x3_bn_out,inception_4a_pool_proj_bn_out], 1)\n\n        inception_4b_1x1_out = self.inception_4b_1x1(inception_4a_output_out)\n        inception_4b_1x1_bn_out = self.inception_4b_1x1_bn(inception_4b_1x1_out)\n        inception_4b_relu_1x1_out = self.inception_4b_relu_1x1(inception_4b_1x1_bn_out)\n        inception_4b_branch1_3x3_reduce_out = self.inception_4b_branch1_3x3_reduce(inception_4a_output_out)\n        inception_4b_branch1_3x3_reduce_bn_out = self.inception_4b_branch1_3x3_reduce_bn(inception_4b_branch1_3x3_reduce_out)\n        inception_4b_branch1_relu_3x3_reduce_out = self.inception_4b_branch1_relu_3x3_reduce(inception_4b_branch1_3x3_reduce_bn_out)\n        inception_4b_branch1_3x3_out = self.inception_4b_branch1_3x3(inception_4b_branch1_3x3_reduce_bn_out)\n        inception_4b_branch1_3x3_bn_out = self.inception_4b_branch1_3x3_bn(inception_4b_branch1_3x3_out)\n        inception_4b_branch1_relu_3x3_out = self.inception_4b_branch1_relu_3x3(inception_4b_branch1_3x3_bn_out)\n        inception_4b_branch2_3x3_reduce_out = self.inception_4b_branch2_3x3_reduce(inception_4a_output_out)\n        inception_4b_branch2_3x3_reduce_bn_out = self.inception_4b_branch2_3x3_reduce_bn(inception_4b_branch2_3x3_reduce_out)\n        inception_4b_branch2_relu_3x3_reduce_out = self.inception_4b_branch2_relu_3x3_reduce(inception_4b_branch2_3x3_reduce_bn_out)\n        inception_4b_branch2_3x3_out = self.inception_4b_branch2_3x3(inception_4b_branch2_3x3_reduce_bn_out)\n        inception_4b_branch2_3x3_bn_out = self.inception_4b_branch2_3x3_bn(inception_4b_branch2_3x3_out)\n        inception_4b_branch2_relu_3x3_out = self.inception_4b_branch2_relu_3x3(inception_4b_branch2_3x3_bn_out)\n        inception_4b_pool_out = self.inception_4b_pool(inception_4a_output_out)\n        inception_4b_pool_proj_out = self.inception_4b_pool_proj(inception_4b_pool_out)\n        inception_4b_pool_proj_bn_out = self.inception_4b_pool_proj_bn(inception_4b_pool_proj_out)\n        inception_4b_relu_pool_proj_out = self.inception_4b_relu_pool_proj(inception_4b_pool_proj_bn_out)\n        inception_4b_output_out = torch.cat([inception_4b_1x1_bn_out,inception_4b_branch1_3x3_bn_out,inception_4b_branch2_3x3_bn_out,inception_4b_pool_proj_bn_out], 1)\n\n        inception_4c_1x1_out = self.inception_4c_1x1(inception_4b_output_out)\n        inception_4c_1x1_bn_out = self.inception_4c_1x1_bn(inception_4c_1x1_out)\n        inception_4c_relu_1x1_out = self.inception_4c_relu_1x1(inception_4c_1x1_bn_out)\n        inception_4c_branch1_3x3_reduce_out = self.inception_4c_branch1_3x3_reduce(inception_4b_output_out)\n        inception_4c_branch1_3x3_reduce_bn_out = self.inception_4c_branch1_3x3_reduce_bn(inception_4c_branch1_3x3_reduce_out)\n        inception_4c_branch1_relu_3x3_reduce_out = self.inception_4c_branch1_relu_3x3_reduce(inception_4c_branch1_3x3_reduce_bn_out)\n        inception_4c_branch1_3x3_out = self.inception_4c_branch1_3x3(inception_4c_branch1_3x3_reduce_bn_out)\n        inception_4c_branch1_3x3_bn_out = self.inception_4c_branch1_3x3_bn(inception_4c_branch1_3x3_out)\n        inception_4c_branch1_relu_3x3_out = self.inception_4c_branch1_relu_3x3(inception_4c_branch1_3x3_bn_out)\n        inception_4c_branch2_3x3_reduce_out = self.inception_4c_branch2_3x3_reduce(inception_4b_output_out)\n        inception_4c_branch2_3x3_reduce_bn_out = self.inception_4c_branch2_3x3_reduce_bn(inception_4c_branch2_3x3_reduce_out)\n        inception_4c_branch2_relu_3x3_reduce_out = self.inception_4c_branch2_relu_3x3_reduce(inception_4c_branch2_3x3_reduce_bn_out)\n        inception_4c_branch2_3x3_out = self.inception_4c_branch2_3x3(inception_4c_branch2_3x3_reduce_bn_out)\n        inception_4c_branch2_3x3_bn_out = self.inception_4c_branch2_3x3_bn(inception_4c_branch2_3x3_out)\n        inception_4c_branch2_relu_3x3_out = self.inception_4c_branch2_relu_3x3(inception_4c_branch2_3x3_bn_out)\n        inception_4c_pool_out = self.inception_4c_pool(inception_4b_output_out)\n        inception_4c_pool_proj_out = self.inception_4c_pool_proj(inception_4c_pool_out)\n        inception_4c_pool_proj_bn_out = self.inception_4c_pool_proj_bn(inception_4c_pool_proj_out)\n        inception_4c_relu_pool_proj_out = self.inception_4c_relu_pool_proj(inception_4c_pool_proj_bn_out)\n        inception_4c_output_out = torch.cat([inception_4c_1x1_bn_out,inception_4c_branch1_3x3_bn_out,inception_4c_branch2_3x3_bn_out,inception_4c_pool_proj_bn_out], 1)\n\n        inception_4d_1x1_out = self.inception_4d_1x1(inception_4c_output_out)\n        inception_4d_1x1_bn_out = self.inception_4d_1x1_bn(inception_4d_1x1_out)\n        inception_4d_relu_1x1_out = self.inception_4d_relu_1x1(inception_4d_1x1_bn_out)\n        inception_4d_branch1_3x3_reduce_out = self.inception_4d_branch1_3x3_reduce(inception_4c_output_out)\n        inception_4d_branch1_3x3_reduce_bn_out = self.inception_4d_branch1_3x3_reduce_bn(inception_4d_branch1_3x3_reduce_out)\n        inception_4d_branch1_relu_3x3_reduce_out = self.inception_4d_branch1_relu_3x3_reduce(inception_4d_branch1_3x3_reduce_bn_out)\n        inception_4d_branch1_3x3_out = self.inception_4d_branch1_3x3(inception_4d_branch1_3x3_reduce_bn_out)\n        inception_4d_branch1_3x3_bn_out = self.inception_4d_branch1_3x3_bn(inception_4d_branch1_3x3_out)\n        inception_4d_branch1_relu_3x3_out = self.inception_4d_branch1_relu_3x3(inception_4d_branch1_3x3_bn_out)\n        inception_4d_branch2_3x3_reduce_out = self.inception_4d_branch2_3x3_reduce(inception_4c_output_out)\n        inception_4d_branch2_3x3_reduce_bn_out = self.inception_4d_branch2_3x3_reduce_bn(inception_4d_branch2_3x3_reduce_out)\n        inception_4d_branch2_relu_3x3_reduce_out = self.inception_4d_branch2_relu_3x3_reduce(inception_4d_branch2_3x3_reduce_bn_out)\n        inception_4d_branch2_3x3_out = self.inception_4d_branch2_3x3(inception_4d_branch2_3x3_reduce_bn_out)\n        inception_4d_branch2_3x3_bn_out = self.inception_4d_branch2_3x3_bn(inception_4d_branch2_3x3_out)\n        inception_4d_branch2_relu_3x3_out = self.inception_4d_branch2_relu_3x3(inception_4d_branch2_3x3_bn_out)\n        inception_4d_pool_out = self.inception_4d_pool(inception_4c_output_out)\n        inception_4d_pool_proj_out = self.inception_4d_pool_proj(inception_4d_pool_out)\n        inception_4d_pool_proj_bn_out = self.inception_4d_pool_proj_bn(inception_4d_pool_proj_out)\n        inception_4d_relu_pool_proj_out = self.inception_4d_relu_pool_proj(inception_4d_pool_proj_bn_out)\n        inception_4d_output_out = torch.cat([inception_4d_1x1_bn_out,inception_4d_branch1_3x3_bn_out,inception_4d_branch2_3x3_bn_out,inception_4d_pool_proj_bn_out], 1)\n\n        inception_4e_1x1_out = self.inception_4e_1x1(inception_4d_output_out)\n        inception_4e_1x1_bn_out = self.inception_4e_1x1_bn(inception_4e_1x1_out)\n        inception_4e_relu_1x1_out = self.inception_4e_relu_1x1(inception_4e_1x1_bn_out)\n        inception_4e_branch1_3x3_reduce_out = self.inception_4e_branch1_3x3_reduce(inception_4d_output_out)\n        inception_4e_branch1_3x3_reduce_bn_out = self.inception_4e_branch1_3x3_reduce_bn(inception_4e_branch1_3x3_reduce_out)\n        inception_4e_branch1_relu_3x3_reduce_out = self.inception_4e_branch1_relu_3x3_reduce(inception_4e_branch1_3x3_reduce_bn_out)\n        inception_4e_branch1_3x3_out = self.inception_4e_branch1_3x3(inception_4e_branch1_3x3_reduce_bn_out)\n        inception_4e_branch1_3x3_bn_out = self.inception_4e_branch1_3x3_bn(inception_4e_branch1_3x3_out)\n        inception_4e_branch1_relu_3x3_out = self.inception_4e_branch1_relu_3x3(inception_4e_branch1_3x3_bn_out)\n        inception_4e_branch2_3x3_reduce_out = self.inception_4e_branch2_3x3_reduce(inception_4d_output_out)\n        inception_4e_branch2_3x3_reduce_bn_out = self.inception_4e_branch2_3x3_reduce_bn(inception_4e_branch2_3x3_reduce_out)\n        inception_4e_branch2_relu_3x3_reduce_out = self.inception_4e_branch2_relu_3x3_reduce(inception_4e_branch2_3x3_reduce_bn_out)\n        inception_4e_branch2_3x3_out = self.inception_4e_branch2_3x3(inception_4e_branch2_3x3_reduce_bn_out)\n        inception_4e_branch2_3x3_bn_out = self.inception_4e_branch2_3x3_bn(inception_4e_branch2_3x3_out)\n        inception_4e_branch2_relu_3x3_out = self.inception_4e_branch2_relu_3x3(inception_4e_branch2_3x3_bn_out)\n        inception_4e_pool_out = self.inception_4e_pool(inception_4d_output_out)\n        inception_4e_pool_proj_out = self.inception_4e_pool_proj(inception_4e_pool_out)\n        inception_4e_pool_proj_bn_out = self.inception_4e_pool_proj_bn(inception_4e_pool_proj_out)\n        inception_4e_relu_pool_proj_out = self.inception_4e_relu_pool_proj(inception_4e_pool_proj_bn_out)\n        inception_4e_output_out = torch.cat([inception_4e_1x1_bn_out,inception_4e_branch1_3x3_bn_out,inception_4e_branch2_3x3_bn_out,inception_4e_pool_proj_bn_out], 1)\n\n        inception_4f_pool_out = self.inception_4f_pool(inception_4e_output_out)\n\n        inception_5a_1x1_out = self.inception_5a_1x1(inception_4f_pool_out)\n        inception_5a_1x1_bn_out = self.inception_5a_1x1_bn(inception_5a_1x1_out)\n        inception_5a_relu_1x1_out = self.inception_5a_relu_1x1(inception_5a_1x1_bn_out)\n        inception_5a_branch1_3x3_reduce_out = self.inception_5a_branch1_3x3_reduce(inception_4f_pool_out)\n        inception_5a_branch1_3x3_reduce_bn_out = self.inception_5a_branch1_3x3_reduce_bn(inception_5a_branch1_3x3_reduce_out)\n        inception_5a_branch1_relu_3x3_reduce_out = self.inception_5a_branch1_relu_3x3_reduce(inception_5a_branch1_3x3_reduce_bn_out)\n        inception_5a_branch1_3x3_out = self.inception_5a_branch1_3x3(inception_5a_branch1_3x3_reduce_bn_out)\n        inception_5a_branch1_3x3_bn_out = self.inception_5a_branch1_3x3_bn(inception_5a_branch1_3x3_out)\n        inception_5a_branch1_relu_3x3_out = self.inception_5a_branch1_relu_3x3(inception_5a_branch1_3x3_bn_out)\n        inception_5a_branch2_3x3_reduce_out = self.inception_5a_branch2_3x3_reduce(inception_4f_pool_out)\n        inception_5a_branch2_3x3_reduce_bn_out = self.inception_5a_branch2_3x3_reduce_bn(inception_5a_branch2_3x3_reduce_out)\n        inception_5a_branch2_relu_3x3_reduce_out = self.inception_5a_branch2_relu_3x3_reduce(inception_5a_branch2_3x3_reduce_bn_out)\n        inception_5a_branch2_3x3_out = self.inception_5a_branch2_3x3(inception_5a_branch2_3x3_reduce_bn_out)\n        inception_5a_branch2_3x3_bn_out = self.inception_5a_branch2_3x3_bn(inception_5a_branch2_3x3_out)\n        inception_5a_branch2_relu_3x3_out = self.inception_5a_branch2_relu_3x3(inception_5a_branch2_3x3_bn_out)\n        inception_5a_pool_out = self.inception_5a_pool(inception_4f_pool_out)\n        inception_5a_pool_proj_out = self.inception_5a_pool_proj(inception_5a_pool_out)\n        inception_5a_pool_proj_bn_out = self.inception_5a_pool_proj_bn(inception_5a_pool_proj_out)\n        inception_5a_relu_pool_proj_out = self.inception_5a_relu_pool_proj(inception_5a_pool_proj_bn_out)\n        inception_5a_output_out = torch.cat([inception_5a_1x1_bn_out,inception_5a_branch1_3x3_bn_out,inception_5a_branch2_3x3_bn_out,inception_5a_pool_proj_bn_out], 1)\n\n        inception_5b_1x1_out = self.inception_5b_1x1(inception_5a_output_out)\n        inception_5b_1x1_bn_out = self.inception_5b_1x1_bn(inception_5b_1x1_out)\n        inception_5b_relu_1x1_out = self.inception_5b_relu_1x1(inception_5b_1x1_bn_out)\n        inception_5b_branch1_3x3_reduce_out = self.inception_5b_branch1_3x3_reduce(inception_5a_output_out)\n        inception_5b_branch1_3x3_reduce_bn_out = self.inception_5b_branch1_3x3_reduce_bn(inception_5b_branch1_3x3_reduce_out)\n        inception_5b_branch1_relu_3x3_reduce_out = self.inception_5b_branch1_relu_3x3_reduce(inception_5b_branch1_3x3_reduce_bn_out)\n        inception_5b_branch1_3x3_out = self.inception_5b_branch1_3x3(inception_5b_branch1_3x3_reduce_bn_out)\n        inception_5b_branch1_3x3_bn_out = self.inception_5b_branch1_3x3_bn(inception_5b_branch1_3x3_out)\n        inception_5b_branch1_relu_3x3_out = self.inception_5b_branch1_relu_3x3(inception_5b_branch1_3x3_bn_out)\n        inception_5b_branch2_3x3_reduce_out = self.inception_5b_branch2_3x3_reduce(inception_5a_output_out)\n        inception_5b_branch2_3x3_reduce_bn_out = self.inception_5b_branch2_3x3_reduce_bn(inception_5b_branch2_3x3_reduce_out)\n        inception_5b_branch2_relu_3x3_reduce_out = self.inception_5b_branch2_relu_3x3_reduce(inception_5b_branch2_3x3_reduce_bn_out)\n        inception_5b_branch2_3x3_out = self.inception_5b_branch2_3x3(inception_5b_branch2_3x3_reduce_bn_out)\n        inception_5b_branch2_3x3_bn_out = self.inception_5b_branch2_3x3_bn(inception_5b_branch2_3x3_out)\n        inception_5b_branch2_relu_3x3_out = self.inception_5b_branch2_relu_3x3(inception_5b_branch2_3x3_bn_out)\n        inception_5b_pool_out = self.inception_5b_pool(inception_5a_output_out)\n        inception_5b_pool_proj_out = self.inception_5b_pool_proj(inception_5b_pool_out)\n        inception_5b_pool_proj_bn_out = self.inception_5b_pool_proj_bn(inception_5b_pool_proj_out)\n        inception_5b_relu_pool_proj_out = self.inception_5b_relu_pool_proj(inception_5b_pool_proj_bn_out)\n        inception_5b_output_out = torch.cat([inception_5b_1x1_bn_out,inception_5b_branch1_3x3_bn_out,inception_5b_branch2_3x3_bn_out,inception_5b_pool_proj_bn_out], 1)\n        return inception_5b_output_out\n\n    def train(self, mode=True):\n        super(InceptionV1_I3D, self).train(mode)\n        if self.bn_eval:\n            for m in self.modules():\n                if isinstance(m, nn.BatchNorm3d):\n                    m.eval()\n                    if self.bn_frozen:\n                        for params in m.parameters():\n                            params.requires_grad = False\n        if self.partial_bn:\n            for n, m in self.named_modules():\n                if 'conv1' not in n and isinstance(m, nn.BatchNorm3d):\n                    m.eval()\n                    m.weight.requires_grad = False\n                    m.bias.requires_grad = False\n"""
mmaction/models/tenons/backbones/resnet.py,2,"b'import logging\n\nimport torch.nn as nn\nimport torch.utils.checkpoint as cp\n\nfrom mmcv.cnn import constant_init, kaiming_init\nfrom mmcv.runner import load_checkpoint\n\nfrom ...registry import BACKBONES\n\n\ndef conv3x3(in_planes, out_planes, stride=1, dilation=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=stride,\n        padding=dilation,\n        dilation=dilation,\n        bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 stride=1,\n                 dilation=1,\n                 downsample=None,\n                 style=\'pytorch\',\n                 with_cp=False):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride, dilation)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n        self.dilation = dilation\n        assert not with_cp\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 stride=1,\n                 dilation=1,\n                 downsample=None,\n                 style=\'pytorch\',\n                 with_cp=False):\n        """"""Bottleneck block for ResNet.\n        If style is ""pytorch"", the stride-two layer is the 3x3 conv layer,\n        if it is ""caffe"", the stride-two layer is the first 1x1 conv layer.\n        """"""\n        super(Bottleneck, self).__init__()\n        assert style in [\'pytorch\', \'caffe\']\n        self.inplanes = inplanes\n        self.planes = planes\n        if style == \'pytorch\':\n            self.conv1_stride = 1\n            self.conv2_stride = stride\n        else:\n            self.conv1_stride = stride\n            self.conv2_stride = 1\n        self.conv1 = nn.Conv2d(\n            inplanes,\n            planes,\n            kernel_size=1,\n            stride=self.conv1_stride,\n            bias=False)\n        self.conv2 = nn.Conv2d(\n            planes,\n            planes,\n            kernel_size=3,\n            stride=self.conv2_stride,\n            padding=dilation,\n            dilation=dilation,\n            bias=False)\n\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(\n            planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.dilation = dilation\n        self.with_cp = with_cp\n\n    def forward(self, x):\n\n        def _inner_forward(x):\n            identity = x\n\n            out = self.conv1(x)\n            out = self.bn1(out)\n            out = self.relu(out)\n\n            out = self.conv2(out)\n            out = self.bn2(out)\n            out = self.relu(out)\n\n            out = self.conv3(out)\n            out = self.bn3(out)\n\n            if self.downsample is not None:\n                identity = self.downsample(x)\n\n            out += identity\n\n            return out\n\n        if self.with_cp and x.requires_grad:\n            out = cp.checkpoint(_inner_forward, x)\n        else:\n            out = _inner_forward(x)\n\n        out = self.relu(out)\n\n        return out\n\n\ndef make_res_layer(block,\n                   inplanes,\n                   planes,\n                   blocks,\n                   stride=1,\n                   dilation=1,\n                   style=\'pytorch\',\n                   with_cp=False):\n    downsample = None\n    if stride != 1 or inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv2d(\n                inplanes,\n                planes * block.expansion,\n                kernel_size=1,\n                stride=stride,\n                bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(\n        block(\n            inplanes,\n            planes,\n            stride,\n            dilation,\n            downsample,\n            style=style,\n            with_cp=with_cp))\n    inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(\n            block(inplanes, planes, 1, dilation, style=style, with_cp=with_cp))\n\n    return nn.Sequential(*layers)\n\n\n@BACKBONES.register_module\nclass ResNet(nn.Module):\n    """"""ResNet backbone.\n\n    Args:\n        depth (int): Depth of resnet, from {18, 34, 50, 101, 152}.\n        num_stages (int): Resnet stages, normally 4.\n        strides (Sequence[int]): Strides of the first block of each stage.\n        dilations (Sequence[int]): Dilation of each stage.\n        out_indices (Sequence[int]): Output from which stages.\n        style (str): `pytorch` or `caffe`. If set to ""pytorch"", the stride-two\n            layer is the 3x3 conv layer, otherwise the stride-two layer is\n            the first 1x1 conv layer.\n        frozen_stages (int): Stages to be frozen (all param fixed). -1 means\n            not freezing any parameters.\n        bn_eval (bool): Whether to set BN layers to eval mode, namely, freeze\n            running stats (mean and var).\n        bn_frozen (bool): Whether to freeze weight and bias of BN layers.\n        partial_bn (bool): Whether to freeze weight and bias of **all but the first** BN layers.\n        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n            memory while slowing down the training speed.\n    """"""\n\n    arch_settings = {\n        18: (BasicBlock, (2, 2, 2, 2)),\n        34: (BasicBlock, (3, 4, 6, 3)),\n        50: (Bottleneck, (3, 4, 6, 3)),\n        101: (Bottleneck, (3, 4, 23, 3)),\n        152: (Bottleneck, (3, 8, 36, 3))\n    }\n\n    def __init__(self,\n                 depth,\n                 pretrained=None,\n                 num_stages=4,\n                 strides=(1, 2, 2, 2),\n                 dilations=(1, 1, 1, 1),\n                 out_indices=(0, 1, 2, 3),\n                 style=\'pytorch\',\n                 frozen_stages=-1,\n                 bn_eval=True,\n                 bn_frozen=False,\n                 partial_bn=False,\n                 with_cp=False):\n        super(ResNet, self).__init__()\n        if depth not in self.arch_settings:\n            raise KeyError(\'invalid depth {} for resnet\'.format(depth))\n        self.depth = depth\n        self.pretrained = pretrained\n        self.num_stages = num_stages\n        assert num_stages >= 1 and num_stages <= 4\n        self.strides = strides\n        self.dilations = dilations\n        assert len(strides) == len(dilations) == num_stages\n        self.out_indices = out_indices\n        assert max(out_indices) < num_stages\n        self.style = style\n        self.frozen_stages = frozen_stages\n        self.bn_eval = bn_eval\n        self.bn_frozen = bn_frozen\n        self.partial_bn = partial_bn\n        self.with_cp = with_cp\n\n        self.block, stage_blocks = self.arch_settings[depth]\n        self.stage_blocks = stage_blocks[:num_stages]\n        self.inplanes = 64\n\n        self.conv1 = nn.Conv2d(\n            3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.res_layers = []\n        for i, num_blocks in enumerate(self.stage_blocks):\n            stride = strides[i]\n            dilation = dilations[i]\n            planes = 64 * 2**i\n            res_layer = make_res_layer(\n                self.block,\n                self.inplanes,\n                planes,\n                num_blocks,\n                stride=stride,\n                dilation=dilation,\n                style=self.style,\n                with_cp=with_cp)\n            self.inplanes = planes * self.block.expansion\n            layer_name = \'layer{}\'.format(i + 1)\n            self.add_module(layer_name, res_layer)\n            self.res_layers.append(layer_name)\n\n        self.feat_dim = self.block.expansion * 64 * 2**(\n            len(self.stage_blocks) - 1)\n\n    def init_weights(self):\n        if isinstance(self.pretrained, str):\n            logger = logging.getLogger()\n            load_checkpoint(self, self.pretrained, strict=False, logger=logger)\n        elif self.pretrained is None:\n            for m in self.modules():\n                if isinstance(m, nn.Conv2d):\n                    kaiming_init(m)\n                elif isinstance(m, nn.BatchNorm2d):\n                    constant_init(m, 1)\n        else:\n            raise TypeError(\'pretrained must be a str or None\')\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        outs = []\n        for i, layer_name in enumerate(self.res_layers):\n            res_layer = getattr(self, layer_name)\n            x = res_layer(x)\n            if i in self.out_indices:\n                outs.append(x)\n        if len(outs) == 1:\n            return outs[0]\n        else:\n            return tuple(outs)\n\n    def train(self, mode=True):\n        super(ResNet, self).train(mode)\n        if self.bn_eval:\n            for m in self.modules():\n                if isinstance(m, nn.BatchNorm2d):\n                    m.eval()\n                    if self.bn_frozen:\n                        for params in m.parameters():\n                            params.requires_grad = False\n        if self.partial_bn:\n            for i in range(1, self.frozen_stages + 1):\n                mod = getattr(self, \'layer{}\'.format(i))\n                for m in mod.modules():\n                    if isinstance(m, nn.BatchNorm2d):\n                        m.eval()\n                        m.weight.requires_grad = False\n                        m.bias.requires_grad = False\n        if mode and self.frozen_stages >= 0:\n            for param in self.conv1.parameters():\n                param.requires_grad = False\n            for param in self.bn1.parameters():\n                param.requires_grad = False\n            self.bn1.eval()\n            self.bn1.weight.requires_grad = False\n            self.bn1.bias.requires_grad = False\n            for i in range(1, self.frozen_stages + 1):\n                mod = getattr(self, \'layer{}\'.format(i))\n                mod.eval()\n                for param in mod.parameters():\n                    param.requires_grad = False\n'"
mmaction/models/tenons/backbones/resnet_i3d.py,2,"b'import logging\n\nimport torch.nn as nn\nimport torch.utils.checkpoint as cp\n\nfrom ....utils.misc import rgetattr, rhasattr\nfrom .resnet import ResNet\nfrom mmcv.cnn import constant_init, kaiming_init\nfrom mmcv.runner import load_checkpoint\n\nfrom ..utils.nonlocal_block import build_nonlocal_block\nfrom ..spatial_temporal_modules.non_local import NonLocalModule\n\nfrom ...registry import BACKBONES\n\ndef conv3x3x3(in_planes, out_planes, spatial_stride=1, temporal_stride=1, dilation=1):\n    ""3x3x3 convolution with padding""\n    return nn.Conv3d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=(temporal_stride, spatial_stride, spatial_stride),\n        padding=dilation,\n        dilation=dilation,\n        bias=False)\n\n\ndef conv1x3x3(in_planes, out_planes, spatial_stride=1, temporal_stride=1, dilation=1):\n    ""1x3x3 convolution with padding""\n    return nn.Conv3d(\n        in_planes,\n        out_planes,\n        kernel_size=(1,3,3),\n        stride=(temporal_stride, spatial_stride, spatial_stride),\n        padding=(0, dilation, dilation),\n        dilation=dilation,\n        bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 spatial_stride=1,\n                 temporal_stride=1,\n                 dilation=1,\n                 downsample=None,\n                 style=\'pytorch\',\n                 if_inflate=True,\n                 inflate_style=None,\n                 with_cp=False):\n        super(BasicBlock, self).__init__()\n        if if_inflate:\n            self.conv1 = conv3x3x3(inplanes, planes, spatial_stride, temporal_stride, dilation)\n        else:\n            self.conv1 = conv1x3x3(inplanes, planes, spatial_stride, temporal_stride, dilation)\n        self.bn1 = nn.BatchNorm3d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        if if_inflate:\n            self.conv2 = conv3x3x3(planes, planes)\n        else:\n            self.conv2 = conv1x3x3(planes, planes)\n        self.bn2 = nn.BatchNorm3d(planes)\n        self.downsample = downsample\n        self.spatial_stride = spatial_stride\n        self.temporal_stride = temporal_stride\n        self.dilation = dilation\n        assert not with_cp\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 spatial_stride=1,\n                 temporal_stride=1,\n                 dilation=1,\n                 downsample=None,\n                 style=\'pytorch\',\n                 if_inflate=True,\n                 inflate_style=\'3x1x1\',\n                 if_nonlocal=True,\n                 nonlocal_cfg=None,\n                 with_cp=False):\n        """"""Bottleneck block for ResNet.\n        If style is ""pytorch"", the stride-two layer is the 3x3 conv layer,\n        if it is ""caffe"", the stride-two layer is the first 1x1 conv layer.\n        """"""\n        super(Bottleneck, self).__init__()\n        assert style in [\'pytorch\', \'caffe\']\n        assert inflate_style in [\'3x1x1\', \'3x3x3\']\n        self.inplanes = inplanes\n        self.planes = planes\n        if style == \'pytorch\':\n            self.conv1_stride = 1\n            self.conv2_stride = spatial_stride\n            self.conv1_stride_t = 1\n            self.conv2_stride_t = temporal_stride\n        else:\n            self.conv1_stride = spatial_stride\n            self.conv2_stride = 1\n            self.conv1_stride_t = temporal_stride\n            self.conv2_stride_t = 1\n        if if_inflate:\n          if inflate_style == \'3x1x1\':\n              self.conv1 = nn.Conv3d(\n                  inplanes,\n                  planes,\n                  kernel_size=(3,1,1),\n                  stride=(self.conv1_stride_t, self.conv1_stride, self.conv1_stride),\n                  padding=(1,0,0),\n                  bias=False)\n              self.conv2 = nn.Conv3d(\n                  planes,\n                  planes,\n                  kernel_size=(1,3,3),\n                  stride=(self.conv2_stride_t, self.conv2_stride, self.conv2_stride),\n                  padding=(0, dilation, dilation),\n                  dilation=(1, dilation, dilation),\n                  bias=False)\n          else:\n              self.conv1 = nn.Conv3d(\n                  inplanes,\n                  planes,\n                  kernel_size=1,\n                  stride=(self.conv1_stride_t, self.conv1_stride, self.conv1_stride),\n                  bias=False)\n              self.conv2 = nn.Conv3d(\n                  planes,\n                  planes,\n                  kernel_size=3,\n                  stride=(self.conv2_stride_t, self.conv2_stride, self.conv2_stride),\n                  padding=(1, dilation, dilation),\n                  dilation=(1, dilation, dilation),\n                  bias=False)\n        else:\n            self.conv1 = nn.Conv3d(\n                inplanes,\n                planes,\n                kernel_size=1,\n                stride=(1, self.conv1_stride, self.conv1_stride),\n                bias=False)\n            self.conv2 = nn.Conv3d(\n                planes,\n                planes,\n                kernel_size=(1,3,3),\n                stride=(1, self.conv2_stride, self.conv2_stride),\n                padding=(0, dilation, dilation),\n                dilation=(1, dilation, dilation),\n                bias=False)\n\n        self.bn1 = nn.BatchNorm3d(planes)\n        self.bn2 = nn.BatchNorm3d(planes)\n        self.conv3 = nn.Conv3d(\n            planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm3d(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.spatial_tride = spatial_stride\n        self.temporal_tride = temporal_stride\n        self.dilation = dilation\n        self.with_cp = with_cp\n\n        if if_nonlocal and nonlocal_cfg is not None:\n            nonlocal_cfg_ = nonlocal_cfg.copy()\n            nonlocal_cfg_[\'in_channels\'] = planes * self.expansion\n            self.nonlocal_block = build_nonlocal_block(nonlocal_cfg_)\n        else:\n            self.nonlocal_block = None\n\n    def forward(self, x):\n\n        def _inner_forward(x):\n            identity = x\n\n            out = self.conv1(x)\n            out = self.bn1(out)\n            out = self.relu(out)\n\n            out = self.conv2(out)\n            out = self.bn2(out)\n            out = self.relu(out)\n\n            out = self.conv3(out)\n            out = self.bn3(out)\n\n            if self.downsample is not None:\n                identity = self.downsample(x)\n\n            out += identity\n\n            return out\n\n        if self.with_cp and x.requires_grad:\n            out = cp.checkpoint(_inner_forward, x)\n        else:\n            out = _inner_forward(x)\n\n        out = self.relu(out)\n\n        if self.nonlocal_block is not None:\n            out = self.nonlocal_block(out)\n\n        return out\n\n\ndef make_res_layer(block,\n                   inplanes,\n                   planes,\n                   blocks,\n                   spatial_stride=1,\n                   temporal_stride=1,\n                   dilation=1,\n                   style=\'pytorch\',\n                   inflate_freq=1,\n                   inflate_style=\'3x1x1\',\n                   nonlocal_freq=1,\n                   nonlocal_cfg=None,\n                   with_cp=False):\n    inflate_freq = inflate_freq if not isinstance(inflate_freq, int) else (inflate_freq, ) * blocks\n    nonlocal_freq = nonlocal_freq if not isinstance(nonlocal_freq, int) else (nonlocal_freq, ) * blocks\n    assert len(inflate_freq) == blocks\n    assert len(nonlocal_freq) == blocks\n    downsample = None\n    if spatial_stride != 1 or inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv3d(\n                inplanes,\n                planes * block.expansion,\n                kernel_size=1,\n                stride=(temporal_stride, spatial_stride, spatial_stride),\n                bias=False),\n            nn.BatchNorm3d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(\n        block(\n            inplanes,\n            planes,\n            spatial_stride,\n            temporal_stride,\n            dilation,\n            downsample,\n            style=style,\n            if_inflate= (inflate_freq[0] == 1),\n            inflate_style=inflate_style,\n            if_nonlocal= (nonlocal_freq[0] == 1),\n            nonlocal_cfg=nonlocal_cfg,\n            with_cp=with_cp))\n    inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(\n            block(inplanes,\n                planes,\n                1, 1,\n                dilation,\n                style=style,\n                if_inflate= (inflate_freq[i] == 1),\n                inflate_style=inflate_style,\n                if_nonlocal= (nonlocal_freq[i] == 1),\n                nonlocal_cfg=nonlocal_cfg,\n                with_cp=with_cp))\n\n    return nn.Sequential(*layers)\n\n\n@BACKBONES.register_module\nclass ResNet_I3D(nn.Module):\n    """"""ResNet_I3D backbone.\n\n    Args:\n        depth (int): Depth of resnet, from {18, 34, 50, 101, 152}.\n        num_stages (int): Resnet stages, normally 4.\n        strides (Sequence[int]): Strides of the first block of each stage.\n        dilations (Sequence[int]): Dilation of each stage.\n        out_indices (Sequence[int]): Output from which stages.\n        style (str): `pytorch` or `caffe`. If set to ""pytorch"", the stride-two\n            layer is the 3x3 conv layer, otherwise the stride-two layer is\n            the first 1x1 conv layer.\n        frozen_stages (int): Stages to be frozen (all param fixed). -1 means\n            not freezing any parameters.\n        bn_eval (bool): Whether to set BN layers to eval mode, namely, freeze\n            running stats (mean and var).\n        bn_frozen (bool): Whether to freeze weight and bias of BN layers.\n        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n            memory while slowing down the training speed.\n    """"""\n\n    arch_settings = {\n        18: (BasicBlock, (2, 2, 2, 2)),\n        34: (BasicBlock, (3, 4, 6, 3)),\n        50: (Bottleneck, (3, 4, 6, 3)),\n        101: (Bottleneck, (3, 4, 23, 3)),\n        152: (Bottleneck, (3, 8, 36, 3))\n    }\n\n    def __init__(self,\n                 depth,\n                 pretrained=None,\n                 pretrained2d=True,\n                 num_stages=4,\n                 spatial_strides=(1, 2, 2, 2),\n                 temporal_strides=(1, 1, 1, 1),\n                 dilations=(1, 1, 1, 1),\n                 out_indices=(0, 1, 2, 3),\n                 conv1_kernel_t=5,\n                 conv1_stride_t=2,\n                 pool1_kernel_t=1,\n                 pool1_stride_t=2,\n                 style=\'pytorch\',\n                 frozen_stages=-1,\n                 inflate_freq=(1, 1, 1, 1),    # For C2D baseline, this is set to -1.\n                 inflate_stride=(1, 1, 1, 1),\n                 inflate_style=\'3x1x1\',\n                 nonlocal_stages=(-1, ),\n                 nonlocal_freq=(0, 1, 1, 0),\n                 nonlocal_cfg=None,\n                 no_pool2=False,\n                 bn_eval=True,\n                 bn_frozen=False,\n                 partial_bn=False,\n                 with_cp=False):\n        super(ResNet_I3D, self).__init__()\n        if depth not in self.arch_settings:\n            raise KeyError(\'invalid depth {} for resnet\'.format(depth))\n        self.depth = depth\n        self.pretrained = pretrained\n        self.pretrained2d = pretrained2d\n        self.num_stages = num_stages\n        assert num_stages >= 1 and num_stages <= 4\n        self.spatial_strides = spatial_strides\n        self.temporal_strides = temporal_strides\n        self.dilations = dilations\n        assert len(spatial_strides) == len(temporal_strides) == len(dilations) == num_stages\n        self.out_indices = out_indices\n        assert max(out_indices) < num_stages\n        self.style = style\n        self.frozen_stages = frozen_stages\n        self.inflate_freqs = inflate_freq if not isinstance(inflate_freq, int) else (inflate_freq, ) * num_stages\n        self.inflate_style = inflate_style\n        self.nonlocal_stages = nonlocal_stages\n        self.nonlocal_freqs = nonlocal_freq if not isinstance(nonlocal_freq, int) else (nonlocal_freq, ) * num_stages\n        self.nonlocal_cfg = nonlocal_cfg\n        self.bn_eval = bn_eval\n        self.bn_frozen = bn_frozen\n        self.partial_bn = partial_bn\n        self.with_cp = with_cp\n\n        self.block, stage_blocks = self.arch_settings[depth]\n        self.stage_blocks = stage_blocks[:num_stages]\n        self.inplanes = 64\n\n\n        self.conv1 = nn.Conv3d(\n            3, 64, kernel_size=(conv1_kernel_t,7,7), stride=(conv1_stride_t,2,2), padding=((conv1_kernel_t-1)//2,3,3), bias=False)\n        self.bn1 = nn.BatchNorm3d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool3d(kernel_size=(pool1_kernel_t,3,3), stride=(pool1_stride_t,2,2), padding=(pool1_kernel_t//2,1,1))\n\t #TODO: Check whether pad=0 differs a lot\n        self.pool2 = nn.MaxPool3d(kernel_size=(2,1,1), stride=(2,1,1), padding=(0,0,0))\n        self.no_pool2 = no_pool2\n\n        self.res_layers = []\n        for i, num_blocks in enumerate(self.stage_blocks):\n            spatial_stride = spatial_strides[i]\n            temporal_stride = temporal_strides[i]\n            dilation = dilations[i]\n            planes = 64 * 2**i\n            res_layer = make_res_layer(\n                self.block,\n                self.inplanes,\n                planes,\n                num_blocks,\n                spatial_stride=spatial_stride,\n                temporal_stride=temporal_stride,\n                dilation=dilation,\n                style=self.style,\n                inflate_freq=self.inflate_freqs[i],\n                inflate_style=self.inflate_style,\n                nonlocal_freq=self.nonlocal_freqs[i],\n                nonlocal_cfg=self.nonlocal_cfg if i in self.nonlocal_stages else None,\n                with_cp=with_cp)\n            self.inplanes = planes * self.block.expansion\n            layer_name = \'layer{}\'.format(i + 1)\n            self.add_module(layer_name, res_layer)\n            self.res_layers.append(layer_name)\n\n        self.feat_dim = self.block.expansion * 64 * 2**(\n            len(self.stage_blocks) - 1)\n\n    def init_weights(self):\n        if isinstance(self.pretrained, str):\n            logger = logging.getLogger()\n            if self.pretrained2d:\n                resnet2d = ResNet(self.depth)\n                load_checkpoint(resnet2d, self.pretrained, strict=False, logger=logger)\n                for name, module in self.named_modules():\n                    if isinstance(module, NonLocalModule):\n                        module.init_weights()\n                    elif isinstance(module, nn.Conv3d) and rhasattr(resnet2d, name):\n                        new_weight = rgetattr(resnet2d, name).weight.data.unsqueeze(2).expand_as(module.weight) / module.weight.data.shape[2]\n                        module.weight.data.copy_(new_weight)\n                        logging.info(""{}.weight loaded from weights file into {}"".format(name, new_weight.shape))\n                        if hasattr(module, \'bias\') and module.bias is not None:\n                            new_bias = rgetattr(resnet2d, name).bias.data\n                            module.bias.data.copy_(new_bias)\n                            logging.info(""{}.bias loaded from weights file into {}"".format(name, new_bias.shape))\n                    elif isinstance(module, nn.BatchNorm3d) and rhasattr(resnet2d, name):\n                          for attr in [\'weight\', \'bias\', \'running_mean\', \'running_var\']:\n                              logging.info(""{}.{} loaded from weights file into {}"".format(name, attr, getattr(rgetattr(resnet2d, name), attr).shape))\n                              setattr(module, attr, getattr(rgetattr(resnet2d, name), attr))\n            else:\n                load_checkpoint(self, self.pretrained, strict=False, logger=logger)\n        elif self.pretrained is None:\n            for m in self.modules():\n                if isinstance(m, nn.Conv3d):\n                    kaiming_init(m)\n                elif isinstance(m, nn.BatchNorm3d):\n                    constant_init(m, 1)\n        else:\n            raise TypeError(\'pretrained must be a str or None\')\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        outs = []\n        for i, layer_name in enumerate(self.res_layers):\n            res_layer = getattr(self, layer_name)\n            x = res_layer(x)\n            if i in self.out_indices:\n                outs.append(x)\n            if self.no_pool2:\n                pass\n            else:\n                if i == 0:\n                    x = self.pool2(x)\n        if len(outs) == 1:\n            return outs[0]\n        else:\n            return tuple(outs)\n\n    def train(self, mode=True):\n        super(ResNet_I3D, self).train(mode)\n        if self.bn_eval:\n            for m in self.modules():\n                if isinstance(m, nn.BatchNorm3d):\n                    m.eval()\n                    if self.bn_frozen:\n                        for params in m.parameters():\n                            params.requires_grad = False\n        if self.partial_bn:\n            for i in range(1, self.frozen_stages + 1):\n                mod = getattr(self, \'layer{}\'.format(i))\n                for m in mod.modules():\n                    if isinstance(m, nn.BatchNorm3d):\n                        m.eval()\n                        m.weight.requires_grad = False\n                        m.bias.requires_grad = False\n        if mode and self.frozen_stages >= 0:\n            for param in self.conv1.parameters():\n                param.requires_grad = False\n            for param in self.bn1.parameters():\n                param.requires_grad = False\n            self.bn1.eval()\n            self.bn1.weight.requires_grad = False\n            self.bn1.bias.requires_grad = False\n            for i in range(1, self.frozen_stages + 1):\n                mod = getattr(self, \'layer{}\'.format(i))\n                mod.eval()\n                for param in mod.parameters():\n                    param.requires_grad = False\n'"
mmaction/models/tenons/backbones/resnet_i3d_slowfast.py,6,"b'import logging\nimport torch\nimport torch.nn as nn\nimport torch.utils.checkpoint as cp\n\nfrom ....utils.misc import rgetattr, rhasattr\nfrom .resnet import ResNet\nfrom mmcv.cnn import constant_init, kaiming_init\nfrom mmcv.runner import load_checkpoint\n\nfrom ..utils.nonlocal_block import build_nonlocal_block\nfrom ..spatial_temporal_modules.non_local import NonLocalModule\n\nfrom ...registry import BACKBONES\nfrom .resnet_i3d import conv3x3x3, conv1x3x3, BasicBlock, Bottleneck\n\n\ndef make_res_layer(block,\n                   inplanes,\n                   planes,\n                   blocks,\n                   # from another pathway\n                   lateral_inplanes=0,\n                   spatial_stride=1,\n                   temporal_stride=1,\n                   dilation=1,\n                   style=\'pytorch\',\n                   inflate_freq=1,\n                   inflate_style=\'3x1x1\',\n                   nonlocal_freq=1,\n                   nonlocal_cfg=None,\n                   with_cp=False):\n    inflate_freq = inflate_freq if not isinstance(inflate_freq, int) else (inflate_freq, ) * blocks\n    nonlocal_freq = nonlocal_freq if not isinstance(nonlocal_freq, int) else (nonlocal_freq, ) * blocks\n    assert len(inflate_freq) == blocks\n    assert len(nonlocal_freq) == blocks\n    downsample = None\n    if spatial_stride != 1 or inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv3d(\n                inplanes + lateral_inplanes,\n                planes * block.expansion,\n                kernel_size=1,\n                stride=(temporal_stride, spatial_stride, spatial_stride),\n                bias=False),\n            nn.BatchNorm3d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(\n        block(\n            inplanes + lateral_inplanes,\n            planes,\n            spatial_stride,\n            temporal_stride,\n            dilation,\n            downsample,\n            style=style,\n            if_inflate= (inflate_freq[0] == 1),\n            inflate_style=inflate_style,\n            nonlocal_cfg=nonlocal_cfg if nonlocal_freq[0] == 1 else None,\n            with_cp=with_cp))\n    inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(\n            block(inplanes,\n                planes,\n                1, 1,\n                dilation,\n                style=style,\n                if_inflate= (inflate_freq[i] == 1),\n                inflate_style=inflate_style,\n                nonlocal_cfg=nonlocal_cfg if nonlocal_freq[i] == 1 else None,\n                with_cp=with_cp))\n\n    return nn.Sequential(*layers)\n\n\nclass pathway(nn.Module):\n    arch_settings = {\n        18: (BasicBlock, (2, 2, 2, 2)),\n        34: (BasicBlock, (3, 4, 6, 3)),\n        50: (Bottleneck, (3, 4, 6, 3)),\n        101: (Bottleneck, (3, 4, 23, 3)),\n        152: (Bottleneck, (3, 8, 36, 3))\n    }\n\n    def __init__(self,\n                 depth,\n                 num_stages=4,\n                 # reduce base channel by\n                 channel_mul_inv=1,\n                 lateral=True,\n                 # alpha\n                 alpha=8,\n                 # beta inv\n                 beta_inv=8,\n                 lateral_type=\'conv\',\n                 lateral_op=\'concat\',\n                 conv1_kernel_t=1,\n                 conv1_stride_t=1,\n                 pool1_kernel_t=1,\n                 pool1_stride_t=1,\n                 spatial_strides=(1, 2, 2, 2),\n                 dilations=(1, 1, 1, 1),\n                 style=\'pytorch\',\n                 inflate_freqs=(1, 1, 1, 1),\n                 inflate_style=\'3x1x1\',\n                 nonlocal_stages=(-1, ),\n                 nonlocal_freqs=(0, 1, 1, 0),\n                 nonlocal_cfg=None,\n                 with_cp=False):\n        super(pathway, self).__init__()\n        self.block, stage_blocks = self.arch_settings[depth]\n        self.stage_blocks = stage_blocks[:num_stages]\n        self.inplanes = 64 // channel_mul_inv\n        if lateral:\n            if lateral_type == \'toC\':\n                lateral_inplanes = self.inplanes * alpha // beta_inv\n            elif lateral_type == \'sampling\':\n                lateral_inplanes = self.inplanes // beta_inv\n            elif lateral_type == \'conv\':\n                lateral_inplanes = self.inplanes * 2 // beta_inv\n                self.conv1_lateral = nn.Conv3d(self.inplanes // beta_inv,\n                    self.inplanes * 2 // beta_inv,\n                    kernel_size=(5, 1, 1),\n                    stride=(alpha, 1, 1),\n                    padding=(2, 0, 0),\n                    bias=False)\n            else:\n                raise NotImplementedError\n        else:\n            lateral_inplanes = 0\n\n        self.conv1 = nn.Conv3d(\n            3, 64 // channel_mul_inv, kernel_size=(conv1_kernel_t, 7, 7), stride=(conv1_stride_t, 2, 2), padding=((conv1_kernel_t-1)//2, 3, 3), bias=False)\n        self.bn1 = nn.BatchNorm3d(64 // channel_mul_inv)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool3d(kernel_size=(pool1_kernel_t, 3, 3), stride=(pool1_stride_t, 2, 2), padding=(pool1_kernel_t//2, 1, 1))\n        # Byebye temporal pooling :)\n        # self.pool2 = nn.MaxPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1), padding=(0, 0, 0))\n\n        self.res_layers = []\n        self.lateral_connections = []\n        for i, num_blocks in enumerate(self.stage_blocks):\n            spatial_stride = spatial_strides[i]\n            temporal_stride = 1    # all temporal strides are set to 1 in SlowFast\n            dilation = dilations[i]\n            planes = 64 * 2**i // channel_mul_inv\n            res_layer = make_res_layer(\n                self.block,\n                self.inplanes,\n                planes,\n                num_blocks,\n                lateral_inplanes=lateral_inplanes,\n                spatial_stride=spatial_stride,\n                temporal_stride=temporal_stride,\n                dilation=dilation,\n                style=style,\n                inflate_freq=inflate_freqs[i],\n                inflate_style=inflate_style,\n                nonlocal_freq=nonlocal_freqs[i],\n                nonlocal_cfg=nonlocal_cfg if i in nonlocal_stages else None,\n                with_cp=with_cp)\n            self.inplanes = planes * self.block.expansion\n            if lateral:\n                if lateral_type == \'toC\':\n                    lateral_inplanes = self.inplanes * alpha // beta_inv\n                elif lateral_type == \'sampling\':\n                    lateral_inplanes = self.inplanes // beta_inv\n                elif lateral_type == \'conv\':\n                    lateral_inplanes = self.inplanes * 2 // beta_inv\n                    lateral_name = \'layer{}_lateral\'.format(i + 1)\n                    setattr(self, lateral_name,\n                            nn.Conv3d(self.inplanes // beta_inv,\n                            self.inplanes * 2 // beta_inv,\n                            kernel_size=(5, 1, 1),\n                            stride=(alpha, 1, 1),\n                            padding=(2, 0, 0),\n                            bias=False))\n                    self.lateral_connections.append(lateral_name)\n            else:\n                lateral_inplanes = 0\n\n            layer_name = \'layer{}\'.format(i + 1)\n            self.add_module(layer_name, res_layer)\n            self.res_layers.append(layer_name)\n\n        self.feat_dim = self.block.expansion * 64 * 2**(\n            len(self.stage_blocks) - 1)\n\n\n@BACKBONES.register_module\nclass ResNet_I3D_SlowFast(nn.Module):\n    """"""ResNet_I3D backbone.\n\n    Args:\n        depth (int): Depth of resnet, from {18, 34, 50, 101, 152}.\n        num_stages (int): Resnet stages, normally 4.\n        strides (Sequence[int]): Strides of the first block of each stage.\n        dilations (Sequence[int]): Dilation of each stage.\n        out_indices (Sequence[int]): Output from which stages.\n        style (str): `pytorch` or `caffe`. If set to ""pytorch"", the stride-two\n            layer is the 3x3 conv layer, otherwise the stride-two layer is\n            the first 1x1 conv layer.\n        frozen_stages (int): Stages to be frozen (all param fixed). -1 means\n            not freezing any parameters.\n        bn_eval (bool): Whether to set BN layers to eval mode, namely, freeze\n            running stats (mean and var).\n        bn_frozen (bool): Whether to freeze weight and bias of BN layers.\n        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n            memory while slowing down the training speed.\n    """"""\n\n    arch_settings = {\n        18: (BasicBlock, (2, 2, 2, 2)),\n        34: (BasicBlock, (3, 4, 6, 3)),\n        50: (Bottleneck, (3, 4, 6, 3)),\n        101: (Bottleneck, (3, 4, 23, 3)),\n        152: (Bottleneck, (3, 8, 36, 3))\n    }\n\n    def __init__(self,\n                 depth,\n                 tau=16,\n                 alpha=8,\n                 beta_inv=8,\n                 pretrained_slow=None,\n                 pretrained_fast=None,\n                 # pretrained=None,\n                 num_stages=4,\n                 slow_only=False,\n                 fast_only=False,\n                 lateral_type=\'conv\',\n                 lateral_op=\'concat\',\n                 spatial_strides=(1, 2, 2, 2),\n                 dilations=(1, 1, 1, 1),\n                 out_indices=(0, 1, 2, 3),\n                 slow_conv1_kernel_t=1,\n                 slow_conv1_stride_t=1,\n                 slow_pool1_kernel_t=1,\n                 slow_pool1_stride_t=1,\n                 fast_conv1_kernel_t=5,\n                 fast_conv1_stride_t=1,\n                 fast_pool1_kernel_t=1,\n                 fast_pool1_stride_t=1,\n                 style=\'pytorch\',\n                 frozen_stages=-1,\n                 slow_inflate_freq=(0, 0, 1, 1),\n                 fast_inflate_freq=(1, 1, 1, 1),\n                 inflate_stride=(1, 1, 1, 1),\n                 inflate_style=\'3x1x1\',\n                 nonlocal_stages=(-1, ),\n                 nonlocal_freq=(0, 1, 1, 0),\n                 nonlocal_cfg=None,\n                 bn_eval=True,\n                 bn_frozen=False,\n                 partial_bn=False,\n                 with_cp=False):\n        super(ResNet_I3D_SlowFast, self).__init__()\n        if depth not in self.arch_settings:\n            raise KeyError(\'invalid depth {} for resnet\'.format(depth))\n        self.depth = depth\n        self.tau = tau\n        self.alpha = alpha\n        self.beta_inv = beta_inv\n        # self.pretrained = pretrained\n        self.pretrained_slow = pretrained_slow\n        self.pretrained_fast = pretrained_fast\n        self.num_stages = num_stages\n        assert num_stages >= 1 and num_stages <= 4\n        self.slow_only = slow_only\n        self.fast_only = fast_only\n        assert not (self.slow_only and self.fast_only)\n        self.lateral_type = lateral_type\n        self.lateral_op = lateral_op\n        assert lateral_type in [\'conv\']   # in [\'toC\', \'sampling\', \'conv\']\n        assert lateral_op in [\'concat\']   # in [\'sum\', \'concat\']\n        self.spatial_strides = spatial_strides\n        self.dilations = dilations\n        assert len(spatial_strides) == len(dilations) == num_stages\n        self.out_indices = out_indices\n        assert max(out_indices) < num_stages\n        self.style = style\n        self.frozen_stages = frozen_stages\n        self.slow_inflate_freq = slow_inflate_freq\n        if isinstance(slow_inflate_freq, int):\n            self.slow_inflate_freq = (slow_inflate_freq, ) * num_stages\n        self.fast_inflate_freq = fast_inflate_freq\n        if isinstance(fast_inflate_freq, int):\n            self.fast_inflate_freq = (fast_inflate_freq, ) * num_stages\n        self.inflate_style = inflate_style\n        self.nonlocal_stages = nonlocal_stages\n        self.nonlocal_freqs = nonlocal_freq if not isinstance(nonlocal_freq, int) else (nonlocal_freq, ) * num_stages\n        self.nonlocal_cfg = nonlocal_cfg\n        self.bn_eval = bn_eval\n        self.bn_frozen = bn_frozen\n        self.partial_bn = partial_bn\n        self.with_cp = with_cp\n\n        if not self.fast_only:\n            self.slow_path = pathway(depth, num_stages=num_stages, channel_mul_inv=1,\n                                     lateral=not self.slow_only,\n                                     alpha=alpha,\n                                     beta_inv=beta_inv,\n                                     lateral_type=lateral_type,\n                                     lateral_op=lateral_op,\n                                     conv1_kernel_t=slow_conv1_kernel_t,\n                                     conv1_stride_t=slow_conv1_stride_t,\n                                     pool1_kernel_t=slow_pool1_kernel_t,\n                                     pool1_stride_t=slow_pool1_stride_t,\n                                     spatial_strides=spatial_strides,\n                                     dilations=dilations,\n                                     style=style,\n                                     inflate_freqs=self.slow_inflate_freq,\n                                     inflate_style=inflate_style,\n                                     nonlocal_stages=nonlocal_stages,\n                                     nonlocal_freqs=nonlocal_freq,\n                                     nonlocal_cfg=nonlocal_cfg,\n                                     with_cp=with_cp)\n        if not self.slow_only:\n            self.fast_path = pathway(depth, num_stages=num_stages, channel_mul_inv=beta_inv,\n                                     lateral=False,\n                                     conv1_kernel_t=fast_conv1_kernel_t,\n                                     conv1_stride_t=fast_conv1_stride_t,\n                                     pool1_kernel_t=fast_pool1_kernel_t,\n                                     pool1_stride_t=fast_pool1_stride_t,\n                                     spatial_strides=spatial_strides,\n                                     dilations=dilations,\n                                     style=style,\n                                     inflate_freqs=self.fast_inflate_freq,\n                                     inflate_style=inflate_style,\n                                     nonlocal_stages=nonlocal_stages,\n                                     nonlocal_freqs=nonlocal_freq,\n                                     nonlocal_cfg=nonlocal_cfg,\n                                     with_cp=with_cp)\n\n    def init_weights(self):\n        logger = logging.getLogger()\n        if not self.fast_only:\n            if self.pretrained_slow:\n                resnet2d = ResNet(self.depth)\n                load_checkpoint(resnet2d, self.pretrained_slow, strict=False, logger=logger)\n                for name, module in self.slow_path.named_modules():\n                    if isinstance(module, NonLocalModule):\n                        module.init_weights()\n                    elif isinstance(module, nn.Conv3d) and rhasattr(resnet2d, name):\n                        old_weight = rgetattr(resnet2d, name).weight.data\n                        old_shape = old_weight.shape\n                        new_shape = module.weight.data.shape\n                        if new_shape[1] != old_shape[1]:\n                            new_ch = new_shape[1] - old_shape[1]\n                            pad_shape = old_shape\n                            pad_shape = pad_shape[:1] + (new_ch, ) + pad_shape[2:]\n                            old_weight = torch.cat((old_weight, torch.zeros(pad_shape).type_as(old_weight).to(old_weight.device)), dim=1)\n\n\n                        new_weight = old_weight.unsqueeze(2).expand_as(module.weight.data) / new_shape[2]\n                        module.weight.data.copy_(new_weight)\n                        logging.info(""{}.weight loaded from weights file into {}"".format(name, new_weight.shape))\n                        if hasattr(module, \'bias\') and module.bias is not None:\n                            new_bias = rgetattr(resnet2d, name).bias.data\n                            module.bias.data.copy_(new_bias)\n                            logging.info(""{}.bias loaded from weights file into {}"".format(name, new_bias.shape))\n                    elif isinstance(module, nn.BatchNorm3d) and rhasattr(resnet2d, name):\n                          for attr in [\'weight\', \'bias\', \'running_mean\', \'running_var\']:\n                              logging.info(""{}.{} loaded from weights file into {}"".format(name, attr, getattr(rgetattr(resnet2d, name), attr).shape))\n                              setattr(module, attr, getattr(rgetattr(resnet2d, name), attr))\n                    else:\n                        print(name)\n            else:\n                for m in self.slow_path.modules():\n                    if isinstance(m, nn.Conv3d):\n                        kaiming_init(m)\n                    elif isinstance(m, nn.BatchNorm3d):\n                        constant_init(m, 1)\n\n        if not self.slow_only:\n            if self.pretrained_fast:\n                resnet2d = ResNet(self.depth, base_channels=64 // self.beta_inv)\n                load_checkpoint(resnet2d, self.pretrained_fast, strict=False, logger=logger)\n                for name, module in self.fast_path.named_modules():\n                    if isinstance(module, NonLocalModule):\n                        module.init_weights()\n                    elif isinstance(module, nn.Conv3d) and rhasattr(resnet2d, name):\n                        old_weight = rgetattr(resnet2d, name).weight.data\n                        old_shape = old_weight.shape\n                        new_shape = module.weight.data.shape\n                        if new_shape[1] != old_shape[1]:\n                            new_ch = new_shape[1] - old_shape[1]\n                            pad_shape = old_shape\n                            pad_shape = pad_shape[:1] + (new_ch, ) + pad_shape[2:]\n                            old_weight = torch.cat((old_weight, torch.zeros(pad_shape).type_as(old_weight).to(old_weight.device)), dim=1)\n\n\n                        new_weight = old_weight.unsqueeze(2).expand_as(module.weight.data) / new_shape[2]\n                        module.weight.data.copy_(new_weight)\n                        logging.info(""{}.weight loaded from weights file into {}"".format(name, new_weight.shape))\n                        if hasattr(module, \'bias\') and module.bias is not None:\n                            new_bias = rgetattr(resnet2d, name).bias.data\n                            module.bias.data.copy_(new_bias)\n                            logging.info(""{}.bias loaded from weights file into {}"".format(name, new_bias.shape))\n                    elif isinstance(module, nn.BatchNorm3d) and rhasattr(resnet2d, name):\n                          for attr in [\'weight\', \'bias\', \'running_mean\', \'running_var\']:\n                              logging.info(""{}.{} loaded from weights file into {}"".format(name, attr, getattr(rgetattr(resnet2d, name), attr).shape))\n                              setattr(module, attr, getattr(rgetattr(resnet2d, name), attr))\n                    else:\n                        print(name)\n            else:\n                for m in self.fast_path.modules():\n                    if isinstance(m, nn.Conv3d):\n                        kaiming_init(m)\n                    elif isinstance(m, nn.BatchNorm3d):\n                        constant_init(m, 1)\n\n    def forward(self, x):\n        if not self.fast_only:\n            x_slow = x[:, :, ::self.tau, :, :]\n\n            x_slow = self.slow_path.conv1(x_slow)\n            x_slow = self.slow_path.bn1(x_slow)\n            x_slow = self.slow_path.relu(x_slow)\n            x_slow = self.slow_path.maxpool(x_slow)\n\n        if not self.slow_only:\n            x_fast = x[:, :, ::self.tau//self.alpha, :, :]\n            x_fast = self.fast_path.conv1(x_fast)\n            x_fast = self.fast_path.bn1(x_fast)\n            x_fast = self.fast_path.relu(x_fast)\n            x_fast = self.fast_path.maxpool(x_fast)\n\n        if not self.fast_only and not self.slow_only:\n            x_fast_lateral = self.slow_path.conv1_lateral(x_fast)\n            x_slow = torch.cat((x_slow, x_fast_lateral), dim=1)\n\n\n        outs = []\n        if not self.fast_only:\n            for i, layer_name in enumerate(self.slow_path.res_layers):\n                res_layer = getattr(self.slow_path, layer_name)\n                x_slow = res_layer(x_slow)\n                if not self.slow_only:\n                    res_layer_fast = getattr(self.fast_path, layer_name)\n                    x_fast = res_layer_fast(x_fast)\n                    if self.lateral_type == \'conv\' and i != 3:\n                        lateral_name = self.slow_path.lateral_connections[i]\n                        conv_lateral = getattr(self.slow_path, lateral_name)\n                        x_fast_lateral = conv_lateral(x_fast)\n                        x_slow = torch.cat((x_slow, x_fast_lateral), dim=1)\n                if i in self.out_indices:\n                    if not self.slow_only:\n                        outs.append((x_slow, x_fast))\n                    else:\n                        outs.append(x_slow)\n        else:\n            for i, layer_name in enumerate(self.fast_path.res_layers):\n                res_layer = getattr(self.fast_path, layer_name)\n                x_fast = res_layer(x_fast)\n                if i in self.out_indices:\n                    outs.append(x_fast)\n\n        if len(outs) == 1:\n            return outs[0]\n        else:\n            return tuple(outs)\n\n    def train(self, mode=True):\n        super(ResNet_I3D_SlowFast, self).train(mode)\n        if self.bn_eval:\n            for m in self.modules():\n                if isinstance(m, nn.BatchNorm3d):\n                    m.eval()\n                    if self.bn_frozen:\n                        for params in m.parameters():\n                            params.requires_grad = False\n        if self.partial_bn:\n            for i in range(1, self.frozen_stages + 1):\n                mod = getattr(self, \'layer{}\'.format(i))\n                for m in mod.modules():\n                    if isinstance(m, nn.BatchNorm3d):\n                        m.eval()\n                        m.weight.requires_grad = False\n                        m.bias.requires_grad = False\n        if mode and self.frozen_stages >= 0:\n            for param in self.conv1.parameters():\n                param.requires_grad = False\n            for param in self.bn1.parameters():\n                param.requires_grad = False\n            self.bn1.eval()\n            self.bn1.weight.requires_grad = False\n            self.bn1.bias.requires_grad = False\n            for i in range(1, self.frozen_stages + 1):\n                mod = getattr(self, \'layer{}\'.format(i))\n                mod.eval()\n                for param in mod.parameters():\n                    param.requires_grad = False\n'"
mmaction/models/tenons/backbones/resnet_r3d.py,2,"b""import logging\nimport torch.nn as nn\nimport torch.utils.checkpoint as cp\nimport torch\nfrom mmcv.cnn import constant_init, kaiming_init\nfrom mmcv.runner import load_checkpoint\nfrom ...registry import BACKBONES\nfrom ..utils.resnet_r3d_utils import *\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self,\n                 input_filters,\n                 num_filters,\n                 base_filters,\n                 down_sampling=False,\n                 down_sampling_temporal=None,\n                 block_type='3d',\n                 is_real_3d=True,\n                 group=1,\n                 with_bn=True):\n\n\n        super(BasicBlock, self).__init__()\n        self.num_filters = num_filters\n        self.base_filters = base_filters\n        self.input_filters = input_filters\n        self.with_bn = with_bn\n        if self.with_bn:\n            conv3d = conv3d_wobias\n        else:\n            conv3d = conv3d_wbias\n\n        if block_type == '2.5d':\n            assert is_real_3d\n        if down_sampling_temporal is None:\n            down_sampling_temporal = down_sampling\n        if down_sampling:\n            if is_real_3d and down_sampling_temporal:\n                self.down_sampling_stride = [2, 2, 2]\n            else:\n                self.down_sampling_stride = [1, 2, 2]\n        else:\n            self.down_sampling_stride = [1, 1, 1]\n\n        self.down_sampling = down_sampling\n\n        self.relu = nn.ReLU()\n        self.conv1 = add_conv3d(input_filters, num_filters,\n                                kernel=[3, 3, 3] if is_real_3d else [1, 3, 3],\n                                stride=self.down_sampling_stride,\n                                pad=[1, 1, 1] if is_real_3d else [0, 1, 1],\n                                block_type=block_type, with_bn=self.with_bn)\n        if self.with_bn:\n            self.bn1 = add_bn(num_filters)\n        self.conv2 = add_conv3d(num_filters, num_filters,\n                                kernel=[3, 3, 3] if is_real_3d else [1, 3, 3],\n                                stride=[1, 1, 1],\n                                pad=[1, 1, 1] if is_real_3d else [0, 1, 1],\n                                block_type=block_type, with_bn=self.with_bn)\n        if self.with_bn:\n            self.bn2 = add_bn(num_filters)\n        if num_filters != input_filters or down_sampling:\n            self.conv3 = conv3d(input_filters, num_filters, kernel=[1, 1, 1],\n                                stride=self.down_sampling_stride, pad=[0, 0, 0])\n            if self.with_bn:\n                self.bn3 = nn.BatchNorm3d(num_filters, eps=1e-3)\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        if self.with_bn:\n            out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        if self.with_bn:\n            out = self.bn2(out)\n\n        if self.down_sampling or self.num_filters != self.input_filters:\n            identity = self.conv3(identity)\n            if self.with_bn:\n                identity = self.bn3(identity)\n\n        out += identity\n        out = self.relu(out)\n        return out\n\n\nclass Bottleneck(nn.Module):\n    def __init__(self,\n                 input_filters,\n                 num_filters,\n                 base_filters,\n                 down_sampling=False,\n                 down_sampling_temporal=None,\n                 block_type='3d',\n                 is_real_3d=True,\n                 group=1,\n                 with_bn=True):\n\n        super(Bottleneck, self).__init__()\n        self.num_filters = num_filters\n        self.base_filters = base_filters\n        self.input_filters = input_filters\n        self.with_bn = with_bn\n        if self.with_bn:\n            conv3d = conv3d_wobias\n        else:\n            conv3d = conv3d_wbias\n\n        if block_type == '2.5d':\n            assert is_real_3d\n        if down_sampling_temporal is None:\n            down_sampling_temporal = down_sampling\n        if down_sampling:\n            if is_real_3d and down_sampling_temporal:\n                self.down_sampling_stride = [2, 2, 2]\n            else:\n                self.down_sampling_stride = [1, 2, 2]\n        else:\n            self.down_sampling_stride = [1, 1, 1]\n\n        self.down_sampling = down_sampling\n        self.relu = nn.ReLU()\n\n        self.conv0 = add_conv3d(input_filters, base_filters, kernel=[\n                                1, 1, 1], stride=[1, 1, 1], pad=[0, 0, 0], with_bn=self.with_bn)\n        if self.with_bn:\n            self.bn0 = add_bn(base_filters)\n\n        self.conv1 = add_conv3d(base_filters, base_filters,\n                                kernel=[3, 3, 3] if is_real_3d else [1, 3, 3],\n                                stride=self.down_sampling_stride,\n                                pad=[1, 1, 1] if is_real_3d else [0, 1, 1],\n                                block_type=block_type, with_bn=self.with_bn)\n        if self.with_bn:\n            self.bn1 = add_bn(base_filters)\n\n        self.conv2 = add_conv3d(base_filters, num_filters, kernel=[\n                                1, 1, 1], pad=[0, 0, 0], stride=[1, 1, 1], with_bn=self.with_bn)\n\n        if self.with_bn:\n            self.bn2 = add_bn(num_filters)\n\n        if num_filters != input_filters or down_sampling:\n            self.conv3 = conv3d(input_filters, num_filters, kernel=[1, 1, 1],\n                                stride=self.down_sampling_stride, pad=[0, 0, 0])\n            if self.with_bn:\n                self.bn3 = nn.BatchNorm3d(num_filters, eps=1e-3)\n\n    def forward(self, x):\n        identity = x\n        if self.with_bn:\n            out = self.relu(self.bn0(self.conv0(x)))\n            out = self.relu(self.bn1(self.conv1(out)))\n            out = self.bn2(self.conv2(out))\n        else:\n            out = self.relu(self.conv0(x))\n            out = self.relu(self.conv1(out))\n            out = self.conv2(out)\n\n        if self.down_sampling or self.num_filters != self.input_filters:\n            identity = self.conv3(identity)\n            if self.with_bn:\n                identity = self.bn3(identity)\n\n        out += identity\n        out = self.relu(out)\n        return out\n\n\ndef make_plain_res_layer(block, num_blocks, in_filters, num_filters, base_filters,\n                         block_type='3d', down_sampling=False, down_sampling_temporal=None,\n                         is_real_3d=True, with_bn=True):\n    layers = []\n    layers.append(block(in_filters, num_filters, base_filters, down_sampling=down_sampling,\n                        down_sampling_temporal=down_sampling_temporal, block_type=block_type,\n                        is_real_3d=is_real_3d, with_bn=with_bn))\n    for i in range(num_blocks - 1):\n        layers.append(block(num_filters, num_filters, base_filters,\n                            block_type=block_type, is_real_3d=is_real_3d, with_bn=with_bn))\n    return module_list(layers)\n\n\nBLOCK_CONFIG = {\n    10: (1, 1, 1, 1),\n    16: (2, 2, 2, 1),\n    18: (2, 2, 2, 2),\n    26: (2, 2, 2, 2),\n    34: (3, 4, 6, 3),\n    50: (3, 4, 6, 3),\n    101: (3, 4, 23, 3),\n    152: (3, 8, 36, 3),\n}\nSHALLOW_FILTER_CONFIG = [\n    [64, 64],\n    [128, 128],\n    [256, 256],\n    [512, 512]\n]\nDEEP_FILTER_CONFIG = [\n    [256, 64],\n    [512, 128],\n    [1024, 256],\n    [2048, 512]\n]\n\n\n@BACKBONES.register_module\nclass ResNet_R3D(nn.Module):\n    def __init__(self,\n                 pretrained=None,\n                 num_input_channels=3,\n                 depth=34,\n                 block_type='2.5d',\n                 channel_multiplier=1.0,\n                 bottleneck_multiplier=1.0,\n                 conv1_kernel_t=3,\n                 conv1_stride_t=1,\n                 use_pool1=False,\n                 bn_eval=True,\n                 bn_frozen=True,\n                 with_bn=True):\n        #         parameter initialization\n        super(ResNet_R3D, self).__init__()\n        self.pretrained = pretrained\n        self.num_input_channels = num_input_channels\n        self.depth = depth\n        self.block_type = block_type\n        self.channel_multiplier = channel_multiplier\n        self.bottleneck_multiplier = bottleneck_multiplier\n        self.conv1_kernel_t = conv1_kernel_t\n        self.conv1_stride_t = conv1_stride_t\n        self.use_pool1 = use_pool1\n        self.relu = nn.ReLU()\n        self.bn_eval = bn_eval\n        self.bn_frozen = bn_frozen\n        self.with_bn = with_bn\n        global comp_count, comp_idx\n        comp_idx = 0\n        comp_count = 0\n\n        if self.with_bn:\n            conv3d = conv3d_wobias\n        else:\n            conv3d = conv3d_wbias\n\n#         stem block\n        if self.block_type in ['2.5d', '2.5d-sep']:\n            self.conv1_s = conv3d(self.num_input_channels, 45, [\n                                  1, 7, 7], [1, 2, 2], [0, 3, 3])\n            if self.with_bn:\n                self.bn1_s = nn.BatchNorm3d(45, eps=1e-3)\n            self.conv1_t = conv3d(45, 64, [self.conv1_kernel_t, 1, 1], [self.conv1_stride_t, 1, 1],\n                                  [(self.conv1_kernel_t - 1) // 2, 0, 0])\n            if self.with_bn:\n                self.bn1_t = nn.BatchNorm3d(64, eps=1e-3)\n        else:\n            self.conv1 = conv3d(self.num_input_channels, 64, [self.conv1_kernel_t, 7, 7],\n                                [self.conv1_stride_t, 2, 2], [(self.conv1_kernel_t - 1) // 2, 3, 3])\n            if self.with_bn:\n                self.bn1 = nn.BatchNorm3d(64, eps=1e-3)\n\n        if self.use_pool1:\n            self.pool1 = nn.MaxPool3d(kernel_size=[1, 3, 3], stride=[\n                                      1, 2, 2], padding=[0, 1, 1])\n\n        self.stage_blocks = BLOCK_CONFIG[self.depth]\n        if self.depth <= 18 or self.depth == 34:\n            self.block = BasicBlock\n        else:\n            self.block = Bottleneck\n        if self.depth <= 34:\n            self.filter_config = SHALLOW_FILTER_CONFIG\n        else:\n            self.filter_config = DEEP_FILTER_CONFIG\n        self.filter_config = np.multiply(\n            self.filter_config, self.channel_multiplier).astype(np.int)\n\n        layer1 = make_plain_res_layer(self.block, self.stage_blocks[0],\n                                      64, self.filter_config[0][0],\n                                      int(self.filter_config[0][1]\n                                          * self.bottleneck_multiplier),\n                                      block_type=self.block_type,\n                                      with_bn=self.with_bn)\n        self.add_module('layer1', layer1)\n        layer2 = make_plain_res_layer(self.block, self.stage_blocks[1],\n                                      self.filter_config[0][0], self.filter_config[1][0],\n                                      int(self.filter_config[1][1]\n                                          * self.bottleneck_multiplier),\n                                      block_type=self.block_type, down_sampling=True,\n                                      with_bn=self.with_bn)\n        self.add_module('layer2', layer2)\n        layer3 = make_plain_res_layer(self.block, self.stage_blocks[2],\n                                      self.filter_config[1][0], self.filter_config[2][0],\n                                      int(self.filter_config[2][1]\n                                          * self.bottleneck_multiplier),\n                                      block_type=self.block_type, down_sampling=True,\n                                      with_bn=self.with_bn)\n        self.add_module('layer3', layer3)\n        layer4 = make_plain_res_layer(self.block, self.stage_blocks[3],\n                                      self.filter_config[2][0], self.filter_config[3][0],\n                                      int(self.filter_config[3][1]\n                                          * self.bottleneck_multiplier),\n                                      block_type=self.block_type, down_sampling=True,\n                                      with_bn=self.with_bn)\n        self.add_module('layer4', layer4)\n        self.res_layers = ['layer1', 'layer2', 'layer3', 'layer4']\n\n    def forward(self, x):\n        if self.block_type in ['2.5d', '2.5d-sep']:\n            if self.with_bn:\n                x = self.relu(self.bn1_s(self.conv1_s(x)))\n                x = self.relu(self.bn1_t(self.conv1_t(x)))\n            else:\n                x = self.relu(self.conv1_s(x))\n                x = self.relu(self.conv1_t(x))\n        else:\n            if self.with_bn:\n                x = self.relu(self.bn1(self.conv1(x)))\n            else:\n                x = self.relu(self.conv1(x))\n\n        if self.use_pool1:\n            x = self.pool1(x)\n\n        for i, layer_name in enumerate(self.res_layers):\n            res_layer = getattr(self, layer_name)\n            x = res_layer(x)\n\n        return x\n\n    def init_weights(self):\n        if isinstance(self.pretrained, str):\n            logger = logging.getLogger()\n            load_checkpoint(self, self.pretrained, strict=False, logger=logger)\n        elif self.pretrained is None:\n            for m in self.modules():\n                if isinstance(m, nn.Conv3d):\n                    kaiming_init(m)\n                elif isinstance(m, nn.BatchNorm3d):\n                    constant_init(m, 1)\n        else:\n            raise TypeError('pretrained must be a str or None')\n\n    def train(self, mode=True):\n        super(ResNet_R3D, self).train(mode)\n        if self.bn_eval and self.with_bn:\n            for m in self.modules():\n                if isinstance(m, nn.BatchNorm3d):\n                    m.eval()\n                    if self.bn_frozen:\n                        for params in m.parameters():\n                            params.requires_grad = False\n"""
mmaction/models/tenons/backbones/resnet_s3d.py,6,"b'import logging\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.checkpoint as cp\n\nfrom ....utils.misc import rgetattr, rhasattr\nfrom .resnet import ResNet \nfrom mmcv.cnn import constant_init, kaiming_init\nfrom mmcv.runner import load_checkpoint\n\nfrom ....ops.trajectory_conv_package.traj_conv import TrajConv\nfrom .. import flownets\n\n\nfrom ...registry import BACKBONES\n\ndef conv3x3x3(in_planes, out_planes, spatial_stride=1, temporal_stride=1, dilation=1):\n    ""3x3x3 convolution with padding""\n    return nn.Conv3d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=(temporal_stride, spatial_stride, spatial_stride),\n        padding=dilation,\n        dilation=dilation,\n        bias=False)\n\n\ndef conv1x3x3(in_planes, out_planes, spatial_stride=1, temporal_stride=1, dilation=1):\n    ""1x3x3 convolution with padding""\n    return nn.Conv3d(\n        in_planes,\n        out_planes,\n        kernel_size=(1,3,3),\n        stride=(temporal_stride, spatial_stride, spatial_stride),\n        padding=(0, dilation, dilation),\n        dilation=dilation,\n        bias=False)\n\n\ndef conv3x1x1(in_planes, out_planes, spatial_stride=1, temporal_stride=1, dilation=1, bias=False):\n    ""3x1x1 convolution with padding""\n    return nn.Conv3d(\n        in_planes,\n        out_planes,\n        kernel_size=(3,1,1),\n        stride=(temporal_stride, spatial_stride, spatial_stride),\n        padding=(dilation,0,0),\n        dilation=dilation,\n        bias=bias)\n\ndef trajconv3x1x1(in_planes, out_planes, spatial_stride=1, temporal_stride=1, dilation=1, bias=False):\n    ""3x1x1 convolution with padding""\n    return TrajConv(\n        in_planes,\n        out_planes,\n        kernel_size=(3,1,1),\n        stride=(temporal_stride, spatial_stride, spatial_stride),\n        padding=(dilation,0,0),\n        dilation=dilation,\n        bias=bias)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 spatial_stride=1,\n                 temporal_stride=1,\n                 dilation=1,\n                 downsample=None,\n                 style=\'pytorch\',\n                 if_inflate=True,\n                 with_cp=False,\n                 with_trajectory=False):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv1x3x3(inplanes, planes, spatial_stride, 1, dilation)\n        self.bn1 = nn.BatchNorm3d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        \n        self.conv2 = conv1x3x3(planes, planes)\n        self.bn2 = nn.BatchNorm3d(planes)\n\n        self.if_inflate = if_inflate\n\n        if self.if_inflate:\n            self.conv1_t = conv3x1x1(planes, planes, 1, temporal_stride, dilation, bias=True)\n            self.bn1_t = nn.BatchNorm3d(planes)\n            if with_trajectory:\n                self.conv2_t = trajconv3x1x1(planes, planes, bias=True)\n            else:\n                self.conv2_t = conv3x1x1(planes, planes, bias=True)\n            self.bn2_t = nn.BatchNorm3d(planes)\n\n        self.downsample = downsample\n        self.spatial_stride = spatial_stride\n        self.temporal_stride = temporal_stride\n        self.dilation = dilation\n        assert not with_cp\n\n        self.with_trajectory = with_trajectory\n\n    def forward(self, input):\n        x, traj_src = input\n\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        if self.if_inflate:\n            out = self.conv1_t(out)\n            out = self.bn1_t(out)\n            out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        \n        if self.if_inflate:\n            out = self.relu(out)\n            if self.with_trajectory:\n                assert traj_src[0] is not None\n                out = self.conv2_t(out, traj_src[0])\n            else:\n                out = self.conv2_t(out)\n            out = self.bn2_t(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out, traj_src[1:]\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 spatial_stride=1,\n                 temporal_stride=1,\n                 dilation=1,\n                 downsample=None,\n                 style=\'pytorch\',\n                 if_inflate=True,\n                 with_cp=False,\n                 with_trajectory=False):\n        """"""Bottleneck block for ResNet.\n        If style is ""pytorch"", the stride-two layer is the 3x3 conv layer,\n        if it is ""caffe"", the stride-two layer is the first 1x1 conv layer.\n        """"""\n        super(Bottleneck, self).__init__()\n        assert style in [\'pytorch\', \'caffe\']\n        self.inplanes = inplanes\n        self.planes = planes\n        if style == \'pytorch\':\n            self.conv1_stride = 1\n            self.conv2_stride = spatial_stride\n            self.conv1_stride_t = 1\n            self.conv2_stride_t = temporal_stride\n        else:\n            self.conv1_stride = spatial_stride\n            self.conv2_stride = 1\n            self.conv1_stride_t = temporal_stride\n            self.conv2_stride_t = 1\n\n        self.conv1 = nn.Conv3d(\n            inplanes,\n            planes,\n            kernel_size=1,\n            stride=(self.conv1_stride_t, self.conv1_stride, self.conv1_stride),\n            bias=False)\n\n        self.conv2 = nn.Conv3d(\n            planes,\n            planes,\n            kernel_size=(1,3,3),\n            stride=(1, self.conv2_stride, self.conv2_stride),\n            padding=(0, dilation, dilation),\n            dilation=(1, dilation, dilation),\n            bias=False)\n\n        self.if_inflate = if_inflate\n        if self.if_inflate:\n            self.conv2_t = nn.Conv3d(\n                planes,\n                planes,\n                kernel_size=(3,1,1),\n                stride=(self.conv2_stride_t,1,1),\n                padding=(1,0,0),\n                dilation=1,\n                bias=True)\n            self.bn2_t = nn.BatchNorm3d(planes)\n\n        self.bn1 = nn.BatchNorm3d(planes)\n        self.bn2 = nn.BatchNorm3d(planes)\n        self.conv3 = nn.Conv3d(\n            planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm3d(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.spatial_tride = spatial_stride\n        self.temporal_tride = temporal_stride\n        self.dilation = dilation\n        self.with_cp = with_cp\n\n        self.with_trajectory = with_trajectory\n\n    def forward(self, x):\n\n        def _inner_forward(xx):\n            x, traj_src = xx\n            identity = x\n\n            out = self.conv1(x)\n            out = self.bn1(out)\n            out = self.relu(out)\n\n            out = self.conv2(out)\n            out = self.bn2(out)\n            out = self.relu(out)\n\n            if self.if_inflate:\n                if self.with_trajectory:\n                    assert traj_src is not None\n                    out = self.conv2_t(out, traj_src[0])\n                else:\n                    out = self.conv2_t(out)\n                out = self.bn2_t(out)\n\n            out = self.conv3(out)\n            out = self.bn3(out)\n\n            if self.downsample is not None:\n                identity = self.downsample(x)\n\n            out += identity\n\n            return out, traj_src[1:]\n\n        if self.with_cp and x.requires_grad:\n            out, traj_remains = cp.checkpoint(_inner_forward, x)\n        else:\n            out, traj_remains = _inner_forward(x)\n\n        out = self.relu(out)\n\n        return out, traj_remains\n\n\ndef make_res_layer(block,\n                   inplanes,\n                   planes,\n                   blocks,\n                   spatial_stride=1,\n                   temporal_stride=1,\n                   dilation=1,\n                   style=\'pytorch\',\n                   inflate_freq=1,\n                   with_cp=False,\n                   traj_src_indices=-1):\n    traj_src_indices = traj_src_indices if not isinstance(traj_src_indices, int) else (traj_src_indices, ) * blocks\n    inflate_freq = inflate_freq if not isinstance(inflate_freq, int) else (inflate_freq, ) * blocks\n    assert len(inflate_freq) == blocks\n    downsample = None\n    if spatial_stride != 1 or inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            nn.Conv3d(\n                inplanes,\n                planes * block.expansion,\n                kernel_size=1,\n                stride=(temporal_stride, spatial_stride, spatial_stride),\n                bias=False),\n            nn.BatchNorm3d(planes * block.expansion),\n        )\n\n    layers = []\n    layers.append(\n        block(\n            inplanes,\n            planes,\n            spatial_stride,\n            temporal_stride,\n            dilation,\n            downsample,\n            style=style,\n            if_inflate=(inflate_freq[0] == 1),\n            with_trajectory=(traj_src_indices[0]>-1),\n            with_cp=with_cp))\n    inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(\n            block(inplanes,\n                planes,\n                1, 1,\n                dilation,\n                style=style,\n                if_inflate= (inflate_freq[i] == 1),\n                with_trajectory=(traj_src_indices[i]>-1),\n                with_cp=with_cp))\n\n    return nn.Sequential(*layers)\n\n\n@BACKBONES.register_module\nclass ResNet_S3D(nn.Module):\n    """"""ResNet_S3D backbone.\n\n    Args:\n        depth (int): Depth of resnet, from {18, 34, 50, 101, 152}.\n        num_stages (int): Resnet stages, normally 4.\n        strides (Sequence[int]): Strides of the first block of each stage.\n        dilations (Sequence[int]): Dilation of each stage.\n        out_indices (Sequence[int]): Output from which stages.\n        style (str): `pytorch` or `caffe`. If set to ""pytorch"", the stride-two\n            layer is the 3x3 conv layer, otherwise the stride-two layer is\n            the first 1x1 conv layer.\n        frozen_stages (int): Stages to be frozen (all param fixed). -1 means\n            not freezing any parameters.\n        bn_eval (bool): Whether to set BN layers to eval mode, namely, freeze\n            running stats (mean and var).\n        bn_frozen (bool): Whether to freeze weight and bias of BN layers.\n        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n            memory while slowing down the training speed.\n    """"""\n\n    arch_settings = {\n        18: (BasicBlock, (2, 2, 2, 2)),\n        34: (BasicBlock, (3, 4, 6, 3)),\n        50: (Bottleneck, (3, 4, 6, 3)),\n        101: (Bottleneck, (3, 4, 23, 3)),\n        152: (Bottleneck, (3, 8, 36, 3))\n    }\n\n    def __init__(self,\n                 depth,\n                 pretrained=None,\n                 num_stages=4,\n                 spatial_strides=(1, 2, 2, 2),\n                 temporal_strides=(1, 1, 1, 1),\n                 dilations=(1, 1, 1, 1),\n                 out_indices=(0, 1, 2, 3),\n                 conv1_kernel_t=5,\n                 conv1_stride_t=2,\n                 pool1_kernel_t=1,\n                 pool1_stride_t=2,\n                 use_pool2=True,\n                 style=\'pytorch\',\n                 frozen_stages=-1,\n                 inflate_freq=(1, 1, 1, 1),    # For C2D baseline, this is set to -1.\n                 bn_eval=True,\n                 bn_frozen=False,\n                 partial_bn=False,\n                 with_cp=False,\n                 with_trajectory=False,\n                 trajectory_source_indices=-1,\n                 trajectory_downsample_method=\'ave\',\n                 conv_bias=0.2):\n        super(ResNet_S3D, self).__init__()\n        if depth not in self.arch_settings:\n            raise KeyError(\'invalid depth {} for resnet\'.format(depth))\n        self.depth = depth\n        self.pretrained = pretrained\n        self.num_stages = num_stages\n        assert num_stages >= 1 and num_stages <= 4\n        self.spatial_strides = spatial_strides\n        self.temporal_strides = temporal_strides\n        self.dilations = dilations\n        assert len(spatial_strides) == len(temporal_strides) == len(dilations) == num_stages\n        self.out_indices = out_indices\n        assert max(out_indices) < num_stages\n        self.style = style\n        self.frozen_stages = frozen_stages\n        self.inflate_freqs = inflate_freq if not isinstance(inflate_freq, int) else (inflate_freq, ) * num_stages\n        self.bn_eval = bn_eval\n        self.bn_frozen = bn_frozen\n        self.partial_bn = partial_bn\n        self.with_cp = with_cp\n\n        self.with_trajectory = with_trajectory\n        self.trajectory_source_indices = trajectory_source_indices \\\n            if not isinstance(trajectory_source_indices, int) else [trajectory_source_indices, ] * num_stages\n        self.trajectory_downsample_method = trajectory_downsample_method\n\n        self.conv_bias = conv_bias\n\n        self.block, stage_blocks = self.arch_settings[depth]\n        self.stage_blocks = stage_blocks[:num_stages]\n        for stage in range(num_stages):\n            self.trajectory_source_indices[stage] = self.trajectory_source_indices[stage] \\\n                if not isinstance(self.trajectory_source_indices[stage], int) else (self.trajectory_source_indices[stage], ) * self.stage_blocks[stage]\n        self.inplanes = 64\n\n        if conv1_kernel_t > 1:\n            self.conv1 = nn.Conv3d(\n                3, 64, kernel_size=(1,7,7), stride=(1,2,2), padding=(0,3,3), bias=False)\n            self.conv1_t = nn.Conv3d(\n                64, 64, kernel_size=(conv1_kernel_t,1,1), stride=(conv1_stride_t,1,1), padding=((conv1_kernel_t-1)//2,1,1), bias=True)\n            self.bn1_t = nn.BatchNorm3d(64)\n        else:\n            self.conv1 = nn.Conv3d(\n                3, 64, kernel_size=(1,7,7), stride=(conv1_stride_t,2,2), padding=(0,3,3), bias=False)\n        self.bn1 = nn.BatchNorm3d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool3d(kernel_size=(pool1_kernel_t,3,3), stride=(pool1_stride_t,2,2), padding=(pool1_kernel_t//2,1,1))\n        self.use_pool2 = use_pool2\n        if self.use_pool2:\n            self.pool2 = nn.MaxPool3d(kernel_size=(3,1,1), stride=(2,1,1), padding=(1,0,0))\n\n        self.res_layers = []\n        for i, num_blocks in enumerate(self.stage_blocks):\n            traj_src_indices = self.trajectory_source_indices[i] \\\n                if not isinstance(self.trajectory_source_indices[i], int) \\\n                else (self.trajectory_source_indices[i], ) * num_blocks\n            spatial_stride = spatial_strides[i]\n            temporal_stride = temporal_strides[i]\n            dilation = dilations[i]\n            planes = 64 * 2**i\n            res_layer = make_res_layer(\n                self.block,\n                self.inplanes,\n                planes,\n                num_blocks,\n                spatial_stride=spatial_stride,\n                temporal_stride=temporal_stride,\n                dilation=dilation,\n                style=self.style,\n                inflate_freq=self.inflate_freqs[i],\n                with_cp=with_cp,\n                traj_src_indices=traj_src_indices)\n            self.inplanes = planes * self.block.expansion\n            layer_name = \'layer{}\'.format(i + 1)\n            self.add_module(layer_name, res_layer)\n            self.res_layers.append(layer_name)\n\n        self.feat_dim = self.block.expansion * 64 * 2**(\n            len(self.stage_blocks) - 1)\n\n    def init_weights(self):\n        if isinstance(self.pretrained, str):\n            logger = logging.getLogger()\n            resnet2d = ResNet(self.depth)\n            load_checkpoint(resnet2d, self.pretrained, strict=False, logger=logger)\n            for name, module in self.named_modules():\n                if isinstance(module, nn.Conv3d) or isinstance(module, TrajConv):\n                    if rhasattr(resnet2d, name):\n                        new_weight = rgetattr(resnet2d, name).weight.data.unsqueeze(2).expand_as(module.weight) / module.weight.data.shape[2]\n                        module.weight.data.copy_(new_weight)\n                        if hasattr(module, \'bias\') and module.bias is not None:\n                            new_bias = rgetattr(resnet2d, name).bias.data\n                            module.bias.data.copy_(new_bias)\n                    else:\n                        kaiming_init(module, bias=self.conv_bias)\n                elif isinstance(module, nn.BatchNorm3d):\n                    if rhasattr(resnet2d, name):\n                        for attr in [\'weight\', \'bias\', \'running_mean\', \'running_var\']:\n                            setattr(module, attr, getattr(rgetattr(resnet2d, name), attr))\n                    else:\n                        constant_init(module, 1)\n        elif self.pretrained is None:\n            for m in self.modules():\n                if isinstance(m, nn.Conv3d):\n                    kaiming_init(m, bias=self.conv_bias)\n                elif isinstance(m, nn.BatchNorm3d):\n                    constant_init(m, 1)\n        else:\n            raise TypeError(\'pretrained must be a str or None\')\n\n    def forward(self, x, trajectory_forward=None, trajectory_backward=None):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        outs = []\n        for i, layer_name in enumerate(self.res_layers):\n            res_layer = getattr(self, layer_name)\n            y = []\n            for j in self.trajectory_source_indices[i]:\n                if j > -1:\n                    flow_forward = trajectory_forward[j]  ## N, 2*T, H, W (..x3y3x4y4..)\n                    flow_backward = trajectory_backward[j]\n                    flow_forward = flow_forward.view((flow_forward.size(0), -1, 2, flow_forward.size(2), flow_forward.size(3)))\n                    flow_backward = flow_backward.view((flow_backward.size(0), -1, 2, flow_backward.size(2), flow_backward.size(3)))\n                    flow_forward_x, flow_forward_y = torch.split(flow_forward, 1, 2)\n                    flow_backward_x, flow_backward_y = torch.split(flow_backward, 1, 2)\n                    flow_backward_x = flow_backward_x.flip(1).view((flow_backward_x.size(0), 1, flow_backward_x.size(1),\n                                                                    flow_backward_x.size(3), flow_backward_x.size(4)))  # N,T,1,H,W => N,1,T,H,W\n                    flow_backward_y = flow_backward_y.flip(1).view((flow_backward_y.size(0), 1, flow_backward_y.size(1),\n                                                                    flow_backward_y.size(3), flow_backward_y.size(4)))\n                    flow_forward_x = flow_forward_x.view((flow_forward_x.size(0), 1, flow_forward_x.size(1),\n                                                          flow_forward_x.size(3), flow_forward_x.size(4)))\n                    flow_forward_y = flow_forward_y.view((flow_forward_y.size(0), 1, flow_forward_y.size(1),\n                                                          flow_forward_y.size(3), flow_forward_y.size(4)))\n                    flow_zero = torch.zeros_like(flow_forward_x)\n                    y.append(torch.cat((flow_backward_y, flow_backward_x, flow_zero, flow_zero, flow_forward_y, flow_forward_x), 1))\n                else:\n                    y.append(None)\n            \n            x, remains = res_layer((x, y))\n            assert len(remains) == 0 ## TODO: delete if check passes\n            if i in self.out_indices:\n                outs.append(x)\n            if self.use_pool2 and i == 0:\n                x = self.pool2(x)\n        if len(outs) == 1:\n            return outs[0]\n        else:\n            return tuple(outs)\n\n    def train(self, mode=True):\n        super(ResNet_S3D, self).train(mode)\n        if self.bn_eval:\n            for m in self.modules():\n                if isinstance(m, nn.BatchNorm3d):\n                    m.eval()\n                    if self.bn_frozen:\n                        for params in m.parameters():\n                            params.requires_grad = False\n        if self.partial_bn:\n            for i in range(1, self.frozen_stages + 1):\n                mod = getattr(self, \'layer{}\'.format(i))\n                for m in mod.modules():\n                    if isinstance(m, nn.BatchNorm3d):\n                        m.eval()\n                        m.weight.requires_grad = False\n                        m.bias.requires_grad = False\n        if mode and self.frozen_stages >= 0:\n            for param in self.conv1.parameters():\n                param.requires_grad = False\n            for param in self.bn1.parameters():\n                param.requires_grad = False\n            self.bn1.eval()\n            self.bn1.weight.requires_grad = False\n            self.bn1.bias.requires_grad = False\n            for i in range(1, self.frozen_stages + 1):\n                mod = getattr(self, \'layer{}\'.format(i))\n                mod.eval()\n                for param in mod.parameters():\n                    param.requires_grad = False\n'"
mmaction/models/tenons/bbox_heads/__init__.py,0,"b""from .bbox_head import BBoxHead\n\n__all__ = [\n    'BBoxHead'\n]"""
mmaction/models/tenons/bbox_heads/bbox_head.py,7,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom mmaction.core.bbox2d import delta2bbox, bbox_target\nfrom mmaction.core.post_processing import multiclass_nms, singleclass_nms\nfrom mmaction.losses import (weighted_cross_entropy, weighted_smoothl1,\n                             multilabel_accuracy,\n                             weighted_binary_cross_entropy,\n                             weighted_multilabel_binary_cross_entropy)\nfrom ...registry import HEADS\n\n\n@HEADS.register_module\nclass BBoxHead(nn.Module):\n    """"""Simplest RoI head, with only two fc layers for classification and\n    regression respectively""""""\n\n    def __init__(self,\n                 with_temporal_pool=False,\n                 with_spatial_pool=False,\n                 temporal_pool_type=\'avg\',\n                 spatial_pool_type=\'max\',\n                 with_cls=True,\n                 with_reg=True,\n                 roi_feat_size=7,\n                 in_channels=256,\n                 num_classes=81,\n                 target_means=[0., 0., 0., 0.],\n                 target_stds=[0.1, 0.1, 0.2, 0.2],\n                 multilabel_classification=True,\n                 reg_class_agnostic=True,\n                 nms_class_agnostic=True):\n        super(BBoxHead, self).__init__()\n        assert with_cls or with_reg\n        self.with_temporal_pool = with_temporal_pool\n        self.with_spatial_pool = with_spatial_pool\n        assert temporal_pool_type in [\'max\', \'avg\']\n        assert spatial_pool_type in [\'max\', \'avg\']\n        self.temporal_pool_type = temporal_pool_type\n        self.spatial_pool_type = spatial_pool_type\n        self.with_cls = with_cls\n        self.with_reg = with_reg\n        self.roi_feat_size = roi_feat_size\n        self.in_channels = in_channels\n        self.num_classes = num_classes\n        self.target_means = target_means\n        self.target_stds = target_stds\n        self.multilabel_classification = multilabel_classification\n        self.reg_class_agnostic = reg_class_agnostic\n        self.nms_class_agnostic = nms_class_agnostic\n\n        in_channels = self.in_channels\n        if self.with_temporal_pool:\n            if self.temporal_pool_type == \'avg\':\n                self.temporal_pool = nn.AvgPool3d((roi_feat_size[0], 1, 1))\n            else:\n                self.temporal_pool = nn.MaxPool3d((roi_feat_size[0], 1, 1))\n        if self.with_spatial_pool:\n            if self.spatial_pool_type == \'avg\':\n                self.spatial_pool = nn.AvgPool3d((1, roi_feat_size[1], roi_feat_size[2]))\n            else:\n                self.spatial_pool = nn.MaxPool3d((1, roi_feat_size[1], roi_feat_size[2]))\n        if not self.with_temporal_pool and not self.with_spatial_pool:\n            in_channels *= (self.roi_feat_size * self.roi_feat_size)\n        if self.with_cls:\n            self.fc_cls = nn.Linear(in_channels, num_classes)\n        if self.with_reg:\n            out_dim_reg = 4 if reg_class_agnostic else 4 * num_classes\n            self.fc_reg = nn.Linear(in_channels, out_dim_reg)\n        self.debug_imgs = None\n\n    def init_weights(self):\n        if self.with_cls:\n            nn.init.normal_(self.fc_cls.weight, 0, 0.01)\n            nn.init.constant_(self.fc_cls.bias, 0)\n        if self.with_reg:\n            nn.init.normal_(self.fc_reg.weight, 0, 0.001)\n            nn.init.constant_(self.fc_reg.bias, 0)\n\n    def forward(self, x):\n        if self.with_temporal_pool:\n            x = self.temporal_pool(x)\n        if self.with_spatial_pool:\n            x = self.spatial_pool(x)\n        x = x.view(x.size(0), -1)\n        cls_score = self.fc_cls(x) if self.with_cls else None\n        bbox_pred = self.fc_reg(x) if self.with_reg else None\n        return cls_score, bbox_pred\n\n    def get_target(self, sampling_results, gt_bboxes, gt_labels,\n                   rcnn_train_cfg):\n        pos_proposals = [res.pos_bboxes for res in sampling_results]\n        neg_proposals = [res.neg_bboxes for res in sampling_results]\n        pos_gt_bboxes = [res.pos_gt_bboxes for res in sampling_results]\n        pos_gt_labels = [res.pos_gt_labels for res in sampling_results]\n        reg_classes = 1 if self.reg_class_agnostic else self.num_classes\n        cls_reg_targets = bbox_target(\n            pos_proposals,\n            neg_proposals,\n            pos_gt_bboxes,\n            pos_gt_labels,\n            rcnn_train_cfg,\n            reg_classes,\n            target_means=self.target_means,\n            target_stds=self.target_stds)\n        return cls_reg_targets\n\n    def loss(self,\n             cls_score,\n             bbox_pred,\n             labels,\n             label_weights,\n             bbox_targets,\n             bbox_weights,\n             class_weights,\n             reduce=True):\n        losses = dict()\n        if cls_score is not None:\n            if not self.multilabel_classification:\n                assert len(labels[0]) == 1\n                losses[\'loss_cls\'] = weighted_cross_entropy(\n                    cls_score, labels, label_weights, reduce=reduce)\n                losses[\'acc\'] = accuracy(cls_score, labels)\n            else:\n                # cls_score = cls_score.sigmoid()\n                losses[\'loss_person_cls\'] = weighted_binary_cross_entropy(\n                    cls_score[:, 0], labels[:, 0] >= 1, label_weights)\n                pos_inds = torch.nonzero(labels[:, 0] > 0).squeeze(1)\n                losses[\'loss_action_cls\'] = weighted_multilabel_binary_cross_entropy(\n                    cls_score[pos_inds, 1:], labels[pos_inds, :], class_weights[pos_inds, 1:])\n                acc, recall_thr, prec_thr, recall_k, prec_k = multilabel_accuracy(\n                    cls_score, labels, topk=(3,5), thr=0.5)\n                losses[\'acc\'] = acc\n                losses[\'recall@thr=0.5\'] = recall_thr\n                losses[\'prec@thr=0.5\'] = prec_thr\n                losses[\'recall@top3\'] = recall_k[0]\n                losses[\'prec@top3\'] = prec_k[0]\n                losses[\'recall@top5\'] = recall_k[1]\n                losses[\'prec@top5\'] = prec_k[1]\n        if bbox_pred is not None:\n            pos_inds = labels > 0\n            if self.reg_class_agnostic:\n                pos_inds = labels[:, 0] > 0\n                pos_bbox_pred = bbox_pred.view(bbox_pred.size(0), 4)[pos_inds]\n            else:\n                pos_inds = labels > 0\n                pos_bbox_pred = bbox_pred.view(bbox_pred.size(0), -1,\n                                               4)[pos_inds, labels[pos_inds]]\n            losses[\'loss_reg\'] = weighted_smoothl1(\n                pos_bbox_pred,\n                bbox_targets[pos_inds],\n                bbox_weights[pos_inds],\n                avg_factor=bbox_targets.size(0))\n        return losses\n\n    def get_det_bboxes(self,\n                       rois,\n                       cls_score,\n                       bbox_pred,\n                       img_shape,\n                       scale_factor,\n                       rescale=False,\n                       cfg=None,\n                       crop_quadruple=None):\n        if isinstance(cls_score, list):\n            cls_score = sum(cls_score) / float(len(cls_score))\n        if not self.multilabel_classification:\n            scores = F.softmax(cls_score, dim=1) if cls_score is not None else None\n        else:\n            scores = cls_score.sigmoid() if cls_score is not None else None\n\n        if bbox_pred is not None:\n            bboxes = delta2bbox(rois[:, 1:], bbox_pred, self.target_means,\n                                self.target_stds, img_shape)\n        else:\n            bboxes = rois[:, 1:]\n            # TODO: add clip here\n\n        def _bbox_crop_undo(bboxes, crop_quadruple):\n            assert bboxes.shape[-1] % 4 == 0\n            assert crop_quadruple is not None\n            decropped = bboxes.clone()\n            x1, y1, tw, th = crop_quadruple\n            decropped[..., 0::2] = bboxes[..., 0::2] + x1\n            decropped[..., 1::2] = bboxes[..., 1::2] + y1\n            return decropped\n\n        if crop_quadruple is not None:\n            bboxes = _bbox_crop_undo(bboxes, crop_quadruple)\n\n        if rescale:\n            bboxes /= scale_factor\n\n        if cfg is None:\n            return bboxes, scores\n        else:\n            # TODO: apply class-agnostic nms\n            if self.nms_class_agnostic:\n                det_bboxes, det_labels = singleclass_nms(\n                    bboxes, scores, cfg.score_thr, cfg.nms, cfg.max_per_img)\n            else:\n                det_bboxes, det_labels = multiclass_nms(\n                    bboxes, scores, cfg.score_thr, cfg.nms, cfg.max_per_img)\n\n            return det_bboxes, det_labels\n\n    def refine_bboxes(self, rois, labels, bbox_preds, pos_is_gts, img_metas):\n        """"""Refine bboxes during training.\n        Args:\n            rois (Tensor): Shape (n*bs, 5), where n is image number per GPU,\n                and bs is the sampled RoIs per image.\n            labels (Tensor): Shape (n*bs, ).\n            bbox_preds (Tensor): Shape (n*bs, 4) or (n*bs, 4*#class).\n            pos_is_gts (list[Tensor]): Flags indicating if each positive bbox\n                is a gt bbox.\n            img_metas (list[dict]): Meta info of each image.\n        Returns:\n            list[Tensor]: Refined bboxes of each image in a mini-batch.\n        """"""\n        img_ids = rois[:, 0].long().unique(sorted=True)\n        assert img_ids.numel() == len(img_metas)\n\n        bboxes_list = []\n        for i in range(len(img_metas)):\n            inds = torch.nonzero(rois[:, 0] == i).squeeze()\n            num_rois = inds.numel()\n\n            bboxes_ = rois[inds, 1:]\n            label_ = labels[inds]\n            bbox_pred_ = bbox_preds[inds]\n            img_meta_ = img_metas[i]\n            pos_is_gts_ = pos_is_gts[i]\n\n            bboxes = self.regress_by_class(bboxes_, label_, bbox_pred_,\n                                           img_meta_)\n            # filter gt bboxes\n            pos_keep = 1 - pos_is_gts_\n            keep_inds = pos_is_gts_.new_ones(num_rois)\n            keep_inds[:len(pos_is_gts_)] = pos_keep\n\n            bboxes_list.append(bboxes[keep_inds])\n\n        return bboxes_list\n\n    def regress_by_class(self, rois, label, bbox_pred, img_meta):\n        """"""Regress the bbox for the predicted class. Used in Cascade R-CNN.\n        Args:\n            rois (Tensor): shape (n, 4) or (n, 5)\n            label (Tensor): shape (n, )\n            bbox_pred (Tensor): shape (n, 4*(#class+1)) or (n, 4)\n            img_meta (dict): Image meta info.\n        Returns:\n            Tensor: Regressed bboxes, the same shape as input rois.\n        """"""\n        assert rois.size(1) == 4 or rois.size(1) == 5\n\n        if not self.reg_class_agnostic:\n            label = label * 4\n            inds = torch.stack((label, label + 1, label + 2, label + 3), 1)\n            bbox_pred = torch.gather(bbox_pred, 1, inds)\n        assert bbox_pred.size(1) == 4\n\n        if rois.size(1) == 4:\n            new_rois = delta2bbox(rois, bbox_pred, self.target_means,\n                                  self.target_stds, img_meta[\'img_shape\'])\n        else:\n            bboxes = delta2bbox(rois[:, 1:], bbox_pred, self.target_means,\n                                self.target_stds, img_meta[\'img_shape\'])\n            new_rois = torch.cat((rois[:, [0]], bboxes), dim=1)\n\n        return new_rois\n'"
mmaction/models/tenons/cls_heads/__init__.py,0,"b""from .cls_head import ClsHead\nfrom .ssn_head import SSNHead\n\n__all__ = [\n    'ClsHead', 'SSNHead'\n]\n"""
mmaction/models/tenons/cls_heads/cls_head.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom ...registry import HEADS\n\n@HEADS.register_module\nclass ClsHead(nn.Module):\n    """"""Simplest classification head""""""\n\n    def __init__(self,\n                 with_avg_pool=True,\n                 temporal_feature_size=1,\n                 spatial_feature_size=7,\n                 dropout_ratio=0.8,\n                 in_channels=2048,\n                 num_classes=101,\n\t\t init_std=0.01,\n                 fcn_testing=False):\n\n        super(ClsHead, self).__init__()\n\n        self.with_avg_pool = with_avg_pool\n        self.dropout_ratio = dropout_ratio\n        self.in_channels = in_channels\n        self.dropout_ratio = dropout_ratio\n        self.temporal_feature_size = temporal_feature_size\n        self.spatial_feature_size = spatial_feature_size\n        self.init_std = init_std\n        self.fcn_testing = fcn_testing\n        self.num_classes = num_classes\n\n        if self.dropout_ratio != 0:\n            self.dropout = nn.Dropout(p=self.dropout_ratio)\n        else:\n            self.dropout = None\n        if self.with_avg_pool:\n            self.avg_pool = nn.AvgPool3d((temporal_feature_size, spatial_feature_size, spatial_feature_size))\n\n        self.fc_cls = nn.Linear(in_channels, num_classes)\n        self.new_cls = None\n\n    def init_weights(self):\n        nn.init.normal_(self.fc_cls.weight, 0, self.init_std)\n        nn.init.constant_(self.fc_cls.bias, 0)\n\n    def forward(self, x):\n        if not self.fcn_testing:\n            if x.ndimension() == 4:\n                x = x.unsqueeze(2)\n            assert x.shape[1] == self.in_channels\n            assert x.shape[2] == self.temporal_feature_size\n            assert x.shape[3] == self.spatial_feature_size\n            assert x.shape[4] == self.spatial_feature_size\n            if self.with_avg_pool:\n                x = self.avg_pool(x)\n            if self.dropout is not None:\n                x = self.dropout(x)\n            x = x.view(x.size(0), -1)\n\n            cls_score = self.fc_cls(x)\n            return cls_score\n        else:\n            if x.ndimension() == 4:\n                x = x.unsqueeze(2)\n            if self.with_avg_pool:\n                x = self.avg_pool(x)\n            if self.new_cls is None:\n                self.new_cls = nn.Conv3d(self.in_channels, self.num_classes, 1,1,0).cuda()\n                self.new_cls.load_state_dict({\'weight\': self.fc_cls.weight.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1),\n                                              \'bias\': self.fc_cls.bias})\n            class_map = self.new_cls(x)\n            return class_map\n\n    def loss(self,\n             cls_score,\n             labels):\n        losses = dict()\n        losses[\'loss_cls\'] = F.cross_entropy(cls_score, labels)\n\n        return losses\n'"
mmaction/models/tenons/cls_heads/ssn_head.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom ...registry import HEADS\nfrom mmaction.losses import completeness_loss, classwise_regression_loss\n\n@HEADS.register_module\nclass SSNHead(nn.Module):\n    """"""SSN\'s classification head""""""\n\n    def __init__(self,\n                 dropout_ratio=0.8,\n                 in_channels_activity=3072,\n                 in_channels_complete=3072,\n                 num_classes=20,\n                 with_bg=False,\n                 with_reg=True,\n\t\t init_std=0.001):\n    \n        super(SSNHead, self).__init__()\n\n        self.dropout_ratio = dropout_ratio\n        self.in_channels_activity = in_channels_activity\n        self.in_channels_complete = in_channels_complete\n        self.num_classes = num_classes - 1 if with_bg else num_classes\n        self.with_reg = with_reg\n        self.init_std = init_std\n\n        if self.dropout_ratio != 0:\n            self.dropout = nn.Dropout(p=self.dropout_ratio)\n        else:\n            self.dropout = None\n\n        self.activity_fc = nn.Linear(in_channels_activity, num_classes + 1)\n        self.completeness_fc = nn.Linear(in_channels_complete, num_classes)\n        if self.with_reg:\n            self.regressor_fc = nn.Linear(in_channels_complete, num_classes * 2)\n\n\n    def init_weights(self):\n        nn.init.normal_(self.activity_fc.weight, 0, self.init_std)\n        nn.init.constant_(self.activity_fc.bias, 0)\n        nn.init.normal_(self.completeness_fc.weight, 0, self.init_std)\n        nn.init.constant_(self.completeness_fc.bias, 0)\n        if self.with_reg:\n            nn.init.normal_(self.regressor_fc.weight, 0, self.init_std)\n            nn.init.constant_(self.regressor_fc.bias, 0)\n\n    def prepare_test_fc(self, stpp_feat_multiplier):\n        # TODO: support the case of standalone=False\n        self.test_fc = nn.Linear(self.activity_fc.in_features,\n                                 self.activity_fc.out_features\n                                 + self.completeness_fc.out_features * stpp_feat_multiplier\n                                 + (self.regressor_fc.out_features * stpp_feat_multiplier if self.with_reg else 0))\n        reorg_comp_weight = self.completeness_fc.weight.data.view(\n                self.completeness_fc.out_features, stpp_feat_multiplier,\n                self.activity_fc.in_features).transpose(0, 1).contiguous().view(-1, self.activity_fc.in_features)\n        reorg_comp_bias = self.completeness_fc.bias.data.view(1, -1).expand(\n                stpp_feat_multiplier, self.completeness_fc.out_features).contiguous().view(-1) / stpp_feat_multiplier\n\n        weight = torch.cat((self.activity_fc.weight.data, reorg_comp_weight))\n        bias = torch.cat((self.activity_fc.bias.data, reorg_comp_bias))\n\n        if self.with_reg:\n            reorg_reg_weight = self.regressor_fc.weight.data.view(\n                    self.regressor_fc.out_features, stpp_feat_multiplier,\n                    self.activity_fc.in_features).transpose(0, 1).contiguous().view(-1, self.activity_fc.in_features)\n            reorg_reg_bias = self.regressor_fc.bias.data.view(1, -1).expand(\n                    stpp_feat_multiplier, self.regressor_fc.out_features).contiguous().view(-1) / stpp_feat_multiplier\n            weight = torch.cat((weight, reorg_reg_weight))\n            bias = torch.cat((bias, reorg_reg_bias))\n\n        self.test_fc.weight.data = weight\n        self.test_fc.bias.data = bias\n        return True\n\n\n    def forward(self, input, test_mode=False):\n        if not test_mode:\n            activity_feat, completeness_feat = input\n            if self.dropout is not None:\n                activity_feat = self.dropout(activity_feat)\n                completeness_feat = self.dropout(completeness_feat)\n\n            act_score = self.activity_fc(activity_feat)\n            comp_score = self.completeness_fc(completeness_feat)\n            bbox_pred = self.regressor_fc(completeness_feat) if self.with_reg else None\n\n            return act_score, comp_score, bbox_pred\n        else:\n            test_score = self.test_fc(input)\n            return test_score\n\n    def loss(self,\n             act_score,\n             comp_score,\n             bbox_pred,\n             prop_type,\n             labels,\n             bbox_targets,\n             train_cfg):\n        losses = dict()\n\n        prop_type = prop_type.view(-1)\n        labels = labels.view(-1)\n        act_indexer = ((prop_type == 0) + (prop_type == 2)).nonzero().squeeze()\n        comp_indexer = ((prop_type == 0) + (prop_type == 1)).nonzero().squeeze()\n\n\n        denum = train_cfg.ssn.sampler.fg_ratio + train_cfg.ssn.sampler.bg_ratio + train_cfg.ssn.sampler.incomplete_ratio\n        fg_per_video = int(train_cfg.ssn.sampler.num_per_video * (train_cfg.ssn.sampler.fg_ratio / denum))\n        bg_per_video = int(train_cfg.ssn.sampler.num_per_video * (train_cfg.ssn.sampler.bg_ratio / denum))\n        incomplete_per_video = train_cfg.ssn.sampler.num_per_video - fg_per_video - bg_per_video\n\n        losses[\'loss_act\'] = F.cross_entropy(act_score[act_indexer, :],\n                                             labels[act_indexer])\n        losses[\'loss_comp\'] = completeness_loss(comp_score[comp_indexer, :],\n                                                labels[comp_indexer],\n                                                fg_per_video,\n                                                fg_per_video + incomplete_per_video,\n                                                ohem_ratio=fg_per_video / incomplete_per_video)\n        losses[\'loss_comp\'] = losses[\'loss_comp\'] * train_cfg.ssn.loss_weight.comp_loss_weight\n        if bbox_pred is not None:\n            reg_indexer = (prop_type == 0).nonzero().squeeze()\n            bbox_targets = bbox_targets.view(-1, 2)\n            bbox_pred = bbox_pred.view(-1, self.completeness_fc.out_features, 2)\n            losses[\'loss_reg\'] = classwise_regression_loss(bbox_pred[reg_indexer, :, :],\n                                                           labels[reg_indexer],\n                                                           bbox_targets[reg_indexer, :])\n            losses[\'loss_reg\'] = losses[\'loss_reg\'] * train_cfg.ssn.loss_weight.reg_loss_weight\n        return losses\n'"
mmaction/models/tenons/flownets/__init__.py,0,"b'from .motionnet import MotionNet\n\n__all__ = [\n    ""MotionNet"",\n]\n'"
mmaction/models/tenons/flownets/motionnet.py,51,"b'import logging\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ....ops.resample2d_package.resample2d import Resample2d\nfrom ....losses import charbonnier_loss, SSIM_loss\nfrom mmcv.cnn import kaiming_init\nfrom mmcv.runner import load_checkpoint\n\nfrom ...registry import FLOWNETS\n\ndef make_smoothness_mask(batch, height, width, tensor_type):\n    mask = torch.ones(batch, 2, height, width).type(tensor_type)\n    mask[: 1, -1, :] = 0\n    mask[: 0, :, -1] = 0\n    return mask\n\ndef make_border_mask(batch, channels, height, width, tensor_type, border_ratio=0.1):\n    border_width = round(border_ratio * min(height, width))\n    mask = torch.ones(batch, channels, height, width).type(tensor_type)\n    mask[:, :, :border_width, :] = 0\n    mask[:, :, -border_width:, :] = 0\n    mask[:, :, :border_width, :] = 0\n    mask[:, :, -border_width:, :] = 0\n    return mask\n\n@FLOWNETS.register_module\nclass MotionNet(nn.Module):\n\n\n    def __init__(self,\n                 num_frames=1,\n                 rgb_disorder=False,\n                 scale=0.0039216,\n                 out_loss_indices=(0, 1, 2, 3, 4),\n                 out_prediction_indices=(0, 1, 2, 3, 4),\n                 out_prediction_rescale=True,\n                 frozen=False,\n                 use_photometric_loss=True,\n                 use_ssim_loss=True,\n                 use_smoothness_loss=True,\n                 photometric_loss_weights=(1, 1, 1, 1, 1),\n                 ssim_loss_weights=(0.16, 0.08, 0.04, 0.02, 0.01),\n                 smoothness_loss_weights=(1, 1, 1, 1, 1),\n                 pretrained=None):\n        super(MotionNet, self).__init__()\n        self.num_frames = num_frames\n        self.rgb_disorder = rgb_disorder\n        self.scale = scale\n        self.out_loss_indices = out_loss_indices\n        self.out_prediction_indices = out_prediction_indices\n        self.out_prediction_rescale = out_prediction_rescale\n        self.use_photometric_loss = use_photometric_loss\n        self.use_ssim_loss = use_ssim_loss\n        self.use_smoothness_loss = use_smoothness_loss\n        if frozen:\n            self.use_photometric_loss = False\n            self.use_ssim_loss = False\n            self.use_smoothness_loss = False\n        self.frozen = frozen\n        self.photometric_loss_weights = photometric_loss_weights\n        self.ssim_loss_weights = ssim_loss_weights\n        self.smoothness_loss_weights = smoothness_loss_weights\n        if use_photometric_loss:\n            assert(len(out_prediction_indices) == len(photometric_loss_weights))\n        if use_ssim_loss:\n            assert(len(out_prediction_indices) == len(ssim_loss_weights))\n        if use_smoothness_loss:\n            assert(len(out_prediction_indices) == len(smoothness_loss_weights))\n        self.pretrained=pretrained\n        inplace = True\n        self.lrn = nn.LocalResponseNorm(9, alpha=1, beta=0.5)  # norm images for photometric losses\n        self.conv1 = nn.Conv2d(3*(num_frames+1), 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=True)\n        self.relu1 = nn.LeakyReLU(negative_slope=0.1, inplace=inplace)\n        self.conv1_1 = nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=True)\n        self.relu1_1 = nn.LeakyReLU(negative_slope=0.1, inplace=inplace)\n        self.conv2 = nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=True)\n        self.relu2 = nn.LeakyReLU(negative_slope=0.1, inplace=inplace)\n        self.conv2_1 = nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=True)\n        self.relu2_1 = nn.LeakyReLU(negative_slope=0.1, inplace=inplace)\n        self.conv3 = nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=True)\n        self.relu3 = nn.LeakyReLU(negative_slope=0.1, inplace=inplace)\n        self.conv3_1 = nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=True)\n        self.relu3_1 = nn.LeakyReLU(negative_slope=0.1, inplace=inplace)\n        self.conv4 = nn.Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=True)\n        self.relu4 = nn.LeakyReLU(negative_slope=0.1, inplace=inplace)\n        self.conv4_1 = nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=True)\n        self.relu4_1 = nn.LeakyReLU(negative_slope=0.1, inplace=inplace)\n        self.conv5 = nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=True)\n        self.relu5 = nn.LeakyReLU(negative_slope=0.1, inplace=inplace)\n        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=True)\n        self.relu5_1 = nn.LeakyReLU(negative_slope=0.1, inplace=inplace)\n        #### bottleneck layer ####\n        self.conv6 = nn.Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=True)\n        self.relu6 = nn.LeakyReLU(negative_slope=0.1, inplace=inplace)\n        self.conv6_1 = nn.Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=True)\n        self.relu6_1 = nn.LeakyReLU(negative_slope=0.1, inplace=inplace)\n        \n        self.conv_pr6 = nn.Conv2d(1024, 2*num_frames, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=True)\n        \n        self.warp6 = Resample2d()\n        self.warp5 = Resample2d()\n        self.warp4 = Resample2d()\n        self.warp3 = Resample2d()\n        self.warp2 = Resample2d()\n\n        #### FlowDelta filter (fixed) ####\n        self.conv_FlowDelta = nn.Conv2d(1, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        self.conv_FlowDelta.weight = nn.Parameter(torch.Tensor([[[[0,0,0],[0,1,-1],[0,0,0]]],[[[0,0,0],[0,1,0],[0,-1,0]]]]))\n        self.conv_FlowDelta.weight.requires_grad = False\n        \n        self.deconv5 = nn.ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n        self.relu_up5 = nn.LeakyReLU(negative_slope=0.1, inplace=inplace)\n        self.upsample_flow6to5 = nn.ConvTranspose2d(2*self.num_frames, 2*self.num_frames, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n        self.smooth_conv5 = nn.Conv2d(2*(self.num_frames+512), 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=True)\n        self.conv_pr5 = nn.Conv2d(512, 2*num_frames, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=True)\n        \n        self.deconv4 = nn.ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n        self.relu_up4 = nn.LeakyReLU(negative_slope=0.1, inplace=inplace)\n        self.upsample_flow5to4 = nn.ConvTranspose2d(2*self.num_frames, 2*self.num_frames, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n        self.smooth_conv4 = nn.Conv2d((2*self.num_frames+512+256), 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=True)\n        self.conv_pr4 = nn.Conv2d(256, 2*num_frames, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=True)\n        \n        self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n        self.relu_up3 = nn.LeakyReLU(negative_slope=0.1, inplace=inplace)\n        self.upsample_flow4to3 = nn.ConvTranspose2d(2*self.num_frames, 2*self.num_frames, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n        self.smooth_conv3 = nn.Conv2d((2*self.num_frames+256+128), 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=True)\n        self.conv_pr3 = nn.Conv2d(128, 2*num_frames, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=True)\n\n        self.deconv2 = nn.ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n        self.relu_up2 = nn.LeakyReLU(negative_slope=0.1, inplace=inplace)\n        self.upsample_flow3to2 = nn.ConvTranspose2d(2*self.num_frames, 2*self.num_frames, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n        self.smooth_conv2 = nn.Conv2d((2*self.num_frames+128+64), 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=True)\n        self.conv_pr2 = nn.Conv2d(64, 2*num_frames, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=True)\n\n        self.init_weights()\n\n    @property\n    def flip_rgb(self):\n        return self.rgb_disorder\n\n    @property\n    def multiframe(self):\n        return self.num_frames > 1\n\n    def forward(self, x, train=True):\n        assert(x.ndimension() == 4)\n        scaling = torch.tensor(self.scale * (self.num_frames + 1)).type(x.type()).unsqueeze(0).unsqueeze(2).unsqueeze(3)\n        x = x * scaling\n        imgs = torch.split(x, 3, 1)\n        assert(len(imgs) == self.num_frames + 1)\n        imgs_norm = [self.lrn(imgs[i]) for i in range(self.num_frames + 1)]\n\n        conv1 = self.conv1(x)\n        conv1_relu = self.relu1(conv1)\n        conv1_1 = self.conv1_1(conv1_relu)\n        conv1_1_relu = self.relu1_1(conv1_1)\n        conv2 = self.conv2(conv1_1_relu)\n        conv2_relu = self.relu2(conv2)\n        conv2_1 = self.conv2_1(conv2_relu)\n        conv2_1_relu = self.relu2_1(conv2_1)\n        conv3 = self.conv3(conv2_1_relu)\n        conv3_relu = self.relu3(conv3)\n        conv3_1 = self.conv3_1(conv3_relu)\n        conv3_1_relu = self.relu3_1(conv3_1)\n        conv4 = self.conv4(conv3_1_relu)\n        conv4_relu = self.relu4(conv4)\n        conv4_1 = self.conv4_1(conv4_relu)\n        conv4_1_relu = self.relu4_1(conv4_1)\n        conv5 = self.conv5(conv4_1_relu)\n        conv5_relu = self.relu5(conv5)\n        conv5_1 = self.conv5_1(conv5_relu)\n        conv5_1_relu = self.relu5_1(conv5_1)\n\n        conv6 = self.conv6(conv5_1_relu)\n        conv6_relu = self.relu6(conv6)\n        conv6_1 = self.conv6_1(conv6_relu)\n        conv6_1_relu = self.relu6_1(conv6_1)\n        predict_flow6 = self.conv_pr6(conv6_1_relu)\n\n        predictions_outs = []\n        photometric_loss_outs = []\n        ssim_loss_outs = []\n        smoothness_loss_outs = []\n        FlowScale6 = predict_flow6 * 0.625\n        \n        if train:\n            #### for loss 6 ####\n            predict_flow6_xs = torch.split(predict_flow6, 2, 1)\n            FlowScale6_xs = torch.split(FlowScale6, 2, 1)\n            downsampled_imgs_6 = [F.interpolate(img_norm, size=predict_flow6_xs[0].size()[-2:], mode=\'bilinear\') for img_norm in imgs_norm]\n            #### warp img1 to back img0 ####\n            #### for photometric loss ####\n            #### for SSIM loss ####\n            Warped6_xs = [self.warp6(downsampled_imgs_6[i+1].contiguous(), FlowScale6_xs[i].contiguous()) for i in range(self.num_frames)]\n            downsampled6_input_concat = torch.cat(downsampled_imgs_6[: self.num_frames], 1)\n            warped6_concat = torch.cat(Warped6_xs, 1)\n            PhotoDifference6 = downsampled6_input_concat - warped6_concat\n            #### flow gradients ####\n            #### for smoothness loss ####\n            U6 = predict_flow6[:, ::2, ...]\n            V6 = predict_flow6[:, 1::2, ...]\n            FlowDeltasU6 = self.conv_FlowDelta(U6.view(-1, 1, U6.size(2), U6.size(3))).view(-1, self.num_frames, 2, U6.size(2), U6.size(3))\n            FlowDeltasU6_xs = torch.split(FlowDeltasU6, 1, 1)\n            FlowDeltasV6 = self.conv_FlowDelta(V6.view(-1, 1, V6.size(2), V6.size(3))).view(-1, self.num_frames, 2, V6.size(2), V6.size(3))\n            FlowDeltasV6_xs = torch.split(FlowDeltasV6, 1, 1)\n\n            SmoothnessMask6 = make_smoothness_mask(U6.size(0), U6.size(2), U6.size(3), U6.type())\n            FlowDeltasUClean6_xs = [FlowDeltasU6_x.squeeze() * SmoothnessMask6 for FlowDeltasU6_x in FlowDeltasU6_xs]\n            FlowDeltasVClean6_xs = [FlowDeltasV6_x.squeeze() * SmoothnessMask6 for FlowDeltasV6_x in FlowDeltasV6_xs]\n            FlowDeltasUClean6 = torch.cat(FlowDeltasUClean6_xs, 1)\n            FlowDeltasVClean6 = torch.cat(FlowDeltasVClean6_xs, 1)\n\n            BorderMask6 = make_border_mask(U6.size(0), 3*U6.size(1), U6.size(2), U6.size(3), U6.type(), border_ratio=0.1)\n\n            photometric_loss_outs.append((PhotoDifference6, BorderMask6))\n            ssim_loss_outs.append((warped6_concat, downsampled6_input_concat))\n            BorderMask6 = make_border_mask(U6.size(0), 2*U6.size(1), U6.size(2), U6.size(3), U6.type(), border_ratio=0.1)\n            smoothness_loss_outs.append((FlowDeltasUClean6, FlowDeltasVClean6, BorderMask6))\n            #### loss 6 ends here ####\n        if self.out_prediction_rescale:\n            predictions_outs.append(FlowScale6)\n        else:\n            predictions_outs.append(predict_flow6)\n\n        deconv5 = self.deconv5(conv6_1_relu)\n        deconv5_relu = self.relu_up5(deconv5)\n        upsampled_flow6_to_5 = self.upsample_flow6to5(predict_flow6)\n        concat5 = torch.cat((conv5_1_relu, deconv5_relu, upsampled_flow6_to_5), 1)\n        smooth_conv5 = self.smooth_conv5(concat5)\n        predict_flow5 = self.conv_pr5(smooth_conv5)\n        FlowScale5 = predict_flow5 * 1.25\n        \n        if train:\n            #### for loss 5 ####\n            predict_flow5_xs = torch.split(predict_flow5, 2, 1)\n            FlowScale5_xs = torch.split(FlowScale5, 2, 1) \n            downsampled_imgs_5 = [F.interpolate(img_norm, size=predict_flow5_xs[0].size()[-2:], mode=\'bilinear\') for img_norm in imgs_norm]\n            #### warp img1 to back img0 ####\n            #### for photometric loss ####\n            #### for SSIM loss ####\n            Warped5_xs = [self.warp5(downsampled_imgs_5[i+1].contiguous(), FlowScale5_xs[i].contiguous()) for i in range(self.num_frames)]\n            downsampled5_input_concat = torch.cat(downsampled_imgs_5[: self.num_frames], 1)\n            warped5_concat = torch.cat(Warped5_xs, 1)\n            PhotoDifference5 = downsampled5_input_concat - warped5_concat\n            #### flow gradients ####\n            #### for smoothness loss ####\n            U5 = predict_flow5[:, ::2, ...]\n            V5 = predict_flow5[:, 1::2, ...]\n            FlowDeltasU5 = self.conv_FlowDelta(U5.view(-1, 1, U5.size(2), U5.size(3))).view(-1, self.num_frames, 2, U5.size(2), U5.size(3))\n            FlowDeltasU5_xs = torch.split(FlowDeltasU5, 1, 1)\n            FlowDeltasV5 = self.conv_FlowDelta(V5.view(-1, 1, V5.size(2), V5.size(3))).view(-1, self.num_frames, 2, V5.size(2), V5.size(3))\n            FlowDeltasV5_xs = torch.split(FlowDeltasV5, 1, 1)\n\n            SmoothnessMask5 = make_smoothness_mask(U5.size(0), U5.size(2), U5.size(3), U5.type())\n            FlowDeltasUClean5_xs = [FlowDeltasU5_x.squeeze() * SmoothnessMask5 for FlowDeltasU5_x in FlowDeltasU5_xs]\n            FlowDeltasVClean5_xs = [FlowDeltasV5_x.squeeze() * SmoothnessMask5 for FlowDeltasV5_x in FlowDeltasV5_xs]\n            FlowDeltasUClean5 = torch.cat(FlowDeltasUClean5_xs, 1)\n            FlowDeltasVClean5 = torch.cat(FlowDeltasVClean5_xs, 1)\n\n            BorderMask5 = make_border_mask(U5.size(0), 3*U5.size(1), U5.size(2), U5.size(3), U5.type(), border_ratio=0.1)\n\n            photometric_loss_outs.append((PhotoDifference5, BorderMask5))\n            ssim_loss_outs.append((warped5_concat, downsampled5_input_concat))\n            BorderMask5 = make_border_mask(U5.size(0), 2*U5.size(1), U5.size(2), U5.size(3), U5.type(), border_ratio=0.1)\n            smoothness_loss_outs.append((FlowDeltasUClean5, FlowDeltasVClean5, BorderMask5))\n            #### loss 5 ends here ####\n        if self.out_prediction_rescale:\n            predictions_outs.append(FlowScale5)\n        else:\n            predictions_outs.append(predict_flow5)\n\n        deconv4 = self.deconv4(smooth_conv5)\n        deconv4_relu = self.relu_up4(deconv4)\n        upsampled_flow5_to_4 = self.upsample_flow5to4(predict_flow5)\n        concat4 = torch.cat((conv4_1_relu, deconv4_relu, upsampled_flow5_to_4), 1)\n        smooth_conv4 = self.smooth_conv4(concat4)\n        predict_flow4 = self.conv_pr4(smooth_conv4)\n        FlowScale4 = predict_flow4 * 2.5\n\n        if train:\n            #### for loss 4 ####\n            predict_flow4_xs = torch.split(predict_flow4, 2, 1)\n            FlowScale4_xs = torch.split(FlowScale4, 2, 1)\n            downsampled_imgs_4 = [F.interpolate(img_norm, size=predict_flow4_xs[0].size()[-2:], mode=\'bilinear\') for img_norm in imgs_norm]\n            #### warp img1 to back img0 ####\n            #### for photometric loss ####\n            #### for SSIM loss ####\n            Warped4_xs = [self.warp4(downsampled_imgs_4[i+1].contiguous(), FlowScale4_xs[i].contiguous()) for i in range(self.num_frames)]\n            downsampled4_input_concat = torch.cat(downsampled_imgs_4[: self.num_frames], 1)\n            warped4_concat = torch.cat(Warped4_xs, 1)\n            PhotoDifference4 = downsampled4_input_concat - warped4_concat\n            #### flow gradients ####\n            #### for smoothness loss ####\n            U4 = predict_flow4[:, ::2, ...]\n            V4 = predict_flow4[:, 1::2, ...]\n            FlowDeltasU4 = self.conv_FlowDelta(U4.view(-1, 1, U4.size(2), U4.size(3))).view(-1, self.num_frames, 2, U4.size(2), U4.size(3))\n            FlowDeltasU4_xs = torch.split(FlowDeltasU4, 1, 1)\n            FlowDeltasV4 = self.conv_FlowDelta(V4.view(-1, 1, V4.size(2), V4.size(3))).view(-1, self.num_frames, 2, V4.size(2), V4.size(3))\n            FlowDeltasV4_xs = torch.split(FlowDeltasV4, 1, 1)\n\n            SmoothnessMask4 = make_smoothness_mask(U4.size(0), U4.size(2), U4.size(3), U4.type())\n            FlowDeltasUClean4_xs = [FlowDeltasU4_x.squeeze() * SmoothnessMask4 for FlowDeltasU4_x in FlowDeltasU4_xs]\n            FlowDeltasVClean4_xs = [FlowDeltasV4_x.squeeze() * SmoothnessMask4 for FlowDeltasV4_x in FlowDeltasV4_xs]\n            FlowDeltasUClean4 = torch.cat(FlowDeltasUClean4_xs, 1)\n            FlowDeltasVClean4 = torch.cat(FlowDeltasVClean4_xs, 1)\n\n            BorderMask4 = make_border_mask(U4.size(0), 3*U4.size(1), U4.size(2), U4.size(3), U4.type(), border_ratio=0.1)\n\n            photometric_loss_outs.append((PhotoDifference4, BorderMask4))\n            ssim_loss_outs.append((warped4_concat, downsampled4_input_concat))\n            BorderMask4 = make_border_mask(U4.size(0), 2*U4.size(1), U4.size(2), U4.size(3), U4.type(), border_ratio=0.1)\n            smoothness_loss_outs.append((FlowDeltasUClean4, FlowDeltasVClean4, BorderMask4))\n            #### loss 4 ends here ####\n        if self.out_prediction_rescale:\n            predictions_outs.append(FlowScale4)\n        else:\n            predictions_outs.append(predict_flow4)\n\n        deconv3 = self.deconv3(smooth_conv4)\n        deconv3_relu = self.relu_up3(deconv3)\n        upsampled_flow4_to_3 = self.upsample_flow4to3(predict_flow4)\n        concat3 = torch.cat((conv3_1_relu, deconv3_relu, upsampled_flow4_to_3), 1)\n        smooth_conv3 = self.smooth_conv3(concat3)\n        predict_flow3 = self.conv_pr3(smooth_conv3)\n        FlowScale3 = predict_flow3 * 5.0\n\n        if train:\n            #### for loss 3 ####\n            predict_flow3_xs = torch.split(predict_flow3, 2, 1)\n            FlowScale3_xs = torch.split(FlowScale3, 2, 1) \n            downsampled_imgs_3 = [F.interpolate(img_norm, size=predict_flow3_xs[0].size()[-2:], mode=\'bilinear\') for img_norm in imgs_norm]\n            #### warp img1 to back img0 ####\n            #### for photometric loss ####\n            #### for SSIM loss ####\n            Warped3_xs = [self.warp3(downsampled_imgs_3[i+1].contiguous(), FlowScale3_xs[i].contiguous()) for i in range(self.num_frames)]\n            downsampled3_input_concat = torch.cat(downsampled_imgs_3[: self.num_frames], 1)\n            warped3_concat = torch.cat(Warped3_xs, 1)\n            PhotoDifference3 = downsampled3_input_concat - warped3_concat\n            #### flow gradients ####\n            #### for smoothness loss ####\n            U3 = predict_flow3[:, ::2, ...]\n            V3 = predict_flow3[:, 1::2, ...]\n            FlowDeltasU3 = self.conv_FlowDelta(U3.view(-1, 1, U3.size(2), U3.size(3))).view(-1, self.num_frames, 2, U3.size(2), U3.size(3))\n            FlowDeltasU3_xs = torch.split(FlowDeltasU3, 1, 1)\n            FlowDeltasV3 = self.conv_FlowDelta(V3.view(-1, 1, V3.size(2), V3.size(3))).view(-1, self.num_frames, 2, V3.size(2), V3.size(3))\n            FlowDeltasV3_xs = torch.split(FlowDeltasV3, 1, 1)\n\n            SmoothnessMask3 = make_smoothness_mask(U3.size(0), U3.size(2), U3.size(3), U3.type())\n            FlowDeltasUClean3_xs = [FlowDeltasU3_x.squeeze() * SmoothnessMask3 for FlowDeltasU3_x in FlowDeltasU3_xs]\n            FlowDeltasVClean3_xs = [FlowDeltasV3_x.squeeze() * SmoothnessMask3 for FlowDeltasV3_x in FlowDeltasV3_xs]\n            FlowDeltasUClean3 = torch.cat(FlowDeltasUClean3_xs, 1)\n            FlowDeltasVClean3 = torch.cat(FlowDeltasVClean3_xs, 1)\n\n            BorderMask3 = make_border_mask(U3.size(0), 3*U3.size(1), U3.size(2), U3.size(3), U3.type(), border_ratio=0.1)\n\n            photometric_loss_outs.append((PhotoDifference3, BorderMask3))\n            ssim_loss_outs.append((warped3_concat, downsampled3_input_concat))\n            BorderMask3 = make_border_mask(U3.size(0), 2*U3.size(1), U3.size(2), U3.size(3), U3.type(), border_ratio=0.1)\n            smoothness_loss_outs.append((FlowDeltasUClean3, FlowDeltasVClean3, BorderMask3))\n            #### loss 3 ends here ####\n        if self.out_prediction_rescale:\n            predictions_outs.append(FlowScale3)\n        else:\n            predictions_outs.append(predict_flow3)\n\n        deconv2 = self.deconv2(smooth_conv3)\n        deconv2_relu = self.relu_up2(deconv2)\n        upsampled_flow3_to_2 = self.upsample_flow3to2(predict_flow3)\n        concat2 = torch.cat((conv2_1_relu, deconv2_relu, upsampled_flow3_to_2), 1)\n        smooth_conv2 = self.smooth_conv2(concat2)\n        predict_flow2 = self.conv_pr2(smooth_conv2)\n        FlowScale2 = predict_flow2 * 10.0\n\n        if train:\n            #### for loss 2 ####\n            predict_flow2_xs = torch.split(predict_flow2, 2, 1)\n            FlowScale2_xs = torch.split(FlowScale2, 2, 1) \n            downsampled_imgs_2 = [F.interpolate(img_norm, size=predict_flow2_xs[0].size()[-2:], mode=\'bilinear\') for img_norm in imgs_norm]\n            #### warp img1 to back img0 ####\n            #### for photometric loss ####\n            #### for SSIM loss ####\n            Warped2_xs = [self.warp2(downsampled_imgs_2[i+1].contiguous(), FlowScale2_xs[i].contiguous()) for i in range(self.num_frames)]\n            downsampled2_input_concat = torch.cat(downsampled_imgs_2[: self.num_frames], 1)\n            warped2_concat = torch.cat(Warped2_xs, 1)\n            PhotoDifference2 = downsampled2_input_concat - warped2_concat\n            #### flow gradients ####\n            #### for smoothness loss ####\n            U2 = predict_flow2[:, ::2, ...]\n            V2 = predict_flow2[:, 1::2, ...]\n            FlowDeltasU2 = self.conv_FlowDelta(U2.view(-1, 1, U2.size(2), U2.size(3))).view(-1, self.num_frames, 2, U2.size(2), U2.size(3))\n            FlowDeltasU2_xs = torch.split(FlowDeltasU2, 1, 1)\n            FlowDeltasV2 = self.conv_FlowDelta(V2.view(-1, 1, V2.size(2), V2.size(3))).view(-1, self.num_frames, 2, V2.size(2), V2.size(3))\n            FlowDeltasV2_xs = torch.split(FlowDeltasV2, 1, 1)\n\n            SmoothnessMask2 = make_smoothness_mask(U2.size(0), U2.size(2), U2.size(3), U2.type())\n            FlowDeltasUClean2_xs = [FlowDeltasU2_x.squeeze() * SmoothnessMask2 for FlowDeltasU2_x in FlowDeltasU2_xs]\n            FlowDeltasVClean2_xs = [FlowDeltasV2_x.squeeze() * SmoothnessMask2 for FlowDeltasV2_x in FlowDeltasV2_xs]\n            FlowDeltasUClean2 = torch.cat(FlowDeltasUClean2_xs, 1)\n            FlowDeltasVClean2 = torch.cat(FlowDeltasVClean2_xs, 1)\n\n            BorderMask2 = make_border_mask(U2.size(0), 3*U2.size(1), U2.size(2), U2.size(3), U2.type(), border_ratio=0.1)\n\n            if self.out_prediction_rescale:\n                predictions_outs.append(FlowScale2)\n            else:\n                predictions_outs.append(predict_flow2)\n            photometric_loss_outs.append((PhotoDifference2, BorderMask2))\n            ssim_loss_outs.append((warped2_concat, downsampled2_input_concat))\n            BorderMask2 = make_border_mask(U2.size(0), 2*U2.size(1), U2.size(2), U2.size(3), U2.type(), border_ratio=0.1)\n            smoothness_loss_outs.append((FlowDeltasUClean2, FlowDeltasVClean2, BorderMask2))\n            #### loss 2 ends here ####\n\n\n        outs_predictions = [predictions_outs[i] for i in self.out_prediction_indices]\n        \n        if train:\n            return tuple(outs_predictions), photometric_loss_outs, ssim_loss_outs, smoothness_loss_outs\n        else:\n            return tuple(outs_predictions), None, None, None\n\n    def init_weights(self):\n        if isinstance(self.pretrained, str):\n            logger = logging.getLogger()\n            load_checkpoint(self, self.pretrained, strict=False, logger=logger)\n        elif self.pretrained is None:\n            for name, m in self.named_modules():\n                if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n                    if name != ""conv_FlowDelta"":\n                        kaiming_init(m)\n                    else:\n                        print(""Fixing conv_FlowDelta"")\n        else:\n            raise TypeError(\'pretrained must be a str or None\')\n\n    def loss(self,\n             photometric_loss_outs,\n             ssim_loss_outs,\n             smoothness_loss_outs,\n             direction=\'forward\'):\n        assert direction in [\'forward\', \'backward\']\n        losses = dict()\n        # outs_photometric = dict()\n        # outs_ssim = dict()\n        # outs_smoothness = dict()\n        if self.use_photometric_loss:\n            for i, ind in enumerate(self.out_loss_indices):\n                losses[\'photometric_loss_{}_{}\'.format(ind, direction)] = self.photometric_loss_weights[i] * charbonnier_loss(photometric_loss_outs[i][0], photometric_loss_outs[i][1], alpha=0.4, beta=255)\n        if self.use_ssim_loss:\n            for i, ind in enumerate(self.out_loss_indices):\n                losses[\'ssim_loss_{}_{}\'.format(ind, direction)] = self.ssim_loss_weights[i] * SSIM_loss(ssim_loss_outs[i][0], ssim_loss_outs[i][1], kernel_size=8, stride=8, c1=0.0001, c2=0.001)\n        if self.use_smoothness_loss:\n            for i, ind in enumerate(self.out_loss_indices):\n                losses[\'smoothness_loss_{}_{}\'.format(ind, direction)] = self.smoothness_loss_weights[i] * charbonnier_loss(smoothness_loss_outs[i][0], smoothness_loss_outs[i][2], alpha=0.3, beta=5) + \\\n                                                           self.smoothness_loss_weights[i] * charbonnier_loss(smoothness_loss_outs[i][1], smoothness_loss_outs[i][2], alpha=0.3, beta=5)\n        return losses \n\n\n    def train(self, mode=True):\n        super(MotionNet, self).train(mode)\n        if self.frozen:\n            for m in self.modules():\n                if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n                    for param in m.parameters():\n                        param.requires_grad = False\n'"
mmaction/models/tenons/necks/__init__.py,0,"b""from .fpn import FPN\n\n__all__ = ['FPN']"""
mmaction/models/tenons/necks/fpn.py,2,"b""import torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import xavier_init\n\nfrom ..utils import ConvModule\nfrom ...registry import NECKS\n\n\n@NECKS.register_module\nclass FPN(nn.Module):\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 num_outs,\n                 start_level=0,\n                 end_level=-1,\n                 add_extra_convs=False,\n                 extra_convs_on_inputs=True,\n                 normalize=None,\n                 activation=None):\n        super(FPN, self).__init__()\n        assert isinstance(in_channels, list)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.num_ins = len(in_channels)\n        self.num_outs = num_outs\n        self.activation = activation\n        self.with_bias = normalize is None\n\n        if end_level == -1:\n            self.backbone_end_level = self.num_ins\n            assert num_outs >= self.num_ins - start_level\n        else:\n            # if end_level < inputs, no extra level is allowed\n            self.backbone_end_level = end_level\n            assert end_level <= len(in_channels)\n            assert num_outs == end_level - start_level\n        self.start_level = start_level\n        self.end_level = end_level\n        self.add_extra_convs = add_extra_convs\n        self.extra_convs_on_inputs = extra_convs_on_inputs\n\n        self.lateral_convs = nn.ModuleList()\n        self.fpn_convs = nn.ModuleList()\n\n        for i in range(self.start_level, self.backbone_end_level):\n            l_conv = ConvModule(\n                in_channels[i],\n                out_channels,\n                1,\n                normalize=normalize,\n                bias=self.with_bias,\n                activation=self.activation,\n                inplace=False)\n            fpn_conv = ConvModule(\n                out_channels,\n                out_channels,\n                3,\n                padding=1,\n                normalize=normalize,\n                bias=self.with_bias,\n                activation=self.activation,\n                inplace=False)\n\n            self.lateral_convs.append(l_conv)\n            self.fpn_convs.append(fpn_conv)\n\n        # add extra conv layers (e.g., RetinaNet)\n        extra_levels = num_outs - self.backbone_end_level + self.start_level\n        if add_extra_convs and extra_levels >= 1:\n            for i in range(extra_levels):\n                if i == 0 and self.extra_convs_on_inputs:\n                    in_channels = self.in_channels[self.backbone_end_level - 1]\n                else:\n                    in_channels = out_channels\n                extra_fpn_conv = ConvModule(\n                    in_channels,\n                    out_channels,\n                    3,\n                    stride=2,\n                    padding=1,\n                    normalize=normalize,\n                    bias=self.with_bias,\n                    activation=self.activation,\n                    inplace=False)\n                self.fpn_convs.append(extra_fpn_conv)\n\n    # default init_weights for conv(msra) and norm in ConvModule\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                xavier_init(m, distribution='uniform')\n\n    def forward(self, inputs):\n        assert len(inputs) == len(self.in_channels)\n\n        # build laterals\n        laterals = [\n            lateral_conv(inputs[i + self.start_level])\n            for i, lateral_conv in enumerate(self.lateral_convs)\n        ]\n\n        # build top-down path\n        used_backbone_levels = len(laterals)\n        for i in range(used_backbone_levels - 1, 0, -1):\n            laterals[i - 1] += F.interpolate(\n                laterals[i], scale_factor=2, mode='nearest')\n\n        # build outputs\n        # part 1: from original levels\n        outs = [\n            self.fpn_convs[i](laterals[i]) for i in range(used_backbone_levels)\n        ]\n        # part 2: add extra levels\n        if self.num_outs > len(outs):\n            # use max pool to get more levels on top of outputs\n            # (e.g., Faster R-CNN, Mask R-CNN)\n            if not self.add_extra_convs:\n                for i in range(self.num_outs - used_backbone_levels):\n                    outs.append(F.max_pool2d(outs[-1], 1, stride=2))\n            # add conv layers on top of original feature maps (RetinaNet)\n            else:\n                if self.extra_convs_on_inputs:\n                    orig = inputs[self.backbone_end_level - 1]\n                    outs.append(self.fpn_convs[used_backbone_levels](orig))\n                else:\n                    outs.append(self.fpn_convs[used_backbone_levels](outs[-1]))\n                for i in range(used_backbone_levels + 1, self.num_outs):\n                    # BUG: we should add relu before each extra conv\n                    outs.append(self.fpn_convs[i](outs[-1]))\n        return tuple(outs)"""
mmaction/models/tenons/roi_extractors/__init__.py,0,"b""from .single_level import SingleRoIExtractor\nfrom .single_level_straight3d import SingleRoIStraight3DExtractor\n\n__all__ = [\n    'SingleRoIExtractor', 'SingleRoIStraight3DExtractor'\n]"""
mmaction/models/tenons/roi_extractors/single_level.py,4,"b'import torch\nimport torch.nn as nn\n\nfrom mmaction import ops\n\nfrom ...registry import ROI_EXTRACTORS\n\n@ROI_EXTRACTORS.register_module\nclass SingleRoIExtractor(nn.Module):\n    """"""Extract RoI features from a single level feature map.\n\n    If there are multiple input feature levels, each RoI is mapped to a level\n    according to its scale.\n\n    Args:\n        roi_layer (dict): Specify RoI layer type and arguments.\n        out_channels (int): Output channels of RoI layers.\n        featmap_strides (int): Strides of input feature maps.\n        finest_scale (int): Scale threshold of mapping to level 0.\n    """"""\n\n    def __init__(self,\n                 roi_layer,\n                 out_channels,\n                 featmap_strides,\n                 finest_scale=56):\n        super(SingleRoIExtractor, self).__init__()\n        self.roi_layers = self.build_roi_layers(roi_layer, featmap_strides)\n        self.out_channels = out_channels\n        self.featmap_strides = featmap_strides\n        self.finest_scale = finest_scale\n\n    @property\n    def num_inputs(self):\n        """"""int: Input feature map levels.""""""\n        return len(self.featmap_strides)\n\n    def init_weights(self):\n        pass\n\n    def build_roi_layers(self, layer_cfg, featmap_strides):\n        cfg = layer_cfg.copy()\n        layer_type = cfg.pop(\'type\')\n        assert hasattr(ops, layer_type)\n        layer_cls = getattr(ops, layer_type)\n        roi_layers = nn.ModuleList(\n            [layer_cls(spatial_scale=1 / s, **cfg) for s in featmap_strides])\n        return roi_layers\n\n    def map_roi_levels(self, rois, num_levels):\n        """"""Map rois to corresponding feature levels by scales.\n\n        - scale < finest_scale: level 0\n        - finest_scale <= scale < finest_scale * 2: level 1\n        - finest_scale * 2 <= scale < finest_scale * 4: level 2\n        - scale >= finest_scale * 4: level 3\n\n        Args:\n            rois (Tensor): Input RoIs, shape (k, 5).\n            num_levels (int): Total level number.\n\n        Returns:\n            Tensor: Level index (0-based) of each RoI, shape (k, )\n        """"""\n        scale = torch.sqrt(\n            (rois[:, 3] - rois[:, 1] + 1) * (rois[:, 4] - rois[:, 2] + 1))\n        target_lvls = torch.floor(torch.log2(scale / self.finest_scale + 1e-6))\n        target_lvls = target_lvls.clamp(min=0, max=num_levels - 1).long()\n        return target_lvls\n\n    def forward(self, feats, rois):\n        if len(feats) == 1:\n            return self.roi_layers[0](feats[0], rois)\n\n        out_size = self.roi_layers[0].out_size\n        num_levels = len(feats)\n        target_lvls = self.map_roi_levels(rois, num_levels)\n        roi_feats = torch.cuda.FloatTensor(rois.size()[0], self.out_channels,\n                                           out_size, out_size).fill_(0)\n        for i in range(num_levels):\n            inds = target_lvls == i\n            if inds.any():\n                rois_ = rois[inds, :]\n                roi_feats_t = self.roi_layers[i](feats[i], rois_)\n                roi_feats[inds] += roi_feats_t\n        return roi_feats\n'"
mmaction/models/tenons/roi_extractors/single_level_straight3d.py,7,"b'import torch\nimport torch.nn as nn\n\nfrom mmaction import ops\n\nfrom ...registry import ROI_EXTRACTORS\n\n@ROI_EXTRACTORS.register_module\nclass SingleRoIStraight3DExtractor(nn.Module):\n    """"""Extract RoI features from a single level feature map.\n\n    If there are multiple input feature levels, each RoI is mapped to a level\n    according to its scale.\n\n    Args:\n        roi_layer (dict): Specify RoI layer type and arguments.\n        out_channels (int): Output channels of RoI layers.\n        featmap_strides (int): Strides of input feature maps.\n        finest_scale (int): Scale threshold of mapping to level 0.\n    """"""\n\n    def __init__(self,\n                 roi_layer,\n                 out_channels,\n                 featmap_strides,\n                 finest_scale=56,\n                 with_temporal_pool=False):\n        super(SingleRoIStraight3DExtractor, self).__init__()\n        self.roi_layers = self.build_roi_layers(roi_layer, featmap_strides)\n        self.out_channels = out_channels\n        self.featmap_strides = featmap_strides\n        self.finest_scale = finest_scale\n        self.with_temporal_pool = with_temporal_pool\n\n    @property\n    def num_inputs(self):\n        """"""int: Input feature map levels.""""""\n        return len(self.featmap_strides)\n\n    def init_weights(self):\n        pass\n\n    def build_roi_layers(self, layer_cfg, featmap_strides):\n        cfg = layer_cfg.copy()\n        layer_type = cfg.pop(\'type\')\n        assert hasattr(ops, layer_type)\n        layer_cls = getattr(ops, layer_type)\n        roi_layers = nn.ModuleList(\n            [layer_cls(spatial_scale=1 / s, **cfg) for s in featmap_strides])\n        return roi_layers\n\n    def map_roi_levels(self, rois, num_levels):\n        """"""Map rois to corresponding feature levels by scales.\n\n        - scale < finest_scale: level 0\n        - finest_scale <= scale < finest_scale * 2: level 1\n        - finest_scale * 2 <= scale < finest_scale * 4: level 2\n        - scale >= finest_scale * 4: level 3\n\n        Args:\n            rois (Tensor): Input RoIs, shape (k, 5).\n            num_levels (int): Total level number.\n\n        Returns:\n            Tensor: Level index (0-based) of each RoI, shape (k, )\n        """"""\n        scale = torch.sqrt(\n            (rois[:, 3] - rois[:, 1] + 1) * (rois[:, 4] - rois[:, 2] + 1))\n        target_lvls = torch.floor(torch.log2(scale / self.finest_scale + 1e-6))\n        target_lvls = target_lvls.clamp(min=0, max=num_levels - 1).long()\n        return target_lvls\n\n    def forward(self, feats, rois):\n        feats = list(feats)\n        if len(feats) == 1:\n            if self.with_temporal_pool:\n                feats[0] = torch.mean(feats[0], 2, keepdim=True)\n            roi_feats = []\n            for t in range(feats[0].size(2)):\n                feat = feats[0][:, :, t, :, :].contiguous()\n                roi_feats.append(self.roi_layers[0](feat, rois))\n            return torch.stack(roi_feats, dim=2)\n\n        if self.with_temporal_pool:\n            for i in range(len(feats)):\n                feats[i] = torch.mean(feats[i], 2, keepdim=True)\n\n        t_size = feats[0].size(2)\n        out_size = self.roi_layers[0].out_size\n        num_levels = len(feats)\n        target_lvls = self.map_roi_levels(rois, num_levels)\n        roi_feats = torch.cuda.FloatTensor(rois.size()[0], self.out_channels,\n                                           t_size, out_size, out_size).fill_(0)\n        for i in range(num_levels):\n            inds = target_lvls == i\n            if inds.any():\n                rois_ = rois[inds, :]\n                for t in range(t_size):\n                    feat_ = feats[i][:, :, t, :, :].contiguous()\n                    roi_feats_t = self.roi_layers[i](feat_, rois_)\n                    roi_feats[inds, :, t, :, :] += roi_feats_t\n        return roi_feats\n'"
mmaction/models/tenons/segmental_consensuses/__init__.py,0,"b""from .simple_consensus import SimpleConsensus\nfrom .stpp import parse_stage_config\nfrom .stpp import StructuredTemporalPyramidPooling\n\n__all__ = [\n    'SimpleConsensus',\n    'StructuredTemporalPyramidPooling',\n    'parse_stage_config'\n]\n"""
mmaction/models/tenons/segmental_consensuses/simple_consensus.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom ...registry import SEGMENTAL_CONSENSUSES\n\nclass _SimpleConsensus(torch.autograd.Function):\n    """"""Simplest segmental consensus module""""""\n\n    def __init__(self,\n                 consensus_type=\'avg\',\n                 dim=1):\n        super(_SimpleConsensus, self).__init__()\n\n        assert consensus_type in [\'avg\']\n        self.consensus_type = consensus_type\n        self.dim = dim\n        self.shape = None\n\n    def forward(self, x):\n        self.shape = x.size()\n        if self.consensus_type == \'avg\':\n            output = x.mean(dim=self.dim, keepdim=True)\n        else:\n            output = None\n        return output\n\n    def backward(self, grad_output):\n        if self.consensus_type == \'avg\':\n            grad_in = grad_output.expand(self.shape) / float(self.shape[self.dim])\n        else:\n            grad_in = None\n        return grad_in\n\n\n@SEGMENTAL_CONSENSUSES.register_module\nclass SimpleConsensus(nn.Module):\n    def __init__(self, consensus_type, dim=1):\n        super(SimpleConsensus, self).__init__()\n\n        assert consensus_type in [\'avg\']\n        self.consensus_type = consensus_type\n        self.dim = dim\n\n    def init_weights(self):\n        pass\n\n    def forward(self, input):\n        return _SimpleConsensus(self.consensus_type, self.dim)(input)'"
mmaction/models/tenons/segmental_consensuses/stpp.py,7,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom ...registry import SEGMENTAL_CONSENSUSES\nimport numpy as np\n\n\ndef parse_stage_config(stage_cfg):\n    if isinstance(stage_cfg, int):\n        return (stage_cfg,), stage_cfg\n    elif isinstance(stage_cfg, tuple) or isinstance(stage_cfg, list):\n        return stage_cfg, sum(stage_cfg)\n    else:\n        raise ValueError(""Incorrect STPP config {}"".format(stage_cfg))\n\n\n@SEGMENTAL_CONSENSUSES.register_module\nclass StructuredTemporalPyramidPooling(nn.Module):\n    def __init__(self, standalong_classifier=False, stpp_cfg=(1, (1,2), 1), num_seg=(2,5,2)):\n        super(StructuredTemporalPyramidPooling, self).__init__()\n\n        self.sc = standalong_classifier\n\n        starting_parts, starting_mult = parse_stage_config(stpp_cfg[0])\n        course_parts, course_mult = parse_stage_config(stpp_cfg[1])\n        ending_parts, ending_mult = parse_stage_config(stpp_cfg[2])\n\n        self.feat_multiplier = starting_mult + course_mult + ending_mult\n        self.parts = (starting_parts, course_parts, ending_parts)\n        self.norm_num = (starting_mult, course_mult, ending_mult)\n\n        self.num_seg = num_seg\n\n    def init_weights(self):\n        pass\n\n    def forward(self, input, scaling):\n        x1 = self.num_seg[0]\n        x2 = x1 + self.num_seg[1]\n        n_seg = x2 + self.num_seg[2]\n\n        feat_dim = input.size(1)\n        src = input.view(-1, n_seg, feat_dim)\n        num_sample = src.size(0)\n\n        scaling = scaling.view(-1, 2)\n\n        def get_stage_stpp(stage_feat, stage_parts, norm_num, scaling):\n            stage_stpp = []\n            stage_len = stage_feat.size(1)\n            for n_part in stage_parts:\n                ticks = torch.arange(0, stage_len + 1e-5, stage_len / n_part)\n                for i in range(n_part):\n                    part_feat = stage_feat[:, int(ticks[i]):int(ticks[i+1]), :].mean(dim=1) / norm_num\n                    if scaling is not None:\n                        part_feat = part_feat * scaling.view(num_sample, 1)\n                    stage_stpp.append(part_feat)\n            return stage_stpp\n\n        feature_parts = []\n        feature_parts.extend(get_stage_stpp(src[:, :x1, :], self.parts[0], self.norm_num[0], scaling[:, 0]))\n        feature_parts.extend(get_stage_stpp(src[:, x1:x2, :], self.parts[1], self.norm_num[1], None))\n        feature_parts.extend(get_stage_stpp(src[:, x2:, :], self.parts[2], self.norm_num[2], scaling[:, 1]))\n        stpp_feat = torch.cat(feature_parts, dim=1)\n        if not self.sc:\n            return stpp_feat, stpp_feat\n        else:\n            course_feat = src[:, x1:x2, :].mean(dim=1)\n            return course_feat, stpp_feat\n\n\n@SEGMENTAL_CONSENSUSES.register_module\nclass STPPReorganized(nn.Module):\n    def __init__(self, feat_dim, act_score_len,\n                 comp_score_len, reg_score_len,\n                 standalong_classifier=False,\n                 with_regression=True,\n                 stpp_cfg=(1, (1,2), 1)):\n        super(STPPReorganized, self).__init__()\n\n        self.sc = standalong_classifier\n        self.feat_dim = feat_dim\n        self.act_score_len = act_score_len\n        self.comp_score_len = comp_score_len\n        self.reg_score_len = reg_score_len\n        self.with_regression = with_regression\n\n        starting_parts, starting_mult = parse_stage_config(stpp_cfg[0])\n        course_parts, course_mult = parse_stage_config(stpp_cfg[1])\n        ending_parts, ending_mult = parse_stage_config(stpp_cfg[2])\n\n        self.feat_multiplier = starting_mult + course_mult + ending_mult\n        self.stpp_cfg = (starting_parts, course_parts, ending_parts)\n\n        self.act_slice = slice(0, self.act_score_len if self.sc else (self.act_score_len * self.feat_multiplier))\n        self.comp_slice = slice(self.act_slice.stop, self.act_slice.stop + self.comp_score_len * self.feat_multiplier)\n        self.reg_slice = slice(self.comp_slice.stop, self.comp_slice.stop + self.reg_score_len * self.feat_multiplier)\n\n\n    def init_weights(self):\n        pass\n\n    def forward(self, input, proposal_ticks, scaling):\n        assert input.size(1) == self.feat_dim\n        n_ticks = proposal_ticks.size(0)\n\n        out_act_scores = torch.zeros((n_ticks, self.act_score_len)).type_as(input)\n        raw_act_scores = input[:, self.act_slice]\n\n        out_comp_scores = torch.zeros((n_ticks, self.comp_score_len)).type_as(input)\n        raw_comp_scores = input[:, self.comp_slice]\n\n        if self.with_regression:\n            out_reg_scores = torch.zeros((n_ticks, self.reg_score_len)).type_as(input)\n            raw_reg_scores = input[:, self.reg_slice]\n        else:\n            out_reg_scores = None\n            raw_reg_scores = None\n\n        def pspool(out_scores, index, raw_scores, ticks, scaling, score_len, stpp_cfg):\n            offset = 0\n            for stage_idx, stage_cfg in enumerate(stpp_cfg):\n                if stage_idx == 0:\n                    s = scaling[0]\n                elif stage_idx == len(stpp_cfg) - 1:\n                    s = scaling[1]\n                else:\n                    s = 1.0\n\n                stage_cnt = sum(stage_cfg)\n                left = ticks[stage_idx]\n                right = max(ticks[stage_idx] + 1, ticks[stage_idx + 1])\n\n                if right <= 0 or left >= raw_scores.size(0):\n                    offset += stage_cnt\n                    continue\n                for n_part in stage_cfg:\n                    part_ticks = np.arange(left, right+1e-5, (right - left) / n_part)\n                    for i in range(n_part):\n                        pl = int(part_ticks[i])\n                        pr = int(part_ticks[i+1])\n                        if pr - pl >= 1:\n                            out_scores[index, :] += raw_scores[pl:pr,\n                                                               offset * score_len : (offset + 1) * score_len].mean(dim=0) * s\n                        offset += 1\n\n        for i in range(n_ticks):\n            ticks = proposal_ticks[i].cpu().numpy()\n            if self.sc:\n                out_act_scores[i, :] = raw_act_scores[ticks[1]: max(ticks[1] + 1, ticks[2]), :].mean(dim=0)\n            else:\n                pspool(out_act_scores, i, raw_act_scores, ticks, scaling[i], self.act_score_len, self.stpp_cfg)\n\n            pspool(out_comp_scores, i, raw_comp_scores, ticks, scaling[i], self.comp_score_len, self.stpp_cfg)\n\n            if self.with_regression:\n                pspool(out_reg_scores, i, raw_reg_scores, ticks, scaling[i], self.reg_score_len, self.stpp_cfg)\n\n        return out_act_scores, out_comp_scores, out_reg_scores\n\n\n\n\n\n\n'"
mmaction/models/tenons/shared_heads/__init__.py,0,"b""from .res_layer import ResLayer\nfrom .res_i3d_layer import ResI3DLayer\n\n__all__ = [\n    'ResLayer', 'ResI3DLayer'\n]"""
mmaction/models/tenons/shared_heads/res_i3d_layer.py,1,"b'import logging\n\nimport torch.nn as nn\nfrom mmcv.cnn import constant_init, kaiming_init\nfrom mmcv.runner import load_checkpoint\n\nfrom ..backbones import ResNet_I3D\nfrom ..backbones.resnet_i3d import make_res_layer\nfrom ...registry import HEADS\n\n\n@HEADS.register_module\nclass ResI3DLayer(nn.Module):\n\n    def __init__(self,\n                 depth,\n                 pretrained=None,\n                 pretrained2d=True,\n                 stage=3,\n                 spatial_stride=2,\n                 temporal_stride=1,\n                 dilation=1,\n                 style=\'pytorch\',\n                 inflate_freq=1,\n                 inflate_style=\'3x1x1\',\n                 bn_eval=True,\n                 bn_frozen=True,\n                 all_frozen=False,\n                 with_cp=False):\n        super(ResI3DLayer, self).__init__()\n        self.bn_eval = bn_eval\n        self.bn_frozen = bn_frozen\n        self.all_frozen = all_frozen\n        self.stage = stage\n        block, stage_blocks = ResNet_I3D.arch_settings[depth]\n        self.pretrained = pretrained\n        self.pretrained2d = pretrained2d\n        stage_block = stage_blocks[stage]\n        planes = 64 * 2**stage\n        inplanes = 64 * 2**(stage - 1) * block.expansion\n\n        self.inflate_freq = inflate_freq if not isinstance(inflate_freq, int) else (inflate_freq, ) * stage\n\n        res_layer = make_res_layer(\n            block,\n            inplanes,\n            planes,\n            stage_block,\n            spatial_stride=spatial_stride,\n            temporal_stride=temporal_stride,\n            dilation=dilation,\n            style=style,\n            inflate_freq=self.inflate_freq,\n            inflate_style=\'3x1x1\',\n            with_cp=with_cp)\n        self.add_module(\'layer{}\'.format(stage + 1), res_layer)\n\n    def init_weights(self):\n        if isinstance(self.pretrained, str):\n            logger = logging.getLogger()\n            if self.pretrained2d:\n                resnet2d = ResNet(self.depth)\n                load_checkpoint(resnet2d, self.pretrained, strict=False, logger=logger)\n                for name, module in self.named_modules():\n                    if isinstance(module, NonLocalModule):\n                        module.init_weights()\n                    elif isinstance(module, nn.Conv3d) and rhasattr(resnet2d, name):\n                        new_weight = rgetattr(resnet2d, name).weight.data.unsqueeze(2).expand_as(module.weight) / module.weight.data.shape[2]\n                        module.weight.data.copy_(new_weight)\n                        logging.info(""{}.weight loaded from weights file into {}"".format(name, new_weight.shape))\n                        if hasattr(module, \'bias\') and module.bias is not None:\n                            new_bias = rgetattr(resnet2d, name).bias.data\n                            module.bias.data.copy_(new_bias)\n                            logging.info(""{}.bias loaded from weights file into {}"".format(name, new_bias.shape))\n                    elif isinstance(module, nn.BatchNorm3d) and rhasattr(resnet2d, name):\n                        for attr in [\'weight\', \'bias\', \'running_mean\', \'running_var\']:\n                            logging.info(""{}.{} loaded from weights file into {}"".format(name, attr, getattr(rgetattr(resnet2d, name), attr).shape))\n                            setattr(module, attr, getattr(rgetattr(resnet2d, name), attr))\n            else:\n                load_checkpoint(self, self.pretrained, strict=False, logger=logger)\n        elif self.pretrained is None:\n            for m in self.modules():\n                if isinstance(m, nn.Conv2d):\n                    kaiming_init(m)\n                elif isinstance(m, nn.BatchNorm2d):\n                    constant_init(m, 1)\n        else:\n            raise TypeError(\'pretrained must be a str or None\')\n\n    def forward(self, x):\n        res_layer = getattr(self, \'layer{}\'.format(self.stage + 1))\n        out = res_layer(x)\n        return out\n\n    def train(self, mode=True):\n        super(ResI3DLayer, self).train(mode)\n        if self.bn_eval:\n            for m in self.modules():\n                if isinstance(m, nn.BatchNorm3d):\n                    m.eval()\n                    if self.bn_frozen:\n                        for params in m.parameters():\n                            params.requires_grad = False\n        if self.bn_frozen:\n            res_layer = getattr(self, \'layer{}\'.format(self.stage + 1))\n            for m in res_layer:\n                if isinstance(m, nn.BatchNorm3d):\n                    m.eval()\n                    m.weight.requires_grad = False\n                    m.bias.requires_grad = False\n        if self.all_frozen:\n            res_layer = getattr(self, \'layer{}\'.format(self.stage + 1))\n            res_layer.eval()\n            for param in res_layer.parameters():\n                param.requires_grad = False\n'"
mmaction/models/tenons/shared_heads/res_layer.py,1,"b""import logging\n\nimport torch.nn as nn\nfrom mmcv.cnn import constant_init, kaiming_init\nfrom mmcv.runner import load_checkpoint\n\nfrom ..backbones import ResNet\nfrom ..backbones.resnet import make_res_layer\nfrom ...registry import HEADS\nfrom ..spatial_temporal_modules.non_local import NonLocalModule\n\n\n@HEADS.register_module\nclass ResLayer(nn.Module):\n\n    def __init__(self,\n                 depth,\n                 pretrained=None,\n                 stage=3,\n                 stride=2,\n                 dilation=1,\n                 style='pytorch',\n                 bn_eval=True,\n                 bn_frozen=True,\n                 all_frozen=False,\n                 with_cp=False):\n        super(ResLayer, self).__init__()\n        self.bn_eval = bn_eval\n        self.bn_frozen = bn_frozen\n        self.all_frozen = all_frozen\n        self.stage = stage\n        block, stage_blocks = ResNet.arch_settings[depth]\n        self.pretrained = pretrained\n        stage_block = stage_blocks[stage]\n        planes = 64 * 2**stage\n        inplanes = 64 * 2**(stage - 1) * block.expansion\n\n        res_layer = make_res_layer(\n            block,\n            inplanes,\n            planes,\n            stage_block,\n            stride=stride,\n            dilation=dilation,\n            style=style,\n            with_cp=with_cp)\n        self.add_module('layer{}'.format(stage + 1), res_layer)\n\n    def init_weights(self):\n        if isinstance(self.pretrained, str):\n            logger = logging.getLogger()\n            load_checkpoint(self, self.pretrained, strict=False, logger=logger)\n        elif self.pretrained is None:\n            for m in self.modules():\n                if isinstance(m, nn.Conv2d):\n                    kaiming_init(m)\n                elif isinstance(m, nn.BatchNorm2d):\n                    constant_init(m, 1)\n        else:\n            raise TypeError('pretrained must be a str or None')\n\n    def forward(self, x):\n        res_layer = getattr(self, 'layer{}'.format(self.stage + 1))\n        out = res_layer(x)\n        return out\n\n    def train(self, mode=True):\n        super(ResLayer, self).train(mode)\n        if self.bn_eval:\n            for m in self.modules():\n                if isinstance(m, nn.BatchNorm2d):\n                    m.eval()\n                    if self.bn_frozen:\n                        for params in m.parameters():\n                            params.requires_grad = False\n        if self.bn_frozen:\n            res_layer = getattr(self, 'layer{}'.format(self.stage + 1))\n            for m in res_layer:\n                if isinstance(m, nn.BatchNorm2d):\n                    m.eval()\n                    m.weight.requires_grad = False\n                    m.bias.requires_grad = False\n        if self.all_frozen:\n            res_layer = getattr(self, 'layer{}'.format(self.stage + 1))\n            res_layer.eval()\n            for param in mod.parameters():\n                param.requires_grad = False\n"""
mmaction/models/tenons/spatial_temporal_modules/__init__.py,0,"b""from .simple_spatial_module import SimpleSpatialModule\nfrom .simple_spatial_temporal_module import SimpleSpatialTemporalModule\nfrom .slowfast_spatial_temporal_module import SlowFastSpatialTemporalModule\n\n__all__ = [\n    'SimpleSpatialModule',\n    'SimpleSpatialTemporalModule',\n    'SlowFastSpatialTemporalModule'\n]\n"""
mmaction/models/tenons/spatial_temporal_modules/non_local.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import constant_init, kaiming_init\nfrom ...registry import SPATIAL_TEMPORAL_MODULES\n\n\n@SPATIAL_TEMPORAL_MODULES.register_module\nclass NonLocalModule(nn.Module):\n    def __init__(self, in_channels=1024, nonlocal_type=""gaussian"", dim=3, embed=True, embed_dim=None, sub_sample=True, use_bn=True):\n        super(NonLocalModule, self).__init__()\n\n        assert nonlocal_type in [\'gaussian\', \'dot\', \'concat\']\n        assert dim == 2 or dim == 3\n        self.nonlocal_type = nonlocal_type\n        self.embed = embed\n        self.embed_dim = embed_dim if embed_dim is not None else in_channels // 2\n        self.sub_sample = sub_sample\n        self.use_bn = use_bn\n\n        if self.embed:\n            if dim == 2:\n                self.theta = nn.Conv2d(in_channels, self.embed_dim, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n                self.phi = nn.Conv2d(in_channels, self.embed_dim, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n                self.g = nn.Conv2d(in_channels, self.embed_dim, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n            elif dim == 3:\n                self.theta = nn.Conv3d(in_channels, self.embed_dim, kernel_size=(1, 1, 1), stride=(1, 1, 1), padding=(0, 0, 0))\n                self.phi = nn.Conv3d(in_channels, self.embed_dim, kernel_size=(1, 1, 1), stride=(1, 1, 1), padding=(0, 0, 0))\n                self.g = nn.Conv3d(in_channels, self.embed_dim, kernel_size=(1, 1, 1), stride=(1, 1, 1), padding=(0, 0, 0))\n\n        if self.nonlocal_type == \'gaussian\':\n            self.softmax = nn.Softmax(dim=2)\n        elif self.nonlocal_type == \'concat\':\n            if dim == 2:\n                self.concat_proj = nn.Sequential(nn.Conv2d(self.embed_dim * 2, 1, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0)),\n                                                 nn.ReLU())\n            elif dim == 3:\n                self.concat_proj = nn.Sequential(nn.Conv3d(self.embed_dim * 2, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1), padding=(0, 0, 0)),\n                                                 nn.ReLU())\n\n        if sub_sample:\n            if dim == 2:\n                self.max_pool = nn.MaxPool2d(kernel_size=(2, 2))\n            elif dim == 3:\n                self.max_pool = nn.MaxPool3d(kernel_size=(1, 2, 2))\n            self.g = nn.Sequential(self.max_pool, self.g)\n            self.phi = nn.Sequential(self.max_pool, self.phi)\n\n        if dim == 2:\n            self.W = nn.Conv2d(self.embed_dim, in_channels, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0)) \n        elif dim == 3:\n            self.W = nn.Conv3d(self.embed_dim, in_channels, kernel_size=(1, 1, 1), stride=(1, 1, 1), padding=(0, 0, 0)) \n\n        if use_bn:\n            if dim == 2:\n                self.bn = nn.BatchNorm2d(in_channels, eps=1e-05, momentum=0.9, affine=True)\n            elif dim == 3:\n                self.bn = nn.BatchNorm3d(in_channels, eps=1e-05, momentum=0.9, affine=True)\n            self.W = nn.Sequential(self.W, self.bn)\n\n \n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Conv3d):\n               kaiming_init(m)\n            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm3d):\n               constant_init(m, 0)\n\n\n    def forward(self, input):\n        if self.embed:\n            theta = self.theta(input)\n            phi = self.phi(input)\n            g = self.g(input)\n        else:\n            theta = input\n            phi = input\n            g = input\n        \n        if self.nonlocal_type in [\'gaussian\', \'dot\']:\n            # reshape [BxC\'xTxHxW] to [BxC\'x(T)HW]\n            theta = theta.reshape(theta.shape[:2] + (-1,))\n            phi = phi.reshape(theta.shape[:2] + (-1,))\n            g = g.reshape(theta.shape[:2] + (-1,))\n            theta_phi = torch.matmul(theta.transpose(1, 2), phi)\n            if self.nonlocal_type == \'gaussian\':\n                p = self.softmax(theta_phi)\n            elif self.nonlocal_type == \'dot\':\n                N = theta_phi.size(-1)\n                p = theta_phi / N\n        elif self.non_local_type == \'concat\':\n            # reshape [BxC\'xTxHxW] to [BxC\'x(T)HWx1]\n            theta = theta.reshape(theta.shape[:2] + (-1,1))\n            # reshape [BxC\'xTxHxW] to [BxC\'x1x(T)HW]\n            phi = phi.reshape(theta.shape[:2] + (1,-1))\n            theta_x = theta.repeat(1, 1, 1, phi.size(3))\n            phi_x = phi.repeat(1, 1, theta.size(2), 1)\n            theta_phi = torch.cat([theta_x, phi_x], dim=1)\n            theta_phi = self.concat_proj(theta_phi)\n            theta_phi = theta_phi.squeeze()\n            N = theta_phi.size(-1)\n            p = theta_phi / N\n        else:\n            NotImplementedError\n\n        # BxC\'xddd , Bxdxddd => BxC\'xd\n        y = torch.matmul(g, p.transpose(1, 2))\n        y = y.reshape(y.shape[:2] + input.shape[2:])\n        z = self.W(y) + input \n\n        return z\n        \n\n'"
mmaction/models/tenons/spatial_temporal_modules/simple_spatial_module.py,2,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom ...registry import SPATIAL_TEMPORAL_MODULES\n\n\n@SPATIAL_TEMPORAL_MODULES.register_module\nclass SimpleSpatialModule(nn.Module):\n    def __init__(self, spatial_type='avg', spatial_size=7):\n        super(SimpleSpatialModule, self).__init__()\n\n        assert spatial_type in ['avg']\n        self.spatial_type = spatial_type\n\n        self.spatial_size = spatial_size if not isinstance(spatial_size, int) else (spatial_size, spatial_size)\n\n        if self.spatial_type == 'avg':\n            self.op = nn.AvgPool2d(self.spatial_size, stride=1, padding=0)\n\n\n    def init_weights(self):\n        pass\n\n    def forward(self, input):\n        return self.op(input)"""
mmaction/models/tenons/spatial_temporal_modules/simple_spatial_temporal_module.py,2,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom ...registry import SPATIAL_TEMPORAL_MODULES\n\n\n@SPATIAL_TEMPORAL_MODULES.register_module\nclass SimpleSpatialTemporalModule(nn.Module):\n    def __init__(self, spatial_type='avg', spatial_size=7, temporal_size=1):\n        super(SimpleSpatialTemporalModule, self).__init__()\n\n        assert spatial_type in ['avg', 'max']\n        self.spatial_type = spatial_type\n\n        self.spatial_size = spatial_size\n        if spatial_size != -1:\n            self.spatial_size = (spatial_size, spatial_size)\n\n        self.temporal_size = temporal_size\n\n        assert not (self.spatial_size == -1) ^ (self.temporal_size == -1)\n\n        if self.temporal_size == -1 and self.spatial_size == -1:\n            self.pool_size = (1, 1, 1)\n            if self.spatial_type == 'avg':\n                self.pool_func = nn.AdaptiveAvgPool3d(self.pool_size)\n            if self.spatial_type == 'max':\n                self.pool_func = nn.AdaptiveMaxPool3d(self.pool_size)\n        else:\n            self.pool_size = (self.temporal_size, ) + self.spatial_size\n            if self.spatial_type == 'avg':\n                self.pool_func = nn.AvgPool3d(self.pool_size, stride=1, padding=0)\n            if self.spatial_type == 'max':\n                self.pool_func = nn.MaxPool3d(self.pool_size, stride=1, padding=0)\n\n\n    def init_weights(self):\n        pass\n\n    def forward(self, input):\n        return self.pool_func(input)\n"""
mmaction/models/tenons/spatial_temporal_modules/slowfast_spatial_temporal_module.py,3,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom ...registry import SPATIAL_TEMPORAL_MODULES\n\n\n@SPATIAL_TEMPORAL_MODULES.register_module\nclass SlowFastSpatialTemporalModule(nn.Module):\n    def __init__(self, adaptive_pool=True, spatial_type='avg', spatial_size=1, temporal_size=1):\n        super(SlowFastSpatialTemporalModule, self).__init__()\n\n        self.adaptive_pool = adaptive_pool\n        assert spatial_type in ['avg']\n        self.spatial_type = spatial_type\n\n        self.spatial_size = spatial_size if not isinstance(spatial_size, int) else (spatial_size, spatial_size)\n        self.temporal_size = temporal_size\n        self.pool_size = (self.temporal_size, ) + self.spatial_size\n\n        if self.adaptive_pool:\n            if self.spatial_type == 'avg':\n                self.op = nn.AdaptiveAvgPool3d(self.pool_size)\n        else:\n            raise NotImplementedError\n\n\n    def init_weights(self):\n        pass\n\n    def forward(self, input):\n        x_slow, x_fast = input\n        x_slow = self.op(x_slow)\n        x_fast = self.op(x_fast)\n        return torch.cat((x_slow, x_fast), dim=1)\n"""
mmaction/models/tenons/utils/__init__.py,0,"b""from .conv_module import ConvModule\nfrom .norm import build_norm_layer\n\n__all__ = [\n    'ConvModule', 'build_norm_layer'\n]"""
mmaction/models/tenons/utils/conv_module.py,1,"b""import warnings\n\nimport torch.nn as nn\nfrom mmcv.cnn import kaiming_init, constant_init\n\nfrom .norm import build_norm_layer\n\n\nclass ConvModule(nn.Module):\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 groups=1,\n                 bias=True,\n                 normalize=None,\n                 activation='relu',\n                 inplace=True,\n                 activate_last=True):\n        super(ConvModule, self).__init__()\n        self.with_norm = normalize is not None\n        self.with_activatation = activation is not None\n        self.with_bias = bias\n        self.activation = activation\n        self.activate_last = activate_last\n\n        if self.with_norm and self.with_bias:\n            warnings.warn('ConvModule has norm and bias at the same time')\n\n        self.conv = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            dilation,\n            groups,\n            bias=bias)\n\n        self.in_channels = self.conv.in_channels\n        self.out_channels = self.conv.out_channels\n        self.kernel_size = self.conv.kernel_size\n        self.stride = self.conv.stride\n        self.padding = self.conv.padding\n        self.dilation = self.conv.dilation\n        self.transposed = self.conv.transposed\n        self.output_padding = self.conv.output_padding\n        self.groups = self.conv.groups\n\n        if self.with_norm:\n            norm_channels = out_channels if self.activate_last else in_channels\n            self.norm_name, norm = build_norm_layer(normalize, norm_channels)\n            self.add_module(self.norm_name, norm)\n\n        if self.with_activatation:\n            assert activation in ['relu'], 'Only ReLU supported.'\n            if self.activation == 'relu':\n                self.activate = nn.ReLU(inplace=inplace)\n\n        # Default using msra init\n        self.init_weights()\n\n    @property\n    def norm(self):\n        return getattr(self, self.norm_name)\n\n    def init_weights(self):\n        nonlinearity = 'relu' if self.activation is None else self.activation\n        kaiming_init(self.conv, nonlinearity=nonlinearity)\n        if self.with_norm:\n            constant_init(self.norm, 1, bias=0)\n\n    def forward(self, x, activate=True, norm=True):\n        if self.activate_last:\n            x = self.conv(x)\n            if norm and self.with_norm:\n                x = self.norm(x)\n            if activate and self.with_activatation:\n                x = self.activate(x)\n        else:\n            if norm and self.with_norm:\n                x = self.norm(x)\n            if activate and self.with_activatation:\n                x = self.activate(x)\n            x = self.conv(x)\n        return x\n"""
mmaction/models/tenons/utils/nonlocal_block.py,0,"b'from ..spatial_temporal_modules.non_local import NonLocalModule\n\n\ndef build_nonlocal_block(cfg):\n    """""" Build nonlocal block\n\n    Args:\n    """"""\n    assert isinstance(cfg, dict)\n    cfg_ = cfg.copy()\n    return NonLocalModule(**cfg_)\n'"
mmaction/models/tenons/utils/norm.py,1,"b'import torch.nn as nn\n\n\nnorm_cfg = {\n    # format: layer_type: (abbreviation, module)\n    \'BN\': (\'bn\', nn.BatchNorm2d),\n    \'SyncBN\': (\'bn\', None),\n    \'GN\': (\'gn\', nn.GroupNorm),\n    # and potentially \'SN\'\n}\n\n\ndef build_norm_layer(cfg, num_features, postfix=\'\'):\n    """""" Build normalization layer\n    Args:\n        cfg (dict): cfg should contain:\n            type (str): identify norm layer type.\n            layer args: args needed to instantiate a norm layer.\n            frozen (bool): [optional] whether stop gradient updates\n                of norm layer, it is helpful to set frozen mode\n                in backbone\'s norms.\n        num_features (int): number of channels from input\n        postfix (int, str): appended into norm abbreation to\n            create named layer.\n    Returns:\n        name (str): abbreation + postfix\n        layer (nn.Module): created norm layer\n    """"""\n    assert isinstance(cfg, dict) and \'type\' in cfg\n    cfg_ = cfg.copy()\n\n    layer_type = cfg_.pop(\'type\')\n    if layer_type not in norm_cfg:\n        raise KeyError(\'Unrecognized norm type {}\'.format(layer_type))\n    else:\n        abbr, norm_layer = norm_cfg[layer_type]\n        if norm_layer is None:\n            raise NotImplementedError\n\n    assert isinstance(postfix, (int, str))\n    name = abbr + str(postfix)\n\n    frozen = cfg_.pop(\'frozen\', False)\n    cfg_.setdefault(\'eps\', 1e-5)\n    if layer_type != \'GN\':\n        layer = norm_layer(num_features, **cfg_)\n    else:\n        assert \'num_groups\' in cfg_\n        layer = norm_layer(num_channels=num_features, **cfg_)\n\n    if frozen:\n        for param in layer.parameters():\n            param.requires_grad = False\n\n    return name, layer'"
mmaction/models/tenons/utils/resnet_r3d_utils.py,1,"b'""""""\nThis file contains helper functions for building the model and for loading model parameters.\nThese helper functions are built to mirror those in the official TensorFlow implementation.\n""""""\n\nimport torch.nn as nn\nimport string\nimport itertools\n\n\ndef conv3d_wobias(in_planes, out_planes, kernel, stride, pad, groups=1):\n    assert len(kernel) == 3\n    assert len(stride) == 3\n    assert len(pad) == 3\n    return nn.Conv3d(in_planes, out_planes, kernel_size=kernel,\n                     stride=stride, padding=pad, groups=groups, bias=False)\n\ndef conv3d_wbias(in_planes, out_planes, kernel, stride, pad, groups=1):\n    assert len(kernel) == 3\n    assert len(stride) == 3\n    assert len(pad) == 3\n    return nn.Conv3d(in_planes, out_planes, kernel_size=kernel,\n                     stride=stride, padding=pad, groups=groups, bias=True)\n\n\nclass module_list(nn.Module):\n    def __init__(self, modules, names=None):\n        super(module_list, self).__init__()\n        self.num = len(modules)\n        self.modules = modules\n        if names is None:\n            alphabet = string.ascii_lowercase\n            alphabet = list(alphabet)\n            if self.num < 26:\n                self.names = alphabet[:self.num]\n            else:\n                alphabet2 = itertools.product(alphabet, alphabet)\n                alphabet2 = list(map(lambda x: x[0] + x[1], alphabet2))\n                self.names = alphabet2[:self.num]\n        else:\n            assert len(names) == self.num\n            self.names = names\n        for m, n in zip(self.modules, self.names):\n            setattr(self, n, m)\n\n    def forward(self, inp):\n        for n in self.names:\n            inp = getattr(self, n)(inp)\n        return inp\n\n\ndef add_conv3d(in_filters, out_filters, kernel, stride, pad, block_type=\'3d\', group=1, with_bn=True):\n    if with_bn:\n        conv3d = conv3d_wobias\n    else:\n        conv3d = conv3d_wbias\n    if block_type == \'2.5d\':\n        i = 3 * in_filters * out_filters * kernel[1] * kernel[2]\n        i /= in_filters * kernel[1] * kernel[2] + 3 * out_filters\n        middle_filters = int(i)\n        conv_s = conv3d(in_filters, middle_filters, kernel=[1, kernel[1], kernel[2]],\n                        stride=[1, stride[1], stride[2]], pad=[0, pad[1], pad[2]])\n        bn_s = nn.BatchNorm3d(middle_filters, eps=1e-3)\n        conv_t = conv3d(middle_filters, out_filters, kernel=[kernel[0], 1, 1],\n                        stride=[stride[0], 1, 1], pad=[pad[0], 0, 0])\n        if with_bn:\n            return module_list([conv_s, bn_s, nn.ReLU(), conv_t], [\'conv_s\', \'bn_s\', \'relu_s\', \'conv_t\'])\n        else:\n            return module_list([conv_s, nn.ReLU(), conv_t], [\'conv_s\', \'relu_s\', \'conv_t\'])\n    if block_type == \'0.3d\':\n        conv_T = conv3d(in_filters, out_filters, kernel=[\n                        1, 1, 1], stride=[1, 1, 1], pad=[0, 0, 0])\n        bn_T = nn.BatchNorm3d(out_filters, eps=1e-3)\n        conv_C = conv3d(out_filters, out_filters,\n                        kernel=kernel, stride=stride, pad=pad)\n        if with_bn:\n            return module_list([conv_T, bn_T, nn.ReLU(), conv_C], [\'conv_T\', \'bn_T\', \'relu_T\', \'conv_C\'])\n        else:\n            return module_list([conv_T, nn.ReLU(), conv_C], [\'conv_T\', \'relu_T\', \'conv_C\'])\n    if block_type == \'3d\':\n        conv = conv3d(in_filters, out_filters,\n                      kernel=kernel, stride=stride, pad=pad)\n        return conv\n    if block_type == \'3d-sep\':\n        assert in_filters == out_filters\n        conv = conv3d(in_filters, out_filters, kernel=kernel,\n                      stride=stride, pad=pad, groups=in_filters)\n        return conv\n    print(\'Unknown Block Type !!!\')\n\n\ndef add_bn(num_filters):\n    bn = nn.BatchNorm3d(num_filters, eps=1e-3)\n    return bn\n'"
mmaction/ops/roi_align/functions/__init__.py,0,b''
mmaction/ops/roi_align/functions/roi_align.py,1,"b'from torch.autograd import Function\n\nfrom .. import roi_align_cuda\n\n\nclass RoIAlignFunction(Function):\n\n    @staticmethod\n    def forward(ctx, features, rois, out_size, spatial_scale, sample_num=0):\n        if isinstance(out_size, int):\n            out_h = out_size\n            out_w = out_size\n        elif isinstance(out_size, tuple):\n            assert len(out_size) == 2\n            assert isinstance(out_size[0], int)\n            assert isinstance(out_size[1], int)\n            out_h, out_w = out_size\n        else:\n            raise TypeError(\n                \'""out_size"" must be an integer or tuple of integers\')\n        ctx.spatial_scale = spatial_scale\n        ctx.sample_num = sample_num\n        ctx.save_for_backward(rois)\n        ctx.feature_size = features.size()\n\n        batch_size, num_channels, data_height, data_width = features.size()\n        num_rois = rois.size(0)\n\n        output = features.new_zeros(num_rois, num_channels, out_h, out_w)\n        if features.is_cuda:\n            roi_align_cuda.forward(features, rois, out_h, out_w, spatial_scale,\n                                   sample_num, output)\n        else:\n            raise NotImplementedError\n\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        feature_size = ctx.feature_size\n        spatial_scale = ctx.spatial_scale\n        sample_num = ctx.sample_num\n        rois = ctx.saved_tensors[0]\n        assert (feature_size is not None and grad_output.is_cuda)\n\n        batch_size, num_channels, data_height, data_width = feature_size\n        out_w = grad_output.size(3)\n        out_h = grad_output.size(2)\n\n        grad_input = grad_rois = None\n        if ctx.needs_input_grad[0]:\n            grad_input = rois.new_zeros(batch_size, num_channels, data_height,\n                                        data_width)\n            roi_align_cuda.backward(grad_output.contiguous(), rois, out_h,\n                                    out_w, spatial_scale, sample_num,\n                                    grad_input)\n\n        return grad_input, grad_rois, None, None, None\n\n\nroi_align = RoIAlignFunction.apply\n'"
mmaction/ops/roi_align/modules/__init__.py,0,b''
mmaction/ops/roi_align/modules/roi_align.py,1,"b'from torch.nn.modules.module import Module\nfrom ..functions.roi_align import RoIAlignFunction\n\n\nclass RoIAlign(Module):\n\n    def __init__(self, out_size, spatial_scale, sample_num=0):\n        super(RoIAlign, self).__init__()\n\n        self.out_size = out_size\n        self.spatial_scale = float(spatial_scale)\n        self.sample_num = int(sample_num)\n\n    def forward(self, features, rois):\n        return RoIAlignFunction.apply(features, rois, self.out_size,\n                                      self.spatial_scale, self.sample_num)\n'"
mmaction/ops/roi_pool/functions/__init__.py,0,b''
mmaction/ops/roi_pool/functions/roi_pool.py,2,"b'import torch\nfrom torch.autograd import Function\n\nfrom .. import roi_pool_cuda\n\n\nclass RoIPoolFunction(Function):\n\n    @staticmethod\n    def forward(ctx, features, rois, out_size, spatial_scale):\n        if isinstance(out_size, int):\n            out_h = out_size\n            out_w = out_size\n        elif isinstance(out_size, tuple):\n            assert len(out_size) == 2\n            assert isinstance(out_size[0], int)\n            assert isinstance(out_size[1], int)\n            out_h, out_w = out_size\n        else:\n            raise TypeError(\n                \'""out_size"" must be an integer or tuple of integers\')\n        assert features.is_cuda\n        ctx.save_for_backward(rois)\n        num_channels = features.size(1)\n        num_rois = rois.size(0)\n        out_size = (num_rois, num_channels, out_h, out_w)\n        output = features.new_zeros(out_size)\n        argmax = features.new_zeros(out_size, dtype=torch.int)\n        roi_pool_cuda.forward(features, rois, out_h, out_w, spatial_scale,\n                              output, argmax)\n        ctx.spatial_scale = spatial_scale\n        ctx.feature_size = features.size()\n        ctx.argmax = argmax\n\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        assert grad_output.is_cuda\n        spatial_scale = ctx.spatial_scale\n        feature_size = ctx.feature_size\n        argmax = ctx.argmax\n        rois = ctx.saved_tensors[0]\n        assert feature_size is not None\n\n        grad_input = grad_rois = None\n        if ctx.needs_input_grad[0]:\n            grad_input = grad_output.new_zeros(feature_size)\n            roi_pool_cuda.backward(grad_output.contiguous(), rois, argmax,\n                                   spatial_scale, grad_input)\n\n        return grad_input, grad_rois, None, None\n\n\nroi_pool = RoIPoolFunction.apply\n'"
mmaction/ops/roi_pool/modules/__init__.py,0,b''
mmaction/ops/roi_pool/modules/roi_pool.py,1,"b'from torch.nn.modules.module import Module\nfrom ..functions.roi_pool import roi_pool\n\n\nclass RoIPool(Module):\n\n    def __init__(self, out_size, spatial_scale):\n        super(RoIPool, self).__init__()\n\n        self.out_size = out_size\n        self.spatial_scale = float(spatial_scale)\n\n    def forward(self, features, rois):\n        return roi_pool(features, rois, self.out_size, self.spatial_scale)\n'"
