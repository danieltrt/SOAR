file_path,api_count,code
setup.py,2,"b'#!/usr/bin/env python\nimport os\nimport subprocess\nimport time\nfrom setuptools import find_packages, setup\n\nimport torch\nfrom torch.utils.cpp_extension import (BuildExtension, CppExtension,\n                                       CUDAExtension)\n\n\ndef readme():\n    with open(\'README.md\', encoding=\'utf-8\') as f:\n        content = f.read()\n    return content\n\n\nversion_file = \'mmdet/version.py\'\n\n\ndef get_git_hash():\n\n    def _minimal_ext_cmd(cmd):\n        # construct minimal environment\n        env = {}\n        for k in [\'SYSTEMROOT\', \'PATH\', \'HOME\']:\n            v = os.environ.get(k)\n            if v is not None:\n                env[k] = v\n        # LANGUAGE is used on win32\n        env[\'LANGUAGE\'] = \'C\'\n        env[\'LANG\'] = \'C\'\n        env[\'LC_ALL\'] = \'C\'\n        out = subprocess.Popen(\n            cmd, stdout=subprocess.PIPE, env=env).communicate()[0]\n        return out\n\n    try:\n        out = _minimal_ext_cmd([\'git\', \'rev-parse\', \'HEAD\'])\n        sha = out.strip().decode(\'ascii\')\n    except OSError:\n        sha = \'unknown\'\n\n    return sha\n\n\ndef get_hash():\n    if os.path.exists(\'.git\'):\n        sha = get_git_hash()[:7]\n    elif os.path.exists(version_file):\n        try:\n            from mmdet.version import __version__\n            sha = __version__.split(\'+\')[-1]\n        except ImportError:\n            raise ImportError(\'Unable to get git version\')\n    else:\n        sha = \'unknown\'\n\n    return sha\n\n\ndef write_version_py():\n    content = """"""# GENERATED VERSION FILE\n# TIME: {}\n\n__version__ = \'{}\'\nshort_version = \'{}\'\nversion_info = ({})\n""""""\n    sha = get_hash()\n    with open(\'mmdet/VERSION\', \'r\') as f:\n        SHORT_VERSION = f.read().strip()\n    VERSION_INFO = \', \'.join(SHORT_VERSION.split(\'.\'))\n    VERSION = SHORT_VERSION + \'+\' + sha\n\n    version_file_str = content.format(time.asctime(), VERSION, SHORT_VERSION,\n                                      VERSION_INFO)\n    with open(version_file, \'w\') as f:\n        f.write(version_file_str)\n\n\ndef get_version():\n    with open(version_file, \'r\') as f:\n        exec(compile(f.read(), version_file, \'exec\'))\n    return locals()[\'__version__\']\n\n\ndef make_cuda_ext(name, module, sources, sources_cuda=[]):\n\n    define_macros = []\n    extra_compile_args = {\'cxx\': []}\n\n    if torch.cuda.is_available() or os.getenv(\'FORCE_CUDA\', \'0\') == \'1\':\n        define_macros += [(\'WITH_CUDA\', None)]\n        extension = CUDAExtension\n        extra_compile_args[\'nvcc\'] = [\n            \'-D__CUDA_NO_HALF_OPERATORS__\',\n            \'-D__CUDA_NO_HALF_CONVERSIONS__\',\n            \'-D__CUDA_NO_HALF2_OPERATORS__\',\n        ]\n        sources += sources_cuda\n    else:\n        print(f\'Compiling {name} without CUDA\')\n        extension = CppExtension\n        # raise EnvironmentError(\'CUDA is required to compile MMDetection!\')\n\n    return extension(\n        name=f\'{module}.{name}\',\n        sources=[os.path.join(*module.split(\'.\'), p) for p in sources],\n        define_macros=define_macros,\n        extra_compile_args=extra_compile_args)\n\n\ndef parse_requirements(fname=\'requirements.txt\', with_version=True):\n    """"""\n    Parse the package dependencies listed in a requirements file but strips\n    specific versioning information.\n\n    Args:\n        fname (str): path to requirements file\n        with_version (bool, default=False): if True include version specs\n\n    Returns:\n        List[str]: list of requirements items\n\n    CommandLine:\n        python -c ""import setup; print(setup.parse_requirements())""\n    """"""\n    import sys\n    from os.path import exists\n    import re\n    require_fpath = fname\n\n    def parse_line(line):\n        """"""\n        Parse information from a line in a requirements text file\n        """"""\n        if line.startswith(\'-r \'):\n            # Allow specifying requirements in other files\n            target = line.split(\' \')[1]\n            for info in parse_require_file(target):\n                yield info\n        else:\n            info = {\'line\': line}\n            if line.startswith(\'-e \'):\n                info[\'package\'] = line.split(\'#egg=\')[1]\n            else:\n                # Remove versioning from the package\n                pat = \'(\' + \'|\'.join([\'>=\', \'==\', \'>\']) + \')\'\n                parts = re.split(pat, line, maxsplit=1)\n                parts = [p.strip() for p in parts]\n\n                info[\'package\'] = parts[0]\n                if len(parts) > 1:\n                    op, rest = parts[1:]\n                    if \';\' in rest:\n                        # Handle platform specific dependencies\n                        # http://setuptools.readthedocs.io/en/latest/setuptools.html#declaring-platform-specific-dependencies\n                        version, platform_deps = map(str.strip,\n                                                     rest.split(\';\'))\n                        info[\'platform_deps\'] = platform_deps\n                    else:\n                        version = rest  # NOQA\n                    info[\'version\'] = (op, version)\n            yield info\n\n    def parse_require_file(fpath):\n        with open(fpath, \'r\') as f:\n            for line in f.readlines():\n                line = line.strip()\n                if line and not line.startswith(\'#\'):\n                    for info in parse_line(line):\n                        yield info\n\n    def gen_packages_items():\n        if exists(require_fpath):\n            for info in parse_require_file(require_fpath):\n                parts = [info[\'package\']]\n                if with_version and \'version\' in info:\n                    parts.extend(info[\'version\'])\n                if not sys.version.startswith(\'3.4\'):\n                    # apparently package_deps are broken in 3.4\n                    platform_deps = info.get(\'platform_deps\')\n                    if platform_deps is not None:\n                        parts.append(\';\' + platform_deps)\n                item = \'\'.join(parts)\n                yield item\n\n    packages = list(gen_packages_items())\n    return packages\n\n\nif __name__ == \'__main__\':\n    write_version_py()\n    setup(\n        name=\'mmdet\',\n        version=get_version(),\n        description=\'Open MMLab Detection Toolbox and Benchmark\',\n        long_description=readme(),\n        author=\'OpenMMLab\',\n        author_email=\'chenkaidev@gmail.com\',\n        keywords=\'computer vision, object detection\',\n        url=\'https://github.com/open-mmlab/mmdetection\',\n        packages=find_packages(exclude=(\'configs\', \'tools\', \'demo\')),\n        package_data={\'mmdet.ops\': [\'*/*.so\']},\n        classifiers=[\n            \'Development Status :: 4 - Beta\',\n            \'License :: OSI Approved :: Apache Software License\',\n            \'Operating System :: OS Independent\',\n            \'Programming Language :: Python :: 3\',\n            \'Programming Language :: Python :: 3.5\',\n            \'Programming Language :: Python :: 3.6\',\n            \'Programming Language :: Python :: 3.7\',\n        ],\n        license=\'Apache License 2.0\',\n        setup_requires=parse_requirements(\'requirements/build.txt\'),\n        tests_require=parse_requirements(\'requirements/tests.txt\'),\n        install_requires=parse_requirements(\'requirements/runtime.txt\'),\n        extras_require={\n            \'all\': parse_requirements(\'requirements.txt\'),\n            \'tests\': parse_requirements(\'requirements/tests.txt\'),\n            \'build\': parse_requirements(\'requirements/build.txt\'),\n            \'optional\': parse_requirements(\'requirements/optional.txt\'),\n        },\n        ext_modules=[\n            make_cuda_ext(\n                name=\'compiling_info\',\n                module=\'mmdet.ops.utils\',\n                sources=[\'src/compiling_info.cpp\']),\n            make_cuda_ext(\n                name=\'nms_ext\',\n                module=\'mmdet.ops.nms\',\n                sources=[\'src/nms_ext.cpp\', \'src/cpu/nms_cpu.cpp\'],\n                sources_cuda=[\n                    \'src/cuda/nms_cuda.cpp\', \'src/cuda/nms_kernel.cu\'\n                ]),\n            make_cuda_ext(\n                name=\'roi_align_ext\',\n                module=\'mmdet.ops.roi_align\',\n                sources=[\n                    \'src/roi_align_ext.cpp\',\n                    \'src/cpu/roi_align_v2.cpp\',\n                ],\n                sources_cuda=[\n                    \'src/cuda/roi_align_kernel.cu\',\n                    \'src/cuda/roi_align_kernel_v2.cu\'\n                ]),\n            make_cuda_ext(\n                name=\'roi_pool_ext\',\n                module=\'mmdet.ops.roi_pool\',\n                sources=[\'src/roi_pool_ext.cpp\'],\n                sources_cuda=[\'src/cuda/roi_pool_kernel.cu\']),\n            make_cuda_ext(\n                name=\'deform_conv_ext\',\n                module=\'mmdet.ops.dcn\',\n                sources=[\'src/deform_conv_ext.cpp\'],\n                sources_cuda=[\n                    \'src/cuda/deform_conv_cuda.cpp\',\n                    \'src/cuda/deform_conv_cuda_kernel.cu\'\n                ]),\n            make_cuda_ext(\n                name=\'deform_pool_ext\',\n                module=\'mmdet.ops.dcn\',\n                sources=[\'src/deform_pool_ext.cpp\'],\n                sources_cuda=[\n                    \'src/cuda/deform_pool_cuda.cpp\',\n                    \'src/cuda/deform_pool_cuda_kernel.cu\'\n                ]),\n            make_cuda_ext(\n                name=\'sigmoid_focal_loss_ext\',\n                module=\'mmdet.ops.sigmoid_focal_loss\',\n                sources=[\'src/sigmoid_focal_loss_ext.cpp\'],\n                sources_cuda=[\'src/cuda/sigmoid_focal_loss_cuda.cu\']),\n            make_cuda_ext(\n                name=\'masked_conv2d_ext\',\n                module=\'mmdet.ops.masked_conv\',\n                sources=[\'src/masked_conv2d_ext.cpp\'],\n                sources_cuda=[\n                    \'src/cuda/masked_conv2d_cuda.cpp\',\n                    \'src/cuda/masked_conv2d_kernel.cu\'\n                ]),\n            make_cuda_ext(\n                name=\'carafe_ext\',\n                module=\'mmdet.ops.carafe\',\n                sources=[\'src/carafe_ext.cpp\'],\n                sources_cuda=[\n                    \'src/cuda/carafe_cuda.cpp\',\n                    \'src/cuda/carafe_cuda_kernel.cu\'\n                ]),\n            make_cuda_ext(\n                name=\'carafe_naive_ext\',\n                module=\'mmdet.ops.carafe\',\n                sources=[\'src/carafe_naive_ext.cpp\'],\n                sources_cuda=[\n                    \'src/cuda/carafe_naive_cuda.cpp\',\n                    \'src/cuda/carafe_naive_cuda_kernel.cu\'\n                ])\n        ],\n        cmdclass={\'build_ext\': BuildExtension},\n        zip_safe=False)\n'"
.dev_scripts/benchmark_filter.py,0,"b""import argparse\nimport os\nimport os.path as osp\n\nimport mmcv\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Filter configs to train')\n    parser.add_argument(\n        '--basic-arch',\n        action='store_true',\n        help='to train models in basic arch')\n    parser.add_argument(\n        '--datasets', action='store_true', help='to train models in dataset')\n    parser.add_argument(\n        '--data-pipeline',\n        action='store_true',\n        help='to train models related to data pipeline, e.g. augmentations')\n    parser.add_argument(\n        '--nn-module',\n        action='store_true',\n        help='to train models related to neural network modules')\n\n    args = parser.parse_args()\n    return args\n\n\nbasic_arch_root = [\n    'cascade_rcnn', 'double_heads', 'fcos', 'foveabox', 'free_anchor',\n    'grid_rcnn', 'guided_anchoring', 'htc', 'libra_rcnn', 'atss', 'mask_rcnn',\n    'ms_rcnn', 'nas_fpn', 'reppoints', 'retinanet', 'ssd', 'gn', 'ghm', 'fsaf'\n]\n\ndatasets_root = ['wider_face', 'pascal_voc', 'cityscapes', 'mask_rcnn']\n\ndata_pipeline_root = [\n    'albu_example', 'instaboost', 'ssd', 'mask_rcnn', 'nas_fpn'\n]\n\nnn_module_root = [\n    'carafe', 'dcn', 'empirical_attention', 'gcnet', 'gn+ws', 'hrnet', 'pafpn',\n    'nas_fpn'\n]\n\nbenchmark_pool = [\n    'configs/cityscapes/mask_rcnn_r50_fpn_1x_cityscapes.py',\n    'configs/htc/htc_r50_fpn_1x_coco.py',\n    'ghm/retinanet_ghm_r50_fpn_1x_coco.py',\n    'configs/carafe/mask_rcnn_r50_fpn_carafe_1x_coco.py',\n    'configs/grid_rcnn/grid_rcnn_r50_fpn_gn-head_2x_coco.py',\n    'configs/albu_example/mask_rcnn_r50_fpn_albu_1x_coco.py',\n    'configs/rpn/rpn_r50_fpn_1x_coco.py',\n    'configs/dcn/mask_rcnn_r50_fpn_mdconv_c3-c5_1x_coco.py',\n    'configs/dcn/faster_rcnn_r50_fpn_dpool_1x_coco.py',\n    'configs/dcn/faster_rcnn_r50_fpn_mdpool_1x_coco.py',\n    'configs/dcn/mask_rcnn_r50_fpn_dconv_c3-c5_1x_coco.py',\n    'configs/libra_rcnn/libra_faster_rcnn_r50_fpn_1x_coco.py',\n    'configs/double_heads/dh_faster_rcnn_r50_fpn_1x_coco.py',\n    'configs/instaboost/mask_rcnn_r50_fpn_instaboost_4x_coco.py',\n    'configs/retinanet/retinanet_r50_caffe_fpn_1x_coco.py',\n    'configs/ssd/ssd300_coco.py',\n    'configs/faster_rcnn/faster_rcnn_r50_fpn_ohem_1x_coco.py',\n    'configs/faster_rcnn/faster_rcnn_r50_caffe_fpn_1x_coco.py',\n    'configs/empirical_attention/faster_rcnn_r50_fpn_attention_1111_dcn_1x_coco.py',  # noqa\n    'configs/reppoints/reppoints_moment_r50_fpn_gn-neck+head_1x_coco.py',\n    'configs/guided_anchoring/ga_faster_r50_caffe_fpn_1x_coco.py',\n    'configs/free_anchor/retinanet_free_anchor_r50_fpn_1x_coco.py',\n    'configs/fsaf/fsaf_r50_fpn_1x_coco.py',\n    'configs/scratch/mask_rcnn_r50_fpn_gn-all_scratch_6x_coco.py',\n    'configs/pafpn/faster_rcnn_r50_pafpn_1x_coco.py',\n    'configs/fp16/retinanet_r50_fpn_fp16_1x_coco.py',\n    'configs/fp16/mask_rcnn_r50_fpn_fp16_1x_coco.py',\n    'configs/cascade_rcnn/cascade_mask_rcnn_r50_fpn_1x_coco.py',\n    'configs/gcnet/mask_rcnn_r50_fpn_r4_gcb_c3-c5_1x_coco.py',\n    'configs/wider_face/ssd300_wider_face.py',\n    'configs/gn/mask_rcnn_r50_fpn_gn-all_2x_coco.py',\n    'configs/fcos/fcos_center_r50_caffe_fpn_gn-head_4x4_1x_coco.py',\n    'configs/atss/atss_r50_fpn_1x_coco.py',\n    'configs/hrnet/mask_rcnn_hrnetv2p_w18_1x_coco.py',\n    'configs/ms_rcnn/ms_rcnn_r50_caffe_fpn_1x_coco.py',\n    'configs/foveabox/fovea_align_r50_fpn_gn-head_4x4_2x_coco.py',\n    'configs/pascal_voc/faster_rcnn_r50_fpn_1x_voc0712.py',\n    'configs/pascal_voc/ssd300_voc0712.py',\n    'configs/nas_fpn/retinanet_r50_nasfpn_crop640_50e_coco.py',\n    'configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_coco.py',\n    'configs/gn+ws/mask_rcnn_r50_fpn_gn_ws-all_2x_coco.py'\n]\n\n\ndef main():\n    args = parse_args()\n\n    benchmark_type = []\n    if args.basic_arch:\n        benchmark_type += basic_arch_root\n    if args.datasets:\n        benchmark_type += datasets_root\n    if args.data_pipeline:\n        benchmark_type += data_pipeline_root\n    if args.nn_module:\n        benchmark_type += nn_module_root\n\n    config_dpath = 'configs/'\n    benchmark_configs = []\n    for cfg_root in benchmark_type:\n        cfg_dir = osp.join(config_dpath, cfg_root)\n        configs = os.scandir(cfg_dir)\n        for cfg in configs:\n            config_path = osp.join(cfg_dir, cfg.name)\n            if (config_path in benchmark_pool\n                    and config_path not in benchmark_configs):\n                benchmark_configs.append(config_path)\n\n    print(f'Totally found {len(benchmark_configs)} configs to benchmark')\n    config_dicts = dict(models=benchmark_configs)\n    mmcv.dump(config_dicts, 'regression_test_configs.json')\n\n\nif __name__ == '__main__':\n    main()\n"""
.dev_scripts/gather_models.py,2,"b""import argparse\nimport glob\nimport json\nimport os.path as osp\nimport shutil\nimport subprocess\n\nimport mmcv\nimport torch\n\n# build schedule look-up table to automatically find the final model\nSCHEDULES_LUT = {\n    '1x': 12,\n    '2x': 24,\n    '20e': 20,\n    '3x': 36,\n    '4x': 48,\n    '24e': 24,\n    '6x': 73\n}\nRESULTS_LUT = ['bbox_mAP', 'segm_mAP']\n\n\ndef process_checkpoint(in_file, out_file):\n    checkpoint = torch.load(in_file, map_location='cpu')\n    # remove optimizer for smaller file size\n    if 'optimizer' in checkpoint:\n        del checkpoint['optimizer']\n    # if it is necessary to remove some sensitive data in checkpoint['meta'],\n    # add the code here.\n    torch.save(checkpoint, out_file)\n    sha = subprocess.check_output(['sha256sum', out_file]).decode()\n    final_file = out_file.rstrip('.pth') + '-{}.pth'.format(sha[:8])\n    subprocess.Popen(['mv', out_file, final_file])\n    return final_file\n\n\ndef get_final_epoch(config):\n    if config.find('grid_rcnn') != -1 and config.find('2x') != -1:\n        # grid_rcnn 2x trains 25 epochs\n        return 25\n\n    for schedule_name, epoch_num in SCHEDULES_LUT.items():\n        if config.find(schedule_name) != -1:\n            return epoch_num\n\n\ndef get_final_results(log_json_path, epoch):\n    result_dict = dict()\n    with open(log_json_path, 'r') as f:\n        for line in f.readlines():\n            log_line = json.loads(line)\n            if 'mode' not in log_line.keys():\n                continue\n\n            if log_line['mode'] == 'train' and log_line['epoch'] == epoch:\n                result_dict['memory'] = log_line['memory']\n\n            if log_line['mode'] == 'val' and log_line['epoch'] == epoch:\n                result_dict.update({\n                    key: log_line[key]\n                    for key in RESULTS_LUT if key in log_line\n                })\n                return result_dict\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Gather benchmarked models')\n    parser.add_argument(\n        'root',\n        type=str,\n        help='root path of benchmarked models to be gathered')\n    parser.add_argument(\n        'out', type=str, help='output path of gathered models to be stored')\n\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n    models_root = args.root\n    models_out = args.out\n    mmcv.mkdir_or_exist(models_out)\n\n    # find all models in the root directory to be gathered\n    raw_configs = list(mmcv.scandir('./configs', '.py', recursive=True))\n\n    # filter configs that is not trained in the experiments dir\n    used_configs = []\n    for raw_config in raw_configs:\n        if osp.exists(osp.join(models_root, raw_config)):\n            used_configs.append(raw_config)\n    print(f'Find {len(used_configs)} models to be gathered')\n\n    # find final_ckpt and log file for trained each config\n    # and parse the best performance\n    model_infos = []\n    for used_config in used_configs:\n        exp_dir = osp.join(models_root, used_config)\n        # check whether the exps is finished\n        final_epoch = get_final_epoch(used_config)\n        final_model = 'epoch_{}.pth'.format(final_epoch)\n        model_path = osp.join(exp_dir, final_model)\n\n        # skip if the model is still training\n        if not osp.exists(model_path):\n            continue\n\n        # get logs\n        log_json_path = glob.glob(osp.join(exp_dir, '*.log.json'))[0]\n        log_txt_path = glob.glob(osp.join(exp_dir, '*.log'))[0]\n        model_performance = get_final_results(log_json_path, final_epoch)\n\n        if model_performance is None:\n            continue\n\n        model_time = osp.split(log_txt_path)[-1].split('.')[0]\n        model_infos.append(\n            dict(\n                config=used_config,\n                results=model_performance,\n                epochs=final_epoch,\n                model_time=model_time,\n                log_json_path=osp.split(log_json_path)[-1]))\n\n    # publish model for each checkpoint\n    publish_model_infos = []\n    for model in model_infos:\n        model_publish_dir = osp.join(models_out, model['config'].rstrip('.py'))\n        mmcv.mkdir_or_exist(model_publish_dir)\n\n        model_name = osp.split(model['config'])[-1].split('.')[0]\n\n        model_name += '_' + model['model_time']\n        publish_model_path = osp.join(model_publish_dir, model_name)\n        trained_model_path = osp.join(models_root, model['config'],\n                                      'epoch_{}.pth'.format(model['epochs']))\n\n        # convert model\n        final_model_path = process_checkpoint(trained_model_path,\n                                              publish_model_path)\n\n        # copy log\n        shutil.copy(\n            osp.join(models_root, model['config'], model['log_json_path']),\n            osp.join(model_publish_dir, f'{model_name}.log.json'))\n        shutil.copy(\n            osp.join(models_root, model['config'],\n                     model['log_json_path'].rstrip('.json')),\n            osp.join(model_publish_dir, f'{model_name}.log'))\n\n        # copy config to guarantee reproducibility\n        config_path = model['config']\n        config_path = osp.join(\n            'configs',\n            config_path) if 'configs' not in config_path else config_path\n        target_cconfig_path = osp.split(config_path)[-1]\n        shutil.copy(config_path,\n                    osp.join(model_publish_dir, target_cconfig_path))\n\n        model['model_path'] = final_model_path\n        publish_model_infos.append(model)\n\n    models = dict(models=publish_model_infos)\n    mmcv.dump(models, osp.join(models_out, 'model_info.json'))\n\n\nif __name__ == '__main__':\n    main()\n"""
demo/image_demo.py,0,"b""from argparse import ArgumentParser\n\nfrom mmdet.apis import inference_detector, init_detector, show_result_pyplot\n\n\ndef main():\n    parser = ArgumentParser()\n    parser.add_argument('img', help='Image file')\n    parser.add_argument('config', help='Config file')\n    parser.add_argument('checkpoint', help='Checkpoint file')\n    parser.add_argument(\n        '--device', default='cuda:0', help='Device used for inference')\n    parser.add_argument(\n        '--score-thr', type=float, default=0.3, help='bbox score threshold')\n    args = parser.parse_args()\n\n    # build the model from a config file and a checkpoint file\n    model = init_detector(args.config, args.checkpoint, device=args.device)\n    # test a single image\n    result = inference_detector(model, args.img)\n    # show the results\n    show_result_pyplot(model, args.img, result, score_thr=args.score_thr)\n\n\nif __name__ == '__main__':\n    main()\n"""
demo/webcam_demo.py,1,"b'import argparse\n\nimport cv2\nimport torch\n\nfrom mmdet.apis import inference_detector, init_detector\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'MMDetection webcam demo\')\n    parser.add_argument(\'config\', help=\'test config file path\')\n    parser.add_argument(\'checkpoint\', help=\'checkpoint file\')\n    parser.add_argument(\n        \'--device\', type=str, default=\'cuda:0\', help=\'CPU/CUDA device option\')\n    parser.add_argument(\n        \'--camera-id\', type=int, default=0, help=\'camera device id\')\n    parser.add_argument(\n        \'--score-thr\', type=float, default=0.5, help=\'bbox score threshold\')\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n\n    device = torch.device(args.device)\n\n    model = init_detector(args.config, args.checkpoint, device=device)\n\n    camera = cv2.VideoCapture(args.camera_id)\n\n    print(\'Press ""Esc"", ""q"" or ""Q"" to exit.\')\n    while True:\n        ret_val, img = camera.read()\n        result = inference_detector(model, img)\n\n        ch = cv2.waitKey(1)\n        if ch == 27 or ch == ord(\'q\') or ch == ord(\'Q\'):\n            break\n\n        model.show_result(\n            img, result, score_thr=args.score_thr, wait_time=1, show=True)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
docs/conf.py,0,"b'# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\n\nsys.path.insert(0, os.path.abspath(\'..\'))\n\n# -- Project information -----------------------------------------------------\n\nproject = \'MMDetection\'\ncopyright = \'2018-2020, OpenMMLab\'\nauthor = \'MMDetection Authors\'\n\n# The full version, including alpha/beta/rc tags\nwith open(\'../mmdet/VERSION\', \'r\') as f:\n    release = f.read().strip()\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.napoleon\',\n    \'sphinx.ext.viewcode\',\n    \'recommonmark\',\n    \'sphinx_markdown_tables\',\n]\n\nautodoc_mock_imports = [\n    \'matplotlib\', \'pycocotools\', \'terminaltables\', \'mmdet.version\',\n    \'mmdet.ops.dcn\', \'mmdet.ops.masked_conv\', \'mmdet.ops.nms\',\n    \'mmdet.ops.roi_align\', \'mmdet.ops.roi_pool\',\n    \'mmdet.ops.sigmoid_focal_loss\', \'mmdet.ops.carafe\', \'mmdet.ops.utils\'\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\nsource_suffix = {\n    \'.rst\': \'restructuredtext\',\n    \'.md\': \'markdown\',\n}\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n'"
mmdet/__init__.py,0,"b""from .version import __version__, short_version\n\n__all__ = ['__version__', 'short_version']\n"""
tests/async_benchmark.py,2,"b'import asyncio\nimport os\nimport shutil\nimport urllib\n\nimport mmcv\nimport torch\n\nfrom mmdet.apis import (async_inference_detector, inference_detector,\n                        init_detector, show_result)\nfrom mmdet.utils.contextmanagers import concurrent\nfrom mmdet.utils.profiling import profile_time\n\n\nasync def main():\n    """"""\n\n    Benchmark between async and synchronous inference interfaces.\n\n    Sample runs for 20 demo images on K80 GPU, model - mask_rcnn_r50_fpn_1x:\n\n    async\tsync\n\n    7981.79 ms\t9660.82 ms\n    8074.52 ms\t9660.94 ms\n    7976.44 ms\t9406.83 ms\n\n    Async variant takes about 0.83-0.85 of the time of the synchronous\n    interface.\n\n    """"""\n    project_dir = os.path.abspath(os.path.dirname(os.path.dirname(__file__)))\n\n    config_file = os.path.join(project_dir,\n                               \'configs/mask_rcnn_r50_fpn_1x_coco.py\')\n    checkpoint_file = os.path.join(\n        project_dir, \'checkpoints/mask_rcnn_r50_fpn_1x_20181010-069fa190.pth\')\n\n    if not os.path.exists(checkpoint_file):\n        url = (\'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/mmdetection\'\n               \'/models/mask_rcnn_r50_fpn_1x_20181010-069fa190.pth\')\n        print(f\'Downloading {url} ...\')\n        local_filename, _ = urllib.request.urlretrieve(url)\n        os.makedirs(os.path.dirname(checkpoint_file), exist_ok=True)\n        shutil.move(local_filename, checkpoint_file)\n        print(f\'Saved as {checkpoint_file}\')\n    else:\n        print(f\'Using existing checkpoint {checkpoint_file}\')\n\n    device = \'cuda:0\'\n    model = init_detector(\n        config_file, checkpoint=checkpoint_file, device=device)\n\n    # queue is used for concurrent inference of multiple images\n    streamqueue = asyncio.Queue()\n    # queue size defines concurrency level\n    streamqueue_size = 4\n\n    for _ in range(streamqueue_size):\n        streamqueue.put_nowait(torch.cuda.Stream(device=device))\n\n    # test a single image and show the results\n    img = mmcv.imread(os.path.join(project_dir, \'demo/demo.jpg\'))\n\n    # warmup\n    await async_inference_detector(model, img)\n\n    async def detect(img):\n        async with concurrent(streamqueue):\n            return await async_inference_detector(model, img)\n\n    num_of_images = 20\n    with profile_time(\'benchmark\', \'async\'):\n        tasks = [\n            asyncio.create_task(detect(img)) for _ in range(num_of_images)\n        ]\n        async_results = await asyncio.gather(*tasks)\n\n    with torch.cuda.stream(torch.cuda.default_stream()):\n        with profile_time(\'benchmark\', \'sync\'):\n            sync_results = [\n                inference_detector(model, img) for _ in range(num_of_images)\n            ]\n\n    result_dir = os.path.join(project_dir, \'demo\')\n    show_result(\n        img,\n        async_results[0],\n        model.CLASSES,\n        score_thr=0.5,\n        show=False,\n        out_file=os.path.join(result_dir, \'result_async.jpg\'))\n    show_result(\n        img,\n        sync_results[0],\n        model.CLASSES,\n        score_thr=0.5,\n        show=False,\n        out_file=os.path.join(result_dir, \'result_sync.jpg\'))\n\n\nif __name__ == \'__main__\':\n    asyncio.run(main())\n'"
tests/test_anchor.py,30,"b'""""""\nCommandLine:\n    pytest tests/test_anchor.py\n    xdoctest tests/test_anchor.py zero\n\n""""""\nimport torch\n\n\ndef test_standard_anchor_generator():\n    from mmdet.core.anchor import build_anchor_generator\n    anchor_generator_cfg = dict(\n        type=\'AnchorGenerator\',\n        scales=[8],\n        ratios=[0.5, 1.0, 2.0],\n        strides=[4, 8])\n\n    anchor_generator = build_anchor_generator(anchor_generator_cfg)\n    assert anchor_generator is not None\n\n\ndef test_strides():\n    from mmdet.core import AnchorGenerator\n    # Square strides\n    self = AnchorGenerator([10], [1.], [1.], [10])\n    anchors = self.grid_anchors([(2, 2)], device=\'cpu\')\n\n    expected_anchors = torch.tensor([[-5., -5., 5., 5.], [5., -5., 15., 5.],\n                                     [-5., 5., 5., 15.], [5., 5., 15., 15.]])\n\n    assert torch.equal(anchors[0], expected_anchors)\n\n    # Different strides in x and y direction\n    self = AnchorGenerator([(10, 20)], [1.], [1.], [10])\n    anchors = self.grid_anchors([(2, 2)], device=\'cpu\')\n\n    expected_anchors = torch.tensor([[-5., -5., 5., 5.], [5., -5., 15., 5.],\n                                     [-5., 15., 5., 25.], [5., 15., 15., 25.]])\n\n    assert torch.equal(anchors[0], expected_anchors)\n\n\ndef test_ssd_anchor_generator():\n    from mmdet.core.anchor import build_anchor_generator\n    if torch.cuda.is_available():\n        device = \'cuda\'\n    else:\n        device = \'cpu\'\n\n    anchor_generator_cfg = dict(\n        type=\'SSDAnchorGenerator\',\n        scale_major=False,\n        input_size=300,\n        basesize_ratio_range=(0.15, 0.9),\n        strides=[8, 16, 32, 64, 100, 300],\n        ratios=[[2], [2, 3], [2, 3], [2, 3], [2], [2]])\n\n    featmap_sizes = [(38, 38), (19, 19), (10, 10), (5, 5), (3, 3), (1, 1)]\n    anchor_generator = build_anchor_generator(anchor_generator_cfg)\n\n    # check base anchors\n    expected_base_anchors = [\n        torch.Tensor([[-6.5000, -6.5000, 14.5000, 14.5000],\n                      [-11.3704, -11.3704, 19.3704, 19.3704],\n                      [-10.8492, -3.4246, 18.8492, 11.4246],\n                      [-3.4246, -10.8492, 11.4246, 18.8492]]),\n        torch.Tensor([[-14.5000, -14.5000, 30.5000, 30.5000],\n                      [-25.3729, -25.3729, 41.3729, 41.3729],\n                      [-23.8198, -7.9099, 39.8198, 23.9099],\n                      [-7.9099, -23.8198, 23.9099, 39.8198],\n                      [-30.9711, -4.9904, 46.9711, 20.9904],\n                      [-4.9904, -30.9711, 20.9904, 46.9711]]),\n        torch.Tensor([[-33.5000, -33.5000, 65.5000, 65.5000],\n                      [-45.5366, -45.5366, 77.5366, 77.5366],\n                      [-54.0036, -19.0018, 86.0036, 51.0018],\n                      [-19.0018, -54.0036, 51.0018, 86.0036],\n                      [-69.7365, -12.5788, 101.7365, 44.5788],\n                      [-12.5788, -69.7365, 44.5788, 101.7365]]),\n        torch.Tensor([[-44.5000, -44.5000, 108.5000, 108.5000],\n                      [-56.9817, -56.9817, 120.9817, 120.9817],\n                      [-76.1873, -22.0937, 140.1873, 86.0937],\n                      [-22.0937, -76.1873, 86.0937, 140.1873],\n                      [-100.5019, -12.1673, 164.5019, 76.1673],\n                      [-12.1673, -100.5019, 76.1673, 164.5019]]),\n        torch.Tensor([[-53.5000, -53.5000, 153.5000, 153.5000],\n                      [-66.2185, -66.2185, 166.2185, 166.2185],\n                      [-96.3711, -23.1855, 196.3711, 123.1855],\n                      [-23.1855, -96.3711, 123.1855, 196.3711]]),\n        torch.Tensor([[19.5000, 19.5000, 280.5000, 280.5000],\n                      [6.6342, 6.6342, 293.3658, 293.3658],\n                      [-34.5549, 57.7226, 334.5549, 242.2774],\n                      [57.7226, -34.5549, 242.2774, 334.5549]]),\n    ]\n    base_anchors = anchor_generator.base_anchors\n    for i, base_anchor in enumerate(base_anchors):\n        assert base_anchor.allclose(expected_base_anchors[i])\n\n    # check valid flags\n    expected_valid_pixels = [5776, 2166, 600, 150, 36, 4]\n    multi_level_valid_flags = anchor_generator.valid_flags(\n        featmap_sizes, (300, 300), device)\n    for i, single_level_valid_flag in enumerate(multi_level_valid_flags):\n        assert single_level_valid_flag.sum() == expected_valid_pixels[i]\n\n    # check number of base anchors for each level\n    assert anchor_generator.num_base_anchors == [4, 6, 6, 6, 4, 4]\n\n    # check anchor generation\n    anchors = anchor_generator.grid_anchors(featmap_sizes, device)\n    assert len(anchors) == 6\n\n\ndef test_anchor_generator_with_tuples():\n    from mmdet.core.anchor import build_anchor_generator\n    if torch.cuda.is_available():\n        device = \'cuda\'\n    else:\n        device = \'cpu\'\n\n    anchor_generator_cfg = dict(\n        type=\'SSDAnchorGenerator\',\n        scale_major=False,\n        input_size=300,\n        basesize_ratio_range=(0.15, 0.9),\n        strides=[8, 16, 32, 64, 100, 300],\n        ratios=[[2], [2, 3], [2, 3], [2, 3], [2], [2]])\n\n    featmap_sizes = [(38, 38), (19, 19), (10, 10), (5, 5), (3, 3), (1, 1)]\n    anchor_generator = build_anchor_generator(anchor_generator_cfg)\n    anchors = anchor_generator.grid_anchors(featmap_sizes, device)\n\n    anchor_generator_cfg_tuples = dict(\n        type=\'SSDAnchorGenerator\',\n        scale_major=False,\n        input_size=300,\n        basesize_ratio_range=(0.15, 0.9),\n        strides=[(8, 8), (16, 16), (32, 32), (64, 64), (100, 100), (300, 300)],\n        ratios=[[2], [2, 3], [2, 3], [2, 3], [2], [2]])\n\n    anchor_generator_tuples = build_anchor_generator(\n        anchor_generator_cfg_tuples)\n    anchors_tuples = anchor_generator_tuples.grid_anchors(\n        featmap_sizes, device)\n    for anchor, anchor_tuples in zip(anchors, anchors_tuples):\n        assert torch.equal(anchor, anchor_tuples)\n\n\ndef test_retina_anchor():\n    from mmdet.models import build_head\n    if torch.cuda.is_available():\n        device = \'cuda\'\n    else:\n        device = \'cpu\'\n\n    # head configs modified from\n    # configs/nas_fpn/retinanet_r50_fpn_crop640_50e.py\n    bbox_head = dict(\n        type=\'RetinaSepBNHead\',\n        num_classes=4,\n        num_ins=5,\n        in_channels=4,\n        stacked_convs=1,\n        feat_channels=4,\n        anchor_generator=dict(\n            type=\'AnchorGenerator\',\n            octave_base_scale=4,\n            scales_per_octave=3,\n            ratios=[0.5, 1.0, 2.0],\n            strides=[8, 16, 32, 64, 128]),\n        bbox_coder=dict(\n            type=\'DeltaXYWHBBoxCoder\',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[1.0, 1.0, 1.0, 1.0]))\n\n    retina_head = build_head(bbox_head)\n    assert retina_head.anchor_generator is not None\n\n    # use the featmap sizes in NASFPN setting to test retina head\n    featmap_sizes = [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\n    # check base anchors\n    expected_base_anchors = [\n        torch.Tensor([[-22.6274, -11.3137, 22.6274, 11.3137],\n                      [-28.5088, -14.2544, 28.5088, 14.2544],\n                      [-35.9188, -17.9594, 35.9188, 17.9594],\n                      [-16.0000, -16.0000, 16.0000, 16.0000],\n                      [-20.1587, -20.1587, 20.1587, 20.1587],\n                      [-25.3984, -25.3984, 25.3984, 25.3984],\n                      [-11.3137, -22.6274, 11.3137, 22.6274],\n                      [-14.2544, -28.5088, 14.2544, 28.5088],\n                      [-17.9594, -35.9188, 17.9594, 35.9188]]),\n        torch.Tensor([[-45.2548, -22.6274, 45.2548, 22.6274],\n                      [-57.0175, -28.5088, 57.0175, 28.5088],\n                      [-71.8376, -35.9188, 71.8376, 35.9188],\n                      [-32.0000, -32.0000, 32.0000, 32.0000],\n                      [-40.3175, -40.3175, 40.3175, 40.3175],\n                      [-50.7968, -50.7968, 50.7968, 50.7968],\n                      [-22.6274, -45.2548, 22.6274, 45.2548],\n                      [-28.5088, -57.0175, 28.5088, 57.0175],\n                      [-35.9188, -71.8376, 35.9188, 71.8376]]),\n        torch.Tensor([[-90.5097, -45.2548, 90.5097, 45.2548],\n                      [-114.0350, -57.0175, 114.0350, 57.0175],\n                      [-143.6751, -71.8376, 143.6751, 71.8376],\n                      [-64.0000, -64.0000, 64.0000, 64.0000],\n                      [-80.6349, -80.6349, 80.6349, 80.6349],\n                      [-101.5937, -101.5937, 101.5937, 101.5937],\n                      [-45.2548, -90.5097, 45.2548, 90.5097],\n                      [-57.0175, -114.0350, 57.0175, 114.0350],\n                      [-71.8376, -143.6751, 71.8376, 143.6751]]),\n        torch.Tensor([[-181.0193, -90.5097, 181.0193, 90.5097],\n                      [-228.0701, -114.0350, 228.0701, 114.0350],\n                      [-287.3503, -143.6751, 287.3503, 143.6751],\n                      [-128.0000, -128.0000, 128.0000, 128.0000],\n                      [-161.2699, -161.2699, 161.2699, 161.2699],\n                      [-203.1873, -203.1873, 203.1873, 203.1873],\n                      [-90.5097, -181.0193, 90.5097, 181.0193],\n                      [-114.0350, -228.0701, 114.0350, 228.0701],\n                      [-143.6751, -287.3503, 143.6751, 287.3503]]),\n        torch.Tensor([[-362.0387, -181.0193, 362.0387, 181.0193],\n                      [-456.1401, -228.0701, 456.1401, 228.0701],\n                      [-574.7006, -287.3503, 574.7006, 287.3503],\n                      [-256.0000, -256.0000, 256.0000, 256.0000],\n                      [-322.5398, -322.5398, 322.5398, 322.5398],\n                      [-406.3747, -406.3747, 406.3747, 406.3747],\n                      [-181.0193, -362.0387, 181.0193, 362.0387],\n                      [-228.0701, -456.1401, 228.0701, 456.1401],\n                      [-287.3503, -574.7006, 287.3503, 574.7006]])\n    ]\n    base_anchors = retina_head.anchor_generator.base_anchors\n    for i, base_anchor in enumerate(base_anchors):\n        assert base_anchor.allclose(expected_base_anchors[i])\n\n    # check valid flags\n    expected_valid_pixels = [57600, 14400, 3600, 900, 225]\n    multi_level_valid_flags = retina_head.anchor_generator.valid_flags(\n        featmap_sizes, (640, 640), device)\n    for i, single_level_valid_flag in enumerate(multi_level_valid_flags):\n        assert single_level_valid_flag.sum() == expected_valid_pixels[i]\n\n    # check number of base anchors for each level\n    assert retina_head.anchor_generator.num_base_anchors == [9, 9, 9, 9, 9]\n\n    # check anchor generation\n    anchors = retina_head.anchor_generator.grid_anchors(featmap_sizes, device)\n    assert len(anchors) == 5\n\n\ndef test_guided_anchor():\n    from mmdet.models import build_head\n    if torch.cuda.is_available():\n        device = \'cuda\'\n    else:\n        device = \'cpu\'\n    # head configs modified from\n    # configs/guided_anchoring/ga_retinanet_r50_fpn_1x_coco.py\n    bbox_head = dict(\n        type=\'GARetinaHead\',\n        num_classes=8,\n        in_channels=4,\n        stacked_convs=1,\n        feat_channels=4,\n        approx_anchor_generator=dict(\n            type=\'AnchorGenerator\',\n            octave_base_scale=4,\n            scales_per_octave=3,\n            ratios=[0.5, 1.0, 2.0],\n            strides=[8, 16, 32, 64, 128]),\n        square_anchor_generator=dict(\n            type=\'AnchorGenerator\',\n            ratios=[1.0],\n            scales=[4],\n            strides=[8, 16, 32, 64, 128]))\n\n    ga_retina_head = build_head(bbox_head)\n    assert ga_retina_head.approx_anchor_generator is not None\n\n    # use the featmap sizes in NASFPN setting to test ga_retina_head\n    featmap_sizes = [(100, 152), (50, 76), (25, 38), (13, 19), (7, 10)]\n    # check base anchors\n    expected_approxs = [\n        torch.Tensor([[-22.6274, -11.3137, 22.6274, 11.3137],\n                      [-28.5088, -14.2544, 28.5088, 14.2544],\n                      [-35.9188, -17.9594, 35.9188, 17.9594],\n                      [-16.0000, -16.0000, 16.0000, 16.0000],\n                      [-20.1587, -20.1587, 20.1587, 20.1587],\n                      [-25.3984, -25.3984, 25.3984, 25.3984],\n                      [-11.3137, -22.6274, 11.3137, 22.6274],\n                      [-14.2544, -28.5088, 14.2544, 28.5088],\n                      [-17.9594, -35.9188, 17.9594, 35.9188]]),\n        torch.Tensor([[-45.2548, -22.6274, 45.2548, 22.6274],\n                      [-57.0175, -28.5088, 57.0175, 28.5088],\n                      [-71.8376, -35.9188, 71.8376, 35.9188],\n                      [-32.0000, -32.0000, 32.0000, 32.0000],\n                      [-40.3175, -40.3175, 40.3175, 40.3175],\n                      [-50.7968, -50.7968, 50.7968, 50.7968],\n                      [-22.6274, -45.2548, 22.6274, 45.2548],\n                      [-28.5088, -57.0175, 28.5088, 57.0175],\n                      [-35.9188, -71.8376, 35.9188, 71.8376]]),\n        torch.Tensor([[-90.5097, -45.2548, 90.5097, 45.2548],\n                      [-114.0350, -57.0175, 114.0350, 57.0175],\n                      [-143.6751, -71.8376, 143.6751, 71.8376],\n                      [-64.0000, -64.0000, 64.0000, 64.0000],\n                      [-80.6349, -80.6349, 80.6349, 80.6349],\n                      [-101.5937, -101.5937, 101.5937, 101.5937],\n                      [-45.2548, -90.5097, 45.2548, 90.5097],\n                      [-57.0175, -114.0350, 57.0175, 114.0350],\n                      [-71.8376, -143.6751, 71.8376, 143.6751]]),\n        torch.Tensor([[-181.0193, -90.5097, 181.0193, 90.5097],\n                      [-228.0701, -114.0350, 228.0701, 114.0350],\n                      [-287.3503, -143.6751, 287.3503, 143.6751],\n                      [-128.0000, -128.0000, 128.0000, 128.0000],\n                      [-161.2699, -161.2699, 161.2699, 161.2699],\n                      [-203.1873, -203.1873, 203.1873, 203.1873],\n                      [-90.5097, -181.0193, 90.5097, 181.0193],\n                      [-114.0350, -228.0701, 114.0350, 228.0701],\n                      [-143.6751, -287.3503, 143.6751, 287.3503]]),\n        torch.Tensor([[-362.0387, -181.0193, 362.0387, 181.0193],\n                      [-456.1401, -228.0701, 456.1401, 228.0701],\n                      [-574.7006, -287.3503, 574.7006, 287.3503],\n                      [-256.0000, -256.0000, 256.0000, 256.0000],\n                      [-322.5398, -322.5398, 322.5398, 322.5398],\n                      [-406.3747, -406.3747, 406.3747, 406.3747],\n                      [-181.0193, -362.0387, 181.0193, 362.0387],\n                      [-228.0701, -456.1401, 228.0701, 456.1401],\n                      [-287.3503, -574.7006, 287.3503, 574.7006]])\n    ]\n    approxs = ga_retina_head.approx_anchor_generator.base_anchors\n    for i, base_anchor in enumerate(approxs):\n        assert base_anchor.allclose(expected_approxs[i])\n\n    # check valid flags\n    expected_valid_pixels = [136800, 34200, 8550, 2223, 630]\n    multi_level_valid_flags = ga_retina_head.approx_anchor_generator \\\n        .valid_flags(featmap_sizes, (800, 1216), device)\n    for i, single_level_valid_flag in enumerate(multi_level_valid_flags):\n        assert single_level_valid_flag.sum() == expected_valid_pixels[i]\n\n    # check number of base anchors for each level\n    assert ga_retina_head.approx_anchor_generator.num_base_anchors == [\n        9, 9, 9, 9, 9\n    ]\n\n    # check approx generation\n    squares = ga_retina_head.square_anchor_generator.grid_anchors(\n        featmap_sizes, device)\n    assert len(squares) == 5\n\n    expected_squares = [\n        torch.Tensor([[-16., -16., 16., 16.]]),\n        torch.Tensor([[-32., -32., 32., 32]]),\n        torch.Tensor([[-64., -64., 64., 64.]]),\n        torch.Tensor([[-128., -128., 128., 128.]]),\n        torch.Tensor([[-256., -256., 256., 256.]])\n    ]\n    squares = ga_retina_head.square_anchor_generator.base_anchors\n    for i, base_anchor in enumerate(squares):\n        assert base_anchor.allclose(expected_squares[i])\n\n    # square_anchor_generator does not check valid flags\n    # check number of base anchors for each level\n    assert (ga_retina_head.square_anchor_generator.num_base_anchors == [\n        1, 1, 1, 1, 1\n    ])\n\n    # check square generation\n    anchors = ga_retina_head.square_anchor_generator.grid_anchors(\n        featmap_sizes, device)\n    assert len(anchors) == 5\n'"
tests/test_assigner.py,67,"b'""""""\nTests the Assigner objects.\n\nCommandLine:\n    pytest tests/test_assigner.py\n    xdoctest tests/test_assigner.py zero\n\n\n\n""""""\nimport torch\n\nfrom mmdet.core.bbox.assigners import (ApproxMaxIoUAssigner,\n                                       CenterRegionAssigner, MaxIoUAssigner,\n                                       PointAssigner)\n\n\ndef test_max_iou_assigner():\n    self = MaxIoUAssigner(\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n    )\n    bboxes = torch.FloatTensor([\n        [0, 0, 10, 10],\n        [10, 10, 20, 20],\n        [5, 5, 15, 15],\n        [32, 32, 38, 42],\n    ])\n    gt_bboxes = torch.FloatTensor([\n        [0, 0, 10, 9],\n        [0, 10, 10, 19],\n    ])\n    gt_labels = torch.LongTensor([2, 3])\n    assign_result = self.assign(bboxes, gt_bboxes, gt_labels=gt_labels)\n    assert len(assign_result.gt_inds) == 4\n    assert len(assign_result.labels) == 4\n\n    expected_gt_inds = torch.LongTensor([1, 0, 2, 0])\n    assert torch.all(assign_result.gt_inds == expected_gt_inds)\n\n\ndef test_max_iou_assigner_with_ignore():\n    self = MaxIoUAssigner(\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n        ignore_iof_thr=0.5,\n        ignore_wrt_candidates=False,\n    )\n    bboxes = torch.FloatTensor([\n        [0, 0, 10, 10],\n        [10, 10, 20, 20],\n        [5, 5, 15, 15],\n        [30, 32, 40, 42],\n    ])\n    gt_bboxes = torch.FloatTensor([\n        [0, 0, 10, 9],\n        [0, 10, 10, 19],\n    ])\n    gt_bboxes_ignore = torch.Tensor([\n        [30, 30, 40, 40],\n    ])\n    assign_result = self.assign(\n        bboxes, gt_bboxes, gt_bboxes_ignore=gt_bboxes_ignore)\n\n    expected_gt_inds = torch.LongTensor([1, 0, 2, -1])\n    assert torch.all(assign_result.gt_inds == expected_gt_inds)\n\n\ndef test_max_iou_assigner_with_empty_gt():\n    """"""\n    Test corner case where an image might have no true detections\n    """"""\n    self = MaxIoUAssigner(\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n    )\n    bboxes = torch.FloatTensor([\n        [0, 0, 10, 10],\n        [10, 10, 20, 20],\n        [5, 5, 15, 15],\n        [32, 32, 38, 42],\n    ])\n    gt_bboxes = torch.FloatTensor([])\n    assign_result = self.assign(bboxes, gt_bboxes)\n\n    expected_gt_inds = torch.LongTensor([0, 0, 0, 0])\n    assert torch.all(assign_result.gt_inds == expected_gt_inds)\n\n\ndef test_max_iou_assigner_with_empty_boxes():\n    """"""\n    Test corner case where an network might predict no boxes\n    """"""\n    self = MaxIoUAssigner(\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n    )\n    bboxes = torch.empty((0, 4))\n    gt_bboxes = torch.FloatTensor([\n        [0, 0, 10, 9],\n        [0, 10, 10, 19],\n    ])\n    gt_labels = torch.LongTensor([2, 3])\n\n    # Test with gt_labels\n    assign_result = self.assign(bboxes, gt_bboxes, gt_labels=gt_labels)\n    assert len(assign_result.gt_inds) == 0\n    assert tuple(assign_result.labels.shape) == (0, )\n\n    # Test without gt_labels\n    assign_result = self.assign(bboxes, gt_bboxes, gt_labels=None)\n    assert len(assign_result.gt_inds) == 0\n    assert assign_result.labels is None\n\n\ndef test_max_iou_assigner_with_empty_boxes_and_ignore():\n    """"""\n    Test corner case where an network might predict no boxes and ignore_iof_thr\n    is on\n    """"""\n    self = MaxIoUAssigner(\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n        ignore_iof_thr=0.5,\n    )\n    bboxes = torch.empty((0, 4))\n    gt_bboxes = torch.FloatTensor([\n        [0, 0, 10, 9],\n        [0, 10, 10, 19],\n    ])\n    gt_bboxes_ignore = torch.Tensor([\n        [30, 30, 40, 40],\n    ])\n    gt_labels = torch.LongTensor([2, 3])\n\n    # Test with gt_labels\n    assign_result = self.assign(\n        bboxes,\n        gt_bboxes,\n        gt_labels=gt_labels,\n        gt_bboxes_ignore=gt_bboxes_ignore)\n    assert len(assign_result.gt_inds) == 0\n    assert tuple(assign_result.labels.shape) == (0, )\n\n    # Test without gt_labels\n    assign_result = self.assign(\n        bboxes, gt_bboxes, gt_labels=None, gt_bboxes_ignore=gt_bboxes_ignore)\n    assert len(assign_result.gt_inds) == 0\n    assert assign_result.labels is None\n\n\ndef test_max_iou_assigner_with_empty_boxes_and_gt():\n    """"""\n    Test corner case where an network might predict no boxes and no gt\n    """"""\n    self = MaxIoUAssigner(\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n    )\n    bboxes = torch.empty((0, 4))\n    gt_bboxes = torch.empty((0, 4))\n    assign_result = self.assign(bboxes, gt_bboxes)\n    assert len(assign_result.gt_inds) == 0\n\n\ndef test_point_assigner():\n    self = PointAssigner()\n    points = torch.FloatTensor([  # [x, y, stride]\n        [0, 0, 1],\n        [10, 10, 1],\n        [5, 5, 1],\n        [32, 32, 1],\n    ])\n    gt_bboxes = torch.FloatTensor([\n        [0, 0, 10, 9],\n        [0, 10, 10, 19],\n    ])\n    assign_result = self.assign(points, gt_bboxes)\n    expected_gt_inds = torch.LongTensor([1, 2, 1, 0])\n    assert torch.all(assign_result.gt_inds == expected_gt_inds)\n\n\ndef test_point_assigner_with_empty_gt():\n    """"""\n    Test corner case where an image might have no true detections\n    """"""\n    self = PointAssigner()\n    points = torch.FloatTensor([  # [x, y, stride]\n        [0, 0, 1],\n        [10, 10, 1],\n        [5, 5, 1],\n        [32, 32, 1],\n    ])\n    gt_bboxes = torch.FloatTensor([])\n    assign_result = self.assign(points, gt_bboxes)\n\n    expected_gt_inds = torch.LongTensor([0, 0, 0, 0])\n    assert torch.all(assign_result.gt_inds == expected_gt_inds)\n\n\ndef test_point_assigner_with_empty_boxes_and_gt():\n    """"""\n    Test corner case where an image might predict no points and no gt\n    """"""\n    self = PointAssigner()\n    points = torch.FloatTensor([])\n    gt_bboxes = torch.FloatTensor([])\n    assign_result = self.assign(points, gt_bboxes)\n    assert len(assign_result.gt_inds) == 0\n\n\ndef test_approx_iou_assigner():\n    self = ApproxMaxIoUAssigner(\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n    )\n    bboxes = torch.FloatTensor([\n        [0, 0, 10, 10],\n        [10, 10, 20, 20],\n        [5, 5, 15, 15],\n        [32, 32, 38, 42],\n    ])\n    gt_bboxes = torch.FloatTensor([\n        [0, 0, 10, 9],\n        [0, 10, 10, 19],\n    ])\n    approxs_per_octave = 1\n    approxs = bboxes\n    squares = bboxes\n    assign_result = self.assign(approxs, squares, approxs_per_octave,\n                                gt_bboxes)\n\n    expected_gt_inds = torch.LongTensor([1, 0, 2, 0])\n    assert torch.all(assign_result.gt_inds == expected_gt_inds)\n\n\ndef test_approx_iou_assigner_with_empty_gt():\n    """"""\n    Test corner case where an image might have no true detections\n    """"""\n    self = ApproxMaxIoUAssigner(\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n    )\n    bboxes = torch.FloatTensor([\n        [0, 0, 10, 10],\n        [10, 10, 20, 20],\n        [5, 5, 15, 15],\n        [32, 32, 38, 42],\n    ])\n    gt_bboxes = torch.FloatTensor([])\n    approxs_per_octave = 1\n    approxs = bboxes\n    squares = bboxes\n    assign_result = self.assign(approxs, squares, approxs_per_octave,\n                                gt_bboxes)\n\n    expected_gt_inds = torch.LongTensor([0, 0, 0, 0])\n    assert torch.all(assign_result.gt_inds == expected_gt_inds)\n\n\ndef test_approx_iou_assigner_with_empty_boxes():\n    """"""\n    Test corner case where an network might predict no boxes\n    """"""\n    self = ApproxMaxIoUAssigner(\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n    )\n    bboxes = torch.empty((0, 4))\n    gt_bboxes = torch.FloatTensor([\n        [0, 0, 10, 9],\n        [0, 10, 10, 19],\n    ])\n    approxs_per_octave = 1\n    approxs = bboxes\n    squares = bboxes\n    assign_result = self.assign(approxs, squares, approxs_per_octave,\n                                gt_bboxes)\n    assert len(assign_result.gt_inds) == 0\n\n\ndef test_approx_iou_assigner_with_empty_boxes_and_gt():\n    """"""\n    Test corner case where an network might predict no boxes and no gt\n    """"""\n    self = ApproxMaxIoUAssigner(\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n    )\n    bboxes = torch.empty((0, 4))\n    gt_bboxes = torch.empty((0, 4))\n    approxs_per_octave = 1\n    approxs = bboxes\n    squares = bboxes\n    assign_result = self.assign(approxs, squares, approxs_per_octave,\n                                gt_bboxes)\n    assert len(assign_result.gt_inds) == 0\n\n\ndef test_random_assign_result():\n    """"""\n    Test random instantiation of assign result to catch corner cases\n    """"""\n    from mmdet.core.bbox.assigners.assign_result import AssignResult\n    AssignResult.random()\n\n    AssignResult.random(num_gts=0, num_preds=0)\n    AssignResult.random(num_gts=0, num_preds=3)\n    AssignResult.random(num_gts=3, num_preds=3)\n    AssignResult.random(num_gts=0, num_preds=3)\n    AssignResult.random(num_gts=7, num_preds=7)\n    AssignResult.random(num_gts=7, num_preds=64)\n    AssignResult.random(num_gts=24, num_preds=3)\n\n\ndef test_center_region_assigner():\n    self = CenterRegionAssigner(pos_scale=0.3, neg_scale=1)\n    bboxes = torch.FloatTensor([[0, 0, 10, 10], [10, 10, 20, 20], [8, 8, 9,\n                                                                   9]])\n    gt_bboxes = torch.FloatTensor([\n        [0, 0, 11, 11],  # match bboxes[0]\n        [10, 10, 20, 20],  # match bboxes[1]\n        [4.5, 4.5, 5.5, 5.5],  # match bboxes[0] but area is too small\n        [0, 0, 10, 10],  # match bboxes[1] and has a smaller area than gt[0]\n    ])\n    gt_labels = torch.LongTensor([2, 3, 4, 5])\n    assign_result = self.assign(bboxes, gt_bboxes, gt_labels=gt_labels)\n    assert len(assign_result.gt_inds) == 3\n    assert len(assign_result.labels) == 3\n    expected_gt_inds = torch.LongTensor([4, 2, 0])\n    assert torch.all(assign_result.gt_inds == expected_gt_inds)\n    shadowed_labels = assign_result.get_extra_property(\'shadowed_labels\')\n    # [8, 8, 9, 9] in the shadowed region of [0, 0, 11, 11] (label: 2)\n    assert torch.any(shadowed_labels == torch.LongTensor([[2, 2]]))\n    # [8, 8, 9, 9] in the shadowed region of [0, 0, 10, 10] (label: 5)\n    assert torch.any(shadowed_labels == torch.LongTensor([[2, 5]]))\n    # [0, 0, 10, 10] is already assigned to [4.5, 4.5, 5.5, 5.5].\n    #   Therefore, [0, 0, 11, 11] (label: 2) is shadowed\n    assert torch.any(shadowed_labels == torch.LongTensor([[0, 2]]))\n\n\ndef test_center_region_assigner_with_ignore():\n    self = CenterRegionAssigner(\n        pos_scale=0.5,\n        neg_scale=1,\n    )\n    bboxes = torch.FloatTensor([\n        [0, 0, 10, 10],\n        [10, 10, 20, 20],\n    ])\n    gt_bboxes = torch.FloatTensor([\n        [0, 0, 10, 10],  # match bboxes[0]\n        [10, 10, 20, 20],  # match bboxes[1]\n    ])\n    gt_bboxes_ignore = torch.FloatTensor([\n        [0, 0, 10, 10],  # match bboxes[0]\n    ])\n    gt_labels = torch.LongTensor([1, 2])\n    assign_result = self.assign(\n        bboxes,\n        gt_bboxes,\n        gt_bboxes_ignore=gt_bboxes_ignore,\n        gt_labels=gt_labels)\n    assert len(assign_result.gt_inds) == 2\n    assert len(assign_result.labels) == 2\n\n    expected_gt_inds = torch.LongTensor([-1, 2])\n    assert torch.all(assign_result.gt_inds == expected_gt_inds)\n\n\ndef test_center_region_assigner_with_empty_bboxes():\n    self = CenterRegionAssigner(\n        pos_scale=0.5,\n        neg_scale=1,\n    )\n    bboxes = torch.empty((0, 4)).float()\n    gt_bboxes = torch.FloatTensor([\n        [0, 0, 10, 10],  # match bboxes[0]\n        [10, 10, 20, 20],  # match bboxes[1]\n    ])\n    gt_labels = torch.LongTensor([1, 2])\n    assign_result = self.assign(bboxes, gt_bboxes, gt_labels=gt_labels)\n    assert assign_result.gt_inds is None or assign_result.gt_inds.numel() == 0\n    assert assign_result.labels is None or assign_result.labels.numel() == 0\n\n\ndef test_center_region_assigner_with_empty_gts():\n    self = CenterRegionAssigner(\n        pos_scale=0.5,\n        neg_scale=1,\n    )\n    bboxes = torch.FloatTensor([\n        [0, 0, 10, 10],\n        [10, 10, 20, 20],\n    ])\n    gt_bboxes = torch.empty((0, 4)).float()\n    gt_labels = torch.empty((0, )).long()\n    assign_result = self.assign(bboxes, gt_bboxes, gt_labels=gt_labels)\n    assert len(assign_result.gt_inds) == 2\n    expected_gt_inds = torch.LongTensor([0, 0])\n    assert torch.all(assign_result.gt_inds == expected_gt_inds)\n'"
tests/test_async.py,4,"b'""""""Tests for async interface.""""""\n\nimport asyncio\nimport os\nimport sys\n\nimport asynctest\nimport mmcv\nimport torch\n\nfrom mmdet.apis import async_inference_detector, init_detector\n\nif sys.version_info >= (3, 7):\n    from mmdet.utils.contextmanagers import concurrent\n\n\nclass AsyncTestCase(asynctest.TestCase):\n    use_default_loop = False\n    forbid_get_event_loop = True\n\n    TEST_TIMEOUT = int(os.getenv(\'ASYNCIO_TEST_TIMEOUT\', \'30\'))\n\n    def _run_test_method(self, method):\n        result = method()\n        if asyncio.iscoroutine(result):\n            self.loop.run_until_complete(\n                asyncio.wait_for(result, timeout=self.TEST_TIMEOUT))\n\n\nclass MaskRCNNDetector:\n\n    def __init__(self,\n                 model_config,\n                 checkpoint=None,\n                 streamqueue_size=3,\n                 device=\'cuda:0\'):\n\n        self.streamqueue_size = streamqueue_size\n        self.device = device\n        # build the model and load checkpoint\n        self.model = init_detector(\n            model_config, checkpoint=None, device=self.device)\n        self.streamqueue = None\n\n    async def init(self):\n        self.streamqueue = asyncio.Queue()\n        for _ in range(self.streamqueue_size):\n            stream = torch.cuda.Stream(device=self.device)\n            self.streamqueue.put_nowait(stream)\n\n    if sys.version_info >= (3, 7):\n\n        async def apredict(self, img):\n            if isinstance(img, str):\n                img = mmcv.imread(img)\n            async with concurrent(self.streamqueue):\n                result = await async_inference_detector(self.model, img)\n            return result\n\n\nclass AsyncInferenceTestCase(AsyncTestCase):\n\n    if sys.version_info >= (3, 7):\n\n        async def test_simple_inference(self):\n            if not torch.cuda.is_available():\n                import pytest\n\n                pytest.skip(\'test requires GPU and torch+cuda\')\n\n            ori_grad_enabled = torch.is_grad_enabled()\n            root_dir = os.path.dirname(os.path.dirname(__name__))\n            model_config = os.path.join(\n                root_dir, \'configs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py\')\n            detector = MaskRCNNDetector(model_config)\n            await detector.init()\n            img_path = os.path.join(root_dir, \'demo/demo.jpg\')\n            bboxes, _ = await detector.apredict(img_path)\n            self.assertTrue(bboxes)\n            # asy inference detector will hack grad_enabled,\n            # so restore here to avoid it to influence other tests\n            torch.set_grad_enabled(ori_grad_enabled)\n'"
tests/test_backbone.py,105,"b'import pytest\nimport torch\nfrom torch.nn.modules import AvgPool2d, GroupNorm\nfrom torch.nn.modules.batchnorm import _BatchNorm\n\nfrom mmdet.models.backbones import RegNet, Res2Net, ResNet, ResNetV1d, ResNeXt\nfrom mmdet.models.backbones.res2net import Bottle2neck\nfrom mmdet.models.backbones.resnet import BasicBlock, Bottleneck\nfrom mmdet.models.backbones.resnext import Bottleneck as BottleneckX\nfrom mmdet.models.utils import ResLayer\nfrom mmdet.ops import DeformConvPack\n\n\ndef is_block(modules):\n    """"""Check if is ResNet building block.""""""\n    if isinstance(modules, (BasicBlock, Bottleneck, BottleneckX, Bottle2neck)):\n        return True\n    return False\n\n\ndef is_norm(modules):\n    """"""Check if is one of the norms.""""""\n    if isinstance(modules, (GroupNorm, _BatchNorm)):\n        return True\n    return False\n\n\ndef all_zeros(modules):\n    """"""Check if the weight(and bias) is all zero.""""""\n    weight_zero = torch.allclose(modules.weight.data,\n                                 torch.zeros_like(modules.weight.data))\n    if hasattr(modules, \'bias\'):\n        bias_zero = torch.allclose(modules.bias.data,\n                                   torch.zeros_like(modules.bias.data))\n    else:\n        bias_zero = True\n\n    return weight_zero and bias_zero\n\n\ndef check_norm_state(modules, train_state):\n    """"""Check if norm layer is in correct train state.""""""\n    for mod in modules:\n        if isinstance(mod, _BatchNorm):\n            if mod.training != train_state:\n                return False\n    return True\n\n\ndef test_resnet_basic_block():\n\n    with pytest.raises(AssertionError):\n        # Not implemented yet.\n        BasicBlock(64, 64, with_cp=True)\n\n    with pytest.raises(AssertionError):\n        # Not implemented yet.\n        dcn = dict(type=\'DCN\', deformable_groups=1, fallback_on_stride=False)\n        BasicBlock(64, 64, dcn=dcn)\n\n    with pytest.raises(AssertionError):\n        # Not implemented yet.\n        plugins = [\n            dict(\n                cfg=dict(type=\'ContextBlock\', ratio=1. / 16),\n                position=\'after_conv3\')\n        ]\n        BasicBlock(64, 64, plugins=plugins)\n\n    with pytest.raises(AssertionError):\n        # Not implemented yet\n        plugins = [\n            dict(\n                cfg=dict(\n                    type=\'GeneralizedAttention\',\n                    spatial_range=-1,\n                    num_heads=8,\n                    attention_type=\'0010\',\n                    kv_stride=2),\n                position=\'after_conv2\')\n        ]\n        BasicBlock(64, 64, plugins=plugins)\n\n    # test BasicBlock structure and forward\n    block = BasicBlock(64, 64)\n    assert block.conv1.in_channels == 64\n    assert block.conv1.out_channels == 64\n    assert block.conv1.kernel_size == (3, 3)\n    assert block.conv2.in_channels == 64\n    assert block.conv2.out_channels == 64\n    assert block.conv2.kernel_size == (3, 3)\n    x = torch.randn(1, 64, 56, 56)\n    x_out = block(x)\n    assert x_out.shape == torch.Size([1, 64, 56, 56])\n\n\ndef test_resnet_bottleneck():\n\n    with pytest.raises(AssertionError):\n        # Style must be in [\'pytorch\', \'caffe\']\n        Bottleneck(64, 64, style=\'tensorflow\')\n\n    with pytest.raises(AssertionError):\n        # Allowed positions are \'after_conv1\', \'after_conv2\', \'after_conv3\'\n        plugins = [\n            dict(\n                cfg=dict(type=\'ContextBlock\', ratio=1. / 16),\n                position=\'after_conv4\')\n        ]\n        Bottleneck(64, 16, plugins=plugins)\n\n    with pytest.raises(AssertionError):\n        # Need to specify different postfix to avoid duplicate plugin name\n        plugins = [\n            dict(\n                cfg=dict(type=\'ContextBlock\', ratio=1. / 16),\n                position=\'after_conv3\'),\n            dict(\n                cfg=dict(type=\'ContextBlock\', ratio=1. / 16),\n                position=\'after_conv3\')\n        ]\n        Bottleneck(64, 16, plugins=plugins)\n\n    with pytest.raises(KeyError):\n        # Plugin type is not supported\n        plugins = [dict(cfg=dict(type=\'WrongPlugin\'), position=\'after_conv3\')]\n        Bottleneck(64, 16, plugins=plugins)\n\n    # Test Bottleneck with checkpoint forward\n    block = Bottleneck(64, 16, with_cp=True)\n    assert block.with_cp\n    x = torch.randn(1, 64, 56, 56)\n    x_out = block(x)\n    assert x_out.shape == torch.Size([1, 64, 56, 56])\n\n    # Test Bottleneck style\n    block = Bottleneck(64, 64, stride=2, style=\'pytorch\')\n    assert block.conv1.stride == (1, 1)\n    assert block.conv2.stride == (2, 2)\n    block = Bottleneck(64, 64, stride=2, style=\'caffe\')\n    assert block.conv1.stride == (2, 2)\n    assert block.conv2.stride == (1, 1)\n\n    # Test Bottleneck DCN\n    dcn = dict(type=\'DCN\', deformable_groups=1, fallback_on_stride=False)\n    with pytest.raises(AssertionError):\n        Bottleneck(64, 64, dcn=dcn, conv_cfg=dict(type=\'Conv\'))\n    block = Bottleneck(64, 64, dcn=dcn)\n    assert isinstance(block.conv2, DeformConvPack)\n\n    # Test Bottleneck forward\n    block = Bottleneck(64, 16)\n    x = torch.randn(1, 64, 56, 56)\n    x_out = block(x)\n    assert x_out.shape == torch.Size([1, 64, 56, 56])\n\n    # Test Bottleneck with 1 ContextBlock after conv3\n    plugins = [\n        dict(\n            cfg=dict(type=\'ContextBlock\', ratio=1. / 16),\n            position=\'after_conv3\')\n    ]\n    block = Bottleneck(64, 16, plugins=plugins)\n    assert block.context_block.in_channels == 64\n    x = torch.randn(1, 64, 56, 56)\n    x_out = block(x)\n    assert x_out.shape == torch.Size([1, 64, 56, 56])\n\n    # Test Bottleneck with 1 GeneralizedAttention after conv2\n    plugins = [\n        dict(\n            cfg=dict(\n                type=\'GeneralizedAttention\',\n                spatial_range=-1,\n                num_heads=8,\n                attention_type=\'0010\',\n                kv_stride=2),\n            position=\'after_conv2\')\n    ]\n    block = Bottleneck(64, 16, plugins=plugins)\n    assert block.gen_attention_block.in_channels == 16\n    x = torch.randn(1, 64, 56, 56)\n    x_out = block(x)\n    assert x_out.shape == torch.Size([1, 64, 56, 56])\n\n    # Test Bottleneck with 1 GeneralizedAttention after conv2, 1 NonLocal2D\n    # after conv2, 1 ContextBlock after conv3\n    plugins = [\n        dict(\n            cfg=dict(\n                type=\'GeneralizedAttention\',\n                spatial_range=-1,\n                num_heads=8,\n                attention_type=\'0010\',\n                kv_stride=2),\n            position=\'after_conv2\'),\n        dict(cfg=dict(type=\'NonLocal2D\'), position=\'after_conv2\'),\n        dict(\n            cfg=dict(type=\'ContextBlock\', ratio=1. / 16),\n            position=\'after_conv3\')\n    ]\n    block = Bottleneck(64, 16, plugins=plugins)\n    assert block.gen_attention_block.in_channels == 16\n    assert block.nonlocal_block.in_channels == 16\n    assert block.context_block.in_channels == 64\n    x = torch.randn(1, 64, 56, 56)\n    x_out = block(x)\n    assert x_out.shape == torch.Size([1, 64, 56, 56])\n\n    # Test Bottleneck with 1 ContextBlock after conv2, 2 ContextBlock after\n    # conv3\n    plugins = [\n        dict(\n            cfg=dict(type=\'ContextBlock\', ratio=1. / 16, postfix=1),\n            position=\'after_conv2\'),\n        dict(\n            cfg=dict(type=\'ContextBlock\', ratio=1. / 16, postfix=2),\n            position=\'after_conv3\'),\n        dict(\n            cfg=dict(type=\'ContextBlock\', ratio=1. / 16, postfix=3),\n            position=\'after_conv3\')\n    ]\n    block = Bottleneck(64, 16, plugins=plugins)\n    assert block.context_block1.in_channels == 16\n    assert block.context_block2.in_channels == 64\n    assert block.context_block3.in_channels == 64\n    x = torch.randn(1, 64, 56, 56)\n    x_out = block(x)\n    assert x_out.shape == torch.Size([1, 64, 56, 56])\n\n\ndef test_resnet_res_layer():\n    # Test ResLayer of 3 Bottleneck w\\o downsample\n    layer = ResLayer(Bottleneck, 64, 16, 3)\n    assert len(layer) == 3\n    assert layer[0].conv1.in_channels == 64\n    assert layer[0].conv1.out_channels == 16\n    for i in range(1, len(layer)):\n        assert layer[i].conv1.in_channels == 64\n        assert layer[i].conv1.out_channels == 16\n    for i in range(len(layer)):\n        assert layer[i].downsample is None\n    x = torch.randn(1, 64, 56, 56)\n    x_out = layer(x)\n    assert x_out.shape == torch.Size([1, 64, 56, 56])\n\n    # Test ResLayer of 3 Bottleneck with downsample\n    layer = ResLayer(Bottleneck, 64, 64, 3)\n    assert layer[0].downsample[0].out_channels == 256\n    for i in range(1, len(layer)):\n        assert layer[i].downsample is None\n    x = torch.randn(1, 64, 56, 56)\n    x_out = layer(x)\n    assert x_out.shape == torch.Size([1, 256, 56, 56])\n\n    # Test ResLayer of 3 Bottleneck with stride=2\n    layer = ResLayer(Bottleneck, 64, 64, 3, stride=2)\n    assert layer[0].downsample[0].out_channels == 256\n    assert layer[0].downsample[0].stride == (2, 2)\n    for i in range(1, len(layer)):\n        assert layer[i].downsample is None\n    x = torch.randn(1, 64, 56, 56)\n    x_out = layer(x)\n    assert x_out.shape == torch.Size([1, 256, 28, 28])\n\n    # Test ResLayer of 3 Bottleneck with stride=2 and average downsample\n    layer = ResLayer(Bottleneck, 64, 64, 3, stride=2, avg_down=True)\n    assert isinstance(layer[0].downsample[0], AvgPool2d)\n    assert layer[0].downsample[1].out_channels == 256\n    assert layer[0].downsample[1].stride == (1, 1)\n    for i in range(1, len(layer)):\n        assert layer[i].downsample is None\n    x = torch.randn(1, 64, 56, 56)\n    x_out = layer(x)\n    assert x_out.shape == torch.Size([1, 256, 28, 28])\n\n\ndef test_resnet_backbone():\n    """"""Test resnet backbone""""""\n    with pytest.raises(KeyError):\n        # ResNet depth should be in [18, 34, 50, 101, 152]\n        ResNet(20)\n\n    with pytest.raises(AssertionError):\n        # In ResNet: 1 <= num_stages <= 4\n        ResNet(50, num_stages=0)\n\n    with pytest.raises(AssertionError):\n        # with checkpoint is not implemented in BasicBlock of ResNet18\n        ResNet(18, with_cp=True)\n\n    with pytest.raises(AssertionError):\n        # len(stage_with_dcn) == num_stages\n        dcn = dict(type=\'DCN\', deformable_groups=1, fallback_on_stride=False)\n        ResNet(50, dcn=dcn, stage_with_dcn=(True, ))\n\n    with pytest.raises(AssertionError):\n        # len(stage_with_plugin) == num_stages\n        plugins = [\n            dict(\n                cfg=dict(type=\'ContextBlock\', ratio=1. / 16),\n                stages=(False, True, True),\n                position=\'after_conv3\')\n        ]\n        ResNet(50, plugins=plugins)\n\n    with pytest.raises(AssertionError):\n        # In ResNet: 1 <= num_stages <= 4\n        ResNet(50, num_stages=5)\n\n    with pytest.raises(AssertionError):\n        # len(strides) == len(dilations) == num_stages\n        ResNet(50, strides=(1, ), dilations=(1, 1), num_stages=3)\n\n    with pytest.raises(TypeError):\n        # pretrained must be a string path\n        model = ResNet(50)\n        model.init_weights(pretrained=0)\n\n    with pytest.raises(AssertionError):\n        # Style must be in [\'pytorch\', \'caffe\']\n        ResNet(50, style=\'tensorflow\')\n\n    # Test ResNet50 norm_eval=True\n    model = ResNet(50, norm_eval=True)\n    model.init_weights()\n    model.train()\n    assert check_norm_state(model.modules(), False)\n\n    # Test ResNet50 with torchvision pretrained weight\n    model = ResNet(depth=50, norm_eval=True)\n    model.init_weights(\'torchvision://resnet50\')\n    model.train()\n    assert check_norm_state(model.modules(), False)\n\n    # Test ResNet50 with first stage frozen\n    frozen_stages = 1\n    model = ResNet(50, frozen_stages=frozen_stages)\n    model.init_weights()\n    model.train()\n    assert model.norm1.training is False\n    for layer in [model.conv1, model.norm1]:\n        for param in layer.parameters():\n            assert param.requires_grad is False\n    for i in range(1, frozen_stages + 1):\n        layer = getattr(model, f\'layer{i}\')\n        for mod in layer.modules():\n            if isinstance(mod, _BatchNorm):\n                assert mod.training is False\n        for param in layer.parameters():\n            assert param.requires_grad is False\n\n    # Test ResNet50V1d with first stage frozen\n    model = ResNetV1d(depth=50, frozen_stages=frozen_stages)\n    assert len(model.stem) == 9\n    model.init_weights()\n    model.train()\n    check_norm_state(model.stem, False)\n    for param in model.stem.parameters():\n        assert param.requires_grad is False\n    for i in range(1, frozen_stages + 1):\n        layer = getattr(model, f\'layer{i}\')\n        for mod in layer.modules():\n            if isinstance(mod, _BatchNorm):\n                assert mod.training is False\n        for param in layer.parameters():\n            assert param.requires_grad is False\n\n    # Test ResNet18 forward\n    model = ResNet(18)\n    model.init_weights()\n    model.train()\n\n    imgs = torch.randn(1, 3, 224, 224)\n    feat = model(imgs)\n    assert len(feat) == 4\n    assert feat[0].shape == torch.Size([1, 64, 56, 56])\n    assert feat[1].shape == torch.Size([1, 128, 28, 28])\n    assert feat[2].shape == torch.Size([1, 256, 14, 14])\n    assert feat[3].shape == torch.Size([1, 512, 7, 7])\n\n    # Test ResNet50 with BatchNorm forward\n    model = ResNet(50)\n    for m in model.modules():\n        if is_norm(m):\n            assert isinstance(m, _BatchNorm)\n    model.init_weights()\n    model.train()\n\n    imgs = torch.randn(1, 3, 224, 224)\n    feat = model(imgs)\n    assert len(feat) == 4\n    assert feat[0].shape == torch.Size([1, 256, 56, 56])\n    assert feat[1].shape == torch.Size([1, 512, 28, 28])\n    assert feat[2].shape == torch.Size([1, 1024, 14, 14])\n    assert feat[3].shape == torch.Size([1, 2048, 7, 7])\n\n    # Test ResNet50 with layers 1, 2, 3 out forward\n    model = ResNet(50, out_indices=(0, 1, 2))\n    model.init_weights()\n    model.train()\n\n    imgs = torch.randn(1, 3, 224, 224)\n    feat = model(imgs)\n    assert len(feat) == 3\n    assert feat[0].shape == torch.Size([1, 256, 56, 56])\n    assert feat[1].shape == torch.Size([1, 512, 28, 28])\n    assert feat[2].shape == torch.Size([1, 1024, 14, 14])\n\n    # Test ResNet50 with checkpoint forward\n    model = ResNet(50, with_cp=True)\n    for m in model.modules():\n        if is_block(m):\n            assert m.with_cp\n    model.init_weights()\n    model.train()\n\n    imgs = torch.randn(1, 3, 224, 224)\n    feat = model(imgs)\n    assert len(feat) == 4\n    assert feat[0].shape == torch.Size([1, 256, 56, 56])\n    assert feat[1].shape == torch.Size([1, 512, 28, 28])\n    assert feat[2].shape == torch.Size([1, 1024, 14, 14])\n    assert feat[3].shape == torch.Size([1, 2048, 7, 7])\n\n    # Test ResNet50 with GroupNorm forward\n    model = ResNet(\n        50, norm_cfg=dict(type=\'GN\', num_groups=32, requires_grad=True))\n    for m in model.modules():\n        if is_norm(m):\n            assert isinstance(m, GroupNorm)\n    model.init_weights()\n    model.train()\n\n    imgs = torch.randn(1, 3, 224, 224)\n    feat = model(imgs)\n    assert len(feat) == 4\n    assert feat[0].shape == torch.Size([1, 256, 56, 56])\n    assert feat[1].shape == torch.Size([1, 512, 28, 28])\n    assert feat[2].shape == torch.Size([1, 1024, 14, 14])\n    assert feat[3].shape == torch.Size([1, 2048, 7, 7])\n\n    # Test ResNet50 with 1 GeneralizedAttention after conv2, 1 NonLocal2D\n    # after conv2, 1 ContextBlock after conv3 in layers 2, 3, 4\n    plugins = [\n        dict(\n            cfg=dict(\n                type=\'GeneralizedAttention\',\n                spatial_range=-1,\n                num_heads=8,\n                attention_type=\'0010\',\n                kv_stride=2),\n            stages=(False, True, True, True),\n            position=\'after_conv2\'),\n        dict(cfg=dict(type=\'NonLocal2D\'), position=\'after_conv2\'),\n        dict(\n            cfg=dict(type=\'ContextBlock\', ratio=1. / 16),\n            stages=(False, True, True, False),\n            position=\'after_conv3\')\n    ]\n    model = ResNet(50, plugins=plugins)\n    for m in model.layer1.modules():\n        if is_block(m):\n            assert not hasattr(m, \'context_block\')\n            assert not hasattr(m, \'gen_attention_block\')\n            assert m.nonlocal_block.in_channels == 64\n    for m in model.layer2.modules():\n        if is_block(m):\n            assert m.nonlocal_block.in_channels == 128\n            assert m.gen_attention_block.in_channels == 128\n            assert m.context_block.in_channels == 512\n\n    for m in model.layer3.modules():\n        if is_block(m):\n            assert m.nonlocal_block.in_channels == 256\n            assert m.gen_attention_block.in_channels == 256\n            assert m.context_block.in_channels == 1024\n\n    for m in model.layer4.modules():\n        if is_block(m):\n            assert m.nonlocal_block.in_channels == 512\n            assert m.gen_attention_block.in_channels == 512\n            assert not hasattr(m, \'context_block\')\n    model.init_weights()\n    model.train()\n\n    imgs = torch.randn(1, 3, 224, 224)\n    feat = model(imgs)\n    assert len(feat) == 4\n    assert feat[0].shape == torch.Size([1, 256, 56, 56])\n    assert feat[1].shape == torch.Size([1, 512, 28, 28])\n    assert feat[2].shape == torch.Size([1, 1024, 14, 14])\n    assert feat[3].shape == torch.Size([1, 2048, 7, 7])\n\n    # Test ResNet50 with 1 ContextBlock after conv2, 1 ContextBlock after\n    # conv3 in layers 2, 3, 4\n    plugins = [\n        dict(\n            cfg=dict(type=\'ContextBlock\', ratio=1. / 16, postfix=1),\n            stages=(False, True, True, False),\n            position=\'after_conv3\'),\n        dict(\n            cfg=dict(type=\'ContextBlock\', ratio=1. / 16, postfix=2),\n            stages=(False, True, True, False),\n            position=\'after_conv3\')\n    ]\n\n    model = ResNet(50, plugins=plugins)\n    for m in model.layer1.modules():\n        if is_block(m):\n            assert not hasattr(m, \'context_block\')\n            assert not hasattr(m, \'context_block1\')\n            assert not hasattr(m, \'context_block2\')\n    for m in model.layer2.modules():\n        if is_block(m):\n            assert not hasattr(m, \'context_block\')\n            assert m.context_block1.in_channels == 512\n            assert m.context_block2.in_channels == 512\n\n    for m in model.layer3.modules():\n        if is_block(m):\n            assert not hasattr(m, \'context_block\')\n            assert m.context_block1.in_channels == 1024\n            assert m.context_block2.in_channels == 1024\n\n    for m in model.layer4.modules():\n        if is_block(m):\n            assert not hasattr(m, \'context_block\')\n            assert not hasattr(m, \'context_block1\')\n            assert not hasattr(m, \'context_block2\')\n    model.init_weights()\n    model.train()\n\n    imgs = torch.randn(1, 3, 224, 224)\n    feat = model(imgs)\n    assert len(feat) == 4\n    assert feat[0].shape == torch.Size([1, 256, 56, 56])\n    assert feat[1].shape == torch.Size([1, 512, 28, 28])\n    assert feat[2].shape == torch.Size([1, 1024, 14, 14])\n    assert feat[3].shape == torch.Size([1, 2048, 7, 7])\n\n    # Test ResNet50 zero initialization of residual\n    model = ResNet(50, zero_init_residual=True)\n    model.init_weights()\n    for m in model.modules():\n        if isinstance(m, Bottleneck):\n            assert all_zeros(m.norm3)\n        elif isinstance(m, BasicBlock):\n            assert all_zeros(m.norm2)\n    model.train()\n\n    imgs = torch.randn(1, 3, 224, 224)\n    feat = model(imgs)\n    assert len(feat) == 4\n    assert feat[0].shape == torch.Size([1, 256, 56, 56])\n    assert feat[1].shape == torch.Size([1, 512, 28, 28])\n    assert feat[2].shape == torch.Size([1, 1024, 14, 14])\n    assert feat[3].shape == torch.Size([1, 2048, 7, 7])\n\n    # Test ResNetV1d forward\n    model = ResNetV1d(depth=50)\n    model.init_weights()\n    model.train()\n\n    imgs = torch.randn(1, 3, 224, 224)\n    feat = model(imgs)\n    assert len(feat) == 4\n    assert feat[0].shape == torch.Size([1, 256, 56, 56])\n    assert feat[1].shape == torch.Size([1, 512, 28, 28])\n    assert feat[2].shape == torch.Size([1, 1024, 14, 14])\n    assert feat[3].shape == torch.Size([1, 2048, 7, 7])\n\n    # Test ResNet50 stem_channels\n    model = ResNet(depth=50, stem_channels=128)\n    model.init_weights()\n    model.train()\n    assert model.conv1.out_channels == 128\n    assert model.layer1[0].conv1.in_channels == 128\n\n    imgs = torch.randn(1, 3, 224, 224)\n    feat = model(imgs)\n    assert len(feat) == 4\n    assert feat[0].shape == torch.Size([1, 256, 56, 56])\n    assert feat[1].shape == torch.Size([1, 512, 28, 28])\n    assert feat[2].shape == torch.Size([1, 1024, 14, 14])\n    assert feat[3].shape == torch.Size([1, 2048, 7, 7])\n\n    # Test ResNet50V1d stem_channels\n    model = ResNetV1d(depth=50, stem_channels=128)\n    model.init_weights()\n    model.train()\n    assert model.stem[0].out_channels == 64\n    assert model.stem[3].out_channels == 64\n    assert model.stem[6].out_channels == 128\n    assert model.layer1[0].conv1.in_channels == 128\n\n    imgs = torch.randn(1, 3, 224, 224)\n    feat = model(imgs)\n    assert len(feat) == 4\n    assert feat[0].shape == torch.Size([1, 256, 56, 56])\n    assert feat[1].shape == torch.Size([1, 512, 28, 28])\n    assert feat[2].shape == torch.Size([1, 1024, 14, 14])\n    assert feat[3].shape == torch.Size([1, 2048, 7, 7])\n\n\ndef test_renext_bottleneck():\n    with pytest.raises(AssertionError):\n        # Style must be in [\'pytorch\', \'caffe\']\n        BottleneckX(64, 64, groups=32, base_width=4, style=\'tensorflow\')\n\n    # Test ResNeXt Bottleneck structure\n    block = BottleneckX(\n        64, 64, groups=32, base_width=4, stride=2, style=\'pytorch\')\n    assert block.conv2.stride == (2, 2)\n    assert block.conv2.groups == 32\n    assert block.conv2.out_channels == 128\n\n    # Test ResNeXt Bottleneck with DCN\n    dcn = dict(type=\'DCN\', deformable_groups=1, fallback_on_stride=False)\n    with pytest.raises(AssertionError):\n        # conv_cfg must be None if dcn is not None\n        BottleneckX(\n            64,\n            64,\n            groups=32,\n            base_width=4,\n            dcn=dcn,\n            conv_cfg=dict(type=\'Conv\'))\n    BottleneckX(64, 64, dcn=dcn)\n\n    # Test ResNeXt Bottleneck forward\n    block = BottleneckX(64, 16, groups=32, base_width=4)\n    x = torch.randn(1, 64, 56, 56)\n    x_out = block(x)\n    assert x_out.shape == torch.Size([1, 64, 56, 56])\n\n\ndef test_resnext_backbone():\n    with pytest.raises(KeyError):\n        # ResNeXt depth should be in [50, 101, 152]\n        ResNeXt(depth=18)\n\n    # Test ResNeXt with group 32, base_width 4\n    model = ResNeXt(depth=50, groups=32, base_width=4)\n    for m in model.modules():\n        if is_block(m):\n            assert m.conv2.groups == 32\n    model.init_weights()\n    model.train()\n\n    imgs = torch.randn(1, 3, 224, 224)\n    feat = model(imgs)\n    assert len(feat) == 4\n    assert feat[0].shape == torch.Size([1, 256, 56, 56])\n    assert feat[1].shape == torch.Size([1, 512, 28, 28])\n    assert feat[2].shape == torch.Size([1, 1024, 14, 14])\n    assert feat[3].shape == torch.Size([1, 2048, 7, 7])\n\n\nregnet_test_data = [\n    (\'regnetx_800mf\',\n     dict(w0=56, wa=35.73, wm=2.28, group_w=16, depth=16,\n          bot_mul=1.0), [64, 128, 288, 672]),\n    (\'regnetx_1.6gf\',\n     dict(w0=80, wa=34.01, wm=2.25, group_w=24, depth=18,\n          bot_mul=1.0), [72, 168, 408, 912]),\n    (\'regnetx_3.2gf\',\n     dict(w0=88, wa=26.31, wm=2.25, group_w=48, depth=25,\n          bot_mul=1.0), [96, 192, 432, 1008]),\n    (\'regnetx_4.0gf\',\n     dict(w0=96, wa=38.65, wm=2.43, group_w=40, depth=23,\n          bot_mul=1.0), [80, 240, 560, 1360]),\n    (\'regnetx_6.4gf\',\n     dict(w0=184, wa=60.83, wm=2.07, group_w=56, depth=17,\n          bot_mul=1.0), [168, 392, 784, 1624]),\n    (\'regnetx_8.0gf\',\n     dict(w0=80, wa=49.56, wm=2.88, group_w=120, depth=23,\n          bot_mul=1.0), [80, 240, 720, 1920]),\n    (\'regnetx_12gf\',\n     dict(w0=168, wa=73.36, wm=2.37, group_w=112, depth=19,\n          bot_mul=1.0), [224, 448, 896, 2240]),\n]\n\n\n@pytest.mark.parametrize(\'arch_name,arch,out_channels\', regnet_test_data)\ndef test_regnet_backbone(arch_name, arch, out_channels):\n    with pytest.raises(AssertionError):\n        # ResNeXt depth should be in [50, 101, 152]\n        RegNet(arch_name + \'233\')\n\n    # Test RegNet with arch_name\n    model = RegNet(arch_name)\n    model.init_weights()\n    model.train()\n\n    imgs = torch.randn(1, 3, 224, 224)\n    feat = model(imgs)\n    assert len(feat) == 4\n    assert feat[0].shape == torch.Size([1, out_channels[0], 56, 56])\n    assert feat[1].shape == torch.Size([1, out_channels[1], 28, 28])\n    assert feat[2].shape == torch.Size([1, out_channels[2], 14, 14])\n    assert feat[3].shape == torch.Size([1, out_channels[3], 7, 7])\n\n    # Test RegNet with arch\n    model = RegNet(arch)\n    assert feat[0].shape == torch.Size([1, out_channels[0], 56, 56])\n    assert feat[1].shape == torch.Size([1, out_channels[1], 28, 28])\n    assert feat[2].shape == torch.Size([1, out_channels[2], 14, 14])\n    assert feat[3].shape == torch.Size([1, out_channels[3], 7, 7])\n\n\ndef test_res2net_bottle2neck():\n    with pytest.raises(AssertionError):\n        # Style must be in [\'pytorch\', \'caffe\']\n        Bottle2neck(64, 64, base_width=26, scales=4, style=\'tensorflow\')\n\n    with pytest.raises(AssertionError):\n        # Scale must be larger than 1\n        Bottle2neck(64, 64, base_width=26, scales=1, style=\'pytorch\')\n\n    # Test Res2Net Bottle2neck structure\n    block = Bottle2neck(\n        64, 64, base_width=26, stride=2, scales=4, style=\'pytorch\')\n    assert block.scales == 4\n\n    # Test Res2Net Bottle2neck with DCN\n    dcn = dict(type=\'DCN\', deformable_groups=1, fallback_on_stride=False)\n    with pytest.raises(AssertionError):\n        # conv_cfg must be None if dcn is not None\n        Bottle2neck(\n            64,\n            64,\n            base_width=26,\n            scales=4,\n            dcn=dcn,\n            conv_cfg=dict(type=\'Conv\'))\n    Bottle2neck(64, 64, dcn=dcn)\n\n    # Test Res2Net Bottle2neck forward\n    block = Bottle2neck(64, 16, base_width=26, scales=4)\n    x = torch.randn(1, 64, 56, 56)\n    x_out = block(x)\n    assert x_out.shape == torch.Size([1, 64, 56, 56])\n\n\ndef test_res2net_backbone():\n    with pytest.raises(KeyError):\n        # Res2Net depth should be in [50, 101, 152]\n        Res2Net(depth=18)\n\n    # Test Res2Net with scales 4, base_width 26\n    model = Res2Net(depth=50, scales=4, base_width=26)\n    for m in model.modules():\n        if is_block(m):\n            assert m.scales == 4\n    model.init_weights()\n    model.train()\n\n    imgs = torch.randn(1, 3, 224, 224)\n    feat = model(imgs)\n    assert len(feat) == 4\n    assert feat[0].shape == torch.Size([1, 256, 56, 56])\n    assert feat[1].shape == torch.Size([1, 512, 28, 28])\n    assert feat[2].shape == torch.Size([1, 1024, 14, 14])\n    assert feat[3].shape == torch.Size([1, 2048, 7, 7])\n'"
tests/test_config.py,5,"b'from os.path import dirname, exists, join, relpath\n\nimport torch\nfrom mmcv.runner import build_optimizer\n\nfrom mmdet.core import BitmapMasks, PolygonMasks\n\n\ndef _get_config_directory():\n    """""" Find the predefined detector config directory """"""\n    try:\n        # Assume we are running in the source mmdetection repo\n        repo_dpath = dirname(dirname(__file__))\n    except NameError:\n        # For IPython development when this __file__ is not defined\n        import mmdet\n        repo_dpath = dirname(dirname(mmdet.__file__))\n    config_dpath = join(repo_dpath, \'configs\')\n    if not exists(config_dpath):\n        raise Exception(\'Cannot find config path\')\n    return config_dpath\n\n\ndef test_config_build_detector():\n    """"""\n    Test that all detection models defined in the configs can be initialized.\n    """"""\n    from mmcv import Config\n    from mmdet.models import build_detector\n\n    config_dpath = _get_config_directory()\n    print(f\'Found config_dpath = {config_dpath}\')\n\n    import glob\n    config_fpaths = list(glob.glob(join(config_dpath, \'**\', \'*.py\')))\n    config_fpaths = [p for p in config_fpaths if p.find(\'_base_\') == -1]\n    config_names = [relpath(p, config_dpath) for p in config_fpaths]\n\n    print(f\'Using {len(config_names)} config files\')\n\n    for config_fname in config_names:\n        config_fpath = join(config_dpath, config_fname)\n        config_mod = Config.fromfile(config_fpath)\n\n        config_mod.model\n        config_mod.train_cfg\n        config_mod.test_cfg\n        print(f\'Building detector, config_fpath = {config_fpath}\')\n\n        # Remove pretrained keys to allow for testing in an offline environment\n        if \'pretrained\' in config_mod.model:\n            config_mod.model[\'pretrained\'] = None\n\n        detector = build_detector(\n            config_mod.model,\n            train_cfg=config_mod.train_cfg,\n            test_cfg=config_mod.test_cfg)\n        assert detector is not None\n\n        optimizer = build_optimizer(detector, config_mod.optimizer)\n        assert isinstance(optimizer, torch.optim.Optimizer)\n\n        if \'roi_head\' in config_mod.model.keys():\n            # for two stage detector\n            # detectors must have bbox head\n            assert detector.roi_head.with_bbox and detector.with_bbox\n            assert detector.roi_head.with_mask == detector.with_mask\n\n            head_config = config_mod.model[\'roi_head\']\n            _check_roi_head(head_config, detector.roi_head)\n        # else:\n        #     # for single stage detector\n        #     # detectors must have bbox head\n        #     # assert detector.with_bbox\n        #     head_config = config_mod.model[\'bbox_head\']\n        #     _check_bbox_head(head_config, detector.bbox_head)\n\n\ndef test_config_data_pipeline():\n    """"""\n    Test whether the data pipeline is valid and can process corner cases.\n    CommandLine:\n        xdoctest -m tests/test_config.py test_config_build_data_pipeline\n    """"""\n    from mmcv import Config\n    from mmdet.datasets.pipelines import Compose\n    import numpy as np\n\n    config_dpath = _get_config_directory()\n    print(f\'Found config_dpath = {config_dpath}\')\n\n    # Only tests a representative subset of configurations\n    # TODO: test pipelines using Albu, current Albu throw None given empty GT\n    config_names = [\n        \'wider_face/ssd300_wider_face.py\',\n        \'pascal_voc/ssd300_voc0712.py\',\n        \'pascal_voc/ssd512_voc0712.py\',\n        # \'albu_example/mask_rcnn_r50_fpn_1x.py\',\n        \'foveabox/fovea_align_r50_fpn_gn-head_mstrain_640-800_4x4_2x_coco.py\',\n        \'mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_coco.py\',\n        \'mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain_1x_coco.py\',\n        \'fp16/mask_rcnn_r50_fpn_fp16_1x_coco.py\'\n    ]\n\n    def dummy_masks(h, w, num_obj=3, mode=\'bitmap\'):\n        assert mode in (\'polygon\', \'bitmap\')\n        if mode == \'bitmap\':\n            masks = np.random.randint(0, 2, (num_obj, h, w), dtype=np.uint8)\n            masks = BitmapMasks(masks, h, w)\n        else:\n            masks = []\n            for i in range(num_obj):\n                masks.append([])\n                masks[-1].append(\n                    np.random.uniform(0, min(h - 1, w - 1), (8 + 4 * i, )))\n                masks[-1].append(\n                    np.random.uniform(0, min(h - 1, w - 1), (10 + 4 * i, )))\n            masks = PolygonMasks(masks, h, w)\n        return masks\n\n    print(f\'Using {len(config_names)} config files\')\n\n    for config_fname in config_names:\n        config_fpath = join(config_dpath, config_fname)\n        config_mod = Config.fromfile(config_fpath)\n\n        # remove loading pipeline\n        loading_pipeline = config_mod.train_pipeline.pop(0)\n        loading_ann_pipeline = config_mod.train_pipeline.pop(0)\n        config_mod.test_pipeline.pop(0)\n\n        train_pipeline = Compose(config_mod.train_pipeline)\n        test_pipeline = Compose(config_mod.test_pipeline)\n\n        print(f\'Building data pipeline, config_fpath = {config_fpath}\')\n\n        print(f\'Test training data pipeline: \\n{train_pipeline!r}\')\n        img = np.random.randint(0, 255, size=(888, 666, 3), dtype=np.uint8)\n        if loading_pipeline.get(\'to_float32\', False):\n            img = img.astype(np.float32)\n        mode = \'bitmap\' if loading_ann_pipeline.get(\'poly2mask\',\n                                                    True) else \'polygon\'\n        results = dict(\n            filename=\'test_img.png\',\n            ori_filename=\'test_img.png\',\n            img=img,\n            img_shape=img.shape,\n            ori_shape=img.shape,\n            gt_bboxes=np.array([[35.2, 11.7, 39.7, 15.7]], dtype=np.float32),\n            gt_labels=np.array([1], dtype=np.int64),\n            gt_masks=dummy_masks(img.shape[0], img.shape[1], mode=mode),\n        )\n        results[\'img_fields\'] = [\'img\']\n        results[\'bbox_fields\'] = [\'gt_bboxes\']\n        results[\'mask_fields\'] = [\'gt_masks\']\n        output_results = train_pipeline(results)\n        assert output_results is not None\n\n        print(f\'Test testing data pipeline: \\n{test_pipeline!r}\')\n        results = dict(\n            filename=\'test_img.png\',\n            ori_filename=\'test_img.png\',\n            img=img,\n            img_shape=img.shape,\n            ori_shape=img.shape,\n            gt_bboxes=np.array([[35.2, 11.7, 39.7, 15.7]], dtype=np.float32),\n            gt_labels=np.array([1], dtype=np.int64),\n            gt_masks=dummy_masks(img.shape[0], img.shape[1], mode=mode),\n        )\n        results[\'img_fields\'] = [\'img\']\n        results[\'bbox_fields\'] = [\'gt_bboxes\']\n        results[\'mask_fields\'] = [\'gt_masks\']\n        output_results = test_pipeline(results)\n        assert output_results is not None\n\n        # test empty GT\n        print(\'Test empty GT with training data pipeline: \'\n              f\'\\n{train_pipeline!r}\')\n        results = dict(\n            filename=\'test_img.png\',\n            ori_filename=\'test_img.png\',\n            img=img,\n            img_shape=img.shape,\n            ori_shape=img.shape,\n            gt_bboxes=np.zeros((0, 4), dtype=np.float32),\n            gt_labels=np.array([], dtype=np.int64),\n            gt_masks=dummy_masks(\n                img.shape[0], img.shape[1], num_obj=0, mode=mode),\n        )\n        results[\'img_fields\'] = [\'img\']\n        results[\'bbox_fields\'] = [\'gt_bboxes\']\n        results[\'mask_fields\'] = [\'gt_masks\']\n        output_results = train_pipeline(results)\n        assert output_results is not None\n\n        print(f\'Test empty GT with testing data pipeline: \\n{test_pipeline!r}\')\n        results = dict(\n            filename=\'test_img.png\',\n            ori_filename=\'test_img.png\',\n            img=img,\n            img_shape=img.shape,\n            ori_shape=img.shape,\n            gt_bboxes=np.zeros((0, 4), dtype=np.float32),\n            gt_labels=np.array([], dtype=np.int64),\n            gt_masks=dummy_masks(\n                img.shape[0], img.shape[1], num_obj=0, mode=mode),\n        )\n        results[\'img_fields\'] = [\'img\']\n        results[\'bbox_fields\'] = [\'gt_bboxes\']\n        results[\'mask_fields\'] = [\'gt_masks\']\n        output_results = test_pipeline(results)\n        assert output_results is not None\n\n\ndef _check_roi_head(config, head):\n    # check consistency between head_config and roi_head\n    assert config[\'type\'] == head.__class__.__name__\n\n    # check roi_align\n    bbox_roi_cfg = config.bbox_roi_extractor\n    bbox_roi_extractor = head.bbox_roi_extractor\n    _check_roi_extractor(bbox_roi_cfg, bbox_roi_extractor)\n\n    # check bbox head infos\n    bbox_cfg = config.bbox_head\n    bbox_head = head.bbox_head\n    _check_bbox_head(bbox_cfg, bbox_head)\n\n    if head.with_mask:\n        # check roi_align\n        if config.mask_roi_extractor:\n            mask_roi_cfg = config.mask_roi_extractor\n            mask_roi_extractor = head.mask_roi_extractor\n            _check_roi_extractor(mask_roi_cfg, mask_roi_extractor,\n                                 bbox_roi_extractor)\n\n        # check mask head infos\n        mask_head = head.mask_head\n        mask_cfg = config.mask_head\n        _check_mask_head(mask_cfg, mask_head)\n\n    # check arch specific settings, e.g., cascade/htc\n    if config[\'type\'] in [\'CascadeRoIHead\', \'HybridTaskCascadeRoIHead\']:\n        assert config.num_stages == len(head.bbox_head)\n        assert config.num_stages == len(head.bbox_roi_extractor)\n\n        if head.with_mask:\n            assert config.num_stages == len(head.mask_head)\n            assert config.num_stages == len(head.mask_roi_extractor)\n\n    elif config[\'type\'] in [\'MaskScoringRoIHead\']:\n        assert (hasattr(head, \'mask_iou_head\')\n                and head.mask_iou_head is not None)\n        mask_iou_cfg = config.mask_iou_head\n        mask_iou_head = head.mask_iou_head\n        assert (mask_iou_cfg.fc_out_channels ==\n                mask_iou_head.fc_mask_iou.in_features)\n\n    elif config[\'type\'] in [\'GridRoIHead\']:\n        grid_roi_cfg = config.grid_roi_extractor\n        grid_roi_extractor = head.grid_roi_extractor\n        _check_roi_extractor(grid_roi_cfg, grid_roi_extractor,\n                             bbox_roi_extractor)\n\n        config.grid_head.grid_points = head.grid_head.grid_points\n\n\ndef _check_roi_extractor(config, roi_extractor, prev_roi_extractor=None):\n    import torch.nn as nn\n    if isinstance(roi_extractor, nn.ModuleList):\n        if prev_roi_extractor:\n            prev_roi_extractor = prev_roi_extractor[0]\n        roi_extractor = roi_extractor[0]\n\n    assert (len(config.featmap_strides) == len(roi_extractor.roi_layers))\n    assert (config.out_channels == roi_extractor.out_channels)\n    from torch.nn.modules.utils import _pair\n    assert (_pair(\n        config.roi_layer.out_size) == roi_extractor.roi_layers[0].out_size)\n\n    if \'use_torchvision\' in config.roi_layer:\n        assert (config.roi_layer.use_torchvision ==\n                roi_extractor.roi_layers[0].use_torchvision)\n    elif \'aligned\' in config.roi_layer:\n        assert (\n            config.roi_layer.aligned == roi_extractor.roi_layers[0].aligned)\n\n    if prev_roi_extractor:\n        assert (roi_extractor.roi_layers[0].aligned ==\n                prev_roi_extractor.roi_layers[0].aligned)\n        assert (roi_extractor.roi_layers[0].use_torchvision ==\n                prev_roi_extractor.roi_layers[0].use_torchvision)\n\n\ndef _check_mask_head(mask_cfg, mask_head):\n    import torch.nn as nn\n    if isinstance(mask_cfg, list):\n        for single_mask_cfg, single_mask_head in zip(mask_cfg, mask_head):\n            _check_mask_head(single_mask_cfg, single_mask_head)\n    elif isinstance(mask_head, nn.ModuleList):\n        for single_mask_head in mask_head:\n            _check_mask_head(mask_cfg, single_mask_head)\n    else:\n        assert mask_cfg[\'type\'] == mask_head.__class__.__name__\n        assert mask_cfg.in_channels == mask_head.in_channels\n        assert (\n            mask_cfg.conv_out_channels == mask_head.conv_logits.in_channels)\n        class_agnostic = mask_cfg.get(\'class_agnostic\', False)\n        out_dim = (1 if class_agnostic else mask_cfg.num_classes)\n        assert mask_head.conv_logits.out_channels == out_dim\n\n\ndef _check_bbox_head(bbox_cfg, bbox_head):\n    import torch.nn as nn\n    if isinstance(bbox_cfg, list):\n        for single_bbox_cfg, single_bbox_head in zip(bbox_cfg, bbox_head):\n            _check_bbox_head(single_bbox_cfg, single_bbox_head)\n    elif isinstance(bbox_head, nn.ModuleList):\n        for single_bbox_head in bbox_head:\n            _check_bbox_head(bbox_cfg, single_bbox_head)\n    else:\n        assert bbox_cfg[\'type\'] == bbox_head.__class__.__name__\n        assert bbox_cfg.in_channels == bbox_head.in_channels\n        with_cls = bbox_cfg.get(\'with_cls\', True)\n        if with_cls:\n            fc_out_channels = bbox_cfg.get(\'fc_out_channels\', 2048)\n            assert (fc_out_channels == bbox_head.fc_cls.in_features)\n            assert bbox_cfg.num_classes + 1 == bbox_head.fc_cls.out_features\n\n        with_reg = bbox_cfg.get(\'with_reg\', True)\n        if with_reg:\n            out_dim = (4 if bbox_cfg.reg_class_agnostic else 4 *\n                       bbox_cfg.num_classes)\n            assert bbox_head.fc_reg.out_features == out_dim\n\n\ndef _check_anchorhead(config, head):\n    # check consistency between head_config and roi_head\n    assert config[\'type\'] == head.__class__.__name__\n    assert config.in_channels == head.in_channels\n\n    num_classes = (\n        config.num_classes -\n        1 if config.loss_cls.get(\'use_sigmoid\', False) else config.num_classes)\n    if config[\'type\'] == \'ATSSHead\':\n        assert (config.feat_channels == head.atss_cls.in_channels)\n        assert (config.feat_channels == head.atss_reg.in_channels)\n        assert (config.feat_channels == head.atss_centerness.in_channels)\n    else:\n        assert (config.in_channels == head.conv_cls.in_channels)\n        assert (config.in_channels == head.conv_reg.in_channels)\n        assert (head.conv_cls.out_channels == num_classes * head.num_anchors)\n        assert head.fc_reg.out_channels == 4 * head.num_anchors\n'"
tests/test_dataset.py,0,"b""import bisect\nimport math\nfrom collections import defaultdict\nfrom unittest.mock import MagicMock\n\nimport numpy as np\nimport pytest\n\nfrom mmdet.datasets import (DATASETS, ClassBalancedDataset, ConcatDataset,\n                            CustomDataset, RepeatDataset)\n\n\n@pytest.mark.parametrize('dataset',\n                         ['CocoDataset', 'VOCDataset', 'CityscapesDataset'])\ndef test_custom_classes_override_default(dataset):\n    dataset_class = DATASETS.get(dataset)\n    dataset_class.load_annotations = MagicMock()\n    if dataset in ['CocoDataset', 'CityscapesDataset']:\n        dataset_class.coco = MagicMock()\n        dataset_class.cat_ids = MagicMock()\n\n    original_classes = dataset_class.CLASSES\n\n    # Test setting classes as a tuple\n    custom_dataset = dataset_class(\n        ann_file=MagicMock(),\n        pipeline=[],\n        classes=('bus', 'car'),\n        test_mode=True,\n        img_prefix='VOC2007' if dataset == 'VOCDataset' else '')\n\n    assert custom_dataset.CLASSES != original_classes\n    assert custom_dataset.CLASSES == ('bus', 'car')\n    assert custom_dataset.custom_classes\n\n    # Test setting classes as a list\n    custom_dataset = dataset_class(\n        ann_file=MagicMock(),\n        pipeline=[],\n        classes=['bus', 'car'],\n        test_mode=True,\n        img_prefix='VOC2007' if dataset == 'VOCDataset' else '')\n\n    assert custom_dataset.CLASSES != original_classes\n    assert custom_dataset.CLASSES == ['bus', 'car']\n    assert custom_dataset.custom_classes\n\n    # Test overriding not a subset\n    custom_dataset = dataset_class(\n        ann_file=MagicMock(),\n        pipeline=[],\n        classes=['foo'],\n        test_mode=True,\n        img_prefix='VOC2007' if dataset == 'VOCDataset' else '')\n\n    assert custom_dataset.CLASSES != original_classes\n    assert custom_dataset.CLASSES == ['foo']\n    assert custom_dataset.custom_classes\n\n    # Test default behavior\n    custom_dataset = dataset_class(\n        ann_file=MagicMock(),\n        pipeline=[],\n        classes=None,\n        test_mode=True,\n        img_prefix='VOC2007' if dataset == 'VOCDataset' else '')\n\n    assert custom_dataset.CLASSES == original_classes\n    assert not custom_dataset.custom_classes\n\n    # Test sending file path\n    import tempfile\n    tmp_file = tempfile.NamedTemporaryFile()\n    with open(tmp_file.name, 'w') as f:\n        f.write('bus\\ncar\\n')\n    custom_dataset = dataset_class(\n        ann_file=MagicMock(),\n        pipeline=[],\n        classes=tmp_file.name,\n        test_mode=True,\n        img_prefix='VOC2007' if dataset == 'VOCDataset' else '')\n    tmp_file.close()\n\n    assert custom_dataset.CLASSES != original_classes\n    assert custom_dataset.CLASSES == ['bus', 'car']\n    assert custom_dataset.custom_classes\n\n\ndef test_dataset_wrapper():\n    CustomDataset.load_annotations = MagicMock()\n    CustomDataset.__getitem__ = MagicMock(side_effect=lambda idx: idx)\n    dataset_a = CustomDataset(\n        ann_file=MagicMock(), pipeline=[], test_mode=True, img_prefix='')\n    len_a = 10\n    cat_ids_list_a = [\n        np.random.randint(0, 80, num).tolist()\n        for num in np.random.randint(1, 20, len_a)\n    ]\n    dataset_a.data_infos = MagicMock()\n    dataset_a.data_infos.__len__.return_value = len_a\n    dataset_a.get_cat_ids = MagicMock(\n        side_effect=lambda idx: cat_ids_list_a[idx])\n    dataset_b = CustomDataset(\n        ann_file=MagicMock(), pipeline=[], test_mode=True, img_prefix='')\n    len_b = 20\n    cat_ids_list_b = [\n        np.random.randint(0, 80, num).tolist()\n        for num in np.random.randint(1, 20, len_b)\n    ]\n    dataset_b.data_infos = MagicMock()\n    dataset_b.data_infos.__len__.return_value = len_b\n    dataset_b.get_cat_ids = MagicMock(\n        side_effect=lambda idx: cat_ids_list_b[idx])\n\n    concat_dataset = ConcatDataset([dataset_a, dataset_b])\n    assert concat_dataset[5] == 5\n    assert concat_dataset[25] == 15\n    assert concat_dataset.get_cat_ids(5) == cat_ids_list_a[5]\n    assert concat_dataset.get_cat_ids(25) == cat_ids_list_b[15]\n    assert len(concat_dataset) == len(dataset_a) + len(dataset_b)\n\n    repeat_dataset = RepeatDataset(dataset_a, 10)\n    assert repeat_dataset[5] == 5\n    assert repeat_dataset[15] == 5\n    assert repeat_dataset[27] == 7\n    assert repeat_dataset.get_cat_ids(5) == cat_ids_list_a[5]\n    assert repeat_dataset.get_cat_ids(15) == cat_ids_list_a[5]\n    assert repeat_dataset.get_cat_ids(27) == cat_ids_list_a[7]\n    assert len(repeat_dataset) == 10 * len(dataset_a)\n\n    category_freq = defaultdict(int)\n    for cat_ids in cat_ids_list_a:\n        cat_ids = set(cat_ids)\n        for cat_id in cat_ids:\n            category_freq[cat_id] += 1\n    for k, v in category_freq.items():\n        category_freq[k] = v / len(cat_ids_list_a)\n\n    mean_freq = np.mean(list(category_freq.values()))\n    repeat_thr = mean_freq\n\n    category_repeat = {\n        cat_id: max(1.0, math.sqrt(repeat_thr / cat_freq))\n        for cat_id, cat_freq in category_freq.items()\n    }\n\n    repeat_factors = []\n    for cat_ids in cat_ids_list_a:\n        cat_ids = set(cat_ids)\n        repeat_factor = max({category_repeat[cat_id] for cat_id in cat_ids})\n        repeat_factors.append(math.ceil(repeat_factor))\n    repeat_factors_cumsum = np.cumsum(repeat_factors)\n    repeat_factor_dataset = ClassBalancedDataset(dataset_a, repeat_thr)\n    assert len(repeat_factor_dataset) == repeat_factors_cumsum[-1]\n    for idx in np.random.randint(0, len(repeat_factor_dataset), 3):\n        assert repeat_factor_dataset[idx] == bisect.bisect_right(\n            repeat_factors_cumsum, idx)\n"""
tests/test_forward.py,8,"b'""""""\npytest tests/test_forward.py\n""""""\nimport copy\nfrom os.path import dirname, exists, join\n\nimport numpy as np\nimport pytest\nimport torch\n\n\ndef _get_config_directory():\n    """""" Find the predefined detector config directory """"""\n    try:\n        # Assume we are running in the source mmdetection repo\n        repo_dpath = dirname(dirname(__file__))\n    except NameError:\n        # For IPython development when this __file__ is not defined\n        import mmdet\n        repo_dpath = dirname(dirname(mmdet.__file__))\n    config_dpath = join(repo_dpath, \'configs\')\n    if not exists(config_dpath):\n        raise Exception(\'Cannot find config path\')\n    return config_dpath\n\n\ndef _get_config_module(fname):\n    """"""\n    Load a configuration as a python module\n    """"""\n    from mmcv import Config\n    config_dpath = _get_config_directory()\n    config_fpath = join(config_dpath, fname)\n    config_mod = Config.fromfile(config_fpath)\n    return config_mod\n\n\ndef _get_detector_cfg(fname):\n    """"""\n    Grab configs necessary to create a detector. These are deep copied to allow\n    for safe modification of parameters without influencing other tests.\n    """"""\n    import mmcv\n    config = _get_config_module(fname)\n    model = copy.deepcopy(config.model)\n    train_cfg = mmcv.Config(copy.deepcopy(config.train_cfg))\n    test_cfg = mmcv.Config(copy.deepcopy(config.test_cfg))\n    return model, train_cfg, test_cfg\n\n\ndef test_rpn_forward():\n    model, train_cfg, test_cfg = _get_detector_cfg(\n        \'rpn/rpn_r50_fpn_1x_coco.py\')\n    model[\'pretrained\'] = None\n\n    from mmdet.models import build_detector\n    detector = build_detector(model, train_cfg=train_cfg, test_cfg=test_cfg)\n\n    input_shape = (1, 3, 224, 224)\n    mm_inputs = _demo_mm_inputs(input_shape)\n\n    imgs = mm_inputs.pop(\'imgs\')\n    img_metas = mm_inputs.pop(\'img_metas\')\n\n    # Test forward train\n    gt_bboxes = mm_inputs[\'gt_bboxes\']\n    losses = detector.forward(\n        imgs, img_metas, gt_bboxes=gt_bboxes, return_loss=True)\n    assert isinstance(losses, dict)\n\n    # Test forward test\n    with torch.no_grad():\n        img_list = [g[None, :] for g in imgs]\n        batch_results = []\n        for one_img, one_meta in zip(img_list, img_metas):\n            result = detector.forward([one_img], [[one_meta]],\n                                      return_loss=False)\n            batch_results.append(result)\n\n\n@pytest.mark.parametrize(\n    \'cfg_file\',\n    [\n        \'retinanet/retinanet_r50_fpn_1x_coco.py\',\n        \'guided_anchoring/ga_retinanet_r50_fpn_1x_coco.py\',\n        \'ghm/retinanet_ghm_r50_fpn_1x_coco.py\',\n        \'fcos/fcos_center_r50_caffe_fpn_gn-head_4x4_1x_coco.py\',\n        \'foveabox/fovea_align_r50_fpn_gn-head_4x4_2x_coco.py\',\n        # \'free_anchor/retinanet_free_anchor_r50_fpn_1x_coco.py\',\n        # \'atss/atss_r50_fpn_1x_coco.py\',  # not ready for topk\n        \'reppoints/reppoints_moment_r50_fpn_1x_coco.py\'\n    ])\ndef test_single_stage_forward_gpu(cfg_file):\n    if not torch.cuda.is_available():\n        import pytest\n        pytest.skip(\'test requires GPU and torch+cuda\')\n\n    model, train_cfg, test_cfg = _get_detector_cfg(cfg_file)\n    model[\'pretrained\'] = None\n\n    from mmdet.models import build_detector\n    detector = build_detector(model, train_cfg=train_cfg, test_cfg=test_cfg)\n\n    input_shape = (2, 3, 224, 224)\n    mm_inputs = _demo_mm_inputs(input_shape)\n\n    imgs = mm_inputs.pop(\'imgs\')\n    img_metas = mm_inputs.pop(\'img_metas\')\n\n    detector = detector.cuda()\n    imgs = imgs.cuda()\n    # Test forward train\n    gt_bboxes = [b.cuda() for b in mm_inputs[\'gt_bboxes\']]\n    gt_labels = [g.cuda() for g in mm_inputs[\'gt_labels\']]\n    losses = detector.forward(\n        imgs,\n        img_metas,\n        gt_bboxes=gt_bboxes,\n        gt_labels=gt_labels,\n        return_loss=True)\n    assert isinstance(losses, dict)\n\n    # Test forward test\n    with torch.no_grad():\n        img_list = [g[None, :] for g in imgs]\n        batch_results = []\n        for one_img, one_meta in zip(img_list, img_metas):\n            result = detector.forward([one_img], [[one_meta]],\n                                      return_loss=False)\n            batch_results.append(result)\n\n\ndef test_faster_rcnn_ohem_forward():\n    model, train_cfg, test_cfg = _get_detector_cfg(\n        \'faster_rcnn/faster_rcnn_r50_fpn_ohem_1x_coco.py\')\n    model[\'pretrained\'] = None\n\n    from mmdet.models import build_detector\n    detector = build_detector(model, train_cfg=train_cfg, test_cfg=test_cfg)\n\n    input_shape = (1, 3, 256, 256)\n\n    # Test forward train with a non-empty truth batch\n    mm_inputs = _demo_mm_inputs(input_shape, num_items=[10])\n    imgs = mm_inputs.pop(\'imgs\')\n    img_metas = mm_inputs.pop(\'img_metas\')\n    gt_bboxes = mm_inputs[\'gt_bboxes\']\n    gt_labels = mm_inputs[\'gt_labels\']\n    losses = detector.forward(\n        imgs,\n        img_metas,\n        gt_bboxes=gt_bboxes,\n        gt_labels=gt_labels,\n        return_loss=True)\n    assert isinstance(losses, dict)\n    from mmdet.apis.train import parse_losses\n    total_loss = float(parse_losses(losses)[0].item())\n    assert total_loss > 0\n\n    # Test forward train with an empty truth batch\n    mm_inputs = _demo_mm_inputs(input_shape, num_items=[0])\n    imgs = mm_inputs.pop(\'imgs\')\n    img_metas = mm_inputs.pop(\'img_metas\')\n    gt_bboxes = mm_inputs[\'gt_bboxes\']\n    gt_labels = mm_inputs[\'gt_labels\']\n    losses = detector.forward(\n        imgs,\n        img_metas,\n        gt_bboxes=gt_bboxes,\n        gt_labels=gt_labels,\n        return_loss=True)\n    assert isinstance(losses, dict)\n    from mmdet.apis.train import parse_losses\n    total_loss = float(parse_losses(losses)[0].item())\n    assert total_loss > 0\n\n\n# HTC is not ready yet\n@pytest.mark.parametrize(\'cfg_file\', [\n    \'cascade_rcnn/cascade_mask_rcnn_r50_fpn_1x_coco.py\',\n    \'mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py\',\n    \'grid_rcnn/grid_rcnn_r50_fpn_gn-head_2x_coco.py\',\n    \'ms_rcnn/ms_rcnn_r50_fpn_1x_coco.py\'\n])\ndef test_two_stage_forward(cfg_file):\n    model, train_cfg, test_cfg = _get_detector_cfg(cfg_file)\n    model[\'pretrained\'] = None\n\n    from mmdet.models import build_detector\n    detector = build_detector(model, train_cfg=train_cfg, test_cfg=test_cfg)\n\n    input_shape = (1, 3, 256, 256)\n\n    # Test forward train with a non-empty truth batch\n    mm_inputs = _demo_mm_inputs(input_shape, num_items=[10])\n    imgs = mm_inputs.pop(\'imgs\')\n    img_metas = mm_inputs.pop(\'img_metas\')\n    gt_bboxes = mm_inputs[\'gt_bboxes\']\n    gt_labels = mm_inputs[\'gt_labels\']\n    gt_masks = mm_inputs[\'gt_masks\']\n    losses = detector.forward(\n        imgs,\n        img_metas,\n        gt_bboxes=gt_bboxes,\n        gt_labels=gt_labels,\n        gt_masks=gt_masks,\n        return_loss=True)\n    assert isinstance(losses, dict)\n    from mmdet.apis.train import parse_losses\n    total_loss = parse_losses(losses)[0].requires_grad_(True)\n    assert float(total_loss.item()) > 0\n    total_loss.backward()\n\n    # Test forward train with an empty truth batch\n    mm_inputs = _demo_mm_inputs(input_shape, num_items=[0])\n    imgs = mm_inputs.pop(\'imgs\')\n    img_metas = mm_inputs.pop(\'img_metas\')\n    gt_bboxes = mm_inputs[\'gt_bboxes\']\n    gt_labels = mm_inputs[\'gt_labels\']\n    gt_masks = mm_inputs[\'gt_masks\']\n    losses = detector.forward(\n        imgs,\n        img_metas,\n        gt_bboxes=gt_bboxes,\n        gt_labels=gt_labels,\n        gt_masks=gt_masks,\n        return_loss=True)\n    assert isinstance(losses, dict)\n    from mmdet.apis.train import parse_losses\n    total_loss = parse_losses(losses)[0].requires_grad_(True)\n    assert float(total_loss.item()) > 0\n    total_loss.backward()\n\n    # Test forward test\n    with torch.no_grad():\n        img_list = [g[None, :] for g in imgs]\n        batch_results = []\n        for one_img, one_meta in zip(img_list, img_metas):\n            result = detector.forward([one_img], [[one_meta]],\n                                      return_loss=False)\n            batch_results.append(result)\n\n\n@pytest.mark.parametrize(\n    \'cfg_file\', [\'ghm/retinanet_ghm_r50_fpn_1x_coco.py\', \'ssd/ssd300_coco.py\'])\ndef test_single_stage_forward_cpu(cfg_file):\n    model, train_cfg, test_cfg = _get_detector_cfg(cfg_file)\n    model[\'pretrained\'] = None\n\n    from mmdet.models import build_detector\n    detector = build_detector(model, train_cfg=train_cfg, test_cfg=test_cfg)\n\n    input_shape = (1, 3, 300, 300)\n    mm_inputs = _demo_mm_inputs(input_shape)\n\n    imgs = mm_inputs.pop(\'imgs\')\n    img_metas = mm_inputs.pop(\'img_metas\')\n\n    # Test forward train\n    gt_bboxes = mm_inputs[\'gt_bboxes\']\n    gt_labels = mm_inputs[\'gt_labels\']\n    losses = detector.forward(\n        imgs,\n        img_metas,\n        gt_bboxes=gt_bboxes,\n        gt_labels=gt_labels,\n        return_loss=True)\n    assert isinstance(losses, dict)\n\n    # Test forward test\n    with torch.no_grad():\n        img_list = [g[None, :] for g in imgs]\n        batch_results = []\n        for one_img, one_meta in zip(img_list, img_metas):\n            result = detector.forward([one_img], [[one_meta]],\n                                      return_loss=False)\n            batch_results.append(result)\n\n\ndef _demo_mm_inputs(input_shape=(1, 3, 300, 300),\n                    num_items=None, num_classes=10):  # yapf: disable\n    """"""\n    Create a superset of inputs needed to run test or train batches.\n\n    Args:\n        input_shape (tuple):\n            input batch dimensions\n\n        num_items (None | List[int]):\n            specifies the number of boxes in each batch item\n\n        num_classes (int):\n            number of different labels a box might have\n    """"""\n    from mmdet.core import BitmapMasks\n\n    (N, C, H, W) = input_shape\n\n    rng = np.random.RandomState(0)\n\n    imgs = rng.rand(*input_shape)\n\n    img_metas = [{\n        \'img_shape\': (H, W, C),\n        \'ori_shape\': (H, W, C),\n        \'pad_shape\': (H, W, C),\n        \'filename\': \'<demo>.png\',\n        \'scale_factor\': 1.0,\n        \'flip\': False,\n    } for _ in range(N)]\n\n    gt_bboxes = []\n    gt_labels = []\n    gt_masks = []\n\n    for batch_idx in range(N):\n        if num_items is None:\n            num_boxes = rng.randint(1, 10)\n        else:\n            num_boxes = num_items[batch_idx]\n\n        cx, cy, bw, bh = rng.rand(num_boxes, 4).T\n\n        tl_x = ((cx * W) - (W * bw / 2)).clip(0, W)\n        tl_y = ((cy * H) - (H * bh / 2)).clip(0, H)\n        br_x = ((cx * W) + (W * bw / 2)).clip(0, W)\n        br_y = ((cy * H) + (H * bh / 2)).clip(0, H)\n\n        boxes = np.vstack([tl_x, tl_y, br_x, br_y]).T\n        class_idxs = rng.randint(1, num_classes, size=num_boxes)\n\n        gt_bboxes.append(torch.FloatTensor(boxes))\n        gt_labels.append(torch.LongTensor(class_idxs))\n\n    mask = np.random.randint(0, 2, (len(boxes), H, W), dtype=np.uint8)\n    gt_masks.append(BitmapMasks(mask, H, W))\n\n    mm_inputs = {\n        \'imgs\': torch.FloatTensor(imgs).requires_grad_(True),\n        \'img_metas\': img_metas,\n        \'gt_bboxes\': gt_bboxes,\n        \'gt_labels\': gt_labels,\n        \'gt_bboxes_ignore\': None,\n        \'gt_masks\': gt_masks,\n    }\n    return mm_inputs\n'"
tests/test_fp16.py,101,"b""import numpy as np\nimport pytest\nimport torch\nimport torch.nn as nn\n\nfrom mmdet.core import auto_fp16, force_fp32\nfrom mmdet.core.fp16.utils import cast_tensor_type\n\n\ndef test_cast_tensor_type():\n    inputs = torch.FloatTensor([5.])\n    src_type = torch.float32\n    dst_type = torch.int32\n    outputs = cast_tensor_type(inputs, src_type, dst_type)\n    assert isinstance(outputs, torch.Tensor)\n    assert outputs.dtype == dst_type\n\n    inputs = 'tensor'\n    src_type = str\n    dst_type = str\n    outputs = cast_tensor_type(inputs, src_type, dst_type)\n    assert isinstance(outputs, str)\n\n    inputs = np.array([5.])\n    src_type = np.ndarray\n    dst_type = np.ndarray\n    outputs = cast_tensor_type(inputs, src_type, dst_type)\n    assert isinstance(outputs, np.ndarray)\n\n    inputs = dict(\n        tensor_a=torch.FloatTensor([1.]), tensor_b=torch.FloatTensor([2.]))\n    src_type = torch.float32\n    dst_type = torch.int32\n    outputs = cast_tensor_type(inputs, src_type, dst_type)\n    assert isinstance(outputs, dict)\n    assert outputs['tensor_a'].dtype == dst_type\n    assert outputs['tensor_b'].dtype == dst_type\n\n    inputs = [torch.FloatTensor([1.]), torch.FloatTensor([2.])]\n    src_type = torch.float32\n    dst_type = torch.int32\n    outputs = cast_tensor_type(inputs, src_type, dst_type)\n    assert isinstance(outputs, list)\n    assert outputs[0].dtype == dst_type\n    assert outputs[1].dtype == dst_type\n\n    inputs = 5\n    outputs = cast_tensor_type(inputs, None, None)\n    assert isinstance(outputs, int)\n\n\ndef test_auto_fp16():\n\n    with pytest.raises(TypeError):\n        # ExampleObject is not a subclass of nn.Module\n\n        class ExampleObject(object):\n\n            @auto_fp16()\n            def __call__(self, x):\n                return x\n\n        model = ExampleObject()\n        input_x = torch.ones(1, dtype=torch.float32)\n        model(input_x)\n\n    # apply to all input args\n    class ExampleModule(nn.Module):\n\n        @auto_fp16()\n        def forward(self, x, y):\n            return x, y\n\n    model = ExampleModule()\n    input_x = torch.ones(1, dtype=torch.float32)\n    input_y = torch.ones(1, dtype=torch.float32)\n    output_x, output_y = model(input_x, input_y)\n    assert output_x.dtype == torch.float32\n    assert output_y.dtype == torch.float32\n\n    model.fp16_enabled = True\n    output_x, output_y = model(input_x, input_y)\n    assert output_x.dtype == torch.half\n    assert output_y.dtype == torch.half\n\n    if torch.cuda.is_available():\n        model.cuda()\n        output_x, output_y = model(input_x.cuda(), input_y.cuda())\n        assert output_x.dtype == torch.half\n        assert output_y.dtype == torch.half\n\n    # apply to specified input args\n    class ExampleModule(nn.Module):\n\n        @auto_fp16(apply_to=('x', ))\n        def forward(self, x, y):\n            return x, y\n\n    model = ExampleModule()\n    input_x = torch.ones(1, dtype=torch.float32)\n    input_y = torch.ones(1, dtype=torch.float32)\n    output_x, output_y = model(input_x, input_y)\n    assert output_x.dtype == torch.float32\n    assert output_y.dtype == torch.float32\n\n    model.fp16_enabled = True\n    output_x, output_y = model(input_x, input_y)\n    assert output_x.dtype == torch.half\n    assert output_y.dtype == torch.float32\n\n    if torch.cuda.is_available():\n        model.cuda()\n        output_x, output_y = model(input_x.cuda(), input_y.cuda())\n        assert output_x.dtype == torch.half\n        assert output_y.dtype == torch.float32\n\n    # apply to optional input args\n    class ExampleModule(nn.Module):\n\n        @auto_fp16(apply_to=('x', 'y'))\n        def forward(self, x, y=None, z=None):\n            return x, y, z\n\n    model = ExampleModule()\n    input_x = torch.ones(1, dtype=torch.float32)\n    input_y = torch.ones(1, dtype=torch.float32)\n    input_z = torch.ones(1, dtype=torch.float32)\n    output_x, output_y, output_z = model(input_x, y=input_y, z=input_z)\n    assert output_x.dtype == torch.float32\n    assert output_y.dtype == torch.float32\n    assert output_z.dtype == torch.float32\n\n    model.fp16_enabled = True\n    output_x, output_y, output_z = model(input_x, y=input_y, z=input_z)\n    assert output_x.dtype == torch.half\n    assert output_y.dtype == torch.half\n    assert output_z.dtype == torch.float32\n\n    if torch.cuda.is_available():\n        model.cuda()\n        output_x, output_y, output_z = model(\n            input_x.cuda(), y=input_y.cuda(), z=input_z.cuda())\n        assert output_x.dtype == torch.half\n        assert output_y.dtype == torch.half\n        assert output_z.dtype == torch.float32\n\n    # out_fp32=True\n    class ExampleModule(nn.Module):\n\n        @auto_fp16(apply_to=('x', 'y'), out_fp32=True)\n        def forward(self, x, y=None, z=None):\n            return x, y, z\n\n    model = ExampleModule()\n    input_x = torch.ones(1, dtype=torch.half)\n    input_y = torch.ones(1, dtype=torch.float32)\n    input_z = torch.ones(1, dtype=torch.float32)\n    output_x, output_y, output_z = model(input_x, y=input_y, z=input_z)\n    assert output_x.dtype == torch.half\n    assert output_y.dtype == torch.float32\n    assert output_z.dtype == torch.float32\n\n    model.fp16_enabled = True\n    output_x, output_y, output_z = model(input_x, y=input_y, z=input_z)\n    assert output_x.dtype == torch.float32\n    assert output_y.dtype == torch.float32\n    assert output_z.dtype == torch.float32\n\n    if torch.cuda.is_available():\n        model.cuda()\n        output_x, output_y, output_z = model(\n            input_x.cuda(), y=input_y.cuda(), z=input_z.cuda())\n        assert output_x.dtype == torch.float32\n        assert output_y.dtype == torch.float32\n        assert output_z.dtype == torch.float32\n\n\ndef test_force_fp32():\n\n    with pytest.raises(TypeError):\n        # ExampleObject is not a subclass of nn.Module\n\n        class ExampleObject(object):\n\n            @force_fp32()\n            def __call__(self, x):\n                return x\n\n        model = ExampleObject()\n        input_x = torch.ones(1, dtype=torch.float32)\n        model(input_x)\n\n    # apply to all input args\n    class ExampleModule(nn.Module):\n\n        @force_fp32()\n        def forward(self, x, y):\n            return x, y\n\n    model = ExampleModule()\n    input_x = torch.ones(1, dtype=torch.half)\n    input_y = torch.ones(1, dtype=torch.half)\n    output_x, output_y = model(input_x, input_y)\n    assert output_x.dtype == torch.half\n    assert output_y.dtype == torch.half\n\n    model.fp16_enabled = True\n    output_x, output_y = model(input_x, input_y)\n    assert output_x.dtype == torch.float32\n    assert output_y.dtype == torch.float32\n\n    if torch.cuda.is_available():\n        model.cuda()\n        output_x, output_y = model(input_x.cuda(), input_y.cuda())\n        assert output_x.dtype == torch.float32\n        assert output_y.dtype == torch.float32\n\n    # apply to specified input args\n    class ExampleModule(nn.Module):\n\n        @force_fp32(apply_to=('x', ))\n        def forward(self, x, y):\n            return x, y\n\n    model = ExampleModule()\n    input_x = torch.ones(1, dtype=torch.half)\n    input_y = torch.ones(1, dtype=torch.half)\n    output_x, output_y = model(input_x, input_y)\n    assert output_x.dtype == torch.half\n    assert output_y.dtype == torch.half\n\n    model.fp16_enabled = True\n    output_x, output_y = model(input_x, input_y)\n    assert output_x.dtype == torch.float32\n    assert output_y.dtype == torch.half\n\n    if torch.cuda.is_available():\n        model.cuda()\n        output_x, output_y = model(input_x.cuda(), input_y.cuda())\n        assert output_x.dtype == torch.float32\n        assert output_y.dtype == torch.half\n\n    # apply to optional input args\n    class ExampleModule(nn.Module):\n\n        @force_fp32(apply_to=('x', 'y'))\n        def forward(self, x, y=None, z=None):\n            return x, y, z\n\n    model = ExampleModule()\n    input_x = torch.ones(1, dtype=torch.half)\n    input_y = torch.ones(1, dtype=torch.half)\n    input_z = torch.ones(1, dtype=torch.half)\n    output_x, output_y, output_z = model(input_x, y=input_y, z=input_z)\n    assert output_x.dtype == torch.half\n    assert output_y.dtype == torch.half\n    assert output_z.dtype == torch.half\n\n    model.fp16_enabled = True\n    output_x, output_y, output_z = model(input_x, y=input_y, z=input_z)\n    assert output_x.dtype == torch.float32\n    assert output_y.dtype == torch.float32\n    assert output_z.dtype == torch.half\n\n    if torch.cuda.is_available():\n        model.cuda()\n        output_x, output_y, output_z = model(\n            input_x.cuda(), y=input_y.cuda(), z=input_z.cuda())\n        assert output_x.dtype == torch.float32\n        assert output_y.dtype == torch.float32\n        assert output_z.dtype == torch.half\n\n    # out_fp16=True\n    class ExampleModule(nn.Module):\n\n        @force_fp32(apply_to=('x', 'y'), out_fp16=True)\n        def forward(self, x, y=None, z=None):\n            return x, y, z\n\n    model = ExampleModule()\n    input_x = torch.ones(1, dtype=torch.float32)\n    input_y = torch.ones(1, dtype=torch.half)\n    input_z = torch.ones(1, dtype=torch.half)\n    output_x, output_y, output_z = model(input_x, y=input_y, z=input_z)\n    assert output_x.dtype == torch.float32\n    assert output_y.dtype == torch.half\n    assert output_z.dtype == torch.half\n\n    model.fp16_enabled = True\n    output_x, output_y, output_z = model(input_x, y=input_y, z=input_z)\n    assert output_x.dtype == torch.half\n    assert output_y.dtype == torch.half\n    assert output_z.dtype == torch.half\n\n    if torch.cuda.is_available():\n        model.cuda()\n        output_x, output_y, output_z = model(\n            input_x.cuda(), y=input_y.cuda(), z=input_z.cuda())\n        assert output_x.dtype == torch.half\n        assert output_y.dtype == torch.half\n        assert output_z.dtype == torch.half\n"""
tests/test_heads.py,36,"b'import mmcv\nimport torch\n\nfrom mmdet.core import bbox2roi, build_assigner, build_sampler\nfrom mmdet.models.dense_heads import AnchorHead, FSAFHead, GuidedAnchorHead\nfrom mmdet.models.roi_heads.bbox_heads import BBoxHead\nfrom mmdet.models.roi_heads.mask_heads import FCNMaskHead, MaskIoUHead\n\n\ndef test_anchor_head_loss():\n    """"""\n    Tests anchor head loss when truth is empty and non-empty\n    """"""\n    s = 256\n    img_metas = [{\n        \'img_shape\': (s, s, 3),\n        \'scale_factor\': 1,\n        \'pad_shape\': (s, s, 3)\n    }]\n\n    cfg = mmcv.Config(\n        dict(\n            assigner=dict(\n                type=\'MaxIoUAssigner\',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.3,\n                min_pos_iou=0.3,\n                match_low_quality=True,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type=\'RandomSampler\',\n                num=256,\n                pos_fraction=0.5,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=False),\n            allowed_border=0,\n            pos_weight=-1,\n            debug=False))\n    self = AnchorHead(num_classes=4, in_channels=1, train_cfg=cfg)\n\n    # Anchor head expects a multiple levels of features per image\n    feat = [\n        torch.rand(1, 1, s // (2**(i + 2)), s // (2**(i + 2)))\n        for i in range(len(self.anchor_generator.strides))\n    ]\n    cls_scores, bbox_preds = self.forward(feat)\n\n    # Test that empty ground truth encourages the network to predict background\n    gt_bboxes = [torch.empty((0, 4))]\n    gt_labels = [torch.LongTensor([])]\n\n    gt_bboxes_ignore = None\n    empty_gt_losses = self.loss(cls_scores, bbox_preds, gt_bboxes, gt_labels,\n                                img_metas, gt_bboxes_ignore)\n    # When there is no truth, the cls loss should be nonzero but there should\n    # be no box loss.\n    empty_cls_loss = sum(empty_gt_losses[\'loss_cls\'])\n    empty_box_loss = sum(empty_gt_losses[\'loss_bbox\'])\n    assert empty_cls_loss.item() > 0, \'cls loss should be non-zero\'\n    assert empty_box_loss.item() == 0, (\n        \'there should be no box loss when there are no true boxes\')\n\n    # When truth is non-empty then both cls and box loss should be nonzero for\n    # random inputs\n    gt_bboxes = [\n        torch.Tensor([[23.6667, 23.8757, 238.6326, 151.8874]]),\n    ]\n    gt_labels = [torch.LongTensor([2])]\n    one_gt_losses = self.loss(cls_scores, bbox_preds, gt_bboxes, gt_labels,\n                              img_metas, gt_bboxes_ignore)\n    onegt_cls_loss = sum(one_gt_losses[\'loss_cls\'])\n    onegt_box_loss = sum(one_gt_losses[\'loss_bbox\'])\n    assert onegt_cls_loss.item() > 0, \'cls loss should be non-zero\'\n    assert onegt_box_loss.item() > 0, \'box loss should be non-zero\'\n\n\ndef test_fsaf_head_loss():\n    """"""Tests anchor head loss when truth is empty and non-empty\n    """"""\n    s = 256\n    img_metas = [{\n        \'img_shape\': (s, s, 3),\n        \'scale_factor\': 1,\n        \'pad_shape\': (s, s, 3)\n    }]\n\n    cfg = dict(\n        reg_decoded_bbox=True,\n        anchor_generator=dict(\n            type=\'AnchorGenerator\',\n            octave_base_scale=1,\n            scales_per_octave=1,\n            ratios=[1.0],\n            strides=[8, 16, 32, 64, 128]),\n        bbox_coder=dict(type=\'TBLRBBoxCoder\', normalizer=4.0),\n        loss_cls=dict(\n            type=\'FocalLoss\',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0,\n            reduction=\'none\'),\n        loss_bbox=dict(\n            type=\'IoULoss\', eps=1e-6, loss_weight=1.0, reduction=\'none\'))\n\n    train_cfg = mmcv.Config(\n        dict(\n            assigner=dict(\n                type=\'CenterRegionAssigner\',\n                pos_scale=0.2,\n                neg_scale=0.2,\n                min_pos_iof=0.01),\n            allowed_border=-1,\n            pos_weight=-1,\n            debug=False))\n    head = FSAFHead(num_classes=4, in_channels=1, train_cfg=train_cfg, **cfg)\n    if torch.cuda.is_available():\n        head.cuda()\n        # FSAF head expects a multiple levels of features per image\n        feat = [\n            torch.rand(1, 1, s // (2**(i + 2)), s // (2**(i + 2))).cuda()\n            for i in range(len(head.anchor_generator.strides))\n        ]\n        cls_scores, bbox_preds = head.forward(feat)\n        gt_bboxes_ignore = None\n\n        # When truth is non-empty then both cls and box loss should be nonzero\n        #  for random inputs\n        gt_bboxes = [\n            torch.Tensor([[23.6667, 23.8757, 238.6326, 151.8874]]).cuda(),\n        ]\n        gt_labels = [torch.LongTensor([2]).cuda()]\n        one_gt_losses = head.loss(cls_scores, bbox_preds, gt_bboxes, gt_labels,\n                                  img_metas, gt_bboxes_ignore)\n        onegt_cls_loss = sum(one_gt_losses[\'loss_cls\'])\n        onegt_box_loss = sum(one_gt_losses[\'loss_bbox\'])\n        assert onegt_cls_loss.item() > 0, \'cls loss should be non-zero\'\n        assert onegt_box_loss.item() > 0, \'box loss should be non-zero\'\n\n        # Test that empty ground truth encourages the network to predict bkg\n        gt_bboxes = [torch.empty((0, 4)).cuda()]\n        gt_labels = [torch.LongTensor([]).cuda()]\n\n        empty_gt_losses = head.loss(cls_scores, bbox_preds, gt_bboxes,\n                                    gt_labels, img_metas, gt_bboxes_ignore)\n        # When there is no truth, the cls loss should be nonzero but there\n        # should be no box loss.\n        empty_cls_loss = sum(empty_gt_losses[\'loss_cls\'])\n        empty_box_loss = sum(empty_gt_losses[\'loss_bbox\'])\n        assert empty_cls_loss.item() > 0, \'cls loss should be non-zero\'\n        assert empty_box_loss.item() == 0, (\n            \'there should be no box loss when there are no true boxes\')\n\n\ndef test_ga_anchor_head_loss():\n    """"""\n    Tests anchor head loss when truth is empty and non-empty\n    """"""\n    s = 256\n    img_metas = [{\n        \'img_shape\': (s, s, 3),\n        \'scale_factor\': 1,\n        \'pad_shape\': (s, s, 3)\n    }]\n\n    cfg = mmcv.Config(\n        dict(\n            assigner=dict(\n                type=\'MaxIoUAssigner\',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.3,\n                min_pos_iou=0.3,\n                match_low_quality=True,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type=\'RandomSampler\',\n                num=256,\n                pos_fraction=0.5,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=False),\n            ga_assigner=dict(\n                type=\'ApproxMaxIoUAssigner\',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.3,\n                min_pos_iou=0.3,\n                ignore_iof_thr=-1),\n            ga_sampler=dict(\n                type=\'RandomSampler\',\n                num=256,\n                pos_fraction=0.5,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=False),\n            allowed_border=-1,\n            center_ratio=0.2,\n            ignore_ratio=0.5,\n            pos_weight=-1,\n            debug=False))\n    head = GuidedAnchorHead(num_classes=4, in_channels=4, train_cfg=cfg)\n\n    # Anchor head expects a multiple levels of features per image\n    if torch.cuda.is_available():\n        head.cuda()\n        feat = [\n            torch.rand(1, 4, s // (2**(i + 2)), s // (2**(i + 2))).cuda()\n            for i in range(len(head.approx_anchor_generator.base_anchors))\n        ]\n        cls_scores, bbox_preds, shape_preds, loc_preds = head.forward(feat)\n\n        # Test that empty ground truth encourages the network to predict\n        # background\n        gt_bboxes = [torch.empty((0, 4)).cuda()]\n        gt_labels = [torch.LongTensor([]).cuda()]\n\n        gt_bboxes_ignore = None\n\n        empty_gt_losses = head.loss(cls_scores, bbox_preds, shape_preds,\n                                    loc_preds, gt_bboxes, gt_labels, img_metas,\n                                    gt_bboxes_ignore)\n\n        # When there is no truth, the cls loss should be nonzero but there\n        # should be no box loss.\n        empty_cls_loss = sum(empty_gt_losses[\'loss_cls\'])\n        empty_box_loss = sum(empty_gt_losses[\'loss_bbox\'])\n        assert empty_cls_loss.item() > 0, \'cls loss should be non-zero\'\n        assert empty_box_loss.item() == 0, (\n            \'there should be no box loss when there are no true boxes\')\n\n        # When truth is non-empty then both cls and box loss should be nonzero\n        # for random inputs\n        gt_bboxes = [\n            torch.Tensor([[23.6667, 23.8757, 238.6326, 151.8874]]).cuda(),\n        ]\n        gt_labels = [torch.LongTensor([2]).cuda()]\n        one_gt_losses = head.loss(cls_scores, bbox_preds, shape_preds,\n                                  loc_preds, gt_bboxes, gt_labels, img_metas,\n                                  gt_bboxes_ignore)\n        onegt_cls_loss = sum(one_gt_losses[\'loss_cls\'])\n        onegt_box_loss = sum(one_gt_losses[\'loss_bbox\'])\n        assert onegt_cls_loss.item() > 0, \'cls loss should be non-zero\'\n        assert onegt_box_loss.item() > 0, \'box loss should be non-zero\'\n\n\ndef test_bbox_head_loss():\n    """"""\n    Tests bbox head loss when truth is empty and non-empty\n    """"""\n    self = BBoxHead(in_channels=8, roi_feat_size=3)\n\n    # Dummy proposals\n    proposal_list = [\n        torch.Tensor([[23.6667, 23.8757, 228.6326, 153.8874]]),\n    ]\n\n    target_cfg = mmcv.Config(dict(pos_weight=1))\n\n    # Test bbox loss when truth is empty\n    gt_bboxes = [torch.empty((0, 4))]\n    gt_labels = [torch.LongTensor([])]\n\n    sampling_results = _dummy_bbox_sampling(proposal_list, gt_bboxes,\n                                            gt_labels)\n\n    bbox_targets = self.get_targets(sampling_results, gt_bboxes, gt_labels,\n                                    target_cfg)\n    labels, label_weights, bbox_targets, bbox_weights = bbox_targets\n\n    # Create dummy features ""extracted"" for each sampled bbox\n    num_sampled = sum(len(res.bboxes) for res in sampling_results)\n    rois = bbox2roi([res.bboxes for res in sampling_results])\n    dummy_feats = torch.rand(num_sampled, 8 * 3 * 3)\n    cls_scores, bbox_preds = self.forward(dummy_feats)\n\n    losses = self.loss(cls_scores, bbox_preds, rois, labels, label_weights,\n                       bbox_targets, bbox_weights)\n    assert losses.get(\'loss_cls\', 0) > 0, \'cls-loss should be non-zero\'\n    assert losses.get(\'loss_bbox\', 0) == 0, \'empty gt loss should be zero\'\n\n    # Test bbox loss when truth is non-empty\n    gt_bboxes = [\n        torch.Tensor([[23.6667, 23.8757, 238.6326, 151.8874]]),\n    ]\n    gt_labels = [torch.LongTensor([2])]\n\n    sampling_results = _dummy_bbox_sampling(proposal_list, gt_bboxes,\n                                            gt_labels)\n    rois = bbox2roi([res.bboxes for res in sampling_results])\n\n    bbox_targets = self.get_targets(sampling_results, gt_bboxes, gt_labels,\n                                    target_cfg)\n    labels, label_weights, bbox_targets, bbox_weights = bbox_targets\n\n    # Create dummy features ""extracted"" for each sampled bbox\n    num_sampled = sum(len(res.bboxes) for res in sampling_results)\n    dummy_feats = torch.rand(num_sampled, 8 * 3 * 3)\n    cls_scores, bbox_preds = self.forward(dummy_feats)\n\n    losses = self.loss(cls_scores, bbox_preds, rois, labels, label_weights,\n                       bbox_targets, bbox_weights)\n    assert losses.get(\'loss_cls\', 0) > 0, \'cls-loss should be non-zero\'\n    assert losses.get(\'loss_bbox\', 0) > 0, \'box-loss should be non-zero\'\n\n\ndef test_refine_boxes():\n    """"""\n    Mirrors the doctest in\n    ``mmdet.models.bbox_heads.bbox_head.BBoxHead.refine_boxes`` but checks for\n    multiple values of n_roi / n_img.\n    """"""\n    self = BBoxHead(reg_class_agnostic=True)\n\n    test_settings = [\n\n        # Corner case: less rois than images\n        {\n            \'n_roi\': 2,\n            \'n_img\': 4,\n            \'rng\': 34285940\n        },\n\n        # Corner case: no images\n        {\n            \'n_roi\': 0,\n            \'n_img\': 0,\n            \'rng\': 52925222\n        },\n\n        # Corner cases: few images / rois\n        {\n            \'n_roi\': 1,\n            \'n_img\': 1,\n            \'rng\': 1200281\n        },\n        {\n            \'n_roi\': 2,\n            \'n_img\': 1,\n            \'rng\': 1200282\n        },\n        {\n            \'n_roi\': 2,\n            \'n_img\': 2,\n            \'rng\': 1200283\n        },\n        {\n            \'n_roi\': 1,\n            \'n_img\': 2,\n            \'rng\': 1200284\n        },\n\n        # Corner case: no rois few images\n        {\n            \'n_roi\': 0,\n            \'n_img\': 1,\n            \'rng\': 23955860\n        },\n        {\n            \'n_roi\': 0,\n            \'n_img\': 2,\n            \'rng\': 25830516\n        },\n\n        # Corner case: no rois many images\n        {\n            \'n_roi\': 0,\n            \'n_img\': 10,\n            \'rng\': 671346\n        },\n        {\n            \'n_roi\': 0,\n            \'n_img\': 20,\n            \'rng\': 699807\n        },\n\n        # Corner case: cal_similarity num rois and images\n        {\n            \'n_roi\': 20,\n            \'n_img\': 20,\n            \'rng\': 1200238\n        },\n        {\n            \'n_roi\': 10,\n            \'n_img\': 20,\n            \'rng\': 1200238\n        },\n        {\n            \'n_roi\': 5,\n            \'n_img\': 5,\n            \'rng\': 1200238\n        },\n\n        # ----------------------------------\n        # Common case: more rois than images\n        {\n            \'n_roi\': 100,\n            \'n_img\': 1,\n            \'rng\': 337156\n        },\n        {\n            \'n_roi\': 150,\n            \'n_img\': 2,\n            \'rng\': 275898\n        },\n        {\n            \'n_roi\': 500,\n            \'n_img\': 5,\n            \'rng\': 4903221\n        },\n    ]\n\n    for demokw in test_settings:\n        try:\n            n_roi = demokw[\'n_roi\']\n            n_img = demokw[\'n_img\']\n            rng = demokw[\'rng\']\n\n            print(f\'Test refine_boxes case: {demokw!r}\')\n            tup = _demodata_refine_boxes(n_roi, n_img, rng=rng)\n            rois, labels, bbox_preds, pos_is_gts, img_metas = tup\n            bboxes_list = self.refine_bboxes(rois, labels, bbox_preds,\n                                             pos_is_gts, img_metas)\n            assert len(bboxes_list) == n_img\n            assert sum(map(len, bboxes_list)) <= n_roi\n            assert all(b.shape[1] == 4 for b in bboxes_list)\n        except Exception:\n            print(f\'Test failed with demokw={demokw!r}\')\n            raise\n\n\ndef _demodata_refine_boxes(n_roi, n_img, rng=0):\n    """"""\n    Create random test data for the\n    ``mmdet.models.bbox_heads.bbox_head.BBoxHead.refine_boxes`` method\n    """"""\n    import numpy as np\n    from mmdet.core.bbox.demodata import random_boxes\n    from mmdet.core.bbox.demodata import ensure_rng\n    try:\n        import kwarray\n    except ImportError:\n        import pytest\n        pytest.skip(\'kwarray is required for this test\')\n    scale = 512\n    rng = ensure_rng(rng)\n    img_metas = [{\'img_shape\': (scale, scale)} for _ in range(n_img)]\n    # Create rois in the expected format\n    roi_boxes = random_boxes(n_roi, scale=scale, rng=rng)\n    if n_img == 0:\n        assert n_roi == 0, \'cannot have any rois if there are no images\'\n        img_ids = torch.empty((0, ), dtype=torch.long)\n        roi_boxes = torch.empty((0, 4), dtype=torch.float32)\n    else:\n        img_ids = rng.randint(0, n_img, (n_roi, ))\n        img_ids = torch.from_numpy(img_ids)\n    rois = torch.cat([img_ids[:, None].float(), roi_boxes], dim=1)\n    # Create other args\n    labels = rng.randint(0, 2, (n_roi, ))\n    labels = torch.from_numpy(labels).long()\n    bbox_preds = random_boxes(n_roi, scale=scale, rng=rng)\n    # For each image, pretend random positive boxes are gts\n    is_label_pos = (labels.numpy() > 0).astype(np.int)\n    lbl_per_img = kwarray.group_items(is_label_pos, img_ids.numpy())\n    pos_per_img = [sum(lbl_per_img.get(gid, [])) for gid in range(n_img)]\n    # randomly generate with numpy then sort with torch\n    _pos_is_gts = [\n        rng.randint(0, 2, (npos, )).astype(np.uint8) for npos in pos_per_img\n    ]\n    pos_is_gts = [\n        torch.from_numpy(p).sort(descending=True)[0] for p in _pos_is_gts\n    ]\n    return rois, labels, bbox_preds, pos_is_gts, img_metas\n\n\ndef test_mask_head_loss():\n    """"""\n    Test mask head loss when mask target is empty\n    """"""\n    self = FCNMaskHead(\n        num_convs=1,\n        roi_feat_size=6,\n        in_channels=8,\n        conv_out_channels=8,\n        num_classes=8)\n\n    # Dummy proposals\n    proposal_list = [\n        torch.Tensor([[23.6667, 23.8757, 228.6326, 153.8874]]),\n    ]\n\n    gt_bboxes = [\n        torch.Tensor([[23.6667, 23.8757, 238.6326, 151.8874]]),\n    ]\n    gt_labels = [torch.LongTensor([2])]\n    sampling_results = _dummy_bbox_sampling(proposal_list, gt_bboxes,\n                                            gt_labels)\n\n    # create dummy mask\n    import numpy as np\n    from mmdet.core import BitmapMasks\n    dummy_mask = np.random.randint(0, 2, (1, 160, 240), dtype=np.uint8)\n    gt_masks = [BitmapMasks(dummy_mask, 160, 240)]\n\n    # create dummy train_cfg\n    train_cfg = mmcv.Config(dict(mask_size=12, mask_thr_binary=0.5))\n\n    # Create dummy features ""extracted"" for each sampled bbox\n    num_sampled = sum(len(res.bboxes) for res in sampling_results)\n    dummy_feats = torch.rand(num_sampled, 8, 6, 6)\n\n    mask_pred = self.forward(dummy_feats)\n    mask_targets = self.get_targets(sampling_results, gt_masks, train_cfg)\n    pos_labels = torch.cat([res.pos_gt_labels for res in sampling_results])\n    loss_mask = self.loss(mask_pred, mask_targets, pos_labels)\n\n    onegt_mask_loss = sum(loss_mask[\'loss_mask\'])\n    assert onegt_mask_loss.item() > 0, \'mask loss should be non-zero\'\n\n    # test mask_iou_head\n    mask_iou_head = MaskIoUHead(\n        num_convs=1,\n        num_fcs=1,\n        roi_feat_size=6,\n        in_channels=8,\n        conv_out_channels=8,\n        fc_out_channels=8,\n        num_classes=8)\n\n    pos_mask_pred = mask_pred[range(mask_pred.size(0)), pos_labels]\n    mask_iou_pred = mask_iou_head(dummy_feats, pos_mask_pred)\n    pos_mask_iou_pred = mask_iou_pred[range(mask_iou_pred.size(0)), pos_labels]\n\n    mask_iou_targets = mask_iou_head.get_targets(sampling_results, gt_masks,\n                                                 pos_mask_pred, mask_targets,\n                                                 train_cfg)\n    loss_mask_iou = mask_iou_head.loss(pos_mask_iou_pred, mask_iou_targets)\n    onegt_mask_iou_loss = loss_mask_iou[\'loss_mask_iou\'].sum()\n    assert onegt_mask_iou_loss.item() >= 0\n\n\ndef _dummy_bbox_sampling(proposal_list, gt_bboxes, gt_labels):\n    """"""\n    Create sample results that can be passed to BBoxHead.get_targets\n    """"""\n    num_imgs = 1\n    feat = torch.rand(1, 1, 3, 3)\n    assign_config = dict(\n        type=\'MaxIoUAssigner\',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n        min_pos_iou=0.5,\n        ignore_iof_thr=-1)\n    sampler_config = dict(\n        type=\'RandomSampler\',\n        num=512,\n        pos_fraction=0.25,\n        neg_pos_ub=-1,\n        add_gt_as_proposals=True)\n    bbox_assigner = build_assigner(assign_config)\n    bbox_sampler = build_sampler(sampler_config)\n    gt_bboxes_ignore = [None for _ in range(num_imgs)]\n    sampling_results = []\n    for i in range(num_imgs):\n        assign_result = bbox_assigner.assign(proposal_list[i], gt_bboxes[i],\n                                             gt_bboxes_ignore[i], gt_labels[i])\n        sampling_result = bbox_sampler.sample(\n            assign_result,\n            proposal_list[i],\n            gt_bboxes[i],\n            gt_labels[i],\n            feats=feat)\n        sampling_results.append(sampling_result)\n\n    return sampling_results\n'"
tests/test_losses.py,4,"b""import pytest\nimport torch\n\n\ndef test_ce_loss():\n    from mmdet.models import build_loss\n\n    # use_mask and use_sigmoid cannot be true at the same time\n    with pytest.raises(AssertionError):\n        loss_cfg = dict(\n            type='CrossEntropyLoss',\n            use_mask=True,\n            use_sigmoid=True,\n            loss_weight=1.0)\n        build_loss(loss_cfg)\n\n    # test loss with class weights\n    loss_cls_cfg = dict(\n        type='CrossEntropyLoss',\n        use_sigmoid=False,\n        class_weight=[0.8, 0.2],\n        loss_weight=1.0)\n    loss_cls = build_loss(loss_cls_cfg)\n    fake_pred = torch.Tensor([[100, -100]])\n    fake_label = torch.Tensor([1]).long()\n    assert torch.allclose(loss_cls(fake_pred, fake_label), torch.tensor(40.))\n\n    loss_cls_cfg = dict(\n        type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0)\n    loss_cls = build_loss(loss_cls_cfg)\n    assert torch.allclose(loss_cls(fake_pred, fake_label), torch.tensor(200.))\n"""
tests/test_masks.py,9,"b'import numpy as np\nimport pytest\nimport torch\n\nfrom mmdet.core import BitmapMasks, PolygonMasks\n\n\ndef dummy_raw_bitmap_masks(size):\n    """"""\n    Args:\n        size (tuple): expected shape of dummy masks, (H, W) or (N, H, W)\n\n    Return:\n        ndarray: dummy mask\n    """"""\n    return np.random.randint(0, 2, size, dtype=np.uint8)\n\n\ndef dummy_raw_polygon_masks(size):\n    """"""\n    Args:\n        size (tuple): expected shape of dummy masks, (N, H, W)\n\n    Return:\n        list[list[ndarray]]: dummy mask\n    """"""\n    num_obj, heigt, width = size\n    polygons = []\n    for _ in range(num_obj):\n        num_points = np.random.randint(5) * 2 + 6\n        polygons.append([np.random.uniform(0, min(heigt, width), num_points)])\n    return polygons\n\n\ndef dummy_bboxes(num, max_height, max_width):\n    x1y1 = np.random.randint(0, min(max_height // 2, max_width // 2), (num, 2))\n    wh = np.random.randint(0, min(max_height // 2, max_width // 2), (num, 2))\n    x2y2 = x1y1 + wh\n    return np.concatenate([x1y1, x2y2], axis=1).squeeze().astype(np.float32)\n\n\ndef test_bitmap_mask_init():\n    # init with empty ndarray masks\n    raw_masks = np.empty((0, 28, 28), dtype=np.uint8)\n    bitmap_masks = BitmapMasks(raw_masks, 28, 28)\n    assert len(bitmap_masks) == 0\n    assert bitmap_masks.height == 28\n    assert bitmap_masks.width == 28\n\n    # init with empty list masks\n    raw_masks = []\n    bitmap_masks = BitmapMasks(raw_masks, 28, 28)\n    assert len(bitmap_masks) == 0\n    assert bitmap_masks.height == 28\n    assert bitmap_masks.width == 28\n\n    # init with ndarray masks contain 3 instances\n    raw_masks = dummy_raw_bitmap_masks((3, 28, 28))\n    bitmap_masks = BitmapMasks(raw_masks, 28, 28)\n    assert len(bitmap_masks) == 3\n    assert bitmap_masks.height == 28\n    assert bitmap_masks.width == 28\n\n    # init with list masks contain 3 instances\n    raw_masks = [dummy_raw_bitmap_masks((28, 28)) for _ in range(3)]\n    bitmap_masks = BitmapMasks(raw_masks, 28, 28)\n    assert len(bitmap_masks) == 3\n    assert bitmap_masks.height == 28\n    assert bitmap_masks.width == 28\n\n    # init with raw masks of unsupported type\n    with pytest.raises(AssertionError):\n        raw_masks = [[dummy_raw_bitmap_masks((28, 28))]]\n        BitmapMasks(raw_masks, 28, 28)\n\n\ndef test_bitmap_mask_rescale():\n    # rescale with empty bitmap masks\n    raw_masks = dummy_raw_bitmap_masks((0, 28, 28))\n    bitmap_masks = BitmapMasks(raw_masks, 28, 28)\n    rescaled_masks = bitmap_masks.rescale((56, 72))\n    assert len(rescaled_masks) == 0\n    assert rescaled_masks.height == 56\n    assert rescaled_masks.width == 56\n\n    # rescale with bitmap masks contain 1 instances\n    raw_masks = np.array([[[1, 0, 0, 0], [0, 1, 0, 1]]])\n    bitmap_masks = BitmapMasks(raw_masks, 2, 4)\n    rescaled_masks = bitmap_masks.rescale((8, 8))\n    assert len(rescaled_masks) == 1\n    assert rescaled_masks.height == 4\n    assert rescaled_masks.width == 8\n    truth = np.array([[[1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0],\n                       [0, 0, 1, 1, 0, 0, 1, 1], [0, 0, 1, 1, 0, 0, 1, 1]]])\n    assert (rescaled_masks.masks == truth).all()\n\n\ndef test_bitmap_mask_resize():\n    # resize with empty bitmap masks\n    raw_masks = dummy_raw_bitmap_masks((0, 28, 28))\n    bitmap_masks = BitmapMasks(raw_masks, 28, 28)\n    resized_masks = bitmap_masks.resize((56, 72))\n    assert len(resized_masks) == 0\n    assert resized_masks.height == 56\n    assert resized_masks.width == 72\n\n    # resize with bitmap masks contain 1 instances\n    raw_masks = np.diag(np.ones(4, dtype=np.uint8))[np.newaxis, ...]\n    bitmap_masks = BitmapMasks(raw_masks, 4, 4)\n    resized_masks = bitmap_masks.resize((8, 8))\n    assert len(resized_masks) == 1\n    assert resized_masks.height == 8\n    assert resized_masks.width == 8\n    truth = np.array([[[1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0],\n                       [0, 0, 1, 1, 0, 0, 0, 0], [0, 0, 1, 1, 0, 0, 0, 0],\n                       [0, 0, 0, 0, 1, 1, 0, 0], [0, 0, 0, 0, 1, 1, 0, 0],\n                       [0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 1, 1]]])\n    assert (resized_masks.masks == truth).all()\n\n\ndef test_bitmap_mask_flip():\n    # flip with empty bitmap masks\n    raw_masks = dummy_raw_bitmap_masks((0, 28, 28))\n    bitmap_masks = BitmapMasks(raw_masks, 28, 28)\n    flipped_masks = bitmap_masks.flip(flip_direction=\'horizontal\')\n    assert len(flipped_masks) == 0\n    assert flipped_masks.height == 28\n    assert flipped_masks.width == 28\n\n    # horizontally flip with bitmap masks contain 3 instances\n    raw_masks = dummy_raw_bitmap_masks((3, 28, 28))\n    bitmap_masks = BitmapMasks(raw_masks, 28, 28)\n    flipped_masks = bitmap_masks.flip(flip_direction=\'horizontal\')\n    flipped_flipped_masks = flipped_masks.flip(flip_direction=\'horizontal\')\n    assert flipped_masks.masks.shape == (3, 28, 28)\n    assert (bitmap_masks.masks == flipped_flipped_masks.masks).all()\n    assert (flipped_masks.masks == raw_masks[:, :, ::-1]).all()\n\n    # vertically flip with bitmap masks contain 3 instances\n    raw_masks = dummy_raw_bitmap_masks((3, 28, 28))\n    bitmap_masks = BitmapMasks(raw_masks, 28, 28)\n    flipped_masks = bitmap_masks.flip(flip_direction=\'vertical\')\n    flipped_flipped_masks = flipped_masks.flip(flip_direction=\'vertical\')\n    assert len(flipped_masks) == 3\n    assert flipped_masks.height == 28\n    assert flipped_masks.width == 28\n    assert (bitmap_masks.masks == flipped_flipped_masks.masks).all()\n    assert (flipped_masks.masks == raw_masks[:, ::-1, :]).all()\n\n\ndef test_bitmap_mask_pad():\n    # pad with empty bitmap masks\n    raw_masks = dummy_raw_bitmap_masks((0, 28, 28))\n    bitmap_masks = BitmapMasks(raw_masks, 28, 28)\n    padded_masks = bitmap_masks.pad((56, 56))\n    assert len(padded_masks) == 0\n    assert padded_masks.height == 56\n    assert padded_masks.width == 56\n\n    # pad with bitmap masks contain 3 instances\n    raw_masks = dummy_raw_bitmap_masks((3, 28, 28))\n    bitmap_masks = BitmapMasks(raw_masks, 28, 28)\n    padded_masks = bitmap_masks.pad((56, 56))\n    assert len(padded_masks) == 3\n    assert padded_masks.height == 56\n    assert padded_masks.width == 56\n    assert (padded_masks.masks[:, 28:, 28:] == 0).all()\n\n\ndef test_bitmap_mask_crop():\n    # crop with empty bitmap masks\n    dummy_bbox = np.array([0, 10, 10, 27], dtype=np.int)\n    raw_masks = dummy_raw_bitmap_masks((0, 28, 28))\n    bitmap_masks = BitmapMasks(raw_masks, 28, 28)\n    cropped_masks = bitmap_masks.crop(dummy_bbox)\n    assert len(cropped_masks) == 0\n    assert cropped_masks.height == 17\n    assert cropped_masks.width == 10\n\n    # crop with bitmap masks contain 3 instances\n    raw_masks = dummy_raw_bitmap_masks((3, 28, 28))\n    bitmap_masks = BitmapMasks(raw_masks, 28, 28)\n    cropped_masks = bitmap_masks.crop(dummy_bbox)\n    assert len(cropped_masks) == 3\n    assert cropped_masks.height == 17\n    assert cropped_masks.width == 10\n    x1, y1, x2, y2 = dummy_bbox\n    assert (cropped_masks.masks == raw_masks[:, y1:y2, x1:x2]).all()\n\n    # crop with invalid bbox\n    with pytest.raises(AssertionError):\n        dummy_bbox = dummy_bboxes(2, 28, 28)\n        bitmap_masks.crop(dummy_bbox)\n\n\ndef test_bitmap_mask_crop_and_resize():\n    dummy_bbox = dummy_bboxes(5, 28, 28)\n    inds = np.random.randint(0, 3, (5, ))\n\n    # crop and resize with empty bitmap masks\n    raw_masks = dummy_raw_bitmap_masks((0, 28, 28))\n    bitmap_masks = BitmapMasks(raw_masks, 28, 28)\n    cropped_resized_masks = bitmap_masks.crop_and_resize(\n        dummy_bbox, (56, 56), inds)\n    assert len(cropped_resized_masks) == 0\n    assert cropped_resized_masks.height == 56\n    assert cropped_resized_masks.width == 56\n\n    # crop and resize with bitmap masks contain 3 instances\n    raw_masks = dummy_raw_bitmap_masks((3, 28, 28))\n    bitmap_masks = BitmapMasks(raw_masks, 28, 28)\n    cropped_resized_masks = bitmap_masks.crop_and_resize(\n        dummy_bbox, (56, 56), inds)\n    assert len(cropped_resized_masks) == 5\n    assert cropped_resized_masks.height == 56\n    assert cropped_resized_masks.width == 56\n\n\ndef test_bitmap_mask_expand():\n    # expand with empty bitmap masks\n    raw_masks = dummy_raw_bitmap_masks((0, 28, 28))\n    bitmap_masks = BitmapMasks(raw_masks, 28, 28)\n    expanded_masks = bitmap_masks.expand(56, 56, 12, 14)\n    assert len(expanded_masks) == 0\n    assert expanded_masks.height == 56\n    assert expanded_masks.width == 56\n\n    # expand with bitmap masks contain 3 instances\n    raw_masks = dummy_raw_bitmap_masks((3, 28, 28))\n    bitmap_masks = BitmapMasks(raw_masks, 28, 28)\n    expanded_masks = bitmap_masks.expand(56, 56, 12, 14)\n    assert len(expanded_masks) == 3\n    assert expanded_masks.height == 56\n    assert expanded_masks.width == 56\n    assert (expanded_masks.masks[:, :12, :14] == 0).all()\n    assert (expanded_masks.masks[:, 12 + 28:, 14 + 28:] == 0).all()\n\n\ndef test_bitmap_mask_area():\n    # area of empty bitmap mask\n    raw_masks = dummy_raw_bitmap_masks((0, 28, 28))\n    bitmap_masks = BitmapMasks(raw_masks, 28, 28)\n    assert bitmap_masks.areas.sum() == 0\n\n    # area of bitmap masks contain 3 instances\n    raw_masks = dummy_raw_bitmap_masks((3, 28, 28))\n    bitmap_masks = BitmapMasks(raw_masks, 28, 28)\n    areas = bitmap_masks.areas\n    assert len(areas) == 3\n    assert (areas == raw_masks.sum((1, 2))).all()\n\n\ndef test_bitmap_mask_to_ndarray():\n    # empty bitmap masks to ndarray\n    raw_masks = dummy_raw_bitmap_masks((0, 28, 28))\n    bitmap_masks = BitmapMasks(raw_masks, 28, 28)\n    ndarray_masks = bitmap_masks.to_ndarray()\n    assert isinstance(ndarray_masks, np.ndarray)\n    assert ndarray_masks.shape == (0, 28, 28)\n\n    # bitmap masks contain 3 instances to ndarray\n    raw_masks = dummy_raw_bitmap_masks((3, 28, 28))\n    bitmap_masks = BitmapMasks(raw_masks, 28, 28)\n    ndarray_masks = bitmap_masks.to_ndarray()\n    assert isinstance(ndarray_masks, np.ndarray)\n    assert ndarray_masks.shape == (3, 28, 28)\n    assert (ndarray_masks == raw_masks).all()\n\n\ndef test_bitmap_mask_to_tensor():\n    # empty bitmap masks to tensor\n    raw_masks = dummy_raw_bitmap_masks((0, 28, 28))\n    bitmap_masks = BitmapMasks(raw_masks, 28, 28)\n    tensor_masks = bitmap_masks.to_tensor(dtype=torch.uint8, device=\'cpu\')\n    assert isinstance(tensor_masks, torch.Tensor)\n    assert tensor_masks.shape == (0, 28, 28)\n\n    # bitmap masks contain 3 instances to tensor\n    raw_masks = dummy_raw_bitmap_masks((3, 28, 28))\n    bitmap_masks = BitmapMasks(raw_masks, 28, 28)\n    tensor_masks = bitmap_masks.to_tensor(dtype=torch.uint8, device=\'cpu\')\n    assert isinstance(tensor_masks, torch.Tensor)\n    assert tensor_masks.shape == (3, 28, 28)\n    assert (tensor_masks.numpy() == raw_masks).all()\n\n\ndef test_bitmap_mask_index():\n    raw_masks = dummy_raw_bitmap_masks((3, 28, 28))\n    bitmap_masks = BitmapMasks(raw_masks, 28, 28)\n    assert (bitmap_masks[0].masks == raw_masks[0]).all()\n    assert (bitmap_masks[range(2)].masks == raw_masks[range(2)]).all()\n\n\ndef test_bitmap_mask_iter():\n    raw_masks = dummy_raw_bitmap_masks((3, 28, 28))\n    bitmap_masks = BitmapMasks(raw_masks, 28, 28)\n    for i, bitmap_mask in enumerate(bitmap_masks):\n        assert bitmap_mask.shape == (28, 28)\n        assert (bitmap_mask == raw_masks[i]).all()\n\n\ndef test_polygon_mask_init():\n    # init with empty masks\n    raw_masks = []\n    polygon_masks = BitmapMasks(raw_masks, 28, 28)\n    assert len(polygon_masks) == 0\n    assert polygon_masks.height == 28\n    assert polygon_masks.width == 28\n\n    # init with masks contain 3 instances\n    raw_masks = dummy_raw_polygon_masks((3, 28, 28))\n    polygon_masks = PolygonMasks(raw_masks, 28, 28)\n    assert isinstance(polygon_masks.masks, list)\n    assert isinstance(polygon_masks.masks[0], list)\n    assert isinstance(polygon_masks.masks[0][0], np.ndarray)\n    assert len(polygon_masks) == 3\n    assert polygon_masks.height == 28\n    assert polygon_masks.width == 28\n    assert polygon_masks.to_ndarray().shape == (3, 28, 28)\n\n    # init with raw masks of unsupported type\n    with pytest.raises(AssertionError):\n        raw_masks = [[[]]]\n        PolygonMasks(raw_masks, 28, 28)\n\n        raw_masks = [dummy_raw_polygon_masks((3, 28, 28))]\n        PolygonMasks(raw_masks, 28, 28)\n\n\ndef test_polygon_mask_rescale():\n    # rescale with empty polygon masks\n    raw_masks = dummy_raw_polygon_masks((0, 28, 28))\n    polygon_masks = PolygonMasks(raw_masks, 28, 28)\n    rescaled_masks = polygon_masks.rescale((56, 72))\n    assert len(rescaled_masks) == 0\n    assert rescaled_masks.height == 56\n    assert rescaled_masks.width == 56\n    assert rescaled_masks.to_ndarray().shape == (0, 56, 56)\n\n    # rescale with polygon masks contain 3 instances\n    raw_masks = [[np.array([1, 1, 3, 1, 4, 3, 2, 4, 1, 3], dtype=np.float)]]\n    polygon_masks = PolygonMasks(raw_masks, 5, 5)\n    rescaled_masks = polygon_masks.rescale((12, 10))\n    assert len(rescaled_masks) == 1\n    assert rescaled_masks.height == 10\n    assert rescaled_masks.width == 10\n    assert rescaled_masks.to_ndarray().shape == (1, 10, 10)\n    truth = np.array(\n        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n         [0, 0, 1, 1, 1, 1, 0, 0, 0, 0], [0, 0, 1, 1, 1, 1, 1, 0, 0, 0],\n         [0, 0, 1, 1, 1, 1, 1, 0, 0, 0], [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n         [0, 0, 0, 1, 1, 1, 1, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n        np.uint8)\n    assert (rescaled_masks.to_ndarray() == truth).all()\n\n\ndef test_polygon_mask_resize():\n    # resize with empty polygon masks\n    raw_masks = dummy_raw_polygon_masks((0, 28, 28))\n    polygon_masks = PolygonMasks(raw_masks, 28, 28)\n    resized_masks = polygon_masks.resize((56, 72))\n    assert len(resized_masks) == 0\n    assert resized_masks.height == 56\n    assert resized_masks.width == 72\n    assert resized_masks.to_ndarray().shape == (0, 56, 72)\n\n    # resize with polygon masks contain 1 instance 1 part\n    raw_masks1 = [[np.array([1, 1, 3, 1, 4, 3, 2, 4, 1, 3], dtype=np.float)]]\n    polygon_masks1 = PolygonMasks(raw_masks1, 5, 5)\n    resized_masks1 = polygon_masks1.resize((10, 10))\n    assert len(resized_masks1) == 1\n    assert resized_masks1.height == 10\n    assert resized_masks1.width == 10\n    assert resized_masks1.to_ndarray().shape == (1, 10, 10)\n    truth1 = np.array(\n        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n         [0, 0, 1, 1, 1, 1, 0, 0, 0, 0], [0, 0, 1, 1, 1, 1, 1, 0, 0, 0],\n         [0, 0, 1, 1, 1, 1, 1, 0, 0, 0], [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n         [0, 0, 0, 1, 1, 1, 1, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n        np.uint8)\n    assert (resized_masks1.to_ndarray() == truth1).all()\n\n    # resize with polygon masks contain 1 instance 2 part\n    raw_masks2 = [[\n        np.array([0., 0., 1., 0., 1., 1.]),\n        np.array([1., 1., 2., 1., 2., 2., 1., 2.])\n    ]]\n    polygon_masks2 = PolygonMasks(raw_masks2, 3, 3)\n    resized_masks2 = polygon_masks2.resize((6, 6))\n    assert len(resized_masks2) == 1\n    assert resized_masks2.height == 6\n    assert resized_masks2.width == 6\n    assert resized_masks2.to_ndarray().shape == (1, 6, 6)\n    truth2 = np.array(\n        [[0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 0, 0],\n         [0, 0, 1, 1, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]], np.uint8)\n    assert (resized_masks2.to_ndarray() == truth2).all()\n\n    # resize with polygon masks contain 2 instances\n    raw_masks3 = [raw_masks1[0], raw_masks2[0]]\n    polygon_masks3 = PolygonMasks(raw_masks3, 5, 5)\n    resized_masks3 = polygon_masks3.resize((10, 10))\n    assert len(resized_masks3) == 2\n    assert resized_masks3.height == 10\n    assert resized_masks3.width == 10\n    assert resized_masks3.to_ndarray().shape == (2, 10, 10)\n    truth3 = np.stack([truth1, np.pad(truth2, ((0, 4), (0, 4)), \'constant\')])\n    assert (resized_masks3.to_ndarray() == truth3).all()\n\n\ndef test_polygon_mask_flip():\n    # flip with empty polygon masks\n    raw_masks = dummy_raw_polygon_masks((0, 28, 28))\n    polygon_masks = PolygonMasks(raw_masks, 28, 28)\n    flipped_masks = polygon_masks.flip(flip_direction=\'horizontal\')\n    assert len(flipped_masks) == 0\n    assert flipped_masks.height == 28\n    assert flipped_masks.width == 28\n    assert flipped_masks.to_ndarray().shape == (0, 28, 28)\n\n    # TODO: fixed flip correctness checking after v2.0_coord is merged\n    # horizontally flip with polygon masks contain 3 instances\n    raw_masks = dummy_raw_polygon_masks((3, 28, 28))\n    polygon_masks = PolygonMasks(raw_masks, 28, 28)\n    flipped_masks = polygon_masks.flip(flip_direction=\'horizontal\')\n    flipped_flipped_masks = flipped_masks.flip(flip_direction=\'horizontal\')\n    assert len(flipped_masks) == 3\n    assert flipped_masks.height == 28\n    assert flipped_masks.width == 28\n    assert flipped_masks.to_ndarray().shape == (3, 28, 28)\n    assert (polygon_masks.to_ndarray() == flipped_flipped_masks.to_ndarray()\n            ).all()\n\n    # vertically flip with polygon masks contain 3 instances\n    raw_masks = dummy_raw_polygon_masks((3, 28, 28))\n    polygon_masks = PolygonMasks(raw_masks, 28, 28)\n    flipped_masks = polygon_masks.flip(flip_direction=\'vertical\')\n    flipped_flipped_masks = flipped_masks.flip(flip_direction=\'vertical\')\n    assert len(flipped_masks) == 3\n    assert flipped_masks.height == 28\n    assert flipped_masks.width == 28\n    assert flipped_masks.to_ndarray().shape == (3, 28, 28)\n    assert (polygon_masks.to_ndarray() == flipped_flipped_masks.to_ndarray()\n            ).all()\n\n\ndef test_polygon_mask_crop():\n    dummy_bbox = np.array([0, 10, 10, 27], dtype=np.int)\n    # crop with empty polygon masks\n    raw_masks = dummy_raw_polygon_masks((0, 28, 28))\n    polygon_masks = PolygonMasks(raw_masks, 28, 28)\n    cropped_masks = polygon_masks.crop(dummy_bbox)\n    assert len(cropped_masks) == 0\n    assert cropped_masks.height == 17\n    assert cropped_masks.width == 10\n    assert cropped_masks.to_ndarray().shape == (0, 17, 10)\n\n    # crop with polygon masks contain 1 instances\n    raw_masks = [[np.array([1., 3., 5., 1., 5., 6., 1, 6])]]\n    polygon_masks = PolygonMasks(raw_masks, 7, 7)\n    bbox = np.array([0, 0, 3, 4])\n    cropped_masks = polygon_masks.crop(bbox)\n    assert len(cropped_masks) == 1\n    assert cropped_masks.height == 4\n    assert cropped_masks.width == 3\n    assert cropped_masks.to_ndarray().shape == (1, 4, 3)\n    truth = np.array([[0, 0, 0], [0, 0, 0], [0, 0, 1], [0, 1, 1]])\n    assert (cropped_masks.to_ndarray() == truth).all()\n\n    # crop with invalid bbox\n    with pytest.raises(AssertionError):\n        dummy_bbox = dummy_bboxes(2, 28, 28)\n        polygon_masks.crop(dummy_bbox)\n\n\ndef test_polygon_mask_pad():\n    # pad with empty polygon masks\n    raw_masks = dummy_raw_polygon_masks((0, 28, 28))\n    polygon_masks = PolygonMasks(raw_masks, 28, 28)\n    padded_masks = polygon_masks.pad((56, 56))\n    assert len(padded_masks) == 0\n    assert padded_masks.height == 56\n    assert padded_masks.width == 56\n    assert padded_masks.to_ndarray().shape == (0, 56, 56)\n\n    # pad with polygon masks contain 3 instances\n    raw_masks = dummy_raw_polygon_masks((3, 28, 28))\n    polygon_masks = PolygonMasks(raw_masks, 28, 28)\n    padded_masks = polygon_masks.pad((56, 56))\n    assert len(padded_masks) == 3\n    assert padded_masks.height == 56\n    assert padded_masks.width == 56\n    assert padded_masks.to_ndarray().shape == (3, 56, 56)\n    assert (padded_masks.to_ndarray()[:, 28:, 28:] == 0).all()\n\n\ndef test_polygon_mask_expand():\n    with pytest.raises(NotImplementedError):\n        raw_masks = dummy_raw_polygon_masks((0, 28, 28))\n        polygon_masks = PolygonMasks(raw_masks, 28, 28)\n        polygon_masks.expand(56, 56, 10, 17)\n\n\ndef test_polygon_mask_crop_and_resize():\n    dummy_bbox = dummy_bboxes(5, 28, 28)\n    inds = np.random.randint(0, 3, (5, ))\n\n    # crop and resize with empty polygon masks\n    raw_masks = dummy_raw_polygon_masks((0, 28, 28))\n    polygon_masks = PolygonMasks(raw_masks, 28, 28)\n    cropped_resized_masks = polygon_masks.crop_and_resize(\n        dummy_bbox, (56, 56), inds)\n    assert len(cropped_resized_masks) == 0\n    assert cropped_resized_masks.height == 56\n    assert cropped_resized_masks.width == 56\n    assert cropped_resized_masks.to_ndarray().shape == (0, 56, 56)\n\n    # crop and resize with polygon masks contain 3 instances\n    raw_masks = dummy_raw_polygon_masks((3, 28, 28))\n    polygon_masks = PolygonMasks(raw_masks, 28, 28)\n    cropped_resized_masks = polygon_masks.crop_and_resize(\n        dummy_bbox, (56, 56), inds)\n    assert len(cropped_resized_masks) == 5\n    assert cropped_resized_masks.height == 56\n    assert cropped_resized_masks.width == 56\n    assert cropped_resized_masks.to_ndarray().shape == (5, 56, 56)\n\n\ndef test_polygon_mask_area():\n    # area of empty polygon masks\n    raw_masks = dummy_raw_polygon_masks((0, 28, 28))\n    polygon_masks = PolygonMasks(raw_masks, 28, 28)\n    assert polygon_masks.areas.sum() == 0\n\n    # area of polygon masks contain 1 instance\n    # here we hack a case that the gap between the area of bitmap and polygon\n    # is minor\n    raw_masks = [[np.array([1, 1, 5, 1, 3, 4])]]\n    polygon_masks = PolygonMasks(raw_masks, 6, 6)\n    polygon_area = polygon_masks.areas\n    bitmap_area = polygon_masks.to_bitmap().areas\n    assert len(polygon_area) == 1\n    assert np.isclose(polygon_area, bitmap_area).all()\n\n\ndef test_polygon_mask_to_bitmap():\n    # polygon masks contain 3 instances to bitmap\n    raw_masks = dummy_raw_polygon_masks((3, 28, 28))\n    polygon_masks = PolygonMasks(raw_masks, 28, 28)\n    bitmap_masks = polygon_masks.to_bitmap()\n    assert (polygon_masks.to_ndarray() == bitmap_masks.to_ndarray()).all()\n\n\ndef test_polygon_mask_to_ndarray():\n    # empty polygon masks to ndarray\n    raw_masks = dummy_raw_polygon_masks((0, 28, 28))\n    polygon_masks = PolygonMasks(raw_masks, 28, 28)\n    ndarray_masks = polygon_masks.to_ndarray()\n    assert isinstance(ndarray_masks, np.ndarray)\n    assert ndarray_masks.shape == (0, 28, 28)\n\n    # polygon masks contain 3 instances to ndarray\n    raw_masks = dummy_raw_polygon_masks((3, 28, 28))\n    polygon_masks = PolygonMasks(raw_masks, 28, 28)\n    ndarray_masks = polygon_masks.to_ndarray()\n    assert isinstance(ndarray_masks, np.ndarray)\n    assert ndarray_masks.shape == (3, 28, 28)\n\n\ndef test_polygon_to_tensor():\n    # empty polygon masks to tensor\n    raw_masks = dummy_raw_polygon_masks((0, 28, 28))\n    polygon_masks = PolygonMasks(raw_masks, 28, 28)\n    tensor_masks = polygon_masks.to_tensor(dtype=torch.uint8, device=\'cpu\')\n    assert isinstance(tensor_masks, torch.Tensor)\n    assert tensor_masks.shape == (0, 28, 28)\n\n    # polygon masks contain 3 instances to tensor\n    raw_masks = dummy_raw_polygon_masks((3, 28, 28))\n    polygon_masks = PolygonMasks(raw_masks, 28, 28)\n    tensor_masks = polygon_masks.to_tensor(dtype=torch.uint8, device=\'cpu\')\n    assert isinstance(tensor_masks, torch.Tensor)\n    assert tensor_masks.shape == (3, 28, 28)\n    assert (tensor_masks.numpy() == polygon_masks.to_ndarray()).all()\n\n\ndef test_polygon_mask_index():\n    raw_masks = dummy_raw_polygon_masks((3, 28, 28))\n    polygon_masks = PolygonMasks(raw_masks, 28, 28)\n    # index by integer\n    polygon_masks[0]\n    # index by list\n    polygon_masks[[0, 1]]\n    # index by ndarray\n    polygon_masks[np.asarray([0, 1])]\n    with pytest.raises(ValueError):\n        # invalid index\n        polygon_masks[torch.Tensor([1, 2])]\n\n\ndef test_polygon_mask_iter():\n    raw_masks = dummy_raw_polygon_masks((3, 28, 28))\n    polygon_masks = PolygonMasks(raw_masks, 28, 28)\n    for i, polygon_mask in enumerate(polygon_masks):\n        assert np.equal(polygon_mask, raw_masks[i]).all()\n'"
tests/test_merge_cells.py,8,"b'""""""\nCommandLine:\n    pytest tests/test_merge_cells.py\n""""""\nimport torch\nimport torch.nn.functional as F\n\nfrom mmdet.ops.merge_cells import (BaseMergeCell, ConcatCell,\n                                   GlobalPoolingCell, SumCell)\n\n\ndef test_sum_cell():\n    inputs_x = torch.randn([2, 256, 32, 32])\n    inputs_y = torch.randn([2, 256, 16, 16])\n    sum_cell = SumCell(256, 256)\n    output = sum_cell(inputs_x, inputs_y, out_size=inputs_x.shape[-2:])\n    assert output.size() == inputs_x.size()\n    output = sum_cell(inputs_x, inputs_y, out_size=inputs_y.shape[-2:])\n    assert output.size() == inputs_y.size()\n    output = sum_cell(inputs_x, inputs_y)\n    assert output.size() == inputs_x.size()\n\n\ndef test_concat_cell():\n    inputs_x = torch.randn([2, 256, 32, 32])\n    inputs_y = torch.randn([2, 256, 16, 16])\n    concat_cell = ConcatCell(256, 256)\n    output = concat_cell(inputs_x, inputs_y, out_size=inputs_x.shape[-2:])\n    assert output.size() == inputs_x.size()\n    output = concat_cell(inputs_x, inputs_y, out_size=inputs_y.shape[-2:])\n    assert output.size() == inputs_y.size()\n    output = concat_cell(inputs_x, inputs_y)\n    assert output.size() == inputs_x.size()\n\n\ndef test_global_pool_cell():\n    inputs_x = torch.randn([2, 256, 32, 32])\n    inputs_y = torch.randn([2, 256, 32, 32])\n    gp_cell = GlobalPoolingCell(with_out_conv=False)\n    gp_cell_out = gp_cell(inputs_x, inputs_y, out_size=inputs_x.shape[-2:])\n    assert (gp_cell_out.size() == inputs_x.size())\n    gp_cell = GlobalPoolingCell(256, 256)\n    gp_cell_out = gp_cell(inputs_x, inputs_y, out_size=inputs_x.shape[-2:])\n    assert (gp_cell_out.size() == inputs_x.size())\n\n\ndef test_resize_methods():\n    inputs_x = torch.randn([2, 256, 128, 128])\n    target_resize_sizes = [(128, 128), (256, 256)]\n    resize_methods_list = [\'nearest\', \'bilinear\']\n\n    for method in resize_methods_list:\n        merge_cell = BaseMergeCell(upsample_mode=method)\n        for target_size in target_resize_sizes:\n            merge_cell_out = merge_cell._resize(inputs_x, target_size)\n            gt_out = F.interpolate(inputs_x, size=target_size, mode=method)\n            assert merge_cell_out.equal(gt_out)\n\n    target_size = (64, 64)  # resize to a smaller size\n    merge_cell = BaseMergeCell()\n    merge_cell_out = merge_cell._resize(inputs_x, target_size)\n    kernel_size = inputs_x.shape[-1] // target_size[-1]\n    gt_out = F.max_pool2d(\n        inputs_x, kernel_size=kernel_size, stride=kernel_size)\n    assert (merge_cell_out == gt_out).all()\n'"
tests/test_necks.py,2,"b'import pytest\nimport torch\nfrom torch.nn.modules.batchnorm import _BatchNorm\n\nfrom mmdet.models.necks import FPN\n\n\ndef test_fpn():\n    """"""Tests fpn """"""\n    s = 64\n    in_channels = [8, 16, 32, 64]\n    feat_sizes = [s // 2**i for i in range(4)]  # [64, 32, 16, 8]\n    out_channels = 8\n    # `num_outs` is not equal to len(in_channels) - start_level\n    with pytest.raises(AssertionError):\n        FPN(in_channels=in_channels,\n            out_channels=out_channels,\n            start_level=1,\n            num_outs=2)\n\n    # `end_level` is larger than len(in_channels) - 1\n    with pytest.raises(AssertionError):\n        FPN(in_channels=in_channels,\n            out_channels=out_channels,\n            start_level=1,\n            end_level=4,\n            num_outs=2)\n\n    # `num_outs` is not equal to end_level - start_level\n    with pytest.raises(AssertionError):\n        FPN(in_channels=in_channels,\n            out_channels=out_channels,\n            start_level=1,\n            end_level=3,\n            num_outs=1)\n\n    # Invalid `add_extra_convs` option\n    with pytest.raises(AssertionError):\n        FPN(in_channels=in_channels,\n            out_channels=out_channels,\n            start_level=1,\n            add_extra_convs=\'on_xxx\',\n            num_outs=5)\n\n    fpn_model = FPN(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        start_level=1,\n        add_extra_convs=True,\n        num_outs=5)\n\n    # FPN expects a multiple levels of features per image\n    feats = [\n        torch.rand(1, in_channels[i], feat_sizes[i], feat_sizes[i])\n        for i in range(len(in_channels))\n    ]\n    outs = fpn_model(feats)\n    assert fpn_model.add_extra_convs == \'on_input\'\n    assert len(outs) == fpn_model.num_outs\n    for i in range(fpn_model.num_outs):\n        outs[i].shape[1] == out_channels\n        outs[i].shape[2] == outs[i].shape[3] == s // (2**i)\n\n    # Tests for fpn with no extra convs (pooling is used instead)\n    fpn_model = FPN(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        start_level=1,\n        add_extra_convs=False,\n        num_outs=5)\n    outs = fpn_model(feats)\n    assert len(outs) == fpn_model.num_outs\n    assert not fpn_model.add_extra_convs\n    for i in range(fpn_model.num_outs):\n        outs[i].shape[1] == out_channels\n        outs[i].shape[2] == outs[i].shape[3] == s // (2**i)\n\n    # Tests for fpn with lateral bns\n    fpn_model = FPN(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        start_level=1,\n        add_extra_convs=True,\n        no_norm_on_lateral=False,\n        norm_cfg=dict(type=\'BN\', requires_grad=True),\n        num_outs=5)\n    outs = fpn_model(feats)\n    assert len(outs) == fpn_model.num_outs\n    assert fpn_model.add_extra_convs == \'on_input\'\n    for i in range(fpn_model.num_outs):\n        outs[i].shape[1] == out_channels\n        outs[i].shape[2] == outs[i].shape[3] == s // (2**i)\n    bn_exist = False\n    for m in fpn_model.modules():\n        if isinstance(m, _BatchNorm):\n            bn_exist = True\n    assert bn_exist\n\n    # Bilinear upsample\n    fpn_model = FPN(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        start_level=1,\n        add_extra_convs=True,\n        upsample_cfg=dict(mode=\'bilinear\', align_corners=True),\n        num_outs=5)\n    fpn_model(feats)\n    outs = fpn_model(feats)\n    assert len(outs) == fpn_model.num_outs\n    assert fpn_model.add_extra_convs == \'on_input\'\n    for i in range(fpn_model.num_outs):\n        outs[i].shape[1] == out_channels\n        outs[i].shape[2] == outs[i].shape[3] == s // (2**i)\n\n    # Scale factor instead of fixed upsample size upsample\n    fpn_model = FPN(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        start_level=1,\n        add_extra_convs=True,\n        upsample_cfg=dict(scale_factor=2),\n        num_outs=5)\n    outs = fpn_model(feats)\n    assert len(outs) == fpn_model.num_outs\n    for i in range(fpn_model.num_outs):\n        outs[i].shape[1] == out_channels\n        outs[i].shape[2] == outs[i].shape[3] == s // (2**i)\n\n    # Extra convs source is \'inputs\'\n    fpn_model = FPN(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        add_extra_convs=\'on_input\',\n        start_level=1,\n        num_outs=5)\n    assert fpn_model.add_extra_convs == \'on_input\'\n    outs = fpn_model(feats)\n    assert len(outs) == fpn_model.num_outs\n    for i in range(fpn_model.num_outs):\n        outs[i].shape[1] == out_channels\n        outs[i].shape[2] == outs[i].shape[3] == s // (2**i)\n\n    # Extra convs source is \'laterals\'\n    fpn_model = FPN(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        add_extra_convs=\'on_lateral\',\n        start_level=1,\n        num_outs=5)\n    assert fpn_model.add_extra_convs == \'on_lateral\'\n    outs = fpn_model(feats)\n    assert len(outs) == fpn_model.num_outs\n    for i in range(fpn_model.num_outs):\n        outs[i].shape[1] == out_channels\n        outs[i].shape[2] == outs[i].shape[3] == s // (2**i)\n\n    # Extra convs source is \'outputs\'\n    fpn_model = FPN(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        add_extra_convs=\'on_output\',\n        start_level=1,\n        num_outs=5)\n    assert fpn_model.add_extra_convs == \'on_output\'\n    outs = fpn_model(feats)\n    assert len(outs) == fpn_model.num_outs\n    for i in range(fpn_model.num_outs):\n        outs[i].shape[1] == out_channels\n        outs[i].shape[2] == outs[i].shape[3] == s // (2**i)\n\n    # extra_convs_on_inputs=False is equal to extra convs source is \'on_output\'\n    fpn_model = FPN(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        add_extra_convs=True,\n        extra_convs_on_inputs=False,\n        start_level=1,\n        num_outs=5,\n    )\n    assert fpn_model.add_extra_convs == \'on_output\'\n    outs = fpn_model(feats)\n    assert len(outs) == fpn_model.num_outs\n    for i in range(fpn_model.num_outs):\n        outs[i].shape[1] == out_channels\n        outs[i].shape[2] == outs[i].shape[3] == s // (2**i)\n\n    # extra_convs_on_inputs=True is equal to extra convs source is \'on_input\'\n    fpn_model = FPN(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        add_extra_convs=True,\n        extra_convs_on_inputs=True,\n        start_level=1,\n        num_outs=5,\n    )\n    assert fpn_model.add_extra_convs == \'on_input\'\n    outs = fpn_model(feats)\n    assert len(outs) == fpn_model.num_outs\n    for i in range(fpn_model.num_outs):\n        outs[i].shape[1] == out_channels\n        outs[i].shape[2] == outs[i].shape[3] == s // (2**i)\n'"
tests/test_nms.py,13,"b'""""""\nCommandLine:\n    pytest tests/test_nms.py\n""""""\nimport numpy as np\nimport pytest\nimport torch\n\nfrom mmdet.ops.nms.nms_wrapper import nms, nms_match\n\n\ndef test_nms_device_and_dtypes_cpu():\n    """"""\n    CommandLine:\n        xdoctest -m tests/test_nms.py test_nms_device_and_dtypes_cpu\n    """"""\n    iou_thr = 0.6\n    base_dets = np.array([[49.1, 32.4, 51.0, 35.9, 0.1],\n                          [49.3, 32.9, 51.0, 35.3, 0.05],\n                          [35.3, 11.5, 39.9, 14.5, 0.9],\n                          [35.2, 11.7, 39.7, 15.7, 0.3]])\n\n    base_expected_suppressed = np.array([[35.3, 11.5, 39.9, 14.5, 0.9],\n                                         [49.1, 32.4, 51.0, 35.9, 0.1]])\n    # CPU can handle float32 and float64\n    dets = base_dets.astype(np.float32)\n    expected_suppressed = base_expected_suppressed.astype(np.float32)\n    suppressed, inds = nms(dets, iou_thr)\n    assert dets.dtype == suppressed.dtype\n    assert np.array_equal(suppressed, expected_suppressed)\n\n    dets = torch.FloatTensor(base_dets)\n    expected_suppressed = torch.FloatTensor(base_expected_suppressed)\n    suppressed, inds = nms(dets, iou_thr)\n    assert dets.dtype == suppressed.dtype\n    assert torch.equal(suppressed, expected_suppressed)\n\n    dets = base_dets.astype(np.float64)\n    expected_suppressed = base_expected_suppressed.astype(np.float64)\n    suppressed, inds = nms(dets, iou_thr)\n    assert dets.dtype == suppressed.dtype\n    assert np.array_equal(suppressed, expected_suppressed)\n\n    dets = torch.DoubleTensor(base_dets)\n    expected_suppressed = torch.DoubleTensor(base_expected_suppressed)\n    suppressed, inds = nms(dets, iou_thr)\n    assert dets.dtype == suppressed.dtype\n    assert torch.equal(suppressed, expected_suppressed)\n\n\ndef test_nms_device_and_dtypes_gpu():\n    """"""\n    CommandLine:\n        xdoctest -m tests/test_nms.py test_nms_device_and_dtypes_gpu\n    """"""\n    if not torch.cuda.is_available():\n        import pytest\n        pytest.skip(\'test requires GPU and torch+cuda\')\n\n    iou_thr = 0.6\n    base_dets = np.array([[49.1, 32.4, 51.0, 35.9, 0.1],\n                          [49.3, 32.9, 51.0, 35.3, 0.05],\n                          [35.3, 11.5, 39.9, 14.5, 0.9],\n                          [35.2, 11.7, 39.7, 15.7, 0.3]])\n\n    base_expected_suppressed = np.array([[35.3, 11.5, 39.9, 14.5, 0.9],\n                                         [49.1, 32.4, 51.0, 35.9, 0.1]])\n\n    for device_id in range(torch.cuda.device_count()):\n        print(f\'Run NMS on device_id = {device_id!r}\')\n        # GPU can handle float32 but not float64\n        dets = base_dets.astype(np.float32)\n        expected_suppressed = base_expected_suppressed.astype(np.float32)\n        suppressed, inds = nms(dets, iou_thr, device_id)\n        assert dets.dtype == suppressed.dtype\n        assert np.array_equal(suppressed, expected_suppressed)\n\n        dets = torch.FloatTensor(base_dets).to(device_id)\n        expected_suppressed = torch.FloatTensor(base_expected_suppressed).to(\n            device_id)\n        suppressed, inds = nms(dets, iou_thr)\n        assert dets.dtype == suppressed.dtype\n        assert torch.equal(suppressed, expected_suppressed)\n\n\ndef test_nms_match():\n    iou_thr = 0.6\n    # empty input\n    empty_dets = np.array([])\n    assert len(nms_match(empty_dets, iou_thr)) == 0\n\n    # non empty ndarray input\n    np_dets = np.array([[49.1, 32.4, 51.0, 35.9, 0.9],\n                        [49.3, 32.9, 51.0, 35.3, 0.9],\n                        [35.3, 11.5, 39.9, 14.5, 0.4],\n                        [35.2, 11.7, 39.7, 15.7, 0.3]])\n    np_groups = nms_match(np_dets, iou_thr)\n    assert isinstance(np_groups[0], np.ndarray)\n    assert len(np_groups) == 2\n    nms_keep_inds = nms(np_dets, iou_thr)[1]\n    assert set([g[0].item() for g in np_groups]) == set(nms_keep_inds.tolist())\n\n    # non empty tensor input\n    tensor_dets = torch.from_numpy(np_dets)\n    tensor_groups = nms_match(tensor_dets, iou_thr)\n    assert isinstance(tensor_groups[0], torch.Tensor)\n    for i in range(len(tensor_groups)):\n        assert np.equal(tensor_groups[i].numpy(), np_groups[i]).all()\n\n    # input of wrong shape\n    wrong_dets = np.zeros((2, 3))\n    with pytest.raises(AssertionError):\n        nms_match(wrong_dets, iou_thr)\n'"
tests/test_pisa_heads.py,16,"b'import mmcv\nimport torch\n\nfrom mmdet.models.dense_heads import PISARetinaHead, PISASSDHead\nfrom mmdet.models.roi_heads import PISARoIHead\n\n\ndef test_pisa_retinanet_head_loss():\n    """"""\n    Tests pisa retinanet head loss when truth is empty and non-empty\n    """"""\n    s = 256\n    img_metas = [{\n        \'img_shape\': (s, s, 3),\n        \'scale_factor\': 1,\n        \'pad_shape\': (s, s, 3)\n    }]\n\n    cfg = mmcv.Config(\n        dict(\n            assigner=dict(\n                type=\'MaxIoUAssigner\',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.3,\n                min_pos_iou=0.3,\n                match_low_quality=True,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type=\'RandomSampler\',\n                num=256,\n                pos_fraction=0.5,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=False),\n            isr=dict(k=2., bias=0.),\n            carl=dict(k=1., bias=0.2),\n            allowed_border=0,\n            pos_weight=-1,\n            debug=False))\n    self = PISARetinaHead(num_classes=4, in_channels=1, train_cfg=cfg)\n\n    # Anchor head expects a multiple levels of features per image\n    feat = [\n        torch.rand(1, 1, s // (2**(i + 2)), s // (2**(i + 2)))\n        for i in range(len(self.anchor_generator.strides))\n    ]\n    cls_scores, bbox_preds = self.forward(feat)\n\n    # Test that empty ground truth encourages the network to predict background\n    gt_bboxes = [torch.empty((0, 4))]\n    gt_labels = [torch.LongTensor([])]\n\n    gt_bboxes_ignore = None\n    empty_gt_losses = self.loss(cls_scores, bbox_preds, gt_bboxes, gt_labels,\n                                img_metas, gt_bboxes_ignore)\n    # When there is no truth, the cls loss should be nonzero but there should\n    # be no box loss.\n    empty_cls_loss = empty_gt_losses[\'loss_cls\'].sum()\n    empty_box_loss = empty_gt_losses[\'loss_bbox\'].sum()\n    assert empty_cls_loss.item() > 0, \'cls loss should be non-zero\'\n    assert empty_box_loss.item() == 0, (\n        \'there should be no box loss when there are no true boxes\')\n\n    # When truth is non-empty then both cls and box loss should be nonzero for\n    # random inputs\n    gt_bboxes = [\n        torch.Tensor([[23.6667, 23.8757, 238.6326, 151.8874]]),\n    ]\n    gt_labels = [torch.LongTensor([2])]\n    one_gt_losses = self.loss(cls_scores, bbox_preds, gt_bboxes, gt_labels,\n                              img_metas, gt_bboxes_ignore)\n    onegt_cls_loss = one_gt_losses[\'loss_cls\'].sum()\n    onegt_box_loss = one_gt_losses[\'loss_bbox\'].sum()\n    assert onegt_cls_loss.item() > 0, \'cls loss should be non-zero\'\n    assert onegt_box_loss.item() > 0, \'box loss should be non-zero\'\n\n\ndef test_pisa_ssd_head_loss():\n    """"""\n    Tests pisa ssd head loss when truth is empty and non-empty\n    """"""\n    s = 256\n    img_metas = [{\n        \'img_shape\': (s, s, 3),\n        \'scale_factor\': 1,\n        \'pad_shape\': (s, s, 3)\n    }]\n\n    cfg = mmcv.Config(\n        dict(\n            assigner=dict(\n                type=\'MaxIoUAssigner\',\n                pos_iou_thr=0.5,\n                neg_iou_thr=0.5,\n                min_pos_iou=0.,\n                ignore_iof_thr=-1,\n                gt_max_assign_all=False),\n            isr=dict(k=2., bias=0.),\n            carl=dict(k=1., bias=0.2),\n            smoothl1_beta=1.,\n            allowed_border=-1,\n            pos_weight=-1,\n            neg_pos_ratio=3,\n            debug=False))\n    ssd_anchor_generator = dict(\n        type=\'SSDAnchorGenerator\',\n        scale_major=False,\n        input_size=300,\n        strides=[1],\n        ratios=([2], ),\n        basesize_ratio_range=(0.15, 0.9))\n    self = PISASSDHead(\n        num_classes=4,\n        in_channels=(1, ),\n        train_cfg=cfg,\n        anchor_generator=ssd_anchor_generator)\n\n    # Anchor head expects a multiple levels of features per image\n    feat = [\n        torch.rand(1, 1, s // (2**(i + 2)), s // (2**(i + 2)))\n        for i in range(len(self.anchor_generator.strides))\n    ]\n    cls_scores, bbox_preds = self.forward(feat)\n\n    # Test that empty ground truth encourages the network to predict background\n    gt_bboxes = [torch.empty((0, 4))]\n    gt_labels = [torch.LongTensor([])]\n\n    gt_bboxes_ignore = None\n    empty_gt_losses = self.loss(cls_scores, bbox_preds, gt_bboxes, gt_labels,\n                                img_metas, gt_bboxes_ignore)\n    # When there is no truth, the cls loss should be nonzero but there should\n    # be no box loss.\n    empty_cls_loss = sum(empty_gt_losses[\'loss_cls\'])\n    empty_box_loss = sum(empty_gt_losses[\'loss_bbox\'])\n    # SSD is special, #pos:#neg = 1: 3, so empth gt will also lead loss cls = 0\n    assert empty_cls_loss.item() == 0, \'cls loss should be non-zero\'\n    assert empty_box_loss.item() == 0, (\n        \'there should be no box loss when there are no true boxes\')\n\n    # When truth is non-empty then both cls and box loss should be nonzero for\n    # random inputs\n    gt_bboxes = [\n        torch.Tensor([[23.6667, 23.8757, 238.6326, 151.8874]]),\n    ]\n    gt_labels = [torch.LongTensor([2])]\n    one_gt_losses = self.loss(cls_scores, bbox_preds, gt_bboxes, gt_labels,\n                              img_metas, gt_bboxes_ignore)\n    onegt_cls_loss = sum(one_gt_losses[\'loss_cls\'])\n    onegt_box_loss = sum(one_gt_losses[\'loss_bbox\'])\n    assert onegt_cls_loss.item() > 0, \'cls loss should be non-zero\'\n    assert onegt_box_loss.item() > 0, \'box loss should be non-zero\'\n\n\ndef test_pisa_roi_head_loss():\n    """"""\n    Tests pisa roi head loss when truth is empty and non-empty\n    """"""\n    train_cfg = mmcv.Config(\n        dict(\n            assigner=dict(\n                type=\'MaxIoUAssigner\',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.3,\n                min_pos_iou=0.3,\n                match_low_quality=True,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type=\'ScoreHLRSampler\',\n                num=4,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True,\n                k=0.5,\n                bias=0.),\n            isr=dict(k=2., bias=0.),\n            carl=dict(k=1., bias=0.2),\n            allowed_border=0,\n            pos_weight=-1,\n            debug=False))\n\n    bbox_roi_extractor = dict(\n        type=\'SingleRoIExtractor\',\n        roi_layer=dict(type=\'RoIAlign\', out_size=7, sample_num=0),\n        out_channels=1,\n        featmap_strides=[1])\n\n    bbox_head = dict(\n        type=\'Shared2FCBBoxHead\',\n        in_channels=1,\n        fc_out_channels=2,\n        roi_feat_size=7,\n        num_classes=4,\n        bbox_coder=dict(\n            type=\'DeltaXYWHBBoxCoder\',\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.1, 0.1, 0.2, 0.2]),\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type=\'CrossEntropyLoss\', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type=\'L1Loss\', loss_weight=1.0))\n\n    self = PISARoIHead(bbox_roi_extractor, bbox_head, train_cfg=train_cfg)\n\n    s = 256\n    img_metas = [{\n        \'img_shape\': (s, s, 3),\n        \'scale_factor\': 1,\n        \'pad_shape\': (s, s, 3)\n    }]\n\n    # Anchor head expects a multiple levels of features per image\n    feat = [\n        torch.rand(1, 1, s // (2**(i + 2)), s // (2**(i + 2)))\n        for i in range(1)\n    ]\n\n    proposal_list = [\n        torch.Tensor([[22.6667, 22.8757, 238.6326, 151.8874], [0, 3, 5, 7]])\n    ]\n\n    # Test that empty ground truth encourages the network to predict background\n    gt_bboxes = [torch.empty((0, 4))]\n    gt_labels = [torch.LongTensor([])]\n    gt_bboxes_ignore = None\n\n    empty_gt_losses = self.forward_train(feat, img_metas, proposal_list,\n                                         gt_bboxes, gt_labels,\n                                         gt_bboxes_ignore)\n\n    # When there is no truth, the cls loss should be nonzero but there should\n    # be no box loss.\n    empty_cls_loss = empty_gt_losses[\'loss_cls\'].sum()\n    empty_box_loss = empty_gt_losses[\'loss_bbox\'].sum()\n    assert empty_cls_loss.item() > 0, \'cls loss should be non-zero\'\n    assert empty_box_loss.item() == 0, (\n        \'there should be no box loss when there are no true boxes\')\n\n    # When truth is non-empty then both cls and box loss should be nonzero for\n    # random inputs\n    gt_bboxes = [\n        torch.Tensor([[23.6667, 23.8757, 238.6326, 151.8874]]),\n    ]\n    gt_labels = [torch.LongTensor([2])]\n\n    one_gt_losses = self.forward_train(feat, img_metas, proposal_list,\n                                       gt_bboxes, gt_labels, gt_bboxes_ignore)\n    onegt_cls_loss = one_gt_losses[\'loss_cls\'].sum()\n    onegt_box_loss = one_gt_losses[\'loss_bbox\'].sum()\n    assert onegt_cls_loss.item() > 0, \'cls loss should be non-zero\'\n    assert onegt_box_loss.item() > 0, \'box loss should be non-zero\'\n'"
tests/test_roi_extractor.py,6,"b""import mmcv\nimport torch\n\nfrom mmdet.models.roi_heads.roi_extractors import SumGenericRoiExtractor\n\n\ndef test_groie():\n    cfg = mmcv.Config(\n        dict(\n            roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n            out_channels=256,\n            featmap_strides=[4, 8, 16, 32],\n            pre_cfg=dict(\n                type='ConvModule',\n                in_channels=256,\n                out_channels=256,\n                kernel_size=5,\n                padding=2,\n                inplace=False,\n            ),\n            post_cfg=dict(\n                type='ConvModule',\n                in_channels=256,\n                out_channels=256,\n                kernel_size=5,\n                padding=2,\n                inplace=False)))\n\n    groie = SumGenericRoiExtractor(**cfg)\n\n    feats = (\n        torch.rand((1, 256, 200, 336)),\n        torch.rand((1, 256, 100, 168)),\n        torch.rand((1, 256, 50, 84)),\n        torch.rand((1, 256, 25, 42)),\n    )\n\n    rois = torch.tensor([[0.0000, 587.8285, 52.1405, 886.2484, 341.5644]])\n\n    res = groie(feats, rois)\n    assert res.shape == torch.Size([1, 256, 7, 7])\n"""
tests/test_sampler.py,36,"b""import torch\n\nfrom mmdet.core.bbox.assigners import MaxIoUAssigner\nfrom mmdet.core.bbox.samplers import (OHEMSampler, RandomSampler,\n                                      ScoreHLRSampler)\n\n\ndef test_random_sampler():\n    assigner = MaxIoUAssigner(\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n        ignore_iof_thr=0.5,\n        ignore_wrt_candidates=False,\n    )\n    bboxes = torch.FloatTensor([\n        [0, 0, 10, 10],\n        [10, 10, 20, 20],\n        [5, 5, 15, 15],\n        [32, 32, 38, 42],\n    ])\n    gt_bboxes = torch.FloatTensor([\n        [0, 0, 10, 9],\n        [0, 10, 10, 19],\n    ])\n    gt_labels = torch.LongTensor([1, 2])\n    gt_bboxes_ignore = torch.Tensor([\n        [30, 30, 40, 40],\n    ])\n    assign_result = assigner.assign(\n        bboxes,\n        gt_bboxes,\n        gt_bboxes_ignore=gt_bboxes_ignore,\n        gt_labels=gt_labels)\n\n    sampler = RandomSampler(\n        num=10, pos_fraction=0.5, neg_pos_ub=-1, add_gt_as_proposals=True)\n\n    sample_result = sampler.sample(assign_result, bboxes, gt_bboxes, gt_labels)\n\n    assert len(sample_result.pos_bboxes) == len(sample_result.pos_inds)\n    assert len(sample_result.neg_bboxes) == len(sample_result.neg_inds)\n\n\ndef test_random_sampler_empty_gt():\n    assigner = MaxIoUAssigner(\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n        ignore_iof_thr=0.5,\n        ignore_wrt_candidates=False,\n    )\n    bboxes = torch.FloatTensor([\n        [0, 0, 10, 10],\n        [10, 10, 20, 20],\n        [5, 5, 15, 15],\n        [32, 32, 38, 42],\n    ])\n    gt_bboxes = torch.empty(0, 4)\n    gt_labels = torch.empty(0, ).long()\n    assign_result = assigner.assign(bboxes, gt_bboxes, gt_labels=gt_labels)\n\n    sampler = RandomSampler(\n        num=10, pos_fraction=0.5, neg_pos_ub=-1, add_gt_as_proposals=True)\n\n    sample_result = sampler.sample(assign_result, bboxes, gt_bboxes, gt_labels)\n\n    assert len(sample_result.pos_bboxes) == len(sample_result.pos_inds)\n    assert len(sample_result.neg_bboxes) == len(sample_result.neg_inds)\n\n\ndef test_random_sampler_empty_pred():\n    assigner = MaxIoUAssigner(\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n        ignore_iof_thr=0.5,\n        ignore_wrt_candidates=False,\n    )\n    bboxes = torch.empty(0, 4)\n    gt_bboxes = torch.FloatTensor([\n        [0, 0, 10, 9],\n        [0, 10, 10, 19],\n    ])\n    gt_labels = torch.LongTensor([1, 2])\n    assign_result = assigner.assign(bboxes, gt_bboxes, gt_labels=gt_labels)\n\n    sampler = RandomSampler(\n        num=10, pos_fraction=0.5, neg_pos_ub=-1, add_gt_as_proposals=True)\n\n    sample_result = sampler.sample(assign_result, bboxes, gt_bboxes, gt_labels)\n\n    assert len(sample_result.pos_bboxes) == len(sample_result.pos_inds)\n    assert len(sample_result.neg_bboxes) == len(sample_result.neg_inds)\n\n\ndef _context_for_ohem():\n    try:\n        from test_forward import _get_detector_cfg\n    except ImportError:\n        # Hack: grab testing utils from test_forward to make a context for ohem\n        import sys\n        from os.path import dirname\n        sys.path.insert(0, dirname(__file__))\n        from test_forward import _get_detector_cfg\n    model, train_cfg, test_cfg = _get_detector_cfg(\n        'faster_rcnn/faster_rcnn_r50_fpn_ohem_1x_coco.py')\n    model['pretrained'] = None\n\n    from mmdet.models import build_detector\n    context = build_detector(\n        model, train_cfg=train_cfg, test_cfg=test_cfg).roi_head\n    return context\n\n\ndef test_ohem_sampler():\n\n    assigner = MaxIoUAssigner(\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n        ignore_iof_thr=0.5,\n        ignore_wrt_candidates=False,\n    )\n    bboxes = torch.FloatTensor([\n        [0, 0, 10, 10],\n        [10, 10, 20, 20],\n        [5, 5, 15, 15],\n        [32, 32, 38, 42],\n    ])\n    gt_bboxes = torch.FloatTensor([\n        [0, 0, 10, 9],\n        [0, 10, 10, 19],\n    ])\n    gt_labels = torch.LongTensor([1, 2])\n    gt_bboxes_ignore = torch.Tensor([\n        [30, 30, 40, 40],\n    ])\n    assign_result = assigner.assign(\n        bboxes,\n        gt_bboxes,\n        gt_bboxes_ignore=gt_bboxes_ignore,\n        gt_labels=gt_labels)\n\n    context = _context_for_ohem()\n\n    sampler = OHEMSampler(\n        num=10,\n        pos_fraction=0.5,\n        context=context,\n        neg_pos_ub=-1,\n        add_gt_as_proposals=True)\n\n    feats = [torch.rand(1, 256, int(2**i), int(2**i)) for i in [6, 5, 4, 3, 2]]\n    sample_result = sampler.sample(\n        assign_result, bboxes, gt_bboxes, gt_labels, feats=feats)\n\n    assert len(sample_result.pos_bboxes) == len(sample_result.pos_inds)\n    assert len(sample_result.neg_bboxes) == len(sample_result.neg_inds)\n\n\ndef test_ohem_sampler_empty_gt():\n\n    assigner = MaxIoUAssigner(\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n        ignore_iof_thr=0.5,\n        ignore_wrt_candidates=False,\n    )\n    bboxes = torch.FloatTensor([\n        [0, 0, 10, 10],\n        [10, 10, 20, 20],\n        [5, 5, 15, 15],\n        [32, 32, 38, 42],\n    ])\n    gt_bboxes = torch.empty(0, 4)\n    gt_labels = torch.LongTensor([])\n    gt_bboxes_ignore = torch.Tensor([])\n    assign_result = assigner.assign(\n        bboxes,\n        gt_bboxes,\n        gt_bboxes_ignore=gt_bboxes_ignore,\n        gt_labels=gt_labels)\n\n    context = _context_for_ohem()\n\n    sampler = OHEMSampler(\n        num=10,\n        pos_fraction=0.5,\n        context=context,\n        neg_pos_ub=-1,\n        add_gt_as_proposals=True)\n\n    feats = [torch.rand(1, 256, int(2**i), int(2**i)) for i in [6, 5, 4, 3, 2]]\n\n    sample_result = sampler.sample(\n        assign_result, bboxes, gt_bboxes, gt_labels, feats=feats)\n\n    assert len(sample_result.pos_bboxes) == len(sample_result.pos_inds)\n    assert len(sample_result.neg_bboxes) == len(sample_result.neg_inds)\n\n\ndef test_ohem_sampler_empty_pred():\n    assigner = MaxIoUAssigner(\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n        ignore_iof_thr=0.5,\n        ignore_wrt_candidates=False,\n    )\n    bboxes = torch.empty(0, 4)\n    gt_bboxes = torch.FloatTensor([\n        [0, 0, 10, 10],\n        [10, 10, 20, 20],\n        [5, 5, 15, 15],\n        [32, 32, 38, 42],\n    ])\n    gt_labels = torch.LongTensor([1, 2, 2, 3])\n    gt_bboxes_ignore = torch.Tensor([])\n    assign_result = assigner.assign(\n        bboxes,\n        gt_bboxes,\n        gt_bboxes_ignore=gt_bboxes_ignore,\n        gt_labels=gt_labels)\n\n    context = _context_for_ohem()\n\n    sampler = OHEMSampler(\n        num=10,\n        pos_fraction=0.5,\n        context=context,\n        neg_pos_ub=-1,\n        add_gt_as_proposals=True)\n\n    feats = [torch.rand(1, 256, int(2**i), int(2**i)) for i in [6, 5, 4, 3, 2]]\n\n    sample_result = sampler.sample(\n        assign_result, bboxes, gt_bboxes, gt_labels, feats=feats)\n\n    assert len(sample_result.pos_bboxes) == len(sample_result.pos_inds)\n    assert len(sample_result.neg_bboxes) == len(sample_result.neg_inds)\n\n\ndef test_random_sample_result():\n    from mmdet.core.bbox.samplers.sampling_result import SamplingResult\n    SamplingResult.random(num_gts=0, num_preds=0)\n    SamplingResult.random(num_gts=0, num_preds=3)\n    SamplingResult.random(num_gts=3, num_preds=3)\n    SamplingResult.random(num_gts=0, num_preds=3)\n    SamplingResult.random(num_gts=7, num_preds=7)\n    SamplingResult.random(num_gts=7, num_preds=64)\n    SamplingResult.random(num_gts=24, num_preds=3)\n\n    for i in range(3):\n        SamplingResult.random(rng=i)\n\n\ndef test_score_hlr_sampler_empty_pred():\n    assigner = MaxIoUAssigner(\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n        ignore_iof_thr=0.5,\n        ignore_wrt_candidates=False,\n    )\n    context = _context_for_ohem()\n    sampler = ScoreHLRSampler(\n        num=10,\n        pos_fraction=0.5,\n        context=context,\n        neg_pos_ub=-1,\n        add_gt_as_proposals=True)\n    gt_bboxes_ignore = torch.Tensor([])\n    feats = [torch.rand(1, 256, int(2**i), int(2**i)) for i in [6, 5, 4, 3, 2]]\n\n    # empty bbox\n    bboxes = torch.empty(0, 4)\n    gt_bboxes = torch.FloatTensor([\n        [0, 0, 10, 10],\n        [10, 10, 20, 20],\n        [5, 5, 15, 15],\n        [32, 32, 38, 42],\n    ])\n    gt_labels = torch.LongTensor([1, 2, 2, 3])\n    assign_result = assigner.assign(\n        bboxes,\n        gt_bboxes,\n        gt_bboxes_ignore=gt_bboxes_ignore,\n        gt_labels=gt_labels)\n    sample_result, _ = sampler.sample(\n        assign_result, bboxes, gt_bboxes, gt_labels, feats=feats)\n    assert len(sample_result.neg_inds) == 0\n    assert len(sample_result.pos_bboxes) == len(sample_result.pos_inds)\n    assert len(sample_result.neg_bboxes) == len(sample_result.neg_inds)\n\n    # empty gt\n    bboxes = torch.FloatTensor([\n        [0, 0, 10, 10],\n        [10, 10, 20, 20],\n        [5, 5, 15, 15],\n        [32, 32, 38, 42],\n    ])\n    gt_bboxes = torch.empty(0, 4)\n    gt_labels = torch.LongTensor([])\n    assign_result = assigner.assign(\n        bboxes,\n        gt_bboxes,\n        gt_bboxes_ignore=gt_bboxes_ignore,\n        gt_labels=gt_labels)\n    sample_result, _ = sampler.sample(\n        assign_result, bboxes, gt_bboxes, gt_labels, feats=feats)\n    assert len(sample_result.pos_inds) == 0\n    assert len(sample_result.pos_bboxes) == len(sample_result.pos_inds)\n    assert len(sample_result.neg_bboxes) == len(sample_result.neg_inds)\n\n    # non-empty input\n    bboxes = torch.FloatTensor([\n        [0, 0, 10, 10],\n        [10, 10, 20, 20],\n        [5, 5, 15, 15],\n        [32, 32, 38, 42],\n    ])\n    gt_bboxes = torch.FloatTensor([\n        [0, 0, 10, 10],\n        [10, 10, 20, 20],\n        [5, 5, 15, 15],\n        [32, 32, 38, 42],\n    ])\n    gt_labels = torch.LongTensor([1, 2, 2, 3])\n    assign_result = assigner.assign(\n        bboxes,\n        gt_bboxes,\n        gt_bboxes_ignore=gt_bboxes_ignore,\n        gt_labels=gt_labels)\n    sample_result, _ = sampler.sample(\n        assign_result, bboxes, gt_bboxes, gt_labels, feats=feats)\n    assert len(sample_result.pos_bboxes) == len(sample_result.pos_inds)\n    assert len(sample_result.neg_bboxes) == len(sample_result.neg_inds)\n"""
tests/test_soft_nms.py,2,"b'""""""\nCommandLine:\n    pytest tests/test_soft_nms.py\n""""""\nimport numpy as np\nimport torch\n\nfrom mmdet.ops.nms.nms_wrapper import soft_nms\n\n\ndef test_soft_nms_device_and_dtypes_cpu():\n    """"""\n    CommandLine:\n        xdoctest -m tests/test_soft_nms.py test_soft_nms_device_and_dtypes_cpu\n    """"""\n    iou_thr = 0.7\n    base_dets = np.array([[49.1, 32.4, 51.0, 35.9, 0.9],\n                          [49.3, 32.9, 51.0, 35.3, 0.9],\n                          [35.3, 11.5, 39.9, 14.5, 0.4],\n                          [35.2, 11.7, 39.7, 15.7, 0.3]])\n\n    # CPU can handle float32 and float64\n    dets = base_dets.astype(np.float32)\n    new_dets, inds = soft_nms(dets, iou_thr)\n    assert dets.dtype == new_dets.dtype\n    assert len(inds) == len(new_dets) == 4\n\n    dets = torch.FloatTensor(base_dets)\n    new_dets, inds = soft_nms(dets, iou_thr)\n    assert dets.dtype == new_dets.dtype\n    assert len(inds) == len(new_dets) == 4\n\n    dets = base_dets.astype(np.float64)\n    new_dets, inds = soft_nms(dets, iou_thr)\n    assert dets.dtype == new_dets.dtype\n    assert len(inds) == len(new_dets) == 4\n\n    dets = torch.DoubleTensor(base_dets)\n    new_dets, inds = soft_nms(dets, iou_thr)\n    assert dets.dtype == new_dets.dtype\n    assert len(inds) == len(new_dets) == 4\n'"
tests/test_utils.py,0,"b""import numpy.testing as npt\n\nfrom mmdet.utils.flops_counter import params_to_string\n\n\ndef test_params_to_string():\n    npt.assert_equal(params_to_string(1e9), '1000.0 M')\n    npt.assert_equal(params_to_string(2e5), '200.0 k')\n    npt.assert_equal(params_to_string(3e-9), '3e-09')\n"""
tests/test_wrappers.py,30,"b'from collections import OrderedDict\nfrom itertools import product\nfrom unittest.mock import patch\n\nimport torch\nimport torch.nn as nn\n\nfrom mmdet.ops import Conv2d, ConvTranspose2d, Linear, MaxPool2d\n\ntorch.__version__ = \'1.1\'  # force test\n\n\ndef test_conv2d():\n    """"""\n    CommandLine:\n        xdoctest -m tests/test_wrappers.py test_conv2d\n    """"""\n\n    test_cases = OrderedDict([(\'in_w\', [10, 20]), (\'in_h\', [10, 20]),\n                              (\'in_channel\', [1, 3]), (\'out_channel\', [1, 3]),\n                              (\'kernel_size\', [3, 5]), (\'stride\', [1, 2]),\n                              (\'padding\', [0, 1]), (\'dilation\', [1, 2])])\n\n    # train mode\n    for in_h, in_w, in_cha, out_cha, k, s, p, d in product(\n            *list(test_cases.values())):\n        # wrapper op with 0-dim input\n        x_empty = torch.randn(0, in_cha, in_h, in_w)\n        torch.manual_seed(0)\n        wrapper = Conv2d(in_cha, out_cha, k, stride=s, padding=p, dilation=d)\n        wrapper_out = wrapper(x_empty)\n\n        # torch op with 3-dim input as shape reference\n        x_normal = torch.randn(3, in_cha, in_h, in_w).requires_grad_(True)\n        torch.manual_seed(0)\n        ref = nn.Conv2d(in_cha, out_cha, k, stride=s, padding=p, dilation=d)\n        ref_out = ref(x_normal)\n\n        assert wrapper_out.shape[0] == 0\n        assert wrapper_out.shape[1:] == ref_out.shape[1:]\n\n        wrapper_out.sum().backward()\n        assert wrapper.weight.grad is not None\n        assert wrapper.weight.grad.shape == wrapper.weight.shape\n\n        assert torch.equal(wrapper(x_normal), ref_out)\n\n    # eval mode\n    x_empty = torch.randn(0, in_cha, in_h, in_w)\n    wrapper = Conv2d(in_cha, out_cha, k, stride=s, padding=p, dilation=d)\n    wrapper.eval()\n    wrapper(x_empty)\n\n\ndef test_conv_transposed_2d():\n    test_cases = OrderedDict([(\'in_w\', [10, 20]), (\'in_h\', [10, 20]),\n                              (\'in_channel\', [1, 3]), (\'out_channel\', [1, 3]),\n                              (\'kernel_size\', [3, 5]), (\'stride\', [1, 2]),\n                              (\'padding\', [0, 1]), (\'dilation\', [1, 2])])\n\n    for in_h, in_w, in_cha, out_cha, k, s, p, d in product(\n            *list(test_cases.values())):\n        # wrapper op with 0-dim input\n        x_empty = torch.randn(0, in_cha, in_h, in_w, requires_grad=True)\n        # out padding must be smaller than either stride or dilation\n        op = min(s, d) - 1\n        torch.manual_seed(0)\n        wrapper = ConvTranspose2d(\n            in_cha,\n            out_cha,\n            k,\n            stride=s,\n            padding=p,\n            dilation=d,\n            output_padding=op)\n        wrapper_out = wrapper(x_empty)\n\n        # torch op with 3-dim input as shape reference\n        x_normal = torch.randn(3, in_cha, in_h, in_w)\n        torch.manual_seed(0)\n        ref = nn.ConvTranspose2d(\n            in_cha,\n            out_cha,\n            k,\n            stride=s,\n            padding=p,\n            dilation=d,\n            output_padding=op)\n        ref_out = ref(x_normal)\n\n        assert wrapper_out.shape[0] == 0\n        assert wrapper_out.shape[1:] == ref_out.shape[1:]\n\n        wrapper_out.sum().backward()\n        assert wrapper.weight.grad is not None\n        assert wrapper.weight.grad.shape == wrapper.weight.shape\n\n        assert torch.equal(wrapper(x_normal), ref_out)\n\n    # eval mode\n    x_empty = torch.randn(0, in_cha, in_h, in_w)\n    wrapper = ConvTranspose2d(\n        in_cha, out_cha, k, stride=s, padding=p, dilation=d, output_padding=op)\n    wrapper.eval()\n    wrapper(x_empty)\n\n\ndef test_max_pool_2d():\n    test_cases = OrderedDict([(\'in_w\', [10, 20]), (\'in_h\', [10, 20]),\n                              (\'in_channel\', [1, 3]), (\'out_channel\', [1, 3]),\n                              (\'kernel_size\', [3, 5]), (\'stride\', [1, 2]),\n                              (\'padding\', [0, 1]), (\'dilation\', [1, 2])])\n\n    for in_h, in_w, in_cha, out_cha, k, s, p, d in product(\n            *list(test_cases.values())):\n        # wrapper op with 0-dim input\n        x_empty = torch.randn(0, in_cha, in_h, in_w, requires_grad=True)\n        wrapper = MaxPool2d(k, stride=s, padding=p, dilation=d)\n        wrapper_out = wrapper(x_empty)\n\n        # torch op with 3-dim input as shape reference\n        x_normal = torch.randn(3, in_cha, in_h, in_w)\n        ref = nn.MaxPool2d(k, stride=s, padding=p, dilation=d)\n        ref_out = ref(x_normal)\n\n        assert wrapper_out.shape[0] == 0\n        assert wrapper_out.shape[1:] == ref_out.shape[1:]\n\n        assert torch.equal(wrapper(x_normal), ref_out)\n\n\ndef test_linear():\n    test_cases = OrderedDict([\n        (\'in_w\', [10, 20]),\n        (\'in_h\', [10, 20]),\n        (\'in_feature\', [1, 3]),\n        (\'out_feature\', [1, 3]),\n    ])\n\n    for in_h, in_w, in_feature, out_feature in product(\n            *list(test_cases.values())):\n        # wrapper op with 0-dim input\n        x_empty = torch.randn(0, in_feature, requires_grad=True)\n        torch.manual_seed(0)\n        wrapper = Linear(in_feature, out_feature)\n        wrapper_out = wrapper(x_empty)\n\n        # torch op with 3-dim input as shape reference\n        x_normal = torch.randn(3, in_feature)\n        torch.manual_seed(0)\n        ref = nn.Linear(in_feature, out_feature)\n        ref_out = ref(x_normal)\n\n        assert wrapper_out.shape[0] == 0\n        assert wrapper_out.shape[1:] == ref_out.shape[1:]\n\n        wrapper_out.sum().backward()\n        assert wrapper.weight.grad is not None\n        assert wrapper.weight.grad.shape == wrapper.weight.shape\n\n        assert torch.equal(wrapper(x_normal), ref_out)\n\n    # eval mode\n    x_empty = torch.randn(0, in_feature)\n    wrapper = Linear(in_feature, out_feature)\n    wrapper.eval()\n    wrapper(x_empty)\n\n\ndef test_nn_op_forward_called():\n    torch.__version__ = \'1.4.1\'\n\n    for m in [\'Conv2d\', \'ConvTranspose2d\', \'MaxPool2d\']:\n        with patch(f\'torch.nn.{m}.forward\') as nn_module_forward:\n            # randn input\n            x_empty = torch.randn(0, 3, 10, 10)\n            wrapper = eval(m)(3, 2, 1)\n            wrapper(x_empty)\n            nn_module_forward.assert_called_with(x_empty)\n\n            # non-randn input\n            x_normal = torch.randn(1, 3, 10, 10)\n            wrapper = eval(m)(3, 2, 1)\n            wrapper(x_normal)\n            nn_module_forward.assert_called_with(x_normal)\n\n    with patch(\'torch.nn.Linear.forward\') as nn_module_forward:\n        # randn input\n        x_empty = torch.randn(0, 3)\n        wrapper = Linear(3, 3)\n        wrapper(x_empty)\n        nn_module_forward.assert_not_called()\n\n        # non-randn input\n        x_normal = torch.randn(1, 3)\n        wrapper = Linear(3, 3)\n        wrapper(x_normal)\n        nn_module_forward.assert_called_with(x_normal)\n'"
tools/analyze_logs.py,0,"b'import argparse\nimport json\nfrom collections import defaultdict\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n\ndef cal_train_time(log_dicts, args):\n    for i, log_dict in enumerate(log_dicts):\n        print(f\'{""-"" * 5}Analyze train time of {args.json_logs[i]}{""-"" * 5}\')\n        all_times = []\n        for epoch in log_dict.keys():\n            if args.include_outliers:\n                all_times.append(log_dict[epoch][\'time\'])\n            else:\n                all_times.append(log_dict[epoch][\'time\'][1:])\n        all_times = np.array(all_times)\n        epoch_ave_time = all_times.mean(-1)\n        slowest_epoch = epoch_ave_time.argmax()\n        fastest_epoch = epoch_ave_time.argmin()\n        std_over_epoch = epoch_ave_time.std()\n        print(f\'slowest epoch {slowest_epoch + 1}, \'\n              f\'average time is {epoch_ave_time[slowest_epoch]:.4f}\')\n        print(f\'fastest epoch {fastest_epoch + 1}, \'\n              f\'average time is {epoch_ave_time[fastest_epoch]:.4f}\')\n        print(f\'time std over epochs is {std_over_epoch:.4f}\')\n        print(f\'average iter time: {np.mean(all_times):.4f} s/iter\')\n        print()\n\n\ndef plot_curve(log_dicts, args):\n    if args.backend is not None:\n        plt.switch_backend(args.backend)\n    sns.set_style(args.style)\n    # if legend is None, use {filename}_{key} as legend\n    legend = args.legend\n    if legend is None:\n        legend = []\n        for json_log in args.json_logs:\n            for metric in args.keys:\n                legend.append(f\'{json_log}_{metric}\')\n    assert len(legend) == (len(args.json_logs) * len(args.keys))\n    metrics = args.keys\n\n    num_metrics = len(metrics)\n    for i, log_dict in enumerate(log_dicts):\n        epochs = list(log_dict.keys())\n        for j, metric in enumerate(metrics):\n            print(f\'plot curve of {args.json_logs[i]}, metric is {metric}\')\n            if metric not in log_dict[epochs[0]]:\n                raise KeyError(\n                    f\'{args.json_logs[i]} does not contain metric {metric}\')\n\n            if \'mAP\' in metric:\n                xs = np.arange(1, max(epochs) + 1)\n                ys = []\n                for epoch in epochs:\n                    ys += log_dict[epoch][metric]\n                ax = plt.gca()\n                ax.set_xticks(xs)\n                plt.xlabel(\'epoch\')\n                plt.plot(xs, ys, label=legend[i * num_metrics + j], marker=\'o\')\n            else:\n                xs = []\n                ys = []\n                num_iters_per_epoch = log_dict[epochs[0]][\'iter\'][-1]\n                for epoch in epochs:\n                    iters = log_dict[epoch][\'iter\']\n                    if log_dict[epoch][\'mode\'][-1] == \'val\':\n                        iters = iters[:-1]\n                    xs.append(\n                        np.array(iters) + (epoch - 1) * num_iters_per_epoch)\n                    ys.append(np.array(log_dict[epoch][metric][:len(iters)]))\n                xs = np.concatenate(xs)\n                ys = np.concatenate(ys)\n                plt.xlabel(\'iter\')\n                plt.plot(\n                    xs, ys, label=legend[i * num_metrics + j], linewidth=0.5)\n            plt.legend()\n        if args.title is not None:\n            plt.title(args.title)\n    if args.out is None:\n        plt.show()\n    else:\n        print(f\'save curve to: {args.out}\')\n        plt.savefig(args.out)\n        plt.cla()\n\n\ndef add_plot_parser(subparsers):\n    parser_plt = subparsers.add_parser(\n        \'plot_curve\', help=\'parser for plotting curves\')\n    parser_plt.add_argument(\n        \'json_logs\',\n        type=str,\n        nargs=\'+\',\n        help=\'path of train log in json format\')\n    parser_plt.add_argument(\n        \'--keys\',\n        type=str,\n        nargs=\'+\',\n        default=[\'bbox_mAP\'],\n        help=\'the metric that you want to plot\')\n    parser_plt.add_argument(\'--title\', type=str, help=\'title of figure\')\n    parser_plt.add_argument(\n        \'--legend\',\n        type=str,\n        nargs=\'+\',\n        default=None,\n        help=\'legend of each plot\')\n    parser_plt.add_argument(\n        \'--backend\', type=str, default=None, help=\'backend of plt\')\n    parser_plt.add_argument(\n        \'--style\', type=str, default=\'dark\', help=\'style of plt\')\n    parser_plt.add_argument(\'--out\', type=str, default=None)\n\n\ndef add_time_parser(subparsers):\n    parser_time = subparsers.add_parser(\n        \'cal_train_time\',\n        help=\'parser for computing the average time per training iteration\')\n    parser_time.add_argument(\n        \'json_logs\',\n        type=str,\n        nargs=\'+\',\n        help=\'path of train log in json format\')\n    parser_time.add_argument(\n        \'--include-outliers\',\n        action=\'store_true\',\n        help=\'include the first value of every epoch when computing \'\n        \'the average time\')\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'Analyze Json Log\')\n    # currently only support plot curve and calculate average train time\n    subparsers = parser.add_subparsers(dest=\'task\', help=\'task parser\')\n    add_plot_parser(subparsers)\n    add_time_parser(subparsers)\n    args = parser.parse_args()\n    return args\n\n\ndef load_json_logs(json_logs):\n    # load and convert json_logs to log_dict, key is epoch, value is a sub dict\n    # keys of sub dict is different metrics, e.g. memory, bbox_mAP\n    # value of sub dict is a list of corresponding values of all iterations\n    log_dicts = [dict() for _ in json_logs]\n    for json_log, log_dict in zip(json_logs, log_dicts):\n        with open(json_log, \'r\') as log_file:\n            for line in log_file:\n                log = json.loads(line.strip())\n                # skip lines without `epoch` field\n                if \'epoch\' not in log:\n                    continue\n                epoch = log.pop(\'epoch\')\n                if epoch not in log_dict:\n                    log_dict[epoch] = defaultdict(list)\n                for k, v in log.items():\n                    log_dict[epoch][k].append(v)\n    return log_dicts\n\n\ndef main():\n    args = parse_args()\n\n    json_logs = args.json_logs\n    for json_log in json_logs:\n        assert json_log.endswith(\'.json\')\n\n    log_dicts = load_json_logs(json_logs)\n\n    eval(args.task)(log_dicts, args)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tools/benchmark.py,4,"b""import argparse\nimport time\n\nimport torch\nfrom mmcv import Config\nfrom mmcv.parallel import MMDataParallel\nfrom mmcv.runner import load_checkpoint\nfrom tools.fuse_conv_bn import fuse_module\n\nfrom mmdet.core import wrap_fp16_model\nfrom mmdet.datasets import build_dataloader, build_dataset\nfrom mmdet.models import build_detector\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='MMDet benchmark a model')\n    parser.add_argument('config', help='test config file path')\n    parser.add_argument('checkpoint', help='checkpoint file')\n    parser.add_argument(\n        '--log-interval', default=50, help='interval of logging')\n    parser.add_argument(\n        '--fuse-conv-bn',\n        action='store_true',\n        help='Whether to fuse conv and bn, this will slightly increase'\n        'the inference speed')\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n\n    cfg = Config.fromfile(args.config)\n    # set cudnn_benchmark\n    if cfg.get('cudnn_benchmark', False):\n        torch.backends.cudnn.benchmark = True\n    cfg.model.pretrained = None\n    cfg.data.test.test_mode = True\n\n    # build the dataloader\n    # TODO: support multiple images per gpu (only minor changes are needed)\n    dataset = build_dataset(cfg.data.test)\n    data_loader = build_dataloader(\n        dataset,\n        samples_per_gpu=1,\n        workers_per_gpu=cfg.data.workers_per_gpu,\n        dist=False,\n        shuffle=False)\n\n    # build the model and load checkpoint\n    model = build_detector(cfg.model, train_cfg=None, test_cfg=cfg.test_cfg)\n    fp16_cfg = cfg.get('fp16', None)\n    if fp16_cfg is not None:\n        wrap_fp16_model(model)\n    load_checkpoint(model, args.checkpoint, map_location='cpu')\n    if args.fuse_conv_bn:\n        model = fuse_module(model)\n\n    model = MMDataParallel(model, device_ids=[0])\n\n    model.eval()\n\n    # the first several iterations may be very slow so skip them\n    num_warmup = 5\n    pure_inf_time = 0\n\n    # benchmark with 2000 image and take the average\n    for i, data in enumerate(data_loader):\n\n        torch.cuda.synchronize()\n        start_time = time.perf_counter()\n\n        with torch.no_grad():\n            model(return_loss=False, rescale=True, **data)\n\n        torch.cuda.synchronize()\n        elapsed = time.perf_counter() - start_time\n\n        if i >= num_warmup:\n            pure_inf_time += elapsed\n            if (i + 1) % args.log_interval == 0:\n                fps = (i + 1 - num_warmup) / pure_inf_time\n                print(f'Done image [{i + 1:<3}/ 2000], fps: {fps:.1f} img / s')\n\n        if (i + 1) == 2000:\n            pure_inf_time += elapsed\n            fps = (i + 1 - num_warmup) / pure_inf_time\n            print(f'Overall fps: {fps:.1f} img / s')\n            break\n\n\nif __name__ == '__main__':\n    main()\n"""
tools/browse_dataset.py,0,"b""import argparse\nimport os\nfrom pathlib import Path\n\nimport mmcv\nfrom mmcv import Config\n\nfrom mmdet.datasets.builder import build_dataset\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Browse a dataset')\n    parser.add_argument('config', help='train config file path')\n    parser.add_argument(\n        '--skip-type',\n        type=str,\n        nargs='+',\n        default=['DefaultFormatBundle', 'Normalize', 'Collect'],\n        help='skip some useless pipeline')\n    parser.add_argument(\n        '--output-dir',\n        default=None,\n        type=str,\n        help='If there is no display interface, you can save it')\n    parser.add_argument('--not-show', default=False, action='store_true')\n    parser.add_argument(\n        '--show-interval',\n        type=int,\n        default=999,\n        help='the interval of show (ms)')\n    args = parser.parse_args()\n    return args\n\n\ndef retrieve_data_cfg(config_path, skip_type):\n    cfg = Config.fromfile(config_path)\n    train_data_cfg = cfg.data.train\n    train_data_cfg['pipeline'] = [\n        x for x in train_data_cfg.pipeline if x['type'] not in skip_type\n    ]\n\n    return cfg\n\n\ndef main():\n    args = parse_args()\n    cfg = retrieve_data_cfg(args.config, args.skip_type)\n\n    dataset = build_dataset(cfg.data.train)\n\n    progress_bar = mmcv.ProgressBar(len(dataset))\n    for item in dataset:\n        filename = os.path.join(args.output_dir,\n                                Path(item['filename']).name\n                                ) if args.output_dir is not None else None\n        mmcv.imshow_det_bboxes(\n            item['img'],\n            item['gt_bboxes'],\n            item['gt_labels'] - 1,\n            class_names=dataset.CLASSES,\n            show=not args.not_show,\n            out_file=filename,\n            wait_time=args.show_interval)\n        progress_bar.update()\n\n\nif __name__ == '__main__':\n    main()\n"""
tools/coco_error_analysis.py,0,"b'import copy\nimport os\nfrom argparse import ArgumentParser\nfrom multiprocessing import Pool\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\n\n\ndef makeplot(rs, ps, outDir, class_name, iou_type):\n    cs = np.vstack([\n        np.ones((2, 3)),\n        np.array([.31, .51, .74]),\n        np.array([.75, .31, .30]),\n        np.array([.36, .90, .38]),\n        np.array([.50, .39, .64]),\n        np.array([1, .6, 0])\n    ])\n    areaNames = [\'allarea\', \'small\', \'medium\', \'large\']\n    types = [\'C75\', \'C50\', \'Loc\', \'Sim\', \'Oth\', \'BG\', \'FN\']\n    for i in range(len(areaNames)):\n        area_ps = ps[..., i, 0]\n        figure_tile = iou_type + \'-\' + class_name + \'-\' + areaNames[i]\n        aps = [ps_.mean() for ps_ in area_ps]\n        ps_curve = [\n            ps_.mean(axis=1) if ps_.ndim > 1 else ps_ for ps_ in area_ps\n        ]\n        ps_curve.insert(0, np.zeros(ps_curve[0].shape))\n        fig = plt.figure()\n        ax = plt.subplot(111)\n        for k in range(len(types)):\n            ax.plot(rs, ps_curve[k + 1], color=[0, 0, 0], linewidth=0.5)\n            ax.fill_between(\n                rs,\n                ps_curve[k],\n                ps_curve[k + 1],\n                color=cs[k],\n                label=str(f\'[{aps[k]:.3f}]\' + types[k]))\n        plt.xlabel(\'recall\')\n        plt.ylabel(\'precision\')\n        plt.xlim(0, 1.)\n        plt.ylim(0, 1.)\n        plt.title(figure_tile)\n        plt.legend()\n        # plt.show()\n        fig.savefig(outDir + f\'/{figure_tile}.png\')\n        plt.close(fig)\n\n\ndef analyze_individual_category(k, cocoDt, cocoGt, catId, iou_type):\n    nm = cocoGt.loadCats(catId)[0]\n    print(f\'--------------analyzing {k + 1}-{nm[""name""]}---------------\')\n    ps_ = {}\n    dt = copy.deepcopy(cocoDt)\n    nm = cocoGt.loadCats(catId)[0]\n    imgIds = cocoGt.getImgIds()\n    dt_anns = dt.dataset[\'annotations\']\n    select_dt_anns = []\n    for ann in dt_anns:\n        if ann[\'category_id\'] == catId:\n            select_dt_anns.append(ann)\n    dt.dataset[\'annotations\'] = select_dt_anns\n    dt.createIndex()\n    # compute precision but ignore superclass confusion\n    gt = copy.deepcopy(cocoGt)\n    child_catIds = gt.getCatIds(supNms=[nm[\'supercategory\']])\n    for idx, ann in enumerate(gt.dataset[\'annotations\']):\n        if (ann[\'category_id\'] in child_catIds\n                and ann[\'category_id\'] != catId):\n            gt.dataset[\'annotations\'][idx][\'ignore\'] = 1\n            gt.dataset[\'annotations\'][idx][\'iscrowd\'] = 1\n            gt.dataset[\'annotations\'][idx][\'category_id\'] = catId\n    cocoEval = COCOeval(gt, copy.deepcopy(dt), iou_type)\n    cocoEval.params.imgIds = imgIds\n    cocoEval.params.maxDets = [100]\n    cocoEval.params.iouThrs = [.1]\n    cocoEval.params.useCats = 1\n    cocoEval.evaluate()\n    cocoEval.accumulate()\n    ps_supercategory = cocoEval.eval[\'precision\'][0, :, k, :, :]\n    ps_[\'ps_supercategory\'] = ps_supercategory\n    # compute precision but ignore any class confusion\n    gt = copy.deepcopy(cocoGt)\n    for idx, ann in enumerate(gt.dataset[\'annotations\']):\n        if ann[\'category_id\'] != catId:\n            gt.dataset[\'annotations\'][idx][\'ignore\'] = 1\n            gt.dataset[\'annotations\'][idx][\'iscrowd\'] = 1\n            gt.dataset[\'annotations\'][idx][\'category_id\'] = catId\n    cocoEval = COCOeval(gt, copy.deepcopy(dt), iou_type)\n    cocoEval.params.imgIds = imgIds\n    cocoEval.params.maxDets = [100]\n    cocoEval.params.iouThrs = [.1]\n    cocoEval.params.useCats = 1\n    cocoEval.evaluate()\n    cocoEval.accumulate()\n    ps_allcategory = cocoEval.eval[\'precision\'][0, :, k, :, :]\n    ps_[\'ps_allcategory\'] = ps_allcategory\n    return k, ps_\n\n\ndef analyze_results(res_file, ann_file, res_types, out_dir):\n    for res_type in res_types:\n        assert res_type in [\'bbox\', \'segm\']\n\n    directory = os.path.dirname(out_dir + \'/\')\n    if not os.path.exists(directory):\n        print(f\'-------------create {out_dir}-----------------\')\n        os.makedirs(directory)\n\n    cocoGt = COCO(ann_file)\n    cocoDt = cocoGt.loadRes(res_file)\n    imgIds = cocoGt.getImgIds()\n    for res_type in res_types:\n        res_out_dir = out_dir + \'/\' + res_type + \'/\'\n        res_directory = os.path.dirname(res_out_dir)\n        if not os.path.exists(res_directory):\n            print(f\'-------------create {res_out_dir}-----------------\')\n            os.makedirs(res_directory)\n        iou_type = res_type\n        cocoEval = COCOeval(\n            copy.deepcopy(cocoGt), copy.deepcopy(cocoDt), iou_type)\n        cocoEval.params.imgIds = imgIds\n        cocoEval.params.iouThrs = [.75, .5, .1]\n        cocoEval.params.maxDets = [100]\n        cocoEval.evaluate()\n        cocoEval.accumulate()\n        ps = cocoEval.eval[\'precision\']\n        ps = np.vstack([ps, np.zeros((4, *ps.shape[1:]))])\n        catIds = cocoGt.getCatIds()\n        recThrs = cocoEval.params.recThrs\n        with Pool(processes=48) as pool:\n            args = [(k, cocoDt, cocoGt, catId, iou_type)\n                    for k, catId in enumerate(catIds)]\n            analyze_results = pool.starmap(analyze_individual_category, args)\n        for k, catId in enumerate(catIds):\n            nm = cocoGt.loadCats(catId)[0]\n            print(f\'--------------saving {k + 1}-{nm[""name""]}---------------\')\n            analyze_result = analyze_results[k]\n            assert k == analyze_result[0]\n            ps_supercategory = analyze_result[1][\'ps_supercategory\']\n            ps_allcategory = analyze_result[1][\'ps_allcategory\']\n            # compute precision but ignore superclass confusion\n            ps[3, :, k, :, :] = ps_supercategory\n            # compute precision but ignore any class confusion\n            ps[4, :, k, :, :] = ps_allcategory\n            # fill in background and false negative errors and plot\n            ps[ps == -1] = 0\n            ps[5, :, k, :, :] = (ps[4, :, k, :, :] > 0)\n            ps[6, :, k, :, :] = 1.0\n            makeplot(recThrs, ps[:, :, k], res_out_dir, nm[\'name\'], iou_type)\n        makeplot(recThrs, ps, res_out_dir, \'allclass\', iou_type)\n\n\ndef main():\n    parser = ArgumentParser(description=\'COCO Error Analysis Tool\')\n    parser.add_argument(\'result\', help=\'result file (json format) path\')\n    parser.add_argument(\'out_dir\', help=\'dir to save analyze result images\')\n    parser.add_argument(\n        \'--ann\',\n        default=\'data/coco/annotations/instances_val2017.json\',\n        help=\'annotation file path\')\n    parser.add_argument(\n        \'--types\', type=str, nargs=\'+\', default=[\'bbox\'], help=\'result types\')\n    args = parser.parse_args()\n    analyze_results(args.result, args.ann, args.types, out_dir=args.out_dir)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tools/detectron2pytorch.py,7,"b'import argparse\nfrom collections import OrderedDict\n\nimport mmcv\nimport torch\n\narch_settings = {50: (3, 4, 6, 3), 101: (3, 4, 23, 3)}\n\n\ndef convert_bn(blobs, state_dict, caffe_name, torch_name, converted_names):\n    # detectron replace bn with affine channel layer\n    state_dict[torch_name + \'.bias\'] = torch.from_numpy(blobs[caffe_name +\n                                                              \'_b\'])\n    state_dict[torch_name + \'.weight\'] = torch.from_numpy(blobs[caffe_name +\n                                                                \'_s\'])\n    bn_size = state_dict[torch_name + \'.weight\'].size()\n    state_dict[torch_name + \'.running_mean\'] = torch.zeros(bn_size)\n    state_dict[torch_name + \'.running_var\'] = torch.ones(bn_size)\n    converted_names.add(caffe_name + \'_b\')\n    converted_names.add(caffe_name + \'_s\')\n\n\ndef convert_conv_fc(blobs, state_dict, caffe_name, torch_name,\n                    converted_names):\n    state_dict[torch_name + \'.weight\'] = torch.from_numpy(blobs[caffe_name +\n                                                                \'_w\'])\n    converted_names.add(caffe_name + \'_w\')\n    if caffe_name + \'_b\' in blobs:\n        state_dict[torch_name + \'.bias\'] = torch.from_numpy(blobs[caffe_name +\n                                                                  \'_b\'])\n        converted_names.add(caffe_name + \'_b\')\n\n\ndef convert(src, dst, depth):\n    """"""Convert keys in detectron pretrained ResNet models to pytorch style.""""""\n    # load arch_settings\n    if depth not in arch_settings:\n        raise ValueError(\'Only support ResNet-50 and ResNet-101 currently\')\n    block_nums = arch_settings[depth]\n    # load caffe model\n    caffe_model = mmcv.load(src, encoding=\'latin1\')\n    blobs = caffe_model[\'blobs\'] if \'blobs\' in caffe_model else caffe_model\n    # convert to pytorch style\n    state_dict = OrderedDict()\n    converted_names = set()\n    convert_conv_fc(blobs, state_dict, \'conv1\', \'conv1\', converted_names)\n    convert_bn(blobs, state_dict, \'res_conv1_bn\', \'bn1\', converted_names)\n    for i in range(1, len(block_nums) + 1):\n        for j in range(block_nums[i - 1]):\n            if j == 0:\n                convert_conv_fc(blobs, state_dict, f\'res{i + 1}_{j}_branch1\',\n                                f\'layer{i}.{j}.downsample.0\', converted_names)\n                convert_bn(blobs, state_dict, f\'res{i + 1}_{j}_branch1_bn\',\n                           f\'layer{i}.{j}.downsample.1\', converted_names)\n            for k, letter in enumerate([\'a\', \'b\', \'c\']):\n                convert_conv_fc(blobs, state_dict,\n                                f\'res{i + 1}_{j}_branch2{letter}\',\n                                f\'layer{i}.{j}.conv{k+1}\', converted_names)\n                convert_bn(blobs, state_dict,\n                           f\'res{i + 1}_{j}_branch2{letter}_bn\',\n                           f\'layer{i}.{j}.bn{k + 1}\', converted_names)\n    # check if all layers are converted\n    for key in blobs:\n        if key not in converted_names:\n            print(f\'Not Convert: {key}\')\n    # save checkpoint\n    checkpoint = dict()\n    checkpoint[\'state_dict\'] = state_dict\n    torch.save(checkpoint, dst)\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Convert model keys\')\n    parser.add_argument(\'src\', help=\'src detectron model path\')\n    parser.add_argument(\'dst\', help=\'save path\')\n    parser.add_argument(\'depth\', type=int, help=\'ResNet model depth\')\n    args = parser.parse_args()\n    convert(args.src, args.dst, args.depth)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tools/fuse_conv_bn.py,3,"b'import argparse\n\nimport torch\nimport torch.nn as nn\nfrom mmcv.runner import save_checkpoint\n\nfrom mmdet.apis import init_detector\n\n\ndef fuse_conv_bn(conv, bn):\n    """""" During inference, the functionary of batch norm layers is turned off\n    but only the mean and var alone channels are used, which exposes the\n    chance to fuse it with the preceding conv layers to save computations and\n    simplify network structures.\n    """"""\n    conv_w = conv.weight\n    conv_b = conv.bias if conv.bias is not None else torch.zeros_like(\n        bn.running_mean)\n\n    factor = bn.weight / torch.sqrt(bn.running_var + bn.eps)\n    conv.weight = nn.Parameter(conv_w *\n                               factor.reshape([conv.out_channels, 1, 1, 1]))\n    conv.bias = nn.Parameter((conv_b - bn.running_mean) * factor + bn.bias)\n    return conv\n\n\ndef fuse_module(m):\n    last_conv = None\n    last_conv_name = None\n\n    for name, child in m.named_children():\n        if isinstance(child, (nn.BatchNorm2d, nn.SyncBatchNorm)):\n            if last_conv is None:  # only fuse BN that is after Conv\n                continue\n            fused_conv = fuse_conv_bn(last_conv, child)\n            m._modules[last_conv_name] = fused_conv\n            # To reduce changes, set BN as Identity instead of deleting it.\n            m._modules[name] = nn.Identity()\n            last_conv = None\n        elif isinstance(child, nn.Conv2d):\n            last_conv = child\n            last_conv_name = name\n        else:\n            fuse_module(child)\n    return m\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=\'fuse Conv and BN layers in a model\')\n    parser.add_argument(\'config\', help=\'config file path\')\n    parser.add_argument(\'checkpoint\', help=\'checkpoint file path\')\n    parser.add_argument(\'out\', help=\'output path of the converted model\')\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n    # build the model from a config file and a checkpoint file\n    model = init_detector(args.config, args.checkpoint)\n    # fuse conv and bn layers of the model\n    fused_model = fuse_module(model)\n    save_checkpoint(fused_model, args.out)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tools/get_flops.py,0,"b""import argparse\n\nfrom mmcv import Config\n\nfrom mmdet.models import build_detector\nfrom mmdet.utils import get_model_complexity_info\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Train a detector')\n    parser.add_argument('config', help='train config file path')\n    parser.add_argument(\n        '--shape',\n        type=int,\n        nargs='+',\n        default=[1280, 800],\n        help='input image size')\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n\n    args = parse_args()\n\n    if len(args.shape) == 1:\n        input_shape = (3, args.shape[0], args.shape[0])\n    elif len(args.shape) == 2:\n        input_shape = (3, ) + tuple(args.shape)\n    else:\n        raise ValueError('invalid input shape')\n\n    cfg = Config.fromfile(args.config)\n    model = build_detector(\n        cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg).cuda()\n    model.eval()\n\n    if hasattr(model, 'forward_dummy'):\n        model.forward = model.forward_dummy\n    else:\n        raise NotImplementedError(\n            'FLOPs counter is currently not currently supported with {}'.\n            format(model.__class__.__name__))\n\n    flops, params = get_model_complexity_info(model, input_shape)\n    split_line = '=' * 30\n    print(f'{split_line}\\nInput shape: {input_shape}\\n'\n          f'Flops: {flops}\\nParams: {params}\\n{split_line}')\n    print('!!!Please be cautious if you use the results in papers. '\n          'You may need to check if all ops are supported and verify that the '\n          'flops computation is correct.')\n\n\nif __name__ == '__main__':\n    main()\n"""
tools/print_config.py,0,"b""import argparse\n\nfrom mmcv import Config, DictAction\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Print the whole config')\n    parser.add_argument('config', help='config file path')\n    parser.add_argument(\n        '--options', nargs='+', action=DictAction, help='arguments in dict')\n    args = parser.parse_args()\n\n    return args\n\n\ndef main():\n    args = parse_args()\n\n    cfg = Config.fromfile(args.config)\n    if args.options is not None:\n        cfg.merge_from_dict(args.options)\n    print(f'Config:\\n{cfg.pretty_text}')\n\n\nif __name__ == '__main__':\n    main()\n"""
tools/publish_model.py,2,"b""import argparse\nimport subprocess\n\nimport torch\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description='Process a checkpoint to be published')\n    parser.add_argument('in_file', help='input checkpoint filename')\n    parser.add_argument('out_file', help='output checkpoint filename')\n    args = parser.parse_args()\n    return args\n\n\ndef process_checkpoint(in_file, out_file):\n    checkpoint = torch.load(in_file, map_location='cpu')\n    # remove optimizer for smaller file size\n    if 'optimizer' in checkpoint:\n        del checkpoint['optimizer']\n    # if it is necessary to remove some sensitive data in checkpoint['meta'],\n    # add the code here.\n    torch.save(checkpoint, out_file)\n    sha = subprocess.check_output(['sha256sum', out_file]).decode()\n    final_file = out_file.rstrip('.pth') + f'-{sha[:8]}.pth'\n    subprocess.Popen(['mv', out_file, final_file])\n\n\ndef main():\n    args = parse_args()\n    process_checkpoint(args.in_file, args.out_file)\n\n\nif __name__ == '__main__':\n    main()\n"""
tools/pytorch2onnx.py,5,"b'import argparse\nimport io\n\nimport mmcv\nimport onnx\nimport torch\nfrom mmcv.runner import load_checkpoint\nfrom onnx import optimizer\nfrom torch.onnx import OperatorExportTypes\n\nfrom mmdet.models import build_detector\nfrom mmdet.ops import RoIAlign, RoIPool\n\n\ndef export_onnx_model(model, inputs, passes):\n    """"""\n    Trace and export a model to onnx format.\n    Modified from https://github.com/facebookresearch/detectron2/\n\n    Args:\n        model (nn.Module):\n        inputs (tuple[args]): the model will be called by `model(*inputs)`\n        passes (None or list[str]): the optimization passed for ONNX model\n\n    Returns:\n        an onnx model\n    """"""\n    assert isinstance(model, torch.nn.Module)\n\n    # make sure all modules are in eval mode, onnx may change the training\n    # state of the module if the states are not consistent\n    def _check_eval(module):\n        assert not module.training\n\n    model.apply(_check_eval)\n\n    # Export the model to ONNX\n    with torch.no_grad():\n        with io.BytesIO() as f:\n            torch.onnx.export(\n                model,\n                inputs,\n                f,\n                operator_export_type=OperatorExportTypes.ONNX_ATEN_FALLBACK,\n                # verbose=True,  # NOTE: uncomment this for debugging\n                # export_params=True,\n            )\n            onnx_model = onnx.load_from_string(f.getvalue())\n\n    # Apply ONNX\'s Optimization\n    if passes is not None:\n        all_passes = optimizer.get_available_passes()\n        assert all(p in all_passes for p in passes), \\\n            f\'Only {all_passes} are supported\'\n    onnx_model = optimizer.optimize(onnx_model, passes)\n    return onnx_model\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=\'MMDet pytorch model conversion to ONNX\')\n    parser.add_argument(\'config\', help=\'test config file path\')\n    parser.add_argument(\'checkpoint\', help=\'checkpoint file\')\n    parser.add_argument(\n        \'--out\', type=str, required=True, help=\'output ONNX filename\')\n    parser.add_argument(\n        \'--shape\',\n        type=int,\n        nargs=\'+\',\n        default=[1280, 800],\n        help=\'input image size\')\n    parser.add_argument(\n        \'--passes\', type=str, nargs=\'+\', help=\'ONNX optimization passes\')\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n\n    if not args.out.endswith(\'.onnx\'):\n        raise ValueError(\'The output file must be a onnx file.\')\n\n    if len(args.shape) == 1:\n        input_shape = (3, args.shape[0], args.shape[0])\n    elif len(args.shape) == 2:\n        input_shape = (3, ) + tuple(args.shape)\n    else:\n        raise ValueError(\'invalid input shape\')\n\n    cfg = mmcv.Config.fromfile(args.config)\n    cfg.model.pretrained = None\n\n    # build the model and load checkpoint\n    model = build_detector(cfg.model, train_cfg=None, test_cfg=cfg.test_cfg)\n    load_checkpoint(model, args.checkpoint, map_location=\'cpu\')\n    # Only support CPU mode for now\n    model.cpu().eval()\n    # Customized ops are not supported, use torchvision ops instead.\n    for m in model.modules():\n        if isinstance(m, (RoIPool, RoIAlign)):\n            # set use_torchvision on-the-fly\n            m.use_torchvision = True\n\n    # TODO: a better way to override forward function\n    if hasattr(model, \'forward_dummy\'):\n        model.forward = model.forward_dummy\n    else:\n        raise NotImplementedError(\n            \'ONNX conversion is currently not currently supported with \'\n            f\'{model.__class__.__name__}\')\n\n    input_data = torch.empty((1, *input_shape),\n                             dtype=next(model.parameters()).dtype,\n                             device=next(model.parameters()).device)\n\n    onnx_model = export_onnx_model(model, (input_data, ), args.passes)\n    # Print a human readable representation of the graph\n    onnx.helper.printable_graph(onnx_model.graph)\n    print(f\'saving model in {args.out}\')\n    onnx.save(onnx_model, args.out)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tools/regnet2mmdet.py,2,"b'import argparse\nfrom collections import OrderedDict\n\nimport torch\n\n\ndef convert_stem(model_key, model_weight, state_dict, converted_names):\n    new_key = model_key.replace(\'stem.conv\', \'conv1\')\n    new_key = new_key.replace(\'stem.bn\', \'bn1\')\n    state_dict[new_key] = model_weight\n    converted_names.add(model_key)\n    print(f\'Convert {model_key} to {new_key}\')\n\n\ndef convert_head(model_key, model_weight, state_dict, converted_names):\n    new_key = model_key.replace(\'head.fc\', \'fc\')\n    state_dict[new_key] = model_weight\n    converted_names.add(model_key)\n    print(f\'Convert {model_key} to {new_key}\')\n\n\ndef convert_reslayer(model_key, model_weight, state_dict, converted_names):\n    split_keys = model_key.split(\'.\')\n    layer, block, module = split_keys[:3]\n    block_id = int(block[1:])\n    layer_name = f\'layer{int(layer[1:])}\'\n    block_name = f\'{block_id - 1}\'\n\n    if block_id == 1 and module == \'bn\':\n        new_key = f\'{layer_name}.{block_name}.downsample.1.{split_keys[-1]}\'\n    elif block_id == 1 and module == \'proj\':\n        new_key = f\'{layer_name}.{block_name}.downsample.0.{split_keys[-1]}\'\n    elif module == \'f\':\n        if split_keys[3] == \'a_bn\':\n            module_name = \'bn1\'\n        elif split_keys[3] == \'b_bn\':\n            module_name = \'bn2\'\n        elif split_keys[3] == \'c_bn\':\n            module_name = \'bn3\'\n        elif split_keys[3] == \'a\':\n            module_name = \'conv1\'\n        elif split_keys[3] == \'b\':\n            module_name = \'conv2\'\n        elif split_keys[3] == \'c\':\n            module_name = \'conv3\'\n        new_key = f\'{layer_name}.{block_name}.{module_name}.{split_keys[-1]}\'\n    else:\n        raise ValueError(f\'Unsupported conversion of key {model_key}\')\n    print(f\'Convert {model_key} to {new_key}\')\n    state_dict[new_key] = model_weight\n    converted_names.add(model_key)\n\n\ndef convert(src, dst):\n    """"""Convert keys in pycls pretrained RegNet models to mmdet style.""""""\n    # load caffe model\n    regnet_model = torch.load(src)\n    blobs = regnet_model[\'model_state\']\n    # convert to pytorch style\n    state_dict = OrderedDict()\n    converted_names = set()\n    for key, weight in blobs.items():\n        if \'stem\' in key:\n            convert_stem(key, weight, state_dict, converted_names)\n        elif \'head\' in key:\n            convert_head(key, weight, state_dict, converted_names)\n        elif key.startswith(\'s\'):\n            convert_reslayer(key, weight, state_dict, converted_names)\n\n    # check if all layers are converted\n    for key in blobs:\n        if key not in converted_names:\n            print(f\'not converted: {key}\')\n    # save checkpoint\n    checkpoint = dict()\n    checkpoint[\'state_dict\'] = state_dict\n    torch.save(checkpoint, dst)\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Convert model keys\')\n    parser.add_argument(\'src\', help=\'src detectron model path\')\n    parser.add_argument(\'dst\', help=\'save path\')\n    args = parser.parse_args()\n    convert(args.src, args.dst)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tools/robustness_eval.py,0,"b""import os.path as osp\nfrom argparse import ArgumentParser\n\nimport mmcv\nimport numpy as np\n\n\ndef print_coco_results(results):\n\n    def _print(result, ap=1, iouThr=None, areaRng='all', maxDets=100):\n        titleStr = 'Average Precision' if ap == 1 else 'Average Recall'\n        typeStr = '(AP)' if ap == 1 else '(AR)'\n        iouStr = '0.50:0.95' \\\n            if iouThr is None else f'{iouThr:0.2f}'\n        iStr = f' {titleStr:<18} {typeStr} @[ IoU={iouStr:<9} | '\n        iStr += f'area={areaRng:>6s} | maxDets={maxDets:>3d} ] = {result:0.3f}'\n        print(iStr)\n\n    stats = np.zeros((12, ))\n    stats[0] = _print(results[0], 1)\n    stats[1] = _print(results[1], 1, iouThr=.5)\n    stats[2] = _print(results[2], 1, iouThr=.75)\n    stats[3] = _print(results[3], 1, areaRng='small')\n    stats[4] = _print(results[4], 1, areaRng='medium')\n    stats[5] = _print(results[5], 1, areaRng='large')\n    stats[6] = _print(results[6], 0, maxDets=1)\n    stats[7] = _print(results[7], 0, maxDets=10)\n    stats[8] = _print(results[8], 0)\n    stats[9] = _print(results[9], 0, areaRng='small')\n    stats[10] = _print(results[10], 0, areaRng='medium')\n    stats[11] = _print(results[11], 0, areaRng='large')\n\n\ndef get_coco_style_results(filename,\n                           task='bbox',\n                           metric=None,\n                           prints='mPC',\n                           aggregate='benchmark'):\n\n    assert aggregate in ['benchmark', 'all']\n\n    if prints == 'all':\n        prints = ['P', 'mPC', 'rPC']\n    elif isinstance(prints, str):\n        prints = [prints]\n    for p in prints:\n        assert p in ['P', 'mPC', 'rPC']\n\n    if metric is None:\n        metrics = [\n            'AP', 'AP50', 'AP75', 'APs', 'APm', 'APl', 'AR1', 'AR10', 'AR100',\n            'ARs', 'ARm', 'ARl'\n        ]\n    elif isinstance(metric, list):\n        metrics = metric\n    else:\n        metrics = [metric]\n\n    for metric_name in metrics:\n        assert metric_name in [\n            'AP', 'AP50', 'AP75', 'APs', 'APm', 'APl', 'AR1', 'AR10', 'AR100',\n            'ARs', 'ARm', 'ARl'\n        ]\n\n    eval_output = mmcv.load(filename)\n\n    num_distortions = len(list(eval_output.keys()))\n    results = np.zeros((num_distortions, 6, len(metrics)), dtype='float32')\n\n    for corr_i, distortion in enumerate(eval_output):\n        for severity in eval_output[distortion]:\n            for metric_j, metric_name in enumerate(metrics):\n                mAP = eval_output[distortion][severity][task][metric_name]\n                results[corr_i, severity, metric_j] = mAP\n\n    P = results[0, 0, :]\n    if aggregate == 'benchmark':\n        mPC = np.mean(results[:15, 1:, :], axis=(0, 1))\n    else:\n        mPC = np.mean(results[:, 1:, :], axis=(0, 1))\n    rPC = mPC / P\n\n    print(f'\\nmodel: {osp.basename(filename)}')\n    if metric is None:\n        if 'P' in prints:\n            print(f'Performance on Clean Data [P] ({task})')\n            print_coco_results(P)\n        if 'mPC' in prints:\n            print(f'Mean Performance under Corruption [mPC] ({task})')\n            print_coco_results(mPC)\n        if 'rPC' in prints:\n            print(f'Realtive Performance under Corruption [rPC] ({task})')\n            print_coco_results(rPC)\n    else:\n        if 'P' in prints:\n            print(f'Performance on Clean Data [P] ({task})')\n            for metric_i, metric_name in enumerate(metrics):\n                print(f'{metric_name:5} =  {P[metric_i]:0.3f}')\n        if 'mPC' in prints:\n            print(f'Mean Performance under Corruption [mPC] ({task})')\n            for metric_i, metric_name in enumerate(metrics):\n                print(f'{metric_name:5} =  {mPC[metric_i]:0.3f}')\n        if 'rPC' in prints:\n            print(f'Relative Performance under Corruption [rPC] ({task})')\n            for metric_i, metric_name in enumerate(metrics):\n                print(f'{metric_name:5} => {rPC[metric_i] * 100:0.1f} %')\n\n    return results\n\n\ndef get_voc_style_results(filename, prints='mPC', aggregate='benchmark'):\n\n    assert aggregate in ['benchmark', 'all']\n\n    if prints == 'all':\n        prints = ['P', 'mPC', 'rPC']\n    elif isinstance(prints, str):\n        prints = [prints]\n    for p in prints:\n        assert p in ['P', 'mPC', 'rPC']\n\n    eval_output = mmcv.load(filename)\n\n    num_distortions = len(list(eval_output.keys()))\n    results = np.zeros((num_distortions, 6, 20), dtype='float32')\n\n    for i, distortion in enumerate(eval_output):\n        for severity in eval_output[distortion]:\n            mAP = [\n                eval_output[distortion][severity][j]['ap']\n                for j in range(len(eval_output[distortion][severity]))\n            ]\n            results[i, severity, :] = mAP\n\n    P = results[0, 0, :]\n    if aggregate == 'benchmark':\n        mPC = np.mean(results[:15, 1:, :], axis=(0, 1))\n    else:\n        mPC = np.mean(results[:, 1:, :], axis=(0, 1))\n    rPC = mPC / P\n\n    print(f'\\nmodel: {osp.basename(filename)}')\n    if 'P' in prints:\n        print(f'Performance on Clean Data [P] in AP50 = {np.mean(P):0.3f}')\n    if 'mPC' in prints:\n        print('Mean Performance under Corruption [mPC] in AP50 = '\n              f'{np.mean(mPC):0.3f}')\n    if 'rPC' in prints:\n        print('Realtive Performance under Corruption [rPC] in % = '\n              f'{np.mean(rPC) * 100:0.1f}')\n\n    return np.mean(results, axis=2, keepdims=True)\n\n\ndef get_results(filename,\n                dataset='coco',\n                task='bbox',\n                metric=None,\n                prints='mPC',\n                aggregate='benchmark'):\n    assert dataset in ['coco', 'voc', 'cityscapes']\n\n    if dataset in ['coco', 'cityscapes']:\n        results = get_coco_style_results(\n            filename,\n            task=task,\n            metric=metric,\n            prints=prints,\n            aggregate=aggregate)\n    elif dataset == 'voc':\n        if task != 'bbox':\n            print('Only bbox analysis is supported for Pascal VOC')\n            print('Will report bbox results\\n')\n        if metric not in [None, ['AP'], ['AP50']]:\n            print('Only the AP50 metric is supported for Pascal VOC')\n            print('Will report AP50 metric\\n')\n        results = get_voc_style_results(\n            filename, prints=prints, aggregate=aggregate)\n\n    return results\n\n\ndef get_distortions_from_file(filename):\n\n    eval_output = mmcv.load(filename)\n\n    return get_distortions_from_results(eval_output)\n\n\ndef get_distortions_from_results(eval_output):\n    distortions = []\n    for i, distortion in enumerate(eval_output):\n        distortions.append(distortion.replace('_', ' '))\n    return distortions\n\n\ndef main():\n    parser = ArgumentParser(description='Corruption Result Analysis')\n    parser.add_argument('filename', help='result file path')\n    parser.add_argument(\n        '--dataset',\n        type=str,\n        choices=['coco', 'voc', 'cityscapes'],\n        default='coco',\n        help='dataset type')\n    parser.add_argument(\n        '--task',\n        type=str,\n        nargs='+',\n        choices=['bbox', 'segm'],\n        default=['bbox'],\n        help='task to report')\n    parser.add_argument(\n        '--metric',\n        nargs='+',\n        choices=[\n            None, 'AP', 'AP50', 'AP75', 'APs', 'APm', 'APl', 'AR1', 'AR10',\n            'AR100', 'ARs', 'ARm', 'ARl'\n        ],\n        default=None,\n        help='metric to report')\n    parser.add_argument(\n        '--prints',\n        type=str,\n        nargs='+',\n        choices=['P', 'mPC', 'rPC'],\n        default='mPC',\n        help='corruption benchmark metric to print')\n    parser.add_argument(\n        '--aggregate',\n        type=str,\n        choices=['all', 'benchmark'],\n        default='benchmark',\n        help='aggregate all results or only those \\\n        for benchmark corruptions')\n\n    args = parser.parse_args()\n\n    for task in args.task:\n        get_results(\n            args.filename,\n            dataset=args.dataset,\n            task=task,\n            metric=args.metric,\n            prints=args.prints,\n            aggregate=args.aggregate)\n\n\nif __name__ == '__main__':\n    main()\n"""
tools/test.py,2,"b'import argparse\nimport os\n\nimport mmcv\nimport torch\nfrom mmcv import Config, DictAction\nfrom mmcv.parallel import MMDataParallel, MMDistributedDataParallel\nfrom mmcv.runner import get_dist_info, init_dist, load_checkpoint\nfrom tools.fuse_conv_bn import fuse_module\n\nfrom mmdet.apis import multi_gpu_test, single_gpu_test\nfrom mmdet.core import wrap_fp16_model\nfrom mmdet.datasets import build_dataloader, build_dataset\nfrom mmdet.models import build_detector\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=\'MMDet test (and eval) a model\')\n    parser.add_argument(\'config\', help=\'test config file path\')\n    parser.add_argument(\'checkpoint\', help=\'checkpoint file\')\n    parser.add_argument(\'--out\', help=\'output result file in pickle format\')\n    parser.add_argument(\n        \'--fuse-conv-bn\',\n        action=\'store_true\',\n        help=\'Whether to fuse conv and bn, this will slightly increase\'\n        \'the inference speed\')\n    parser.add_argument(\n        \'--format-only\',\n        action=\'store_true\',\n        help=\'Format the output results without perform evaluation. It is\'\n        \'useful when you want to format the result to a specific format and \'\n        \'submit it to the test server\')\n    parser.add_argument(\n        \'--eval\',\n        type=str,\n        nargs=\'+\',\n        help=\'evaluation metrics, which depends on the dataset, e.g., ""bbox"",\'\n        \' ""segm"", ""proposal"" for COCO, and ""mAP"", ""recall"" for PASCAL VOC\')\n    parser.add_argument(\'--show\', action=\'store_true\', help=\'show results\')\n    parser.add_argument(\n        \'--show-dir\', help=\'directory where painted images will be saved\')\n    parser.add_argument(\n        \'--show-score-thr\',\n        type=float,\n        default=0.3,\n        help=\'score threshold (default: 0.3)\')\n    parser.add_argument(\n        \'--gpu-collect\',\n        action=\'store_true\',\n        help=\'whether to use gpu to collect results.\')\n    parser.add_argument(\n        \'--tmpdir\',\n        help=\'tmp directory used for collecting results from multiple \'\n        \'workers, available when gpu-collect is not specified\')\n    parser.add_argument(\n        \'--options\', nargs=\'+\', action=DictAction, help=\'arguments in dict\')\n    parser.add_argument(\n        \'--launcher\',\n        choices=[\'none\', \'pytorch\', \'slurm\', \'mpi\'],\n        default=\'none\',\n        help=\'job launcher\')\n    parser.add_argument(\'--local_rank\', type=int, default=0)\n    args = parser.parse_args()\n    if \'LOCAL_RANK\' not in os.environ:\n        os.environ[\'LOCAL_RANK\'] = str(args.local_rank)\n    return args\n\n\ndef main():\n    args = parse_args()\n\n    assert args.out or args.eval or args.format_only or args.show \\\n        or args.show_dir, \\\n        (\'Please specify at least one operation (save/eval/format/show the \'\n         \'results / save the results) with the argument ""--out"", ""--eval""\'\n         \', ""--format-only"", ""--show"" or ""--show-dir""\')\n\n    if args.eval and args.format_only:\n        raise ValueError(\'--eval and --format_only cannot be both specified\')\n\n    if args.out is not None and not args.out.endswith((\'.pkl\', \'.pickle\')):\n        raise ValueError(\'The output file must be a pkl file.\')\n\n    cfg = Config.fromfile(args.config)\n    # set cudnn_benchmark\n    if cfg.get(\'cudnn_benchmark\', False):\n        torch.backends.cudnn.benchmark = True\n    cfg.model.pretrained = None\n    cfg.data.test.test_mode = True\n\n    # init distributed env first, since logger depends on the dist info.\n    if args.launcher == \'none\':\n        distributed = False\n    else:\n        distributed = True\n        init_dist(args.launcher, **cfg.dist_params)\n\n    # build the dataloader\n    # TODO: support multiple images per gpu (only minor changes are needed)\n    dataset = build_dataset(cfg.data.test)\n    data_loader = build_dataloader(\n        dataset,\n        samples_per_gpu=1,\n        workers_per_gpu=cfg.data.workers_per_gpu,\n        dist=distributed,\n        shuffle=False)\n\n    # build the model and load checkpoint\n    model = build_detector(cfg.model, train_cfg=None, test_cfg=cfg.test_cfg)\n    fp16_cfg = cfg.get(\'fp16\', None)\n    if fp16_cfg is not None:\n        wrap_fp16_model(model)\n    checkpoint = load_checkpoint(model, args.checkpoint, map_location=\'cpu\')\n    if args.fuse_conv_bn:\n        model = fuse_module(model)\n    # old versions did not save class info in checkpoints, this walkaround is\n    # for backward compatibility\n    if \'CLASSES\' in checkpoint[\'meta\']:\n        model.CLASSES = checkpoint[\'meta\'][\'CLASSES\']\n    else:\n        model.CLASSES = dataset.CLASSES\n\n    if not distributed:\n        model = MMDataParallel(model, device_ids=[0])\n        outputs = single_gpu_test(model, data_loader, args.show, args.show_dir,\n                                  args.show_score_thr)\n    else:\n        model = MMDistributedDataParallel(\n            model.cuda(),\n            device_ids=[torch.cuda.current_device()],\n            broadcast_buffers=False)\n        outputs = multi_gpu_test(model, data_loader, args.tmpdir,\n                                 args.gpu_collect)\n\n    rank, _ = get_dist_info()\n    if rank == 0:\n        if args.out:\n            print(f\'\\nwriting results to {args.out}\')\n            mmcv.dump(outputs, args.out)\n        kwargs = {} if args.options is None else args.options\n        if args.format_only:\n            dataset.format_results(outputs, **kwargs)\n        if args.eval:\n            dataset.evaluate(outputs, args.eval, **kwargs)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tools/test_robustness.py,9,"b'import argparse\nimport copy\nimport os\nimport os.path as osp\nimport shutil\nimport tempfile\n\nimport mmcv\nimport torch\nimport torch.distributed as dist\nfrom mmcv.parallel import MMDataParallel, MMDistributedDataParallel\nfrom mmcv.runner import get_dist_info, init_dist, load_checkpoint\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\nfrom robustness_eval import get_results\n\nfrom mmdet import datasets\nfrom mmdet.apis import set_random_seed\nfrom mmdet.core import encode_mask_results, eval_map, wrap_fp16_model\nfrom mmdet.datasets import build_dataloader, build_dataset\nfrom mmdet.models import build_detector\n\n\ndef coco_eval_with_return(result_files,\n                          result_types,\n                          coco,\n                          max_dets=(100, 300, 1000)):\n    for res_type in result_types:\n        assert res_type in [\'proposal\', \'bbox\', \'segm\', \'keypoints\']\n\n    if mmcv.is_str(coco):\n        coco = COCO(coco)\n    assert isinstance(coco, COCO)\n\n    eval_results = {}\n    for res_type in result_types:\n        result_file = result_files[res_type]\n        assert result_file.endswith(\'.json\')\n\n        coco_dets = coco.loadRes(result_file)\n        img_ids = coco.getImgIds()\n        iou_type = \'bbox\' if res_type == \'proposal\' else res_type\n        cocoEval = COCOeval(coco, coco_dets, iou_type)\n        cocoEval.params.imgIds = img_ids\n        if res_type == \'proposal\':\n            cocoEval.params.useCats = 0\n            cocoEval.params.maxDets = list(max_dets)\n        cocoEval.evaluate()\n        cocoEval.accumulate()\n        cocoEval.summarize()\n        if res_type == \'segm\' or res_type == \'bbox\':\n            metric_names = [\n                \'AP\', \'AP50\', \'AP75\', \'APs\', \'APm\', \'APl\', \'AR1\', \'AR10\',\n                \'AR100\', \'ARs\', \'ARm\', \'ARl\'\n            ]\n            eval_results[res_type] = {\n                metric_names[i]: cocoEval.stats[i]\n                for i in range(len(metric_names))\n            }\n        else:\n            eval_results[res_type] = cocoEval.stats\n\n    return eval_results\n\n\ndef voc_eval_with_return(result_file,\n                         dataset,\n                         iou_thr=0.5,\n                         logger=\'print\',\n                         only_ap=True):\n    det_results = mmcv.load(result_file)\n    annotations = [dataset.get_ann_info(i) for i in range(len(dataset))]\n    if hasattr(dataset, \'year\') and dataset.year == 2007:\n        dataset_name = \'voc07\'\n    else:\n        dataset_name = dataset.CLASSES\n    mean_ap, eval_results = eval_map(\n        det_results,\n        annotations,\n        scale_ranges=None,\n        iou_thr=iou_thr,\n        dataset=dataset_name,\n        logger=logger)\n\n    if only_ap:\n        eval_results = [{\n            \'ap\': eval_results[i][\'ap\']\n        } for i in range(len(eval_results))]\n\n    return mean_ap, eval_results\n\n\ndef single_gpu_test(model, data_loader, show=False):\n    model.eval()\n    results = []\n    dataset = data_loader.dataset\n    prog_bar = mmcv.ProgressBar(len(dataset))\n    for i, data in enumerate(data_loader):\n        with torch.no_grad():\n            result = model(return_loss=False, rescale=not show, **data)\n\n        if show:\n            model.module.show_result(data, result, dataset.img_norm_cfg)\n\n        # encode mask results\n        if isinstance(result, tuple):\n            bbox_results, mask_results = result\n            encoded_mask_results = encode_mask_results(mask_results)\n            result = bbox_results, encoded_mask_results\n        results.append(result)\n\n        batch_size = data[\'img\'][0].size(0)\n        for _ in range(batch_size):\n            prog_bar.update()\n    return results\n\n\ndef multi_gpu_test(model, data_loader, tmpdir=None):\n    model.eval()\n    results = []\n    dataset = data_loader.dataset\n    rank, world_size = get_dist_info()\n    if rank == 0:\n        prog_bar = mmcv.ProgressBar(len(dataset))\n    for i, data in enumerate(data_loader):\n        with torch.no_grad():\n            result = model(return_loss=False, rescale=True, **data)\n            # encode mask results\n            if isinstance(result, tuple):\n                bbox_results, mask_results = result\n                encoded_mask_results = encode_mask_results(mask_results)\n                result = bbox_results, encoded_mask_results\n        results.append(result)\n\n        results.append(result)\n\n        if rank == 0:\n            batch_size = data[\'img\'][0].size(0)\n            for _ in range(batch_size * world_size):\n                prog_bar.update()\n\n    # collect results from all ranks\n    results = collect_results(results, len(dataset), tmpdir)\n\n    return results\n\n\ndef collect_results(result_part, size, tmpdir=None):\n    rank, world_size = get_dist_info()\n    # create a tmp dir if it is not specified\n    if tmpdir is None:\n        MAX_LEN = 512\n        # 32 is whitespace\n        dir_tensor = torch.full((MAX_LEN, ),\n                                32,\n                                dtype=torch.uint8,\n                                device=\'cuda\')\n        if rank == 0:\n            tmpdir = tempfile.mkdtemp()\n            tmpdir = torch.tensor(\n                bytearray(tmpdir.encode()), dtype=torch.uint8, device=\'cuda\')\n            dir_tensor[:len(tmpdir)] = tmpdir\n        dist.broadcast(dir_tensor, 0)\n        tmpdir = dir_tensor.cpu().numpy().tobytes().decode().rstrip()\n    else:\n        mmcv.mkdir_or_exist(tmpdir)\n    # dump the part result to the dir\n    mmcv.dump(result_part, osp.join(tmpdir, f\'part_{rank}.pkl\'))\n    dist.barrier()\n    # collect all parts\n    if rank != 0:\n        return None\n    else:\n        # load results of all parts from tmp dir\n        part_list = []\n        for i in range(world_size):\n            part_file = osp.join(tmpdir, f\'part_{i}.pkl\')\n            part_list.append(mmcv.load(part_file))\n        # sort the results\n        ordered_results = []\n        for res in zip(*part_list):\n            ordered_results.extend(list(res))\n        # the dataloader may pad some samples\n        ordered_results = ordered_results[:size]\n        # remove tmp dir\n        shutil.rmtree(tmpdir)\n        return ordered_results\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'MMDet test detector\')\n    parser.add_argument(\'config\', help=\'test config file path\')\n    parser.add_argument(\'checkpoint\', help=\'checkpoint file\')\n    parser.add_argument(\'--out\', help=\'output result file\')\n    parser.add_argument(\n        \'--corruptions\',\n        type=str,\n        nargs=\'+\',\n        default=\'benchmark\',\n        choices=[\n            \'all\', \'benchmark\', \'noise\', \'blur\', \'weather\', \'digital\',\n            \'holdout\', \'None\', \'gaussian_noise\', \'shot_noise\', \'impulse_noise\',\n            \'defocus_blur\', \'glass_blur\', \'motion_blur\', \'zoom_blur\', \'snow\',\n            \'frost\', \'fog\', \'brightness\', \'contrast\', \'elastic_transform\',\n            \'pixelate\', \'jpeg_compression\', \'speckle_noise\', \'gaussian_blur\',\n            \'spatter\', \'saturate\'\n        ],\n        help=\'corruptions\')\n    parser.add_argument(\n        \'--severities\',\n        type=int,\n        nargs=\'+\',\n        default=[0, 1, 2, 3, 4, 5],\n        help=\'corruption severity levels\')\n    parser.add_argument(\n        \'--eval\',\n        type=str,\n        nargs=\'+\',\n        choices=[\'proposal\', \'proposal_fast\', \'bbox\', \'segm\', \'keypoints\'],\n        help=\'eval types\')\n    parser.add_argument(\n        \'--iou-thr\',\n        type=float,\n        default=0.5,\n        help=\'IoU threshold for pascal voc evaluation\')\n    parser.add_argument(\n        \'--summaries\',\n        type=bool,\n        default=False,\n        help=\'Print summaries for every corruption and severity\')\n    parser.add_argument(\n        \'--workers\', type=int, default=32, help=\'workers per gpu\')\n    parser.add_argument(\'--show\', action=\'store_true\', help=\'show results\')\n    parser.add_argument(\'--tmpdir\', help=\'tmp dir for writing some results\')\n    parser.add_argument(\'--seed\', type=int, default=None, help=\'random seed\')\n    parser.add_argument(\n        \'--launcher\',\n        choices=[\'none\', \'pytorch\', \'slurm\', \'mpi\'],\n        default=\'none\',\n        help=\'job launcher\')\n    parser.add_argument(\'--local_rank\', type=int, default=0)\n    parser.add_argument(\n        \'--final-prints\',\n        type=str,\n        nargs=\'+\',\n        choices=[\'P\', \'mPC\', \'rPC\'],\n        default=\'mPC\',\n        help=\'corruption benchmark metric to print at the end\')\n    parser.add_argument(\n        \'--final-prints-aggregate\',\n        type=str,\n        choices=[\'all\', \'benchmark\'],\n        default=\'benchmark\',\n        help=\'aggregate all results or only those for benchmark corruptions\')\n    args = parser.parse_args()\n    if \'LOCAL_RANK\' not in os.environ:\n        os.environ[\'LOCAL_RANK\'] = str(args.local_rank)\n    return args\n\n\ndef main():\n    args = parse_args()\n\n    assert args.out or args.show, \\\n        (\'Please specify at least one operation (save or show the results) \'\n         \'with the argument ""--out"" or ""--show""\')\n\n    if args.out is not None and not args.out.endswith((\'.pkl\', \'.pickle\')):\n        raise ValueError(\'The output file must be a pkl file.\')\n\n    cfg = mmcv.Config.fromfile(args.config)\n    # set cudnn_benchmark\n    if cfg.get(\'cudnn_benchmark\', False):\n        torch.backends.cudnn.benchmark = True\n    cfg.model.pretrained = None\n    cfg.data.test.test_mode = True\n    if args.workers == 0:\n        args.workers = cfg.data.workers_per_gpu\n\n    # init distributed env first, since logger depends on the dist info.\n    if args.launcher == \'none\':\n        distributed = False\n    else:\n        distributed = True\n        init_dist(args.launcher, **cfg.dist_params)\n\n    # set random seeds\n    if args.seed is not None:\n        set_random_seed(args.seed)\n\n    if \'all\' in args.corruptions:\n        corruptions = [\n            \'gaussian_noise\', \'shot_noise\', \'impulse_noise\', \'defocus_blur\',\n            \'glass_blur\', \'motion_blur\', \'zoom_blur\', \'snow\', \'frost\', \'fog\',\n            \'brightness\', \'contrast\', \'elastic_transform\', \'pixelate\',\n            \'jpeg_compression\', \'speckle_noise\', \'gaussian_blur\', \'spatter\',\n            \'saturate\'\n        ]\n    elif \'benchmark\' in args.corruptions:\n        corruptions = [\n            \'gaussian_noise\', \'shot_noise\', \'impulse_noise\', \'defocus_blur\',\n            \'glass_blur\', \'motion_blur\', \'zoom_blur\', \'snow\', \'frost\', \'fog\',\n            \'brightness\', \'contrast\', \'elastic_transform\', \'pixelate\',\n            \'jpeg_compression\'\n        ]\n    elif \'noise\' in args.corruptions:\n        corruptions = [\'gaussian_noise\', \'shot_noise\', \'impulse_noise\']\n    elif \'blur\' in args.corruptions:\n        corruptions = [\n            \'defocus_blur\', \'glass_blur\', \'motion_blur\', \'zoom_blur\'\n        ]\n    elif \'weather\' in args.corruptions:\n        corruptions = [\'snow\', \'frost\', \'fog\', \'brightness\']\n    elif \'digital\' in args.corruptions:\n        corruptions = [\n            \'contrast\', \'elastic_transform\', \'pixelate\', \'jpeg_compression\'\n        ]\n    elif \'holdout\' in args.corruptions:\n        corruptions = [\'speckle_noise\', \'gaussian_blur\', \'spatter\', \'saturate\']\n    elif \'None\' in args.corruptions:\n        corruptions = [\'None\']\n        args.severities = [0]\n    else:\n        corruptions = args.corruptions\n\n    rank, _ = get_dist_info()\n    aggregated_results = {}\n    for corr_i, corruption in enumerate(corruptions):\n        aggregated_results[corruption] = {}\n        for sev_i, corruption_severity in enumerate(args.severities):\n            # evaluate severity 0 (= no corruption) only once\n            if corr_i > 0 and corruption_severity == 0:\n                aggregated_results[corruption][0] = \\\n                    aggregated_results[corruptions[0]][0]\n                continue\n\n            test_data_cfg = copy.deepcopy(cfg.data.test)\n            # assign corruption and severity\n            if corruption_severity > 0:\n                corruption_trans = dict(\n                    type=\'Corrupt\',\n                    corruption=corruption,\n                    severity=corruption_severity)\n                # TODO: hard coded ""1"", we assume that the first step is\n                # loading images, which needs to be fixed in the future\n                test_data_cfg[\'pipeline\'].insert(1, corruption_trans)\n\n            # print info\n            print(f\'\\nTesting {corruption} at severity {corruption_severity}\')\n\n            # build the dataloader\n            # TODO: support multiple images per gpu\n            #       (only minor changes are needed)\n            dataset = build_dataset(test_data_cfg)\n            data_loader = build_dataloader(\n                dataset,\n                samples_per_gpu=1,\n                workers_per_gpu=args.workers,\n                dist=distributed,\n                shuffle=False)\n\n            # build the model and load checkpoint\n            model = build_detector(\n                cfg.model, train_cfg=None, test_cfg=cfg.test_cfg)\n            fp16_cfg = cfg.get(\'fp16\', None)\n            if fp16_cfg is not None:\n                wrap_fp16_model(model)\n            checkpoint = load_checkpoint(\n                model, args.checkpoint, map_location=\'cpu\')\n            # old versions did not save class info in checkpoints,\n            # this walkaround is for backward compatibility\n            if \'CLASSES\' in checkpoint[\'meta\']:\n                model.CLASSES = checkpoint[\'meta\'][\'CLASSES\']\n            else:\n                model.CLASSES = dataset.CLASSES\n\n            if not distributed:\n                model = MMDataParallel(model, device_ids=[0])\n                outputs = single_gpu_test(model, data_loader, args.show)\n            else:\n                model = MMDistributedDataParallel(\n                    model.cuda(),\n                    device_ids=[torch.cuda.current_device()],\n                    broadcast_buffers=False)\n                outputs = multi_gpu_test(model, data_loader, args.tmpdir)\n\n            if args.out and rank == 0:\n                eval_results_filename = (\n                    osp.splitext(args.out)[0] + \'_results\' +\n                    osp.splitext(args.out)[1])\n                mmcv.dump(outputs, args.out)\n                eval_types = args.eval\n                if cfg.dataset_type == \'VOCDataset\':\n                    if eval_types:\n                        for eval_type in eval_types:\n                            if eval_type == \'bbox\':\n                                test_dataset = mmcv.runner.obj_from_dict(\n                                    cfg.data.test, datasets)\n                                logger = \'print\' if args.summaries else None\n                                mean_ap, eval_results = \\\n                                    voc_eval_with_return(\n                                        args.out, test_dataset,\n                                        args.iou_thr, logger)\n                                aggregated_results[corruption][\n                                    corruption_severity] = eval_results\n                            else:\n                                print(\'\\nOnly ""bbox"" evaluation \\\n                                is supported for pascal voc\')\n                else:\n                    if eval_types:\n                        print(f\'Starting evaluate {"" and "".join(eval_types)}\')\n                        if eval_types == [\'proposal_fast\']:\n                            result_file = args.out\n                        else:\n                            if not isinstance(outputs[0], dict):\n                                result_files = dataset.results2json(\n                                    outputs, args.out)\n                            else:\n                                for name in outputs[0]:\n                                    print(f\'\\nEvaluating {name}\')\n                                    outputs_ = [out[name] for out in outputs]\n                                    result_file = args.out\n                                    + f\'.{name}\'\n                                    result_files = dataset.results2json(\n                                        outputs_, result_file)\n                        eval_results = coco_eval_with_return(\n                            result_files, eval_types, dataset.coco)\n                        aggregated_results[corruption][\n                            corruption_severity] = eval_results\n                    else:\n                        print(\'\\nNo task was selected for evaluation;\'\n                              \'\\nUse --eval to select a task\')\n\n                # save results after each evaluation\n                mmcv.dump(aggregated_results, eval_results_filename)\n\n    if rank == 0:\n        # print filan results\n        print(\'\\nAggregated results:\')\n        prints = args.final_prints\n        aggregate = args.final_prints_aggregate\n\n        if cfg.dataset_type == \'VOCDataset\':\n            get_results(\n                eval_results_filename,\n                dataset=\'voc\',\n                prints=prints,\n                aggregate=aggregate)\n        else:\n            get_results(\n                eval_results_filename,\n                dataset=\'coco\',\n                prints=prints,\n                aggregate=aggregate)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tools/train.py,1,"b""import argparse\nimport copy\nimport os\nimport os.path as osp\nimport time\n\nimport mmcv\nimport torch\nfrom mmcv import Config, DictAction\nfrom mmcv.runner import init_dist\n\nfrom mmdet import __version__\nfrom mmdet.apis import set_random_seed, train_detector\nfrom mmdet.datasets import build_dataset\nfrom mmdet.models import build_detector\nfrom mmdet.utils import collect_env, get_root_logger\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Train a detector')\n    parser.add_argument('config', help='train config file path')\n    parser.add_argument('--work-dir', help='the dir to save logs and models')\n    parser.add_argument(\n        '--resume-from', help='the checkpoint file to resume from')\n    parser.add_argument(\n        '--no-validate',\n        action='store_true',\n        help='whether not to evaluate the checkpoint during training')\n    group_gpus = parser.add_mutually_exclusive_group()\n    group_gpus.add_argument(\n        '--gpus',\n        type=int,\n        help='number of gpus to use '\n        '(only applicable to non-distributed training)')\n    group_gpus.add_argument(\n        '--gpu-ids',\n        type=int,\n        nargs='+',\n        help='ids of gpus to use '\n        '(only applicable to non-distributed training)')\n    parser.add_argument('--seed', type=int, default=None, help='random seed')\n    parser.add_argument(\n        '--deterministic',\n        action='store_true',\n        help='whether to set deterministic options for CUDNN backend.')\n    parser.add_argument(\n        '--options', nargs='+', action=DictAction, help='arguments in dict')\n    parser.add_argument(\n        '--launcher',\n        choices=['none', 'pytorch', 'slurm', 'mpi'],\n        default='none',\n        help='job launcher')\n    parser.add_argument('--local_rank', type=int, default=0)\n    parser.add_argument(\n        '--autoscale-lr',\n        action='store_true',\n        help='automatically scale lr with the number of gpus')\n    args = parser.parse_args()\n    if 'LOCAL_RANK' not in os.environ:\n        os.environ['LOCAL_RANK'] = str(args.local_rank)\n\n    return args\n\n\ndef main():\n    args = parse_args()\n\n    cfg = Config.fromfile(args.config)\n    if args.options is not None:\n        cfg.merge_from_dict(args.options)\n    # set cudnn_benchmark\n    if cfg.get('cudnn_benchmark', False):\n        torch.backends.cudnn.benchmark = True\n\n    # work_dir is determined in this priority: CLI > segment in file > filename\n    if args.work_dir is not None:\n        # update configs according to CLI args if args.work_dir is not None\n        cfg.work_dir = args.work_dir\n    elif cfg.get('work_dir', None) is None:\n        # use config filename as default work_dir if cfg.work_dir is None\n        cfg.work_dir = osp.join('./work_dirs',\n                                osp.splitext(osp.basename(args.config))[0])\n    if args.resume_from is not None:\n        cfg.resume_from = args.resume_from\n    if args.gpu_ids is not None:\n        cfg.gpu_ids = args.gpu_ids\n    else:\n        cfg.gpu_ids = range(1) if args.gpus is None else range(args.gpus)\n\n    if args.autoscale_lr:\n        # apply the linear scaling rule (https://arxiv.org/abs/1706.02677)\n        cfg.optimizer['lr'] = cfg.optimizer['lr'] * len(cfg.gpu_ids) / 8\n\n    # init distributed env first, since logger depends on the dist info.\n    if args.launcher == 'none':\n        distributed = False\n    else:\n        distributed = True\n        init_dist(args.launcher, **cfg.dist_params)\n\n    # create work_dir\n    mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n    # dump config\n    cfg.dump(osp.join(cfg.work_dir, osp.basename(args.config)))\n    # init the logger before other steps\n    timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())\n    log_file = osp.join(cfg.work_dir, f'{timestamp}.log')\n    logger = get_root_logger(log_file=log_file, log_level=cfg.log_level)\n\n    # init the meta dict to record some important information such as\n    # environment info and seed, which will be logged\n    meta = dict()\n    # log env info\n    env_info_dict = collect_env()\n    env_info = '\\n'.join([(f'{k}: {v}') for k, v in env_info_dict.items()])\n    dash_line = '-' * 60 + '\\n'\n    logger.info('Environment info:\\n' + dash_line + env_info + '\\n' +\n                dash_line)\n    meta['env_info'] = env_info\n\n    # log some basic info\n    logger.info(f'Distributed training: {distributed}')\n    logger.info(f'Config:\\n{cfg.pretty_text}')\n\n    # set random seeds\n    if args.seed is not None:\n        logger.info(f'Set random seed to {args.seed}, '\n                    f'deterministic: {args.deterministic}')\n        set_random_seed(args.seed, deterministic=args.deterministic)\n    cfg.seed = args.seed\n    meta['seed'] = args.seed\n\n    model = build_detector(\n        cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n\n    datasets = [build_dataset(cfg.data.train)]\n    if len(cfg.workflow) == 2:\n        val_dataset = copy.deepcopy(cfg.data.val)\n        val_dataset.pipeline = cfg.data.train.pipeline\n        datasets.append(build_dataset(val_dataset))\n    if cfg.checkpoint_config is not None:\n        # save mmdet version, config file content and class names in\n        # checkpoints as meta data\n        cfg.checkpoint_config.meta = dict(\n            mmdet_version=__version__,\n            config=cfg.pretty_text,\n            CLASSES=datasets[0].CLASSES)\n    # add an attribute for visualization convenience\n    model.CLASSES = datasets[0].CLASSES\n    train_detector(\n        model,\n        datasets,\n        cfg,\n        distributed=distributed,\n        validate=(not args.no_validate),\n        timestamp=timestamp,\n        meta=meta)\n\n\nif __name__ == '__main__':\n    main()\n"""
tools/upgrade_model_version.py,5,"b'import argparse\nimport re\nimport tempfile\nfrom collections import OrderedDict\n\nimport torch\nfrom mmcv import Config\n\n\ndef is_head(key):\n    valid_head_list = [\n        \'bbox_head\', \'mask_head\', \'semantic_head\', \'grid_head\', \'mask_iou_head\'\n    ]\n\n    return any(key.startswith(h) for h in valid_head_list)\n\n\ndef parse_config(config_strings):\n    temp_file = tempfile.NamedTemporaryFile()\n    config_path = f\'{temp_file.name}.py\'\n    with open(config_path, \'w\') as f:\n        f.write(config_strings)\n\n    config = Config.fromfile(config_path)\n    is_two_stage = True\n    is_ssd = False\n    is_retina = False\n    reg_cls_agnostic = False\n    if \'rpn_head\' not in config.model:\n        is_two_stage = False\n        # check whether it is SSD\n        if config.model.bbox_head.type == \'SSDHead\':\n            is_ssd = True\n        elif config.model.bbox_head.type == \'RetinaHead\':\n            is_retina = True\n    elif isinstance(config.model[\'bbox_head\'], list):\n        reg_cls_agnostic = True\n    elif \'reg_class_agnostic\' in config.model.bbox_head:\n        reg_cls_agnostic = config.model.bbox_head \\\n            .reg_class_agnostic\n    temp_file.close()\n    return is_two_stage, is_ssd, is_retina, reg_cls_agnostic\n\n\ndef reorder_cls_channel(val, num_classes=81):\n    # bias\n    if val.dim() == 1:\n        new_val = torch.cat((val[1:], val[:1]), dim=0)\n    # weight\n    else:\n        out_channels, in_channels = val.shape[:2]\n        # conv_cls for softmax output\n        if out_channels != num_classes and out_channels % num_classes == 0:\n            new_val = val.reshape(-1, num_classes, in_channels, *val.shape[2:])\n            new_val = torch.cat((new_val[:, 1:], new_val[:, :1]), dim=1)\n            new_val = new_val.reshape(val.size())\n        # fc_cls\n        elif out_channels == num_classes:\n            new_val = torch.cat((val[1:], val[:1]), dim=0)\n        # agnostic | retina_cls | rpn_cls\n        else:\n            new_val = val\n\n    return new_val\n\n\ndef truncate_cls_channel(val, num_classes=81):\n\n    # bias\n    if val.dim() == 1:\n        if val.size(0) % num_classes == 0:\n            new_val = val[:num_classes - 1]\n        else:\n            new_val = val\n    # weight\n    else:\n        out_channels, in_channels = val.shape[:2]\n        # conv_logits\n        if out_channels % num_classes == 0:\n            new_val = val.reshape(num_classes, in_channels, *val.shape[2:])[1:]\n            new_val = new_val.reshape(-1, *val.shape[1:])\n        # agnostic\n        else:\n            new_val = val\n\n    return new_val\n\n\ndef truncate_reg_channel(val, num_classes=81):\n    # bias\n    if val.dim() == 1:\n        # fc_reg|rpn_reg\n        if val.size(0) % num_classes == 0:\n            new_val = val.reshape(num_classes, -1)[:num_classes - 1]\n            new_val = new_val.reshape(-1)\n        # agnostic\n        else:\n            new_val = val\n    # weight\n    else:\n        out_channels, in_channels = val.shape[:2]\n        # fc_reg|rpn_reg\n        if out_channels % num_classes == 0:\n            new_val = val.reshape(num_classes, -1, in_channels,\n                                  *val.shape[2:])[1:]\n            new_val = new_val.reshape(-1, *val.shape[1:])\n        # agnostic\n        else:\n            new_val = val\n\n    return new_val\n\n\ndef convert(in_file, out_file, num_classes):\n    """"""Convert keys in checkpoints.\n\n    There can be some breaking changes during the development of mmdetection,\n    and this tool is used for upgrading checkpoints trained with old versions\n    to the latest one.\n    """"""\n    checkpoint = torch.load(in_file)\n    in_state_dict = checkpoint.pop(\'state_dict\')\n    out_state_dict = OrderedDict()\n    meta_info = checkpoint[\'meta\']\n    is_two_stage, is_ssd, is_retina, reg_cls_agnostic = parse_config(\n        meta_info[\'config\'])\n    if meta_info[\'mmdet_version\'] <= \'0.5.3\' and is_retina:\n        upgrade_retina = True\n    else:\n        upgrade_retina = False\n\n    for key, val in in_state_dict.items():\n        new_key = key\n        new_val = val\n        if is_two_stage and is_head(key):\n            new_key = \'roi_head.{}\'.format(key)\n\n        # classification\n        m = re.search(\n            r\'(conv_cls|retina_cls|rpn_cls|fc_cls|fcos_cls|\'\n            r\'fovea_cls).(weight|bias)\', new_key)\n        if m is not None:\n            print(f\'reorder cls channels of {new_key}\')\n            new_val = reorder_cls_channel(val, num_classes)\n\n        # regression\n        m = re.search(r\'(fc_reg|rpn_reg).(weight|bias)\', new_key)\n        if m is not None and not reg_cls_agnostic:\n            print(f\'truncate regression channels of {new_key}\')\n            new_val = truncate_reg_channel(val, num_classes)\n\n        # mask head\n        m = re.search(r\'(conv_logits).(weight|bias)\', new_key)\n        if m is not None:\n            print(f\'truncate mask prediction channels of {new_key}\')\n            new_val = truncate_cls_channel(val, num_classes)\n\n        m = re.search(r\'(cls_convs|reg_convs).\\d.(weight|bias)\', key)\n        # Legacy issues in RetinaNet since V1.x\n        # Use ConvModule instead of nn.Conv2d in RetinaNet\n        # cls_convs.0.weight -> cls_convs.0.conv.weight\n        if m is not None and upgrade_retina:\n            param = m.groups()[1]\n            new_key = key.replace(param, f\'conv.{param}\')\n            out_state_dict[new_key] = val\n            print(f\'rename the name of {key} to {new_key}\')\n            continue\n\n        m = re.search(r\'(cls_convs).\\d.(weight|bias)\', key)\n        if m is not None and is_ssd:\n            print(f\'reorder cls channels of {new_key}\')\n            new_val = reorder_cls_channel(val, num_classes)\n\n        out_state_dict[new_key] = new_val\n    checkpoint[\'state_dict\'] = out_state_dict\n    torch.save(checkpoint, out_file)\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Upgrade model version\')\n    parser.add_argument(\'in_file\', help=\'input checkpoint file\')\n    parser.add_argument(\'out_file\', help=\'output checkpoint file\')\n    parser.add_argument(\n        \'--num-classes\',\n        type=int,\n        default=81,\n        help=\'number of classes of the original model\')\n    args = parser.parse_args()\n    convert(args.in_file, args.out_file, args.num_classes)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
configs/_base_/default_runtime.py,0,"b""checkpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/albu_example/mask_rcnn_r50_fpn_albu_1x_coco.py,0,"b""_base_ = '../mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\nalbu_train_transforms = [\n    dict(\n        type='ShiftScaleRotate',\n        shift_limit=0.0625,\n        scale_limit=0.0,\n        rotate_limit=0,\n        interpolation=1,\n        p=0.5),\n    dict(\n        type='RandomBrightnessContrast',\n        brightness_limit=[0.1, 0.3],\n        contrast_limit=[0.1, 0.3],\n        p=0.2),\n    dict(\n        type='OneOf',\n        transforms=[\n            dict(\n                type='RGBShift',\n                r_shift_limit=10,\n                g_shift_limit=10,\n                b_shift_limit=10,\n                p=1.0),\n            dict(\n                type='HueSaturationValue',\n                hue_shift_limit=20,\n                sat_shift_limit=30,\n                val_shift_limit=20,\n                p=1.0)\n        ],\n        p=0.1),\n    dict(type='JpegCompression', quality_lower=85, quality_upper=95, p=0.2),\n    dict(type='ChannelShuffle', p=0.1),\n    dict(\n        type='OneOf',\n        transforms=[\n            dict(type='Blur', blur_limit=3, p=1.0),\n            dict(type='MedianBlur', blur_limit=3, p=1.0)\n        ],\n        p=0.1),\n]\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='Pad', size_divisor=32),\n    dict(\n        type='Albu',\n        transforms=albu_train_transforms,\n        bbox_params=dict(\n            type='BboxParams',\n            format='pascal_voc',\n            label_fields=['gt_labels'],\n            min_visibility=0.0,\n            filter_lost_elements=True),\n        keymap={\n            'img': 'image',\n            'gt_masks': 'masks',\n            'gt_bboxes': 'bboxes'\n        },\n        update_pad_shape=False,\n        skip_img_without_anno=True),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='DefaultFormatBundle'),\n    dict(\n        type='Collect',\n        keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'],\n        meta_keys=('filename', 'ori_shape', 'img_shape', 'img_norm_cfg',\n                   'pad_shape', 'scale_factor'))\n]\ndata = dict(train=dict(pipeline=train_pipeline))\n"""
configs/atss/atss_r50_fpn_1x_coco.py,0,"b""_base_ = [\n    '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\nmodel = dict(\n    type='ATSS',\n    pretrained='torchvision://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        norm_eval=True,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        start_level=1,\n        add_extra_convs='on_output',\n        num_outs=5),\n    bbox_head=dict(\n        type='ATSSHead',\n        num_classes=80,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        anchor_generator=dict(\n            type='AnchorGenerator',\n            ratios=[1.0],\n            octave_base_scale=8,\n            scales_per_octave=1,\n            strides=[8, 16, 32, 64, 128]),\n        bbox_coder=dict(\n            type='DeltaXYWHBBoxCoder',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[0.1, 0.1, 0.2, 0.2]),\n        loss_cls=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_bbox=dict(type='GIoULoss', loss_weight=2.0),\n        loss_centerness=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0)))\n# training and testing settings\ntrain_cfg = dict(\n    assigner=dict(type='ATSSAssigner', topk=9),\n    allowed_border=-1,\n    pos_weight=-1,\n    debug=False)\ntest_cfg = dict(\n    nms_pre=1000,\n    min_bbox_size=0,\n    score_thr=0.05,\n    nms=dict(type='nms', iou_thr=0.6),\n    max_per_img=100)\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\n"""
configs/carafe/faster_rcnn_r50_fpn_carafe_1x_coco.py,0,"b""_base_ = '../faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    neck=dict(\n        type='FPN_CARAFE',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5,\n        start_level=0,\n        end_level=-1,\n        norm_cfg=None,\n        act_cfg=None,\n        order=('conv', 'norm', 'act'),\n        upsample_cfg=dict(\n            type='carafe',\n            up_kernel=5,\n            up_group=1,\n            encoder_kernel=3,\n            encoder_dilation=1,\n            compressed_channels=64)))\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=64),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=64),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n"""
configs/carafe/mask_rcnn_r50_fpn_carafe_1x_coco.py,0,"b""_base_ = '../mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    neck=dict(\n        type='FPN_CARAFE',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5,\n        start_level=0,\n        end_level=-1,\n        norm_cfg=None,\n        act_cfg=None,\n        order=('conv', 'norm', 'act'),\n        upsample_cfg=dict(\n            type='carafe',\n            up_kernel=5,\n            up_group=1,\n            encoder_kernel=3,\n            encoder_dilation=1,\n            compressed_channels=64)),\n    roi_head=dict(\n        mask_head=dict(\n            upsample_cfg=dict(\n                type='carafe',\n                scale_factor=2,\n                up_kernel=5,\n                up_group=1,\n                encoder_kernel=3,\n                encoder_dilation=1,\n                compressed_channels=64))))\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=64),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=64),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n"""
configs/cascade_rcnn/cascade_mask_rcnn_r101_caffe_fpn_1x_coco.py,0,"b""_base_ = './cascade_mask_rcnn_r50_caffe_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://detectron2/resnet101_caffe',\n    backbone=dict(depth=101))\n"""
configs/cascade_rcnn/cascade_mask_rcnn_r101_fpn_1x_coco.py,0,"b""_base_ = './cascade_mask_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(pretrained='torchvision://resnet101', backbone=dict(depth=101))\n"""
configs/cascade_rcnn/cascade_mask_rcnn_r101_fpn_20e_coco.py,0,"b""_base_ = './cascade_mask_rcnn_r50_fpn_20e_coco.py'\nmodel = dict(pretrained='torchvision://resnet101', backbone=dict(depth=101))\n"""
configs/cascade_rcnn/cascade_mask_rcnn_r50_caffe_fpn_1x_coco.py,0,"b""_base_ = ['./cascade_mask_rcnn_r50_fpn_1x_coco.py']\n\nmodel = dict(\n    pretrained='open-mmlab://detectron2/resnet50_caffe',\n    backbone=dict(\n        norm_cfg=dict(requires_grad=False), norm_eval=True, style='caffe'))\n\nimg_norm_cfg = dict(\n    mean=[103.530, 116.280, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n"""
configs/cascade_rcnn/cascade_mask_rcnn_r50_fpn_1x_coco.py,0,"b""_base_ = [\n    '../_base_/models/cascade_mask_rcnn_r50_fpn.py',\n    '../_base_/datasets/coco_instance.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\n"""
configs/cascade_rcnn/cascade_mask_rcnn_r50_fpn_20e_coco.py,0,"b""_base_ = [\n    '../_base_/models/cascade_mask_rcnn_r50_fpn.py',\n    '../_base_/datasets/coco_instance.py',\n    '../_base_/schedules/schedule_20e.py', '../_base_/default_runtime.py'\n]\n"""
configs/cascade_rcnn/cascade_mask_rcnn_x101_32x4d_fpn_1x_coco.py,0,"b""_base_ = './cascade_mask_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_32x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/cascade_rcnn/cascade_mask_rcnn_x101_32x4d_fpn_20e_coco.py,0,"b""_base_ = './cascade_mask_rcnn_r50_fpn_20e_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_32x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/cascade_rcnn/cascade_mask_rcnn_x101_64x4d_fpn_1x_coco.py,0,"b""_base_ = './cascade_mask_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_64x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/cascade_rcnn/cascade_mask_rcnn_x101_64x4d_fpn_20e_coco.py,0,"b""_base_ = './cascade_mask_rcnn_r50_fpn_20e_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_64x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/cascade_rcnn/cascade_rcnn_r101_caffe_fpn_1x_coco.py,0,"b""_base_ = './cascade_rcnn_r50_caffe_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://detectron2/resnet101_caffe',\n    backbone=dict(depth=101))\n"""
configs/cascade_rcnn/cascade_rcnn_r101_fpn_1x_coco.py,0,"b""_base_ = './cascade_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(pretrained='torchvision://resnet101', backbone=dict(depth=101))\n"""
configs/cascade_rcnn/cascade_rcnn_r101_fpn_20e_coco.py,0,"b""_base_ = './cascade_rcnn_r50_fpn_20e_coco.py'\nmodel = dict(pretrained='torchvision://resnet101', backbone=dict(depth=101))\n"""
configs/cascade_rcnn/cascade_rcnn_r50_caffe_fpn_1x_coco.py,0,"b""_base_ = './cascade_rcnn_r50_fpn_1x_coco.py'\n\nmodel = dict(\n    pretrained='open-mmlab://detectron2/resnet50_caffe',\n    backbone=dict(norm_cfg=dict(requires_grad=False), style='caffe'))\n\n# use caffe img_norm\nimg_norm_cfg = dict(\n    mean=[103.530, 116.280, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n"""
configs/cascade_rcnn/cascade_rcnn_r50_fpn_1x_coco.py,0,"b""_base_ = [\n    '../_base_/models/cascade_rcnn_r50_fpn.py',\n    '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\n"""
configs/cascade_rcnn/cascade_rcnn_r50_fpn_20e_coco.py,0,"b""_base_ = './cascade_rcnn_r50_fpn_1x_coco.py'\n# learning policy\nlr_config = dict(step=[16, 19])\ntotal_epochs = 20\n"""
configs/cascade_rcnn/cascade_rcnn_x101_32x4d_fpn_1x_coco.py,0,"b""_base_ = './cascade_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_32x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/cascade_rcnn/cascade_rcnn_x101_32x4d_fpn_20e_coco.py,0,"b""_base_ = './cascade_rcnn_r50_fpn_20e_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_32x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/cascade_rcnn/cascade_rcnn_x101_64x4d_fpn_1x_coco.py,0,"b""_base_ = './cascade_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    type='CascadeRCNN',\n    pretrained='open-mmlab://resnext101_64x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/cascade_rcnn/cascade_rcnn_x101_64x4d_fpn_20e_coco.py,0,"b""_base_ = './cascade_rcnn_r50_fpn_20e_coco.py'\nmodel = dict(\n    type='CascadeRCNN',\n    pretrained='open-mmlab://resnext101_64x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/cityscapes/faster_rcnn_r50_fpn_1x_cityscapes.py,0,"b""_base_ = [\n    '../_base_/models/faster_rcnn_r50_fpn.py',\n    '../_base_/datasets/cityscapes_detection.py',\n    '../_base_/default_runtime.py'\n]\nmodel = dict(\n    pretrained=None,\n    roi_head=dict(\n        bbox_head=dict(\n            type='Shared2FCBBoxHead',\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=8,\n            bbox_coder=dict(\n                type='DeltaXYWHBBoxCoder',\n                target_means=[0., 0., 0., 0.],\n                target_stds=[0.1, 0.1, 0.2, 0.2]),\n            reg_class_agnostic=False,\n            loss_cls=dict(\n                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n            loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0))))\n# optimizer\n# lr is set for a batch size of 8\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=None)\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=0.001,\n    # [7] yields higher performance than [6]\n    step=[7])\ntotal_epochs = 8  # actual epoch = 8 * 8 = 64\nlog_config = dict(interval=100)\n# For better, more stable performance initialize from COCO\nload_from = 'https://open-mmlab.s3.ap-northeast-2.amazonaws.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'  # noqa\n"""
configs/cityscapes/mask_rcnn_r50_fpn_1x_cityscapes.py,0,"b""_base_ = [\n    '../_base_/models/mask_rcnn_r50_fpn.py',\n    '../_base_/datasets/cityscapes_instance.py', '../_base_/default_runtime.py'\n]\nmodel = dict(\n    pretrained=None,\n    roi_head=dict(\n        bbox_head=dict(\n            type='Shared2FCBBoxHead',\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=8,\n            bbox_coder=dict(\n                type='DeltaXYWHBBoxCoder',\n                target_means=[0., 0., 0., 0.],\n                target_stds=[0.1, 0.1, 0.2, 0.2]),\n            reg_class_agnostic=False,\n            loss_cls=dict(\n                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n            loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n        mask_head=dict(\n            type='FCNMaskHead',\n            num_convs=4,\n            in_channels=256,\n            conv_out_channels=256,\n            num_classes=8,\n            loss_mask=dict(\n                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))))\n# optimizer\n# lr is set for a batch size of 8\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=None)\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=0.001,\n    # [7] yields higher performance than [6]\n    step=[7])\ntotal_epochs = 8  # actual epoch = 8 * 8 = 64\nlog_config = dict(interval=100)\n# For better, more stable performance initialize from COCO\nload_from = 'https://open-mmlab.s3.ap-northeast-2.amazonaws.com/mmdetection/v2.0/mask_rcnn/mask_rcnn_r50_fpn_1x_coco/mask_rcnn_r50_fpn_1x_coco_20200205-d4b0c5d6.pth'  # noqa\n"""
configs/dcn/cascade_mask_rcnn_r101_fpn_dconv_c3-c5_1x_coco.py,0,"b""_base_ = '../cascade_rcnn/cascade_mask_rcnn_r101_fpn_1x_coco.py'\nmodel = dict(\n    backbone=dict(\n        dcn=dict(type='DCN', deformable_groups=1, fallback_on_stride=False),\n        stage_with_dcn=(False, True, True, True)))\n"""
configs/dcn/cascade_mask_rcnn_r50_fpn_dconv_c3-c5_1x_coco.py,0,"b""_base_ = '../cascade_rcnn/cascade_mask_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    backbone=dict(\n        dcn=dict(type='DCN', deformable_groups=1, fallback_on_stride=False),\n        stage_with_dcn=(False, True, True, True)))\n"""
configs/dcn/cascade_mask_rcnn_x101_32x4d_fpn_dconv_c3-c5_1x_coco.py,0,"b""_base_ = '../cascade_rcnn/cascade_mask_rcnn_x101_32x4d_fpn_1x_coco.py'\nmodel = dict(\n    backbone=dict(\n        dcn=dict(type='DCN', deformable_groups=1, fallback_on_stride=False),\n        stage_with_dcn=(False, True, True, True)))\n"""
configs/dcn/cascade_rcnn_r101_fpn_dconv_c3-c5_1x_coco.py,0,"b""_base_ = '../cascade_rcnn/cascade_rcnn_r101_fpn_1x_coco.py'\nmodel = dict(\n    backbone=dict(\n        dcn=dict(type='DCN', deformable_groups=1, fallback_on_stride=False),\n        stage_with_dcn=(False, True, True, True)))\n"""
configs/dcn/cascade_rcnn_r50_fpn_dconv_c3-c5_1x_coco.py,0,"b""_base_ = '../cascade_rcnn/cascade_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    backbone=dict(\n        dcn=dict(type='DCN', deformable_groups=1, fallback_on_stride=False),\n        stage_with_dcn=(False, True, True, True)))\n"""
configs/dcn/faster_rcnn_r101_fpn_dconv_c3-c5_1x_coco.py,0,"b""_base_ = '../faster_rcnn/faster_rcnn_r101_fpn_1x_coco.py'\nmodel = dict(\n    backbone=dict(\n        dcn=dict(type='DCN', deformable_groups=1, fallback_on_stride=False),\n        stage_with_dcn=(False, True, True, True)))\n"""
configs/dcn/faster_rcnn_r50_fpn_dconv_c3-c5_1x_coco.py,0,"b""_base_ = '../faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    backbone=dict(\n        dcn=dict(type='DCN', deformable_groups=1, fallback_on_stride=False),\n        stage_with_dcn=(False, True, True, True)))\n"""
configs/dcn/faster_rcnn_r50_fpn_dpool_1x_coco.py,0,"b""_base_ = '../faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    roi_head=dict(\n        bbox_roi_extractor=dict(\n            type='SingleRoIExtractor',\n            roi_layer=dict(\n                _delete_=True,\n                type='DeformRoIPoolingPack',\n                out_size=7,\n                out_channels=256,\n                no_trans=False,\n                group_size=1,\n                trans_std=0.1),\n            out_channels=256,\n            featmap_strides=[4, 8, 16, 32])))\n"""
configs/dcn/faster_rcnn_r50_fpn_mdconv_c3-c5_1x_coco.py,0,"b""_base_ = '../faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    backbone=dict(\n        dcn=dict(type='DCNv2', deformable_groups=1, fallback_on_stride=False),\n        stage_with_dcn=(False, True, True, True)))\n"""
configs/dcn/faster_rcnn_r50_fpn_mdconv_c3-c5_group4_1x_coco.py,0,"b""_base_ = '../faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    backbone=dict(\n        dcn=dict(type='DCNv2', deformable_groups=4, fallback_on_stride=False),\n        stage_with_dcn=(False, True, True, True)))\n"""
configs/dcn/faster_rcnn_r50_fpn_mdpool_1x_coco.py,0,"b""_base_ = '../faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    roi_head=dict(\n        bbox_roi_extractor=dict(\n            type='SingleRoIExtractor',\n            roi_layer=dict(\n                _delete_=True,\n                type='ModulatedDeformRoIPoolingPack',\n                out_size=7,\n                out_channels=256,\n                no_trans=False,\n                group_size=1,\n                trans_std=0.1),\n            out_channels=256,\n            featmap_strides=[4, 8, 16, 32])))\n"""
configs/dcn/faster_rcnn_x101_32x4d_fpn_dconv_c3-c5_1x_coco.py,0,"b""_base_ = '../faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_32x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch',\n        dcn=dict(type='DCN', deformable_groups=1, fallback_on_stride=False),\n        stage_with_dcn=(False, True, True, True)))\n"""
configs/dcn/mask_rcnn_r101_fpn_dconv_c3-c5_1x_coco.py,0,"b""_base_ = '../mask_rcnn/mask_rcnn_r101_fpn_1x_coco.py'\nmodel = dict(\n    backbone=dict(\n        dcn=dict(type='DCN', deformable_groups=1, fallback_on_stride=False),\n        stage_with_dcn=(False, True, True, True)))\n"""
configs/dcn/mask_rcnn_r50_fpn_dconv_c3-c5_1x_coco.py,0,"b""_base_ = '../mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    backbone=dict(\n        dcn=dict(type='DCN', deformable_groups=1, fallback_on_stride=False),\n        stage_with_dcn=(False, True, True, True)))\n"""
configs/dcn/mask_rcnn_r50_fpn_mdconv_c3-c5_1x_coco.py,0,"b""_base_ = '../mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    backbone=dict(\n        dcn=dict(type='DCNv2', deformable_groups=1, fallback_on_stride=False),\n        stage_with_dcn=(False, True, True, True)))\n"""
configs/double_heads/dh_faster_rcnn_r50_fpn_1x_coco.py,0,"b""_base_ = '../faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    roi_head=dict(\n        type='DoubleHeadRoIHead',\n        reg_roi_scale_factor=1.3,\n        bbox_head=dict(\n            _delete_=True,\n            type='DoubleConvFCBBoxHead',\n            num_convs=4,\n            num_fcs=2,\n            in_channels=256,\n            conv_out_channels=1024,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=80,\n            bbox_coder=dict(\n                type='DeltaXYWHBBoxCoder',\n                target_means=[0., 0., 0., 0.],\n                target_stds=[0.1, 0.1, 0.2, 0.2]),\n            reg_class_agnostic=False,\n            loss_cls=dict(\n                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=2.0),\n            loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=2.0))))\n"""
configs/empirical_attention/faster_rcnn_r50_fpn_attention_0010_1x_coco.py,0,"b""_base_ = '../faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    backbone=dict(plugins=[\n        dict(\n            cfg=dict(\n                type='GeneralizedAttention',\n                spatial_range=-1,\n                num_heads=8,\n                attention_type='0010',\n                kv_stride=2),\n            stages=(False, False, True, True),\n            position='after_conv2')\n    ]))\n"""
configs/empirical_attention/faster_rcnn_r50_fpn_attention_0010_dcn_1x_coco.py,0,"b""_base_ = '../faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    backbone=dict(\n        plugins=[\n            dict(\n                cfg=dict(\n                    type='GeneralizedAttention',\n                    spatial_range=-1,\n                    num_heads=8,\n                    attention_type='0010',\n                    kv_stride=2),\n                stages=(False, False, True, True),\n                position='after_conv2')\n        ],\n        dcn=dict(type='DCN', deformable_groups=1, fallback_on_stride=False),\n        stage_with_dcn=(False, True, True, True)))\n"""
configs/empirical_attention/faster_rcnn_r50_fpn_attention_1111_1x_coco.py,0,"b""_base_ = '../faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    backbone=dict(plugins=[\n        dict(\n            cfg=dict(\n                type='GeneralizedAttention',\n                spatial_range=-1,\n                num_heads=8,\n                attention_type='1111',\n                kv_stride=2),\n            stages=(False, False, True, True),\n            position='after_conv2')\n    ]))\n"""
configs/empirical_attention/faster_rcnn_r50_fpn_attention_1111_dcn_1x_coco.py,0,"b""_base_ = '../faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    backbone=dict(\n        plugins=[\n            dict(\n                cfg=dict(\n                    type='GeneralizedAttention',\n                    spatial_range=-1,\n                    num_heads=8,\n                    attention_type='1111',\n                    kv_stride=2),\n                stages=(False, False, True, True),\n                position='after_conv2')\n        ],\n        dcn=dict(type='DCN', deformable_groups=1, fallback_on_stride=False),\n        stage_with_dcn=(False, True, True, True)))\n"""
configs/fast_rcnn/fast_rcnn_r101_caffe_fpn_1x_coco.py,0,"b""_base_ = './fast_rcnn_r50_caffe_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://detectron2/resnet101_caffe',\n    backbone=dict(depth=101))\n"""
configs/fast_rcnn/fast_rcnn_r101_fpn_1x_coco.py,0,"b""_base_ = './fast_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(pretrained='torchvision://resnet101', backbone=dict(depth=101))\n"""
configs/fast_rcnn/fast_rcnn_r101_fpn_2x_coco.py,0,"b""_base_ = './fast_rcnn_r50_fpn_2x_coco.py'\nmodel = dict(pretrained='torchvision://resnet101', backbone=dict(depth=101))\n"""
configs/fast_rcnn/fast_rcnn_r50_caffe_fpn_1x_coco.py,0,"b""_base_ = './fast_rcnn_r50_fpn_1x_coco.py'\n\nmodel = dict(\n    pretrained='open-mmlab://detectron2/resnet50_caffe',\n    backbone=dict(\n        norm_cfg=dict(type='BN', requires_grad=False), style='caffe'))\n\n# use caffe img_norm\nimg_norm_cfg = dict(\n    mean=[103.530, 116.280, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadProposals', num_max_proposals=2000),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'proposals', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadProposals', num_max_proposals=None),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='ToTensor', keys=['proposals']),\n            dict(\n                type='ToDataContainer',\n                fields=[dict(key='proposals', stack=False)]),\n            dict(type='Collect', keys=['img', 'proposals']),\n        ])\n]\ndata = dict(\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n"""
configs/fast_rcnn/fast_rcnn_r50_fpn_1x_coco.py,0,"b""_base_ = [\n    '../_base_/models/fast_rcnn_r50_fpn.py',\n    '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadProposals', num_max_proposals=2000),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'proposals', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadProposals', num_max_proposals=None),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='ToTensor', keys=['proposals']),\n            dict(\n                type='ToDataContainer',\n                fields=[dict(key='proposals', stack=False)]),\n            dict(type='Collect', keys=['img', 'proposals']),\n        ])\n]\ndata = dict(\n    samples_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        proposal_file=data_root + 'proposals/rpn_r50_fpn_1x_train2017.pkl',\n        pipeline=train_pipeline),\n    val=dict(\n        proposal_file=data_root + 'proposals/rpn_r50_fpn_1x_val2017.pkl',\n        pipeline=test_pipeline),\n    test=dict(\n        proposal_file=data_root + 'proposals/rpn_r50_fpn_1x_val2017.pkl',\n        pipeline=test_pipeline))\n"""
configs/fast_rcnn/fast_rcnn_r50_fpn_2x_coco.py,0,"b""_base_ = './fast_rcnn_r50_fpn_1x_coco.py'\n\n# learning policy\nlr_config = dict(step=[16, 22])\ntotal_epochs = 24\n"""
configs/faster_rcnn/faster_rcnn_r101_caffe_fpn_1x_coco.py,0,"b""_base_ = './faster_rcnn_r50_caffe_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://detectron2/resnet101_caffe',\n    backbone=dict(depth=101))\n"""
configs/faster_rcnn/faster_rcnn_r101_fpn_1x_coco.py,0,"b""_base_ = './faster_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(pretrained='torchvision://resnet101', backbone=dict(depth=101))\n"""
configs/faster_rcnn/faster_rcnn_r101_fpn_2x_coco.py,0,"b""_base_ = './faster_rcnn_r50_fpn_2x_coco.py'\nmodel = dict(pretrained='torchvision://resnet101', backbone=dict(depth=101))\n"""
configs/faster_rcnn/faster_rcnn_r50_caffe_c4_1x_coco.py,0,"b""_base_ = [\n    '../_base_/models/faster_rcnn_r50_caffe_c4.py',\n    '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\n# use caffe img_norm\nimg_norm_cfg = dict(\n    mean=[103.530, 116.280, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\n"""
configs/faster_rcnn/faster_rcnn_r50_caffe_fpn_1x_coco.py,0,"b""_base_ = './faster_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://detectron2/resnet50_caffe',\n    backbone=dict(\n        norm_cfg=dict(requires_grad=False), norm_eval=True, style='caffe'))\n# use caffe img_norm\nimg_norm_cfg = dict(\n    mean=[103.530, 116.280, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n"""
configs/faster_rcnn/faster_rcnn_r50_caffe_fpn_mstrain_1x_coco.py,0,"b""_base_ = './faster_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://detectron2/resnet50_caffe',\n    backbone=dict(\n        norm_cfg=dict(requires_grad=False), norm_eval=True, style='caffe'))\n# use caffe img_norm\nimg_norm_cfg = dict(\n    mean=[103.530, 116.280, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(\n        type='Resize',\n        img_scale=[(1333, 640), (1333, 672), (1333, 704), (1333, 736),\n                   (1333, 768), (1333, 800)],\n        multiscale_mode='value',\n        keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n"""
configs/faster_rcnn/faster_rcnn_r50_caffe_fpn_mstrain_2x_coco.py,0,"b""_base_ = './faster_rcnn_r50_caffe_fpn_mstrain_1x_coco.py'\n# learning policy\nlr_config = dict(step=[16, 23])\ntotal_epochs = 24\n"""
configs/faster_rcnn/faster_rcnn_r50_caffe_fpn_mstrain_3x_coco.py,0,"b""_base_ = './faster_rcnn_r50_caffe_fpn_mstrain_1x_coco.py'\n# learning policy\nlr_config = dict(step=[28, 34])\ntotal_epochs = 36\n"""
configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco-person-bicycle-car.py,0,"b""_base_ = './faster_rcnn_r50_fpn_1x_coco.py'\nclasses = ('person', 'bicycle', 'car')\ndata = dict(\n    train=dict(classes=classes),\n    val=dict(classes=classes),\n    test=dict(classes=classes))\n# TODO: Update model url after bumping to V2.0\nload_from = 'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/mmdetection/models/faster_rcnn_r50_fpn_1x_20181010-3d1b3351.pth'  # noqa\n"""
configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco-person.py,0,"b""_base_ = './faster_rcnn_r50_fpn_1x_coco.py'\nclasses = ('person', )\ndata = dict(\n    train=dict(classes=classes),\n    val=dict(classes=classes),\n    test=dict(classes=classes))\n"""
configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py,0,"b""_base_ = [\n    '../_base_/models/faster_rcnn_r50_fpn.py',\n    '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\n"""
configs/faster_rcnn/faster_rcnn_r50_fpn_2x_coco.py,0,"b""_base_ = [\n    '../_base_/models/faster_rcnn_r50_fpn.py',\n    '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_2x.py', '../_base_/default_runtime.py'\n]\n"""
configs/faster_rcnn/faster_rcnn_r50_fpn_bounded_iou_1x_coco.py,0,"b""_base_ = './faster_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    roi_head=dict(\n        bbox_head=dict(\n            reg_decoded_bbox=True,\n            loss_bbox=dict(type='BoundedIoULoss', loss_weight=10.0))))\n"""
configs/faster_rcnn/faster_rcnn_r50_fpn_giou_1x_coco.py,0,"b""_base_ = './faster_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    roi_head=dict(\n        bbox_head=dict(\n            reg_decoded_bbox=True,\n            loss_bbox=dict(type='GIoULoss', loss_weight=10.0))))\n"""
configs/faster_rcnn/faster_rcnn_r50_fpn_iou_1x_coco.py,0,"b""_base_ = './faster_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    roi_head=dict(\n        bbox_head=dict(\n            reg_decoded_bbox=True,\n            loss_bbox=dict(type='IoULoss', loss_weight=10.0))))\n"""
configs/faster_rcnn/faster_rcnn_r50_fpn_ohem_1x_coco.py,0,"b""_base_ = './faster_rcnn_r50_fpn_1x_coco.py'\ntrain_cfg = dict(rcnn=dict(sampler=dict(type='OHEMSampler')))\n"""
configs/faster_rcnn/faster_rcnn_r50_fpn_soft_nms_1x_coco.py,0,"b""_base_ = [\n    '../_base_/models/faster_rcnn_r50_fpn.py',\n    '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\n\ntest_cfg = dict(\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='soft_nms', iou_thr=0.5),\n        max_per_img=100))\n"""
configs/faster_rcnn/faster_rcnn_x101_32x4d_fpn_1x_coco.py,0,"b""_base_ = './faster_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_32x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/faster_rcnn/faster_rcnn_x101_32x4d_fpn_2x_coco.py,0,"b""_base_ = './faster_rcnn_r50_fpn_2x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_32x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/faster_rcnn/faster_rcnn_x101_64x4d_fpn_1x_coco.py,0,"b""_base_ = './faster_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_64x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/faster_rcnn/faster_rcnn_x101_64x4d_fpn_2x_coco.py,0,"b""_base_ = './faster_rcnn_r50_fpn_2x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_64x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/fcos/fcos_center_r50_caffe_fpn_gn-head_4x4_1x_coco.py,0,"b""_base_ = './fcos_r50_caffe_fpn_gn-head_4x4_1x_coco.py'\nmodel = dict(bbox_head=dict(center_sampling=True, center_sample_radius=1.5))\n"""
configs/fcos/fcos_r101_caffe_fpn_gn-head_4x4_1x_coco.py,0,"b""_base_ = './fcos_r50_caffe_fpn_gn-head_4x4_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://detectron/resnet101_caffe',\n    backbone=dict(depth=101))\n"""
configs/fcos/fcos_r101_caffe_fpn_gn-head_4x4_2x_coco.py,0,"b""_base_ = ['./fcos_r50_caffe_fpn_gn-head_4x4_2x_coco.py']\nmodel = dict(\n    pretrained='open-mmlab://detectron/resnet101_caffe',\n    backbone=dict(depth=101))\n"""
configs/fcos/fcos_r101_caffe_fpn_gn-head_mstrain_640-800_4x4_2x_coco.py,0,"b""_base_ = './fcos_r50_caffe_fpn_gn-head_4x4_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://detectron/resnet101_caffe',\n    backbone=dict(depth=101))\nimg_norm_cfg = dict(\n    mean=[102.9801, 115.9465, 122.7717], std=[1.0, 1.0, 1.0], to_rgb=False)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(\n        type='Resize',\n        img_scale=[(1333, 640), (1333, 800)],\n        multiscale_mode='value',\n        keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    samples_per_gpu=4,\n    workers_per_gpu=4,\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n# learning policy\nlr_config = dict(step=[16, 22])\ntotal_epochs = 24\n"""
configs/fcos/fcos_r50_caffe_fpn_4x4_1x_coco.py,0,"b""_base_ = [\n    '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\n# model settings\nmodel = dict(\n    type='FCOS',\n    pretrained='open-mmlab://detectron/resnet50_caffe',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=False),\n        norm_eval=True,\n        style='caffe'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        start_level=1,\n        add_extra_convs=True,\n        extra_convs_on_inputs=False,  # use P5\n        num_outs=5,\n        relu_before_extra_convs=True),\n    bbox_head=dict(\n        type='FCOSHead',\n        num_classes=80,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        strides=[8, 16, 32, 64, 128],\n        norm_cfg=None,\n        loss_cls=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_bbox=dict(type='IoULoss', loss_weight=1.0),\n        loss_centerness=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0)))\n# training and testing settings\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.4,\n        min_pos_iou=0,\n        ignore_iof_thr=-1),\n    allowed_border=-1,\n    pos_weight=-1,\n    debug=False)\ntest_cfg = dict(\n    nms_pre=1000,\n    min_bbox_size=0,\n    score_thr=0.05,\n    nms=dict(type='nms', iou_thr=0.5),\n    max_per_img=100)\nimg_norm_cfg = dict(\n    mean=[102.9801, 115.9465, 122.7717], std=[1.0, 1.0, 1.0], to_rgb=False)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    samples_per_gpu=4,\n    workers_per_gpu=4,\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n# optimizer\noptimizer = dict(\n    lr=0.01, paramwise_cfg=dict(bias_lr_mult=2., bias_decay_mult=0.))\noptimizer_config = dict(\n    _delete_=True, grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(warmup='constant')\ntotal_epochs = 12\n"""
configs/fcos/fcos_r50_caffe_fpn_gn-head_4x4_1x_coco.py,0,"b""_base_ = [\n    '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\n# model settings\nmodel = dict(\n    type='FCOS',\n    pretrained='open-mmlab://detectron/resnet50_caffe',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=False),\n        norm_eval=True,\n        style='caffe'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        start_level=1,\n        add_extra_convs=True,\n        extra_convs_on_inputs=False,  # use P5\n        num_outs=5,\n        relu_before_extra_convs=True),\n    bbox_head=dict(\n        type='FCOSHead',\n        num_classes=80,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        strides=[8, 16, 32, 64, 128],\n        loss_cls=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_bbox=dict(type='IoULoss', loss_weight=1.0),\n        loss_centerness=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0)))\n# training and testing settings\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.4,\n        min_pos_iou=0,\n        ignore_iof_thr=-1),\n    allowed_border=-1,\n    pos_weight=-1,\n    debug=False)\ntest_cfg = dict(\n    nms_pre=1000,\n    min_bbox_size=0,\n    score_thr=0.05,\n    nms=dict(type='nms', iou_thr=0.5),\n    max_per_img=100)\nimg_norm_cfg = dict(\n    mean=[102.9801, 115.9465, 122.7717], std=[1.0, 1.0, 1.0], to_rgb=False)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    samples_per_gpu=4,\n    workers_per_gpu=4,\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n# optimizer\noptimizer = dict(\n    lr=0.01, paramwise_cfg=dict(bias_lr_mult=2., bias_decay_mult=0.))\noptimizer_config = dict(\n    _delete_=True, grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(warmup='constant')\ntotal_epochs = 12\n"""
configs/fcos/fcos_r50_caffe_fpn_gn-head_4x4_2x_coco.py,0,"b""_base_ = './fcos_r50_caffe_fpn_gn-head_4x4_1x_coco.py'\n\n# learning policy\nlr_config = dict(step=[16, 22])\ntotal_epochs = 24\n"""
configs/fcos/fcos_r50_caffe_fpn_gn-head_mstrain_640-800_4x4_2x_coco.py,0,"b""_base_ = './fcos_r50_caffe_fpn_gn-head_4x4_1x_coco.py'\nimg_norm_cfg = dict(\n    mean=[102.9801, 115.9465, 122.7717], std=[1.0, 1.0, 1.0], to_rgb=False)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(\n        type='Resize',\n        img_scale=[(1333, 640), (1333, 800)],\n        multiscale_mode='value',\n        keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n# learning policy\nlr_config = dict(step=[16, 22])\ntotal_epochs = 24\n"""
configs/fcos/fcos_x101_64x4d_fpn_gn-head_mstrain_640-800_4x2_2x_coco.py,0,"b""_base_ = './fcos_r50_caffe_fpn_gn-head_4x4_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_64x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        norm_eval=True,\n        style='pytorch'))\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(\n        type='Resize',\n        img_scale=[(1333, 640), (1333, 800)],\n        multiscale_mode='value',\n        keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    samples_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n# optimizer\noptimizer = dict(\n    lr=0.01, paramwise_cfg=dict(bias_lr_mult=2., bias_decay_mult=0.))\noptimizer_config = dict(\n    _delete_=True, grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(warmup='constant', step=[16, 22])\ntotal_epochs = 24\n"""
configs/foveabox/fovea_align_r101_fpn_gn-head_4x4_2x_coco.py,0,"b""_base_ = './fovea_r50_fpn_4x4_1x_coco.py'\nmodel = dict(\n    pretrained='torchvision://resnet101',\n    backbone=dict(depth=101),\n    bbox_head=dict(\n        with_deform=True,\n        norm_cfg=dict(type='GN', num_groups=32, requires_grad=True)))\n# learning policy\nlr_config = dict(step=[16, 22])\ntotal_epochs = 24\n"""
configs/foveabox/fovea_align_r101_fpn_gn-head_mstrain_640-800_4x4_2x_coco.py,0,"b""_base_ = './fovea_r50_fpn_4x4_1x_coco.py'\nmodel = dict(\n    pretrained='torchvision://resnet101',\n    backbone=dict(depth=101),\n    bbox_head=dict(\n        with_deform=True,\n        norm_cfg=dict(type='GN', num_groups=32, requires_grad=True)))\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(\n        type='Resize',\n        img_scale=[(1333, 640), (1333, 800)],\n        multiscale_mode='value',\n        keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ndata = dict(train=dict(pipeline=train_pipeline))\n# learning policy\nlr_config = dict(step=[16, 22])\ntotal_epochs = 24\n"""
configs/foveabox/fovea_align_r50_fpn_gn-head_4x4_2x_coco.py,0,"b""_base_ = './fovea_r50_fpn_4x4_1x_coco.py'\nmodel = dict(\n    bbox_head=dict(\n        with_deform=True,\n        norm_cfg=dict(type='GN', num_groups=32, requires_grad=True)))\n# learning policy\nlr_config = dict(step=[16, 22])\ntotal_epochs = 24\noptimizer_config = dict(\n    _delete_=True, grad_clip=dict(max_norm=35, norm_type=2))\n"""
configs/foveabox/fovea_align_r50_fpn_gn-head_mstrain_640-800_4x4_2x_coco.py,0,"b""_base_ = './fovea_r50_fpn_4x4_1x_coco.py'\nmodel = dict(\n    bbox_head=dict(\n        with_deform=True,\n        norm_cfg=dict(type='GN', num_groups=32, requires_grad=True)))\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(\n        type='Resize',\n        img_scale=[(1333, 640), (1333, 800)],\n        multiscale_mode='value',\n        keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ndata = dict(train=dict(pipeline=train_pipeline))\n# learning policy\nlr_config = dict(step=[16, 22])\ntotal_epochs = 24\n"""
configs/foveabox/fovea_r101_fpn_4x4_1x_coco.py,0,"b""_base_ = './fovea_r50_fpn_4x4_1x_coco.py'\nmodel = dict(pretrained='torchvision://resnet101', backbone=dict(depth=101))\n"""
configs/foveabox/fovea_r101_fpn_4x4_2x_coco.py,0,"b""_base_ = './fovea_r50_fpn_4x4_2x_coco.py'\nmodel = dict(pretrained='torchvision://resnet101', backbone=dict(depth=101))\n"""
configs/foveabox/fovea_r50_fpn_4x4_1x_coco.py,0,"b""_base_ = [\n    '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\n# model settings\nmodel = dict(\n    type='FOVEA',\n    pretrained='torchvision://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        norm_eval=True,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        start_level=1,\n        num_outs=5,\n        add_extra_convs='on_input'),\n    bbox_head=dict(\n        type='FoveaHead',\n        num_classes=80,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        strides=[8, 16, 32, 64, 128],\n        base_edge_list=[16, 32, 64, 128, 256],\n        scale_ranges=((1, 64), (32, 128), (64, 256), (128, 512), (256, 2048)),\n        sigma=0.4,\n        with_deform=False,\n        loss_cls=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=1.50,\n            alpha=0.4,\n            loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=0.11, loss_weight=1.0)))\n# training and testing settings\ntrain_cfg = dict()\ntest_cfg = dict(\n    nms_pre=1000,\n    score_thr=0.05,\n    nms=dict(type='nms', iou_thr=0.5),\n    max_per_img=100)\ndata = dict(samples_per_gpu=4, workers_per_gpu=4)\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\n"""
configs/foveabox/fovea_r50_fpn_4x4_2x_coco.py,0,"b""_base_ = './fovea_r50_fpn_4x4_1x_coco.py'\n# learning policy\nlr_config = dict(step=[16, 22])\ntotal_epochs = 24\n"""
configs/fp16/faster_rcnn_r50_fpn_fp16_1x_coco.py,0,"b""_base_ = '../faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'\n# fp16 settings\nfp16 = dict(loss_scale=512.)\n"""
configs/fp16/mask_rcnn_r50_fpn_fp16_1x_coco.py,0,"b""_base_ = '../mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py'\n# fp16 settings\nfp16 = dict(loss_scale=512.)\n"""
configs/fp16/retinanet_r50_fpn_fp16_1x_coco.py,0,"b""_base_ = '../retinanet/retinanet_r50_fpn_1x_coco.py'\n# fp16 settings\nfp16 = dict(loss_scale=512.)\n"""
configs/free_anchor/retinanet_free_anchor_r101_fpn_1x_coco.py,0,"b""_base_ = './retinanet_free_anchor_r50_fpn_1x_coco.py'\nmodel = dict(pretrained='torchvision://resnet101', backbone=dict(depth=101))\n"""
configs/free_anchor/retinanet_free_anchor_r50_fpn_1x_coco.py,0,"b""_base_ = '../retinanet/retinanet_r50_fpn_1x_coco.py'\nmodel = dict(\n    bbox_head=dict(\n        _delete_=True,\n        type='FreeAnchorRetinaHead',\n        num_classes=80,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        anchor_generator=dict(\n            type='AnchorGenerator',\n            octave_base_scale=4,\n            scales_per_octave=3,\n            ratios=[0.5, 1.0, 2.0],\n            strides=[8, 16, 32, 64, 128]),\n        bbox_coder=dict(\n            type='DeltaXYWHBBoxCoder',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[0.1, 0.1, 0.2, 0.2]),\n        loss_bbox=dict(type='SmoothL1Loss', beta=0.11, loss_weight=0.75)))\noptimizer_config = dict(\n    _delete_=True, grad_clip=dict(max_norm=35, norm_type=2))\n"""
configs/free_anchor/retinanet_free_anchor_x101_32x4d_fpn_1x_coco.py,0,"b""_base_ = './retinanet_free_anchor_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_32x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'))\n"""
configs/fsaf/fsaf_r101_fpn_1x_coco.py,0,"b""_base_ = './fsaf_r50_fpn_1x_coco.py'\nmodel = dict(pretrained='torchvision://resnet101', backbone=dict(depth=101))\n"""
configs/fsaf/fsaf_r50_fpn_1x_coco.py,0,"b""_base_ = '../retinanet/retinanet_r50_fpn_1x_coco.py'\n# model settings\nmodel = dict(\n    type='FSAF',\n    bbox_head=dict(\n        type='FSAFHead',\n        num_classes=80,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        reg_decoded_bbox=True,\n        # Only anchor-free branch is implemented. The anchor generator only\n        #  generates 1 anchor at each feature point, as a substitute of the\n        #  grid of features.\n        anchor_generator=dict(\n            type='AnchorGenerator',\n            octave_base_scale=1,\n            scales_per_octave=1,\n            ratios=[1.0],\n            strides=[8, 16, 32, 64, 128]),\n        bbox_coder=dict(_delete_=True, type='TBLRBBoxCoder', normalizer=4.0),\n        loss_cls=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0,\n            reduction='none'),\n        loss_bbox=dict(\n            _delete_=True,\n            type='IoULoss',\n            eps=1e-6,\n            loss_weight=1.0,\n            reduction='none'),\n    ))\n\n# training and testing settings\ntrain_cfg = dict(\n    assigner=dict(\n        _delete_=True,\n        type='CenterRegionAssigner',\n        pos_scale=0.2,\n        neg_scale=0.2,\n        min_pos_iof=0.01),\n    allowed_border=-1,\n    pos_weight=-1,\n    debug=False)\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(\n    _delete_=True, grad_clip=dict(max_norm=10, norm_type=2))\n"""
configs/fsaf/fsaf_x101_64x4d_fpn_1x_coco.py,0,"b""_base_ = './fsaf_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_64x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/gcnet/cascade_mask_rcnn_x101_32x4d_fpn_syncbn-backbone_1x_coco.py,0,"b""_base_ = '../cascade_rcnn/cascade_mask_rcnn_x101_32x4d_fpn_1x_coco.py'\nmodel = dict(\n    backbone=dict(\n        norm_cfg=dict(type='SyncBN', requires_grad=True), norm_eval=False))\n"""
configs/gcnet/cascade_mask_rcnn_x101_32x4d_fpn_syncbn-backbone_dconv_c3-c5_1x_coco.py,0,"b""_base_ = '../dcn/cascade_mask_rcnn_r50_fpn_dconv_c3-c5_1x_coco.py'\nmodel = dict(\n    backbone=dict(\n        norm_cfg=dict(type='SyncBN', requires_grad=True), norm_eval=False))\n"""
configs/gcnet/cascade_mask_rcnn_x101_32x4d_fpn_syncbn-backbone_dconv_c3-c5_r16_gcb_c3-c5_1x_coco.py,0,"b""_base_ = '../dcn/cascade_rcnn_r50_fpn_dconv_c3-c5_1x_coco.py'\nmodel = dict(\n    backbone=dict(\n        norm_cfg=dict(type='SyncBN', requires_grad=True),\n        norm_eval=False,\n        plugins=[\n            dict(\n                cfg=dict(type='ContextBlock', ratio=1. / 16),\n                stages=(False, True, True, True),\n                position='after_conv3')\n        ]))\n"""
configs/gcnet/cascade_mask_rcnn_x101_32x4d_fpn_syncbn-backbone_dconv_c3-c5_r4_gcb_c3-c5_1x_coco.py,0,"b""_base_ = '../dcn/cascade_mask_rcnn_r50_fpn_dconv_c3-c5_1x_coco.py'\nmodel = dict(\n    backbone=dict(\n        norm_cfg=dict(type='SyncBN', requires_grad=True),\n        norm_eval=False,\n        plugins=[\n            dict(\n                cfg=dict(type='ContextBlock', ratio=1. / 4),\n                stages=(False, True, True, True),\n                position='after_conv3')\n        ]))\n"""
configs/gcnet/cascade_mask_rcnn_x101_32x4d_fpn_syncbn-backbone_r16_gcb_c3-c5_1x_coco.py,0,"b""_base_ = '../cascade_rcnn/cascade_mask_rcnn_x101_32x4d_fpn_1x_coco.py'\nmodel = dict(\n    backbone=dict(\n        norm_cfg=dict(type='SyncBN', requires_grad=True),\n        norm_eval=False,\n        plugins=[\n            dict(\n                cfg=dict(type='ContextBlock', ratio=1. / 16),\n                stages=(False, True, True, True),\n                position='after_conv3')\n        ]))\n"""
configs/gcnet/cascade_mask_rcnn_x101_32x4d_fpn_syncbn-backbone_r4_gcb_c3-c5_1x_coco.py,0,"b""_base_ = '../cascade_rcnn/cascade_mask_rcnn_x101_32x4d_fpn_1x_coco.py'\nmodel = dict(\n    backbone=dict(\n        norm_cfg=dict(type='SyncBN', requires_grad=True),\n        norm_eval=False,\n        plugins=[\n            dict(\n                cfg=dict(type='ContextBlock', ratio=1. / 4),\n                stages=(False, True, True, True),\n                position='after_conv3')\n        ]))\n"""
configs/gcnet/mask_rcnn_r101_fpn_r16_gcb_c3-c5_1x_coco.py,0,"b""_base_ = '../mask_rcnn/mask_rcnn_r101_fpn_1x_coco.py'\nmodel = dict(\n    backbone=dict(plugins=[\n        dict(\n            cfg=dict(type='ContextBlock', ratio=1. / 16),\n            stages=(False, True, True, True),\n            position='after_conv3')\n    ]))\n"""
configs/gcnet/mask_rcnn_r101_fpn_r4_gcb_c3-c5_1x_coco.py,0,"b""_base_ = '../mask_rcnn/mask_rcnn_r101_fpn_1x_coco.py'\nmodel = dict(\n    backbone=dict(plugins=[\n        dict(\n            cfg=dict(type='ContextBlock', ratio=1. / 4),\n            stages=(False, True, True, True),\n            position='after_conv3')\n    ]))\n"""
configs/gcnet/mask_rcnn_r101_fpn_syncbn-backbone_1x_coco.py,0,"b""_base_ = '../mask_rcnn/mask_rcnn_r101_fpn_1x_coco.py'\nmodel = dict(\n    backbone=dict(\n        norm_cfg=dict(type='SyncBN', requires_grad=True), norm_eval=False))\n"""
configs/gcnet/mask_rcnn_r101_fpn_syncbn-backbone_r16_gcb_c3-c5_1x_coco.py,0,"b""_base_ = '../mask_rcnn/mask_rcnn_r101_fpn_1x_coco.py'\nmodel = dict(\n    backbone=dict(\n        norm_cfg=dict(type='SyncBN', requires_grad=True),\n        norm_eval=False,\n        plugins=[\n            dict(\n                cfg=dict(type='ContextBlock', ratio=1. / 16),\n                stages=(False, True, True, True),\n                position='after_conv3')\n        ]))\n"""
configs/gcnet/mask_rcnn_r101_fpn_syncbn-backbone_r4_gcb_c3-c5_1x_coco.py,0,"b""_base_ = '../mask_rcnn/mask_rcnn_r101_fpn_1x_coco.py'\nmodel = dict(\n    backbone=dict(\n        norm_cfg=dict(type='SyncBN', requires_grad=True),\n        norm_eval=False,\n        plugins=[\n            dict(\n                cfg=dict(type='ContextBlock', ratio=1. / 4),\n                stages=(False, True, True, True),\n                position='after_conv3')\n        ]))\n"""
configs/gcnet/mask_rcnn_r50_fpn_r16_gcb_c3-c5_1x_coco.py,0,"b""_base_ = '../mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    backbone=dict(plugins=[\n        dict(\n            cfg=dict(type='ContextBlock', ratio=1. / 16),\n            stages=(False, True, True, True),\n            position='after_conv3')\n    ]))\n"""
configs/gcnet/mask_rcnn_r50_fpn_r4_gcb_c3-c5_1x_coco.py,0,"b""_base_ = '../mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    backbone=dict(plugins=[\n        dict(\n            cfg=dict(type='ContextBlock', ratio=1. / 4),\n            stages=(False, True, True, True),\n            position='after_conv3')\n    ]))\n"""
configs/gcnet/mask_rcnn_r50_fpn_syncbn-backbone_1x_coco.py,0,"b""_base_ = '../mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    backbone=dict(\n        norm_cfg=dict(type='SyncBN', requires_grad=True), norm_eval=False))\n"""
configs/gcnet/mask_rcnn_r50_fpn_syncbn-backbone_r16_gcb_c3-c5_1x_coco.py,0,"b""_base_ = '../mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    backbone=dict(\n        norm_cfg=dict(type='SyncBN', requires_grad=True),\n        norm_eval=False,\n        plugins=[\n            dict(\n                cfg=dict(type='ContextBlock', ratio=1. / 16),\n                stages=(False, True, True, True),\n                position='after_conv3')\n        ]))\n"""
configs/gcnet/mask_rcnn_r50_fpn_syncbn-backbone_r4_gcb_c3-c5_1x_coco.py,0,"b""_base_ = '../mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    backbone=dict(\n        norm_cfg=dict(type='SyncBN', requires_grad=True),\n        norm_eval=False,\n        plugins=[\n            dict(\n                cfg=dict(type='ContextBlock', ratio=1. / 4),\n                stages=(False, True, True, True),\n                position='after_conv3')\n        ]))\n"""
configs/gcnet/mask_rcnn_x101_32x4d_fpn_syncbn-backbone_1x_coco.py,0,"b""_base_ = '../mask_rcnn/mask_rcnn_x101_32x4d_fpn_1x_coco.py'\nmodel = dict(\n    backbone=dict(\n        norm_cfg=dict(type='SyncBN', requires_grad=True), norm_eval=False))\n"""
configs/gcnet/mask_rcnn_x101_32x4d_fpn_syncbn-backbone_r16_gcb_c3-c5_1x_coco.py,0,"b""_base_ = '../mask_rcnn/mask_rcnn_x101_32x4d_fpn_1x_coco.py'\nmodel = dict(\n    backbone=dict(\n        norm_cfg=dict(type='SyncBN', requires_grad=True),\n        norm_eval=False,\n        plugins=[\n            dict(\n                cfg=dict(type='ContextBlock', ratio=1. / 16),\n                stages=(False, True, True, True),\n                position='after_conv3')\n        ]))\n"""
configs/gcnet/mask_rcnn_x101_32x4d_fpn_syncbn-backbone_r4_gcb_c3-c5_1x_coco.py,0,"b""_base_ = '../mask_rcnn/mask_rcnn_x101_32x4d_fpn_1x_coco.py'\nmodel = dict(\n    backbone=dict(\n        norm_cfg=dict(type='SyncBN', requires_grad=True),\n        norm_eval=False,\n        plugins=[\n            dict(\n                cfg=dict(type='ContextBlock', ratio=1. / 4),\n                stages=(False, True, True, True),\n                position='after_conv3')\n        ]))\n"""
configs/ghm/retinanet_ghm_r101_fpn_1x_coco.py,0,"b""_base_ = './retinanet_ghm_r50_fpn_1x_coco.py'\nmodel = dict(pretrained='torchvision://resnet101', backbone=dict(depth=101))\n"""
configs/ghm/retinanet_ghm_r50_fpn_1x_coco.py,0,"b""_base_ = '../retinanet/retinanet_r50_fpn_1x_coco.py'\nmodel = dict(\n    bbox_head=dict(\n        loss_cls=dict(\n            _delete_=True,\n            type='GHMC',\n            bins=30,\n            momentum=0.75,\n            use_sigmoid=True,\n            loss_weight=1.0),\n        loss_bbox=dict(\n            _delete_=True,\n            type='GHMR',\n            mu=0.02,\n            bins=10,\n            momentum=0.7,\n            loss_weight=10.0)))\noptimizer_config = dict(\n    _delete_=True, grad_clip=dict(max_norm=35, norm_type=2))\n"""
configs/ghm/retinanet_ghm_x101_32x4d_fpn_1x_coco.py,0,"b""_base_ = './retinanet_ghm_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_32x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/ghm/retinanet_ghm_x101_64x4d_fpn_1x_coco.py,0,"b""_base_ = './retinanet_ghm_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_64x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/gn+ws/faster_rcnn_r101_fpn_gn_ws-all_1x_coco.py,0,"b""_base_ = './faster_rcnn_r50_fpn_gn_ws-all_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://jhu/resnet101_gn_ws', backbone=dict(depth=101))\n"""
configs/gn+ws/faster_rcnn_r50_fpn_gn_ws-all_1x_coco.py,0,"b""_base_ = '../faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'\nconv_cfg = dict(type='ConvWS')\nnorm_cfg = dict(type='GN', num_groups=32, requires_grad=True)\nmodel = dict(\n    pretrained='open-mmlab://jhu/resnet50_gn_ws',\n    backbone=dict(conv_cfg=conv_cfg, norm_cfg=norm_cfg),\n    neck=dict(conv_cfg=conv_cfg, norm_cfg=norm_cfg),\n    roi_head=dict(\n        bbox_head=dict(\n            type='Shared4Conv1FCBBoxHead',\n            conv_out_channels=256,\n            conv_cfg=conv_cfg,\n            norm_cfg=norm_cfg)))\n"""
configs/gn+ws/faster_rcnn_x101_32x4d_fpn_gn_ws-all_1x_coco.py,0,"b""_base_ = './faster_rcnn_r50_fpn_gn_ws-all_1x_coco.py'\nconv_cfg = dict(type='ConvWS')\nnorm_cfg = dict(type='GN', num_groups=32, requires_grad=True)\nmodel = dict(\n    pretrained='open-mmlab://jhu/resnext101_32x4d_gn_ws',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch',\n        conv_cfg=conv_cfg,\n        norm_cfg=norm_cfg))\n"""
configs/gn+ws/faster_rcnn_x50_32x4d_fpn_gn_ws-all_1x_coco.py,0,"b""_base_ = './faster_rcnn_r50_fpn_gn_ws-all_1x_coco.py'\nconv_cfg = dict(type='ConvWS')\nnorm_cfg = dict(type='GN', num_groups=32, requires_grad=True)\nmodel = dict(\n    pretrained='open-mmlab://jhu/resnext50_32x4d_gn_ws',\n    backbone=dict(\n        type='ResNeXt',\n        depth=50,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch',\n        conv_cfg=conv_cfg,\n        norm_cfg=norm_cfg))\n"""
configs/gn+ws/mask_rcnn_r101_fpn_gn_ws-all_20_23_24e_coco.py,0,"b""_base_ = './mask_rcnn_r101_fpn_gn_ws-all_2x_coco.py'\n# learning policy\nlr_config = dict(step=[20, 23])\ntotal_epochs = 24\n"""
configs/gn+ws/mask_rcnn_r101_fpn_gn_ws-all_2x_coco.py,0,"b""_base_ = './mask_rcnn_r50_fpn_gn_ws-all_2x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://jhu/resnet101_gn_ws', backbone=dict(depth=101))\n"""
configs/gn+ws/mask_rcnn_r50_fpn_gn_ws-all_20_23_24e_coco.py,0,"b""_base_ = './mask_rcnn_r50_fpn_gn_ws-all_2x_coco.py'\n# learning policy\nlr_config = dict(step=[20, 23])\ntotal_epochs = 24\n"""
configs/gn+ws/mask_rcnn_r50_fpn_gn_ws-all_2x_coco.py,0,"b""_base_ = '../mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py'\nconv_cfg = dict(type='ConvWS')\nnorm_cfg = dict(type='GN', num_groups=32, requires_grad=True)\nmodel = dict(\n    pretrained='open-mmlab://jhu/resnet50_gn_ws',\n    backbone=dict(conv_cfg=conv_cfg, norm_cfg=norm_cfg),\n    neck=dict(conv_cfg=conv_cfg, norm_cfg=norm_cfg),\n    roi_head=dict(\n        bbox_head=dict(\n            type='Shared4Conv1FCBBoxHead',\n            conv_out_channels=256,\n            conv_cfg=conv_cfg,\n            norm_cfg=norm_cfg),\n        mask_head=dict(conv_cfg=conv_cfg, norm_cfg=norm_cfg)))\n# learning policy\nlr_config = dict(step=[16, 22])\ntotal_epochs = 24\n"""
configs/gn+ws/mask_rcnn_x101_32x4d_fpn_gn_ws-all_20_23_24e_coco.py,0,"b""_base_ = './mask_rcnn_x101_32x4d_fpn_gn_ws-all_2x_coco.py'\n# learning policy\nlr_config = dict(step=[20, 23])\ntotal_epochs = 24\n"""
configs/gn+ws/mask_rcnn_x101_32x4d_fpn_gn_ws-all_2x_coco.py,0,"b""_base_ = './mask_rcnn_r50_fpn_gn_ws-all_2x_coco.py'\n# model settings\nconv_cfg = dict(type='ConvWS')\nnorm_cfg = dict(type='GN', num_groups=32, requires_grad=True)\nmodel = dict(\n    pretrained='open-mmlab://jhu/resnext101_32x4d_gn_ws',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch',\n        conv_cfg=conv_cfg,\n        norm_cfg=norm_cfg))\n"""
configs/gn+ws/mask_rcnn_x50_32x4d_fpn_gn_ws-all_20_23_24e_coco.py,0,"b""_base_ = './mask_rcnn_x50_32x4d_fpn_gn_ws-all_2x_coco.py'\n# learning policy\nlr_config = dict(step=[20, 23])\ntotal_epochs = 24\n"""
configs/gn+ws/mask_rcnn_x50_32x4d_fpn_gn_ws-all_2x_coco.py,0,"b""_base_ = './mask_rcnn_r50_fpn_gn_ws-all_2x_coco.py'\n# model settings\nconv_cfg = dict(type='ConvWS')\nnorm_cfg = dict(type='GN', num_groups=32, requires_grad=True)\nmodel = dict(\n    pretrained='open-mmlab://jhu/resnext50_32x4d_gn_ws',\n    backbone=dict(\n        type='ResNeXt',\n        depth=50,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch',\n        conv_cfg=conv_cfg,\n        norm_cfg=norm_cfg))\n"""
configs/gn/mask_rcnn_r101_fpn_gn-all_2x_coco.py,0,"b""_base_ = './mask_rcnn_r50_fpn_gn-all_2x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://detectron/resnet101_gn', backbone=dict(depth=101))\n"""
configs/gn/mask_rcnn_r101_fpn_gn-all_3x_coco.py,0,"b""_base_ = './mask_rcnn_r101_fpn_gn-all_2x_coco.py'\n\n# learning policy\nlr_config = dict(step=[28, 34])\ntotal_epochs = 36\n"""
configs/gn/mask_rcnn_r50_fpn_gn-all_2x_coco.py,0,"b""_base_ = '../mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py'\nnorm_cfg = dict(type='GN', num_groups=32, requires_grad=True)\nmodel = dict(\n    pretrained='open-mmlab://detectron/resnet50_gn',\n    backbone=dict(norm_cfg=norm_cfg),\n    neck=dict(norm_cfg=norm_cfg),\n    roi_head=dict(\n        bbox_head=dict(\n            type='Shared4Conv1FCBBoxHead',\n            conv_out_channels=256,\n            norm_cfg=norm_cfg),\n        mask_head=dict(norm_cfg=norm_cfg)))\nimg_norm_cfg = dict(\n    mean=[103.530, 116.280, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n# learning policy\nlr_config = dict(step=[16, 22])\ntotal_epochs = 24\n"""
configs/gn/mask_rcnn_r50_fpn_gn-all_3x_coco.py,0,"b""_base_ = './mask_rcnn_r50_fpn_gn-all_2x_coco.py'\n\n# learning policy\nlr_config = dict(step=[28, 34])\ntotal_epochs = 36\n"""
configs/gn/mask_rcnn_r50_fpn_gn-all_contrib_2x_coco.py,0,"b""_base_ = '../mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py'\nnorm_cfg = dict(type='GN', num_groups=32, requires_grad=True)\nmodel = dict(\n    pretrained='open-mmlab://contrib/resnet50_gn',\n    backbone=dict(norm_cfg=norm_cfg),\n    neck=dict(norm_cfg=norm_cfg),\n    roi_head=dict(\n        bbox_head=dict(\n            type='Shared4Conv1FCBBoxHead',\n            conv_out_channels=256,\n            norm_cfg=norm_cfg),\n        mask_head=dict(norm_cfg=norm_cfg)))\n# learning policy\nlr_config = dict(step=[16, 22])\ntotal_epochs = 24\n"""
configs/gn/mask_rcnn_r50_fpn_gn-all_contrib_3x_coco.py,0,"b""_base_ = './mask_rcnn_r50_fpn_gn-all_contrib_2x_coco.py'\n\n# learning policy\nlr_config = dict(step=[28, 34])\ntotal_epochs = 36\n"""
configs/grid_rcnn/grid_rcnn_r101_fpn_gn-head_2x_coco.py,0,"b""_base_ = './grid_rcnn_r50_fpn_gn-head_2x_coco.py'\n\nmodel = dict(pretrained='torchvision://resnet101', backbone=dict(depth=101))\n"""
configs/grid_rcnn/grid_rcnn_r50_fpn_gn-head_1x_coco.py,0,"b""_base_ = ['../grid_rcnn/grid_rcnn_r50_fpn_gn-head_2x_coco.py']\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=0.001,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# runtime settings\ntotal_epochs = 12\n"""
configs/grid_rcnn/grid_rcnn_r50_fpn_gn-head_2x_coco.py,0,"b""_base_ = [\n    '../_base_/datasets/coco_detection.py', '../_base_/default_runtime.py'\n]\n# model settings\nmodel = dict(\n    type='GridRCNN',\n    pretrained='torchvision://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        norm_eval=True,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_generator=dict(\n            type='AnchorGenerator',\n            scales=[8],\n            ratios=[0.5, 1.0, 2.0],\n            strides=[4, 8, 16, 32, 64]),\n        bbox_coder=dict(\n            type='DeltaXYWHBBoxCoder',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[1.0, 1.0, 1.0, 1.0]),\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    roi_head=dict(\n        type='GridRoIHead',\n        bbox_roi_extractor=dict(\n            type='SingleRoIExtractor',\n            roi_layer=dict(type='RoIAlign', out_size=7, sample_num=0),\n            out_channels=256,\n            featmap_strides=[4, 8, 16, 32]),\n        bbox_head=dict(\n            type='Shared2FCBBoxHead',\n            with_reg=False,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=80,\n            bbox_coder=dict(\n                type='DeltaXYWHBBoxCoder',\n                target_means=[0., 0., 0., 0.],\n                target_stds=[0.1, 0.1, 0.2, 0.2]),\n            reg_class_agnostic=False),\n        grid_roi_extractor=dict(\n            type='SingleRoIExtractor',\n            roi_layer=dict(type='RoIAlign', out_size=14, sample_num=0),\n            out_channels=256,\n            featmap_strides=[4, 8, 16, 32]),\n        grid_head=dict(\n            type='GridHead',\n            grid_points=9,\n            num_convs=8,\n            in_channels=256,\n            point_feat_channels=64,\n            norm_cfg=dict(type='GN', num_groups=36),\n            loss_grid=dict(\n                type='CrossEntropyLoss', use_sigmoid=True, loss_weight=15))))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_radius=1,\n        pos_weight=-1,\n        max_num_grid=192,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.03, nms=dict(type='nms', iou_thr=0.3), max_per_img=100))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=None)\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=3665,\n    warmup_ratio=1.0 / 80,\n    step=[17, 23])\ntotal_epochs = 25\n"""
configs/grid_rcnn/grid_rcnn_x101_32x4d_fpn_gn-head_2x_coco.py,0,"b""_base_ = './grid_rcnn_r50_fpn_gn-head_2x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_32x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=None)\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=3665,\n    warmup_ratio=1.0 / 80,\n    step=[17, 23])\ntotal_epochs = 25\n"""
configs/grid_rcnn/grid_rcnn_x101_64x4d_fpn_gn-head_2x_coco.py,0,"b""_base_ = './grid_rcnn_x101_32x4d_fpn_gn-head_2x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_64x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'))\n"""
configs/groie/faster_rcnn_r50_fpn_groie_1x_coco.py,0,"b""_base_ = '../faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'\n# model settings\nmodel = dict(\n    roi_head=dict(\n        bbox_roi_extractor=dict(\n            type='SumGenericRoiExtractor',\n            roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n            out_channels=256,\n            featmap_strides=[4, 8, 16, 32],\n            pre_cfg=dict(\n                type='ConvModule',\n                in_channels=256,\n                out_channels=256,\n                kernel_size=5,\n                padding=2,\n                inplace=False,\n            ),\n            post_cfg=dict(\n                type='GeneralizedAttention',\n                in_channels=256,\n                spatial_range=-1,\n                num_heads=6,\n                attention_type='0100',\n                kv_stride=2))))\n"""
configs/groie/grid_rcnn_r50_fpn_gn-head_groie_1x_coco.py,0,"b""_base_ = '../grid_rcnn/grid_rcnn_r50_fpn_gn-head_1x_coco.py'\n# model settings\nmodel = dict(\n    roi_head=dict(\n        bbox_roi_extractor=dict(\n            type='SumGenericRoiExtractor',\n            roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n            out_channels=256,\n            featmap_strides=[4, 8, 16, 32],\n            pre_cfg=dict(\n                type='ConvModule',\n                in_channels=256,\n                out_channels=256,\n                kernel_size=5,\n                padding=2,\n                inplace=False,\n            ),\n            post_cfg=dict(\n                type='GeneralizedAttention',\n                in_channels=256,\n                spatial_range=-1,\n                num_heads=6,\n                attention_type='0100',\n                kv_stride=2)),\n        grid_roi_extractor=dict(\n            type='SumGenericRoiExtractor',\n            roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n            out_channels=256,\n            featmap_strides=[4, 8, 16, 32],\n            pre_cfg=dict(\n                type='ConvModule',\n                in_channels=256,\n                out_channels=256,\n                kernel_size=5,\n                padding=2,\n                inplace=False,\n            ),\n            post_cfg=dict(\n                type='GeneralizedAttention',\n                in_channels=256,\n                spatial_range=-1,\n                num_heads=6,\n                attention_type='0100',\n                kv_stride=2))))\n"""
configs/groie/mask_rcnn_r101_fpn_syncbn-backbone_r4_gcb_c3-c5_groie_1x_coco.py,0,"b""_base_ = '../gcnet/mask_rcnn_r101_fpn_syncbn-backbone_r4_gcb_c3-c5_1x_coco.py'\n# model settings\nmodel = dict(\n    roi_head=dict(\n        bbox_roi_extractor=dict(\n            type='SumGenericRoiExtractor',\n            roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n            out_channels=256,\n            featmap_strides=[4, 8, 16, 32],\n            pre_cfg=dict(\n                type='ConvModule',\n                in_channels=256,\n                out_channels=256,\n                kernel_size=5,\n                padding=2,\n                inplace=False,\n            ),\n            post_cfg=dict(\n                type='GeneralizedAttention',\n                in_channels=256,\n                spatial_range=-1,\n                num_heads=6,\n                attention_type='0100',\n                kv_stride=2)),\n        mask_roi_extractor=dict(\n            type='SumGenericRoiExtractor',\n            roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n            out_channels=256,\n            featmap_strides=[4, 8, 16, 32],\n            pre_cfg=dict(\n                type='ConvModule',\n                in_channels=256,\n                out_channels=256,\n                kernel_size=5,\n                padding=2,\n                inplace=False,\n            ),\n            post_cfg=dict(\n                type='GeneralizedAttention',\n                in_channels=256,\n                spatial_range=-1,\n                num_heads=6,\n                attention_type='0100',\n                kv_stride=2))))\n"""
configs/groie/mask_rcnn_r50_fpn_groie_1x_coco.py,0,"b""_base_ = '../mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py'\n# model settings\nmodel = dict(\n    roi_head=dict(\n        bbox_roi_extractor=dict(\n            type='SumGenericRoiExtractor',\n            roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n            out_channels=256,\n            featmap_strides=[4, 8, 16, 32],\n            pre_cfg=dict(\n                type='ConvModule',\n                in_channels=256,\n                out_channels=256,\n                kernel_size=5,\n                padding=2,\n                inplace=False,\n            ),\n            post_cfg=dict(\n                type='GeneralizedAttention',\n                in_channels=256,\n                spatial_range=-1,\n                num_heads=6,\n                attention_type='0100',\n                kv_stride=2)),\n        mask_roi_extractor=dict(\n            type='SumGenericRoiExtractor',\n            roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n            out_channels=256,\n            featmap_strides=[4, 8, 16, 32],\n            pre_cfg=dict(\n                type='ConvModule',\n                in_channels=256,\n                out_channels=256,\n                kernel_size=5,\n                padding=2,\n                inplace=False,\n            ),\n            post_cfg=dict(\n                type='GeneralizedAttention',\n                in_channels=256,\n                spatial_range=-1,\n                num_heads=6,\n                attention_type='0100',\n                kv_stride=2))))\n"""
configs/groie/mask_rcnn_r50_fpn_syncbn-backbone_r4_gcb_c3-c5_groie_1x_coco.py,0,"b""_base_ = '../gcnet/mask_rcnn_r50_fpn_syncbn-backbone_r4_gcb_c3-c5_1x_coco.py'\n# model settings\nmodel = dict(\n    roi_head=dict(\n        bbox_roi_extractor=dict(\n            type='SumGenericRoiExtractor',\n            roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n            out_channels=256,\n            featmap_strides=[4, 8, 16, 32],\n            pre_cfg=dict(\n                type='ConvModule',\n                in_channels=256,\n                out_channels=256,\n                kernel_size=5,\n                padding=2,\n                inplace=False,\n            ),\n            post_cfg=dict(\n                type='GeneralizedAttention',\n                in_channels=256,\n                spatial_range=-1,\n                num_heads=6,\n                attention_type='0100',\n                kv_stride=2)),\n        mask_roi_extractor=dict(\n            type='SumGenericRoiExtractor',\n            roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n            out_channels=256,\n            featmap_strides=[4, 8, 16, 32],\n            pre_cfg=dict(\n                type='ConvModule',\n                in_channels=256,\n                out_channels=256,\n                kernel_size=5,\n                padding=2,\n                inplace=False,\n            ),\n            post_cfg=dict(\n                type='GeneralizedAttention',\n                in_channels=256,\n                spatial_range=-1,\n                num_heads=6,\n                attention_type='0100',\n                kv_stride=2))))\n"""
configs/guided_anchoring/ga_fast_r50_caffe_fpn_1x_coco.py,0,"b""_base_ = '../fast_rcnn/fast_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://detectron2/resnet50_caffe',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=False),\n        norm_eval=True,\n        style='caffe'),\n    roi_head=dict(\n        bbox_head=dict(bbox_coder=dict(target_stds=[0.05, 0.05, 0.1, 0.1]))))\n# model training and testing settings\ntrain_cfg = dict(\n    rcnn=dict(\n        assigner=dict(pos_iou_thr=0.6, neg_iou_thr=0.6, min_pos_iou=0.6),\n        sampler=dict(num=256)))\ntest_cfg = dict(rcnn=dict(score_thr=1e-3))\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[103.530, 116.280, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadProposals', num_max_proposals=300),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'proposals', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadProposals', num_max_proposals=None),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img', 'proposals']),\n        ])\n]\ndata = dict(\n    train=dict(\n        proposal_file=data_root + 'proposals/ga_rpn_r50_fpn_1x_train2017.pkl',\n        pipeline=train_pipeline),\n    val=dict(\n        proposal_file=data_root + 'proposals/ga_rpn_r50_fpn_1x_val2017.pkl',\n        pipeline=test_pipeline),\n    test=dict(\n        proposal_file=data_root + 'proposals/ga_rpn_r50_fpn_1x_val2017.pkl',\n        pipeline=test_pipeline))\noptimizer_config = dict(\n    _delete_=True, grad_clip=dict(max_norm=35, norm_type=2))\n"""
configs/guided_anchoring/ga_faster_r101_caffe_fpn_1x_coco.py,0,"b""_base_ = './ga_faster_r50_caffe_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://detectron2/resnet101_caffe',\n    backbone=dict(depth=101))\n"""
configs/guided_anchoring/ga_faster_r50_caffe_fpn_1x_coco.py,0,"b""_base_ = '../faster_rcnn/faster_rcnn_r50_caffe_fpn_1x_coco.py'\nmodel = dict(\n    rpn_head=dict(\n        _delete_=True,\n        type='GARPNHead',\n        in_channels=256,\n        feat_channels=256,\n        approx_anchor_generator=dict(\n            type='AnchorGenerator',\n            octave_base_scale=8,\n            scales_per_octave=3,\n            ratios=[0.5, 1.0, 2.0],\n            strides=[4, 8, 16, 32, 64]),\n        square_anchor_generator=dict(\n            type='AnchorGenerator',\n            ratios=[1.0],\n            scales=[8],\n            strides=[4, 8, 16, 32, 64]),\n        anchor_coder=dict(\n            type='DeltaXYWHBBoxCoder',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[0.07, 0.07, 0.14, 0.14]),\n        bbox_coder=dict(\n            type='DeltaXYWHBBoxCoder',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[0.07, 0.07, 0.11, 0.11]),\n        loc_filter_thr=0.01,\n        loss_loc=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_shape=dict(type='BoundedIoULoss', beta=0.2, loss_weight=1.0),\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n    roi_head=dict(\n        bbox_head=dict(bbox_coder=dict(target_stds=[0.05, 0.05, 0.1, 0.1]))))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        ga_assigner=dict(\n            type='ApproxMaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        ga_sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=-1,\n        center_ratio=0.2,\n        ignore_ratio=0.5),\n    rpn_proposal=dict(max_num=300),\n    rcnn=dict(\n        assigner=dict(pos_iou_thr=0.6, neg_iou_thr=0.6, min_pos_iou=0.6),\n        sampler=dict(type='RandomSampler', num=256)))\ntest_cfg = dict(rpn=dict(max_num=300), rcnn=dict(score_thr=1e-3))\noptimizer_config = dict(\n    _delete_=True, grad_clip=dict(max_norm=35, norm_type=2))\n"""
configs/guided_anchoring/ga_faster_r50_fpn_1x_coco.py,0,"b""_base_ = '../faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    rpn_head=dict(\n        _delete_=True,\n        type='GARPNHead',\n        in_channels=256,\n        feat_channels=256,\n        approx_anchor_generator=dict(\n            type='AnchorGenerator',\n            octave_base_scale=8,\n            scales_per_octave=3,\n            ratios=[0.5, 1.0, 2.0],\n            strides=[4, 8, 16, 32, 64]),\n        square_anchor_generator=dict(\n            type='AnchorGenerator',\n            ratios=[1.0],\n            scales=[8],\n            strides=[4, 8, 16, 32, 64]),\n        anchor_coder=dict(\n            type='DeltaXYWHBBoxCoder',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[0.07, 0.07, 0.14, 0.14]),\n        bbox_coder=dict(\n            type='DeltaXYWHBBoxCoder',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[0.07, 0.07, 0.11, 0.11]),\n        loc_filter_thr=0.01,\n        loss_loc=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_shape=dict(type='BoundedIoULoss', beta=0.2, loss_weight=1.0),\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n    roi_head=dict(\n        bbox_head=dict(bbox_coder=dict(target_stds=[0.05, 0.05, 0.1, 0.1]))))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        ga_assigner=dict(\n            type='ApproxMaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        ga_sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=-1,\n        center_ratio=0.2,\n        ignore_ratio=0.5),\n    rpn_proposal=dict(max_num=300),\n    rcnn=dict(\n        assigner=dict(pos_iou_thr=0.6, neg_iou_thr=0.6, min_pos_iou=0.6),\n        sampler=dict(type='RandomSampler', num=256)))\ntest_cfg = dict(rpn=dict(max_num=300), rcnn=dict(score_thr=1e-3))\noptimizer_config = dict(\n    _delete_=True, grad_clip=dict(max_norm=35, norm_type=2))\n"""
configs/guided_anchoring/ga_faster_x101_32x4d_fpn_1x_coco.py,0,"b""_base_ = './ga_faster_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_32x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/guided_anchoring/ga_faster_x101_64x4d_fpn_1x_coco.py,0,"b""_base_ = './ga_faster_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_64x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/guided_anchoring/ga_retinanet_r101_caffe_fpn_1x_coco.py,0,"b""_base_ = './ga_retinanet_r50_caffe_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://detectron2/resnet101_caffe',\n    backbone=dict(depth=101))\n"""
configs/guided_anchoring/ga_retinanet_r101_caffe_fpn_mstrain_2x.py,0,"b""# model settings\nmodel = dict(\n    type='RetinaNet',\n    pretrained='open-mmlab://detectron2/resnet101_caffe',\n    backbone=dict(\n        type='ResNet',\n        depth=101,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=False),\n        norm_eval=True,\n        style='caffe'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        start_level=1,\n        add_extra_convs=True,\n        num_outs=5),\n    bbox_head=dict(\n        type='GARetinaHead',\n        num_classes=81,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        approx_anchor_generator=dict(\n            type='AnchorGenerator',\n            octave_base_scale=4,\n            scales_per_octave=3,\n            ratios=[0.5, 1.0, 2.0],\n            strides=[8, 16, 32, 64, 128]),\n        square_anchor_generator=dict(\n            type='AnchorGenerator',\n            ratios=[1.0],\n            scales=[4],\n            strides=[8, 16, 32, 64, 128]),\n        anchor_coder=dict(\n            type='DeltaXYWHBBoxCoder',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[1.0, 1.0, 1.0, 1.0]),\n        bbox_coder=dict(\n            type='DeltaXYWHBBoxCoder',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[1.0, 1.0, 1.0, 1.0]),\n        loc_filter_thr=0.01,\n        loss_loc=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_shape=dict(type='BoundedIoULoss', beta=0.2, loss_weight=1.0),\n        loss_cls=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=0.04, loss_weight=1.0)))\n# training and testing settings\ntrain_cfg = dict(\n    ga_assigner=dict(\n        type='ApproxMaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.4,\n        min_pos_iou=0.4,\n        ignore_iof_thr=-1),\n    ga_sampler=dict(\n        type='RandomSampler',\n        num=256,\n        pos_fraction=0.5,\n        neg_pos_ub=-1,\n        add_gt_as_proposals=False),\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n        min_pos_iou=0.0,\n        ignore_iof_thr=-1),\n    allowed_border=-1,\n    pos_weight=-1,\n    center_ratio=0.2,\n    ignore_ratio=0.5,\n    debug=False)\ntest_cfg = dict(\n    nms_pre=1000,\n    min_bbox_size=0,\n    score_thr=0.05,\n    nms=dict(type='nms', iou_thr=0.5),\n    max_per_img=100)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[103.530, 116.280, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(\n        type='Resize',\n        img_scale=[(1333, 480), (1333, 960)],\n        keep_ratio=True,\n        multiscale_mode='range'),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    samples_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        pipeline=train_pipeline),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        pipeline=test_pipeline),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        pipeline=test_pipeline))\nevaluation = dict(interval=1, metric='bbox')\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(\n    _delete_=True, grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 22])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 24\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/ga_retinanet_r101_caffe_fpn_mstrain_2x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/guided_anchoring/ga_retinanet_r50_caffe_fpn_1x_coco.py,0,"b""_base_ = '../retinanet/retinanet_r50_caffe_fpn_1x_coco.py'\nmodel = dict(\n    bbox_head=dict(\n        _delete_=True,\n        type='GARetinaHead',\n        num_classes=80,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        approx_anchor_generator=dict(\n            type='AnchorGenerator',\n            octave_base_scale=4,\n            scales_per_octave=3,\n            ratios=[0.5, 1.0, 2.0],\n            strides=[8, 16, 32, 64, 128]),\n        square_anchor_generator=dict(\n            type='AnchorGenerator',\n            ratios=[1.0],\n            scales=[4],\n            strides=[8, 16, 32, 64, 128]),\n        anchor_coder=dict(\n            type='DeltaXYWHBBoxCoder',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[1.0, 1.0, 1.0, 1.0]),\n        bbox_coder=dict(\n            type='DeltaXYWHBBoxCoder',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[1.0, 1.0, 1.0, 1.0]),\n        loc_filter_thr=0.01,\n        loss_loc=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_shape=dict(type='BoundedIoULoss', beta=0.2, loss_weight=1.0),\n        loss_cls=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=0.04, loss_weight=1.0)))\n# training and testing settings\ntrain_cfg = dict(\n    ga_assigner=dict(\n        type='ApproxMaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.4,\n        min_pos_iou=0.4,\n        ignore_iof_thr=-1),\n    ga_sampler=dict(\n        type='RandomSampler',\n        num=256,\n        pos_fraction=0.5,\n        neg_pos_ub=-1,\n        add_gt_as_proposals=False),\n    assigner=dict(neg_iou_thr=0.5, min_pos_iou=0.0),\n    center_ratio=0.2,\n    ignore_ratio=0.5)\noptimizer_config = dict(\n    _delete_=True, grad_clip=dict(max_norm=35, norm_type=2))\n"""
configs/guided_anchoring/ga_retinanet_r50_fpn_1x_coco.py,0,"b""_base_ = '../retinanet/retinanet_r50_fpn_1x_coco.py'\nmodel = dict(\n    bbox_head=dict(\n        _delete_=True,\n        type='GARetinaHead',\n        num_classes=80,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        approx_anchor_generator=dict(\n            type='AnchorGenerator',\n            octave_base_scale=4,\n            scales_per_octave=3,\n            ratios=[0.5, 1.0, 2.0],\n            strides=[8, 16, 32, 64, 128]),\n        square_anchor_generator=dict(\n            type='AnchorGenerator',\n            ratios=[1.0],\n            scales=[4],\n            strides=[8, 16, 32, 64, 128]),\n        anchor_coder=dict(\n            type='DeltaXYWHBBoxCoder',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[1.0, 1.0, 1.0, 1.0]),\n        bbox_coder=dict(\n            type='DeltaXYWHBBoxCoder',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[1.0, 1.0, 1.0, 1.0]),\n        loc_filter_thr=0.01,\n        loss_loc=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_shape=dict(type='BoundedIoULoss', beta=0.2, loss_weight=1.0),\n        loss_cls=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=0.04, loss_weight=1.0)))\n# training and testing settings\ntrain_cfg = dict(\n    ga_assigner=dict(\n        type='ApproxMaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.4,\n        min_pos_iou=0.4,\n        ignore_iof_thr=-1),\n    ga_sampler=dict(\n        type='RandomSampler',\n        num=256,\n        pos_fraction=0.5,\n        neg_pos_ub=-1,\n        add_gt_as_proposals=False),\n    assigner=dict(neg_iou_thr=0.5, min_pos_iou=0.0),\n    center_ratio=0.2,\n    ignore_ratio=0.5)\noptimizer_config = dict(\n    _delete_=True, grad_clip=dict(max_norm=35, norm_type=2))\n"""
configs/guided_anchoring/ga_retinanet_x101_32x4d_fpn_1x_coco.py,0,"b""_base_ = './ga_retinanet_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_32x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/guided_anchoring/ga_retinanet_x101_64x4d_fpn_1x_coco.py,0,"b""_base_ = './ga_retinanet_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_64x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/guided_anchoring/ga_rpn_r101_caffe_fpn_1x_coco.py,0,"b""_base_ = './ga_rpn_r50_caffe_fpn_1x_coco.py'\n# model settings\nmodel = dict(\n    pretrained='open-mmlab://detectron2/resnet101_caffe',\n    backbone=dict(depth=101))\n"""
configs/guided_anchoring/ga_rpn_r50_caffe_fpn_1x_coco.py,0,"b""_base_ = '../rpn/rpn_r50_caffe_fpn_1x_coco.py'\nmodel = dict(\n    rpn_head=dict(\n        _delete_=True,\n        type='GARPNHead',\n        in_channels=256,\n        feat_channels=256,\n        approx_anchor_generator=dict(\n            type='AnchorGenerator',\n            octave_base_scale=8,\n            scales_per_octave=3,\n            ratios=[0.5, 1.0, 2.0],\n            strides=[4, 8, 16, 32, 64]),\n        square_anchor_generator=dict(\n            type='AnchorGenerator',\n            ratios=[1.0],\n            scales=[8],\n            strides=[4, 8, 16, 32, 64]),\n        anchor_coder=dict(\n            type='DeltaXYWHBBoxCoder',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[0.07, 0.07, 0.14, 0.14]),\n        bbox_coder=dict(\n            type='DeltaXYWHBBoxCoder',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[0.07, 0.07, 0.11, 0.11]),\n        loc_filter_thr=0.01,\n        loss_loc=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_shape=dict(type='BoundedIoULoss', beta=0.2, loss_weight=1.0),\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        ga_assigner=dict(\n            type='ApproxMaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        ga_sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=-1,\n        center_ratio=0.2,\n        ignore_ratio=0.5))\noptimizer_config = dict(\n    _delete_=True, grad_clip=dict(max_norm=35, norm_type=2))\n"""
configs/guided_anchoring/ga_rpn_r50_fpn_1x_coco.py,0,"b""_base_ = '../rpn/rpn_r50_fpn_1x_coco.py'\nmodel = dict(\n    rpn_head=dict(\n        _delete_=True,\n        type='GARPNHead',\n        in_channels=256,\n        feat_channels=256,\n        approx_anchor_generator=dict(\n            type='AnchorGenerator',\n            octave_base_scale=8,\n            scales_per_octave=3,\n            ratios=[0.5, 1.0, 2.0],\n            strides=[4, 8, 16, 32, 64]),\n        square_anchor_generator=dict(\n            type='AnchorGenerator',\n            ratios=[1.0],\n            scales=[8],\n            strides=[4, 8, 16, 32, 64]),\n        anchor_coder=dict(\n            type='DeltaXYWHBBoxCoder',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[0.07, 0.07, 0.14, 0.14]),\n        bbox_coder=dict(\n            type='DeltaXYWHBBoxCoder',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[0.07, 0.07, 0.11, 0.11]),\n        loc_filter_thr=0.01,\n        loss_loc=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_shape=dict(type='BoundedIoULoss', beta=0.2, loss_weight=1.0),\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        ga_assigner=dict(\n            type='ApproxMaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        ga_sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=-1,\n        center_ratio=0.2,\n        ignore_ratio=0.5))\noptimizer_config = dict(\n    _delete_=True, grad_clip=dict(max_norm=35, norm_type=2))\n"""
configs/guided_anchoring/ga_rpn_x101_32x4d_fpn_1x_coco.py,0,"b""_base_ = './ga_rpn_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_32x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/guided_anchoring/ga_rpn_x101_64x4d_fpn_1x_coco.py,0,"b""_base_ = './ga_rpn_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_64x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/hrnet/cascade_mask_rcnn_hrnetv2p_w18_20e_coco.py,0,"b""_base_ = './cascade_mask_rcnn_hrnetv2p_w32_20e_coco.py'\n# model settings\nmodel = dict(\n    pretrained='open-mmlab://msra/hrnetv2_w18',\n    backbone=dict(\n        extra=dict(\n            stage2=dict(num_channels=(18, 36)),\n            stage3=dict(num_channels=(18, 36, 72)),\n            stage4=dict(num_channels=(18, 36, 72, 144)))),\n    neck=dict(type='HRFPN', in_channels=[18, 36, 72, 144], out_channels=256))\n"""
configs/hrnet/cascade_mask_rcnn_hrnetv2p_w32_20e_coco.py,0,"b""_base_ = '../cascade_rcnn/cascade_mask_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://msra/hrnetv2_w32',\n    backbone=dict(\n        _delete_=True,\n        type='HRNet',\n        extra=dict(\n            stage1=dict(\n                num_modules=1,\n                num_branches=1,\n                block='BOTTLENECK',\n                num_blocks=(4, ),\n                num_channels=(64, )),\n            stage2=dict(\n                num_modules=1,\n                num_branches=2,\n                block='BASIC',\n                num_blocks=(4, 4),\n                num_channels=(32, 64)),\n            stage3=dict(\n                num_modules=4,\n                num_branches=3,\n                block='BASIC',\n                num_blocks=(4, 4, 4),\n                num_channels=(32, 64, 128)),\n            stage4=dict(\n                num_modules=3,\n                num_branches=4,\n                block='BASIC',\n                num_blocks=(4, 4, 4, 4),\n                num_channels=(32, 64, 128, 256)))),\n    neck=dict(\n        _delete_=True,\n        type='HRFPN',\n        in_channels=[32, 64, 128, 256],\n        out_channels=256))\n# learning policy\nlr_config = dict(step=[16, 19])\ntotal_epochs = 20\n"""
configs/hrnet/cascade_mask_rcnn_hrnetv2p_w40_20e_coco.py,0,"b""_base_ = './cascade_mask_rcnn_hrnetv2p_w32_20e_coco.py'\n# model settings\nmodel = dict(\n    pretrained='open-mmlab://msra/hrnetv2_w40',\n    backbone=dict(\n        type='HRNet',\n        extra=dict(\n            stage2=dict(num_channels=(40, 80)),\n            stage3=dict(num_channels=(40, 80, 160)),\n            stage4=dict(num_channels=(40, 80, 160, 320)))),\n    neck=dict(type='HRFPN', in_channels=[40, 80, 160, 320], out_channels=256))\n"""
configs/hrnet/cascade_rcnn_hrnetv2p_w18_20e_coco.py,0,"b""_base_ = './cascade_rcnn_hrnetv2p_w32_20e_coco.py'\n# model settings\nmodel = dict(\n    pretrained='open-mmlab://msra/hrnetv2_w18',\n    backbone=dict(\n        extra=dict(\n            stage2=dict(num_channels=(18, 36)),\n            stage3=dict(num_channels=(18, 36, 72)),\n            stage4=dict(num_channels=(18, 36, 72, 144)))),\n    neck=dict(type='HRFPN', in_channels=[18, 36, 72, 144], out_channels=256))\n"""
configs/hrnet/cascade_rcnn_hrnetv2p_w32_20e_coco.py,0,"b""_base_ = '../cascade_rcnn/cascade_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://msra/hrnetv2_w32',\n    backbone=dict(\n        _delete_=True,\n        type='HRNet',\n        extra=dict(\n            stage1=dict(\n                num_modules=1,\n                num_branches=1,\n                block='BOTTLENECK',\n                num_blocks=(4, ),\n                num_channels=(64, )),\n            stage2=dict(\n                num_modules=1,\n                num_branches=2,\n                block='BASIC',\n                num_blocks=(4, 4),\n                num_channels=(32, 64)),\n            stage3=dict(\n                num_modules=4,\n                num_branches=3,\n                block='BASIC',\n                num_blocks=(4, 4, 4),\n                num_channels=(32, 64, 128)),\n            stage4=dict(\n                num_modules=3,\n                num_branches=4,\n                block='BASIC',\n                num_blocks=(4, 4, 4, 4),\n                num_channels=(32, 64, 128, 256)))),\n    neck=dict(\n        _delete_=True,\n        type='HRFPN',\n        in_channels=[32, 64, 128, 256],\n        out_channels=256))\n# learning policy\nlr_config = dict(step=[16, 19])\ntotal_epochs = 20\n"""
configs/hrnet/cascade_rcnn_hrnetv2p_w40_20e_coco.py,0,"b""_base_ = './cascade_rcnn_hrnetv2p_w32_20e_coco.py'\n# model settings\nmodel = dict(\n    pretrained='open-mmlab://msra/hrnetv2_w40',\n    backbone=dict(\n        type='HRNet',\n        extra=dict(\n            stage2=dict(num_channels=(40, 80)),\n            stage3=dict(num_channels=(40, 80, 160)),\n            stage4=dict(num_channels=(40, 80, 160, 320)))),\n    neck=dict(type='HRFPN', in_channels=[40, 80, 160, 320], out_channels=256))\n"""
configs/hrnet/faster_rcnn_hrnetv2p_w18_1x_coco.py,0,"b""_base_ = './faster_rcnn_hrnetv2p_w32_1x_coco.py'\n# model settings\nmodel = dict(\n    pretrained='open-mmlab://msra/hrnetv2_w18',\n    backbone=dict(\n        extra=dict(\n            stage2=dict(num_channels=(18, 36)),\n            stage3=dict(num_channels=(18, 36, 72)),\n            stage4=dict(num_channels=(18, 36, 72, 144)))),\n    neck=dict(type='HRFPN', in_channels=[18, 36, 72, 144], out_channels=256))\n"""
configs/hrnet/faster_rcnn_hrnetv2p_w18_2x_coco.py,0,"b""_base_ = './faster_rcnn_hrnetv2p_w18_1x_coco.py'\n\n# learning policy\nlr_config = dict(step=[16, 22])\ntotal_epochs = 24\n"""
configs/hrnet/faster_rcnn_hrnetv2p_w32_1x_coco.py,0,"b""_base_ = '../faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://msra/hrnetv2_w32',\n    backbone=dict(\n        _delete_=True,\n        type='HRNet',\n        extra=dict(\n            stage1=dict(\n                num_modules=1,\n                num_branches=1,\n                block='BOTTLENECK',\n                num_blocks=(4, ),\n                num_channels=(64, )),\n            stage2=dict(\n                num_modules=1,\n                num_branches=2,\n                block='BASIC',\n                num_blocks=(4, 4),\n                num_channels=(32, 64)),\n            stage3=dict(\n                num_modules=4,\n                num_branches=3,\n                block='BASIC',\n                num_blocks=(4, 4, 4),\n                num_channels=(32, 64, 128)),\n            stage4=dict(\n                num_modules=3,\n                num_branches=4,\n                block='BASIC',\n                num_blocks=(4, 4, 4, 4),\n                num_channels=(32, 64, 128, 256)))),\n    neck=dict(\n        _delete_=True,\n        type='HRFPN',\n        in_channels=[32, 64, 128, 256],\n        out_channels=256))\n"""
configs/hrnet/faster_rcnn_hrnetv2p_w32_2x_coco.py,0,"b""_base_ = './faster_rcnn_hrnetv2p_w32_1x_coco.py'\n# learning policy\nlr_config = dict(step=[16, 22])\ntotal_epochs = 24\n"""
configs/hrnet/faster_rcnn_hrnetv2p_w40_1x_coco.py,0,"b""_base_ = './faster_rcnn_hrnetv2p_w32_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://msra/hrnetv2_w40',\n    backbone=dict(\n        type='HRNet',\n        extra=dict(\n            stage2=dict(num_channels=(40, 80)),\n            stage3=dict(num_channels=(40, 80, 160)),\n            stage4=dict(num_channels=(40, 80, 160, 320)))),\n    neck=dict(type='HRFPN', in_channels=[40, 80, 160, 320], out_channels=256))\n"""
configs/hrnet/faster_rcnn_hrnetv2p_w40_2x_coco.py,0,"b""_base_ = './faster_rcnn_hrnetv2p_w40_1x_coco.py'\n# learning policy\nlr_config = dict(step=[16, 22])\ntotal_epochs = 24\n"""
configs/hrnet/fcos_hrnetv2p_w18_gn-head_4x4_1x_coco.py,0,"b""_base_ = './fcos_hrnetv2p_w32_gn-head_4x4_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://msra/hrnetv2_w18',\n    backbone=dict(\n        extra=dict(\n            stage2=dict(num_channels=(18, 36)),\n            stage3=dict(num_channels=(18, 36, 72)),\n            stage4=dict(num_channels=(18, 36, 72, 144)))),\n    neck=dict(type='HRFPN', in_channels=[18, 36, 72, 144], out_channels=256))\n"""
configs/hrnet/fcos_hrnetv2p_w18_gn-head_4x4_2x_coco.py,0,"b""_base_ = './fcos_hrnetv2p_w18_gn-head_4x4_1x_coco.py'\n# learning policy\nlr_config = dict(step=[16, 22])\ntotal_epochs = 24\n"""
configs/hrnet/fcos_hrnetv2p_w18_gn-head_mstrain_640-800_4x4_2x_coco.py,0,"b""_base_ = './fcos_hrnetv2p_w32_gn-head_mstrain_640-800_4x4_2x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://msra/hrnetv2_w18',\n    backbone=dict(\n        extra=dict(\n            stage2=dict(num_channels=(18, 36)),\n            stage3=dict(num_channels=(18, 36, 72)),\n            stage4=dict(num_channels=(18, 36, 72, 144)))),\n    neck=dict(type='HRFPN', in_channels=[18, 36, 72, 144], out_channels=256))\n"""
configs/hrnet/fcos_hrnetv2p_w32_gn-head_4x4_1x_coco.py,0,"b""_base_ = '../fcos/fcos_r50_caffe_fpn_gn-head_4x4_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://msra/hrnetv2_w32',\n    backbone=dict(\n        _delete_=True,\n        type='HRNet',\n        extra=dict(\n            stage1=dict(\n                num_modules=1,\n                num_branches=1,\n                block='BOTTLENECK',\n                num_blocks=(4, ),\n                num_channels=(64, )),\n            stage2=dict(\n                num_modules=1,\n                num_branches=2,\n                block='BASIC',\n                num_blocks=(4, 4),\n                num_channels=(32, 64)),\n            stage3=dict(\n                num_modules=4,\n                num_branches=3,\n                block='BASIC',\n                num_blocks=(4, 4, 4),\n                num_channels=(32, 64, 128)),\n            stage4=dict(\n                num_modules=3,\n                num_branches=4,\n                block='BASIC',\n                num_blocks=(4, 4, 4, 4),\n                num_channels=(32, 64, 128, 256)))),\n    neck=dict(\n        _delete_=True,\n        type='HRFPN',\n        in_channels=[32, 64, 128, 256],\n        out_channels=256,\n        stride=2,\n        num_outs=5))\n"""
configs/hrnet/fcos_hrnetv2p_w32_gn-head_4x4_2x_coco.py,0,"b""_base_ = './fcos_hrnetv2p_w32_gn-head_4x4_1x_coco.py'\n# learning policy\nlr_config = dict(step=[16, 22])\ntotal_epochs = 24\n"""
configs/hrnet/fcos_hrnetv2p_w32_gn-head_mstrain_640-800_4x4_2x_coco.py,0,"b""_base_ = './fcos_hrnetv2p_w32_gn-head_4x4_1x_coco.py'\nimg_norm_cfg = dict(\n    mean=[103.530, 116.280, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(\n        type='Resize',\n        img_scale=[(1333, 640), (1333, 800)],\n        multiscale_mode='value',\n        keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n# learning policy\nlr_config = dict(step=[16, 22])\ntotal_epochs = 24\n"""
configs/hrnet/fcos_hrnetv2p_w40_gn-head_mstrain_640-800_4x4_2x_coco.py,0,"b""_base_ = './fcos_hrnetv2p_w32_gn-head_mstrain_640-800_4x4_2x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://msra/hrnetv2_w40',\n    backbone=dict(\n        type='HRNet',\n        extra=dict(\n            stage2=dict(num_channels=(40, 80)),\n            stage3=dict(num_channels=(40, 80, 160)),\n            stage4=dict(num_channels=(40, 80, 160, 320)))),\n    neck=dict(type='HRFPN', in_channels=[40, 80, 160, 320], out_channels=256))\n"""
configs/hrnet/htc_hrnetv2p_w18_20e_coco.py,0,"b""_base_ = './htc_hrnetv2p_w32_20e_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://msra/hrnetv2_w18',\n    backbone=dict(\n        extra=dict(\n            stage2=dict(num_channels=(18, 36)),\n            stage3=dict(num_channels=(18, 36, 72)),\n            stage4=dict(num_channels=(18, 36, 72, 144)))),\n    neck=dict(type='HRFPN', in_channels=[18, 36, 72, 144], out_channels=256))\n"""
configs/hrnet/htc_hrnetv2p_w32_20e_coco.py,0,"b""_base_ = '../htc/htc_r50_fpn_20e_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://msra/hrnetv2_w32',\n    backbone=dict(\n        _delete_=True,\n        type='HRNet',\n        extra=dict(\n            stage1=dict(\n                num_modules=1,\n                num_branches=1,\n                block='BOTTLENECK',\n                num_blocks=(4, ),\n                num_channels=(64, )),\n            stage2=dict(\n                num_modules=1,\n                num_branches=2,\n                block='BASIC',\n                num_blocks=(4, 4),\n                num_channels=(32, 64)),\n            stage3=dict(\n                num_modules=4,\n                num_branches=3,\n                block='BASIC',\n                num_blocks=(4, 4, 4),\n                num_channels=(32, 64, 128)),\n            stage4=dict(\n                num_modules=3,\n                num_branches=4,\n                block='BASIC',\n                num_blocks=(4, 4, 4, 4),\n                num_channels=(32, 64, 128, 256)))),\n    neck=dict(\n        _delete_=True,\n        type='HRFPN',\n        in_channels=[32, 64, 128, 256],\n        out_channels=256))\n"""
configs/hrnet/htc_hrnetv2p_w40_20e_coco.py,0,"b""_base_ = './htc_hrnetv2p_w32_20e_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://msra/hrnetv2_w40',\n    backbone=dict(\n        type='HRNet',\n        extra=dict(\n            stage2=dict(num_channels=(40, 80)),\n            stage3=dict(num_channels=(40, 80, 160)),\n            stage4=dict(num_channels=(40, 80, 160, 320)))),\n    neck=dict(type='HRFPN', in_channels=[40, 80, 160, 320], out_channels=256))\n"""
configs/hrnet/htc_hrnetv2p_w40_28e_coco.py,0,"b""_base_ = './htc_hrnetv2p_w40_20e_coco.py'\n# learning policy\nlr_config = dict(step=[24, 27])\ntotal_epochs = 28\n"""
configs/hrnet/htc_x101_64x4d_fpn_16x1_28e_coco.py,0,"b""_base_ = '../htc/htc_x101_64x4d_fpn_16x1_20e_coco.py'\n# learning policy\nlr_config = dict(step=[24, 27])\ntotal_epochs = 28\n"""
configs/hrnet/mask_rcnn_hrnetv2p_w18_1x_coco.py,0,"b""_base_ = './mask_rcnn_hrnetv2p_w32_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://msra/hrnetv2_w18',\n    backbone=dict(\n        extra=dict(\n            stage2=dict(num_channels=(18, 36)),\n            stage3=dict(num_channels=(18, 36, 72)),\n            stage4=dict(num_channels=(18, 36, 72, 144)))),\n    neck=dict(type='HRFPN', in_channels=[18, 36, 72, 144], out_channels=256))\n"""
configs/hrnet/mask_rcnn_hrnetv2p_w18_2x_coco.py,0,"b""_base_ = './mask_rcnn_hrnetv2p_w18_1x_coco.py'\n# learning policy\nlr_config = dict(step=[16, 22])\ntotal_epochs = 24\n"""
configs/hrnet/mask_rcnn_hrnetv2p_w32_1x_coco.py,0,"b""_base_ = '../mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://msra/hrnetv2_w32',\n    backbone=dict(\n        _delete_=True,\n        type='HRNet',\n        extra=dict(\n            stage1=dict(\n                num_modules=1,\n                num_branches=1,\n                block='BOTTLENECK',\n                num_blocks=(4, ),\n                num_channels=(64, )),\n            stage2=dict(\n                num_modules=1,\n                num_branches=2,\n                block='BASIC',\n                num_blocks=(4, 4),\n                num_channels=(32, 64)),\n            stage3=dict(\n                num_modules=4,\n                num_branches=3,\n                block='BASIC',\n                num_blocks=(4, 4, 4),\n                num_channels=(32, 64, 128)),\n            stage4=dict(\n                num_modules=3,\n                num_branches=4,\n                block='BASIC',\n                num_blocks=(4, 4, 4, 4),\n                num_channels=(32, 64, 128, 256)))),\n    neck=dict(\n        _delete_=True,\n        type='HRFPN',\n        in_channels=[32, 64, 128, 256],\n        out_channels=256))\n"""
configs/hrnet/mask_rcnn_hrnetv2p_w32_2x_coco.py,0,"b""_base_ = './mask_rcnn_hrnetv2p_w32_1x_coco.py'\n# learning policy\nlr_config = dict(step=[16, 22])\ntotal_epochs = 24\n"""
configs/hrnet/mask_rcnn_hrnetv2p_w40_1x_coco.py,0,"b""_base_ = './mask_rcnn_hrnetv2p_w18_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://msra/hrnetv2_w40',\n    backbone=dict(\n        type='HRNet',\n        extra=dict(\n            stage2=dict(num_channels=(40, 80)),\n            stage3=dict(num_channels=(40, 80, 160)),\n            stage4=dict(num_channels=(40, 80, 160, 320)))),\n    neck=dict(type='HRFPN', in_channels=[40, 80, 160, 320], out_channels=256))\n"""
configs/hrnet/mask_rcnn_hrnetv2p_w40_2x_coco.py,0,"b""_base_ = './mask_rcnn_hrnetv2p_w40_1x_coco.py'\n# learning policy\nlr_config = dict(step=[16, 22])\ntotal_epochs = 24\n"""
configs/htc/htc_r101_fpn_20e_coco.py,0,"b""_base_ = './htc_r50_fpn_1x_coco.py'\nmodel = dict(pretrained='torchvision://resnet101', backbone=dict(depth=101))\n# learning policy\nlr_config = dict(step=[16, 19])\ntotal_epochs = 20\n"""
configs/htc/htc_r50_fpn_1x_coco.py,0,"b""_base_ = './htc_without_semantic_r50_fpn_1x_coco.py'\nmodel = dict(\n    roi_head=dict(\n        semantic_roi_extractor=dict(\n            type='SingleRoIExtractor',\n            roi_layer=dict(type='RoIAlign', out_size=14, sample_num=0),\n            out_channels=256,\n            featmap_strides=[8]),\n        semantic_head=dict(\n            type='FusedSemanticHead',\n            num_ins=5,\n            fusion_level=1,\n            num_convs=4,\n            in_channels=256,\n            conv_out_channels=256,\n            num_classes=183,\n            ignore_label=255,\n            loss_weight=0.2)))\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='LoadAnnotations', with_bbox=True, with_mask=True, with_seg=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='SegRescale', scale_factor=1 / 8),\n    dict(type='DefaultFormatBundle'),\n    dict(\n        type='Collect',\n        keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks', 'gt_semantic_seg']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip', flip_ratio=0.5),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    train=dict(\n        seg_prefix=data_root + 'stuffthingmaps/train2017/',\n        pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n"""
configs/htc/htc_r50_fpn_20e_coco.py,0,"b""_base_ = './htc_r50_fpn_1x_coco.py'\n# learning policy\nlr_config = dict(step=[16, 19])\ntotal_epochs = 20\n"""
configs/htc/htc_without_semantic_r50_fpn_1x_coco.py,0,"b""_base_ = [\n    '../_base_/datasets/coco_instance.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\n# model settings\nmodel = dict(\n    type='HybridTaskCascade',\n    pretrained='torchvision://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        norm_eval=True,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_generator=dict(\n            type='AnchorGenerator',\n            scales=[8],\n            ratios=[0.5, 1.0, 2.0],\n            strides=[4, 8, 16, 32, 64]),\n        bbox_coder=dict(\n            type='DeltaXYWHBBoxCoder',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[1.0, 1.0, 1.0, 1.0]),\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    roi_head=dict(\n        type='HybridTaskCascadeRoIHead',\n        interleaved=True,\n        mask_info_flow=True,\n        num_stages=3,\n        stage_loss_weights=[1, 0.5, 0.25],\n        bbox_roi_extractor=dict(\n            type='SingleRoIExtractor',\n            roi_layer=dict(type='RoIAlign', out_size=7, sample_num=0),\n            out_channels=256,\n            featmap_strides=[4, 8, 16, 32]),\n        bbox_head=[\n            dict(\n                type='Shared2FCBBoxHead',\n                in_channels=256,\n                fc_out_channels=1024,\n                roi_feat_size=7,\n                num_classes=80,\n                bbox_coder=dict(\n                    type='DeltaXYWHBBoxCoder',\n                    target_means=[0., 0., 0., 0.],\n                    target_stds=[0.1, 0.1, 0.2, 0.2]),\n                reg_class_agnostic=True,\n                loss_cls=dict(\n                    type='CrossEntropyLoss',\n                    use_sigmoid=False,\n                    loss_weight=1.0),\n                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,\n                               loss_weight=1.0)),\n            dict(\n                type='Shared2FCBBoxHead',\n                in_channels=256,\n                fc_out_channels=1024,\n                roi_feat_size=7,\n                num_classes=80,\n                bbox_coder=dict(\n                    type='DeltaXYWHBBoxCoder',\n                    target_means=[0., 0., 0., 0.],\n                    target_stds=[0.05, 0.05, 0.1, 0.1]),\n                reg_class_agnostic=True,\n                loss_cls=dict(\n                    type='CrossEntropyLoss',\n                    use_sigmoid=False,\n                    loss_weight=1.0),\n                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,\n                               loss_weight=1.0)),\n            dict(\n                type='Shared2FCBBoxHead',\n                in_channels=256,\n                fc_out_channels=1024,\n                roi_feat_size=7,\n                num_classes=80,\n                bbox_coder=dict(\n                    type='DeltaXYWHBBoxCoder',\n                    target_means=[0., 0., 0., 0.],\n                    target_stds=[0.033, 0.033, 0.067, 0.067]),\n                reg_class_agnostic=True,\n                loss_cls=dict(\n                    type='CrossEntropyLoss',\n                    use_sigmoid=False,\n                    loss_weight=1.0),\n                loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0))\n        ],\n        mask_roi_extractor=dict(\n            type='SingleRoIExtractor',\n            roi_layer=dict(type='RoIAlign', out_size=14, sample_num=0),\n            out_channels=256,\n            featmap_strides=[4, 8, 16, 32]),\n        mask_head=[\n            dict(\n                type='HTCMaskHead',\n                with_conv_res=False,\n                num_convs=4,\n                in_channels=256,\n                conv_out_channels=256,\n                num_classes=80,\n                loss_mask=dict(\n                    type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)),\n            dict(\n                type='HTCMaskHead',\n                num_convs=4,\n                in_channels=256,\n                conv_out_channels=256,\n                num_classes=80,\n                loss_mask=dict(\n                    type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)),\n            dict(\n                type='HTCMaskHead',\n                num_convs=4,\n                in_channels=256,\n                conv_out_channels=256,\n                num_classes=80,\n                loss_mask=dict(\n                    type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))\n        ]))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=[\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.5,\n                neg_iou_thr=0.5,\n                min_pos_iou=0.5,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.6,\n                neg_iou_thr=0.6,\n                min_pos_iou=0.6,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.7,\n                min_pos_iou=0.7,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False)\n    ])\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.001,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip', flip_ratio=0.5),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    val=dict(pipeline=test_pipeline), test=dict(pipeline=test_pipeline))\n"""
configs/htc/htc_x101_32x4d_fpn_16x1_20e_coco.py,0,"b""_base_ = './htc_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_32x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        norm_eval=True,\n        style='pytorch'))\ndata = dict(samples_per_gpu=1, workers_per_gpu=1)\n# learning policy\nlr_config = dict(step=[16, 19])\ntotal_epochs = 20\n"""
configs/htc/htc_x101_64x4d_fpn_16x1_20e_coco.py,0,"b""_base_ = './htc_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_64x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        norm_eval=True,\n        style='pytorch'))\ndata = dict(samples_per_gpu=1, workers_per_gpu=1)\n# learning policy\nlr_config = dict(step=[16, 19])\ntotal_epochs = 20\n"""
configs/htc/htc_x101_64x4d_fpn_dconv_c3-c5_mstrain_400_1400_16x1_20e_coco.py,0,"b""_base_ = './htc_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_64x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        norm_eval=True,\n        style='pytorch',\n        dcn=dict(type='DCN', deformable_groups=1, fallback_on_stride=False),\n        stage_with_dcn=(False, True, True, True)))\n# dataset settings\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='LoadAnnotations', with_bbox=True, with_mask=True, with_seg=True),\n    dict(\n        type='Resize',\n        img_scale=[(1600, 400), (1600, 1400)],\n        multiscale_mode='range',\n        keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='SegRescale', scale_factor=1 / 8),\n    dict(type='DefaultFormatBundle'),\n    dict(\n        type='Collect',\n        keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks', 'gt_semantic_seg']),\n]\ndata = dict(\n    samples_per_gpu=1, workers_per_gpu=1, train=dict(pipeline=train_pipeline))\n# learning policy\nlr_config = dict(step=[16, 19])\ntotal_epochs = 20\n"""
configs/instaboost/cascade_mask_rcnn_r101_fpn_instaboost_4x_coco.py,0,"b""_base_ = './cascade_mask_rcnn_r50_fpn_instaboost_4x_coco.py'\n\nmodel = dict(pretrained='torchvision://resnet101', backbone=dict(depth=101))\n"""
configs/instaboost/cascade_mask_rcnn_r50_fpn_instaboost_4x_coco.py,0,"b""_base_ = '../cascade_rcnn/cascade_mask_rcnn_r50_fpn_1x_coco.py'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='InstaBoost',\n        action_candidate=('normal', 'horizontal', 'skip'),\n        action_prob=(1, 0, 0),\n        scale=(0.8, 1.2),\n        dx=15,\n        dy=15,\n        theta=(-1, 1),\n        color_prob=0.5,\n        hflag=False,\n        aug_ratio=0.5),\n    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']),\n]\ndata = dict(train=dict(pipeline=train_pipeline))\n# learning policy\nlr_config = dict(step=[32, 44])\ntotal_epochs = 48\n"""
configs/instaboost/cascade_mask_rcnn_x101_64x4d_fpn_instaboost_4x_coco.py,0,"b""_base_ = './cascade_mask_rcnn_r50_fpn_instaboost_4x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_64x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/instaboost/mask_rcnn_r101_fpn_instaboost_4x_coco.py,0,"b""_base_ = './mask_rcnn_r50_fpn_instaboost_4x_coco.py'\nmodel = dict(pretrained='torchvision://resnet101', backbone=dict(depth=101))\n"""
configs/instaboost/mask_rcnn_r50_fpn_instaboost_4x_coco.py,0,"b""_base_ = '../mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='InstaBoost',\n        action_candidate=('normal', 'horizontal', 'skip'),\n        action_prob=(1, 0, 0),\n        scale=(0.8, 1.2),\n        dx=15,\n        dy=15,\n        theta=(-1, 1),\n        color_prob=0.5,\n        hflag=False,\n        aug_ratio=0.5),\n    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']),\n]\ndata = dict(train=dict(pipeline=train_pipeline))\n# learning policy\nlr_config = dict(step=[32, 44])\ntotal_epochs = 48\n"""
configs/instaboost/mask_rcnn_x101_64x4d_fpn_instaboost_4x_coco.py,0,"b""_base_ = './mask_rcnn_r50_fpn_instaboost_4x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_64x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/legacy_1.x/cascade_mask_rcnn_r50_fpn_1x_coco_v1.py,0,"b""_base_ = [\n    '../_base_/models/cascade_mask_rcnn_r50_fpn.py',\n    '../_base_/datasets/coco_instance.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\nmodel = dict(\n    type='CascadeRCNN',\n    pretrained='torchvision://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        norm_eval=True,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        anchor_generator=dict(type='LegacyAnchorGenerator', center_offset=0.5),\n        bbox_coder=dict(\n            type='LegacyDeltaXYWHBBoxCoder',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[1.0, 1.0, 1.0, 1.0])),\n    roi_head=dict(\n        bbox_roi_extractor=dict(\n            type='SingleRoIExtractor',\n            roi_layer=dict(\n                type='RoIAlign', out_size=7, sample_num=2, aligned=False)),\n        bbox_head=[\n            dict(\n                type='Shared2FCBBoxHead',\n                reg_class_agnostic=True,\n                in_channels=256,\n                fc_out_channels=1024,\n                roi_feat_size=7,\n                num_classes=80,\n                bbox_coder=dict(\n                    type='LegacyDeltaXYWHBBoxCoder',\n                    target_means=[0., 0., 0., 0.],\n                    target_stds=[0.1, 0.1, 0.2, 0.2])),\n            dict(\n                type='Shared2FCBBoxHead',\n                reg_class_agnostic=True,\n                in_channels=256,\n                fc_out_channels=1024,\n                roi_feat_size=7,\n                num_classes=80,\n                bbox_coder=dict(\n                    type='LegacyDeltaXYWHBBoxCoder',\n                    target_means=[0., 0., 0., 0.],\n                    target_stds=[0.05, 0.05, 0.1, 0.1])),\n            dict(\n                type='Shared2FCBBoxHead',\n                reg_class_agnostic=True,\n                in_channels=256,\n                fc_out_channels=1024,\n                roi_feat_size=7,\n                num_classes=80,\n                bbox_coder=dict(\n                    type='LegacyDeltaXYWHBBoxCoder',\n                    target_means=[0., 0., 0., 0.],\n                    target_stds=[0.033, 0.033, 0.067, 0.067])),\n        ],\n        mask_roi_extractor=dict(\n            type='SingleRoIExtractor',\n            roi_layer=dict(\n                type='RoIAlign', out_size=14, sample_num=2, aligned=False))))\ndist_params = dict(backend='nccl', port=29515)\n"""
configs/legacy_1.x/faster_rcnn_r50_fpn_1x_coco_v1.py,0,"b""_base_ = [\n    '../_base_/models/faster_rcnn_r50_fpn.py',\n    '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\n\nmodel = dict(\n    type='FasterRCNN',\n    pretrained='torchvision://resnet50',\n    rpn_head=dict(\n        type='RPNHead',\n        anchor_generator=dict(\n            type='LegacyAnchorGenerator',\n            center_offset=0.5,\n            scales=[8],\n            ratios=[0.5, 1.0, 2.0],\n            strides=[4, 8, 16, 32, 64]),\n        bbox_coder=dict(type='LegacyDeltaXYWHBBoxCoder'),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    roi_head=dict(\n        type='StandardRoIHead',\n        bbox_roi_extractor=dict(\n            type='SingleRoIExtractor',\n            roi_layer=dict(\n                type='RoIAlign', out_size=7, sample_num=2, aligned=False),\n            out_channels=256,\n            featmap_strides=[4, 8, 16, 32]),\n        bbox_head=dict(\n            bbox_coder=dict(type='LegacyDeltaXYWHBBoxCoder'),\n            loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0))))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn_proposal=dict(nms_post=2000, max_num=2000),\n    rcnn=dict(assigner=dict(match_low_quality=True)))\n"""
configs/legacy_1.x/mask_rcnn_r50_fpn_1x_coco_v1.py,0,"b""_base_ = [\n    '../_base_/models/mask_rcnn_r50_fpn.py',\n    '../_base_/datasets/coco_instance.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\n\nmodel = dict(\n    rpn_head=dict(\n        anchor_generator=dict(type='LegacyAnchorGenerator', center_offset=0.5),\n        bbox_coder=dict(type='LegacyDeltaXYWHBBoxCoder'),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    roi_head=dict(\n        bbox_roi_extractor=dict(\n            type='SingleRoIExtractor',\n            roi_layer=dict(\n                type='RoIAlign', out_size=7, sample_num=2, aligned=False)),\n        mask_roi_extractor=dict(\n            type='SingleRoIExtractor',\n            roi_layer=dict(\n                type='RoIAlign', out_size=14, sample_num=2, aligned=False)),\n        bbox_head=dict(\n            bbox_coder=dict(type='LegacyDeltaXYWHBBoxCoder'),\n            loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0))))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn_proposal=dict(nms_post=2000, max_num=2000),\n    rcnn=dict(assigner=dict(match_low_quality=True)))\n"""
configs/legacy_1.x/retinanet_r50_caffe_fpn_1x_coco_v1.py,0,"b""_base_ = './retinanet_r50_fpn_1x_coco_v1.py'\nmodel = dict(\n    pretrained='open-mmlab://detectron/resnet50_caffe',\n    backbone=dict(\n        norm_cfg=dict(requires_grad=False), norm_eval=True, style='caffe'))\n# use caffe img_norm\nimg_norm_cfg = dict(\n    mean=[102.9801, 115.9465, 122.7717], std=[1.0, 1.0, 1.0], to_rgb=False)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n"""
configs/legacy_1.x/retinanet_r50_fpn_1x_coco_v1.py,0,"b""_base_ = [\n    '../_base_/models/retinanet_r50_fpn.py',\n    '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\nmodel = dict(\n    bbox_head=dict(\n        type='RetinaHead',\n        anchor_generator=dict(\n            type='LegacyAnchorGenerator',\n            center_offset=0.5,\n            octave_base_scale=4,\n            scales_per_octave=3,\n            ratios=[0.5, 1.0, 2.0],\n            strides=[8, 16, 32, 64, 128]),\n        bbox_coder=dict(type='LegacyDeltaXYWHBBoxCoder'),\n        loss_bbox=dict(type='SmoothL1Loss', beta=0.11, loss_weight=1.0)))\n"""
configs/legacy_1.x/ssd300_coco_v1.py,0,"b""_base_ = [\n    '../_base_/models/ssd300.py', '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_2x.py', '../_base_/default_runtime.py'\n]\n# model settings\ninput_size = 300\nmodel = dict(\n    bbox_head=dict(\n        type='SSDHead',\n        anchor_generator=dict(\n            type='LegacySSDAnchorGenerator',\n            scale_major=False,\n            input_size=input_size,\n            basesize_ratio_range=(0.15, 0.9),\n            strides=[8, 16, 32, 64, 100, 300],\n            ratios=[[2], [2, 3], [2, 3], [2, 3], [2], [2]]),\n        bbox_coder=dict(\n            type='LegacyDeltaXYWHBBoxCoder',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[0.1, 0.1, 0.2, 0.2])))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(mean=[123.675, 116.28, 103.53], std=[1, 1, 1], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile', to_float32=True),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(\n        type='PhotoMetricDistortion',\n        brightness_delta=32,\n        contrast_range=(0.5, 1.5),\n        saturation_range=(0.5, 1.5),\n        hue_delta=18),\n    dict(\n        type='Expand',\n        mean=img_norm_cfg['mean'],\n        to_rgb=img_norm_cfg['to_rgb'],\n        ratio_range=(1, 4)),\n    dict(\n        type='MinIoURandomCrop',\n        min_ious=(0.1, 0.3, 0.5, 0.7, 0.9),\n        min_crop_size=0.3),\n    dict(type='Resize', img_scale=(300, 300), keep_ratio=False),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(300, 300),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=False),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    samples_per_gpu=8,\n    workers_per_gpu=3,\n    train=dict(\n        _delete_=True,\n        type='RepeatDataset',\n        times=5,\n        dataset=dict(\n            type=dataset_type,\n            ann_file=data_root + 'annotations/instances_train2017.json',\n            img_prefix=data_root + 'train2017/',\n            pipeline=train_pipeline)),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n# optimizer\noptimizer = dict(type='SGD', lr=2e-3, momentum=0.9, weight_decay=5e-4)\noptimizer_config = dict(_delete_=True)\ndist_params = dict(backend='nccl', port=29555)\n"""
configs/libra_rcnn/libra_fast_rcnn_r50_fpn_1x_coco.py,0,"b""_base_ = '../fast_rcnn/fast_rcnn_r50_fpn_1x_coco.py'\n# model settings\nmodel = dict(\n    neck=[\n        dict(\n            type='FPN',\n            in_channels=[256, 512, 1024, 2048],\n            out_channels=256,\n            num_outs=5),\n        dict(\n            type='BFP',\n            in_channels=256,\n            num_levels=5,\n            refine_level=2,\n            refine_type='non_local')\n    ],\n    roi_head=dict(\n        bbox_head=dict(\n            loss_bbox=dict(\n                _delete_=True,\n                type='BalancedL1Loss',\n                alpha=0.5,\n                gamma=1.5,\n                beta=1.0,\n                loss_weight=1.0))))\n# model training and testing settings\ntrain_cfg = dict(\n    rcnn=dict(\n        sampler=dict(\n            _delete_=True,\n            type='CombinedSampler',\n            num=512,\n            pos_fraction=0.25,\n            add_gt_as_proposals=True,\n            pos_sampler=dict(type='InstanceBalancedPosSampler'),\n            neg_sampler=dict(\n                type='IoUBalancedNegSampler',\n                floor_thr=-1,\n                floor_fraction=0,\n                num_bins=3))))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\ndata = dict(\n    train=dict(proposal_file=data_root +\n               'libra_proposals/rpn_r50_fpn_1x_train2017.pkl'),\n    val=dict(proposal_file=data_root +\n             'libra_proposals/rpn_r50_fpn_1x_val2017.pkl'),\n    test=dict(proposal_file=data_root +\n              'libra_proposals/rpn_r50_fpn_1x_val2017.pkl'))\n"""
configs/libra_rcnn/libra_faster_rcnn_r101_fpn_1x_coco.py,0,"b""_base_ = './libra_faster_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(pretrained='torchvision://resnet101', backbone=dict(depth=101))\n"""
configs/libra_rcnn/libra_faster_rcnn_r50_fpn_1x_coco.py,0,"b""_base_ = '../faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'\n# model settings\nmodel = dict(\n    neck=[\n        dict(\n            type='FPN',\n            in_channels=[256, 512, 1024, 2048],\n            out_channels=256,\n            num_outs=5),\n        dict(\n            type='BFP',\n            in_channels=256,\n            num_levels=5,\n            refine_level=2,\n            refine_type='non_local')\n    ],\n    roi_head=dict(\n        bbox_head=dict(\n            loss_bbox=dict(\n                _delete_=True,\n                type='BalancedL1Loss',\n                alpha=0.5,\n                gamma=1.5,\n                beta=1.0,\n                loss_weight=1.0))))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(sampler=dict(neg_pos_ub=5), allowed_border=-1),\n    rcnn=dict(\n        sampler=dict(\n            _delete_=True,\n            type='CombinedSampler',\n            num=512,\n            pos_fraction=0.25,\n            add_gt_as_proposals=True,\n            pos_sampler=dict(type='InstanceBalancedPosSampler'),\n            neg_sampler=dict(\n                type='IoUBalancedNegSampler',\n                floor_thr=-1,\n                floor_fraction=0,\n                num_bins=3))))\n"""
configs/libra_rcnn/libra_faster_rcnn_x101_64x4d_fpn_1x_coco.py,0,"b""_base_ = './libra_faster_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_64x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/libra_rcnn/libra_retinanet_r50_fpn_1x_coco.py,0,"b""_base_ = '../retinanet/retinanet_r50_fpn_1x_coco.py'\n# model settings\nmodel = dict(\n    neck=[\n        dict(\n            type='FPN',\n            in_channels=[256, 512, 1024, 2048],\n            out_channels=256,\n            start_level=1,\n            add_extra_convs='on_input',\n            num_outs=5),\n        dict(\n            type='BFP',\n            in_channels=256,\n            num_levels=5,\n            refine_level=1,\n            refine_type='non_local')\n    ],\n    bbox_head=dict(\n        loss_bbox=dict(\n            _delete_=True,\n            type='BalancedL1Loss',\n            alpha=0.5,\n            gamma=1.5,\n            beta=0.11,\n            loss_weight=1.0)))\n"""
configs/lvis/mask_rcnn_r101_fpn_sample1e-3_mstrain_2x_lvis.py,0,"b""_base_ = './mask_rcnn_r50_fpn_sample1e-3_mstrain_2x_lvis.py'\nmodel = dict(pretrained='torchvision://resnet101', backbone=dict(depth=101))\n"""
configs/lvis/mask_rcnn_r50_fpn_sample1e-3_mstrain_2x_lvis.py,0,"b""_base_ = [\n    '../_base_/models/mask_rcnn_r50_fpn.py',\n    '../_base_/datasets/lvis_instance.py',\n    '../_base_/schedules/schedule_2x.py', '../_base_/default_runtime.py'\n]\nmodel = dict(\n    roi_head=dict(\n        bbox_head=dict(num_classes=1230), mask_head=dict(num_classes=1230)))\ntest_cfg = dict(\n    rcnn=dict(\n        score_thr=0.0001,\n        # LVIS allows up to 300\n        max_per_img=300))\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n    dict(\n        type='Resize',\n        img_scale=[(1333, 640), (1333, 672), (1333, 704), (1333, 736),\n                   (1333, 768), (1333, 800)],\n        multiscale_mode='value',\n        keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']),\n]\ndata = dict(train=dict(dataset=dict(pipeline=train_pipeline)))\n"""
configs/lvis/mask_rcnn_x101_32x4d_fpn_sample1e-3_mstrain_2x_lvis.py,0,"b""_base_ = './mask_rcnn_r50_fpn_sample1e-3_mstrain_2x_lvis.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_32x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/lvis/mask_rcnn_x101_64x4d_fpn_sample1e-3_mstrain_2x_lvis.py,0,"b""_base_ = './mask_rcnn_r50_fpn_sample1e-3_mstrain_2x_lvis.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_64x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/mask_rcnn/mask_rcnn_r101_caffe_fpn_1x_coco.py,0,"b""_base_ = './mask_rcnn_r50_caffe_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://detectron2/resnet101_caffe',\n    backbone=dict(depth=101))\n"""
configs/mask_rcnn/mask_rcnn_r101_fpn_1x_coco.py,0,"b""_base_ = './mask_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(pretrained='torchvision://resnet101', backbone=dict(depth=101))\n"""
configs/mask_rcnn/mask_rcnn_r101_fpn_2x_coco.py,0,"b""_base_ = './mask_rcnn_r50_fpn_2x_coco.py'\nmodel = dict(pretrained='torchvision://resnet101', backbone=dict(depth=101))\n"""
configs/mask_rcnn/mask_rcnn_r50_caffe_c4_1x_coco.py,0,"b""_base_ = [\n    '../_base_/models/mask_rcnn_r50_caffe_c4.py',\n    '../_base_/datasets/coco_instance.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\n# use caffe img_norm\nimg_norm_cfg = dict(\n    mean=[103.530, 116.280, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\n"""
configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_1x_coco.py,0,"b""_base_ = './mask_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://detectron2/resnet50_caffe',\n    backbone=dict(norm_cfg=dict(requires_grad=False), style='caffe'))\n# use caffe img_norm\nimg_norm_cfg = dict(\n    mean=[103.530, 116.280, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n"""
configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_coco.py,0,"b""_base_ = './mask_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://detectron2/resnet50_caffe',\n    backbone=dict(norm_cfg=dict(requires_grad=False), style='caffe'))\n# use caffe img_norm\nimg_norm_cfg = dict(\n    mean=[103.530, 116.280, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='LoadAnnotations',\n        with_bbox=True,\n        with_mask=True,\n        poly2mask=False),\n    dict(\n        type='Resize',\n        img_scale=[(1333, 640), (1333, 672), (1333, 704), (1333, 736),\n                   (1333, 768), (1333, 800)],\n        multiscale_mode='value',\n        keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n"""
configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_2x_coco.py,0,"b""_base_ = './mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_coco.py'\n# learning policy\nlr_config = dict(step=[16, 23])\ntotal_epochs = 24\n"""
configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco.py,0,"b""_base_ = './mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_coco.py'\n# learning policy\nlr_config = dict(step=[28, 34])\ntotal_epochs = 36\n"""
configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain_1x_coco.py,0,"b""_base_ = './mask_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://detectron2/resnet50_caffe',\n    backbone=dict(norm_cfg=dict(requires_grad=False), style='caffe'))\n# use caffe img_norm\nimg_norm_cfg = dict(\n    mean=[103.530, 116.280, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n    dict(\n        type='Resize',\n        img_scale=[(1333, 640), (1333, 672), (1333, 704), (1333, 736),\n                   (1333, 768), (1333, 800)],\n        multiscale_mode='value',\n        keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n"""
configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_poly_1x_coco_v1.py,0,"b""_base_ = './mask_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnet50_caffe_bgr',\n    backbone=dict(norm_cfg=dict(requires_grad=False), style='caffe'),\n    rpn_head=dict(\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    roi_head=dict(\n        bbox_roi_extractor=dict(\n            roi_layer=dict(\n                type='RoIAlign', out_size=7, sample_num=2, aligned=False)),\n        bbox_head=dict(\n            loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n        mask_roi_extractor=dict(\n            roi_layer=dict(\n                type='RoIAlign', out_size=14, sample_num=2, aligned=False))))\n# use caffe img_norm\nimg_norm_cfg = dict(\n    mean=[103.530, 116.280, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='LoadAnnotations',\n        with_bbox=True,\n        with_mask=True,\n        poly2mask=False),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n"""
configs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py,0,"b""_base_ = [\n    '../_base_/models/mask_rcnn_r50_fpn.py',\n    '../_base_/datasets/coco_instance.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\n"""
configs/mask_rcnn/mask_rcnn_r50_fpn_2x_coco.py,0,"b""_base_ = [\n    '../_base_/models/mask_rcnn_r50_fpn.py',\n    '../_base_/datasets/coco_instance.py',\n    '../_base_/schedules/schedule_2x.py', '../_base_/default_runtime.py'\n]\n"""
configs/mask_rcnn/mask_rcnn_r50_fpn_poly_1x_coco.py,0,"b""_base_ = [\n    '../_base_/models/mask_rcnn_r50_fpn.py',\n    '../_base_/datasets/coco_instance.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\n\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='LoadAnnotations',\n        with_bbox=True,\n        with_mask=True,\n        poly2mask=False),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']),\n]\ndata = dict(train=dict(pipeline=train_pipeline))\n"""
configs/mask_rcnn/mask_rcnn_x101_32x4d_fpn_1x_coco.py,0,"b""_base_ = './mask_rcnn_r101_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_32x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/mask_rcnn/mask_rcnn_x101_32x4d_fpn_2x_coco.py,0,"b""_base_ = './mask_rcnn_r101_fpn_2x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_32x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/mask_rcnn/mask_rcnn_x101_32x8d_fpn_1x_coco.py,0,"b""_base_ = './mask_rcnn_r101_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://detectron2/resnext101_32x8d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=8,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=False),\n        style='pytorch'))\n\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[103.530, 116.280, 123.675],\n    std=[57.375, 57.120, 58.395],\n    to_rgb=False)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    samples_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        pipeline=train_pipeline),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        pipeline=test_pipeline),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        pipeline=test_pipeline))\n"""
configs/mask_rcnn/mask_rcnn_x101_32x8d_fpn_mstrain-poly_1x_coco.py,0,"b""_base_ = './mask_rcnn_r101_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://detectron2/resnext101_32x8d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=8,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=False),\n        style='pytorch'))\n\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[103.530, 116.280, 123.675],\n    std=[57.375, 57.120, 58.395],\n    to_rgb=False)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='LoadAnnotations',\n        with_bbox=True,\n        with_mask=True,\n        poly2mask=False),\n    dict(\n        type='Resize',\n        img_scale=[(1333, 640), (1333, 672), (1333, 704), (1333, 736),\n                   (1333, 768), (1333, 800)],\n        multiscale_mode='value',\n        keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n"""
configs/mask_rcnn/mask_rcnn_x101_32x8d_fpn_mstrain-poly_3x_coco.py,0,"b""_base_ = './mask_rcnn_r101_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://detectron2/resnext101_32x8d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=8,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=False),\n        style='pytorch'))\n\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[103.530, 116.280, 123.675],\n    std=[57.375, 57.120, 58.395],\n    to_rgb=False)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='LoadAnnotations',\n        with_bbox=True,\n        with_mask=True,\n        poly2mask=False),\n    dict(\n        type='Resize',\n        img_scale=[(1333, 640), (1333, 672), (1333, 704), (1333, 736),\n                   (1333, 768), (1333, 800)],\n        multiscale_mode='value',\n        keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n\nlr_config = dict(step=[28, 34])\ntotal_epochs = 36\n"""
configs/mask_rcnn/mask_rcnn_x101_64x4d_fpn_1x_coco.py,0,"b""_base_ = './mask_rcnn_x101_32x4d_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_64x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/mask_rcnn/mask_rcnn_x101_64x4d_fpn_2x_coco.py,0,"b""_base_ = './mask_rcnn_x101_32x4d_fpn_2x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_64x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/ms_rcnn/ms_rcnn_r101_caffe_fpn_1x_coco.py,0,"b""_base_ = './ms_rcnn_r50_caffe_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://detectron2/resnet101_caffe',\n    backbone=dict(depth=101))\n"""
configs/ms_rcnn/ms_rcnn_r101_caffe_fpn_2x_coco.py,0,"b""_base_ = './ms_rcnn_r101_caffe_fpn_1x_coco.py'\n# learning policy\nlr_config = dict(step=[16, 22])\ntotal_epochs = 24\n"""
configs/ms_rcnn/ms_rcnn_r50_caffe_fpn_1x_coco.py,0,"b""_base_ = '../mask_rcnn/mask_rcnn_r50_caffe_fpn_1x_coco.py'\nmodel = dict(\n    type='MaskScoringRCNN',\n    roi_head=dict(\n        type='MaskScoringRoIHead',\n        mask_iou_head=dict(\n            type='MaskIoUHead',\n            num_convs=4,\n            num_fcs=2,\n            roi_feat_size=14,\n            in_channels=256,\n            conv_out_channels=256,\n            fc_out_channels=1024,\n            num_classes=80)))\n# model training and testing settings\ntrain_cfg = dict(rcnn=dict(mask_thr_binary=0.5))\n"""
configs/ms_rcnn/ms_rcnn_r50_caffe_fpn_2x_coco.py,0,"b""_base_ = './ms_rcnn_r50_caffe_fpn_1x_coco.py'\n# learning policy\nlr_config = dict(step=[16, 22])\ntotal_epochs = 24\n"""
configs/ms_rcnn/ms_rcnn_r50_fpn_1x_coco.py,0,"b""_base_ = '../mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    type='MaskScoringRCNN',\n    roi_head=dict(\n        type='MaskScoringRoIHead',\n        mask_iou_head=dict(\n            type='MaskIoUHead',\n            num_convs=4,\n            num_fcs=2,\n            roi_feat_size=14,\n            in_channels=256,\n            conv_out_channels=256,\n            fc_out_channels=1024,\n            num_classes=80)))\n# model training and testing settings\ntrain_cfg = dict(rcnn=dict(mask_thr_binary=0.5))\n"""
configs/ms_rcnn/ms_rcnn_x101_32x4d_fpn_1x_coco.py,0,"b""_base_ = './ms_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_32x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/ms_rcnn/ms_rcnn_x101_64x4d_fpn_1x_coco.py,0,"b""_base_ = './ms_rcnn_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_64x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/ms_rcnn/ms_rcnn_x101_64x4d_fpn_2x_coco.py,0,"b""_base_ = './ms_rcnn_x101_64x4d_fpn_1x_coco.py'\n# learning policy\nlr_config = dict(step=[16, 22])\ntotal_epochs = 24\n"""
configs/nas_fcos/nas_fcos_fcoshead_r50_caffe_fpn_gn-head_4x4_1x_coco.py,0,"b""_base_ = [\n    '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\n\nmodel = dict(\n    type='NASFCOS',\n    pretrained='open-mmlab://resnet50_caffe_bgr',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=False, eps=0),\n        style='caffe'),\n    neck=dict(\n        type='NASFCOS_FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        start_level=1,\n        add_extra_convs=True,\n        num_outs=5,\n        norm_cfg=dict(type='BN'),\n        conv_cfg=dict(type='DCNv2', deformable_groups=2)),\n    bbox_head=dict(\n        type='FCOSHead',\n        num_classes=80,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        strides=[8, 16, 32, 64, 128],\n        norm_cfg=dict(type='GN', num_groups=32),\n        loss_cls=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_bbox=dict(type='IoULoss', loss_weight=1.0),\n        loss_centerness=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0)))\n\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.4,\n        min_pos_iou=0,\n        ignore_iof_thr=-1),\n    allowed_border=-1,\n    pos_weight=-1,\n    debug=False)\ntest_cfg = dict(\n    nms_pre=1000,\n    min_bbox_size=0,\n    score_thr=0.05,\n    nms=dict(type='nms', iou_thr=0.6),\n    max_per_img=100)\n\nimg_norm_cfg = dict(\n    mean=[103.530, 116.280, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)\n\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\n\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\n\ndata = dict(\n    samples_per_gpu=4,\n    workers_per_gpu=2,\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n\noptimizer = dict(\n    lr=0.01, paramwise_cfg=dict(bias_lr_mult=2., bias_decay_mult=0.))\n"""
configs/nas_fcos/nas_fcos_nashead_r50_caffe_fpn_gn-head_4x4_1x_coco.py,0,"b""_base_ = [\n    '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\n\nmodel = dict(\n    type='NASFCOS',\n    pretrained='open-mmlab://resnet50_caffe_bgr',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=False, eps=0),\n        style='caffe'),\n    neck=dict(\n        type='NASFCOS_FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        start_level=1,\n        add_extra_convs=True,\n        num_outs=5,\n        norm_cfg=dict(type='BN'),\n        conv_cfg=dict(type='DCNv2', deformable_groups=2)),\n    bbox_head=dict(\n        type='NASFCOSHead',\n        num_classes=80,\n        in_channels=256,\n        feat_channels=256,\n        strides=[8, 16, 32, 64, 128],\n        norm_cfg=dict(type='GN', num_groups=32),\n        loss_cls=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_bbox=dict(type='IoULoss', loss_weight=1.0),\n        loss_centerness=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0)))\n\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.4,\n        min_pos_iou=0,\n        ignore_iof_thr=-1),\n    allowed_border=-1,\n    pos_weight=-1,\n    debug=False)\ntest_cfg = dict(\n    nms_pre=1000,\n    min_bbox_size=0,\n    score_thr=0.05,\n    nms=dict(type='nms', iou_thr=0.6),\n    max_per_img=100)\n\nimg_norm_cfg = dict(\n    mean=[103.530, 116.280, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)\n\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\n\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\n\ndata = dict(\n    samples_per_gpu=4,\n    workers_per_gpu=2,\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n\noptimizer = dict(\n    lr=0.01, paramwise_cfg=dict(bias_lr_mult=2., bias_decay_mult=0.))\n"""
configs/nas_fpn/retinanet_r50_fpn_crop640_50e_coco.py,0,"b""_base_ = [\n    '../_base_/models/retinanet_r50_fpn.py',\n    '../_base_/datasets/coco_detection.py', '../_base_/default_runtime.py'\n]\ncudnn_benchmark = True\nnorm_cfg = dict(type='BN', requires_grad=True)\nmodel = dict(\n    pretrained='torchvision://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=norm_cfg,\n        norm_eval=False,\n        style='pytorch'),\n    neck=dict(\n        relu_before_extra_convs=True,\n        no_norm_on_lateral=True,\n        norm_cfg=norm_cfg),\n    bbox_head=dict(type='RetinaSepBNHead', num_ins=5, norm_cfg=norm_cfg))\n# training and testing settings\ntrain_cfg = dict(assigner=dict(neg_iou_thr=0.5))\n# dataset settings\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(\n        type='Resize',\n        img_scale=(640, 640),\n        ratio_range=(0.8, 1.2),\n        keep_ratio=True),\n    dict(type='RandomCrop', crop_size=(640, 640)),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size=(640, 640)),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(640, 640),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=64),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    samples_per_gpu=8,\n    workers_per_gpu=4,\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n# optimizer\noptimizer = dict(\n    type='SGD',\n    lr=0.08,\n    momentum=0.9,\n    weight_decay=0.0001,\n    paramwise_cfg=dict(norm_decay_mult=0, bypass_duplicate=True))\noptimizer_config = dict(grad_clip=None)\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=1000,\n    warmup_ratio=0.1,\n    step=[30, 40])\n# runtime settings\ntotal_epochs = 50\n"""
configs/nas_fpn/retinanet_r50_nasfpn_crop640_50e_coco.py,0,"b""_base_ = [\n    '../_base_/models/retinanet_r50_fpn.py',\n    '../_base_/datasets/coco_detection.py', '../_base_/default_runtime.py'\n]\ncudnn_benchmark = True\n# model settings\nnorm_cfg = dict(type='BN', requires_grad=True)\nmodel = dict(\n    type='RetinaNet',\n    pretrained='torchvision://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=norm_cfg,\n        norm_eval=False,\n        style='pytorch'),\n    neck=dict(type='NASFPN', stack_times=7, norm_cfg=norm_cfg),\n    bbox_head=dict(type='RetinaSepBNHead', num_ins=5, norm_cfg=norm_cfg))\n# training and testing settings\ntrain_cfg = dict(assigner=dict(neg_iou_thr=0.5))\n# dataset settings\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(\n        type='Resize',\n        img_scale=(640, 640),\n        ratio_range=(0.8, 1.2),\n        keep_ratio=True),\n    dict(type='RandomCrop', crop_size=(640, 640)),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size=(640, 640)),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(640, 640),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=128),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    samples_per_gpu=8,\n    workers_per_gpu=4,\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n# optimizer\noptimizer = dict(\n    type='SGD',\n    lr=0.08,\n    momentum=0.9,\n    weight_decay=0.0001,\n    paramwise_cfg=dict(norm_decay_mult=0, bypass_duplicate=True))\noptimizer_config = dict(grad_clip=None)\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=1000,\n    warmup_ratio=0.1,\n    step=[30, 40])\n# runtime settings\ntotal_epochs = 50\n"""
configs/pafpn/faster_rcnn_r50_pafpn_1x_coco.py,0,"b""_base_ = '../faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'\n\nmodel = dict(\n    neck=dict(\n        type='PAFPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5))\n"""
configs/pascal_voc/faster_rcnn_r50_fpn_1x_voc0712.py,0,"b""_base_ = [\n    '../_base_/models/faster_rcnn_r50_fpn.py', '../_base_/datasets/voc0712.py',\n    '../_base_/default_runtime.py'\n]\nmodel = dict(roi_head=dict(bbox_head=dict(num_classes=20)))\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=None)\n# learning policy\n# actual epoch = 3 * 3 = 9\nlr_config = dict(policy='step', step=[3])\n# runtime settings\ntotal_epochs = 4  # actual epoch = 4 * 3 = 12\n"""
configs/pascal_voc/ssd300_voc0712.py,0,"b""_base_ = [\n    '../_base_/models/ssd300.py', '../_base_/datasets/voc0712.py',\n    '../_base_/default_runtime.py'\n]\nmodel = dict(\n    bbox_head=dict(\n        num_classes=20, anchor_generator=dict(basesize_ratio_range=(0.2,\n                                                                    0.9))))\n# dataset settings\ndataset_type = 'VOCDataset'\ndata_root = 'data/VOCdevkit/'\nimg_norm_cfg = dict(mean=[123.675, 116.28, 103.53], std=[1, 1, 1], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile', to_float32=True),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(\n        type='PhotoMetricDistortion',\n        brightness_delta=32,\n        contrast_range=(0.5, 1.5),\n        saturation_range=(0.5, 1.5),\n        hue_delta=18),\n    dict(\n        type='Expand',\n        mean=img_norm_cfg['mean'],\n        to_rgb=img_norm_cfg['to_rgb'],\n        ratio_range=(1, 4)),\n    dict(\n        type='MinIoURandomCrop',\n        min_ious=(0.1, 0.3, 0.5, 0.7, 0.9),\n        min_crop_size=0.3),\n    dict(type='Resize', img_scale=(300, 300), keep_ratio=False),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(300, 300),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=False),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    samples_per_gpu=8,\n    workers_per_gpu=3,\n    train=dict(\n        type='RepeatDataset', times=10, dataset=dict(pipeline=train_pipeline)),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n# optimizer\noptimizer = dict(type='SGD', lr=1e-3, momentum=0.9, weight_decay=5e-4)\noptimizer_config = dict()\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=0.001,\n    step=[16, 20])\ncheckpoint_config = dict(interval=1)\n# runtime settings\ntotal_epochs = 24\n"""
configs/pascal_voc/ssd512_voc0712.py,0,"b""_base_ = 'ssd300_voc0712.py'\ninput_size = 512\nmodel = dict(\n    backbone=dict(input_size=input_size),\n    bbox_head=dict(\n        in_channels=(512, 1024, 512, 256, 256, 256, 256),\n        anchor_generator=dict(\n            input_size=input_size,\n            strides=[8, 16, 32, 64, 128, 256, 512],\n            basesize_ratio_range=(0.15, 0.9),\n            ratios=([2], [2, 3], [2, 3], [2, 3], [2, 3], [2], [2]))))\nimg_norm_cfg = dict(mean=[123.675, 116.28, 103.53], std=[1, 1, 1], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile', to_float32=True),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(\n        type='PhotoMetricDistortion',\n        brightness_delta=32,\n        contrast_range=(0.5, 1.5),\n        saturation_range=(0.5, 1.5),\n        hue_delta=18),\n    dict(\n        type='Expand',\n        mean=img_norm_cfg['mean'],\n        to_rgb=img_norm_cfg['to_rgb'],\n        ratio_range=(1, 4)),\n    dict(\n        type='MinIoURandomCrop',\n        min_ious=(0.1, 0.3, 0.5, 0.7, 0.9),\n        min_crop_size=0.3),\n    dict(type='Resize', img_scale=(512, 512), keep_ratio=False),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(512, 512),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=False),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    train=dict(dataset=dict(pipeline=train_pipeline)),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n"""
configs/pisa/pisa_faster_rcnn_r50_fpn_1x_coco.py,0,"b""_base_ = '../faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'\n\nmodel = dict(\n    roi_head=dict(\n        type='PISARoIHead',\n        bbox_head=dict(\n            loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0))))\n\ntrain_cfg = dict(\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        sampler=dict(\n            type='ScoreHLRSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True,\n            k=0.5,\n            bias=0.),\n        isr=dict(k=2, bias=0),\n        carl=dict(k=1, bias=0.2)))\n\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0))\n"""
configs/pisa/pisa_faster_rcnn_x101_32x4d_fpn_1x_coco.py,0,"b""_base_ = '../faster_rcnn/faster_rcnn_x101_32x4d_fpn_1x_coco.py'\n\nmodel = dict(\n    roi_head=dict(\n        type='PISARoIHead',\n        bbox_head=dict(\n            loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0))))\n\ntrain_cfg = dict(\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        sampler=dict(\n            type='ScoreHLRSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True,\n            k=0.5,\n            bias=0.),\n        isr=dict(k=2, bias=0),\n        carl=dict(k=1, bias=0.2)))\n\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0))\n"""
configs/pisa/pisa_mask_rcnn_r50_fpn_1x_coco.py,0,"b""_base_ = '../mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py'\n\nmodel = dict(\n    roi_head=dict(\n        type='PISARoIHead',\n        bbox_head=dict(\n            loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0))))\n\ntrain_cfg = dict(\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        sampler=dict(\n            type='ScoreHLRSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True,\n            k=0.5,\n            bias=0.),\n        isr=dict(k=2, bias=0),\n        carl=dict(k=1, bias=0.2)))\n\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0))\n"""
configs/pisa/pisa_mask_rcnn_x101_32x4d_fpn_1x_coco.py,0,"b""_base_ = '../mask_rcnn/mask_rcnn_x101_32x4d_fpn_1x_coco.py'\n\nmodel = dict(\n    roi_head=dict(\n        type='PISARoIHead',\n        bbox_head=dict(\n            loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0))))\n\ntrain_cfg = dict(\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        sampler=dict(\n            type='ScoreHLRSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True,\n            k=0.5,\n            bias=0.),\n        isr=dict(k=2, bias=0),\n        carl=dict(k=1, bias=0.2)))\n\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0))\n"""
configs/pisa/pisa_retinanet_r50_fpn_1x_coco.py,0,"b""_base_ = '../retinanet/retinanet_r50_fpn_1x_coco.py'\n\nmodel = dict(\n    bbox_head=dict(\n        type='PISARetinaHead',\n        loss_bbox=dict(type='SmoothL1Loss', beta=0.11, loss_weight=1.0)))\n\ntrain_cfg = dict(isr=dict(k=2., bias=0.), carl=dict(k=1., bias=0.2))\n"""
configs/pisa/pisa_retinanet_x101_32x4d_fpn_1x_coco.py,0,"b""_base_ = '../retinanet/retinanet_x101_32x4d_fpn_1x_coco.py'\n\nmodel = dict(\n    bbox_head=dict(\n        type='PISARetinaHead',\n        loss_bbox=dict(type='SmoothL1Loss', beta=0.11, loss_weight=1.0)))\n\ntrain_cfg = dict(isr=dict(k=2., bias=0.), carl=dict(k=1., bias=0.2))\n"""
configs/pisa/pisa_ssd300_coco.py,0,"b""_base_ = '../ssd/ssd300_coco.py'\n\nmodel = dict(bbox_head=dict(type='PISASSDHead'))\n\ntrain_cfg = dict(isr=dict(k=2., bias=0.), carl=dict(k=1., bias=0.2))\n\noptimizer_config = dict(\n    _delete_=True, grad_clip=dict(max_norm=35, norm_type=2))\n"""
configs/pisa/pisa_ssd512_coco.py,0,"b""_base_ = '../ssd/ssd512_coco.py'\n\nmodel = dict(bbox_head=dict(type='PISASSDHead'))\n\ntrain_cfg = dict(isr=dict(k=2., bias=0.), carl=dict(k=1., bias=0.2))\n\noptimizer_config = dict(\n    _delete_=True, grad_clip=dict(max_norm=35, norm_type=2))\n"""
configs/regnet/faster_rcnn_regnetx-3GF_fpn_1x_coco.py,0,"b""_base_ = [\n    '../_base_/models/faster_rcnn_r50_fpn.py',\n    '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\nmodel = dict(\n    pretrained='open-mmlab://regnetx_3.2gf',\n    backbone=dict(\n        _delete_=True,\n        type='RegNet',\n        arch='regnetx_3.2gf',\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        norm_eval=True,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[96, 192, 432, 1008],\n        out_channels=256,\n        num_outs=5))\nimg_norm_cfg = dict(\n    # The mean and std is used in PyCls when training RegNets\n    mean=[103.53, 116.28, 123.675],\n    std=[57.375, 57.12, 58.395],\n    to_rgb=False)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile', to_float32=True),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.00005)\n"""
configs/regnet/faster_rcnn_regnetx-3GF_fpn_2x_coco.py,0,"b""_base_ = [\n    '../_base_/models/faster_rcnn_r50_fpn.py',\n    '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_2x.py', '../_base_/default_runtime.py'\n]\nmodel = dict(\n    pretrained='open-mmlab://regnetx_3.2gf',\n    backbone=dict(\n        _delete_=True,\n        type='RegNet',\n        arch='regnetx_3.2gf',\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        norm_eval=True,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[96, 192, 432, 1008],\n        out_channels=256,\n        num_outs=5))\nimg_norm_cfg = dict(\n    # The mean and std is used in PyCls when training RegNets\n    mean=[103.53, 116.28, 123.675],\n    std=[57.375, 57.12, 58.395],\n    to_rgb=False)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile', to_float32=True),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.00005)\n"""
configs/regnet/faster_rcnn_regnetx-3GF_fpn_mstrxin_3x_coco.py,0,"b""_base_ = [\n    '../_base_/models/faster_rcnn_r50_fpn.py',\n    '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\nmodel = dict(\n    pretrained='open-mmlab://regnetx_3.2gf',\n    backbone=dict(\n        _delete_=True,\n        type='RegNet',\n        arch='regnetx_3.2gf',\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        norm_eval=True,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[96, 192, 432, 1008],\n        out_channels=256,\n        num_outs=5))\nimg_norm_cfg = dict(\n    # The mean and std is used in PyCls when training RegNets\n    mean=[103.53, 116.28, 123.675],\n    std=[57.375, 57.12, 58.395],\n    to_rgb=False)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile', to_float32=True),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(\n        type='Resize',\n        img_scale=[(1333, 640), (1333, 672), (1333, 704), (1333, 736),\n                   (1333, 768), (1333, 800)],\n        multiscale_mode='value',\n        keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.00005)\nlr_config = dict(step=[28, 34])\ntotal_epochs = 36\n"""
configs/regnet/mask_rcnn_regnetx-12GF_fpn_1x_coco.py,0,"b""_base_ = './mask_rcnn_regnetx-3GF_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://regnetx_12gf',\n    backbone=dict(\n        type='RegNet',\n        arch='regnetx_12gf',\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        norm_eval=True,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[224, 448, 896, 2240],\n        out_channels=256,\n        num_outs=5))\n"""
configs/regnet/mask_rcnn_regnetx-3GF_fpn_1x_coco.py,0,"b""_base_ = [\n    '../_base_/models/mask_rcnn_r50_fpn.py',\n    '../_base_/datasets/coco_instance.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\nmodel = dict(\n    pretrained='open-mmlab://regnetx_3.2gf',\n    backbone=dict(\n        _delete_=True,\n        type='RegNet',\n        arch='regnetx_3.2gf',\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        norm_eval=True,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[96, 192, 432, 1008],\n        out_channels=256,\n        num_outs=5))\nimg_norm_cfg = dict(\n    # The mean and std is used in PyCls when training RegNets\n    mean=[103.53, 116.28, 123.675],\n    std=[57.375, 57.12, 58.395],\n    to_rgb=False)\ntrain_pipeline = [\n    # RegNet convert images to float32 directly after loading in PyCls\n    dict(type='LoadImageFromFile', to_float32=True),\n    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.00005)\n"""
configs/regnet/mask_rcnn_regnetx-3GF_fpn_mdconv_c3-c5_1x_coco.py,0,"b""_base_ = [\n    '../_base_/models/mask_rcnn_r50_fpn.py',\n    '../_base_/datasets/coco_instance.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\nmodel = dict(\n    pretrained='open-mmlab://regnetx_3.2gf',\n    backbone=dict(\n        _delete_=True,\n        type='RegNet',\n        arch='regnetx_3.2gf',\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        dcn=dict(type='DCNv2', deformable_groups=1, fallback_on_stride=False),\n        stage_with_dcn=(False, True, True, True),\n        norm_eval=True,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[96, 192, 432, 1008],\n        out_channels=256,\n        num_outs=5))\nimg_norm_cfg = dict(\n    # The mean and std is used in PyCls when training RegNets\n    mean=[103.53, 116.28, 123.675],\n    std=[57.375, 57.12, 58.395],\n    to_rgb=False)\ntrain_pipeline = [\n    # RegNet convert images to float32 directly after loading in PyCls\n    dict(type='LoadImageFromFile', to_float32=True),\n    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.00005)\n"""
configs/regnet/mask_rcnn_regnetx-3GF_fpn_mstrain_3x_coco.py,0,"b""_base_ = [\n    '../_base_/models/mask_rcnn_r50_fpn.py',\n    '../_base_/datasets/coco_instance.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\nmodel = dict(\n    pretrained='open-mmlab://regnetx_3.2gf',\n    backbone=dict(\n        _delete_=True,\n        type='RegNet',\n        arch='regnetx_3.2gf',\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        norm_eval=True,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[96, 192, 432, 1008],\n        out_channels=256,\n        num_outs=5))\nimg_norm_cfg = dict(\n    # The mean and std is used in PyCls when training RegNets\n    mean=[103.53, 116.28, 123.675],\n    std=[57.375, 57.12, 58.395],\n    to_rgb=False)\ntrain_pipeline = [\n    # RegNet convert images to float32 directly after loading in PyCls\n    dict(type='LoadImageFromFile', to_float32=True),\n    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n    dict(\n        type='Resize',\n        img_scale=[(1333, 640), (1333, 672), (1333, 704), (1333, 736),\n                   (1333, 768), (1333, 800)],\n        multiscale_mode='value',\n        keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.00005)\nlr_config = dict(step=[28, 34])\ntotal_epochs = 36\noptimizer_config = dict(\n    _delete_=True, grad_clip=dict(max_norm=35, norm_type=2))\n"""
configs/regnet/mask_rcnn_regnetx-4GF_fpn_1x_coco.py,0,"b""_base_ = './mask_rcnn_regnetx-3GF_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://regnetx_4.0gf',\n    backbone=dict(\n        type='RegNet',\n        arch='regnetx_4.0gf',\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        norm_eval=True,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[80, 240, 560, 1360],\n        out_channels=256,\n        num_outs=5))\n"""
configs/regnet/mask_rcnn_regnetx-6GF_fpn_1x_coco.py,0,"b""_base_ = './mask_rcnn_regnetx-3GF_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://regnetx_6.4gf',\n    backbone=dict(\n        type='RegNet',\n        arch='regnetx_6.4gf',\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        norm_eval=True,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[168, 392, 784, 1624],\n        out_channels=256,\n        num_outs=5))\n"""
configs/regnet/mask_rcnn_regnetx-8GF_fpn_1x_coco.py,0,"b""_base_ = './mask_rcnn_regnetx-3GF_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://regnetx_8.0gf',\n    backbone=dict(\n        type='RegNet',\n        arch='regnetx_8.0gf',\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        norm_eval=True,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[80, 240, 720, 1920],\n        out_channels=256,\n        num_outs=5))\n"""
configs/regnet/retinanet_r50_regnetx-1GF_fpn_1x_coco.py,0,"b""_base_ = './retinanet_r50_regnetx-3GF_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://regnetx_1.6gf',\n    backbone=dict(\n        type='RegNet',\n        arch='regnetx_1.6gf',\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        norm_eval=True,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[72, 168, 408, 912],\n        out_channels=256,\n        num_outs=5))\n"""
configs/regnet/retinanet_r50_regnetx-3GF_fpn_1x_coco.py,0,"b""_base_ = [\n    '../_base_/models/retinanet_r50_fpn.py',\n    '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\nmodel = dict(\n    pretrained='open-mmlab://regnetx_3.2gf',\n    backbone=dict(\n        _delete_=True,\n        type='RegNet',\n        arch='regnetx_3.2gf',\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        norm_eval=True,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[96, 192, 432, 1008],\n        out_channels=256,\n        num_outs=5))\nimg_norm_cfg = dict(\n    # The mean and std is used in PyCls when training RegNets\n    mean=[103.53, 116.28, 123.675],\n    std=[57.375, 57.12, 58.395],\n    to_rgb=False)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile', to_float32=True),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.00005)\noptimizer_config = dict(\n    _delete_=True, grad_clip=dict(max_norm=35, norm_type=2))\n"""
configs/regnet/retinanet_r50_regnetx-800MF_fpn_1x_coco.py,0,"b""_base_ = './retinanet_r50_regnetx-3GF_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://regnetx_800mf',\n    backbone=dict(\n        type='RegNet',\n        arch='regnetx_800mf',\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        norm_eval=True,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[64, 128, 288, 672],\n        out_channels=256,\n        num_outs=5))\n"""
configs/reppoints/bbox_r50_grid_center_fpn_gn-neck+head_1x_coco.py,0,"b""_base_ = './reppoints_moment_r50_fpn_gn-neck+head_1x_coco.py'\nmodel = dict(bbox_head=dict(transform_method='minmax', use_grid_points=True))\n"""
configs/reppoints/bbox_r50_grid_fpn_gn-neck+head_1x_coco.py,0,"b""_base_ = './reppoints_moment_r50_fpn_gn-neck+head_1x_coco.py'\nmodel = dict(bbox_head=dict(transform_method='minmax', use_grid_points=True))\n# training and testing settings\ntrain_cfg = dict(\n    init=dict(\n        assigner=dict(\n            _delete_=True,\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.4,\n            min_pos_iou=0,\n            ignore_iof_thr=-1)))\n"""
configs/reppoints/reppoints_minmax_r50_fpn_gn-neck+head_1x_coco.py,0,"b""_base_ = './reppoints_moment_r50_fpn_gn-neck+head_1x_coco.py'\nmodel = dict(bbox_head=dict(transform_method='minmax'))\n"""
configs/reppoints/reppoints_moment_r101_fpn_dconv_c3-c5_gn-neck+head_2x_coco.py,0,"b""_base_ = './reppoints_moment_r50_fpn_gn-neck+head_2x_coco.py'\nmodel = dict(\n    pretrained='torchvision://resnet101',\n    backbone=dict(\n        depth=101,\n        dcn=dict(type='DCN', deformable_groups=1, fallback_on_stride=False),\n        stage_with_dcn=(False, True, True, True)))\n"""
configs/reppoints/reppoints_moment_r101_fpn_gn-neck+head_2x_coco.py,0,"b""_base_ = './reppoints_moment_r50_fpn_gn-neck+head_2x_coco.py'\nmodel = dict(pretrained='torchvision://resnet101', backbone=dict(depth=101))\n"""
configs/reppoints/reppoints_moment_r50_fpn_1x_coco.py,0,"b""_base_ = [\n    '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\nmodel = dict(\n    type='RepPointsDetector',\n    pretrained='torchvision://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        norm_eval=True,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        start_level=1,\n        add_extra_convs='on_input',\n        num_outs=5),\n    bbox_head=dict(\n        type='RepPointsHead',\n        num_classes=80,\n        in_channels=256,\n        feat_channels=256,\n        point_feat_channels=256,\n        stacked_convs=3,\n        num_points=9,\n        gradient_mul=0.1,\n        point_strides=[8, 16, 32, 64, 128],\n        point_base_scale=4,\n        loss_cls=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_bbox_init=dict(type='SmoothL1Loss', beta=0.11, loss_weight=0.5),\n        loss_bbox_refine=dict(type='SmoothL1Loss', beta=0.11, loss_weight=1.0),\n        transform_method='moment'))\n# training and testing settings\ntrain_cfg = dict(\n    init=dict(\n        assigner=dict(type='PointAssigner', scale=4, pos_num=1),\n        allowed_border=-1,\n        pos_weight=-1,\n        debug=False),\n    refine=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.4,\n            min_pos_iou=0,\n            ignore_iof_thr=-1),\n        allowed_border=-1,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    nms_pre=1000,\n    min_bbox_size=0,\n    score_thr=0.05,\n    nms=dict(type='nms', iou_thr=0.5),\n    max_per_img=100)\noptimizer = dict(lr=0.01)\n"""
configs/reppoints/reppoints_moment_r50_fpn_gn-neck+head_1x_coco.py,0,"b""_base_ = './reppoints_moment_r50_fpn_1x_coco.py'\nnorm_cfg = dict(type='GN', num_groups=32, requires_grad=True)\nmodel = dict(neck=dict(norm_cfg=norm_cfg), bbox_head=dict(norm_cfg=norm_cfg))\noptimizer = dict(lr=0.01)\n"""
configs/reppoints/reppoints_moment_r50_fpn_gn-neck+head_2x_coco.py,0,"b""_base_ = './reppoints_moment_r50_fpn_gn-neck+head_1x_coco.py'\nlr_config = dict(step=[16, 22])\ntotal_epochs = 24\n"""
configs/reppoints/reppoints_moment_x101_fpn_dconv_c3-c5_gn-neck+head_2x_coco.py,0,"b""_base_ = './reppoints_moment_r50_fpn_gn-neck+head_2x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_32x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch',\n        dcn=dict(type='DCN', deformable_groups=1, fallback_on_stride=False),\n        stage_with_dcn=(False, True, True, True)))\n"""
configs/reppoints/reppoints_partial_minmax_r50_fpn_gn-neck+head_1x_coco.py,0,"b""_base_ = './reppoints_moment_r50_fpn_gn-neck+head_1x_coco.py'\nmodel = dict(bbox_head=dict(transform_method='partial_minmax'))\n"""
configs/res2net/cascade_mask_rcnn_r2_101_fpn_20e_coco.py,0,"b""_base_ = '../cascade_rcnn/cascade_mask_rcnn_r50_fpn_20e_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://res2net101_v1d_26w_4s',\n    backbone=dict(type='Res2Net', depth=101, scales=4, base_width=26))\n"""
configs/res2net/cascade_rcnn_r2_101_fpn_20e_coco.py,0,"b""_base_ = '../cascade_rcnn/cascade_rcnn_r50_fpn_20e_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://res2net101_v1d_26w_4s',\n    backbone=dict(type='Res2Net', depth=101, scales=4, base_width=26))\n"""
configs/res2net/faster_rcnn_r2_101_fpn_2x_coco.py,0,"b""_base_ = '../faster_rcnn/faster_rcnn_r50_fpn_2x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://res2net101_v1d_26w_4s',\n    backbone=dict(type='Res2Net', depth=101, scales=4, base_width=26))\n"""
configs/res2net/htc_r2_101_fpn_20e_coco.py,0,"b""_base_ = '../htc/htc_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://res2net101_v1d_26w_4s',\n    backbone=dict(type='Res2Net', depth=101, scales=4, base_width=26))\n# learning policy\nlr_config = dict(step=[16, 19])\ntotal_epochs = 20\n"""
configs/res2net/mask_rcnn_r2_101_fpn_2x_coco.py,0,"b""_base_ = '../mask_rcnn/mask_rcnn_r50_fpn_2x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://res2net101_v1d_26w_4s',\n    backbone=dict(type='Res2Net', depth=101, scales=4, base_width=26))\n"""
configs/retinanet/retinanet_r101_caffe_fpn_1x_coco.py,0,"b""_base_ = './retinanet_r50_caffe_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://detectron2/resnet101_caffe',\n    backbone=dict(depth=101))\n"""
configs/retinanet/retinanet_r101_fpn_1x_coco.py,0,"b""_base_ = './retinanet_r50_fpn_1x_coco.py'\nmodel = dict(pretrained='torchvision://resnet101', backbone=dict(depth=101))\n"""
configs/retinanet/retinanet_r101_fpn_2x_coco.py,0,"b""_base_ = './retinanet_r50_fpn_2x_coco.py'\nmodel = dict(pretrained='torchvision://resnet101', backbone=dict(depth=101))\n"""
configs/retinanet/retinanet_r50_caffe_fpn_1x_coco.py,0,"b""_base_ = './retinanet_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://detectron2/resnet50_caffe',\n    backbone=dict(\n        norm_cfg=dict(requires_grad=False), norm_eval=True, style='caffe'))\n# use caffe img_norm\nimg_norm_cfg = dict(\n    mean=[103.530, 116.280, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n"""
configs/retinanet/retinanet_r50_caffe_fpn_mstrain_1x_coco.py,0,"b""_base_ = './retinanet_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://detectron2/resnet50_caffe',\n    backbone=dict(\n        norm_cfg=dict(requires_grad=False), norm_eval=True, style='caffe'))\n# use caffe img_norm\nimg_norm_cfg = dict(\n    mean=[103.530, 116.280, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(\n        type='Resize',\n        img_scale=[(1333, 640), (1333, 672), (1333, 704), (1333, 736),\n                   (1333, 768), (1333, 800)],\n        multiscale_mode='value',\n        keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n"""
configs/retinanet/retinanet_r50_caffe_fpn_mstrain_2x_coco.py,0,"b""_base_ = './retinanet_r50_caffe_fpn_mstrain_1x_coco.py'\n# learning policy\nlr_config = dict(step=[16, 23])\ntotal_epochs = 24\n"""
configs/retinanet/retinanet_r50_caffe_fpn_mstrain_3x_coco.py,0,"b""_base_ = './retinanet_r50_caffe_fpn_mstrain_1x_coco.py'\n# learning policy\nlr_config = dict(step=[28, 34])\ntotal_epochs = 36\n"""
configs/retinanet/retinanet_r50_fpn_1x_coco.py,0,"b""_base_ = [\n    '../_base_/models/retinanet_r50_fpn.py',\n    '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\n"""
configs/retinanet/retinanet_r50_fpn_2x_coco.py,0,"b""_base_ = './retinanet_r50_fpn_1x_coco.py'\n# learning policy\nlr_config = dict(step=[16, 22])\ntotal_epochs = 24\n"""
configs/retinanet/retinanet_x101_32x4d_fpn_1x_coco.py,0,"b""_base_ = './retinanet_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_32x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/retinanet/retinanet_x101_32x4d_fpn_2x_coco.py,0,"b""_base_ = './retinanet_r50_fpn_2x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_32x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/retinanet/retinanet_x101_64x4d_fpn_1x_coco.py,0,"b""_base_ = './retinanet_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_64x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/retinanet/retinanet_x101_64x4d_fpn_2x_coco.py,0,"b""_base_ = './retinanet_r50_fpn_2x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_64x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/rpn/rpn_r101_caffe_fpn_1x_coco.py,0,"b""_base_ = './rpn_r50_caffe_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://detectron2/resnet101_caffe',\n    backbone=dict(depth=101))\n"""
configs/rpn/rpn_r101_fpn_1x_coco.py,0,"b""_base_ = './rpn_r50_fpn_1x_coco.py'\nmodel = dict(pretrained='torchvision://resnet101', backbone=dict(depth=101))\n"""
configs/rpn/rpn_r101_fpn_2x_coco.py,0,"b""_base_ = './rpn_r50_fpn_2x_coco.py'\nmodel = dict(pretrained='torchvision://resnet101', backbone=dict(depth=101))\n"""
configs/rpn/rpn_r50_caffe_c4_1x_coco.py,0,"b""_base_ = [\n    '../_base_/models/rpn_r50_caffe_c4.py',\n    '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\n# dataset settings\nimg_norm_cfg = dict(\n    mean=[103.530, 116.280, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True, with_label=False),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\nevaluation = dict(interval=1, metric='proposal_fast')\n"""
configs/rpn/rpn_r50_caffe_fpn_1x_coco.py,0,"b""_base_ = './rpn_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://detectron2/resnet50_caffe',\n    backbone=dict(\n        norm_cfg=dict(requires_grad=False), norm_eval=True, style='caffe'))\n# use caffe img_norm\nimg_norm_cfg = dict(\n    mean=[103.530, 116.280, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True, with_label=False),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    train=dict(pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n"""
configs/rpn/rpn_r50_fpn_1x_coco.py,0,"b""_base_ = [\n    '../_base_/models/rpn_r50_fpn.py', '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True, with_label=False),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes']),\n]\ndata = dict(train=dict(pipeline=train_pipeline))\nevaluation = dict(interval=1, metric='proposal_fast')\n"""
configs/rpn/rpn_r50_fpn_2x_coco.py,0,"b""_base_ = './rpn_r50_fpn_1x_coco.py'\n\n# learning policy\nlr_config = dict(step=[16, 22])\ntotal_epochs = 24\n"""
configs/rpn/rpn_x101_32x4d_fpn_1x_coco.py,0,"b""_base_ = './rpn_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_32x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/rpn/rpn_x101_32x4d_fpn_2x_coco.py,0,"b""_base_ = './rpn_r50_fpn_2x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_32x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/rpn/rpn_x101_64x4d_fpn_1x_coco.py,0,"b""_base_ = './rpn_r50_fpn_1x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_64x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/rpn/rpn_x101_64x4d_fpn_2x_coco.py,0,"b""_base_ = './rpn_r50_fpn_2x_coco.py'\nmodel = dict(\n    pretrained='open-mmlab://resnext101_64x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        style='pytorch'))\n"""
configs/scratch/faster_rcnn_r50_fpn_gn-all_scratch_6x_coco.py,0,"b""_base_ = [\n    '../_base_/models/faster_rcnn_r50_fpn.py',\n    '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\nnorm_cfg = dict(type='GN', num_groups=32, requires_grad=True)\nmodel = dict(\n    pretrained=None,\n    backbone=dict(\n        frozen_stages=-1, zero_init_residual=False, norm_cfg=norm_cfg),\n    neck=dict(norm_cfg=norm_cfg),\n    roi_head=dict(\n        bbox_head=dict(\n            type='Shared4Conv1FCBBoxHead',\n            conv_out_channels=256,\n            norm_cfg=norm_cfg)))\n# optimizer\noptimizer = dict(paramwise_cfg=dict(norm_decay_mult=0))\noptimizer_config = dict(_delete_=True, grad_clip=None)\n# learning policy\nlr_config = dict(warmup_ratio=0.1, step=[65, 71])\ntotal_epochs = 73\n"""
configs/scratch/mask_rcnn_r50_fpn_gn-all_scratch_6x_coco.py,0,"b""_base_ = [\n    '../_base_/models/mask_rcnn_r50_fpn.py',\n    '../_base_/datasets/coco_instance.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\nnorm_cfg = dict(type='GN', num_groups=32, requires_grad=True)\nmodel = dict(\n    pretrained=None,\n    backbone=dict(\n        frozen_stages=-1, zero_init_residual=False, norm_cfg=norm_cfg),\n    neck=dict(norm_cfg=norm_cfg),\n    roi_head=dict(\n        bbox_head=dict(\n            type='Shared4Conv1FCBBoxHead',\n            conv_out_channels=256,\n            norm_cfg=norm_cfg),\n        mask_head=dict(norm_cfg=norm_cfg)))\n# optimizer\noptimizer = dict(paramwise_cfg=dict(norm_decay_mult=0))\noptimizer_config = dict(_delete_=True, grad_clip=None)\n# learning policy\nlr_config = dict(warmup_ratio=0.1, step=[65, 71])\ntotal_epochs = 73\n"""
configs/ssd/ssd300_coco.py,0,"b""_base_ = [\n    '../_base_/models/ssd300.py', '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_2x.py', '../_base_/default_runtime.py'\n]\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(mean=[123.675, 116.28, 103.53], std=[1, 1, 1], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile', to_float32=True),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(\n        type='PhotoMetricDistortion',\n        brightness_delta=32,\n        contrast_range=(0.5, 1.5),\n        saturation_range=(0.5, 1.5),\n        hue_delta=18),\n    dict(\n        type='Expand',\n        mean=img_norm_cfg['mean'],\n        to_rgb=img_norm_cfg['to_rgb'],\n        ratio_range=(1, 4)),\n    dict(\n        type='MinIoURandomCrop',\n        min_ious=(0.1, 0.3, 0.5, 0.7, 0.9),\n        min_crop_size=0.3),\n    dict(type='Resize', img_scale=(300, 300), keep_ratio=False),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(300, 300),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=False),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    samples_per_gpu=8,\n    workers_per_gpu=3,\n    train=dict(\n        _delete_=True,\n        type='RepeatDataset',\n        times=5,\n        dataset=dict(\n            type=dataset_type,\n            ann_file=data_root + 'annotations/instances_train2017.json',\n            img_prefix=data_root + 'train2017/',\n            pipeline=train_pipeline)),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n# optimizer\noptimizer = dict(type='SGD', lr=2e-3, momentum=0.9, weight_decay=5e-4)\noptimizer_config = dict(_delete_=True)\n"""
configs/ssd/ssd512_coco.py,0,"b""_base_ = 'ssd300_coco.py'\ninput_size = 512\nmodel = dict(\n    backbone=dict(input_size=input_size),\n    bbox_head=dict(\n        in_channels=(512, 1024, 512, 256, 256, 256, 256),\n        anchor_generator=dict(\n            type='SSDAnchorGenerator',\n            scale_major=False,\n            input_size=input_size,\n            basesize_ratio_range=(0.1, 0.9),\n            strides=[8, 16, 32, 64, 128, 256, 512],\n            ratios=[[2], [2, 3], [2, 3], [2, 3], [2, 3], [2], [2]])))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(mean=[123.675, 116.28, 103.53], std=[1, 1, 1], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile', to_float32=True),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(\n        type='PhotoMetricDistortion',\n        brightness_delta=32,\n        contrast_range=(0.5, 1.5),\n        saturation_range=(0.5, 1.5),\n        hue_delta=18),\n    dict(\n        type='Expand',\n        mean=img_norm_cfg['mean'],\n        to_rgb=img_norm_cfg['to_rgb'],\n        ratio_range=(1, 4)),\n    dict(\n        type='MinIoURandomCrop',\n        min_ious=(0.1, 0.3, 0.5, 0.7, 0.9),\n        min_crop_size=0.3),\n    dict(type='Resize', img_scale=(512, 512), keep_ratio=False),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(512, 512),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=False),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    samples_per_gpu=8,\n    workers_per_gpu=3,\n    train=dict(\n        _delete_=True,\n        type='RepeatDataset',\n        times=5,\n        dataset=dict(\n            type=dataset_type,\n            ann_file=data_root + 'annotations/instances_train2017.json',\n            img_prefix=data_root + 'train2017/',\n            pipeline=train_pipeline)),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\n# optimizer\noptimizer = dict(type='SGD', lr=2e-3, momentum=0.9, weight_decay=5e-4)\noptimizer_config = dict(_delete_=True)\n"""
configs/wider_face/ssd300_wider_face.py,0,"b""_base_ = [\n    '../_base_/models/ssd300.py', '../_base_/datasets/wider_face.py',\n    '../_base_/default_runtime.py'\n]\nmodel = dict(bbox_head=dict(num_classes=2))\n# optimizer\noptimizer = dict(type='SGD', lr=0.012, momentum=0.9, weight_decay=5e-4)\noptimizer_config = dict()\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=1000,\n    warmup_ratio=0.001,\n    step=[16, 20])\n# runtime settings\ntotal_epochs = 24\nlog_config = dict(interval=1)\n"""
mmdet/apis/__init__.py,0,"b""from .inference import (async_inference_detector, inference_detector,\n                        init_detector, show_result_pyplot)\nfrom .test import multi_gpu_test, single_gpu_test\nfrom .train import get_root_logger, set_random_seed, train_detector\n\n__all__ = [\n    'get_root_logger', 'set_random_seed', 'train_detector', 'init_detector',\n    'async_inference_detector', 'inference_detector', 'show_result_pyplot',\n    'multi_gpu_test', 'single_gpu_test'\n]\n"""
mmdet/apis/inference.py,3,"b'import warnings\n\nimport matplotlib.pyplot as plt\nimport mmcv\nimport torch\nfrom mmcv.parallel import collate, scatter\nfrom mmcv.runner import load_checkpoint\n\nfrom mmdet.core import get_classes\nfrom mmdet.datasets.pipelines import Compose\nfrom mmdet.models import build_detector\nfrom mmdet.ops import RoIAlign, RoIPool\n\n\ndef init_detector(config, checkpoint=None, device=\'cuda:0\'):\n    """"""Initialize a detector from config file.\n\n    Args:\n        config (str or :obj:`mmcv.Config`): Config file path or the config\n            object.\n        checkpoint (str, optional): Checkpoint path. If left as None, the model\n            will not load any weights.\n\n    Returns:\n        nn.Module: The constructed detector.\n    """"""\n    if isinstance(config, str):\n        config = mmcv.Config.fromfile(config)\n    elif not isinstance(config, mmcv.Config):\n        raise TypeError(\'config must be a filename or Config object, \'\n                        f\'but got {type(config)}\')\n    config.model.pretrained = None\n    model = build_detector(config.model, test_cfg=config.test_cfg)\n    if checkpoint is not None:\n        checkpoint = load_checkpoint(model, checkpoint)\n        if \'CLASSES\' in checkpoint[\'meta\']:\n            model.CLASSES = checkpoint[\'meta\'][\'CLASSES\']\n        else:\n            warnings.simplefilter(\'once\')\n            warnings.warn(\'Class names are not saved in the checkpoint\\\'s \'\n                          \'meta data, use COCO classes by default.\')\n            model.CLASSES = get_classes(\'coco\')\n    model.cfg = config  # save the config in the model for convenience\n    model.to(device)\n    model.eval()\n    return model\n\n\nclass LoadImage(object):\n\n    def __call__(self, results):\n        if isinstance(results[\'img\'], str):\n            results[\'filename\'] = results[\'img\']\n            results[\'ori_filename\'] = results[\'img\']\n        else:\n            results[\'filename\'] = None\n            results[\'ori_filename\'] = None\n        img = mmcv.imread(results[\'img\'])\n        results[\'img\'] = img\n        results[\'img_fields\'] = [\'img\']\n        results[\'img_shape\'] = img.shape\n        results[\'ori_shape\'] = img.shape\n        return results\n\n\ndef inference_detector(model, img):\n    """"""Inference image(s) with the detector.\n\n    Args:\n        model (nn.Module): The loaded detector.\n        imgs (str/ndarray or list[str/ndarray]): Either image files or loaded\n            images.\n\n    Returns:\n        If imgs is a str, a generator will be returned, otherwise return the\n        detection results directly.\n    """"""\n    cfg = model.cfg\n    device = next(model.parameters()).device  # model device\n    # build the data pipeline\n    test_pipeline = [LoadImage()] + cfg.data.test.pipeline[1:]\n    test_pipeline = Compose(test_pipeline)\n    # prepare data\n    data = dict(img=img)\n    data = test_pipeline(data)\n    data = collate([data], samples_per_gpu=1)\n    if next(model.parameters()).is_cuda:\n        # scatter to specified GPU\n        data = scatter(data, [device])[0]\n    else:\n        # Use torchvision ops for CPU mode instead\n        for m in model.modules():\n            if isinstance(m, (RoIPool, RoIAlign)):\n                if not m.aligned:\n                    # aligned=False is not implemented on CPU\n                    # set use_torchvision on-the-fly\n                    m.use_torchvision = True\n        warnings.warn(\'We set use_torchvision=True in CPU mode.\')\n        # just get the actual data from DataContainer\n        data[\'img_metas\'] = data[\'img_metas\'][0].data\n\n    # forward the model\n    with torch.no_grad():\n        result = model(return_loss=False, rescale=True, **data)\n    return result\n\n\nasync def async_inference_detector(model, img):\n    """"""Async inference image(s) with the detector.\n\n    Args:\n        model (nn.Module): The loaded detector.\n        imgs (str/ndarray or list[str/ndarray]): Either image files or loaded\n            images.\n\n    Returns:\n        Awaitable detection results.\n    """"""\n    cfg = model.cfg\n    device = next(model.parameters()).device  # model device\n    # build the data pipeline\n    test_pipeline = [LoadImage()] + cfg.data.test.pipeline[1:]\n    test_pipeline = Compose(test_pipeline)\n    # prepare data\n    data = dict(img=img)\n    data = test_pipeline(data)\n    data = scatter(collate([data], samples_per_gpu=1), [device])[0]\n\n    # We don\'t restore `torch.is_grad_enabled()` value during concurrent\n    # inference since execution can overlap\n    torch.set_grad_enabled(False)\n    result = await model.aforward_test(rescale=True, **data)\n    return result\n\n\ndef show_result_pyplot(model, img, result, score_thr=0.3, fig_size=(15, 10)):\n    """"""Visualize the detection results on the image.\n\n    Args:\n        model (nn.Module): The loaded detector.\n        img (str or np.ndarray): Image filename or loaded image.\n        result (tuple[list] or list): The detection result, can be either\n            (bbox, segm) or just bbox.\n        score_thr (float): The threshold to visualize the bboxes and masks.\n        fig_size (tuple): Figure size of the pyplot figure.\n    """"""\n    if hasattr(model, \'module\'):\n        model = model.module\n    img = model.show_result(img, result, score_thr=score_thr, show=False)\n    plt.figure(figsize=fig_size)\n    plt.imshow(mmcv.bgr2rgb(img))\n    plt.show()\n'"
mmdet/apis/test.py,12,"b'import os.path as osp\nimport pickle\nimport shutil\nimport tempfile\nimport time\n\nimport mmcv\nimport torch\nimport torch.distributed as dist\nfrom mmcv.runner import get_dist_info\n\nfrom mmdet.core import encode_mask_results, tensor2imgs\n\n\ndef single_gpu_test(model,\n                    data_loader,\n                    show=False,\n                    out_dir=None,\n                    show_score_thr=0.3):\n    model.eval()\n    results = []\n    dataset = data_loader.dataset\n    prog_bar = mmcv.ProgressBar(len(dataset))\n    for i, data in enumerate(data_loader):\n        with torch.no_grad():\n            result = model(return_loss=False, rescale=True, **data)\n\n        if show or out_dir:\n            img_tensor = data[\'img\'][0]\n            img_metas = data[\'img_metas\'][0].data[0]\n            imgs = tensor2imgs(img_tensor, **img_metas[0][\'img_norm_cfg\'])\n            assert len(imgs) == len(img_metas)\n\n            for img, img_meta in zip(imgs, img_metas):\n                h, w, _ = img_meta[\'img_shape\']\n                img_show = img[:h, :w, :]\n\n                ori_h, ori_w = img_meta[\'ori_shape\'][:-1]\n                img_show = mmcv.imresize(img_show, (ori_w, ori_h))\n\n                if out_dir:\n                    out_file = osp.join(out_dir, img_meta[\'ori_filename\'])\n                else:\n                    out_file = None\n\n                model.module.show_result(\n                    img_show,\n                    result,\n                    show=show,\n                    out_file=out_file,\n                    score_thr=show_score_thr)\n\n        # encode mask results\n        if isinstance(result, tuple):\n            bbox_results, mask_results = result\n            encoded_mask_results = encode_mask_results(mask_results)\n            result = bbox_results, encoded_mask_results\n        results.append(result)\n\n        batch_size = data[\'img\'][0].size(0)\n        for _ in range(batch_size):\n            prog_bar.update()\n    return results\n\n\ndef multi_gpu_test(model, data_loader, tmpdir=None, gpu_collect=False):\n    """"""Test model with multiple gpus.\n\n    This method tests model with multiple gpus and collects the results\n    under two different modes: gpu and cpu modes. By setting \'gpu_collect=True\'\n    it encodes results to gpu tensors and use gpu communication for results\n    collection. On cpu mode it saves the results on different gpus to \'tmpdir\'\n    and collects them by the rank 0 worker.\n\n    Args:\n        model (nn.Module): Model to be tested.\n        data_loader (nn.Dataloader): Pytorch data loader.\n        tmpdir (str): Path of directory to save the temporary results from\n            different gpus under cpu mode.\n        gpu_collect (bool): Option to use either gpu or cpu to collect results.\n\n    Returns:\n        list: The prediction results.\n    """"""\n    model.eval()\n    results = []\n    dataset = data_loader.dataset\n    rank, world_size = get_dist_info()\n    if rank == 0:\n        prog_bar = mmcv.ProgressBar(len(dataset))\n    time.sleep(2)  # This line can prevent deadlock problem in some cases.\n    for i, data in enumerate(data_loader):\n        with torch.no_grad():\n            result = model(return_loss=False, rescale=True, **data)\n            # encode mask results\n            if isinstance(result, tuple):\n                bbox_results, mask_results = result\n                encoded_mask_results = encode_mask_results(mask_results)\n                result = bbox_results, encoded_mask_results\n        results.append(result)\n\n        if rank == 0:\n            batch_size = (\n                len(data[\'img_meta\']._data)\n                if \'img_meta\' in data else data[\'img\'][0].size(0))\n            for _ in range(batch_size * world_size):\n                prog_bar.update()\n\n    # collect results from all ranks\n    if gpu_collect:\n        results = collect_results_gpu(results, len(dataset))\n    else:\n        results = collect_results_cpu(results, len(dataset), tmpdir)\n    return results\n\n\ndef collect_results_cpu(result_part, size, tmpdir=None):\n    rank, world_size = get_dist_info()\n    # create a tmp dir if it is not specified\n    if tmpdir is None:\n        MAX_LEN = 512\n        # 32 is whitespace\n        dir_tensor = torch.full((MAX_LEN, ),\n                                32,\n                                dtype=torch.uint8,\n                                device=\'cuda\')\n        if rank == 0:\n            tmpdir = tempfile.mkdtemp()\n            tmpdir = torch.tensor(\n                bytearray(tmpdir.encode()), dtype=torch.uint8, device=\'cuda\')\n            dir_tensor[:len(tmpdir)] = tmpdir\n        dist.broadcast(dir_tensor, 0)\n        tmpdir = dir_tensor.cpu().numpy().tobytes().decode().rstrip()\n    else:\n        mmcv.mkdir_or_exist(tmpdir)\n    # dump the part result to the dir\n    mmcv.dump(result_part, osp.join(tmpdir, f\'part_{rank}.pkl\'))\n    dist.barrier()\n    # collect all parts\n    if rank != 0:\n        return None\n    else:\n        # load results of all parts from tmp dir\n        part_list = []\n        for i in range(world_size):\n            part_file = osp.join(tmpdir, f\'part_{i}.pkl\')\n            part_list.append(mmcv.load(part_file))\n        # sort the results\n        ordered_results = []\n        for res in zip(*part_list):\n            ordered_results.extend(list(res))\n        # the dataloader may pad some samples\n        ordered_results = ordered_results[:size]\n        # remove tmp dir\n        shutil.rmtree(tmpdir)\n        return ordered_results\n\n\ndef collect_results_gpu(result_part, size):\n    rank, world_size = get_dist_info()\n    # dump result part to tensor with pickle\n    part_tensor = torch.tensor(\n        bytearray(pickle.dumps(result_part)), dtype=torch.uint8, device=\'cuda\')\n    # gather all result part tensor shape\n    shape_tensor = torch.tensor(part_tensor.shape, device=\'cuda\')\n    shape_list = [shape_tensor.clone() for _ in range(world_size)]\n    dist.all_gather(shape_list, shape_tensor)\n    # padding result part tensor to max length\n    shape_max = torch.tensor(shape_list).max()\n    part_send = torch.zeros(shape_max, dtype=torch.uint8, device=\'cuda\')\n    part_send[:shape_tensor[0]] = part_tensor\n    part_recv_list = [\n        part_tensor.new_zeros(shape_max) for _ in range(world_size)\n    ]\n    # gather all result part\n    dist.all_gather(part_recv_list, part_send)\n\n    if rank == 0:\n        part_list = []\n        for recv, shape in zip(part_recv_list, shape_list):\n            part_list.append(\n                pickle.loads(recv[:shape[0]].cpu().numpy().tobytes()))\n        # sort the results\n        ordered_results = []\n        for res in zip(*part_list):\n            ordered_results.extend(list(res))\n        # the dataloader may pad some samples\n        ordered_results = ordered_results[:size]\n        return ordered_results\n'"
mmdet/apis/train.py,10,"b'import random\nfrom collections import OrderedDict\n\nimport numpy as np\nimport torch\nimport torch.distributed as dist\nfrom mmcv.parallel import MMDataParallel, MMDistributedDataParallel\nfrom mmcv.runner import (DistSamplerSeedHook, OptimizerHook, Runner,\n                         build_optimizer)\n\nfrom mmdet.core import DistEvalHook, EvalHook, Fp16OptimizerHook\nfrom mmdet.datasets import build_dataloader, build_dataset\nfrom mmdet.utils import get_root_logger\n\n\ndef set_random_seed(seed, deterministic=False):\n    """"""Set random seed.\n\n    Args:\n        seed (int): Seed to be used.\n        deterministic (bool): Whether to set the deterministic option for\n            CUDNN backend, i.e., set `torch.backends.cudnn.deterministic`\n            to True and `torch.backends.cudnn.benchmark` to False.\n            Default: False.\n    """"""\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    if deterministic:\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\n\ndef parse_losses(losses):\n    log_vars = OrderedDict()\n    for loss_name, loss_value in losses.items():\n        if isinstance(loss_value, torch.Tensor):\n            log_vars[loss_name] = loss_value.mean()\n        elif isinstance(loss_value, list):\n            log_vars[loss_name] = sum(_loss.mean() for _loss in loss_value)\n        else:\n            raise TypeError(f\'{loss_name} is not a tensor or list of tensors\')\n\n    loss = sum(_value for _key, _value in log_vars.items() if \'loss\' in _key)\n\n    log_vars[\'loss\'] = loss\n    for loss_name, loss_value in log_vars.items():\n        # reduce loss when distributed training\n        if dist.is_available() and dist.is_initialized():\n            loss_value = loss_value.data.clone()\n            dist.all_reduce(loss_value.div_(dist.get_world_size()))\n        log_vars[loss_name] = loss_value.item()\n\n    return loss, log_vars\n\n\ndef batch_processor(model, data, train_mode):\n    """"""Process a data batch.\n\n    This method is required as an argument of Runner, which defines how to\n    process a data batch and obtain proper outputs. The first 3 arguments of\n    batch_processor are fixed.\n\n    Args:\n        model (nn.Module): A PyTorch model.\n        data (dict): The data batch in a dict.\n        train_mode (bool): Training mode or not. It may be useless for some\n            models.\n\n    Returns:\n        dict: A dict containing losses and log vars.\n    """"""\n    losses = model(**data)\n    loss, log_vars = parse_losses(losses)\n\n    outputs = dict(\n        loss=loss, log_vars=log_vars, num_samples=len(data[\'img\'].data))\n\n    return outputs\n\n\ndef train_detector(model,\n                   dataset,\n                   cfg,\n                   distributed=False,\n                   validate=False,\n                   timestamp=None,\n                   meta=None):\n    logger = get_root_logger(cfg.log_level)\n\n    # prepare data loaders\n    dataset = dataset if isinstance(dataset, (list, tuple)) else [dataset]\n    if \'imgs_per_gpu\' in cfg.data:\n        logger.warning(\'""imgs_per_gpu"" is deprecated in MMDet V2.0. \'\n                       \'Please use ""samples_per_gpu"" instead\')\n        if \'samples_per_gpu\' in cfg.data:\n            logger.warning(\n                f\'Got ""imgs_per_gpu""={cfg.data.imgs_per_gpu} and \'\n                f\'""samples_per_gpu""={cfg.data.samples_per_gpu}, ""imgs_per_gpu""\'\n                f\'={cfg.data.imgs_per_gpu} is used in this experiments\')\n        else:\n            logger.warning(\n                \'Automatically set ""samples_per_gpu""=""imgs_per_gpu""=\'\n                f\'{cfg.data.imgs_per_gpu} in this experiments\')\n        cfg.data.samples_per_gpu = cfg.data.imgs_per_gpu\n\n    data_loaders = [\n        build_dataloader(\n            ds,\n            cfg.data.samples_per_gpu,\n            cfg.data.workers_per_gpu,\n            # cfg.gpus will be ignored if distributed\n            len(cfg.gpu_ids),\n            dist=distributed,\n            seed=cfg.seed) for ds in dataset\n    ]\n\n    # put model on gpus\n    if distributed:\n        find_unused_parameters = cfg.get(\'find_unused_parameters\', False)\n        # Sets the `find_unused_parameters` parameter in\n        # torch.nn.parallel.DistributedDataParallel\n        model = MMDistributedDataParallel(\n            model.cuda(),\n            device_ids=[torch.cuda.current_device()],\n            broadcast_buffers=False,\n            find_unused_parameters=find_unused_parameters)\n    else:\n        model = MMDataParallel(\n            model.cuda(cfg.gpu_ids[0]), device_ids=cfg.gpu_ids)\n\n    # build runner\n    optimizer = build_optimizer(model, cfg.optimizer)\n    runner = Runner(\n        model,\n        batch_processor,\n        optimizer,\n        cfg.work_dir,\n        logger=logger,\n        meta=meta)\n    # an ugly workaround to make .log and .log.json filenames the same\n    runner.timestamp = timestamp\n\n    # fp16 setting\n    fp16_cfg = cfg.get(\'fp16\', None)\n    if fp16_cfg is not None:\n        optimizer_config = Fp16OptimizerHook(\n            **cfg.optimizer_config, **fp16_cfg, distributed=distributed)\n    elif distributed and \'type\' not in cfg.optimizer_config:\n        optimizer_config = OptimizerHook(**cfg.optimizer_config)\n    else:\n        optimizer_config = cfg.optimizer_config\n\n    # register hooks\n    runner.register_training_hooks(cfg.lr_config, optimizer_config,\n                                   cfg.checkpoint_config, cfg.log_config,\n                                   cfg.get(\'momentum_config\', None))\n    if distributed:\n        runner.register_hook(DistSamplerSeedHook())\n\n    # register eval hooks\n    if validate:\n        val_dataset = build_dataset(cfg.data.val, dict(test_mode=True))\n        val_dataloader = build_dataloader(\n            val_dataset,\n            samples_per_gpu=1,\n            workers_per_gpu=cfg.data.workers_per_gpu,\n            dist=distributed,\n            shuffle=False)\n        eval_cfg = cfg.get(\'evaluation\', {})\n        eval_hook = DistEvalHook if distributed else EvalHook\n        runner.register_hook(eval_hook(val_dataloader, **eval_cfg))\n\n    if cfg.resume_from:\n        runner.resume(cfg.resume_from)\n    elif cfg.load_from:\n        runner.load_checkpoint(cfg.load_from)\n    runner.run(data_loaders, cfg.workflow, cfg.total_epochs)\n'"
mmdet/core/__init__.py,0,"b'from .anchor import *  # noqa: F401, F403\nfrom .bbox import *  # noqa: F401, F403\nfrom .evaluation import *  # noqa: F401, F403\nfrom .fp16 import *  # noqa: F401, F403\nfrom .mask import *  # noqa: F401, F403\nfrom .post_processing import *  # noqa: F401, F403\nfrom .utils import *  # noqa: F401, F403\n'"
mmdet/datasets/__init__.py,0,"b""from .builder import DATASETS, PIPELINES, build_dataloader, build_dataset\nfrom .cityscapes import CityscapesDataset\nfrom .coco import CocoDataset\nfrom .custom import CustomDataset\nfrom .dataset_wrappers import (ClassBalancedDataset, ConcatDataset,\n                               RepeatDataset)\nfrom .lvis import LVISDataset\nfrom .samplers import DistributedGroupSampler, DistributedSampler, GroupSampler\nfrom .voc import VOCDataset\nfrom .wider_face import WIDERFaceDataset\nfrom .xml_style import XMLDataset\n\n__all__ = [\n    'CustomDataset', 'XMLDataset', 'CocoDataset', 'VOCDataset',\n    'CityscapesDataset', 'LVISDataset', 'GroupSampler',\n    'DistributedGroupSampler', 'DistributedSampler', 'build_dataloader',\n    'ConcatDataset', 'RepeatDataset', 'ClassBalancedDataset',\n    'WIDERFaceDataset', 'DATASETS', 'PIPELINES', 'build_dataset'\n]\n"""
mmdet/datasets/builder.py,1,"b'import copy\nimport platform\nimport random\nfrom functools import partial\n\nimport numpy as np\nfrom mmcv.parallel import collate\nfrom mmcv.runner import get_dist_info\nfrom mmcv.utils import Registry, build_from_cfg\nfrom torch.utils.data import DataLoader\n\nfrom .samplers import DistributedGroupSampler, DistributedSampler, GroupSampler\n\nif platform.system() != \'Windows\':\n    # https://github.com/pytorch/pytorch/issues/973\n    import resource\n    rlimit = resource.getrlimit(resource.RLIMIT_NOFILE)\n    hard_limit = rlimit[1]\n    soft_limit = min(4096, hard_limit)\n    resource.setrlimit(resource.RLIMIT_NOFILE, (soft_limit, hard_limit))\n\nDATASETS = Registry(\'dataset\')\nPIPELINES = Registry(\'pipeline\')\n\n\ndef _concat_dataset(cfg, default_args=None):\n    from .dataset_wrappers import ConcatDataset\n    ann_files = cfg[\'ann_file\']\n    img_prefixes = cfg.get(\'img_prefix\', None)\n    seg_prefixes = cfg.get(\'seg_prefix\', None)\n    proposal_files = cfg.get(\'proposal_file\', None)\n\n    datasets = []\n    num_dset = len(ann_files)\n    for i in range(num_dset):\n        data_cfg = copy.deepcopy(cfg)\n        data_cfg[\'ann_file\'] = ann_files[i]\n        if isinstance(img_prefixes, (list, tuple)):\n            data_cfg[\'img_prefix\'] = img_prefixes[i]\n        if isinstance(seg_prefixes, (list, tuple)):\n            data_cfg[\'seg_prefix\'] = seg_prefixes[i]\n        if isinstance(proposal_files, (list, tuple)):\n            data_cfg[\'proposal_file\'] = proposal_files[i]\n        datasets.append(build_dataset(data_cfg, default_args))\n\n    return ConcatDataset(datasets)\n\n\ndef build_dataset(cfg, default_args=None):\n    from .dataset_wrappers import (ConcatDataset, RepeatDataset,\n                                   ClassBalancedDataset)\n    if isinstance(cfg, (list, tuple)):\n        dataset = ConcatDataset([build_dataset(c, default_args) for c in cfg])\n    elif cfg[\'type\'] == \'RepeatDataset\':\n        dataset = RepeatDataset(\n            build_dataset(cfg[\'dataset\'], default_args), cfg[\'times\'])\n    elif cfg[\'type\'] == \'ClassBalancedDataset\':\n        dataset = ClassBalancedDataset(\n            build_dataset(cfg[\'dataset\'], default_args), cfg[\'oversample_thr\'])\n    elif isinstance(cfg.get(\'ann_file\'), (list, tuple)):\n        dataset = _concat_dataset(cfg, default_args)\n    else:\n        dataset = build_from_cfg(cfg, DATASETS, default_args)\n\n    return dataset\n\n\ndef build_dataloader(dataset,\n                     samples_per_gpu,\n                     workers_per_gpu,\n                     num_gpus=1,\n                     dist=True,\n                     shuffle=True,\n                     seed=None,\n                     **kwargs):\n    """"""Build PyTorch DataLoader.\n\n    In distributed training, each GPU/process has a dataloader.\n    In non-distributed training, there is only one dataloader for all GPUs.\n\n    Args:\n        dataset (Dataset): A PyTorch dataset.\n        samples_per_gpu (int): Number of training samples on each GPU, i.e.,\n            batch size of each GPU.\n        workers_per_gpu (int): How many subprocesses to use for data loading\n            for each GPU.\n        num_gpus (int): Number of GPUs. Only used in non-distributed training.\n        dist (bool): Distributed training/test or not. Default: True.\n        shuffle (bool): Whether to shuffle the data at every epoch.\n            Default: True.\n        kwargs: any keyword argument to be used to initialize DataLoader\n\n    Returns:\n        DataLoader: A PyTorch dataloader.\n    """"""\n    rank, world_size = get_dist_info()\n    if dist:\n        # DistributedGroupSampler will definitely shuffle the data to satisfy\n        # that images on each GPU are in the same group\n        if shuffle:\n            sampler = DistributedGroupSampler(dataset, samples_per_gpu,\n                                              world_size, rank)\n        else:\n            sampler = DistributedSampler(\n                dataset, world_size, rank, shuffle=False)\n        batch_size = samples_per_gpu\n        num_workers = workers_per_gpu\n    else:\n        sampler = GroupSampler(dataset, samples_per_gpu) if shuffle else None\n        batch_size = num_gpus * samples_per_gpu\n        num_workers = num_gpus * workers_per_gpu\n\n    init_fn = partial(\n        worker_init_fn, num_workers=num_workers, rank=rank,\n        seed=seed) if seed is not None else None\n\n    data_loader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        sampler=sampler,\n        num_workers=num_workers,\n        collate_fn=partial(collate, samples_per_gpu=samples_per_gpu),\n        pin_memory=False,\n        worker_init_fn=init_fn,\n        **kwargs)\n\n    return data_loader\n\n\ndef worker_init_fn(worker_id, num_workers, rank, seed):\n    # The seed of each worker equals to\n    # num_worker * rank + worker_id + user_seed\n    worker_seed = num_workers * rank + worker_id + seed\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n'"
mmdet/datasets/cityscapes.py,0,"b'# Modified from https://github.com/facebookresearch/detectron2/blob/master/detectron2/data/datasets/cityscapes.py # noqa\n# and https://github.com/mcordts/cityscapesScripts/blob/master/cityscapesscripts/evaluation/evalInstanceLevelSemanticLabeling.py # noqa\n\nimport glob\nimport os\nimport os.path as osp\nimport tempfile\n\nimport mmcv\nimport numpy as np\nimport pycocotools.mask as maskUtils\nfrom mmcv.utils import print_log\n\nfrom .builder import DATASETS\nfrom .coco import CocoDataset\n\n\n@DATASETS.register_module()\nclass CityscapesDataset(CocoDataset):\n\n    CLASSES = (\'person\', \'rider\', \'car\', \'truck\', \'bus\', \'train\', \'motorcycle\',\n               \'bicycle\')\n\n    def _filter_imgs(self, min_size=32):\n        """"""Filter images too small or without ground truths.""""""\n        valid_inds = []\n        ids_with_ann = set(_[\'image_id\'] for _ in self.coco.anns.values())\n        for i, img_info in enumerate(self.data_infos):\n            img_id = img_info[\'id\']\n            ann_ids = self.coco.getAnnIds(imgIds=[img_id])\n            ann_info = self.coco.loadAnns(ann_ids)\n            all_iscrowd = all([_[\'iscrowd\'] for _ in ann_info])\n            if self.filter_empty_gt and (self.img_ids[i] not in ids_with_ann\n                                         or all_iscrowd):\n                continue\n            if min(img_info[\'width\'], img_info[\'height\']) >= min_size:\n                valid_inds.append(i)\n        return valid_inds\n\n    def _parse_ann_info(self, img_info, ann_info):\n        """"""Parse bbox and mask annotation.\n\n        Args:\n            img_info (dict): Image info of an image.\n            ann_info (list[dict]): Annotation info of an image.\n\n        Returns:\n            dict: A dict containing the following keys: bboxes, bboxes_ignore,\n                labels, masks, seg_map.\n                ""masks"" are already decoded into binary masks.\n        """"""\n        gt_bboxes = []\n        gt_labels = []\n        gt_bboxes_ignore = []\n        gt_masks_ann = []\n\n        for i, ann in enumerate(ann_info):\n            if ann.get(\'ignore\', False):\n                continue\n            x1, y1, w, h = ann[\'bbox\']\n            if ann[\'area\'] <= 0 or w < 1 or h < 1:\n                continue\n            if ann[\'category_id\'] not in self.cat_ids:\n                continue\n            bbox = [x1, y1, x1 + w, y1 + h]\n            if ann.get(\'iscrowd\', False):\n                gt_bboxes_ignore.append(bbox)\n            else:\n                gt_bboxes.append(bbox)\n                gt_labels.append(self.cat2label[ann[\'category_id\']])\n                gt_masks_ann.append(ann[\'segmentation\'])\n\n        if gt_bboxes:\n            gt_bboxes = np.array(gt_bboxes, dtype=np.float32)\n            gt_labels = np.array(gt_labels, dtype=np.int64)\n        else:\n            gt_bboxes = np.zeros((0, 4), dtype=np.float32)\n            gt_labels = np.array([], dtype=np.int64)\n\n        if gt_bboxes_ignore:\n            gt_bboxes_ignore = np.array(gt_bboxes_ignore, dtype=np.float32)\n        else:\n            gt_bboxes_ignore = np.zeros((0, 4), dtype=np.float32)\n\n        ann = dict(\n            bboxes=gt_bboxes,\n            labels=gt_labels,\n            bboxes_ignore=gt_bboxes_ignore,\n            masks=gt_masks_ann,\n            seg_map=img_info[\'segm_file\'])\n\n        return ann\n\n    def results2txt(self, results, outfile_prefix):\n        """"""Dump the detection results to a txt file.\n\n        Args:\n            results (list[list | tuple | ndarray]): Testing results of the\n                dataset.\n            outfile_prefix (str): The filename prefix of the json files.\n                If the prefix is ""somepath/xxx"",\n                the txt files will be named ""somepath/xxx.txt"".\n\n        Returns:\n            list[str: str]: result txt files which contains corresponding\n            instance segmentation images.\n        """"""\n        try:\n            import cityscapesscripts.helpers.labels as CSLabels\n        except ImportError:\n            raise ImportError(\'Please run ""pip install citscapesscripts"" to \'\n                              \'install cityscapesscripts first.\')\n        result_files = []\n        os.makedirs(outfile_prefix, exist_ok=True)\n        prog_bar = mmcv.ProgressBar(len(self))\n        for idx in range(len(self)):\n            result = results[idx]\n            filename = self.data_infos[idx][\'filename\']\n            basename = osp.splitext(osp.basename(filename))[0]\n            pred_txt = osp.join(outfile_prefix, basename + \'_pred.txt\')\n\n            bbox_result, segm_result = result\n            bboxes = np.vstack(bbox_result)\n            segms = mmcv.concat_list(segm_result)\n            labels = [\n                np.full(bbox.shape[0], i, dtype=np.int32)\n                for i, bbox in enumerate(bbox_result)\n            ]\n            labels = np.concatenate(labels)\n\n            assert len(bboxes) == len(segms) == len(labels)\n            num_instances = len(bboxes)\n            prog_bar.update()\n            with open(pred_txt, \'w\') as fout:\n                for i in range(num_instances):\n                    pred_class = labels[i]\n                    classes = self.CLASSES[pred_class]\n                    class_id = CSLabels.name2label[classes].id\n                    score = bboxes[i, -1]\n                    mask = maskUtils.decode(segms[i]).astype(np.uint8)\n                    png_filename = osp.join(outfile_prefix,\n                                            basename + f\'_{i}_{classes}.png\')\n                    mmcv.imwrite(mask, png_filename)\n                    fout.write(f\'{osp.basename(png_filename)} {class_id} \'\n                               f\'{score}\\n\')\n            result_files.append(pred_txt)\n\n        return result_files\n\n    def format_results(self, results, txtfile_prefix=None):\n        """"""Format the results to txt (standard format for Cityscapes evaluation).\n\n        Args:\n            results (list): Testing results of the dataset.\n            txtfile_prefix (str | None): The prefix of txt files. It includes\n                the file path and the prefix of filename, e.g., ""a/b/prefix"".\n                If not specified, a temp file will be created. Default: None.\n\n        Returns:\n            tuple: (result_files, tmp_dir), result_files is a dict containing\n                the json filepaths, tmp_dir is the temporal directory created\n                for saving txt/png files when txtfile_prefix is not specified.\n        """"""\n        assert isinstance(results, list), \'results must be a list\'\n        assert len(results) == len(self), (\n            \'The length of results is not equal to the dataset len: {} != {}\'.\n            format(len(results), len(self)))\n\n        assert isinstance(results, list), \'results must be a list\'\n        assert len(results) == len(self), (\n            \'The length of results is not equal to the dataset len: {} != {}\'.\n            format(len(results), len(self)))\n\n        if txtfile_prefix is None:\n            tmp_dir = tempfile.TemporaryDirectory()\n            txtfile_prefix = osp.join(tmp_dir.name, \'results\')\n        else:\n            tmp_dir = None\n        result_files = self.results2txt(results, txtfile_prefix)\n\n        return result_files, tmp_dir\n\n    def evaluate(self,\n                 results,\n                 metric=\'bbox\',\n                 logger=None,\n                 outfile_prefix=None,\n                 classwise=False,\n                 proposal_nums=(100, 300, 1000),\n                 iou_thrs=np.arange(0.5, 0.96, 0.05)):\n        """"""Evaluation in Cityscapes protocol.\n\n        Args:\n            results (list): Testing results of the dataset.\n            metric (str | list[str]): Metrics to be evaluated.\n            logger (logging.Logger | str | None): Logger used for printing\n                related information during evaluation. Default: None.\n            outfile_prefix (str | None):\n            classwise (bool): Whether to evaluating the AP for each class.\n            proposal_nums (Sequence[int]): Proposal number used for evaluating\n                recalls, such as recall@100, recall@1000.\n                Default: (100, 300, 1000).\n            iou_thrs (Sequence[float]): IoU threshold used for evaluating\n                recalls. If set to a list, the average recall of all IoUs will\n                also be computed. Default: 0.5.\n\n        Returns:\n            dict[str: float]\n        """"""\n        eval_results = dict()\n\n        metrics = metric.copy() if isinstance(metric, list) else [metric]\n\n        if \'cityscapes\' in metrics:\n            eval_results.update(\n                self._evaluate_cityscapes(results, outfile_prefix, logger))\n            metrics.remove(\'cityscapes\')\n\n        # left metrics are all coco metric\n        if len(metrics) > 0:\n            # create CocoDataset with CityscapesDataset annotation\n            self_coco = CocoDataset(self.ann_file, self.pipeline.transforms,\n                                    None, self.data_root, self.img_prefix,\n                                    self.seg_prefix, self.proposal_file,\n                                    self.test_mode, self.filter_empty_gt)\n            # TODO: remove this in the future\n            # reload annotations of correct class\n            self_coco.CLASSES = self.CLASSES\n            self_coco.data_infos = self_coco.load_annotations(self.ann_file)\n            eval_results.update(\n                self_coco.evaluate(results, metrics, logger, outfile_prefix,\n                                   classwise, proposal_nums, iou_thrs))\n\n        return eval_results\n\n    def _evaluate_cityscapes(self, results, txtfile_prefix, logger):\n        try:\n            import cityscapesscripts.evaluation.evalInstanceLevelSemanticLabeling as CSEval  # noqa\n        except ImportError:\n            raise ImportError(\'Please run ""pip install citscapesscripts"" to \'\n                              \'install cityscapesscripts first.\')\n        msg = \'Evaluating in Cityscapes style\'\n        if logger is None:\n            msg = \'\\n\' + msg\n        print_log(msg, logger=logger)\n\n        result_files, tmp_dir = self.format_results(results, txtfile_prefix)\n\n        if tmp_dir is None:\n            result_dir = osp.join(txtfile_prefix, \'results\')\n        else:\n            result_dir = osp.join(tmp_dir.name, \'results\')\n\n        eval_results = {}\n        print_log(f\'Evaluating results under {result_dir} ...\', logger=logger)\n\n        # set global states in cityscapes evaluation API\n        CSEval.args.cityscapesPath = os.path.join(self.img_prefix, \'../..\')\n        CSEval.args.predictionPath = os.path.abspath(result_dir)\n        CSEval.args.predictionWalk = None\n        CSEval.args.JSONOutput = False\n        CSEval.args.colorized = False\n        CSEval.args.gtInstancesFile = os.path.join(result_dir,\n                                                   \'gtInstances.json\')\n        CSEval.args.groundTruthSearch = os.path.join(\n            self.img_prefix.replace(\'leftImg8bit\', \'gtFine\'),\n            \'*/*_gtFine_instanceIds.png\')\n\n        groundTruthImgList = glob.glob(CSEval.args.groundTruthSearch)\n        assert len(groundTruthImgList), \'Cannot find ground truth images\' \\\n            f\' in {CSEval.args.groundTruthSearch}.\'\n        predictionImgList = []\n        for gt in groundTruthImgList:\n            predictionImgList.append(CSEval.getPrediction(gt, CSEval.args))\n        CSEval_results = CSEval.evaluateImgLists(predictionImgList,\n                                                 groundTruthImgList,\n                                                 CSEval.args)[\'averages\']\n\n        eval_results[\'mAP\'] = CSEval_results[\'allAp\']\n        eval_results[\'AP@50\'] = CSEval_results[\'allAp50%\']\n        if tmp_dir is not None:\n            tmp_dir.cleanup()\n        return eval_results\n'"
mmdet/datasets/coco.py,0,"b'import itertools\nimport logging\nimport os.path as osp\nimport tempfile\n\nimport mmcv\nimport numpy as np\nfrom mmcv.utils import print_log\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\nfrom terminaltables import AsciiTable\n\nfrom mmdet.core import eval_recalls\nfrom .builder import DATASETS\nfrom .custom import CustomDataset\n\n\n@DATASETS.register_module()\nclass CocoDataset(CustomDataset):\n\n    CLASSES = (\'person\', \'bicycle\', \'car\', \'motorcycle\', \'airplane\', \'bus\',\n               \'train\', \'truck\', \'boat\', \'traffic light\', \'fire hydrant\',\n               \'stop sign\', \'parking meter\', \'bench\', \'bird\', \'cat\', \'dog\',\n               \'horse\', \'sheep\', \'cow\', \'elephant\', \'bear\', \'zebra\', \'giraffe\',\n               \'backpack\', \'umbrella\', \'handbag\', \'tie\', \'suitcase\', \'frisbee\',\n               \'skis\', \'snowboard\', \'sports ball\', \'kite\', \'baseball bat\',\n               \'baseball glove\', \'skateboard\', \'surfboard\', \'tennis racket\',\n               \'bottle\', \'wine glass\', \'cup\', \'fork\', \'knife\', \'spoon\', \'bowl\',\n               \'banana\', \'apple\', \'sandwich\', \'orange\', \'broccoli\', \'carrot\',\n               \'hot dog\', \'pizza\', \'donut\', \'cake\', \'chair\', \'couch\',\n               \'potted plant\', \'bed\', \'dining table\', \'toilet\', \'tv\', \'laptop\',\n               \'mouse\', \'remote\', \'keyboard\', \'cell phone\', \'microwave\',\n               \'oven\', \'toaster\', \'sink\', \'refrigerator\', \'book\', \'clock\',\n               \'vase\', \'scissors\', \'teddy bear\', \'hair drier\', \'toothbrush\')\n\n    def load_annotations(self, ann_file):\n        self.coco = COCO(ann_file)\n        self.cat_ids = self.coco.get_cat_ids(cat_names=self.CLASSES)\n        self.cat2label = {cat_id: i for i, cat_id in enumerate(self.cat_ids)}\n        self.img_ids = self.coco.get_img_ids()\n        data_infos = []\n        for i in self.img_ids:\n            info = self.coco.load_imgs([i])[0]\n            info[\'filename\'] = info[\'file_name\']\n            data_infos.append(info)\n        return data_infos\n\n    def get_ann_info(self, idx):\n        img_id = self.data_infos[idx][\'id\']\n        ann_ids = self.coco.get_ann_ids(img_ids=[img_id])\n        ann_info = self.coco.load_anns(ann_ids)\n        return self._parse_ann_info(self.data_infos[idx], ann_info)\n\n    def get_cat_ids(self, idx):\n        img_id = self.data_infos[idx][\'id\']\n        ann_ids = self.coco.get_ann_ids(img_ids=[img_id])\n        ann_info = self.coco.load_anns(ann_ids)\n        return [ann[\'category_id\'] for ann in ann_info]\n\n    def _filter_imgs(self, min_size=32):\n        """"""Filter images too small or without ground truths.""""""\n        valid_inds = []\n        ids_with_ann = set(_[\'image_id\'] for _ in self.coco.anns.values())\n        for i, img_info in enumerate(self.data_infos):\n            if self.filter_empty_gt and self.img_ids[i] not in ids_with_ann:\n                continue\n            if min(img_info[\'width\'], img_info[\'height\']) >= min_size:\n                valid_inds.append(i)\n        return valid_inds\n\n    def get_subset_by_classes(self):\n        """"""Get img ids that contain any category in class_ids.\n\n        Different from the coco.getImgIds(), this function returns the id if\n        the img contains one of the categories rather than all.\n\n        Args:\n            class_ids (list[int]): list of category ids\n\n        Return:\n            ids (list[int]): integer list of img ids\n        """"""\n\n        ids = set()\n        for i, class_id in enumerate(self.cat_ids):\n            ids |= set(self.coco.cat_img_map[class_id])\n        self.img_ids = list(ids)\n\n        data_infos = []\n        for i in self.img_ids:\n            info = self.coco.load_imgs([i])[0]\n            info[\'filename\'] = info[\'file_name\']\n            data_infos.append(info)\n        return data_infos\n\n    def _parse_ann_info(self, img_info, ann_info):\n        """"""Parse bbox and mask annotation.\n\n        Args:\n            ann_info (list[dict]): Annotation info of an image.\n            with_mask (bool): Whether to parse mask annotations.\n\n        Returns:\n            dict: A dict containing the following keys: bboxes, bboxes_ignore,\n                labels, masks, seg_map. ""masks"" are raw annotations and not\n                decoded into binary masks.\n        """"""\n        gt_bboxes = []\n        gt_labels = []\n        gt_bboxes_ignore = []\n        gt_masks_ann = []\n\n        for i, ann in enumerate(ann_info):\n            if ann.get(\'ignore\', False):\n                continue\n            x1, y1, w, h = ann[\'bbox\']\n            if ann[\'area\'] <= 0 or w < 1 or h < 1:\n                continue\n            if ann[\'category_id\'] not in self.cat_ids:\n                continue\n            bbox = [x1, y1, x1 + w, y1 + h]\n            if ann.get(\'iscrowd\', False):\n                gt_bboxes_ignore.append(bbox)\n            else:\n                gt_bboxes.append(bbox)\n                gt_labels.append(self.cat2label[ann[\'category_id\']])\n                gt_masks_ann.append(ann[\'segmentation\'])\n\n        if gt_bboxes:\n            gt_bboxes = np.array(gt_bboxes, dtype=np.float32)\n            gt_labels = np.array(gt_labels, dtype=np.int64)\n        else:\n            gt_bboxes = np.zeros((0, 4), dtype=np.float32)\n            gt_labels = np.array([], dtype=np.int64)\n\n        if gt_bboxes_ignore:\n            gt_bboxes_ignore = np.array(gt_bboxes_ignore, dtype=np.float32)\n        else:\n            gt_bboxes_ignore = np.zeros((0, 4), dtype=np.float32)\n\n        seg_map = img_info[\'filename\'].replace(\'jpg\', \'png\')\n\n        ann = dict(\n            bboxes=gt_bboxes,\n            labels=gt_labels,\n            bboxes_ignore=gt_bboxes_ignore,\n            masks=gt_masks_ann,\n            seg_map=seg_map)\n\n        return ann\n\n    def xyxy2xywh(self, bbox):\n        _bbox = bbox.tolist()\n        return [\n            _bbox[0],\n            _bbox[1],\n            _bbox[2] - _bbox[0],\n            _bbox[3] - _bbox[1],\n        ]\n\n    def _proposal2json(self, results):\n        json_results = []\n        for idx in range(len(self)):\n            img_id = self.img_ids[idx]\n            bboxes = results[idx]\n            for i in range(bboxes.shape[0]):\n                data = dict()\n                data[\'image_id\'] = img_id\n                data[\'bbox\'] = self.xyxy2xywh(bboxes[i])\n                data[\'score\'] = float(bboxes[i][4])\n                data[\'category_id\'] = 1\n                json_results.append(data)\n        return json_results\n\n    def _det2json(self, results):\n        json_results = []\n        for idx in range(len(self)):\n            img_id = self.img_ids[idx]\n            result = results[idx]\n            for label in range(len(result)):\n                bboxes = result[label]\n                for i in range(bboxes.shape[0]):\n                    data = dict()\n                    data[\'image_id\'] = img_id\n                    data[\'bbox\'] = self.xyxy2xywh(bboxes[i])\n                    data[\'score\'] = float(bboxes[i][4])\n                    data[\'category_id\'] = self.cat_ids[label]\n                    json_results.append(data)\n        return json_results\n\n    def _segm2json(self, results):\n        bbox_json_results = []\n        segm_json_results = []\n        for idx in range(len(self)):\n            img_id = self.img_ids[idx]\n            det, seg = results[idx]\n            for label in range(len(det)):\n                # bbox results\n                bboxes = det[label]\n                for i in range(bboxes.shape[0]):\n                    data = dict()\n                    data[\'image_id\'] = img_id\n                    data[\'bbox\'] = self.xyxy2xywh(bboxes[i])\n                    data[\'score\'] = float(bboxes[i][4])\n                    data[\'category_id\'] = self.cat_ids[label]\n                    bbox_json_results.append(data)\n\n                # segm results\n                # some detectors use different scores for bbox and mask\n                if isinstance(seg, tuple):\n                    segms = seg[0][label]\n                    mask_score = seg[1][label]\n                else:\n                    segms = seg[label]\n                    mask_score = [bbox[4] for bbox in bboxes]\n                for i in range(bboxes.shape[0]):\n                    data = dict()\n                    data[\'image_id\'] = img_id\n                    data[\'bbox\'] = self.xyxy2xywh(bboxes[i])\n                    data[\'score\'] = float(mask_score[i])\n                    data[\'category_id\'] = self.cat_ids[label]\n                    if isinstance(segms[i][\'counts\'], bytes):\n                        segms[i][\'counts\'] = segms[i][\'counts\'].decode()\n                    data[\'segmentation\'] = segms[i]\n                    segm_json_results.append(data)\n        return bbox_json_results, segm_json_results\n\n    def results2json(self, results, outfile_prefix):\n        """"""Dump the detection results to a json file.\n\n        There are 3 types of results: proposals, bbox predictions, mask\n        predictions, and they have different data types. This method will\n        automatically recognize the type, and dump them to json files.\n\n        Args:\n            results (list[list | tuple | ndarray]): Testing results of the\n                dataset.\n            outfile_prefix (str): The filename prefix of the json files. If the\n                prefix is ""somepath/xxx"", the json files will be named\n                ""somepath/xxx.bbox.json"", ""somepath/xxx.segm.json"",\n                ""somepath/xxx.proposal.json"".\n\n        Returns:\n            dict[str: str]: Possible keys are ""bbox"", ""segm"", ""proposal"", and\n                values are corresponding filenames.\n        """"""\n        result_files = dict()\n        if isinstance(results[0], list):\n            json_results = self._det2json(results)\n            result_files[\'bbox\'] = f\'{outfile_prefix}.bbox.json\'\n            result_files[\'proposal\'] = f\'{outfile_prefix}.bbox.json\'\n            mmcv.dump(json_results, result_files[\'bbox\'])\n        elif isinstance(results[0], tuple):\n            json_results = self._segm2json(results)\n            result_files[\'bbox\'] = f\'{outfile_prefix}.bbox.json\'\n            result_files[\'proposal\'] = f\'{outfile_prefix}.bbox.json\'\n            result_files[\'segm\'] = f\'{outfile_prefix}.segm.json\'\n            mmcv.dump(json_results[0], result_files[\'bbox\'])\n            mmcv.dump(json_results[1], result_files[\'segm\'])\n        elif isinstance(results[0], np.ndarray):\n            json_results = self._proposal2json(results)\n            result_files[\'proposal\'] = f\'{outfile_prefix}.proposal.json\'\n            mmcv.dump(json_results, result_files[\'proposal\'])\n        else:\n            raise TypeError(\'invalid type of results\')\n        return result_files\n\n    def fast_eval_recall(self, results, proposal_nums, iou_thrs, logger=None):\n        gt_bboxes = []\n        for i in range(len(self.img_ids)):\n            ann_ids = self.coco.get_ann_ids(img_ids=self.img_ids[i])\n            ann_info = self.coco.load_anns(ann_ids)\n            if len(ann_info) == 0:\n                gt_bboxes.append(np.zeros((0, 4)))\n                continue\n            bboxes = []\n            for ann in ann_info:\n                if ann.get(\'ignore\', False) or ann[\'iscrowd\']:\n                    continue\n                x1, y1, w, h = ann[\'bbox\']\n                bboxes.append([x1, y1, x1 + w, y1 + h])\n            bboxes = np.array(bboxes, dtype=np.float32)\n            if bboxes.shape[0] == 0:\n                bboxes = np.zeros((0, 4))\n            gt_bboxes.append(bboxes)\n\n        recalls = eval_recalls(\n            gt_bboxes, results, proposal_nums, iou_thrs, logger=logger)\n        ar = recalls.mean(axis=1)\n        return ar\n\n    def format_results(self, results, jsonfile_prefix=None, **kwargs):\n        """"""Format the results to json (standard format for COCO evaluation).\n\n        Args:\n            results (list): Testing results of the dataset.\n            jsonfile_prefix (str | None): The prefix of json files. It includes\n                the file path and the prefix of filename, e.g., ""a/b/prefix"".\n                If not specified, a temp file will be created. Default: None.\n\n        Returns:\n            tuple: (result_files, tmp_dir), result_files is a dict containing\n                the json filepaths, tmp_dir is the temporal directory created\n                for saving json files when jsonfile_prefix is not specified.\n        """"""\n        assert isinstance(results, list), \'results must be a list\'\n        assert len(results) == len(self), (\n            \'The length of results is not equal to the dataset len: {} != {}\'.\n            format(len(results), len(self)))\n\n        if jsonfile_prefix is None:\n            tmp_dir = tempfile.TemporaryDirectory()\n            jsonfile_prefix = osp.join(tmp_dir.name, \'results\')\n        else:\n            tmp_dir = None\n        result_files = self.results2json(results, jsonfile_prefix)\n        return result_files, tmp_dir\n\n    def evaluate(self,\n                 results,\n                 metric=\'bbox\',\n                 logger=None,\n                 jsonfile_prefix=None,\n                 classwise=False,\n                 proposal_nums=(100, 300, 1000),\n                 iou_thrs=np.arange(0.5, 0.96, 0.05)):\n        """"""Evaluation in COCO protocol.\n\n        Args:\n            results (list): Testing results of the dataset.\n            metric (str | list[str]): Metrics to be evaluated.\n            logger (logging.Logger | str | None): Logger used for printing\n                related information during evaluation. Default: None.\n            jsonfile_prefix (str | None): The prefix of json files. It includes\n                the file path and the prefix of filename, e.g., ""a/b/prefix"".\n                If not specified, a temp file will be created. Default: None.\n            classwise (bool): Whether to evaluating the AP for each class.\n            proposal_nums (Sequence[int]): Proposal number used for evaluating\n                recalls, such as recall@100, recall@1000.\n                Default: (100, 300, 1000).\n            iou_thrs (Sequence[float]): IoU threshold used for evaluating\n                recalls. If set to a list, the average recall of all IoUs will\n                also be computed. Default: 0.5.\n\n        Returns:\n            dict[str: float]\n        """"""\n\n        metrics = metric if isinstance(metric, list) else [metric]\n        allowed_metrics = [\'bbox\', \'segm\', \'proposal\', \'proposal_fast\']\n        for metric in metrics:\n            if metric not in allowed_metrics:\n                raise KeyError(f\'metric {metric} is not supported\')\n\n        result_files, tmp_dir = self.format_results(results, jsonfile_prefix)\n\n        eval_results = {}\n        cocoGt = self.coco\n        for metric in metrics:\n            msg = f\'Evaluating {metric}...\'\n            if logger is None:\n                msg = \'\\n\' + msg\n            print_log(msg, logger=logger)\n\n            if metric == \'proposal_fast\':\n                ar = self.fast_eval_recall(\n                    results, proposal_nums, iou_thrs, logger=\'silent\')\n                log_msg = []\n                for i, num in enumerate(proposal_nums):\n                    eval_results[f\'AR@{num}\'] = ar[i]\n                    log_msg.append(f\'\\nAR@{num}\\t{ar[i]:.4f}\')\n                log_msg = \'\'.join(log_msg)\n                print_log(log_msg, logger=logger)\n                continue\n\n            if metric not in result_files:\n                raise KeyError(f\'{metric} is not in results\')\n            try:\n                cocoDt = cocoGt.loadRes(result_files[metric])\n            except IndexError:\n                print_log(\n                    \'The testing results of the whole dataset is empty.\',\n                    logger=logger,\n                    level=logging.ERROR)\n                break\n\n            iou_type = \'bbox\' if metric == \'proposal\' else metric\n            cocoEval = COCOeval(cocoGt, cocoDt, iou_type)\n            cocoEval.params.catIds = self.cat_ids\n            cocoEval.params.imgIds = self.img_ids\n            if metric == \'proposal\':\n                cocoEval.params.useCats = 0\n                cocoEval.params.maxDets = list(proposal_nums)\n                cocoEval.evaluate()\n                cocoEval.accumulate()\n                cocoEval.summarize()\n                metric_items = [\n                    \'AR@100\', \'AR@300\', \'AR@1000\', \'AR_s@1000\', \'AR_m@1000\',\n                    \'AR_l@1000\'\n                ]\n                for i, item in enumerate(metric_items):\n                    val = float(f\'{cocoEval.stats[i + 6]:.3f}\')\n                    eval_results[item] = val\n            else:\n                cocoEval.evaluate()\n                cocoEval.accumulate()\n                cocoEval.summarize()\n                if classwise:  # Compute per-category AP\n                    # Compute per-category AP\n                    # from https://github.com/facebookresearch/detectron2/\n                    precisions = cocoEval.eval[\'precision\']\n                    # precision: (iou, recall, cls, area range, max dets)\n                    assert len(self.cat_ids) == precisions.shape[2]\n\n                    results_per_category = []\n                    for idx, catId in enumerate(self.cat_ids):\n                        # area range index 0: all area ranges\n                        # max dets index -1: typically 100 per image\n                        nm = self.coco.loadCats(catId)[0]\n                        precision = precisions[:, :, idx, 0, -1]\n                        precision = precision[precision > -1]\n                        if precision.size:\n                            ap = np.mean(precision)\n                        else:\n                            ap = float(\'nan\')\n                        results_per_category.append(\n                            (f\'{nm[""name""]}\', f\'{float(ap):0.3f}\'))\n\n                    num_columns = min(6, len(results_per_category) * 2)\n                    results_flatten = list(\n                        itertools.chain(*results_per_category))\n                    headers = [\'category\', \'AP\'] * (num_columns // 2)\n                    results_2d = itertools.zip_longest(*[\n                        results_flatten[i::num_columns]\n                        for i in range(num_columns)\n                    ])\n                    table_data = [headers]\n                    table_data += [result for result in results_2d]\n                    table = AsciiTable(table_data)\n                    print_log(\'\\n\' + table.table, logger=logger)\n\n                metric_items = [\n                    \'mAP\', \'mAP_50\', \'mAP_75\', \'mAP_s\', \'mAP_m\', \'mAP_l\'\n                ]\n                for i in range(len(metric_items)):\n                    key = f\'{metric}_{metric_items[i]}\'\n                    val = float(f\'{cocoEval.stats[i]:.3f}\')\n                    eval_results[key] = val\n                ap = cocoEval.stats[:6]\n                eval_results[f\'{metric}_mAP_copypaste\'] = (\n                    f\'{ap[0]:.3f} {ap[1]:.3f} {ap[2]:.3f} {ap[3]:.3f} \'\n                    f\'{ap[4]:.3f} {ap[5]:.3f}\')\n        if tmp_dir is not None:\n            tmp_dir.cleanup()\n        return eval_results\n'"
mmdet/datasets/custom.py,1,"b'import os.path as osp\n\nimport mmcv\nimport numpy as np\nfrom torch.utils.data import Dataset\n\nfrom mmdet.core import eval_map, eval_recalls\nfrom .builder import DATASETS\nfrom .pipelines import Compose\n\n\n@DATASETS.register_module()\nclass CustomDataset(Dataset):\n    """"""Custom dataset for detection.\n\n    The annotation format is shown as follows. The `ann` field is optional for\n    testing.\n\n    .. code-block:: none\n\n        [\n            {\n                \'filename\': \'a.jpg\',\n                \'width\': 1280,\n                \'height\': 720,\n                \'ann\': {\n                    \'bboxes\': <np.ndarray> (n, 4),\n                    \'labels\': <np.ndarray> (n, ),\n                    \'bboxes_ignore\': <np.ndarray> (k, 4), (optional field)\n                    \'labels_ignore\': <np.ndarray> (k, 4) (optional field)\n                }\n            },\n            ...\n        ]\n    """"""\n\n    CLASSES = None\n\n    def __init__(self,\n                 ann_file,\n                 pipeline,\n                 classes=None,\n                 data_root=None,\n                 img_prefix=\'\',\n                 seg_prefix=None,\n                 proposal_file=None,\n                 test_mode=False,\n                 filter_empty_gt=True):\n        self.ann_file = ann_file\n        self.data_root = data_root\n        self.img_prefix = img_prefix\n        self.seg_prefix = seg_prefix\n        self.proposal_file = proposal_file\n        self.test_mode = test_mode\n        self.filter_empty_gt = filter_empty_gt\n        self.CLASSES = self.get_classes(classes)\n\n        # join paths if data_root is specified\n        if self.data_root is not None:\n            if not osp.isabs(self.ann_file):\n                self.ann_file = osp.join(self.data_root, self.ann_file)\n            if not (self.img_prefix is None or osp.isabs(self.img_prefix)):\n                self.img_prefix = osp.join(self.data_root, self.img_prefix)\n            if not (self.seg_prefix is None or osp.isabs(self.seg_prefix)):\n                self.seg_prefix = osp.join(self.data_root, self.seg_prefix)\n            if not (self.proposal_file is None\n                    or osp.isabs(self.proposal_file)):\n                self.proposal_file = osp.join(self.data_root,\n                                              self.proposal_file)\n        # load annotations (and proposals)\n        self.data_infos = self.load_annotations(self.ann_file)\n        # filter data infos if classes are customized\n        if self.custom_classes:\n            self.data_infos = self.get_subset_by_classes()\n\n        if self.proposal_file is not None:\n            self.proposals = self.load_proposals(self.proposal_file)\n        else:\n            self.proposals = None\n        # filter images too small\n        if not test_mode:\n            valid_inds = self._filter_imgs()\n            self.data_infos = [self.data_infos[i] for i in valid_inds]\n            if self.proposals is not None:\n                self.proposals = [self.proposals[i] for i in valid_inds]\n        # set group flag for the sampler\n        if not self.test_mode:\n            self._set_group_flag()\n        # processing pipeline\n        self.pipeline = Compose(pipeline)\n\n    def __len__(self):\n        return len(self.data_infos)\n\n    def load_annotations(self, ann_file):\n        return mmcv.load(ann_file)\n\n    def load_proposals(self, proposal_file):\n        return mmcv.load(proposal_file)\n\n    def get_ann_info(self, idx):\n        return self.data_infos[idx][\'ann\']\n\n    def get_cat_ids(self, idx):\n        return self.data_infos[idx][\'ann\'][\'labels\'].astype(np.int).tolist()\n\n    def pre_pipeline(self, results):\n        results[\'img_prefix\'] = self.img_prefix\n        results[\'seg_prefix\'] = self.seg_prefix\n        results[\'proposal_file\'] = self.proposal_file\n        results[\'bbox_fields\'] = []\n        results[\'mask_fields\'] = []\n        results[\'seg_fields\'] = []\n\n    def _filter_imgs(self, min_size=32):\n        """"""Filter images too small.""""""\n        valid_inds = []\n        for i, img_info in enumerate(self.data_infos):\n            if min(img_info[\'width\'], img_info[\'height\']) >= min_size:\n                valid_inds.append(i)\n        return valid_inds\n\n    def _set_group_flag(self):\n        """"""Set flag according to image aspect ratio.\n\n        Images with aspect ratio greater than 1 will be set as group 1,\n        otherwise group 0.\n        """"""\n        self.flag = np.zeros(len(self), dtype=np.uint8)\n        for i in range(len(self)):\n            img_info = self.data_infos[i]\n            if img_info[\'width\'] / img_info[\'height\'] > 1:\n                self.flag[i] = 1\n\n    def _rand_another(self, idx):\n        pool = np.where(self.flag == self.flag[idx])[0]\n        return np.random.choice(pool)\n\n    def __getitem__(self, idx):\n        if self.test_mode:\n            return self.prepare_test_img(idx)\n        while True:\n            data = self.prepare_train_img(idx)\n            if data is None:\n                idx = self._rand_another(idx)\n                continue\n            return data\n\n    def prepare_train_img(self, idx):\n        img_info = self.data_infos[idx]\n        ann_info = self.get_ann_info(idx)\n        results = dict(img_info=img_info, ann_info=ann_info)\n        if self.proposals is not None:\n            results[\'proposals\'] = self.proposals[idx]\n        self.pre_pipeline(results)\n        return self.pipeline(results)\n\n    def prepare_test_img(self, idx):\n        img_info = self.data_infos[idx]\n        results = dict(img_info=img_info)\n        if self.proposals is not None:\n            results[\'proposals\'] = self.proposals[idx]\n        self.pre_pipeline(results)\n        return self.pipeline(results)\n\n    @classmethod\n    def get_classes(cls, classes=None):\n        """"""Get class names of current dataset\n\n        Args:\n            classes (Sequence[str] | str | None): If classes is None, use\n                default CLASSES defined by builtin dataset. If classes is a\n                string, take it as a file name. The file contains the name of\n                classes where each line contains one class name. If classes is\n                a tuple or list, override the CLASSES defined by the dataset.\n\n        """"""\n        if classes is None:\n            cls.custom_classes = False\n            return cls.CLASSES\n\n        cls.custom_classes = True\n        if isinstance(classes, str):\n            # take it as a file path\n            class_names = mmcv.list_from_file(classes)\n        elif isinstance(classes, (tuple, list)):\n            class_names = classes\n        else:\n            raise ValueError(f\'Unsupported type {type(classes)} of classes.\')\n\n        return class_names\n\n    def get_subset_by_classes(self):\n        return self.data_infos\n\n    def format_results(self, results, **kwargs):\n        pass\n\n    def evaluate(self,\n                 results,\n                 metric=\'mAP\',\n                 logger=None,\n                 proposal_nums=(100, 300, 1000),\n                 iou_thr=0.5,\n                 scale_ranges=None):\n        """"""Evaluate the dataset.\n\n        Args:\n            results (list): Testing results of the dataset.\n            metric (str | list[str]): Metrics to be evaluated.\n            logger (logging.Logger | None | str): Logger used for printing\n                related information during evaluation. Default: None.\n            proposal_nums (Sequence[int]): Proposal number used for evaluating\n                recalls, such as recall@100, recall@1000.\n                Default: (100, 300, 1000).\n            iou_thr (float | list[float]): IoU threshold. It must be a float\n                when evaluating mAP, and can be a list when evaluating recall.\n                Default: 0.5.\n            scale_ranges (list[tuple] | None): Scale ranges for evaluating mAP.\n                Default: None.\n        """"""\n        if not isinstance(metric, str):\n            assert len(metric) == 1\n            metric = metric[0]\n        allowed_metrics = [\'mAP\', \'recall\']\n        if metric not in allowed_metrics:\n            raise KeyError(f\'metric {metric} is not supported\')\n        annotations = [self.get_ann_info(i) for i in range(len(self))]\n        eval_results = {}\n        if metric == \'mAP\':\n            assert isinstance(iou_thr, float)\n            mean_ap, _ = eval_map(\n                results,\n                annotations,\n                scale_ranges=scale_ranges,\n                iou_thr=iou_thr,\n                dataset=self.CLASSES,\n                logger=logger)\n            eval_results[\'mAP\'] = mean_ap\n        elif metric == \'recall\':\n            gt_bboxes = [ann[\'bboxes\'] for ann in annotations]\n            if isinstance(iou_thr, float):\n                iou_thr = [iou_thr]\n            recalls = eval_recalls(\n                gt_bboxes, results, proposal_nums, iou_thr, logger=logger)\n            for i, num in enumerate(proposal_nums):\n                for j, iou in enumerate(iou_thr):\n                    eval_results[f\'recall@{num}@{iou}\'] = recalls[i, j]\n            if recalls.shape[1] > 1:\n                ar = recalls.mean(axis=1)\n                for i, num in enumerate(proposal_nums):\n                    eval_results[f\'AR@{num}\'] = ar[i]\n        return eval_results\n'"
mmdet/datasets/dataset_wrappers.py,2,"b'import bisect\nimport math\nfrom collections import defaultdict\n\nimport numpy as np\nfrom torch.utils.data.dataset import ConcatDataset as _ConcatDataset\n\nfrom .builder import DATASETS\n\n\n@DATASETS.register_module()\nclass ConcatDataset(_ConcatDataset):\n    """"""A wrapper of concatenated dataset.\n\n    Same as :obj:`torch.utils.data.dataset.ConcatDataset`, but\n    concat the group flag for image aspect ratio.\n\n    Args:\n        datasets (list[:obj:`Dataset`]): A list of datasets.\n    """"""\n\n    def __init__(self, datasets):\n        super(ConcatDataset, self).__init__(datasets)\n        self.CLASSES = datasets[0].CLASSES\n        if hasattr(datasets[0], \'flag\'):\n            flags = []\n            for i in range(0, len(datasets)):\n                flags.append(datasets[i].flag)\n            self.flag = np.concatenate(flags)\n\n    def get_cat_ids(self, idx):\n        if idx < 0:\n            if -idx > len(self):\n                raise ValueError(\n                    \'absolute value of index should not exceed dataset length\')\n            idx = len(self) + idx\n        dataset_idx = bisect.bisect_right(self.cumulative_sizes, idx)\n        if dataset_idx == 0:\n            sample_idx = idx\n        else:\n            sample_idx = idx - self.cumulative_sizes[dataset_idx - 1]\n        return self.datasets[dataset_idx].get_cat_ids(sample_idx)\n\n\n@DATASETS.register_module()\nclass RepeatDataset(object):\n    """"""A wrapper of repeated dataset.\n\n    The length of repeated dataset will be `times` larger than the original\n    dataset. This is useful when the data loading time is long but the dataset\n    is small. Using RepeatDataset can reduce the data loading time between\n    epochs.\n\n    Args:\n        dataset (:obj:`Dataset`): The dataset to be repeated.\n        times (int): Repeat times.\n    """"""\n\n    def __init__(self, dataset, times):\n        self.dataset = dataset\n        self.times = times\n        self.CLASSES = dataset.CLASSES\n        if hasattr(self.dataset, \'flag\'):\n            self.flag = np.tile(self.dataset.flag, times)\n\n        self._ori_len = len(self.dataset)\n\n    def __getitem__(self, idx):\n        return self.dataset[idx % self._ori_len]\n\n    def get_cat_ids(self, idx):\n        return self.dataset.get_cat_ids(idx % self._ori_len)\n\n    def __len__(self):\n        return self.times * self._ori_len\n\n\n# Modified from https://github.com/facebookresearch/detectron2/blob/41d475b75a230221e21d9cac5d69655e3415e3a4/detectron2/data/samplers/distributed_sampler.py#L57 # noqa\n@DATASETS.register_module()\nclass ClassBalancedDataset(object):\n    """"""A wrapper of repeated dataset with repeat factor.\n\n    Suitable for training on class imbalanced datasets like LVIS. Following\n    the sampling strategy in [1], in each epoch, an image may appear multiple\n    times based on its ""repeat factor"".\n    The repeat factor for an image is a function of the frequency the rarest\n    category labeled in that image. The ""frequency of category c"" in [0, 1]\n    is defined by the fraction of images in the training set (without repeats)\n    in which category c appears.\n    The dataset needs to instantiate :func:`self.get_cat_ids(idx)` to support\n    ClassBalancedDataset.\n    The repeat factor is computed as followed.\n    1. For each category c, compute the fraction # of images\n        that contain it: f(c)\n    2. For each category c, compute the category-level repeat factor:\n        r(c) = max(1, sqrt(t/f(c)))\n    3. For each image I, compute the image-level repeat factor:\n        r(I) = max_{c in I} r(c)\n\n    References:\n        .. [1]  https://arxiv.org/pdf/1903.00621v2.pdf\n\n    Args:\n        dataset (:obj:`CustomDataset`): The dataset to be repeated.\n        oversample_thr (float): frequency threshold below which data is\n            repeated. For categories with `f_c` >= `oversample_thr`, there is\n            no oversampling. For categories with `f_c` < `oversample_thr`, the\n            degree of oversampling following the square-root inverse frequency\n            heuristic above.\n    """"""\n\n    def __init__(self, dataset, oversample_thr):\n        self.dataset = dataset\n        self.oversample_thr = oversample_thr\n        self.CLASSES = dataset.CLASSES\n\n        repeat_factors = self._get_repeat_factors(dataset, oversample_thr)\n        repeat_indices = []\n        for dataset_index, repeat_factor in enumerate(repeat_factors):\n            repeat_indices.extend([dataset_index] * math.ceil(repeat_factor))\n        self.repeat_indices = repeat_indices\n\n        flags = []\n        if hasattr(self.dataset, \'flag\'):\n            for flag, repeat_factor in zip(self.dataset.flag, repeat_factors):\n                flags.extend([flag] * int(math.ceil(repeat_factor)))\n            assert len(flags) == len(repeat_indices)\n        self.flag = np.asarray(flags, dtype=np.uint8)\n\n    def _get_repeat_factors(self, dataset, repeat_thr):\n        # 1. For each category c, compute the fraction # of images\n        #   that contain it: f(c)\n        category_freq = defaultdict(int)\n        num_images = len(dataset)\n        for idx in range(num_images):\n            cat_ids = set(self.dataset.get_cat_ids(idx))\n            for cat_id in cat_ids:\n                category_freq[cat_id] += 1\n        for k, v in category_freq.items():\n            category_freq[k] = v / num_images\n\n        # 2. For each category c, compute the category-level repeat factor:\n        #    r(c) = max(1, sqrt(t/f(c)))\n        category_repeat = {\n            cat_id: max(1.0, math.sqrt(repeat_thr / cat_freq))\n            for cat_id, cat_freq in category_freq.items()\n        }\n\n        # 3. For each image I, compute the image-level repeat factor:\n        #    r(I) = max_{c in I} r(c)\n        repeat_factors = []\n        for idx in range(num_images):\n            cat_ids = set(self.dataset.get_cat_ids(idx))\n            repeat_factor = max(\n                {category_repeat[cat_id]\n                 for cat_id in cat_ids})\n            repeat_factors.append(repeat_factor)\n\n        return repeat_factors\n\n    def __getitem__(self, idx):\n        ori_index = self.repeat_indices[idx]\n        return self.dataset[ori_index]\n\n    def __len__(self):\n        return len(self.repeat_indices)\n'"
mmdet/datasets/lvis.py,0,"b'import itertools\nimport logging\nimport os.path as osp\nimport tempfile\n\nimport numpy as np\nfrom mmcv.utils import print_log\nfrom terminaltables import AsciiTable\n\nfrom .builder import DATASETS\nfrom .coco import CocoDataset\n\n\n@DATASETS.register_module()\nclass LVISDataset(CocoDataset):\n\n    CLASSES = (\n        \'acorn\', \'aerosol_can\', \'air_conditioner\', \'airplane\', \'alarm_clock\',\n        \'alcohol\', \'alligator\', \'almond\', \'ambulance\', \'amplifier\', \'anklet\',\n        \'antenna\', \'apple\', \'apple_juice\', \'applesauce\', \'apricot\', \'apron\',\n        \'aquarium\', \'armband\', \'armchair\', \'armoire\', \'armor\', \'artichoke\',\n        \'trash_can\', \'ashtray\', \'asparagus\', \'atomizer\', \'avocado\', \'award\',\n        \'awning\', \'ax\', \'baby_buggy\', \'basketball_backboard\', \'backpack\',\n        \'handbag\', \'suitcase\', \'bagel\', \'bagpipe\', \'baguet\', \'bait\', \'ball\',\n        \'ballet_skirt\', \'balloon\', \'bamboo\', \'banana\', \'Band_Aid\', \'bandage\',\n        \'bandanna\', \'banjo\', \'banner\', \'barbell\', \'barge\', \'barrel\',\n        \'barrette\', \'barrow\', \'baseball_base\', \'baseball\', \'baseball_bat\',\n        \'baseball_cap\', \'baseball_glove\', \'basket\', \'basketball_hoop\',\n        \'basketball\', \'bass_horn\', \'bat_(animal)\', \'bath_mat\', \'bath_towel\',\n        \'bathrobe\', \'bathtub\', \'batter_(food)\', \'battery\', \'beachball\', \'bead\',\n        \'beaker\', \'bean_curd\', \'beanbag\', \'beanie\', \'bear\', \'bed\',\n        \'bedspread\', \'cow\', \'beef_(food)\', \'beeper\', \'beer_bottle\', \'beer_can\',\n        \'beetle\', \'bell\', \'bell_pepper\', \'belt\', \'belt_buckle\', \'bench\',\n        \'beret\', \'bib\', \'Bible\', \'bicycle\', \'visor\', \'binder\', \'binoculars\',\n        \'bird\', \'birdfeeder\', \'birdbath\', \'birdcage\', \'birdhouse\',\n        \'birthday_cake\', \'birthday_card\', \'biscuit_(bread)\', \'pirate_flag\',\n        \'black_sheep\', \'blackboard\', \'blanket\', \'blazer\', \'blender\', \'blimp\',\n        \'blinker\', \'blueberry\', \'boar\', \'gameboard\', \'boat\', \'bobbin\',\n        \'bobby_pin\', \'boiled_egg\', \'bolo_tie\', \'deadbolt\', \'bolt\', \'bonnet\',\n        \'book\', \'book_bag\', \'bookcase\', \'booklet\', \'bookmark\',\n        \'boom_microphone\', \'boot\', \'bottle\', \'bottle_opener\', \'bouquet\',\n        \'bow_(weapon)\', \'bow_(decorative_ribbons)\', \'bow-tie\', \'bowl\',\n        \'pipe_bowl\', \'bowler_hat\', \'bowling_ball\', \'bowling_pin\',\n        \'boxing_glove\', \'suspenders\', \'bracelet\', \'brass_plaque\', \'brassiere\',\n        \'bread-bin\', \'breechcloth\', \'bridal_gown\', \'briefcase\',\n        \'bristle_brush\', \'broccoli\', \'broach\', \'broom\', \'brownie\',\n        \'brussels_sprouts\', \'bubble_gum\', \'bucket\', \'horse_buggy\', \'bull\',\n        \'bulldog\', \'bulldozer\', \'bullet_train\', \'bulletin_board\',\n        \'bulletproof_vest\', \'bullhorn\', \'corned_beef\', \'bun\', \'bunk_bed\',\n        \'buoy\', \'burrito\', \'bus_(vehicle)\', \'business_card\', \'butcher_knife\',\n        \'butter\', \'butterfly\', \'button\', \'cab_(taxi)\', \'cabana\', \'cabin_car\',\n        \'cabinet\', \'locker\', \'cake\', \'calculator\', \'calendar\', \'calf\',\n        \'camcorder\', \'camel\', \'camera\', \'camera_lens\', \'camper_(vehicle)\',\n        \'can\', \'can_opener\', \'candelabrum\', \'candle\', \'candle_holder\',\n        \'candy_bar\', \'candy_cane\', \'walking_cane\', \'canister\', \'cannon\',\n        \'canoe\', \'cantaloup\', \'canteen\', \'cap_(headwear)\', \'bottle_cap\',\n        \'cape\', \'cappuccino\', \'car_(automobile)\', \'railcar_(part_of_a_train)\',\n        \'elevator_car\', \'car_battery\', \'identity_card\', \'card\', \'cardigan\',\n        \'cargo_ship\', \'carnation\', \'horse_carriage\', \'carrot\', \'tote_bag\',\n        \'cart\', \'carton\', \'cash_register\', \'casserole\', \'cassette\', \'cast\',\n        \'cat\', \'cauliflower\', \'caviar\', \'cayenne_(spice)\', \'CD_player\',\n        \'celery\', \'cellular_telephone\', \'chain_mail\', \'chair\', \'chaise_longue\',\n        \'champagne\', \'chandelier\', \'chap\', \'checkbook\', \'checkerboard\',\n        \'cherry\', \'chessboard\', \'chest_of_drawers_(furniture)\',\n        \'chicken_(animal)\', \'chicken_wire\', \'chickpea\', \'Chihuahua\',\n        \'chili_(vegetable)\', \'chime\', \'chinaware\', \'crisp_(potato_chip)\',\n        \'poker_chip\', \'chocolate_bar\', \'chocolate_cake\', \'chocolate_milk\',\n        \'chocolate_mousse\', \'choker\', \'chopping_board\', \'chopstick\',\n        \'Christmas_tree\', \'slide\', \'cider\', \'cigar_box\', \'cigarette\',\n        \'cigarette_case\', \'cistern\', \'clarinet\', \'clasp\', \'cleansing_agent\',\n        \'clementine\', \'clip\', \'clipboard\', \'clock\', \'clock_tower\',\n        \'clothes_hamper\', \'clothespin\', \'clutch_bag\', \'coaster\', \'coat\',\n        \'coat_hanger\', \'coatrack\', \'cock\', \'coconut\', \'coffee_filter\',\n        \'coffee_maker\', \'coffee_table\', \'coffeepot\', \'coil\', \'coin\',\n        \'colander\', \'coleslaw\', \'coloring_material\', \'combination_lock\',\n        \'pacifier\', \'comic_book\', \'computer_keyboard\', \'concrete_mixer\',\n        \'cone\', \'control\', \'convertible_(automobile)\', \'sofa_bed\', \'cookie\',\n        \'cookie_jar\', \'cooking_utensil\', \'cooler_(for_food)\',\n        \'cork_(bottle_plug)\', \'corkboard\', \'corkscrew\', \'edible_corn\',\n        \'cornbread\', \'cornet\', \'cornice\', \'cornmeal\', \'corset\',\n        \'romaine_lettuce\', \'costume\', \'cougar\', \'coverall\', \'cowbell\',\n        \'cowboy_hat\', \'crab_(animal)\', \'cracker\', \'crape\', \'crate\', \'crayon\',\n        \'cream_pitcher\', \'credit_card\', \'crescent_roll\', \'crib\', \'crock_pot\',\n        \'crossbar\', \'crouton\', \'crow\', \'crown\', \'crucifix\', \'cruise_ship\',\n        \'police_cruiser\', \'crumb\', \'crutch\', \'cub_(animal)\', \'cube\',\n        \'cucumber\', \'cufflink\', \'cup\', \'trophy_cup\', \'cupcake\', \'hair_curler\',\n        \'curling_iron\', \'curtain\', \'cushion\', \'custard\', \'cutting_tool\',\n        \'cylinder\', \'cymbal\', \'dachshund\', \'dagger\', \'dartboard\',\n        \'date_(fruit)\', \'deck_chair\', \'deer\', \'dental_floss\', \'desk\',\n        \'detergent\', \'diaper\', \'diary\', \'die\', \'dinghy\', \'dining_table\', \'tux\',\n        \'dish\', \'dish_antenna\', \'dishrag\', \'dishtowel\', \'dishwasher\',\n        \'dishwasher_detergent\', \'diskette\', \'dispenser\', \'Dixie_cup\', \'dog\',\n        \'dog_collar\', \'doll\', \'dollar\', \'dolphin\', \'domestic_ass\', \'eye_mask\',\n        \'doorbell\', \'doorknob\', \'doormat\', \'doughnut\', \'dove\', \'dragonfly\',\n        \'drawer\', \'underdrawers\', \'dress\', \'dress_hat\', \'dress_suit\',\n        \'dresser\', \'drill\', \'drinking_fountain\', \'drone\', \'dropper\',\n        \'drum_(musical_instrument)\', \'drumstick\', \'duck\', \'duckling\',\n        \'duct_tape\', \'duffel_bag\', \'dumbbell\', \'dumpster\', \'dustpan\',\n        \'Dutch_oven\', \'eagle\', \'earphone\', \'earplug\', \'earring\', \'easel\',\n        \'eclair\', \'eel\', \'egg\', \'egg_roll\', \'egg_yolk\', \'eggbeater\',\n        \'eggplant\', \'electric_chair\', \'refrigerator\', \'elephant\', \'elk\',\n        \'envelope\', \'eraser\', \'escargot\', \'eyepatch\', \'falcon\', \'fan\',\n        \'faucet\', \'fedora\', \'ferret\', \'Ferris_wheel\', \'ferry\', \'fig_(fruit)\',\n        \'fighter_jet\', \'figurine\', \'file_cabinet\', \'file_(tool)\', \'fire_alarm\',\n        \'fire_engine\', \'fire_extinguisher\', \'fire_hose\', \'fireplace\',\n        \'fireplug\', \'fish\', \'fish_(food)\', \'fishbowl\', \'fishing_boat\',\n        \'fishing_rod\', \'flag\', \'flagpole\', \'flamingo\', \'flannel\', \'flash\',\n        \'flashlight\', \'fleece\', \'flip-flop_(sandal)\', \'flipper_(footwear)\',\n        \'flower_arrangement\', \'flute_glass\', \'foal\', \'folding_chair\',\n        \'food_processor\', \'football_(American)\', \'football_helmet\',\n        \'footstool\', \'fork\', \'forklift\', \'freight_car\', \'French_toast\',\n        \'freshener\', \'frisbee\', \'frog\', \'fruit_juice\', \'fruit_salad\',\n        \'frying_pan\', \'fudge\', \'funnel\', \'futon\', \'gag\', \'garbage\',\n        \'garbage_truck\', \'garden_hose\', \'gargle\', \'gargoyle\', \'garlic\',\n        \'gasmask\', \'gazelle\', \'gelatin\', \'gemstone\', \'giant_panda\',\n        \'gift_wrap\', \'ginger\', \'giraffe\', \'cincture\',\n        \'glass_(drink_container)\', \'globe\', \'glove\', \'goat\', \'goggles\',\n        \'goldfish\', \'golf_club\', \'golfcart\', \'gondola_(boat)\', \'goose\',\n        \'gorilla\', \'gourd\', \'surgical_gown\', \'grape\', \'grasshopper\', \'grater\',\n        \'gravestone\', \'gravy_boat\', \'green_bean\', \'green_onion\', \'griddle\',\n        \'grillroom\', \'grinder_(tool)\', \'grits\', \'grizzly\', \'grocery_bag\',\n        \'guacamole\', \'guitar\', \'gull\', \'gun\', \'hair_spray\', \'hairbrush\',\n        \'hairnet\', \'hairpin\', \'ham\', \'hamburger\', \'hammer\', \'hammock\',\n        \'hamper\', \'hamster\', \'hair_dryer\', \'hand_glass\', \'hand_towel\',\n        \'handcart\', \'handcuff\', \'handkerchief\', \'handle\', \'handsaw\',\n        \'hardback_book\', \'harmonium\', \'hat\', \'hatbox\', \'hatch\', \'veil\',\n        \'headband\', \'headboard\', \'headlight\', \'headscarf\', \'headset\',\n        \'headstall_(for_horses)\', \'hearing_aid\', \'heart\', \'heater\',\n        \'helicopter\', \'helmet\', \'heron\', \'highchair\', \'hinge\', \'hippopotamus\',\n        \'hockey_stick\', \'hog\', \'home_plate_(baseball)\', \'honey\', \'fume_hood\',\n        \'hook\', \'horse\', \'hose\', \'hot-air_balloon\', \'hotplate\', \'hot_sauce\',\n        \'hourglass\', \'houseboat\', \'hummingbird\', \'hummus\', \'polar_bear\',\n        \'icecream\', \'popsicle\', \'ice_maker\', \'ice_pack\', \'ice_skate\',\n        \'ice_tea\', \'igniter\', \'incense\', \'inhaler\', \'iPod\',\n        \'iron_(for_clothing)\', \'ironing_board\', \'jacket\', \'jam\', \'jean\',\n        \'jeep\', \'jelly_bean\', \'jersey\', \'jet_plane\', \'jewelry\', \'joystick\',\n        \'jumpsuit\', \'kayak\', \'keg\', \'kennel\', \'kettle\', \'key\', \'keycard\',\n        \'kilt\', \'kimono\', \'kitchen_sink\', \'kitchen_table\', \'kite\', \'kitten\',\n        \'kiwi_fruit\', \'knee_pad\', \'knife\', \'knight_(chess_piece)\',\n        \'knitting_needle\', \'knob\', \'knocker_(on_a_door)\', \'koala\', \'lab_coat\',\n        \'ladder\', \'ladle\', \'ladybug\', \'lamb_(animal)\', \'lamb-chop\', \'lamp\',\n        \'lamppost\', \'lampshade\', \'lantern\', \'lanyard\', \'laptop_computer\',\n        \'lasagna\', \'latch\', \'lawn_mower\', \'leather\', \'legging_(clothing)\',\n        \'Lego\', \'lemon\', \'lemonade\', \'lettuce\', \'license_plate\', \'life_buoy\',\n        \'life_jacket\', \'lightbulb\', \'lightning_rod\', \'lime\', \'limousine\',\n        \'linen_paper\', \'lion\', \'lip_balm\', \'lipstick\', \'liquor\', \'lizard\',\n        \'Loafer_(type_of_shoe)\', \'log\', \'lollipop\', \'lotion\',\n        \'speaker_(stero_equipment)\', \'loveseat\', \'machine_gun\', \'magazine\',\n        \'magnet\', \'mail_slot\', \'mailbox_(at_home)\', \'mallet\', \'mammoth\',\n        \'mandarin_orange\', \'manger\', \'manhole\', \'map\', \'marker\', \'martini\',\n        \'mascot\', \'mashed_potato\', \'masher\', \'mask\', \'mast\',\n        \'mat_(gym_equipment)\', \'matchbox\', \'mattress\', \'measuring_cup\',\n        \'measuring_stick\', \'meatball\', \'medicine\', \'melon\', \'microphone\',\n        \'microscope\', \'microwave_oven\', \'milestone\', \'milk\', \'minivan\',\n        \'mint_candy\', \'mirror\', \'mitten\', \'mixer_(kitchen_tool)\', \'money\',\n        \'monitor_(computer_equipment) computer_monitor\', \'monkey\', \'motor\',\n        \'motor_scooter\', \'motor_vehicle\', \'motorboat\', \'motorcycle\',\n        \'mound_(baseball)\', \'mouse_(animal_rodent)\',\n        \'mouse_(computer_equipment)\', \'mousepad\', \'muffin\', \'mug\', \'mushroom\',\n        \'music_stool\', \'musical_instrument\', \'nailfile\', \'nameplate\', \'napkin\',\n        \'neckerchief\', \'necklace\', \'necktie\', \'needle\', \'nest\', \'newsstand\',\n        \'nightshirt\', \'nosebag_(for_animals)\', \'noseband_(for_animals)\',\n        \'notebook\', \'notepad\', \'nut\', \'nutcracker\', \'oar\', \'octopus_(food)\',\n        \'octopus_(animal)\', \'oil_lamp\', \'olive_oil\', \'omelet\', \'onion\',\n        \'orange_(fruit)\', \'orange_juice\', \'oregano\', \'ostrich\', \'ottoman\',\n        \'overalls_(clothing)\', \'owl\', \'packet\', \'inkpad\', \'pad\', \'paddle\',\n        \'padlock\', \'paintbox\', \'paintbrush\', \'painting\', \'pajamas\', \'palette\',\n        \'pan_(for_cooking)\', \'pan_(metal_container)\', \'pancake\', \'pantyhose\',\n        \'papaya\', \'paperclip\', \'paper_plate\', \'paper_towel\', \'paperback_book\',\n        \'paperweight\', \'parachute\', \'parakeet\', \'parasail_(sports)\',\n        \'parchment\', \'parka\', \'parking_meter\', \'parrot\',\n        \'passenger_car_(part_of_a_train)\', \'passenger_ship\', \'passport\',\n        \'pastry\', \'patty_(food)\', \'pea_(food)\', \'peach\', \'peanut_butter\',\n        \'pear\', \'peeler_(tool_for_fruit_and_vegetables)\', \'pegboard\',\n        \'pelican\', \'pen\', \'pencil\', \'pencil_box\', \'pencil_sharpener\',\n        \'pendulum\', \'penguin\', \'pennant\', \'penny_(coin)\', \'pepper\',\n        \'pepper_mill\', \'perfume\', \'persimmon\', \'baby\', \'pet\', \'petfood\',\n        \'pew_(church_bench)\', \'phonebook\', \'phonograph_record\', \'piano\',\n        \'pickle\', \'pickup_truck\', \'pie\', \'pigeon\', \'piggy_bank\', \'pillow\',\n        \'pin_(non_jewelry)\', \'pineapple\', \'pinecone\', \'ping-pong_ball\',\n        \'pinwheel\', \'tobacco_pipe\', \'pipe\', \'pistol\', \'pita_(bread)\',\n        \'pitcher_(vessel_for_liquid)\', \'pitchfork\', \'pizza\', \'place_mat\',\n        \'plate\', \'platter\', \'playing_card\', \'playpen\', \'pliers\',\n        \'plow_(farm_equipment)\', \'pocket_watch\', \'pocketknife\',\n        \'poker_(fire_stirring_tool)\', \'pole\', \'police_van\', \'polo_shirt\',\n        \'poncho\', \'pony\', \'pool_table\', \'pop_(soda)\', \'portrait\',\n        \'postbox_(public)\', \'postcard\', \'poster\', \'pot\', \'flowerpot\', \'potato\',\n        \'potholder\', \'pottery\', \'pouch\', \'power_shovel\', \'prawn\', \'printer\',\n        \'projectile_(weapon)\', \'projector\', \'propeller\', \'prune\', \'pudding\',\n        \'puffer_(fish)\', \'puffin\', \'pug-dog\', \'pumpkin\', \'puncher\', \'puppet\',\n        \'puppy\', \'quesadilla\', \'quiche\', \'quilt\', \'rabbit\', \'race_car\',\n        \'racket\', \'radar\', \'radiator\', \'radio_receiver\', \'radish\', \'raft\',\n        \'rag_doll\', \'raincoat\', \'ram_(animal)\', \'raspberry\', \'rat\',\n        \'razorblade\', \'reamer_(juicer)\', \'rearview_mirror\', \'receipt\',\n        \'recliner\', \'record_player\', \'red_cabbage\', \'reflector\',\n        \'remote_control\', \'rhinoceros\', \'rib_(food)\', \'rifle\', \'ring\',\n        \'river_boat\', \'road_map\', \'robe\', \'rocking_chair\', \'roller_skate\',\n        \'Rollerblade\', \'rolling_pin\', \'root_beer\',\n        \'router_(computer_equipment)\', \'rubber_band\', \'runner_(carpet)\',\n        \'plastic_bag\', \'saddle_(on_an_animal)\', \'saddle_blanket\', \'saddlebag\',\n        \'safety_pin\', \'sail\', \'salad\', \'salad_plate\', \'salami\',\n        \'salmon_(fish)\', \'salmon_(food)\', \'salsa\', \'saltshaker\',\n        \'sandal_(type_of_shoe)\', \'sandwich\', \'satchel\', \'saucepan\', \'saucer\',\n        \'sausage\', \'sawhorse\', \'saxophone\', \'scale_(measuring_instrument)\',\n        \'scarecrow\', \'scarf\', \'school_bus\', \'scissors\', \'scoreboard\',\n        \'scrambled_eggs\', \'scraper\', \'scratcher\', \'screwdriver\',\n        \'scrubbing_brush\', \'sculpture\', \'seabird\', \'seahorse\', \'seaplane\',\n        \'seashell\', \'seedling\', \'serving_dish\', \'sewing_machine\', \'shaker\',\n        \'shampoo\', \'shark\', \'sharpener\', \'Sharpie\', \'shaver_(electric)\',\n        \'shaving_cream\', \'shawl\', \'shears\', \'sheep\', \'shepherd_dog\',\n        \'sherbert\', \'shield\', \'shirt\', \'shoe\', \'shopping_bag\', \'shopping_cart\',\n        \'short_pants\', \'shot_glass\', \'shoulder_bag\', \'shovel\', \'shower_head\',\n        \'shower_curtain\', \'shredder_(for_paper)\', \'sieve\', \'signboard\', \'silo\',\n        \'sink\', \'skateboard\', \'skewer\', \'ski\', \'ski_boot\', \'ski_parka\',\n        \'ski_pole\', \'skirt\', \'sled\', \'sleeping_bag\', \'sling_(bandage)\',\n        \'slipper_(footwear)\', \'smoothie\', \'snake\', \'snowboard\', \'snowman\',\n        \'snowmobile\', \'soap\', \'soccer_ball\', \'sock\', \'soda_fountain\',\n        \'carbonated_water\', \'sofa\', \'softball\', \'solar_array\', \'sombrero\',\n        \'soup\', \'soup_bowl\', \'soupspoon\', \'sour_cream\', \'soya_milk\',\n        \'space_shuttle\', \'sparkler_(fireworks)\', \'spatula\', \'spear\',\n        \'spectacles\', \'spice_rack\', \'spider\', \'sponge\', \'spoon\', \'sportswear\',\n        \'spotlight\', \'squirrel\', \'stapler_(stapling_machine)\', \'starfish\',\n        \'statue_(sculpture)\', \'steak_(food)\', \'steak_knife\',\n        \'steamer_(kitchen_appliance)\', \'steering_wheel\', \'stencil\',\n        \'stepladder\', \'step_stool\', \'stereo_(sound_system)\', \'stew\', \'stirrer\',\n        \'stirrup\', \'stockings_(leg_wear)\', \'stool\', \'stop_sign\', \'brake_light\',\n        \'stove\', \'strainer\', \'strap\', \'straw_(for_drinking)\', \'strawberry\',\n        \'street_sign\', \'streetlight\', \'string_cheese\', \'stylus\', \'subwoofer\',\n        \'sugar_bowl\', \'sugarcane_(plant)\', \'suit_(clothing)\', \'sunflower\',\n        \'sunglasses\', \'sunhat\', \'sunscreen\', \'surfboard\', \'sushi\', \'mop\',\n        \'sweat_pants\', \'sweatband\', \'sweater\', \'sweatshirt\', \'sweet_potato\',\n        \'swimsuit\', \'sword\', \'syringe\', \'Tabasco_sauce\', \'table-tennis_table\',\n        \'table\', \'table_lamp\', \'tablecloth\', \'tachometer\', \'taco\', \'tag\',\n        \'taillight\', \'tambourine\', \'army_tank\', \'tank_(storage_vessel)\',\n        \'tank_top_(clothing)\', \'tape_(sticky_cloth_or_paper)\', \'tape_measure\',\n        \'tapestry\', \'tarp\', \'tartan\', \'tassel\', \'tea_bag\', \'teacup\',\n        \'teakettle\', \'teapot\', \'teddy_bear\', \'telephone\', \'telephone_booth\',\n        \'telephone_pole\', \'telephoto_lens\', \'television_camera\',\n        \'television_set\', \'tennis_ball\', \'tennis_racket\', \'tequila\',\n        \'thermometer\', \'thermos_bottle\', \'thermostat\', \'thimble\', \'thread\',\n        \'thumbtack\', \'tiara\', \'tiger\', \'tights_(clothing)\', \'timer\', \'tinfoil\',\n        \'tinsel\', \'tissue_paper\', \'toast_(food)\', \'toaster\', \'toaster_oven\',\n        \'toilet\', \'toilet_tissue\', \'tomato\', \'tongs\', \'toolbox\', \'toothbrush\',\n        \'toothpaste\', \'toothpick\', \'cover\', \'tortilla\', \'tow_truck\', \'towel\',\n        \'towel_rack\', \'toy\', \'tractor_(farm_equipment)\', \'traffic_light\',\n        \'dirt_bike\', \'trailer_truck\', \'train_(railroad_vehicle)\', \'trampoline\',\n        \'tray\', \'tree_house\', \'trench_coat\', \'triangle_(musical_instrument)\',\n        \'tricycle\', \'tripod\', \'trousers\', \'truck\', \'truffle_(chocolate)\',\n        \'trunk\', \'vat\', \'turban\', \'turkey_(bird)\', \'turkey_(food)\', \'turnip\',\n        \'turtle\', \'turtleneck_(clothing)\', \'typewriter\', \'umbrella\',\n        \'underwear\', \'unicycle\', \'urinal\', \'urn\', \'vacuum_cleaner\', \'valve\',\n        \'vase\', \'vending_machine\', \'vent\', \'videotape\', \'vinegar\', \'violin\',\n        \'vodka\', \'volleyball\', \'vulture\', \'waffle\', \'waffle_iron\', \'wagon\',\n        \'wagon_wheel\', \'walking_stick\', \'wall_clock\', \'wall_socket\', \'wallet\',\n        \'walrus\', \'wardrobe\', \'wasabi\', \'automatic_washer\', \'watch\',\n        \'water_bottle\', \'water_cooler\', \'water_faucet\', \'water_filter\',\n        \'water_heater\', \'water_jug\', \'water_gun\', \'water_scooter\', \'water_ski\',\n        \'water_tower\', \'watering_can\', \'watermelon\', \'weathervane\', \'webcam\',\n        \'wedding_cake\', \'wedding_ring\', \'wet_suit\', \'wheel\', \'wheelchair\',\n        \'whipped_cream\', \'whiskey\', \'whistle\', \'wick\', \'wig\', \'wind_chime\',\n        \'windmill\', \'window_box_(for_plants)\', \'windshield_wiper\', \'windsock\',\n        \'wine_bottle\', \'wine_bucket\', \'wineglass\', \'wing_chair\',\n        \'blinder_(for_horses)\', \'wok\', \'wolf\', \'wooden_spoon\', \'wreath\',\n        \'wrench\', \'wristband\', \'wristlet\', \'yacht\', \'yak\', \'yogurt\',\n        \'yoke_(animal_equipment)\', \'zebra\', \'zucchini\')\n\n    def load_annotations(self, ann_file):\n        try:\n            from lvis import LVIS\n        except ImportError:\n            raise ImportError(\'Please follow config/lvis/README.md to \'\n                              \'install open-mmlab forked lvis first.\')\n        self.coco = LVIS(ann_file)\n        assert not self.custom_classes, \'LVIS custom classes is not supported\'\n        self.cat_ids = self.coco.get_cat_ids()\n        self.cat2label = {cat_id: i for i, cat_id in enumerate(self.cat_ids)}\n        self.img_ids = self.coco.get_img_ids()\n        data_infos = []\n        for i in self.img_ids:\n            info = self.coco.load_imgs([i])[0]\n            info[\'filename\'] = info[\'file_name\']\n            data_infos.append(info)\n        return data_infos\n\n    def evaluate(self,\n                 results,\n                 metric=\'bbox\',\n                 logger=None,\n                 jsonfile_prefix=None,\n                 classwise=False,\n                 proposal_nums=(100, 300, 1000),\n                 iou_thrs=np.arange(0.5, 0.96, 0.05)):\n        """"""Evaluation in LVIS protocol.\n        Args:\n            results (list): Testing results of the dataset.\n            metric (str | list[str]): Metrics to be evaluated.\n            logger (logging.Logger | str | None): Logger used for printing\n                related information during evaluation. Default: None.\n            jsonfile_prefix (str | None):\n            classwise (bool): Whether to evaluating the AP for each class.\n            proposal_nums (Sequence[int]): Proposal number used for evaluating\n                recalls, such as recall@100, recall@1000.\n                Default: (100, 300, 1000).\n            iou_thrs (Sequence[float]): IoU threshold used for evaluating\n                recalls. If set to a list, the average recall of all IoUs will\n                also be computed. Default: 0.5.\n        Returns:\n            dict[str: float]\n        """"""\n        try:\n            from lvis import LVISResults, LVISEval\n        except ImportError:\n            raise ImportError(\'Please follow config/lvis/README.md to \'\n                              \'install open-mmlab forked lvis first.\')\n        assert isinstance(results, list), \'results must be a list\'\n        assert len(results) == len(self), (\n            \'The length of results is not equal to the dataset len: {} != {}\'.\n            format(len(results), len(self)))\n\n        metrics = metric if isinstance(metric, list) else [metric]\n        allowed_metrics = [\'bbox\', \'segm\', \'proposal\', \'proposal_fast\']\n        for metric in metrics:\n            if metric not in allowed_metrics:\n                raise KeyError(\'metric {} is not supported\'.format(metric))\n\n        if jsonfile_prefix is None:\n            tmp_dir = tempfile.TemporaryDirectory()\n            jsonfile_prefix = osp.join(tmp_dir.name, \'results\')\n        else:\n            tmp_dir = None\n        result_files = self.results2json(results, jsonfile_prefix)\n\n        eval_results = {}\n        # get original api\n        lvis_gt = self.coco\n        for metric in metrics:\n            msg = \'Evaluating {}...\'.format(metric)\n            if logger is None:\n                msg = \'\\n\' + msg\n            print_log(msg, logger=logger)\n\n            if metric == \'proposal_fast\':\n                ar = self.fast_eval_recall(\n                    results, proposal_nums, iou_thrs, logger=\'silent\')\n                log_msg = []\n                for i, num in enumerate(proposal_nums):\n                    eval_results[\'AR@{}\'.format(num)] = ar[i]\n                    log_msg.append(\'\\nAR@{}\\t{:.4f}\'.format(num, ar[i]))\n                log_msg = \'\'.join(log_msg)\n                print_log(log_msg, logger=logger)\n                continue\n\n            if metric not in result_files:\n                raise KeyError(\'{} is not in results\'.format(metric))\n            try:\n                lvis_dt = LVISResults(lvis_gt, result_files[metric])\n            except IndexError:\n                print_log(\n                    \'The testing results of the whole dataset is empty.\',\n                    logger=logger,\n                    level=logging.ERROR)\n                break\n\n            iou_type = \'bbox\' if metric == \'proposal\' else metric\n            lvis_eval = LVISEval(lvis_gt, lvis_dt, iou_type)\n            lvis_eval.params.imgIds = self.img_ids\n            if metric == \'proposal\':\n                lvis_eval.params.useCats = 0\n                lvis_eval.params.maxDets = list(proposal_nums)\n                lvis_eval.evaluate()\n                lvis_eval.accumulate()\n                lvis_eval.summarize()\n                for k, v in lvis_eval.get_results().items():\n                    if k.startswith(\'AR\'):\n                        val = float(\'{:.3f}\'.format(float(v)))\n                        eval_results[k] = val\n            else:\n                lvis_eval.evaluate()\n                lvis_eval.accumulate()\n                lvis_eval.summarize()\n                lvis_results = lvis_eval.get_results()\n                if classwise:  # Compute per-category AP\n                    # Compute per-category AP\n                    # from https://github.com/facebookresearch/detectron2/\n                    precisions = lvis_eval.eval[\'precision\']\n                    # precision: (iou, recall, cls, area range, max dets)\n                    assert len(self.cat_ids) == precisions.shape[2]\n\n                    results_per_category = []\n                    for idx, catId in enumerate(self.cat_ids):\n                        # area range index 0: all area ranges\n                        # max dets index -1: typically 100 per image\n                        nm = self.coco.load_cats(catId)[0]\n                        precision = precisions[:, :, idx, 0, -1]\n                        precision = precision[precision > -1]\n                        if precision.size:\n                            ap = np.mean(precision)\n                        else:\n                            ap = float(\'nan\')\n                        results_per_category.append(\n                            (f\'{nm[""name""]}\', f\'{float(ap):0.3f}\'))\n\n                    num_columns = min(6, len(results_per_category) * 2)\n                    results_flatten = list(\n                        itertools.chain(*results_per_category))\n                    headers = [\'category\', \'AP\'] * (num_columns // 2)\n                    results_2d = itertools.zip_longest(*[\n                        results_flatten[i::num_columns]\n                        for i in range(num_columns)\n                    ])\n                    table_data = [headers]\n                    table_data += [result for result in results_2d]\n                    table = AsciiTable(table_data)\n                    print_log(\'\\n\' + table.table, logger=logger)\n\n                for k, v in lvis_results.items():\n                    if k.startswith(\'AP\'):\n                        key = \'{}_{}\'.format(metric, k)\n                        val = float(\'{:.3f}\'.format(float(v)))\n                        eval_results[key] = val\n                ap_summary = \' \'.join([\n                    \'{}:{:.3f}\'.format(k, float(v))\n                    for k, v in lvis_results.items() if k.startswith(\'AP\')\n                ])\n                eval_results[\'{}_mAP_copypaste\'.format(metric)] = ap_summary\n            lvis_eval.print_results()\n        if tmp_dir is not None:\n            tmp_dir.cleanup()\n        return eval_results\n'"
mmdet/datasets/voc.py,0,"b""from mmdet.core import eval_map, eval_recalls\nfrom .builder import DATASETS\nfrom .xml_style import XMLDataset\n\n\n@DATASETS.register_module()\nclass VOCDataset(XMLDataset):\n\n    CLASSES = ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car',\n               'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse',\n               'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train',\n               'tvmonitor')\n\n    def __init__(self, **kwargs):\n        super(VOCDataset, self).__init__(**kwargs)\n        if 'VOC2007' in self.img_prefix:\n            self.year = 2007\n        elif 'VOC2012' in self.img_prefix:\n            self.year = 2012\n        else:\n            raise ValueError('Cannot infer dataset year from img_prefix')\n\n    def evaluate(self,\n                 results,\n                 metric='mAP',\n                 logger=None,\n                 proposal_nums=(100, 300, 1000),\n                 iou_thr=0.5,\n                 scale_ranges=None):\n        if not isinstance(metric, str):\n            assert len(metric) == 1\n            metric = metric[0]\n        allowed_metrics = ['mAP', 'recall']\n        if metric not in allowed_metrics:\n            raise KeyError(f'metric {metric} is not supported')\n        annotations = [self.get_ann_info(i) for i in range(len(self))]\n        eval_results = {}\n        if metric == 'mAP':\n            assert isinstance(iou_thr, float)\n            if self.year == 2007:\n                ds_name = 'voc07'\n            else:\n                ds_name = self.dataset.CLASSES\n            mean_ap, _ = eval_map(\n                results,\n                annotations,\n                scale_ranges=None,\n                iou_thr=iou_thr,\n                dataset=ds_name,\n                logger=logger)\n            eval_results['mAP'] = mean_ap\n        elif metric == 'recall':\n            gt_bboxes = [ann['bboxes'] for ann in annotations]\n            if isinstance(iou_thr, float):\n                iou_thr = [iou_thr]\n            recalls = eval_recalls(\n                gt_bboxes, results, proposal_nums, iou_thr, logger=logger)\n            for i, num in enumerate(proposal_nums):\n                for j, iou in enumerate(iou_thr):\n                    eval_results[f'recall@{num}@{iou}'] = recalls[i, j]\n            if recalls.shape[1] > 1:\n                ar = recalls.mean(axis=1)\n                for i, num in enumerate(proposal_nums):\n                    eval_results[f'AR@{num}'] = ar[i]\n        return eval_results\n"""
mmdet/datasets/wider_face.py,0,"b'import os.path as osp\nimport xml.etree.ElementTree as ET\n\nimport mmcv\n\nfrom .builder import DATASETS\nfrom .xml_style import XMLDataset\n\n\n@DATASETS.register_module()\nclass WIDERFaceDataset(XMLDataset):\n    """"""\n    Reader for the WIDER Face dataset in PASCAL VOC format.\n    Conversion scripts can be found in\n    https://github.com/sovrasov/wider-face-pascal-voc-annotations\n    """"""\n    CLASSES = (\'face\', )\n\n    def __init__(self, **kwargs):\n        super(WIDERFaceDataset, self).__init__(**kwargs)\n\n    def load_annotations(self, ann_file):\n        data_infos = []\n        img_ids = mmcv.list_from_file(ann_file)\n        for img_id in img_ids:\n            filename = f\'{img_id}.jpg\'\n            xml_path = osp.join(self.img_prefix, \'Annotations\',\n                                f\'{img_id}.xml\')\n            tree = ET.parse(xml_path)\n            root = tree.getroot()\n            size = root.find(\'size\')\n            width = int(size.find(\'width\').text)\n            height = int(size.find(\'height\').text)\n            folder = root.find(\'folder\').text\n            data_infos.append(\n                dict(\n                    id=img_id,\n                    filename=osp.join(folder, filename),\n                    width=width,\n                    height=height))\n\n        return data_infos\n'"
mmdet/datasets/xml_style.py,0,"b'import os.path as osp\nimport xml.etree.ElementTree as ET\n\nimport mmcv\nimport numpy as np\nfrom PIL import Image\n\nfrom .builder import DATASETS\nfrom .custom import CustomDataset\n\n\n@DATASETS.register_module()\nclass XMLDataset(CustomDataset):\n\n    def __init__(self, min_size=None, **kwargs):\n        super(XMLDataset, self).__init__(**kwargs)\n        self.cat2label = {cat: i for i, cat in enumerate(self.CLASSES)}\n        self.min_size = min_size\n\n    def load_annotations(self, ann_file):\n        data_infos = []\n        img_ids = mmcv.list_from_file(ann_file)\n        for img_id in img_ids:\n            filename = f\'JPEGImages/{img_id}.jpg\'\n            xml_path = osp.join(self.img_prefix, \'Annotations\',\n                                f\'{img_id}.xml\')\n            tree = ET.parse(xml_path)\n            root = tree.getroot()\n            size = root.find(\'size\')\n            width = 0\n            height = 0\n            if size is not None:\n                width = int(size.find(\'width\').text)\n                height = int(size.find(\'height\').text)\n            else:\n                img_path = osp.join(self.img_prefix, \'JPEGImages\',\n                                    \'{}.jpg\'.format(img_id))\n                img = Image.open(img_path)\n                width, height = img.size\n            data_infos.append(\n                dict(id=img_id, filename=filename, width=width, height=height))\n\n        return data_infos\n\n    def get_subset_by_classes(self):\n        """"""Filter imgs by user-defined categories\n        """"""\n        subset_data_infos = []\n        for data_info in self.data_infos:\n            img_id = data_info[\'id\']\n            xml_path = osp.join(self.img_prefix, \'Annotations\',\n                                f\'{img_id}.xml\')\n            tree = ET.parse(xml_path)\n            root = tree.getroot()\n            for obj in root.findall(\'object\'):\n                name = obj.find(\'name\').text\n                if name in self.CLASSES:\n                    subset_data_infos.append(data_info)\n                    break\n\n        return subset_data_infos\n\n    def get_ann_info(self, idx):\n        img_id = self.data_infos[idx][\'id\']\n        xml_path = osp.join(self.img_prefix, \'Annotations\', f\'{img_id}.xml\')\n        tree = ET.parse(xml_path)\n        root = tree.getroot()\n        bboxes = []\n        labels = []\n        bboxes_ignore = []\n        labels_ignore = []\n        for obj in root.findall(\'object\'):\n            name = obj.find(\'name\').text\n            if name not in self.CLASSES:\n                continue\n            label = self.cat2label[name]\n            difficult = int(obj.find(\'difficult\').text)\n            bnd_box = obj.find(\'bndbox\')\n            # TODO: check whether it is necessary to use int\n            # Coordinates may be float type\n            bbox = [\n                int(float(bnd_box.find(\'xmin\').text)),\n                int(float(bnd_box.find(\'ymin\').text)),\n                int(float(bnd_box.find(\'xmax\').text)),\n                int(float(bnd_box.find(\'ymax\').text))\n            ]\n            ignore = False\n            if self.min_size:\n                assert not self.test_mode\n                w = bbox[2] - bbox[0]\n                h = bbox[3] - bbox[1]\n                if w < self.min_size or h < self.min_size:\n                    ignore = True\n            if difficult or ignore:\n                bboxes_ignore.append(bbox)\n                labels_ignore.append(label)\n            else:\n                bboxes.append(bbox)\n                labels.append(label)\n        if not bboxes:\n            bboxes = np.zeros((0, 4))\n            labels = np.zeros((0, ))\n        else:\n            bboxes = np.array(bboxes, ndmin=2) - 1\n            labels = np.array(labels)\n        if not bboxes_ignore:\n            bboxes_ignore = np.zeros((0, 4))\n            labels_ignore = np.zeros((0, ))\n        else:\n            bboxes_ignore = np.array(bboxes_ignore, ndmin=2) - 1\n            labels_ignore = np.array(labels_ignore)\n        ann = dict(\n            bboxes=bboxes.astype(np.float32),\n            labels=labels.astype(np.int64),\n            bboxes_ignore=bboxes_ignore.astype(np.float32),\n            labels_ignore=labels_ignore.astype(np.int64))\n        return ann\n\n    def get_cat_ids(self, idx):\n        cat_ids = []\n        img_id = self.data_infos[idx][\'id\']\n        xml_path = osp.join(self.img_prefix, \'Annotations\', f\'{img_id}.xml\')\n        tree = ET.parse(xml_path)\n        root = tree.getroot()\n        for obj in root.findall(\'object\'):\n            name = obj.find(\'name\').text\n            if name not in self.CLASSES:\n                continue\n            label = self.cat2label[name]\n            cat_ids.append(label)\n\n        return cat_ids\n'"
mmdet/models/__init__.py,0,"b""from .backbones import *  # noqa: F401,F403\nfrom .builder import (BACKBONES, DETECTORS, HEADS, LOSSES, NECKS,\n                      ROI_EXTRACTORS, SHARED_HEADS, build_backbone,\n                      build_detector, build_head, build_loss, build_neck,\n                      build_roi_extractor, build_shared_head)\nfrom .dense_heads import *  # noqa: F401,F403\nfrom .detectors import *  # noqa: F401,F403\nfrom .losses import *  # noqa: F401,F403\nfrom .necks import *  # noqa: F401,F403\nfrom .roi_heads import *  # noqa: F401,F403\n\n__all__ = [\n    'BACKBONES', 'NECKS', 'ROI_EXTRACTORS', 'SHARED_HEADS', 'HEADS', 'LOSSES',\n    'DETECTORS', 'build_backbone', 'build_neck', 'build_roi_extractor',\n    'build_shared_head', 'build_head', 'build_loss', 'build_detector'\n]\n"""
mmdet/models/builder.py,0,"b""from mmcv.utils import Registry, build_from_cfg\nfrom torch import nn\n\nBACKBONES = Registry('backbone')\nNECKS = Registry('neck')\nROI_EXTRACTORS = Registry('roi_extractor')\nSHARED_HEADS = Registry('shared_head')\nHEADS = Registry('head')\nLOSSES = Registry('loss')\nDETECTORS = Registry('detector')\n\n\ndef build(cfg, registry, default_args=None):\n    if isinstance(cfg, list):\n        modules = [\n            build_from_cfg(cfg_, registry, default_args) for cfg_ in cfg\n        ]\n        return nn.Sequential(*modules)\n    else:\n        return build_from_cfg(cfg, registry, default_args)\n\n\ndef build_backbone(cfg):\n    return build(cfg, BACKBONES)\n\n\ndef build_neck(cfg):\n    return build(cfg, NECKS)\n\n\ndef build_roi_extractor(cfg):\n    return build(cfg, ROI_EXTRACTORS)\n\n\ndef build_shared_head(cfg):\n    return build(cfg, SHARED_HEADS)\n\n\ndef build_head(cfg):\n    return build(cfg, HEADS)\n\n\ndef build_loss(cfg):\n    return build(cfg, LOSSES)\n\n\ndef build_detector(cfg, train_cfg=None, test_cfg=None):\n    return build(cfg, DETECTORS, dict(train_cfg=train_cfg, test_cfg=test_cfg))\n"""
mmdet/ops/__init__.py,0,"b""from .context_block import ContextBlock\nfrom .conv_ws import ConvWS2d, conv_ws_2d\nfrom .dcn import (DeformConv, DeformConvPack, DeformRoIPooling,\n                  DeformRoIPoolingPack, ModulatedDeformConv,\n                  ModulatedDeformConvPack, ModulatedDeformRoIPoolingPack,\n                  deform_conv, deform_roi_pooling, modulated_deform_conv)\nfrom .generalized_attention import GeneralizedAttention\nfrom .masked_conv import MaskedConv2d\nfrom .nms import batched_nms, nms, nms_match, soft_nms\nfrom .non_local import NonLocal2D\nfrom .plugin import build_plugin_layer\nfrom .roi_align import RoIAlign, roi_align\nfrom .roi_pool import RoIPool, roi_pool\nfrom .sigmoid_focal_loss import SigmoidFocalLoss, sigmoid_focal_loss\nfrom .utils import get_compiler_version, get_compiling_cuda_version\nfrom .wrappers import Conv2d, ConvTranspose2d, Linear, MaxPool2d\n\n__all__ = [\n    'nms', 'soft_nms', 'RoIAlign', 'roi_align', 'RoIPool', 'roi_pool',\n    'DeformConv', 'DeformConvPack', 'DeformRoIPooling', 'DeformRoIPoolingPack',\n    'ModulatedDeformRoIPoolingPack', 'ModulatedDeformConv',\n    'ModulatedDeformConvPack', 'deform_conv', 'modulated_deform_conv',\n    'deform_roi_pooling', 'SigmoidFocalLoss', 'sigmoid_focal_loss',\n    'MaskedConv2d', 'ContextBlock', 'GeneralizedAttention', 'NonLocal2D',\n    'get_compiler_version', 'get_compiling_cuda_version', 'ConvWS2d',\n    'conv_ws_2d', 'build_plugin_layer', 'batched_nms', 'Conv2d',\n    'ConvTranspose2d', 'MaxPool2d', 'Linear', 'nms_match'\n]\n"""
mmdet/ops/context_block.py,2,"b'import torch\nfrom mmcv.cnn import constant_init, kaiming_init\nfrom torch import nn\n\n\ndef last_zero_init(m):\n    if isinstance(m, nn.Sequential):\n        constant_init(m[-1], val=0)\n    else:\n        constant_init(m, val=0)\n\n\nclass ContextBlock(nn.Module):\n    """"""ContextBlock module in GCNet.\n\n    See \'GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond\'\n    (https://arxiv.org/abs/1904.11492) for details.\n\n    Args:\n        in_channels (int): Channels of the input feature map.\n        ratio (float): Ratio of channels of transform bottleneck\n        pooling_type (str): Pooling method for context modeling\n        fusion_types (list[str]|tuple[str]): Fusion method for feature fusion,\n            options: \'channels_add\', \'channel_mul\'\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 ratio,\n                 pooling_type=\'att\',\n                 fusion_types=(\'channel_add\', )):\n        super(ContextBlock, self).__init__()\n        assert pooling_type in [\'avg\', \'att\']\n        assert isinstance(fusion_types, (list, tuple))\n        valid_fusion_types = [\'channel_add\', \'channel_mul\']\n        assert all([f in valid_fusion_types for f in fusion_types])\n        assert len(fusion_types) > 0, \'at least one fusion should be used\'\n        self.in_channels = in_channels\n        self.ratio = ratio\n        self.planes = int(in_channels * ratio)\n        self.pooling_type = pooling_type\n        self.fusion_types = fusion_types\n        if pooling_type == \'att\':\n            self.conv_mask = nn.Conv2d(in_channels, 1, kernel_size=1)\n            self.softmax = nn.Softmax(dim=2)\n        else:\n            self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        if \'channel_add\' in fusion_types:\n            self.channel_add_conv = nn.Sequential(\n                nn.Conv2d(self.in_channels, self.planes, kernel_size=1),\n                nn.LayerNorm([self.planes, 1, 1]),\n                nn.ReLU(inplace=True),  # yapf: disable\n                nn.Conv2d(self.planes, self.in_channels, kernel_size=1))\n        else:\n            self.channel_add_conv = None\n        if \'channel_mul\' in fusion_types:\n            self.channel_mul_conv = nn.Sequential(\n                nn.Conv2d(self.in_channels, self.planes, kernel_size=1),\n                nn.LayerNorm([self.planes, 1, 1]),\n                nn.ReLU(inplace=True),  # yapf: disable\n                nn.Conv2d(self.planes, self.in_channels, kernel_size=1))\n        else:\n            self.channel_mul_conv = None\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        if self.pooling_type == \'att\':\n            kaiming_init(self.conv_mask, mode=\'fan_in\')\n            self.conv_mask.inited = True\n\n        if self.channel_add_conv is not None:\n            last_zero_init(self.channel_add_conv)\n        if self.channel_mul_conv is not None:\n            last_zero_init(self.channel_mul_conv)\n\n    def spatial_pool(self, x):\n        batch, channel, height, width = x.size()\n        if self.pooling_type == \'att\':\n            input_x = x\n            # [N, C, H * W]\n            input_x = input_x.view(batch, channel, height * width)\n            # [N, 1, C, H * W]\n            input_x = input_x.unsqueeze(1)\n            # [N, 1, H, W]\n            context_mask = self.conv_mask(x)\n            # [N, 1, H * W]\n            context_mask = context_mask.view(batch, 1, height * width)\n            # [N, 1, H * W]\n            context_mask = self.softmax(context_mask)\n            # [N, 1, H * W, 1]\n            context_mask = context_mask.unsqueeze(-1)\n            # [N, 1, C, 1]\n            context = torch.matmul(input_x, context_mask)\n            # [N, C, 1, 1]\n            context = context.view(batch, channel, 1, 1)\n        else:\n            # [N, C, 1, 1]\n            context = self.avg_pool(x)\n\n        return context\n\n    def forward(self, x):\n        # [N, C, 1, 1]\n        context = self.spatial_pool(x)\n\n        out = x\n        if self.channel_mul_conv is not None:\n            # [N, C, 1, 1]\n            channel_mul_term = torch.sigmoid(self.channel_mul_conv(context))\n            out = out * channel_mul_term\n        if self.channel_add_conv is not None:\n            # [N, C, 1, 1]\n            channel_add_term = self.channel_add_conv(context)\n            out = out + channel_add_term\n\n        return out\n'"
mmdet/ops/conv_ws.py,2,"b""import torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import CONV_LAYERS\n\n\ndef conv_ws_2d(input,\n               weight,\n               bias=None,\n               stride=1,\n               padding=0,\n               dilation=1,\n               groups=1,\n               eps=1e-5):\n    c_in = weight.size(0)\n    weight_flat = weight.view(c_in, -1)\n    mean = weight_flat.mean(dim=1, keepdim=True).view(c_in, 1, 1, 1)\n    std = weight_flat.std(dim=1, keepdim=True).view(c_in, 1, 1, 1)\n    weight = (weight - mean) / (std + eps)\n    return F.conv2d(input, weight, bias, stride, padding, dilation, groups)\n\n\n@CONV_LAYERS.register_module('ConvWS')\nclass ConvWS2d(nn.Conv2d):\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 groups=1,\n                 bias=True,\n                 eps=1e-5):\n        super(ConvWS2d, self).__init__(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            groups=groups,\n            bias=bias)\n        self.eps = eps\n\n    def forward(self, x):\n        return conv_ws_2d(x, self.weight, self.bias, self.stride, self.padding,\n                          self.dilation, self.groups, self.eps)\n"""
mmdet/ops/generalized_attention.py,26,"b'import math\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import kaiming_init\n\n\nclass GeneralizedAttention(nn.Module):\n    """"""GeneralizedAttention module.\n\n    See \'An Empirical Study of Spatial Attention Mechanisms in Deep Networks\'\n    (https://arxiv.org/abs/1711.07971) for details.\n\n    Args:\n        in_channels (int): Channels of the input feature map.\n        spatial_range (int): The spatial range.\n            -1 indicates no spatial range constraint.\n        num_heads (int): The head number of empirical_attention module.\n        position_embedding_dim (int): The position embedding dimension.\n        position_magnitude (int): A multiplier acting on coord difference.\n        kv_stride (int): The feature stride acting on key/value feature map.\n        q_stride (int): The feature stride acting on query feature map.\n        attention_type (str): A binary indicator string for indicating which\n            items in generalized empirical_attention module are used.\n            \'1000\' indicates \'query and key content\' (appr - appr) item,\n            \'0100\' indicates \'query content and relative position\'\n              (appr - position) item,\n            \'0010\' indicates \'key content only\' (bias - appr) item,\n            \'0001\' indicates \'relative position only\' (bias - position) item.\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 spatial_range=-1,\n                 num_heads=9,\n                 position_embedding_dim=-1,\n                 position_magnitude=1,\n                 kv_stride=2,\n                 q_stride=1,\n                 attention_type=\'1111\'):\n\n        super(GeneralizedAttention, self).__init__()\n\n        # hard range means local range for non-local operation\n        self.position_embedding_dim = (\n            position_embedding_dim\n            if position_embedding_dim > 0 else in_channels)\n\n        self.position_magnitude = position_magnitude\n        self.num_heads = num_heads\n        self.in_channels = in_channels\n        self.spatial_range = spatial_range\n        self.kv_stride = kv_stride\n        self.q_stride = q_stride\n        self.attention_type = [bool(int(_)) for _ in attention_type]\n        self.qk_embed_dim = in_channels // num_heads\n        out_c = self.qk_embed_dim * num_heads\n\n        if self.attention_type[0] or self.attention_type[1]:\n            self.query_conv = nn.Conv2d(\n                in_channels=in_channels,\n                out_channels=out_c,\n                kernel_size=1,\n                bias=False)\n            self.query_conv.kaiming_init = True\n\n        if self.attention_type[0] or self.attention_type[2]:\n            self.key_conv = nn.Conv2d(\n                in_channels=in_channels,\n                out_channels=out_c,\n                kernel_size=1,\n                bias=False)\n            self.key_conv.kaiming_init = True\n\n        self.v_dim = in_channels // num_heads\n        self.value_conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=self.v_dim * num_heads,\n            kernel_size=1,\n            bias=False)\n        self.value_conv.kaiming_init = True\n\n        if self.attention_type[1] or self.attention_type[3]:\n            self.appr_geom_fc_x = nn.Linear(\n                self.position_embedding_dim // 2, out_c, bias=False)\n            self.appr_geom_fc_x.kaiming_init = True\n\n            self.appr_geom_fc_y = nn.Linear(\n                self.position_embedding_dim // 2, out_c, bias=False)\n            self.appr_geom_fc_y.kaiming_init = True\n\n        if self.attention_type[2]:\n            stdv = 1.0 / math.sqrt(self.qk_embed_dim * 2)\n            appr_bias_value = -2 * stdv * torch.rand(out_c) + stdv\n            self.appr_bias = nn.Parameter(appr_bias_value)\n\n        if self.attention_type[3]:\n            stdv = 1.0 / math.sqrt(self.qk_embed_dim * 2)\n            geom_bias_value = -2 * stdv * torch.rand(out_c) + stdv\n            self.geom_bias = nn.Parameter(geom_bias_value)\n\n        self.proj_conv = nn.Conv2d(\n            in_channels=self.v_dim * num_heads,\n            out_channels=in_channels,\n            kernel_size=1,\n            bias=True)\n        self.proj_conv.kaiming_init = True\n        self.gamma = nn.Parameter(torch.zeros(1))\n\n        if self.spatial_range >= 0:\n            # only works when non local is after 3*3 conv\n            if in_channels == 256:\n                max_len = 84\n            elif in_channels == 512:\n                max_len = 42\n\n            max_len_kv = int((max_len - 1.0) / self.kv_stride + 1)\n            local_constraint_map = np.ones(\n                (max_len, max_len, max_len_kv, max_len_kv), dtype=np.int)\n            for iy in range(max_len):\n                for ix in range(max_len):\n                    local_constraint_map[\n                        iy, ix,\n                        max((iy - self.spatial_range) //\n                            self.kv_stride, 0):min((iy + self.spatial_range +\n                                                    1) // self.kv_stride +\n                                                   1, max_len),\n                        max((ix - self.spatial_range) //\n                            self.kv_stride, 0):min((ix + self.spatial_range +\n                                                    1) // self.kv_stride +\n                                                   1, max_len)] = 0\n\n            self.local_constraint_map = nn.Parameter(\n                torch.from_numpy(local_constraint_map).byte(),\n                requires_grad=False)\n\n        if self.q_stride > 1:\n            self.q_downsample = nn.AvgPool2d(\n                kernel_size=1, stride=self.q_stride)\n        else:\n            self.q_downsample = None\n\n        if self.kv_stride > 1:\n            self.kv_downsample = nn.AvgPool2d(\n                kernel_size=1, stride=self.kv_stride)\n        else:\n            self.kv_downsample = None\n\n        self.init_weights()\n\n    def get_position_embedding(self,\n                               h,\n                               w,\n                               h_kv,\n                               w_kv,\n                               q_stride,\n                               kv_stride,\n                               device,\n                               feat_dim,\n                               wave_length=1000):\n        h_idxs = torch.linspace(0, h - 1, h).cuda(device)\n        h_idxs = h_idxs.view((h, 1)) * q_stride\n\n        w_idxs = torch.linspace(0, w - 1, w).cuda(device)\n        w_idxs = w_idxs.view((w, 1)) * q_stride\n\n        h_kv_idxs = torch.linspace(0, h_kv - 1, h_kv).cuda(device)\n        h_kv_idxs = h_kv_idxs.view((h_kv, 1)) * kv_stride\n\n        w_kv_idxs = torch.linspace(0, w_kv - 1, w_kv).cuda(device)\n        w_kv_idxs = w_kv_idxs.view((w_kv, 1)) * kv_stride\n\n        # (h, h_kv, 1)\n        h_diff = h_idxs.unsqueeze(1) - h_kv_idxs.unsqueeze(0)\n        h_diff *= self.position_magnitude\n\n        # (w, w_kv, 1)\n        w_diff = w_idxs.unsqueeze(1) - w_kv_idxs.unsqueeze(0)\n        w_diff *= self.position_magnitude\n\n        feat_range = torch.arange(0, feat_dim / 4).cuda(device)\n\n        dim_mat = torch.Tensor([wave_length]).cuda(device)\n        dim_mat = dim_mat**((4. / feat_dim) * feat_range)\n        dim_mat = dim_mat.view((1, 1, -1))\n\n        embedding_x = torch.cat(\n            ((w_diff / dim_mat).sin(), (w_diff / dim_mat).cos()), dim=2)\n\n        embedding_y = torch.cat(\n            ((h_diff / dim_mat).sin(), (h_diff / dim_mat).cos()), dim=2)\n\n        return embedding_x, embedding_y\n\n    def forward(self, x_input):\n        num_heads = self.num_heads\n\n        # use empirical_attention\n        if self.q_downsample is not None:\n            x_q = self.q_downsample(x_input)\n        else:\n            x_q = x_input\n        n, _, h, w = x_q.shape\n\n        if self.kv_downsample is not None:\n            x_kv = self.kv_downsample(x_input)\n        else:\n            x_kv = x_input\n        _, _, h_kv, w_kv = x_kv.shape\n\n        if self.attention_type[0] or self.attention_type[1]:\n            proj_query = self.query_conv(x_q).view(\n                (n, num_heads, self.qk_embed_dim, h * w))\n            proj_query = proj_query.permute(0, 1, 3, 2)\n\n        if self.attention_type[0] or self.attention_type[2]:\n            proj_key = self.key_conv(x_kv).view(\n                (n, num_heads, self.qk_embed_dim, h_kv * w_kv))\n\n        if self.attention_type[1] or self.attention_type[3]:\n            position_embed_x, position_embed_y = self.get_position_embedding(\n                h, w, h_kv, w_kv, self.q_stride, self.kv_stride,\n                x_input.device, self.position_embedding_dim)\n            # (n, num_heads, w, w_kv, dim)\n            position_feat_x = self.appr_geom_fc_x(position_embed_x).\\\n                view(1, w, w_kv, num_heads, self.qk_embed_dim).\\\n                permute(0, 3, 1, 2, 4).\\\n                repeat(n, 1, 1, 1, 1)\n\n            # (n, num_heads, h, h_kv, dim)\n            position_feat_y = self.appr_geom_fc_y(position_embed_y).\\\n                view(1, h, h_kv, num_heads, self.qk_embed_dim).\\\n                permute(0, 3, 1, 2, 4).\\\n                repeat(n, 1, 1, 1, 1)\n\n            position_feat_x /= math.sqrt(2)\n            position_feat_y /= math.sqrt(2)\n\n        # accelerate for saliency only\n        if (np.sum(self.attention_type) == 1) and self.attention_type[2]:\n            appr_bias = self.appr_bias.\\\n                view(1, num_heads, 1, self.qk_embed_dim).\\\n                repeat(n, 1, 1, 1)\n\n            energy = torch.matmul(appr_bias, proj_key).\\\n                view(n, num_heads, 1, h_kv * w_kv)\n\n            h = 1\n            w = 1\n        else:\n            # (n, num_heads, h*w, h_kv*w_kv), query before key, 540mb for\n            if not self.attention_type[0]:\n                energy = torch.zeros(\n                    n,\n                    num_heads,\n                    h,\n                    w,\n                    h_kv,\n                    w_kv,\n                    dtype=x_input.dtype,\n                    device=x_input.device)\n\n            # attention_type[0]: appr - appr\n            # attention_type[1]: appr - position\n            # attention_type[2]: bias - appr\n            # attention_type[3]: bias - position\n            if self.attention_type[0] or self.attention_type[2]:\n                if self.attention_type[0] and self.attention_type[2]:\n                    appr_bias = self.appr_bias.\\\n                        view(1, num_heads, 1, self.qk_embed_dim)\n                    energy = torch.matmul(proj_query + appr_bias, proj_key).\\\n                        view(n, num_heads, h, w, h_kv, w_kv)\n\n                elif self.attention_type[0]:\n                    energy = torch.matmul(proj_query, proj_key).\\\n                        view(n, num_heads, h, w, h_kv, w_kv)\n\n                elif self.attention_type[2]:\n                    appr_bias = self.appr_bias.\\\n                        view(1, num_heads, 1, self.qk_embed_dim).\\\n                        repeat(n, 1, 1, 1)\n\n                    energy += torch.matmul(appr_bias, proj_key).\\\n                        view(n, num_heads, 1, 1, h_kv, w_kv)\n\n            if self.attention_type[1] or self.attention_type[3]:\n                if self.attention_type[1] and self.attention_type[3]:\n                    geom_bias = self.geom_bias.\\\n                        view(1, num_heads, 1, self.qk_embed_dim)\n\n                    proj_query_reshape = (proj_query + geom_bias).\\\n                        view(n, num_heads, h, w, self.qk_embed_dim)\n\n                    energy_x = torch.matmul(\n                        proj_query_reshape.permute(0, 1, 3, 2, 4),\n                        position_feat_x.permute(0, 1, 2, 4, 3))\n                    energy_x = energy_x.\\\n                        permute(0, 1, 3, 2, 4).unsqueeze(4)\n\n                    energy_y = torch.matmul(\n                        proj_query_reshape,\n                        position_feat_y.permute(0, 1, 2, 4, 3))\n                    energy_y = energy_y.unsqueeze(5)\n\n                    energy += energy_x + energy_y\n\n                elif self.attention_type[1]:\n                    proj_query_reshape = proj_query.\\\n                        view(n, num_heads, h, w, self.qk_embed_dim)\n                    proj_query_reshape = proj_query_reshape.\\\n                        permute(0, 1, 3, 2, 4)\n                    position_feat_x_reshape = position_feat_x.\\\n                        permute(0, 1, 2, 4, 3)\n                    position_feat_y_reshape = position_feat_y.\\\n                        permute(0, 1, 2, 4, 3)\n\n                    energy_x = torch.matmul(proj_query_reshape,\n                                            position_feat_x_reshape)\n                    energy_x = energy_x.permute(0, 1, 3, 2, 4).unsqueeze(4)\n\n                    energy_y = torch.matmul(proj_query_reshape,\n                                            position_feat_y_reshape)\n                    energy_y = energy_y.unsqueeze(5)\n\n                    energy += energy_x + energy_y\n\n                elif self.attention_type[3]:\n                    geom_bias = self.geom_bias.\\\n                        view(1, num_heads, self.qk_embed_dim, 1).\\\n                        repeat(n, 1, 1, 1)\n\n                    position_feat_x_reshape = position_feat_x.\\\n                        view(n, num_heads, w*w_kv, self.qk_embed_dim)\n\n                    position_feat_y_reshape = position_feat_y.\\\n                        view(n, num_heads, h * h_kv, self.qk_embed_dim)\n\n                    energy_x = torch.matmul(position_feat_x_reshape, geom_bias)\n                    energy_x = energy_x.view(n, num_heads, 1, w, 1, w_kv)\n\n                    energy_y = torch.matmul(position_feat_y_reshape, geom_bias)\n                    energy_y = energy_y.view(n, num_heads, h, 1, h_kv, 1)\n\n                    energy += energy_x + energy_y\n\n            energy = energy.view(n, num_heads, h * w, h_kv * w_kv)\n\n        if self.spatial_range >= 0:\n            cur_local_constraint_map = \\\n                self.local_constraint_map[:h, :w, :h_kv, :w_kv].\\\n                contiguous().\\\n                view(1, 1, h*w, h_kv*w_kv)\n\n            energy = energy.masked_fill_(cur_local_constraint_map,\n                                         float(\'-inf\'))\n\n        attention = F.softmax(energy, 3)\n\n        proj_value = self.value_conv(x_kv)\n        proj_value_reshape = proj_value.\\\n            view((n, num_heads, self.v_dim, h_kv * w_kv)).\\\n            permute(0, 1, 3, 2)\n\n        out = torch.matmul(attention, proj_value_reshape).\\\n            permute(0, 1, 3, 2).\\\n            contiguous().\\\n            view(n, self.v_dim * self.num_heads, h, w)\n\n        out = self.proj_conv(out)\n        out = self.gamma * out + x_input\n        return out\n\n    def init_weights(self):\n        for m in self.modules():\n            if hasattr(m, \'kaiming_init\') and m.kaiming_init:\n                kaiming_init(\n                    m,\n                    mode=\'fan_in\',\n                    nonlinearity=\'leaky_relu\',\n                    bias=0,\n                    distribution=\'uniform\',\n                    a=1)\n'"
mmdet/ops/merge_cells.py,3,"b'from abc import abstractmethod\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import ConvModule\n\n\nclass BaseMergeCell(nn.Module):\n    """"""The basic class for cells used in NAS-FPN and NAS-FCOS.\n\n    BaseMergeCell takes 2 inputs. After applying concolution\n    on them, they are resized to the target size. Then,\n    they go through binary_op, which depends on the type of cell.\n    If with_out_conv is True, the result of output will go through\n    another convolution layer.\n\n    Args:\n        in_channels (int): number of input channels in out_conv layer.\n        out_channels (int): number of output channels in out_conv layer.\n        with_out_conv (bool): Whether to use out_conv layer\n        out_conv_cfg (dict): Config dict for convolution layer, which should\n            contain ""groups"", ""kernel_size"", ""padding"", ""bias"" to build\n            out_conv layer.\n        out_norm_cfg (dict): Config dict for normalization layer in out_conv.\n        out_conv_order (tuple): The order of conv/norm/activation layers in\n            out_conv.\n        with_input1_conv (bool): Whether to use convolution on input1.\n        with_input2_conv (bool): Whether to use convolution on input2.\n        input_conv_cfg (dict): Config dict for building input1_conv layer and\n            input2_conv layer, which is expected to contain the type of\n            convolution.\n            Default: None, which means using conv2d.\n        input_norm_cfg (dict): Config dict for normalization layer in\n            input1_conv and input2_conv layer. Default: None.\n        upsample_mode (str): Interpolation method used to resize the output\n            of input1_conv and input2_conv to target size. Currently, we\n            support [\'nearest\', \'bilinear\']. Default: \'nearest\'.\n\n    """"""\n\n    def __init__(self,\n                 fused_channels=256,\n                 out_channels=256,\n                 with_out_conv=True,\n                 out_conv_cfg=dict(\n                     groups=1, kernel_size=3, padding=1, bias=True),\n                 out_norm_cfg=None,\n                 out_conv_order=(\'act\', \'conv\', \'norm\'),\n                 with_input1_conv=False,\n                 with_input2_conv=False,\n                 input_conv_cfg=None,\n                 input_norm_cfg=None,\n                 upsample_mode=\'nearest\'):\n        super(BaseMergeCell, self).__init__()\n        assert upsample_mode in [\'nearest\', \'bilinear\']\n        self.with_out_conv = with_out_conv\n        self.with_input1_conv = with_input1_conv\n        self.with_input2_conv = with_input2_conv\n        self.upsample_mode = upsample_mode\n\n        if self.with_out_conv:\n            self.out_conv = ConvModule(\n                fused_channels,\n                out_channels,\n                **out_conv_cfg,\n                norm_cfg=out_norm_cfg,\n                order=out_conv_order)\n\n        self.input1_conv = self._build_input_conv(\n            out_channels, input_conv_cfg,\n            input_norm_cfg) if with_input1_conv else nn.Sequential()\n        self.input2_conv = self._build_input_conv(\n            out_channels, input_conv_cfg,\n            input_norm_cfg) if with_input2_conv else nn.Sequential()\n\n    def _build_input_conv(self, channel, conv_cfg, norm_cfg):\n        return ConvModule(\n            channel,\n            channel,\n            3,\n            padding=1,\n            conv_cfg=conv_cfg,\n            norm_cfg=norm_cfg,\n            bias=True)\n\n    @abstractmethod\n    def _binary_op(self, x1, x2):\n        pass\n\n    def _resize(self, x, size):\n        if x.shape[-2:] == size:\n            return x\n        elif x.shape[-2:] < size:\n            return F.interpolate(x, size=size, mode=self.upsample_mode)\n        else:\n            assert x.shape[-2] % size[-2] == 0 and x.shape[-1] % size[-1] == 0\n            kernel_size = x.shape[-1] // size[-1]\n            x = F.max_pool2d(x, kernel_size=kernel_size, stride=kernel_size)\n            return x\n\n    def forward(self, x1, x2, out_size=None):\n        assert x1.shape[:2] == x2.shape[:2]\n        assert out_size is None or len(out_size) == 2\n        if out_size is None:  # resize to larger one\n            out_size = max(x1.size()[2:], x2.size()[2:])\n\n        x1 = self.input1_conv(x1)\n        x2 = self.input2_conv(x2)\n\n        x1 = self._resize(x1, out_size)\n        x2 = self._resize(x2, out_size)\n\n        x = self._binary_op(x1, x2)\n        if self.with_out_conv:\n            x = self.out_conv(x)\n        return x\n\n\nclass SumCell(BaseMergeCell):\n\n    def __init__(self, in_channels, out_channels, **kwargs):\n        super(SumCell, self).__init__(in_channels, out_channels, **kwargs)\n\n    def _binary_op(self, x1, x2):\n        return x1 + x2\n\n\nclass ConcatCell(BaseMergeCell):\n\n    def __init__(self, in_channels, out_channels, **kwargs):\n        super(ConcatCell, self).__init__(in_channels * 2, out_channels,\n                                         **kwargs)\n\n    def _binary_op(self, x1, x2):\n        ret = torch.cat([x1, x2], dim=1)\n        return ret\n\n\nclass GlobalPoolingCell(BaseMergeCell):\n\n    def __init__(self, in_channels=None, out_channels=None, **kwargs):\n        super().__init__(in_channels, out_channels, **kwargs)\n        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n\n    def _binary_op(self, x1, x2):\n        x2_att = self.global_pool(x2).sigmoid()\n        return x2 + x2_att * x1\n'"
mmdet/ops/non_local.py,4,"b'import torch\nimport torch.nn as nn\nfrom mmcv.cnn import ConvModule, constant_init, normal_init\n\n\nclass NonLocal2D(nn.Module):\n    """"""Non-local module.\n\n    See https://arxiv.org/abs/1711.07971 for details.\n\n    Args:\n        in_channels (int): Channels of the input feature map.\n        reduction (int): Channel reduction ratio.\n        use_scale (bool): Whether to scale pairwise_weight by 1/inter_channels.\n        conv_cfg (dict): The config dict for convolution layers.\n            (only applicable to conv_out)\n        norm_cfg (dict): The config dict for normalization layers.\n            (only applicable to conv_out)\n        mode (str): Options are `embedded_gaussian` and `dot_product`.\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 reduction=2,\n                 use_scale=True,\n                 conv_cfg=None,\n                 norm_cfg=None,\n                 mode=\'embedded_gaussian\'):\n        super(NonLocal2D, self).__init__()\n        self.in_channels = in_channels\n        self.reduction = reduction\n        self.use_scale = use_scale\n        self.inter_channels = in_channels // reduction\n        self.mode = mode\n        assert mode in [\'embedded_gaussian\', \'dot_product\']\n\n        # g, theta, phi are actually `nn.Conv2d`. Here we use ConvModule for\n        # potential usage.\n        self.g = ConvModule(\n            self.in_channels, self.inter_channels, kernel_size=1, act_cfg=None)\n        self.theta = ConvModule(\n            self.in_channels, self.inter_channels, kernel_size=1, act_cfg=None)\n        self.phi = ConvModule(\n            self.in_channels, self.inter_channels, kernel_size=1, act_cfg=None)\n        self.conv_out = ConvModule(\n            self.inter_channels,\n            self.in_channels,\n            kernel_size=1,\n            conv_cfg=conv_cfg,\n            norm_cfg=norm_cfg,\n            act_cfg=None)\n\n        self.init_weights()\n\n    def init_weights(self, std=0.01, zeros_init=True):\n        for m in [self.g, self.theta, self.phi]:\n            normal_init(m.conv, std=std)\n        if zeros_init:\n            constant_init(self.conv_out.conv, 0)\n        else:\n            normal_init(self.conv_out.conv, std=std)\n\n    def embedded_gaussian(self, theta_x, phi_x):\n        # pairwise_weight: [N, HxW, HxW]\n        pairwise_weight = torch.matmul(theta_x, phi_x)\n        if self.use_scale:\n            # theta_x.shape[-1] is `self.inter_channels`\n            pairwise_weight /= theta_x.shape[-1]**0.5\n        pairwise_weight = pairwise_weight.softmax(dim=-1)\n        return pairwise_weight\n\n    def dot_product(self, theta_x, phi_x):\n        # pairwise_weight: [N, HxW, HxW]\n        pairwise_weight = torch.matmul(theta_x, phi_x)\n        pairwise_weight /= pairwise_weight.shape[-1]\n        return pairwise_weight\n\n    def forward(self, x):\n        n, _, h, w = x.shape\n\n        # g_x: [N, HxW, C]\n        g_x = self.g(x).view(n, self.inter_channels, -1)\n        g_x = g_x.permute(0, 2, 1)\n\n        # theta_x: [N, HxW, C]\n        theta_x = self.theta(x).view(n, self.inter_channels, -1)\n        theta_x = theta_x.permute(0, 2, 1)\n\n        # phi_x: [N, C, HxW]\n        phi_x = self.phi(x).view(n, self.inter_channels, -1)\n\n        pairwise_func = getattr(self, self.mode)\n        # pairwise_weight: [N, HxW, HxW]\n        pairwise_weight = pairwise_func(theta_x, phi_x)\n\n        # y: [N, HxW, C]\n        y = torch.matmul(pairwise_weight, g_x)\n        # y: [N, C, H, W]\n        y = y.permute(0, 2, 1).reshape(n, self.inter_channels, h, w)\n\n        output = x + self.conv_out(y)\n\n        return output\n'"
mmdet/ops/plugin.py,0,"b'from mmcv.cnn import ConvModule\n\nfrom .context_block import ContextBlock\nfrom .generalized_attention import GeneralizedAttention\nfrom .non_local import NonLocal2D\n\nplugin_cfg = {\n    # format: layer_type: (abbreviation, module)\n    \'ContextBlock\': (\'context_block\', ContextBlock),\n    \'GeneralizedAttention\': (\'gen_attention_block\', GeneralizedAttention),\n    \'NonLocal2D\': (\'nonlocal_block\', NonLocal2D),\n    \'ConvModule\': (\'conv_block\', ConvModule),\n}\n\n\ndef build_plugin_layer(cfg, postfix=\'\', **kwargs):\n    """""" Build plugin layer\n\n    Args:\n        cfg (None or dict): cfg should contain:\n            type (str): identify plugin layer type.\n            layer args: args needed to instantiate a plugin layer.\n        postfix (int, str): appended into norm abbreviation to\n            create named layer.\n\n    Returns:\n        name (str): abbreviation + postfix\n        layer (nn.Module): created plugin layer\n    """"""\n    assert isinstance(cfg, dict) and \'type\' in cfg\n    cfg_ = cfg.copy()\n\n    layer_type = cfg_.pop(\'type\')\n    if layer_type not in plugin_cfg:\n        raise KeyError(f\'Unrecognized plugin type {layer_type}\')\n    else:\n        abbr, plugin_layer = plugin_cfg[layer_type]\n\n    assert isinstance(postfix, (int, str))\n    name = abbr + str(postfix)\n\n    layer = plugin_layer(**kwargs, **cfg_)\n\n    return name, layer\n'"
mmdet/ops/wrappers.py,7,"b'""""""\nModified from https://github.com/facebookresearch/detectron2/blob/master\n/detectron2/layers/wrappers.py\nWrap some nn modules to support empty tensor input.\nCurrently, these wrappers are mainly used in mask heads like fcn_mask_head\nand maskiou_heads since mask heads are trained on only positive RoIs.\n""""""\nimport math\n\nimport torch\nimport torch.nn as nn\nfrom mmcv.cnn import CONV_LAYERS\nfrom torch.nn.modules.utils import _pair\n\n\nclass NewEmptyTensorOp(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, x, new_shape):\n        ctx.shape = x.shape\n        return x.new_empty(new_shape)\n\n    @staticmethod\n    def backward(ctx, grad):\n        shape = ctx.shape\n        return NewEmptyTensorOp.apply(grad, shape), None\n\n\n@CONV_LAYERS.register_module(\'Conv\', force=True)\nclass Conv2d(nn.Conv2d):\n\n    def forward(self, x):\n        if x.numel() == 0 and torch.__version__ <= \'1.4\':\n            out_shape = [x.shape[0], self.out_channels]\n            for i, k, p, s, d in zip(x.shape[-2:], self.kernel_size,\n                                     self.padding, self.stride, self.dilation):\n                o = (i + 2 * p - (d * (k - 1) + 1)) // s + 1\n                out_shape.append(o)\n            empty = NewEmptyTensorOp.apply(x, out_shape)\n            if self.training:\n                # produce dummy gradient to avoid DDP warning.\n                dummy = sum(x.view(-1)[0] for x in self.parameters()) * 0.0\n                return empty + dummy\n            else:\n                return empty\n\n        return super().forward(x)\n\n\nclass ConvTranspose2d(nn.ConvTranspose2d):\n\n    def forward(self, x):\n        if x.numel() == 0 and torch.__version__ <= \'1.4.0\':\n            out_shape = [x.shape[0], self.out_channels]\n            for i, k, p, s, d, op in zip(x.shape[-2:], self.kernel_size,\n                                         self.padding, self.stride,\n                                         self.dilation, self.output_padding):\n                out_shape.append((i - 1) * s - 2 * p + (d * (k - 1) + 1) + op)\n            empty = NewEmptyTensorOp.apply(x, out_shape)\n            if self.training:\n                # produce dummy gradient to avoid DDP warning.\n                dummy = sum(x.view(-1)[0] for x in self.parameters()) * 0.0\n                return empty + dummy\n            else:\n                return empty\n\n        return super(ConvTranspose2d, self).forward(x)\n\n\nclass MaxPool2d(nn.MaxPool2d):\n\n    def forward(self, x):\n        if x.numel() == 0 and torch.__version__ <= \'1.4\':\n            out_shape = list(x.shape[:2])\n            for i, k, p, s, d in zip(x.shape[-2:], _pair(self.kernel_size),\n                                     _pair(self.padding), _pair(self.stride),\n                                     _pair(self.dilation)):\n                o = (i + 2 * p - (d * (k - 1) + 1)) / s + 1\n                o = math.ceil(o) if self.ceil_mode else math.floor(o)\n                out_shape.append(o)\n            empty = NewEmptyTensorOp.apply(x, out_shape)\n            return empty\n\n        return super().forward(x)\n\n\nclass Linear(torch.nn.Linear):\n\n    def forward(self, x):\n        if x.numel() == 0:\n            out_shape = [x.shape[0], self.out_features]\n            empty = NewEmptyTensorOp.apply(x, out_shape)\n            if self.training:\n                # produce dummy gradient to avoid DDP warning.\n                dummy = sum(x.view(-1)[0] for x in self.parameters()) * 0.0\n                return empty + dummy\n            else:\n                return empty\n\n        return super().forward(x)\n'"
mmdet/utils/__init__.py,0,"b""from .collect_env import collect_env\nfrom .flops_counter import get_model_complexity_info\nfrom .logger import get_root_logger\n\n__all__ = ['get_model_complexity_info', 'get_root_logger', 'collect_env']\n"""
mmdet/utils/collect_env.py,6,"b'import os.path as osp\nimport subprocess\nimport sys\nfrom collections import defaultdict\n\nimport cv2\nimport mmcv\nimport torch\nimport torchvision\n\nimport mmdet\n\n\ndef collect_env():\n    env_info = {}\n    env_info[\'sys.platform\'] = sys.platform\n    env_info[\'Python\'] = sys.version.replace(\'\\n\', \'\')\n\n    cuda_available = torch.cuda.is_available()\n    env_info[\'CUDA available\'] = cuda_available\n\n    if cuda_available:\n        from torch.utils.cpp_extension import CUDA_HOME\n        env_info[\'CUDA_HOME\'] = CUDA_HOME\n\n        if CUDA_HOME is not None and osp.isdir(CUDA_HOME):\n            try:\n                nvcc = osp.join(CUDA_HOME, \'bin/nvcc\')\n                nvcc = subprocess.check_output(\n                    f\'""{nvcc}"" -V | tail -n1\', shell=True)\n                nvcc = nvcc.decode(\'utf-8\').strip()\n            except subprocess.SubprocessError:\n                nvcc = \'Not Available\'\n            env_info[\'NVCC\'] = nvcc\n\n        devices = defaultdict(list)\n        for k in range(torch.cuda.device_count()):\n            devices[torch.cuda.get_device_name(k)].append(str(k))\n        for name, devids in devices.items():\n            env_info[\'GPU \' + \',\'.join(devids)] = name\n\n    gcc = subprocess.check_output(\'gcc --version | head -n1\', shell=True)\n    gcc = gcc.decode(\'utf-8\').strip()\n    env_info[\'GCC\'] = gcc\n\n    env_info[\'PyTorch\'] = torch.__version__\n    env_info[\'PyTorch compiling details\'] = torch.__config__.show()\n\n    env_info[\'TorchVision\'] = torchvision.__version__\n\n    env_info[\'OpenCV\'] = cv2.__version__\n\n    env_info[\'MMCV\'] = mmcv.__version__\n    env_info[\'MMDetection\'] = mmdet.__version__\n    from mmdet.ops import get_compiler_version, get_compiling_cuda_version\n    env_info[\'MMDetection Compiler\'] = get_compiler_version()\n    env_info[\'MMDetection CUDA Compiler\'] = get_compiling_cuda_version()\n    return env_info\n\n\nif __name__ == \'__main__\':\n    for name, val in collect_env().items():\n        print(f\'{name}: {val}\')\n'"
mmdet/utils/contextmanagers.py,17,"b'import asyncio\nimport contextlib\nimport logging\nimport os\nimport time\nfrom typing import List\n\nimport torch\n\nlogger = logging.getLogger(__name__)\n\nDEBUG_COMPLETED_TIME = bool(os.environ.get(\'DEBUG_COMPLETED_TIME\', False))\n\n\n@contextlib.asynccontextmanager\nasync def completed(trace_name=\'\',\n                    name=\'\',\n                    sleep_interval=0.05,\n                    streams: List[torch.cuda.Stream] = None):\n    """"""\n    Async context manager that waits for work to complete on\n    given CUDA streams.\n\n    """"""\n    if not torch.cuda.is_available():\n        yield\n        return\n\n    stream_before_context_switch = torch.cuda.current_stream()\n    if not streams:\n        streams = [stream_before_context_switch]\n    else:\n        streams = [s if s else stream_before_context_switch for s in streams]\n\n    end_events = [\n        torch.cuda.Event(enable_timing=DEBUG_COMPLETED_TIME) for _ in streams\n    ]\n\n    if DEBUG_COMPLETED_TIME:\n        start = torch.cuda.Event(enable_timing=True)\n        stream_before_context_switch.record_event(start)\n\n        cpu_start = time.monotonic()\n    logger.debug(\'%s %s starting, streams: %s\', trace_name, name, streams)\n    grad_enabled_before = torch.is_grad_enabled()\n    try:\n        yield\n    finally:\n        current_stream = torch.cuda.current_stream()\n        assert current_stream == stream_before_context_switch\n\n        if DEBUG_COMPLETED_TIME:\n            cpu_end = time.monotonic()\n        for i, stream in enumerate(streams):\n            event = end_events[i]\n            stream.record_event(event)\n\n        grad_enabled_after = torch.is_grad_enabled()\n\n        # observed change of torch.is_grad_enabled() during concurrent run of\n        # async_test_bboxes code\n        assert (grad_enabled_before == grad_enabled_after\n                ), \'Unexpected is_grad_enabled() value change\'\n\n        are_done = [e.query() for e in end_events]\n        logger.debug(\'%s %s completed: %s streams: %s\', trace_name, name,\n                     are_done, streams)\n        with torch.cuda.stream(stream_before_context_switch):\n            while not all(are_done):\n                await asyncio.sleep(sleep_interval)\n                are_done = [e.query() for e in end_events]\n                logger.debug(\n                    \'%s %s completed: %s streams: %s\',\n                    trace_name,\n                    name,\n                    are_done,\n                    streams,\n                )\n\n        current_stream = torch.cuda.current_stream()\n        assert current_stream == stream_before_context_switch\n\n        if DEBUG_COMPLETED_TIME:\n            cpu_time = (cpu_end - cpu_start) * 1000\n            stream_times_ms = \'\'\n            for i, stream in enumerate(streams):\n                elapsed_time = start.elapsed_time(end_events[i])\n                stream_times_ms += f\' {stream} {elapsed_time:.2f} ms\'\n            logger.info(\'%s %s %.2f ms %s\', trace_name, name, cpu_time,\n                        stream_times_ms)\n\n\n@contextlib.asynccontextmanager\nasync def concurrent(streamqueue: asyncio.Queue,\n                     trace_name=\'concurrent\',\n                     name=\'stream\'):\n    """"""Run code concurrently in different streams.\n\n    :param streamqueue: asyncio.Queue instance.\n\n    Queue tasks define the pool of streams used for concurrent execution.\n\n    """"""\n    if not torch.cuda.is_available():\n        yield\n        return\n\n    initial_stream = torch.cuda.current_stream()\n\n    with torch.cuda.stream(initial_stream):\n        stream = await streamqueue.get()\n        assert isinstance(stream, torch.cuda.Stream)\n\n        try:\n            with torch.cuda.stream(stream):\n                logger.debug(\'%s %s is starting, stream: %s\', trace_name, name,\n                             stream)\n                yield\n                current = torch.cuda.current_stream()\n                assert current == stream\n                logger.debug(\'%s %s has finished, stream: %s\', trace_name,\n                             name, stream)\n        finally:\n            streamqueue.task_done()\n            streamqueue.put_nowait(stream)\n'"
mmdet/utils/flops_counter.py,6,"b'# Modified from flops-counter.pytorch by Vladislav Sovrasov\n# original repo: https://github.com/sovrasov/flops-counter.pytorch\n\n# MIT License\n\n# Copyright (c) 2018 Vladislav Sovrasov\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport sys\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.nn.modules.batchnorm import _BatchNorm\nfrom torch.nn.modules.conv import _ConvNd, _ConvTransposeMixin\nfrom torch.nn.modules.pooling import (_AdaptiveAvgPoolNd, _AdaptiveMaxPoolNd,\n                                      _AvgPoolNd, _MaxPoolNd)\n\n\ndef get_model_complexity_info(model,\n                              input_res,\n                              print_per_layer_stat=True,\n                              as_strings=True,\n                              input_constructor=None,\n                              ost=sys.stdout):\n    assert type(input_res) is tuple\n    assert len(input_res) >= 2\n    flops_model = add_flops_counting_methods(model)\n    flops_model.eval().start_flops_count()\n    if input_constructor:\n        input = input_constructor(input_res)\n        _ = flops_model(**input)\n    else:\n        batch = torch.ones(()).new_empty(\n            (1, *input_res),\n            dtype=next(flops_model.parameters()).dtype,\n            device=next(flops_model.parameters()).device)\n        flops_model(batch)\n\n    if print_per_layer_stat:\n        print_model_with_flops(flops_model, ost=ost)\n    flops_count = flops_model.compute_average_flops_cost()\n    params_count = get_model_parameters_number(flops_model)\n    flops_model.stop_flops_count()\n\n    if as_strings:\n        return flops_to_string(flops_count), params_to_string(params_count)\n\n    return flops_count, params_count\n\n\ndef flops_to_string(flops, units=\'GMac\', precision=2):\n    if units is None:\n        if flops // 10**9 > 0:\n            return str(round(flops / 10.**9, precision)) + \' GMac\'\n        elif flops // 10**6 > 0:\n            return str(round(flops / 10.**6, precision)) + \' MMac\'\n        elif flops // 10**3 > 0:\n            return str(round(flops / 10.**3, precision)) + \' KMac\'\n        else:\n            return str(flops) + \' Mac\'\n    else:\n        if units == \'GMac\':\n            return str(round(flops / 10.**9, precision)) + \' \' + units\n        elif units == \'MMac\':\n            return str(round(flops / 10.**6, precision)) + \' \' + units\n        elif units == \'KMac\':\n            return str(round(flops / 10.**3, precision)) + \' \' + units\n        else:\n            return str(flops) + \' Mac\'\n\n\ndef params_to_string(params_num):\n    """"""converting number to string\n\n    :param float params_num: number\n    :returns str: number\n\n    >>> params_to_string(1e9)\n    \'1000.0 M\'\n    >>> params_to_string(2e5)\n    \'200.0 k\'\n    >>> params_to_string(3e-9)\n    \'3e-09\'\n    """"""\n    if params_num // 10**6 > 0:\n        return str(round(params_num / 10**6, 2)) + \' M\'\n    elif params_num // 10**3:\n        return str(round(params_num / 10**3, 2)) + \' k\'\n    else:\n        return str(params_num)\n\n\ndef print_model_with_flops(model, units=\'GMac\', precision=3, ost=sys.stdout):\n    total_flops = model.compute_average_flops_cost()\n\n    def accumulate_flops(self):\n        if is_supported_instance(self):\n            return self.__flops__ / model.__batch_counter__\n        else:\n            sum = 0\n            for m in self.children():\n                sum += m.accumulate_flops()\n            return sum\n\n    def flops_repr(self):\n        accumulated_flops_cost = self.accumulate_flops()\n        return \', \'.join([\n            flops_to_string(\n                accumulated_flops_cost, units=units, precision=precision),\n            f\'{accumulated_flops_cost / total_flops:.3%} MACs\',\n            self.original_extra_repr()\n        ])\n\n    def add_extra_repr(m):\n        m.accumulate_flops = accumulate_flops.__get__(m)\n        flops_extra_repr = flops_repr.__get__(m)\n        if m.extra_repr != flops_extra_repr:\n            m.original_extra_repr = m.extra_repr\n            m.extra_repr = flops_extra_repr\n            assert m.extra_repr != m.original_extra_repr\n\n    def del_extra_repr(m):\n        if hasattr(m, \'original_extra_repr\'):\n            m.extra_repr = m.original_extra_repr\n            del m.original_extra_repr\n        if hasattr(m, \'accumulate_flops\'):\n            del m.accumulate_flops\n\n    model.apply(add_extra_repr)\n    print(model, file=ost)\n    model.apply(del_extra_repr)\n\n\ndef get_model_parameters_number(model):\n    params_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    return params_num\n\n\ndef add_flops_counting_methods(net_main_module):\n    # adding additional methods to the existing module object,\n    # this is done this way so that each function has access to self object\n    net_main_module.start_flops_count = start_flops_count.__get__(\n        net_main_module)\n    net_main_module.stop_flops_count = stop_flops_count.__get__(\n        net_main_module)\n    net_main_module.reset_flops_count = reset_flops_count.__get__(\n        net_main_module)\n    net_main_module.compute_average_flops_cost = \\\n        compute_average_flops_cost.__get__(net_main_module)\n\n    net_main_module.reset_flops_count()\n\n    # Adding variables necessary for masked flops computation\n    net_main_module.apply(add_flops_mask_variable_or_reset)\n\n    return net_main_module\n\n\ndef compute_average_flops_cost(self):\n    """"""\n    A method that will be available after add_flops_counting_methods() is\n    called on a desired net object.\n    Returns current mean flops consumption per image.\n    """"""\n\n    batches_count = self.__batch_counter__\n    flops_sum = 0\n    for module in self.modules():\n        if is_supported_instance(module):\n            flops_sum += module.__flops__\n\n    return flops_sum / batches_count\n\n\ndef start_flops_count(self):\n    """"""\n    A method that will be available after add_flops_counting_methods() is\n    called on a desired net object.\n    Activates the computation of mean flops consumption per image.\n    Call it before you run the network.\n    """"""\n    add_batch_counter_hook_function(self)\n    self.apply(add_flops_counter_hook_function)\n\n\ndef stop_flops_count(self):\n    """"""\n    A method that will be available after add_flops_counting_methods() is\n    called on a desired net object.\n    Stops computing the mean flops consumption per image.\n    Call whenever you want to pause the computation.\n    """"""\n    remove_batch_counter_hook_function(self)\n    self.apply(remove_flops_counter_hook_function)\n\n\ndef reset_flops_count(self):\n    """"""\n    A method that will be available after add_flops_counting_methods() is\n    called on a desired net object.\n    Resets statistics computed so far.\n    """"""\n    add_batch_counter_variables_or_reset(self)\n    self.apply(add_flops_counter_variable_or_reset)\n\n\ndef add_flops_mask(module, mask):\n\n    def add_flops_mask_func(module):\n        if isinstance(module, torch.nn.Conv2d):\n            module.__mask__ = mask\n\n    module.apply(add_flops_mask_func)\n\n\ndef remove_flops_mask(module):\n    module.apply(add_flops_mask_variable_or_reset)\n\n\ndef is_supported_instance(module):\n    for mod in hook_mapping:\n        if issubclass(type(module), mod):\n            return True\n    return False\n\n\ndef empty_flops_counter_hook(module, input, output):\n    module.__flops__ += 0\n\n\ndef upsample_flops_counter_hook(module, input, output):\n    output_size = output[0]\n    batch_size = output_size.shape[0]\n    output_elements_count = batch_size\n    for val in output_size.shape[1:]:\n        output_elements_count *= val\n    module.__flops__ += int(output_elements_count)\n\n\ndef relu_flops_counter_hook(module, input, output):\n    active_elements_count = output.numel()\n    module.__flops__ += int(active_elements_count)\n\n\ndef linear_flops_counter_hook(module, input, output):\n    input = input[0]\n    batch_size = input.shape[0]\n    module.__flops__ += int(batch_size * input.shape[1] * output.shape[1])\n\n\ndef pool_flops_counter_hook(module, input, output):\n    input = input[0]\n    module.__flops__ += int(np.prod(input.shape))\n\n\ndef bn_flops_counter_hook(module, input, output):\n    input = input[0]\n\n    batch_flops = np.prod(input.shape)\n    if module.affine:\n        batch_flops *= 2\n    module.__flops__ += int(batch_flops)\n\n\ndef gn_flops_counter_hook(module, input, output):\n    elems = np.prod(input[0].shape)\n    # there is no precise FLOPs estimation of computing mean and variance,\n    # and we just set it 2 * elems: half muladds for computing\n    # means and half for computing vars\n    batch_flops = 3 * elems\n    if module.affine:\n        batch_flops += elems\n    module.__flops__ += int(batch_flops)\n\n\ndef deconv_flops_counter_hook(conv_module, input, output):\n    # Can have multiple inputs, getting the first one\n    input = input[0]\n\n    batch_size = input.shape[0]\n    input_height, input_width = input.shape[2:]\n\n    kernel_height, kernel_width = conv_module.kernel_size\n    in_channels = conv_module.in_channels\n    out_channels = conv_module.out_channels\n    groups = conv_module.groups\n\n    filters_per_channel = out_channels // groups\n    conv_per_position_flops = (\n        kernel_height * kernel_width * in_channels * filters_per_channel)\n\n    active_elements_count = batch_size * input_height * input_width\n    overall_conv_flops = conv_per_position_flops * active_elements_count\n    bias_flops = 0\n    if conv_module.bias is not None:\n        output_height, output_width = output.shape[2:]\n        bias_flops = out_channels * batch_size * output_height * output_height\n    overall_flops = overall_conv_flops + bias_flops\n\n    conv_module.__flops__ += int(overall_flops)\n\n\ndef conv_flops_counter_hook(conv_module, input, output):\n    # Can have multiple inputs, getting the first one\n    input = input[0]\n\n    batch_size = input.shape[0]\n    output_dims = list(output.shape[2:])\n\n    kernel_dims = list(conv_module.kernel_size)\n    in_channels = conv_module.in_channels\n    out_channels = conv_module.out_channels\n    groups = conv_module.groups\n\n    filters_per_channel = out_channels // groups\n    conv_per_position_flops = np.prod(\n        kernel_dims) * in_channels * filters_per_channel\n\n    active_elements_count = batch_size * np.prod(output_dims)\n\n    if conv_module.__mask__ is not None:\n        # (b, 1, h, w)\n        output_height, output_width = output.shape[2:]\n        flops_mask = conv_module.__mask__.expand(batch_size, 1, output_height,\n                                                 output_width)\n        active_elements_count = flops_mask.sum()\n\n    overall_conv_flops = conv_per_position_flops * active_elements_count\n\n    bias_flops = 0\n\n    if conv_module.bias is not None:\n\n        bias_flops = out_channels * active_elements_count\n\n    overall_flops = overall_conv_flops + bias_flops\n\n    conv_module.__flops__ += int(overall_flops)\n\n\nhook_mapping = {\n    # conv\n    _ConvNd: conv_flops_counter_hook,\n    # deconv\n    _ConvTransposeMixin: deconv_flops_counter_hook,\n    # fc\n    nn.Linear: linear_flops_counter_hook,\n    # pooling\n    _AvgPoolNd: pool_flops_counter_hook,\n    _MaxPoolNd: pool_flops_counter_hook,\n    _AdaptiveAvgPoolNd: pool_flops_counter_hook,\n    _AdaptiveMaxPoolNd: pool_flops_counter_hook,\n    # activation\n    nn.ReLU: relu_flops_counter_hook,\n    nn.PReLU: relu_flops_counter_hook,\n    nn.ELU: relu_flops_counter_hook,\n    nn.LeakyReLU: relu_flops_counter_hook,\n    nn.ReLU6: relu_flops_counter_hook,\n    # normalization\n    _BatchNorm: bn_flops_counter_hook,\n    nn.GroupNorm: gn_flops_counter_hook,\n    # upsample\n    nn.Upsample: upsample_flops_counter_hook,\n}\n\n\ndef batch_counter_hook(module, input, output):\n    batch_size = 1\n    if len(input) > 0:\n        # Can have multiple inputs, getting the first one\n        input = input[0]\n        batch_size = len(input)\n    else:\n        print(\'Warning! No positional inputs found for a module, \'\n              \'assuming batch size is 1.\')\n    module.__batch_counter__ += batch_size\n\n\ndef add_batch_counter_variables_or_reset(module):\n    module.__batch_counter__ = 0\n\n\ndef add_batch_counter_hook_function(module):\n    if hasattr(module, \'__batch_counter_handle__\'):\n        return\n\n    handle = module.register_forward_hook(batch_counter_hook)\n    module.__batch_counter_handle__ = handle\n\n\ndef remove_batch_counter_hook_function(module):\n    if hasattr(module, \'__batch_counter_handle__\'):\n        module.__batch_counter_handle__.remove()\n        del module.__batch_counter_handle__\n\n\ndef add_flops_counter_variable_or_reset(module):\n    if is_supported_instance(module):\n        module.__flops__ = 0\n\n\ndef add_flops_counter_hook_function(module):\n    if is_supported_instance(module):\n        if hasattr(module, \'__flops_handle__\'):\n            return\n\n        for mod_type, counter_hook in hook_mapping.items():\n            if issubclass(type(module), mod_type):\n                handle = module.register_forward_hook(counter_hook)\n                break\n\n        module.__flops_handle__ = handle\n\n\ndef remove_flops_counter_hook_function(module):\n    if is_supported_instance(module):\n        if hasattr(module, \'__flops_handle__\'):\n            module.__flops_handle__.remove()\n            del module.__flops_handle__\n\n\n# --- Masked flops counting\n# Also being run in the initialization\ndef add_flops_mask_variable_or_reset(module):\n    if is_supported_instance(module):\n        module.__mask__ = None\n'"
mmdet/utils/logger.py,0,"b""import logging\n\nfrom mmcv.utils import get_logger\n\n\ndef get_root_logger(log_file=None, log_level=logging.INFO):\n    logger = get_logger(name='mmdet', log_file=log_file, log_level=log_level)\n\n    return logger\n"""
mmdet/utils/profiling.py,4,"b'import contextlib\nimport sys\nimport time\n\nimport torch\n\nif sys.version_info >= (3, 7):\n\n    @contextlib.contextmanager\n    def profile_time(trace_name,\n                     name,\n                     enabled=True,\n                     stream=None,\n                     end_stream=None):\n        """"""Print time spent by CPU and GPU.\n\n        Useful as a temporary context manager to find sweet spots of\n        code suitable for async implementation.\n\n        """"""\n        if (not enabled) or not torch.cuda.is_available():\n            yield\n            return\n        stream = stream if stream else torch.cuda.current_stream()\n        end_stream = end_stream if end_stream else stream\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        stream.record_event(start)\n        try:\n            cpu_start = time.monotonic()\n            yield\n        finally:\n            cpu_end = time.monotonic()\n            end_stream.record_event(end)\n            end.synchronize()\n            cpu_time = (cpu_end - cpu_start) * 1000\n            gpu_time = start.elapsed_time(end)\n            msg = f\'{trace_name} {name} cpu_time {cpu_time:.2f} ms \'\n            msg += f\'gpu_time {gpu_time:.2f} ms stream {stream}\'\n            print(msg, end_stream)\n'"
mmdet/utils/util_mixins.py,0,"b'""""""\nThis module defines the :class:`NiceRepr` mixin class, which defines a\n``__repr__`` and ``__str__`` method that only depend on a custom ``__nice__``\nmethod, which you must define. This means you only have to overload one\nfunction instead of two.  Furthermore, if the object defines a ``__len__``\nmethod, then the ``__nice__`` method defaults to something sensible, otherwise\nit is treated as abstract and raises ``NotImplementedError``.\n\nTo use simply have your object inherit from :class:`NiceRepr`\n(multi-inheritance should be ok).\n\nThis code was copied from the ubelt library: https://github.com/Erotemic/ubelt\n\nExample:\n    >>> # Objects that define __nice__ have a default __str__ and __repr__\n    >>> class Student(NiceRepr):\n    ...    def __init__(self, name):\n    ...        self.name = name\n    ...    def __nice__(self):\n    ...        return self.name\n    >>> s1 = Student(\'Alice\')\n    >>> s2 = Student(\'Bob\')\n    >>> print(f\'s1 = {s1}\')\n    >>> print(f\'s2 = {s2}\')\n    s1 = <Student(Alice)>\n    s2 = <Student(Bob)>\n\nExample:\n    >>> # Objects that define __len__ have a default __nice__\n    >>> class Group(NiceRepr):\n    ...    def __init__(self, data):\n    ...        self.data = data\n    ...    def __len__(self):\n    ...        return len(self.data)\n    >>> g = Group([1, 2, 3])\n    >>> print(f\'g = {g}\')\n    g = <Group(3)>\n\n""""""\nimport warnings\n\n\nclass NiceRepr(object):\n    """"""\n    Inherit from this class and define ``__nice__`` to ""nicely"" print your\n    objects.\n\n    Defines ``__str__`` and ``__repr__`` in terms of ``__nice__`` function\n    Classes that inherit from :class:`NiceRepr` should redefine ``__nice__``.\n    If the inheriting class has a ``__len__``, method then the default\n    ``__nice__`` method will return its length.\n\n    Example:\n        >>> class Foo(NiceRepr):\n        ...    def __nice__(self):\n        ...        return \'info\'\n        >>> foo = Foo()\n        >>> assert str(foo) == \'<Foo(info)>\'\n        >>> assert repr(foo).startswith(\'<Foo(info) at \')\n\n    Example:\n        >>> class Bar(NiceRepr):\n        ...    pass\n        >>> bar = Bar()\n        >>> import pytest\n        >>> with pytest.warns(None) as record:\n        >>>     assert \'object at\' in str(bar)\n        >>>     assert \'object at\' in repr(bar)\n\n    Example:\n        >>> class Baz(NiceRepr):\n        ...    def __len__(self):\n        ...        return 5\n        >>> baz = Baz()\n        >>> assert str(baz) == \'<Baz(5)>\'\n    """"""\n\n    def __nice__(self):\n        if hasattr(self, \'__len__\'):\n            # It is a common pattern for objects to use __len__ in __nice__\n            # As a convenience we define a default __nice__ for these objects\n            return str(len(self))\n        else:\n            # In all other cases force the subclass to overload __nice__\n            raise NotImplementedError(\n                f\'Define the __nice__ method for {self.__class__!r}\')\n\n    def __repr__(self):\n        try:\n            nice = self.__nice__()\n            classname = self.__class__.__name__\n            return f\'<{classname}({nice}) at {hex(id(self))}>\'\n        except NotImplementedError as ex:\n            warnings.warn(str(ex), category=RuntimeWarning)\n            return object.__repr__(self)\n\n    def __str__(self):\n        try:\n            classname = self.__class__.__name__\n            nice = self.__nice__()\n            return f\'<{classname}({nice})>\'\n        except NotImplementedError as ex:\n            warnings.warn(str(ex), category=RuntimeWarning)\n            return object.__repr__(self)\n'"
tests/test_pipelines/test_loading.py,0,"b'import copy\nimport os.path as osp\n\nimport numpy as np\n\nfrom mmdet.datasets.pipelines import (LoadImageFromFile,\n                                      LoadMultiChannelImageFromFiles)\n\n\nclass TestLoading(object):\n\n    @classmethod\n    def setup_class(cls):\n        cls.data_prefix = osp.join(osp.dirname(__file__), \'../data\')\n\n    def test_load_img(self):\n        results = dict(\n            img_prefix=self.data_prefix, img_info=dict(filename=\'color.jpg\'))\n        transform = LoadImageFromFile()\n        results = transform(copy.deepcopy(results))\n        assert results[\'filename\'] == osp.join(self.data_prefix, \'color.jpg\')\n        assert results[\'ori_filename\'] == \'color.jpg\'\n        assert results[\'img\'].shape == (288, 512, 3)\n        assert results[\'img\'].dtype == np.uint8\n        assert results[\'img_shape\'] == (288, 512, 3)\n        assert results[\'ori_shape\'] == (288, 512, 3)\n        assert results[\'pad_shape\'] == (288, 512, 3)\n        assert results[\'scale_factor\'] == 1.0\n        np.testing.assert_equal(results[\'img_norm_cfg\'][\'mean\'],\n                                np.zeros(3, dtype=np.float32))\n        assert repr(transform) == transform.__class__.__name__ + \\\n            ""(to_float32=False, color_type=\'color\', "" + \\\n            ""file_client_args={\'backend\': \'disk\'})""\n\n        # no img_prefix\n        results = dict(\n            img_prefix=None, img_info=dict(filename=\'tests/data/color.jpg\'))\n        transform = LoadImageFromFile()\n        results = transform(copy.deepcopy(results))\n        assert results[\'filename\'] == \'tests/data/color.jpg\'\n        assert results[\'ori_filename\'] == \'tests/data/color.jpg\'\n        assert results[\'img\'].shape == (288, 512, 3)\n\n        # to_float32\n        transform = LoadImageFromFile(to_float32=True)\n        results = transform(copy.deepcopy(results))\n        assert results[\'img\'].dtype == np.float32\n\n        # gray image\n        results = dict(\n            img_prefix=self.data_prefix, img_info=dict(filename=\'gray.jpg\'))\n        transform = LoadImageFromFile()\n        results = transform(copy.deepcopy(results))\n        assert results[\'img\'].shape == (288, 512, 3)\n        assert results[\'img\'].dtype == np.uint8\n\n        transform = LoadImageFromFile(color_type=\'unchanged\')\n        results = transform(copy.deepcopy(results))\n        assert results[\'img\'].shape == (288, 512)\n        assert results[\'img\'].dtype == np.uint8\n        np.testing.assert_equal(results[\'img_norm_cfg\'][\'mean\'],\n                                np.zeros(1, dtype=np.float32))\n\n    def test_load_multi_channel_img(self):\n        results = dict(\n            img_prefix=self.data_prefix,\n            img_info=dict(filename=[\'color.jpg\', \'color.jpg\']))\n        transform = LoadMultiChannelImageFromFiles()\n        results = transform(copy.deepcopy(results))\n        assert results[\'filename\'] == [\n            osp.join(self.data_prefix, \'color.jpg\'),\n            osp.join(self.data_prefix, \'color.jpg\')\n        ]\n        assert results[\'ori_filename\'] == [\'color.jpg\', \'color.jpg\']\n        assert results[\'img\'].shape == (288, 512, 3, 2)\n        assert results[\'img\'].dtype == np.uint8\n        assert results[\'img_shape\'] == (288, 512, 3, 2)\n        assert results[\'ori_shape\'] == (288, 512, 3, 2)\n        assert results[\'pad_shape\'] == (288, 512, 3, 2)\n        assert results[\'scale_factor\'] == 1.0\n        assert repr(transform) == transform.__class__.__name__ + \\\n            ""(to_float32=False, color_type=\'unchanged\', "" + \\\n            ""file_client_args={\'backend\': \'disk\'})""\n'"
tests/test_pipelines/test_transform.py,0,"b""import copy\nimport os.path as osp\n\nimport mmcv\nimport numpy as np\nimport pytest\nfrom mmcv.utils import build_from_cfg\n\nfrom mmdet.core.evaluation.bbox_overlaps import bbox_overlaps\nfrom mmdet.datasets.builder import PIPELINES\n\n\ndef test_resize():\n    # test assertion if img_scale is a list\n    with pytest.raises(AssertionError):\n        transform = dict(type='Resize', img_scale=[1333, 800], keep_ratio=True)\n        build_from_cfg(transform, PIPELINES)\n\n    # test assertion if len(img_scale) while ratio_range is not None\n    with pytest.raises(AssertionError):\n        transform = dict(\n            type='Resize',\n            img_scale=[(1333, 800), (1333, 600)],\n            ratio_range=(0.9, 1.1),\n            keep_ratio=True)\n        build_from_cfg(transform, PIPELINES)\n\n    # test assertion for invalid multiscale_mode\n    with pytest.raises(AssertionError):\n        transform = dict(\n            type='Resize',\n            img_scale=[(1333, 800), (1333, 600)],\n            keep_ratio=True,\n            multiscale_mode='2333')\n        build_from_cfg(transform, PIPELINES)\n\n    transform = dict(type='Resize', img_scale=(1333, 800), keep_ratio=True)\n    resize_module = build_from_cfg(transform, PIPELINES)\n\n    results = dict()\n    img = mmcv.imread(\n        osp.join(osp.dirname(__file__), '../data/color.jpg'), 'color')\n    results['img'] = img\n    results['img2'] = copy.deepcopy(img)\n    results['img_shape'] = img.shape\n    results['ori_shape'] = img.shape\n    # Set initial values for default meta_keys\n    results['pad_shape'] = img.shape\n    results['scale_factor'] = 1.0\n    results['img_fields'] = ['img', 'img2']\n\n    results = resize_module(results)\n    assert np.equal(results['img'], results['img2']).all()\n\n    results.pop('scale')\n    transform = dict(\n        type='Resize',\n        img_scale=(1280, 800),\n        multiscale_mode='value',\n        keep_ratio=False)\n    resize_module = build_from_cfg(transform, PIPELINES)\n    results = resize_module(results)\n    assert np.equal(results['img'], results['img2']).all()\n    assert results['img_shape'] == (800, 1280, 3)\n\n\ndef test_flip():\n    # test assertion for invalid flip_ratio\n    with pytest.raises(AssertionError):\n        transform = dict(type='RandomFlip', flip_ratio=1.5)\n        build_from_cfg(transform, PIPELINES)\n\n    # test assertion for invalid direction\n    with pytest.raises(AssertionError):\n        transform = dict(\n            type='RandomFlip', flip_ratio=1, direction='horizonta')\n        build_from_cfg(transform, PIPELINES)\n\n    transform = dict(type='RandomFlip', flip_ratio=1)\n    flip_module = build_from_cfg(transform, PIPELINES)\n\n    results = dict()\n    img = mmcv.imread(\n        osp.join(osp.dirname(__file__), '../data/color.jpg'), 'color')\n    original_img = copy.deepcopy(img)\n    results['img'] = img\n    results['img2'] = copy.deepcopy(img)\n    results['img_shape'] = img.shape\n    results['ori_shape'] = img.shape\n    # Set initial values for default meta_keys\n    results['pad_shape'] = img.shape\n    results['scale_factor'] = 1.0\n    results['img_fields'] = ['img', 'img2']\n\n    results = flip_module(results)\n    assert np.equal(results['img'], results['img2']).all()\n\n    flip_module = build_from_cfg(transform, PIPELINES)\n    results = flip_module(results)\n    assert np.equal(results['img'], results['img2']).all()\n    assert np.equal(original_img, results['img']).all()\n\n\ndef test_random_crop():\n    # test assertion for invalid random crop\n    with pytest.raises(AssertionError):\n        transform = dict(type='RandomCrop', crop_size=(-1, 0))\n        build_from_cfg(transform, PIPELINES)\n\n    results = dict()\n    img = mmcv.imread(\n        osp.join(osp.dirname(__file__), '../data/color.jpg'), 'color')\n    results['img'] = img\n\n    results['img_shape'] = img.shape\n    results['ori_shape'] = img.shape\n    # TODO: add img_fields test\n    results['bbox_fields'] = ['gt_bboxes', 'gt_bboxes_ignore']\n    # Set initial values for default meta_keys\n    results['pad_shape'] = img.shape\n    results['scale_factor'] = 1.0\n\n    def create_random_bboxes(num_bboxes, img_w, img_h):\n        bboxes_left_top = np.random.uniform(0, 0.5, size=(num_bboxes, 2))\n        bboxes_right_bottom = np.random.uniform(0.5, 1, size=(num_bboxes, 2))\n        bboxes = np.concatenate((bboxes_left_top, bboxes_right_bottom), 1)\n        bboxes = (bboxes * np.array([img_w, img_h, img_w, img_h])).astype(\n            np.int)\n        return bboxes\n\n    h, w, _ = img.shape\n    gt_bboxes = create_random_bboxes(8, w, h)\n    gt_bboxes_ignore = create_random_bboxes(2, w, h)\n    results['gt_bboxes'] = gt_bboxes\n    results['gt_bboxes_ignore'] = gt_bboxes_ignore\n    transform = dict(type='RandomCrop', crop_size=(h - 20, w - 20))\n    crop_module = build_from_cfg(transform, PIPELINES)\n    results = crop_module(results)\n    assert results['img'].shape[:2] == (h - 20, w - 20)\n    # All bboxes should be reserved after crop\n    assert results['img_shape'][:2] == (h - 20, w - 20)\n    assert results['gt_bboxes'].shape[0] == 8\n    assert results['gt_bboxes_ignore'].shape[0] == 2\n\n    def area(bboxes):\n        return np.prod(bboxes[:, 2:4] - bboxes[:, 0:2], axis=1)\n\n    assert (area(results['gt_bboxes']) <= area(gt_bboxes)).all()\n    assert (area(results['gt_bboxes_ignore']) <= area(gt_bboxes_ignore)).all()\n\n\ndef test_min_iou_random_crop():\n\n    def create_random_bboxes(num_bboxes, img_w, img_h):\n        bboxes_left_top = np.random.uniform(0, 0.5, size=(num_bboxes, 2))\n        bboxes_right_bottom = np.random.uniform(0.5, 1, size=(num_bboxes, 2))\n        bboxes = np.concatenate((bboxes_left_top, bboxes_right_bottom), 1)\n        bboxes = (bboxes * np.array([img_w, img_h, img_w, img_h])).astype(\n            np.int)\n        return bboxes\n\n    results = dict()\n    img = mmcv.imread(\n        osp.join(osp.dirname(__file__), '../data/color.jpg'), 'color')\n    results['img'] = img\n\n    results['img_shape'] = img.shape\n    results['ori_shape'] = img.shape\n    results['bbox_fields'] = ['gt_bboxes', 'gt_bboxes_ignore']\n    # Set initial values for default meta_keys\n    results['pad_shape'] = img.shape\n    results['scale_factor'] = 1.0\n    h, w, _ = img.shape\n    gt_bboxes = create_random_bboxes(1, w, h)\n    gt_bboxes_ignore = create_random_bboxes(1, w, h)\n    results['gt_bboxes'] = gt_bboxes\n    results['gt_bboxes_ignore'] = gt_bboxes_ignore\n    transform = dict(type='MinIoURandomCrop')\n    crop_module = build_from_cfg(transform, PIPELINES)\n\n    # Test for img_fields\n    results_test = copy.deepcopy(results)\n    results_test['img1'] = results_test['img']\n    results_test['img_fields'] = ['img', 'img1']\n    with pytest.raises(AssertionError):\n        crop_module(results_test)\n    results = crop_module(results)\n    patch = np.array([0, 0, results['img_shape'][1], results['img_shape'][0]])\n    ious = bbox_overlaps(patch.reshape(-1, 4),\n                         results['gt_bboxes']).reshape(-1)\n    ious_ignore = bbox_overlaps(\n        patch.reshape(-1, 4), results['gt_bboxes_ignore']).reshape(-1)\n    mode = crop_module.mode\n    if mode == 1:\n        assert np.equal(results['gt_bboxes'], gt_bboxes).all()\n        assert np.equal(results['gt_bboxes_ignore'], gt_bboxes_ignore).all()\n    else:\n        assert (ious >= mode).all()\n        assert (ious_ignore >= mode).all()\n\n\ndef test_pad():\n    # test assertion if both size_divisor and size is None\n    with pytest.raises(AssertionError):\n        transform = dict(type='Pad')\n        build_from_cfg(transform, PIPELINES)\n\n    transform = dict(type='Pad', size_divisor=32)\n    transform = build_from_cfg(transform, PIPELINES)\n    results = dict()\n    img = mmcv.imread(\n        osp.join(osp.dirname(__file__), '../data/color.jpg'), 'color')\n    original_img = copy.deepcopy(img)\n    results['img'] = img\n    results['img2'] = copy.deepcopy(img)\n    results['img_shape'] = img.shape\n    results['ori_shape'] = img.shape\n    # Set initial values for default meta_keys\n    results['pad_shape'] = img.shape\n    results['scale_factor'] = 1.0\n    results['img_fields'] = ['img', 'img2']\n\n    results = transform(results)\n    assert np.equal(results['img'], results['img2']).all()\n    # original img already divisible by 32\n    assert np.equal(results['img'], original_img).all()\n    img_shape = results['img'].shape\n    assert img_shape[0] % 32 == 0\n    assert img_shape[1] % 32 == 0\n\n    resize_transform = dict(\n        type='Resize', img_scale=(1333, 800), keep_ratio=True)\n    resize_module = build_from_cfg(resize_transform, PIPELINES)\n    results = resize_module(results)\n    results = transform(results)\n    img_shape = results['img'].shape\n    assert np.equal(results['img'], results['img2']).all()\n    assert img_shape[0] % 32 == 0\n    assert img_shape[1] % 32 == 0\n\n\ndef test_normalize():\n    img_norm_cfg = dict(\n        mean=[123.675, 116.28, 103.53],\n        std=[58.395, 57.12, 57.375],\n        to_rgb=True)\n    transform = dict(type='Normalize', **img_norm_cfg)\n    transform = build_from_cfg(transform, PIPELINES)\n    results = dict()\n    img = mmcv.imread(\n        osp.join(osp.dirname(__file__), '../data/color.jpg'), 'color')\n    original_img = copy.deepcopy(img)\n    results['img'] = img\n    results['img2'] = copy.deepcopy(img)\n    results['img_shape'] = img.shape\n    results['ori_shape'] = img.shape\n    # Set initial values for default meta_keys\n    results['pad_shape'] = img.shape\n    results['scale_factor'] = 1.0\n    results['img_fields'] = ['img', 'img2']\n\n    results = transform(results)\n    assert np.equal(results['img'], results['img2']).all()\n\n    mean = np.array(img_norm_cfg['mean'])\n    std = np.array(img_norm_cfg['std'])\n    converted_img = (original_img[..., ::-1] - mean) / std\n    assert np.allclose(results['img'], converted_img)\n\n\ndef test_albu_transform():\n    results = dict(\n        img_prefix=osp.join(osp.dirname(__file__), '../data'),\n        img_info=dict(filename='color.jpg'))\n\n    # Define simple pipeline\n    load = dict(type='LoadImageFromFile')\n    load = build_from_cfg(load, PIPELINES)\n\n    albu_transform = dict(\n        type='Albu', transforms=[dict(type='ChannelShuffle', p=1)])\n    albu_transform = build_from_cfg(albu_transform, PIPELINES)\n\n    normalize = dict(type='Normalize', mean=[0] * 3, std=[0] * 3, to_rgb=True)\n    normalize = build_from_cfg(normalize, PIPELINES)\n\n    # Execute transforms\n    results = load(results)\n    results = albu_transform(results)\n    results = normalize(results)\n\n    assert results['img'].dtype == np.float32\n"""
tools/convert_datasets/cityscapes.py,0,"b""import argparse\nimport glob\nimport os.path as osp\n\nimport cityscapesscripts.helpers.labels as CSLabels\nimport mmcv\nimport numpy as np\nimport pycocotools.mask as maskUtils\n\n\ndef collect_files(img_dir, gt_dir):\n    suffix = 'leftImg8bit.png'\n    files = []\n    for img_file in glob.glob(osp.join(img_dir, '**/*.png')):\n        assert img_file.endswith(suffix), img_file\n        inst_file = gt_dir + img_file[\n            len(img_dir):-len(suffix)] + 'gtFine_instanceIds.png'\n        # Note that labelIds are not converted to trainId for seg map\n        segm_file = gt_dir + img_file[\n            len(img_dir):-len(suffix)] + 'gtFine_labelIds.png'\n        files.append((img_file, inst_file, segm_file))\n    assert len(files), f'No images found in {img_dir}'\n    print(f'Loaded {len(files)} images from {img_dir}')\n\n    return files\n\n\ndef collect_annotations(files, nproc=1):\n    print('Loading annotation images')\n    if nproc > 1:\n        images = mmcv.track_parallel_progress(\n            load_img_info, files, nproc=nproc)\n    else:\n        images = mmcv.track_progress(load_img_info, files)\n\n    return images\n\n\ndef load_img_info(files):\n    img_file, inst_file, segm_file = files\n    inst_img = mmcv.imread(inst_file, 'unchanged')\n    # ids < 24 are stuff labels (filtering them first is about 5% faster)\n    unique_inst_ids = np.unique(inst_img[inst_img >= 24])\n    anno_info = []\n    for inst_id in unique_inst_ids:\n        # For non-crowd annotations, inst_id // 1000 is the label_id\n        # Crowd annotations have <1000 instance ids\n        label_id = inst_id // 1000 if inst_id >= 1000 else inst_id\n        label = CSLabels.id2label[label_id]\n        if not label.hasInstances or label.ignoreInEval:\n            continue\n\n        category_id = label.id\n        iscrowd = int(inst_id < 1000)\n        mask = np.asarray(inst_img == inst_id, dtype=np.uint8, order='F')\n        mask_rle = maskUtils.encode(mask[:, :, None])[0]\n\n        area = maskUtils.area(mask_rle)\n        # convert to COCO style XYWH format\n        bbox = maskUtils.toBbox(mask_rle)\n\n        # for json encoding\n        mask_rle['counts'] = mask_rle['counts'].decode()\n\n        anno = dict(\n            iscrowd=iscrowd,\n            category_id=category_id,\n            bbox=bbox.tolist(),\n            area=area.tolist(),\n            segmentation=mask_rle)\n        anno_info.append(anno)\n    video_name = osp.basename(osp.dirname(img_file))\n    img_info = dict(\n        # remove img_prefix for filename\n        file_name=osp.join(video_name, osp.basename(img_file)),\n        height=inst_img.shape[0],\n        width=inst_img.shape[1],\n        anno_info=anno_info,\n        segm_file=osp.join(video_name, osp.basename(segm_file)))\n\n    return img_info\n\n\ndef cvt_annotations(image_infos, out_json_name):\n    out_json = dict()\n    img_id = 0\n    ann_id = 0\n    out_json['images'] = []\n    out_json['categories'] = []\n    out_json['annotations'] = []\n    for image_info in image_infos:\n        image_info['id'] = img_id\n        anno_infos = image_info.pop('anno_info')\n        out_json['images'].append(image_info)\n        for anno_info in anno_infos:\n            anno_info['image_id'] = img_id\n            anno_info['id'] = ann_id\n            out_json['annotations'].append(anno_info)\n            ann_id += 1\n        img_id += 1\n    for label in CSLabels.labels:\n        if label.hasInstances and not label.ignoreInEval:\n            cat = dict(id=label.id, name=label.name)\n            out_json['categories'].append(cat)\n\n    if len(out_json['annotations']) == 0:\n        out_json.pop('annotations')\n\n    mmcv.dump(out_json, out_json_name)\n    return out_json\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description='Convert Cityscapes annotations to COCO format')\n    parser.add_argument('cityscapes_path', help='cityscapes data path')\n    parser.add_argument('--img-dir', default='leftImg8bit', type=str)\n    parser.add_argument('--gt-dir', default='gtFine', type=str)\n    parser.add_argument('-o', '--out-dir', help='output path')\n    parser.add_argument(\n        '--nproc', default=1, type=int, help='number of process')\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n    cityscapes_path = args.cityscapes_path\n    out_dir = args.out_dir if args.out_dir else cityscapes_path\n    mmcv.mkdir_or_exist(out_dir)\n\n    img_dir = osp.join(cityscapes_path, args.img_dir)\n    gt_dir = osp.join(cityscapes_path, args.gt_dir)\n\n    set_name = dict(\n        train='instancesonly_filtered_gtFine_train.json',\n        val='instancesonly_filtered_gtFine_val.json',\n        test='instancesonly_filtered_gtFine_test.json')\n\n    for split, json_name in set_name.items():\n        print(f'Converting {split} into {json_name}')\n        with mmcv.Timer(\n                print_tmpl='It tooks {}s to convert Cityscapes annotation'):\n            files = collect_files(\n                osp.join(img_dir, split), osp.join(gt_dir, split))\n            image_infos = collect_annotations(files, nproc=args.nproc)\n            cvt_annotations(image_infos, osp.join(out_dir, json_name))\n\n\nif __name__ == '__main__':\n    main()\n"""
tools/convert_datasets/pascal_voc.py,0,"b'import argparse\nimport os.path as osp\nimport xml.etree.ElementTree as ET\n\nimport mmcv\nimport numpy as np\n\nfrom mmdet.core import voc_classes\n\nlabel_ids = {name: i for i, name in enumerate(voc_classes())}\n\n\ndef parse_xml(args):\n    xml_path, img_path = args\n    tree = ET.parse(xml_path)\n    root = tree.getroot()\n    size = root.find(\'size\')\n    w = int(size.find(\'width\').text)\n    h = int(size.find(\'height\').text)\n    bboxes = []\n    labels = []\n    bboxes_ignore = []\n    labels_ignore = []\n    for obj in root.findall(\'object\'):\n        name = obj.find(\'name\').text\n        label = label_ids[name]\n        difficult = int(obj.find(\'difficult\').text)\n        bnd_box = obj.find(\'bndbox\')\n        bbox = [\n            int(bnd_box.find(\'xmin\').text),\n            int(bnd_box.find(\'ymin\').text),\n            int(bnd_box.find(\'xmax\').text),\n            int(bnd_box.find(\'ymax\').text)\n        ]\n        if difficult:\n            bboxes_ignore.append(bbox)\n            labels_ignore.append(label)\n        else:\n            bboxes.append(bbox)\n            labels.append(label)\n    if not bboxes:\n        bboxes = np.zeros((0, 4))\n        labels = np.zeros((0, ))\n    else:\n        bboxes = np.array(bboxes, ndmin=2) - 1\n        labels = np.array(labels)\n    if not bboxes_ignore:\n        bboxes_ignore = np.zeros((0, 4))\n        labels_ignore = np.zeros((0, ))\n    else:\n        bboxes_ignore = np.array(bboxes_ignore, ndmin=2) - 1\n        labels_ignore = np.array(labels_ignore)\n    annotation = {\n        \'filename\': img_path,\n        \'width\': w,\n        \'height\': h,\n        \'ann\': {\n            \'bboxes\': bboxes.astype(np.float32),\n            \'labels\': labels.astype(np.int64),\n            \'bboxes_ignore\': bboxes_ignore.astype(np.float32),\n            \'labels_ignore\': labels_ignore.astype(np.int64)\n        }\n    }\n    return annotation\n\n\ndef cvt_annotations(devkit_path, years, split, out_file):\n    if not isinstance(years, list):\n        years = [years]\n    annotations = []\n    for year in years:\n        filelist = osp.join(devkit_path,\n                            f\'VOC{year}/ImageSets/Main/{split}.txt\')\n        if not osp.isfile(filelist):\n            print(f\'filelist does not exist: {filelist}, \'\n                  f\'skip voc{year} {split}\')\n            return\n        img_names = mmcv.list_from_file(filelist)\n        xml_paths = [\n            osp.join(devkit_path, f\'VOC{year}/Annotations/{img_name}.xml\')\n            for img_name in img_names\n        ]\n        img_paths = [\n            f\'VOC{year}/JPEGImages/{img_name}.jpg\' for img_name in img_names\n        ]\n        part_annotations = mmcv.track_progress(parse_xml,\n                                               list(zip(xml_paths, img_paths)))\n        annotations.extend(part_annotations)\n    mmcv.dump(annotations, out_file)\n    return annotations\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=\'Convert PASCAL VOC annotations to mmdetection format\')\n    parser.add_argument(\'devkit_path\', help=\'pascal voc devkit path\')\n    parser.add_argument(\'-o\', \'--out-dir\', help=\'output path\')\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n    devkit_path = args.devkit_path\n    out_dir = args.out_dir if args.out_dir else devkit_path\n    mmcv.mkdir_or_exist(out_dir)\n\n    years = []\n    if osp.isdir(osp.join(devkit_path, \'VOC2007\')):\n        years.append(\'2007\')\n    if osp.isdir(osp.join(devkit_path, \'VOC2012\')):\n        years.append(\'2012\')\n    if \'2007\' in years and \'2012\' in years:\n        years.append([\'2007\', \'2012\'])\n    if not years:\n        raise IOError(f\'The devkit path {devkit_path} contains neither \'\n                      \'""VOC2007"" nor ""VOC2012"" subfolder\')\n    for year in years:\n        if year == \'2007\':\n            prefix = \'voc07\'\n        elif year == \'2012\':\n            prefix = \'voc12\'\n        elif year == [\'2007\', \'2012\']:\n            prefix = \'voc0712\'\n        for split in [\'train\', \'val\', \'trainval\']:\n            dataset_name = prefix + \'_\' + split\n            print(f\'processing {dataset_name} ...\')\n            cvt_annotations(devkit_path, year, split,\n                            osp.join(out_dir, dataset_name + \'.pkl\'))\n        if not isinstance(year, list):\n            dataset_name = prefix + \'_test\'\n            print(f\'processing {dataset_name} ...\')\n            cvt_annotations(devkit_path, year, \'test\',\n                            osp.join(out_dir, dataset_name + \'.pkl\'))\n    print(\'Done!\')\n\n\nif __name__ == \'__main__\':\n    main()\n'"
configs/_base_/datasets/cityscapes_detection.py,0,"b""dataset_type = 'CityscapesDataset'\ndata_root = 'data/cityscapes/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(\n        type='Resize', img_scale=[(2048, 800), (2048, 1024)], keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(2048, 1024),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    samples_per_gpu=1,\n    workers_per_gpu=2,\n    train=dict(\n        type='RepeatDataset',\n        times=8,\n        dataset=dict(\n            type=dataset_type,\n            ann_file=data_root +\n            'annotations/instancesonly_filtered_gtFine_train.json',\n            img_prefix=data_root + 'leftImg8bit/train/',\n            pipeline=train_pipeline)),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root +\n        'annotations/instancesonly_filtered_gtFine_val.json',\n        img_prefix=data_root + 'leftImg8bit/val/',\n        pipeline=test_pipeline),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root +\n        'annotations/instancesonly_filtered_gtFine_test.json',\n        img_prefix=data_root + 'leftImg8bit/test/',\n        pipeline=test_pipeline))\nevaluation = dict(interval=1, metric='bbox')\n"""
configs/_base_/datasets/cityscapes_instance.py,0,"b""_base_ = './cityscapes_detection.py'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n    dict(\n        type='Resize', img_scale=[(2048, 800), (2048, 1024)], keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']),\n]\ndata = dict(train=dict(dataset=dict(pipeline=train_pipeline)))\nevaluation = dict(metric=['bbox', 'segm'])\n"""
configs/_base_/datasets/coco_detection.py,0,"b""dataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    samples_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        pipeline=train_pipeline),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        pipeline=test_pipeline),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        pipeline=test_pipeline))\nevaluation = dict(interval=1, metric='bbox')\n"""
configs/_base_/datasets/coco_instance.py,0,"b""_base_ = 'coco_detection.py'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']),\n]\ndata = dict(train=dict(pipeline=train_pipeline))\nevaluation = dict(metric=['bbox', 'segm'])\n"""
configs/_base_/datasets/coco_instance_semantic.py,0,"b""_base_ = 'coco_detection.py'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='LoadAnnotations', with_bbox=True, with_mask=True, with_seg=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='SegRescale', scale_factor=1 / 8),\n    dict(type='DefaultFormatBundle'),\n    dict(\n        type='Collect',\n        keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks', 'gt_semantic_seg']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1333, 800),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip', flip_ratio=0.5),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    train=dict(\n        seg_prefix=data_root + 'stuffthingmaps/train2017/',\n        pipeline=train_pipeline),\n    val=dict(pipeline=test_pipeline),\n    test=dict(pipeline=test_pipeline))\nevaluation = dict(metric=['bbox', 'segm'])\n"""
configs/_base_/datasets/lvis_instance.py,0,"b""_base_ = 'coco_instance.py'\ndataset_type = 'LVISDataset'\ndata_root = 'data/lvis/'\ndata = dict(\n    samples_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type='ClassBalancedDataset',\n        oversample_thr=1e-3,\n        dataset=dict(\n            type=dataset_type,\n            ann_file=data_root + 'annotations/lvis_v0.5_train.json',\n            img_prefix=data_root + 'train2017/')),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/lvis_v0.5_val.json',\n        img_prefix=data_root + 'val2017/'),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/lvis_v0.5_val.json',\n        img_prefix=data_root + 'val2017/'))\nevaluation = dict(metric=['bbox', 'segm'])\n"""
configs/_base_/datasets/voc0712.py,0,"b""# dataset settings\ndataset_type = 'VOCDataset'\ndata_root = 'data/VOCdevkit/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(type='Resize', img_scale=(1000, 600), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(1000, 600),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='Pad', size_divisor=32),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    samples_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type='RepeatDataset',\n        times=3,\n        dataset=dict(\n            type=dataset_type,\n            ann_file=[\n                data_root + 'VOC2007/ImageSets/Main/trainval.txt',\n                data_root + 'VOC2012/ImageSets/Main/trainval.txt'\n            ],\n            img_prefix=[data_root + 'VOC2007/', data_root + 'VOC2012/'],\n            pipeline=train_pipeline)),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'VOC2007/ImageSets/Main/test.txt',\n        img_prefix=data_root + 'VOC2007/',\n        pipeline=test_pipeline),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'VOC2007/ImageSets/Main/test.txt',\n        img_prefix=data_root + 'VOC2007/',\n        pipeline=test_pipeline))\nevaluation = dict(interval=1, metric='mAP')\n"""
configs/_base_/datasets/wider_face.py,0,"b""# dataset settings\ndataset_type = 'WIDERFaceDataset'\ndata_root = 'data/WIDERFace/'\nimg_norm_cfg = dict(mean=[123.675, 116.28, 103.53], std=[1, 1, 1], to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile', to_float32=True),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(\n        type='PhotoMetricDistortion',\n        brightness_delta=32,\n        contrast_range=(0.5, 1.5),\n        saturation_range=(0.5, 1.5),\n        hue_delta=18),\n    dict(\n        type='Expand',\n        mean=img_norm_cfg['mean'],\n        to_rgb=img_norm_cfg['to_rgb'],\n        ratio_range=(1, 4)),\n    dict(\n        type='MinIoURandomCrop',\n        min_ious=(0.1, 0.3, 0.5, 0.7, 0.9),\n        min_crop_size=0.3),\n    dict(type='Resize', img_scale=(300, 300), keep_ratio=False),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(300, 300),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=False),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']),\n        ])\n]\ndata = dict(\n    samples_per_gpu=60,\n    workers_per_gpu=2,\n    train=dict(\n        type='RepeatDataset',\n        times=2,\n        dataset=dict(\n            type=dataset_type,\n            ann_file=data_root + 'train.txt',\n            img_prefix=data_root + 'WIDER_train/',\n            min_size=17,\n            pipeline=train_pipeline)),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'val.txt',\n        img_prefix=data_root + 'WIDER_val/',\n        pipeline=test_pipeline),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'val.txt',\n        img_prefix=data_root + 'WIDER_val/',\n        pipeline=test_pipeline))\n"""
configs/_base_/models/cascade_mask_rcnn_r50_fpn.py,0,"b""# model settings\nmodel = dict(\n    type='CascadeRCNN',\n    pretrained='torchvision://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        norm_eval=True,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_generator=dict(\n            type='AnchorGenerator',\n            scales=[8],\n            ratios=[0.5, 1.0, 2.0],\n            strides=[4, 8, 16, 32, 64]),\n        bbox_coder=dict(\n            type='DeltaXYWHBBoxCoder',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[1.0, 1.0, 1.0, 1.0]),\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    roi_head=dict(\n        type='CascadeRoIHead',\n        num_stages=3,\n        stage_loss_weights=[1, 0.5, 0.25],\n        bbox_roi_extractor=dict(\n            type='SingleRoIExtractor',\n            roi_layer=dict(type='RoIAlign', out_size=7, sample_num=0),\n            out_channels=256,\n            featmap_strides=[4, 8, 16, 32]),\n        bbox_head=[\n            dict(\n                type='Shared2FCBBoxHead',\n                in_channels=256,\n                fc_out_channels=1024,\n                roi_feat_size=7,\n                num_classes=80,\n                bbox_coder=dict(\n                    type='DeltaXYWHBBoxCoder',\n                    target_means=[0., 0., 0., 0.],\n                    target_stds=[0.1, 0.1, 0.2, 0.2]),\n                reg_class_agnostic=True,\n                loss_cls=dict(\n                    type='CrossEntropyLoss',\n                    use_sigmoid=False,\n                    loss_weight=1.0),\n                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,\n                               loss_weight=1.0)),\n            dict(\n                type='Shared2FCBBoxHead',\n                in_channels=256,\n                fc_out_channels=1024,\n                roi_feat_size=7,\n                num_classes=80,\n                bbox_coder=dict(\n                    type='DeltaXYWHBBoxCoder',\n                    target_means=[0., 0., 0., 0.],\n                    target_stds=[0.05, 0.05, 0.1, 0.1]),\n                reg_class_agnostic=True,\n                loss_cls=dict(\n                    type='CrossEntropyLoss',\n                    use_sigmoid=False,\n                    loss_weight=1.0),\n                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,\n                               loss_weight=1.0)),\n            dict(\n                type='Shared2FCBBoxHead',\n                in_channels=256,\n                fc_out_channels=1024,\n                roi_feat_size=7,\n                num_classes=80,\n                bbox_coder=dict(\n                    type='DeltaXYWHBBoxCoder',\n                    target_means=[0., 0., 0., 0.],\n                    target_stds=[0.033, 0.033, 0.067, 0.067]),\n                reg_class_agnostic=True,\n                loss_cls=dict(\n                    type='CrossEntropyLoss',\n                    use_sigmoid=False,\n                    loss_weight=1.0),\n                loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0))\n        ],\n        mask_roi_extractor=dict(\n            type='SingleRoIExtractor',\n            roi_layer=dict(type='RoIAlign', out_size=14, sample_num=0),\n            out_channels=256,\n            featmap_strides=[4, 8, 16, 32]),\n        mask_head=dict(\n            type='FCNMaskHead',\n            num_convs=4,\n            in_channels=256,\n            conv_out_channels=256,\n            num_classes=80,\n            loss_mask=dict(\n                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            match_low_quality=True,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=[\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.5,\n                neg_iou_thr=0.5,\n                min_pos_iou=0.5,\n                match_low_quality=False,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.6,\n                neg_iou_thr=0.6,\n                min_pos_iou=0.6,\n                match_low_quality=False,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.7,\n                min_pos_iou=0.7,\n                match_low_quality=False,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False)\n    ])\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\n"""
configs/_base_/models/cascade_rcnn_r50_fpn.py,0,"b""# model settings\nmodel = dict(\n    type='CascadeRCNN',\n    pretrained='torchvision://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        norm_eval=True,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_generator=dict(\n            type='AnchorGenerator',\n            scales=[8],\n            ratios=[0.5, 1.0, 2.0],\n            strides=[4, 8, 16, 32, 64]),\n        bbox_coder=dict(\n            type='DeltaXYWHBBoxCoder',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[1.0, 1.0, 1.0, 1.0]),\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    roi_head=dict(\n        type='CascadeRoIHead',\n        num_stages=3,\n        stage_loss_weights=[1, 0.5, 0.25],\n        bbox_roi_extractor=dict(\n            type='SingleRoIExtractor',\n            roi_layer=dict(type='RoIAlign', out_size=7, sample_num=0),\n            out_channels=256,\n            featmap_strides=[4, 8, 16, 32]),\n        bbox_head=[\n            dict(\n                type='Shared2FCBBoxHead',\n                in_channels=256,\n                fc_out_channels=1024,\n                roi_feat_size=7,\n                num_classes=80,\n                bbox_coder=dict(\n                    type='DeltaXYWHBBoxCoder',\n                    target_means=[0., 0., 0., 0.],\n                    target_stds=[0.1, 0.1, 0.2, 0.2]),\n                reg_class_agnostic=True,\n                loss_cls=dict(\n                    type='CrossEntropyLoss',\n                    use_sigmoid=False,\n                    loss_weight=1.0),\n                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,\n                               loss_weight=1.0)),\n            dict(\n                type='Shared2FCBBoxHead',\n                in_channels=256,\n                fc_out_channels=1024,\n                roi_feat_size=7,\n                num_classes=80,\n                bbox_coder=dict(\n                    type='DeltaXYWHBBoxCoder',\n                    target_means=[0., 0., 0., 0.],\n                    target_stds=[0.05, 0.05, 0.1, 0.1]),\n                reg_class_agnostic=True,\n                loss_cls=dict(\n                    type='CrossEntropyLoss',\n                    use_sigmoid=False,\n                    loss_weight=1.0),\n                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,\n                               loss_weight=1.0)),\n            dict(\n                type='Shared2FCBBoxHead',\n                in_channels=256,\n                fc_out_channels=1024,\n                roi_feat_size=7,\n                num_classes=80,\n                bbox_coder=dict(\n                    type='DeltaXYWHBBoxCoder',\n                    target_means=[0., 0., 0., 0.],\n                    target_stds=[0.033, 0.033, 0.067, 0.067]),\n                reg_class_agnostic=True,\n                loss_cls=dict(\n                    type='CrossEntropyLoss',\n                    use_sigmoid=False,\n                    loss_weight=1.0),\n                loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0))\n        ]))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            match_low_quality=True,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=[\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.5,\n                neg_iou_thr=0.5,\n                min_pos_iou=0.5,\n                match_low_quality=False,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.6,\n                neg_iou_thr=0.6,\n                min_pos_iou=0.6,\n                match_low_quality=False,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.7,\n                min_pos_iou=0.7,\n                match_low_quality=False,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            pos_weight=-1,\n            debug=False)\n    ])\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100))\n"""
configs/_base_/models/fast_rcnn_r50_fpn.py,0,"b""# model settings\nmodel = dict(\n    type='FastRCNN',\n    pretrained='torchvision://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        norm_eval=True,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    roi_head=dict(\n        type='StandardRoIHead',\n        bbox_roi_extractor=dict(\n            type='SingleRoIExtractor',\n            roi_layer=dict(type='RoIAlign', out_size=7, sample_num=0),\n            out_channels=256,\n            featmap_strides=[4, 8, 16, 32]),\n        bbox_head=dict(\n            type='Shared2FCBBoxHead',\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=80,\n            bbox_coder=dict(\n                type='DeltaXYWHBBoxCoder',\n                target_means=[0., 0., 0., 0.],\n                target_stds=[0.1, 0.1, 0.2, 0.2]),\n            reg_class_agnostic=False,\n            loss_cls=dict(\n                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n            loss_bbox=dict(type='L1Loss', loss_weight=1.0))))\n# model training and testing settings\ntrain_cfg = dict(\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            match_low_quality=False,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100))\n"""
configs/_base_/models/faster_rcnn_r50_caffe_c4.py,0,"b""# model settings\nnorm_cfg = dict(type='BN', requires_grad=False)\nmodel = dict(\n    type='FasterRCNN',\n    pretrained='open-mmlab://detectron2/resnet50_caffe',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=3,\n        strides=(1, 2, 2),\n        dilations=(1, 1, 1),\n        out_indices=(2, ),\n        frozen_stages=1,\n        norm_cfg=norm_cfg,\n        norm_eval=True,\n        style='caffe'),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=1024,\n        feat_channels=1024,\n        anchor_generator=dict(\n            type='AnchorGenerator',\n            scales=[2, 4, 8, 16, 32],\n            ratios=[0.5, 1.0, 2.0],\n            strides=[16]),\n        bbox_coder=dict(\n            type='DeltaXYWHBBoxCoder',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[1.0, 1.0, 1.0, 1.0]),\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),\n    roi_head=dict(\n        type='StandardRoIHead',\n        shared_head=dict(\n            type='ResLayer',\n            depth=50,\n            stage=3,\n            stride=2,\n            dilation=1,\n            style='caffe',\n            norm_cfg=norm_cfg,\n            norm_eval=True),\n        bbox_roi_extractor=dict(\n            type='SingleRoIExtractor',\n            roi_layer=dict(type='RoIAlign', out_size=14, sample_num=0),\n            out_channels=1024,\n            featmap_strides=[16]),\n        bbox_head=dict(\n            type='BBoxHead',\n            with_avg_pool=True,\n            roi_feat_size=7,\n            in_channels=2048,\n            num_classes=80,\n            bbox_coder=dict(\n                type='DeltaXYWHBBoxCoder',\n                target_means=[0., 0., 0., 0.],\n                target_stds=[0.1, 0.1, 0.2, 0.2]),\n            reg_class_agnostic=False,\n            loss_cls=dict(\n                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n            loss_bbox=dict(type='L1Loss', loss_weight=1.0))))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            match_low_quality=True,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=12000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            match_low_quality=False,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=6000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100))\n"""
configs/_base_/models/faster_rcnn_r50_fpn.py,0,"b""model = dict(\n    type='FasterRCNN',\n    pretrained='torchvision://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        norm_eval=True,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_generator=dict(\n            type='AnchorGenerator',\n            scales=[8],\n            ratios=[0.5, 1.0, 2.0],\n            strides=[4, 8, 16, 32, 64]),\n        bbox_coder=dict(\n            type='DeltaXYWHBBoxCoder',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[1.0, 1.0, 1.0, 1.0]),\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),\n    roi_head=dict(\n        type='StandardRoIHead',\n        bbox_roi_extractor=dict(\n            type='SingleRoIExtractor',\n            roi_layer=dict(type='RoIAlign', out_size=7, sample_num=0),\n            out_channels=256,\n            featmap_strides=[4, 8, 16, 32]),\n        bbox_head=dict(\n            type='Shared2FCBBoxHead',\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=80,\n            bbox_coder=dict(\n                type='DeltaXYWHBBoxCoder',\n                target_means=[0., 0., 0., 0.],\n                target_stds=[0.1, 0.1, 0.2, 0.2]),\n            reg_class_agnostic=False,\n            loss_cls=dict(\n                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n            loss_bbox=dict(type='L1Loss', loss_weight=1.0))))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            match_low_quality=True,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=-1,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            match_low_quality=False,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100)\n    # soft-nms is also supported for rcnn testing\n    # e.g., nms=dict(type='soft_nms', iou_thr=0.5, min_score=0.05)\n)\n"""
configs/_base_/models/mask_rcnn_r50_caffe_c4.py,0,"b""# model settings\nnorm_cfg = dict(type='BN', requires_grad=False)\nmodel = dict(\n    type='MaskRCNN',\n    pretrained='open-mmlab://detectron2/resnet50_caffe',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=3,\n        strides=(1, 2, 2),\n        dilations=(1, 1, 1),\n        out_indices=(2, ),\n        frozen_stages=1,\n        norm_cfg=norm_cfg,\n        norm_eval=True,\n        style='caffe'),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=1024,\n        feat_channels=1024,\n        anchor_generator=dict(\n            type='AnchorGenerator',\n            scales=[2, 4, 8, 16, 32],\n            ratios=[0.5, 1.0, 2.0],\n            strides=[16]),\n        bbox_coder=dict(\n            type='DeltaXYWHBBoxCoder',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[1.0, 1.0, 1.0, 1.0]),\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),\n    roi_head=dict(\n        type='StandardRoIHead',\n        shared_head=dict(\n            type='ResLayer',\n            depth=50,\n            stage=3,\n            stride=2,\n            dilation=1,\n            style='caffe',\n            norm_cfg=norm_cfg,\n            norm_eval=True),\n        bbox_roi_extractor=dict(\n            type='SingleRoIExtractor',\n            roi_layer=dict(type='RoIAlign', out_size=14, sample_num=0),\n            out_channels=1024,\n            featmap_strides=[16]),\n        bbox_head=dict(\n            type='BBoxHead',\n            with_avg_pool=True,\n            roi_feat_size=7,\n            in_channels=2048,\n            num_classes=80,\n            bbox_coder=dict(\n                type='DeltaXYWHBBoxCoder',\n                target_means=[0., 0., 0., 0.],\n                target_stds=[0.1, 0.1, 0.2, 0.2]),\n            reg_class_agnostic=False,\n            loss_cls=dict(\n                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n            loss_bbox=dict(type='L1Loss', loss_weight=1.0)),\n        mask_roi_extractor=None,\n        mask_head=dict(\n            type='FCNMaskHead',\n            num_convs=0,\n            in_channels=2048,\n            conv_out_channels=256,\n            num_classes=80,\n            loss_mask=dict(\n                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            match_low_quality=True,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=12000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            match_low_quality=False,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        mask_size=14,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=6000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\n"""
configs/_base_/models/mask_rcnn_r50_fpn.py,0,"b""# model settings\nmodel = dict(\n    type='MaskRCNN',\n    pretrained='torchvision://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        norm_eval=True,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_generator=dict(\n            type='AnchorGenerator',\n            scales=[8],\n            ratios=[0.5, 1.0, 2.0],\n            strides=[4, 8, 16, 32, 64]),\n        bbox_coder=dict(\n            type='DeltaXYWHBBoxCoder',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[1.0, 1.0, 1.0, 1.0]),\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),\n    roi_head=dict(\n        type='StandardRoIHead',\n        bbox_roi_extractor=dict(\n            type='SingleRoIExtractor',\n            roi_layer=dict(type='RoIAlign', out_size=7, sample_num=0),\n            out_channels=256,\n            featmap_strides=[4, 8, 16, 32]),\n        bbox_head=dict(\n            type='Shared2FCBBoxHead',\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=80,\n            bbox_coder=dict(\n                type='DeltaXYWHBBoxCoder',\n                target_means=[0., 0., 0., 0.],\n                target_stds=[0.1, 0.1, 0.2, 0.2]),\n            reg_class_agnostic=False,\n            loss_cls=dict(\n                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n            loss_bbox=dict(type='L1Loss', loss_weight=1.0)),\n        mask_roi_extractor=dict(\n            type='SingleRoIExtractor',\n            roi_layer=dict(type='RoIAlign', out_size=14, sample_num=0),\n            out_channels=256,\n            featmap_strides=[4, 8, 16, 32]),\n        mask_head=dict(\n            type='FCNMaskHead',\n            num_convs=4,\n            in_channels=256,\n            conv_out_channels=256,\n            num_classes=80,\n            loss_mask=dict(\n                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            match_low_quality=True,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=-1,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            match_low_quality=True,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        mask_size=28,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\n"""
configs/_base_/models/retinanet_r50_fpn.py,0,"b""# model settings\nmodel = dict(\n    type='RetinaNet',\n    pretrained='torchvision://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        norm_eval=True,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        start_level=1,\n        add_extra_convs='on_input',\n        num_outs=5),\n    bbox_head=dict(\n        type='RetinaHead',\n        num_classes=80,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        anchor_generator=dict(\n            type='AnchorGenerator',\n            octave_base_scale=4,\n            scales_per_octave=3,\n            ratios=[0.5, 1.0, 2.0],\n            strides=[8, 16, 32, 64, 128]),\n        bbox_coder=dict(\n            type='DeltaXYWHBBoxCoder',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[1.0, 1.0, 1.0, 1.0]),\n        loss_cls=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_bbox=dict(type='L1Loss', loss_weight=1.0)))\n# training and testing settings\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.4,\n        min_pos_iou=0,\n        ignore_iof_thr=-1),\n    allowed_border=-1,\n    pos_weight=-1,\n    debug=False)\ntest_cfg = dict(\n    nms_pre=1000,\n    min_bbox_size=0,\n    score_thr=0.05,\n    nms=dict(type='nms', iou_thr=0.5),\n    max_per_img=100)\n"""
configs/_base_/models/rpn_r50_caffe_c4.py,0,"b""# model settings\nmodel = dict(\n    type='RPN',\n    pretrained='open-mmlab://detectron2/resnet50_caffe',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=3,\n        strides=(1, 2, 2),\n        dilations=(1, 1, 1),\n        out_indices=(2, ),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=False),\n        norm_eval=True,\n        style='caffe'),\n    neck=None,\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=1024,\n        feat_channels=1024,\n        anchor_generator=dict(\n            type='AnchorGenerator',\n            scales=[2, 4, 8, 16, 32],\n            ratios=[0.5, 1.0, 2.0],\n            strides=[16]),\n        bbox_coder=dict(\n            type='DeltaXYWHBBoxCoder',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[1.0, 1.0, 1.0, 1.0]),\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='L1Loss', loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=12000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0))\n"""
configs/_base_/models/rpn_r50_fpn.py,0,"b""# model settings\nmodel = dict(\n    type='RPN',\n    pretrained='torchvision://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        norm_eval=True,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_generator=dict(\n            type='AnchorGenerator',\n            scales=[8],\n            ratios=[0.5, 1.0, 2.0],\n            strides=[4, 8, 16, 32, 64]),\n        bbox_coder=dict(\n            type='DeltaXYWHBBoxCoder',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[1.0, 1.0, 1.0, 1.0]),\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='L1Loss', loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0))\n"""
configs/_base_/models/ssd300.py,0,"b""# model settings\ninput_size = 300\nmodel = dict(\n    type='SingleStageDetector',\n    pretrained='open-mmlab://vgg16_caffe',\n    backbone=dict(\n        type='SSDVGG',\n        input_size=input_size,\n        depth=16,\n        with_last_pool=False,\n        ceil_mode=True,\n        out_indices=(3, 4),\n        out_feature_indices=(22, 34),\n        l2_norm_scale=20),\n    neck=None,\n    bbox_head=dict(\n        type='SSDHead',\n        in_channels=(512, 1024, 512, 256, 256, 256),\n        num_classes=80,\n        anchor_generator=dict(\n            type='SSDAnchorGenerator',\n            scale_major=False,\n            input_size=input_size,\n            basesize_ratio_range=(0.15, 0.9),\n            strides=[8, 16, 32, 64, 100, 300],\n            ratios=[[2], [2, 3], [2, 3], [2, 3], [2], [2]]),\n        bbox_coder=dict(\n            type='DeltaXYWHBBoxCoder',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[0.1, 0.1, 0.2, 0.2])))\ncudnn_benchmark = True\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n        min_pos_iou=0.,\n        ignore_iof_thr=-1,\n        gt_max_assign_all=False),\n    smoothl1_beta=1.,\n    allowed_border=-1,\n    pos_weight=-1,\n    neg_pos_ratio=3,\n    debug=False)\ntest_cfg = dict(\n    nms=dict(type='nms', iou_thr=0.45),\n    min_bbox_size=0,\n    score_thr=0.02,\n    max_per_img=200)\n"""
configs/_base_/schedules/schedule_1x.py,0,"b""# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=None)\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=0.001,\n    step=[8, 11])\ntotal_epochs = 12\n"""
configs/_base_/schedules/schedule_20e.py,0,"b""_base_ = './schedule_1x.py'\n# learning policy\nlr_config = dict(step=[16, 19])\ntotal_epochs = 20\n"""
configs/_base_/schedules/schedule_2x.py,0,"b""_base_ = './schedule_1x.py'\n# learning policy\nlr_config = dict(step=[16, 22])\ntotal_epochs = 24\n"""
mmdet/core/anchor/__init__.py,0,"b""from .anchor_generator import AnchorGenerator, LegacyAnchorGenerator\nfrom .builder import ANCHOR_GENERATORS, build_anchor_generator\nfrom .point_generator import PointGenerator\nfrom .utils import anchor_inside_flags, calc_region, images_to_levels\n\n__all__ = [\n    'AnchorGenerator', 'LegacyAnchorGenerator', 'anchor_inside_flags',\n    'PointGenerator', 'images_to_levels', 'calc_region',\n    'build_anchor_generator', 'ANCHOR_GENERATORS'\n]\n"""
mmdet/core/anchor/anchor_generator.py,19,"b'import mmcv\nimport numpy as np\nimport torch\nfrom torch.nn.modules.utils import _pair\n\nfrom .builder import ANCHOR_GENERATORS\n\n\n@ANCHOR_GENERATORS.register_module()\nclass AnchorGenerator(object):\n    """"""Standard anchor generator for 2D anchor-based detectors\n\n    Args:\n        strides (list[int] | list[tuple[int, int]]): Strides of anchors\n            in multiple feature levels.\n        ratios (list[float]): The list of ratios between the height and width\n            of anchors in a single level.\n        scales (list[int] | None): Anchor scales for anchors in a single level.\n            It cannot be set at the same time if `octave_base_scale` and\n            `scales_per_octave` are set.\n        base_sizes (list[int] | None): The basic sizes\n            of anchors in multiple levels.\n            If None is given, strides will be used as base_sizes.\n            (If strides are non square, the shortest stride is taken.)\n        scale_major (bool): Whether to multiply scales first when generating\n            base anchors. If true, the anchors in the same row will have the\n            same scales. By default it is True in V2.0\n        octave_base_scale (int): The base scale of octave.\n        scales_per_octave (int): Number of scales for each octave.\n            `octave_base_scale` and `scales_per_octave` are usually used in\n            retinanet and the `scales` should be None when they are set.\n        centers (list[tuple[float, float]] | None): The centers of the anchor\n            relative to the feature grid center in multiple feature levels.\n            By default it is set to be None and not used. If a list of tuple of\n            float is given, they will be used to shift the centers of anchors.\n        center_offset (float): The offset of center in propotion to anchors\'\n            width and height. By default it is 0 in V2.0.\n\n    Examples:\n        >>> from mmdet.core import AnchorGenerator\n        >>> self = AnchorGenerator([16], [1.], [1.], [9])\n        >>> all_anchors = self.grid_anchors([(2, 2)], device=\'cpu\')\n        >>> print(all_anchors)\n        [tensor([[-4.5000, -4.5000,  4.5000,  4.5000],\n                [11.5000, -4.5000, 20.5000,  4.5000],\n                [-4.5000, 11.5000,  4.5000, 20.5000],\n                [11.5000, 11.5000, 20.5000, 20.5000]])]\n        >>> self = AnchorGenerator([16, 32], [1.], [1.], [9, 18])\n        >>> all_anchors = self.grid_anchors([(2, 2), (1, 1)], device=\'cpu\')\n        >>> print(all_anchors)\n        [tensor([[-4.5000, -4.5000,  4.5000,  4.5000],\n                [11.5000, -4.5000, 20.5000,  4.5000],\n                [-4.5000, 11.5000,  4.5000, 20.5000],\n                [11.5000, 11.5000, 20.5000, 20.5000]]), \\\n        tensor([[-9., -9., 9., 9.]])]\n    """"""\n\n    def __init__(self,\n                 strides,\n                 ratios,\n                 scales=None,\n                 base_sizes=None,\n                 scale_major=True,\n                 octave_base_scale=None,\n                 scales_per_octave=None,\n                 centers=None,\n                 center_offset=0.):\n        # check center and center_offset\n        if center_offset != 0:\n            assert centers is None, \'center cannot be set when center_offset\' \\\n                f\'!=0, {centers} is given.\'\n        if not (0 <= center_offset <= 1):\n            raise ValueError(\'center_offset should be in range [0, 1], \'\n                             f\'{center_offset} is given.\')\n        if centers is not None:\n            assert len(centers) == len(strides), \\\n                \'The number of strides should be the same as centers, got \' \\\n                f\'{strides} and {centers}\'\n\n        # calculate base sizes of anchors\n        self.strides = [_pair(stride) for stride in strides]\n        self.base_sizes = [min(stride) for stride in self.strides\n                           ] if base_sizes is None else base_sizes\n        assert len(self.base_sizes) == len(self.strides), \\\n            \'The number of strides should be the same as base sizes, got \' \\\n            f\'{self.strides} and {self.base_sizes}\'\n\n        # calculate scales of anchors\n        assert ((octave_base_scale is not None\n                and scales_per_octave is not None) ^ (scales is not None)), \\\n            \'scales and octave_base_scale with scales_per_octave cannot\' \\\n            \' be set at the same time\'\n        if scales is not None:\n            self.scales = torch.Tensor(scales)\n        elif octave_base_scale is not None and scales_per_octave is not None:\n            octave_scales = np.array(\n                [2**(i / scales_per_octave) for i in range(scales_per_octave)])\n            scales = octave_scales * octave_base_scale\n            self.scales = torch.Tensor(scales)\n        else:\n            raise ValueError(\'Either scales or octave_base_scale with \'\n                             \'scales_per_octave should be set\')\n\n        self.octave_base_scale = octave_base_scale\n        self.scales_per_octave = scales_per_octave\n        self.ratios = torch.Tensor(ratios)\n        self.scale_major = scale_major\n        self.centers = centers\n        self.center_offset = center_offset\n        self.base_anchors = self.gen_base_anchors()\n\n    @property\n    def num_base_anchors(self):\n        return [base_anchors.size(0) for base_anchors in self.base_anchors]\n\n    @property\n    def num_levels(self):\n        return len(self.strides)\n\n    def gen_base_anchors(self):\n        multi_level_base_anchors = []\n        for i, base_size in enumerate(self.base_sizes):\n            center = None\n            if self.centers is not None:\n                center = self.centers[i]\n            multi_level_base_anchors.append(\n                self.gen_single_level_base_anchors(\n                    base_size,\n                    scales=self.scales,\n                    ratios=self.ratios,\n                    center=center))\n        return multi_level_base_anchors\n\n    def gen_single_level_base_anchors(self,\n                                      base_size,\n                                      scales,\n                                      ratios,\n                                      center=None):\n        w = base_size\n        h = base_size\n        if center is None:\n            x_center = self.center_offset * w\n            y_center = self.center_offset * h\n        else:\n            x_center, y_center = center\n\n        h_ratios = torch.sqrt(ratios)\n        w_ratios = 1 / h_ratios\n        if self.scale_major:\n            ws = (w * w_ratios[:, None] * scales[None, :]).view(-1)\n            hs = (h * h_ratios[:, None] * scales[None, :]).view(-1)\n        else:\n            ws = (w * scales[:, None] * w_ratios[None, :]).view(-1)\n            hs = (h * scales[:, None] * h_ratios[None, :]).view(-1)\n\n        # use float anchor and the anchor\'s center is aligned with the\n        # pixel center\n        base_anchors = [\n            x_center - 0.5 * ws, y_center - 0.5 * hs, x_center + 0.5 * ws,\n            y_center + 0.5 * hs\n        ]\n        base_anchors = torch.stack(base_anchors, dim=-1)\n\n        return base_anchors\n\n    def _meshgrid(self, x, y, row_major=True):\n        xx = x.repeat(len(y))\n        yy = y.view(-1, 1).repeat(1, len(x)).view(-1)\n        if row_major:\n            return xx, yy\n        else:\n            return yy, xx\n\n    def grid_anchors(self, featmap_sizes, device=\'cuda\'):\n        """"""Generate grid anchors in multiple feature levels\n\n        Args:\n            featmap_sizes (list[tuple]): List of feature map sizes in\n                multiple feature levels.\n            device (str): Device where the anchors will be put on.\n\n        Return:\n            list[torch.Tensor]: Anchors in multiple feature levels.\n                The sizes of each tensor should be [N, 4], where\n                N = width * height * num_base_anchors, width and height\n                are the sizes of the corresponding feature lavel,\n                num_base_anchors is the number of anchors for that level.\n        """"""\n        assert self.num_levels == len(featmap_sizes)\n        multi_level_anchors = []\n        for i in range(self.num_levels):\n            anchors = self.single_level_grid_anchors(\n                self.base_anchors[i].to(device),\n                featmap_sizes[i],\n                self.strides[i],\n                device=device)\n            multi_level_anchors.append(anchors)\n        return multi_level_anchors\n\n    def single_level_grid_anchors(self,\n                                  base_anchors,\n                                  featmap_size,\n                                  stride=(16, 16),\n                                  device=\'cuda\'):\n        feat_h, feat_w = featmap_size\n        shift_x = torch.arange(0, feat_w, device=device) * stride[0]\n        shift_y = torch.arange(0, feat_h, device=device) * stride[1]\n        shift_xx, shift_yy = self._meshgrid(shift_x, shift_y)\n        shifts = torch.stack([shift_xx, shift_yy, shift_xx, shift_yy], dim=-1)\n        shifts = shifts.type_as(base_anchors)\n        # first feat_w elements correspond to the first row of shifts\n        # add A anchors (1, A, 4) to K shifts (K, 1, 4) to get\n        # shifted anchors (K, A, 4), reshape to (K*A, 4)\n\n        all_anchors = base_anchors[None, :, :] + shifts[:, None, :]\n        all_anchors = all_anchors.view(-1, 4)\n        # first A rows correspond to A anchors of (0, 0) in feature map,\n        # then (0, 1), (0, 2), ...\n        return all_anchors\n\n    def valid_flags(self, featmap_sizes, pad_shape, device=\'cuda\'):\n        """"""Generate valid flags of anchors in multiple feature levels\n\n        Args:\n            featmap_sizes (list(tuple)): List of feature map sizes in\n                multiple feature levels.\n            pad_shape (tuple): The padded shape of the image.\n            device (str): Device where the anchors will be put on.\n\n        Return:\n            list(torch.Tensor): Valid flags of anchors in multiple levels.\n        """"""\n        assert self.num_levels == len(featmap_sizes)\n        multi_level_flags = []\n        for i in range(self.num_levels):\n            anchor_stride = self.strides[i]\n            feat_h, feat_w = featmap_sizes[i]\n            h, w = pad_shape[:2]\n            valid_feat_h = min(int(np.ceil(h / anchor_stride[0])), feat_h)\n            valid_feat_w = min(int(np.ceil(w / anchor_stride[1])), feat_w)\n            flags = self.single_level_valid_flags((feat_h, feat_w),\n                                                  (valid_feat_h, valid_feat_w),\n                                                  self.num_base_anchors[i],\n                                                  device=device)\n            multi_level_flags.append(flags)\n        return multi_level_flags\n\n    def single_level_valid_flags(self,\n                                 featmap_size,\n                                 valid_size,\n                                 num_base_anchors,\n                                 device=\'cuda\'):\n        feat_h, feat_w = featmap_size\n        valid_h, valid_w = valid_size\n        assert valid_h <= feat_h and valid_w <= feat_w\n        valid_x = torch.zeros(feat_w, dtype=torch.bool, device=device)\n        valid_y = torch.zeros(feat_h, dtype=torch.bool, device=device)\n        valid_x[:valid_w] = 1\n        valid_y[:valid_h] = 1\n        valid_xx, valid_yy = self._meshgrid(valid_x, valid_y)\n        valid = valid_xx & valid_yy\n        valid = valid[:, None].expand(valid.size(0),\n                                      num_base_anchors).contiguous().view(-1)\n        return valid\n\n    def __repr__(self):\n        indent_str = \'    \'\n        repr_str = self.__class__.__name__ + \'(\\n\'\n        repr_str += f\'{indent_str}strides={self.strides},\\n\'\n        repr_str += f\'{indent_str}ratios={self.ratios},\\n\'\n        repr_str += f\'{indent_str}scales={self.scales},\\n\'\n        repr_str += f\'{indent_str}base_sizes={self.base_sizes},\\n\'\n        repr_str += f\'{indent_str}scale_major={self.scale_major},\\n\'\n        repr_str += f\'{indent_str}octave_base_scale=\'\n        repr_str += f\'{self.octave_base_scale},\\n\'\n        repr_str += f\'{indent_str}scales_per_octave=\'\n        repr_str += f\'{self.scales_per_octave},\\n\'\n        repr_str += f\'{indent_str}num_levels={self.num_levels}\\n\'\n        repr_str += f\'{indent_str}centers={self.centers},\\n\'\n        repr_str += f\'{indent_str}center_offset={self.center_offset})\'\n        return repr_str\n\n\n@ANCHOR_GENERATORS.register_module()\nclass SSDAnchorGenerator(AnchorGenerator):\n    """"""Anchor generator for SSD\n\n    Args:\n        strides (list[int]  | list[tuple[int, int]]): Strides of anchors\n            in multiple feature levels.\n        ratios (list[float]): The list of ratios between the height and width\n            of anchors in a single level.\n        basesize_ratio_range (tuple(float)): Ratio range of anchors.\n        input_size (int): Size of feature map, 300 for SSD300,\n            512 for SSD512.\n        scale_major (bool): Whether to multiply scales first when generating\n            base anchors. If true, the anchors in the same row will have the\n            same scales. It is always set to be False in SSD.\n    """"""\n\n    def __init__(self,\n                 strides,\n                 ratios,\n                 basesize_ratio_range,\n                 input_size=300,\n                 scale_major=True):\n        assert len(strides) == len(ratios)\n        assert mmcv.is_tuple_of(basesize_ratio_range, float)\n\n        self.strides = [_pair(stride) for stride in strides]\n        self.input_size = input_size\n        self.centers = [(stride[0] / 2., stride[1] / 2.)\n                        for stride in self.strides]\n        self.basesize_ratio_range = basesize_ratio_range\n\n        # calculate anchor ratios and sizes\n        min_ratio, max_ratio = basesize_ratio_range\n        min_ratio = int(min_ratio * 100)\n        max_ratio = int(max_ratio * 100)\n        step = int(np.floor(max_ratio - min_ratio) / (self.num_levels - 2))\n        min_sizes = []\n        max_sizes = []\n        for ratio in range(int(min_ratio), int(max_ratio) + 1, step):\n            min_sizes.append(int(self.input_size * ratio / 100))\n            max_sizes.append(int(self.input_size * (ratio + step) / 100))\n        if self.input_size == 300:\n            if basesize_ratio_range[0] == 0.15:  # SSD300 COCO\n                min_sizes.insert(0, int(self.input_size * 7 / 100))\n                max_sizes.insert(0, int(self.input_size * 15 / 100))\n            elif basesize_ratio_range[0] == 0.2:  # SSD300 VOC\n                min_sizes.insert(0, int(self.input_size * 10 / 100))\n                max_sizes.insert(0, int(self.input_size * 20 / 100))\n            else:\n                raise ValueError(\n                    \'basesize_ratio_range[0] should be either 0.15\'\n                    \'or 0.2 when input_size is 300, got \'\n                    f\'{basesize_ratio_range[0]}.\')\n        elif self.input_size == 512:\n            if basesize_ratio_range[0] == 0.1:  # SSD512 COCO\n                min_sizes.insert(0, int(self.input_size * 4 / 100))\n                max_sizes.insert(0, int(self.input_size * 10 / 100))\n            elif basesize_ratio_range[0] == 0.15:  # SSD512 VOC\n                min_sizes.insert(0, int(self.input_size * 7 / 100))\n                max_sizes.insert(0, int(self.input_size * 15 / 100))\n            else:\n                raise ValueError(\'basesize_ratio_range[0] should be either 0.1\'\n                                 \'or 0.15 when input_size is 512, got\'\n                                 \' {basesize_ratio_range[0]}.\')\n        else:\n            raise ValueError(\'Only support 300 or 512 in SSDAnchorGenerator\'\n                             f\', got {self.input_size}.\')\n\n        anchor_ratios = []\n        anchor_scales = []\n        for k in range(len(self.strides)):\n            scales = [1., np.sqrt(max_sizes[k] / min_sizes[k])]\n            anchor_ratio = [1.]\n            for r in ratios[k]:\n                anchor_ratio += [1 / r, r]  # 4 or 6 ratio\n            anchor_ratios.append(torch.Tensor(anchor_ratio))\n            anchor_scales.append(torch.Tensor(scales))\n\n        self.base_sizes = min_sizes\n        self.scales = anchor_scales\n        self.ratios = anchor_ratios\n        self.scale_major = scale_major\n        self.center_offset = 0\n        self.base_anchors = self.gen_base_anchors()\n\n    def gen_base_anchors(self):\n        multi_level_base_anchors = []\n        for i, base_size in enumerate(self.base_sizes):\n            base_anchors = self.gen_single_level_base_anchors(\n                base_size,\n                scales=self.scales[i],\n                ratios=self.ratios[i],\n                center=self.centers[i])\n            indices = list(range(len(self.ratios[i])))\n            indices.insert(1, len(indices))\n            base_anchors = torch.index_select(base_anchors, 0,\n                                              torch.LongTensor(indices))\n            multi_level_base_anchors.append(base_anchors)\n        return multi_level_base_anchors\n\n    def __repr__(self):\n        indent_str = \'    \'\n        repr_str = self.__class__.__name__ + \'(\\n\'\n        repr_str += f\'{indent_str}strides={self.strides},\\n\'\n        repr_str += f\'{indent_str}scales={self.scales},\\n\'\n        repr_str += f\'{indent_str}scale_major={self.scale_major},\\n\'\n        repr_str += f\'{indent_str}input_size={self.input_size},\\n\'\n        repr_str += f\'{indent_str}scales={self.scales},\\n\'\n        repr_str += f\'{indent_str}ratios={self.ratios},\\n\'\n        repr_str += f\'{indent_str}num_levels={self.num_levels},\\n\'\n        repr_str += f\'{indent_str}base_sizes={self.base_sizes},\\n\'\n        repr_str += f\'{indent_str}basesize_ratio_range=\'\n        repr_str += f\'{self.basesize_ratio_range})\'\n        return repr_str\n\n\n@ANCHOR_GENERATORS.register_module()\nclass LegacyAnchorGenerator(AnchorGenerator):\n    """"""Legacy anchor generator used in MMDetection V1.x\n\n    Difference to the V2.0 anchor generator:\n\n    1. The center offset of V1.x anchors are set to be 0.5 rather than 0.\n    2. The width/height are minused by 1 when calculating the anchors\' centers\n       and corners to meet the V1.x coordinate system.\n    3. The anchors\' corners are quantized.\n\n    Args:\n        strides (list[int] | list[tuple[int]]): Strides of anchors\n            in multiple feature levels.\n        ratios (list[float]): The list of ratios between the height and width\n            of anchors in a single level.\n        scales (list[int] | None): Anchor scales for anchors in a single level.\n            It cannot be set at the same time if `octave_base_scale` and\n            `scales_per_octave` are set.\n        base_sizes (list[int]): The basic sizes of anchors in multiple levels.\n            If None is given, strides will be used to generate base_sizes.\n        scale_major (bool): Whether to multiply scales first when generating\n            base anchors. If true, the anchors in the same row will have the\n            same scales. By default it is True in V2.0\n        octave_base_scale (int): The base scale of octave.\n        scales_per_octave (int): Number of scales for each octave.\n            `octave_base_scale` and `scales_per_octave` are usually used in\n            retinanet and the `scales` should be None when they are set.\n        centers (list[tuple[float, float]] | None): The centers of the anchor\n            relative to the feature grid center in multiple feature levels.\n            By default it is set to be None and not used. It a list of float\n            is given, this list will be used to shift the centers of anchors.\n        center_offset (float): The offset of center in propotion to anchors\'\n            width and height. By default it is 0.5 in V2.0 but it should be 0.5\n            in v1.x models.\n\n    Examples:\n        >>> from mmdet.core import LegacyAnchorGenerator\n        >>> self = LegacyAnchorGenerator(\n        >>>     [16], [1.], [1.], [9], center_offset=0.5)\n        >>> all_anchors = self.grid_anchors(((2, 2),), device=\'cpu\')\n        >>> print(all_anchors)\n        [tensor([[ 0.,  0.,  8.,  8.],\n                [16.,  0., 24.,  8.],\n                [ 0., 16.,  8., 24.],\n                [16., 16., 24., 24.]])]\n    """"""\n\n    def gen_single_level_base_anchors(self,\n                                      base_size,\n                                      scales,\n                                      ratios,\n                                      center=None):\n        w = base_size\n        h = base_size\n        if center is None:\n            x_center = self.center_offset * (w - 1)\n            y_center = self.center_offset * (h - 1)\n        else:\n            x_center, y_center = center\n\n        h_ratios = torch.sqrt(ratios)\n        w_ratios = 1 / h_ratios\n        if self.scale_major:\n            ws = (w * w_ratios[:, None] * scales[None, :]).view(-1)\n            hs = (h * h_ratios[:, None] * scales[None, :]).view(-1)\n        else:\n            ws = (w * scales[:, None] * w_ratios[None, :]).view(-1)\n            hs = (h * scales[:, None] * h_ratios[None, :]).view(-1)\n\n        # use float anchor and the anchor\'s center is aligned with the\n        # pixel center\n        base_anchors = [\n            x_center - 0.5 * (ws - 1), y_center - 0.5 * (hs - 1),\n            x_center + 0.5 * (ws - 1), y_center + 0.5 * (hs - 1)\n        ]\n        base_anchors = torch.stack(base_anchors, dim=-1).round()\n\n        return base_anchors\n\n\n@ANCHOR_GENERATORS.register_module()\nclass LegacySSDAnchorGenerator(SSDAnchorGenerator, LegacyAnchorGenerator):\n    """"""Legacy anchor generator used in MMDetection V1.x\n\n    The difference between `LegacySSDAnchorGenerator` and `SSDAnchorGenerator`\n    can be found in `LegacyAnchorGenerator`.\n    """"""\n\n    def __init__(self,\n                 strides,\n                 ratios,\n                 basesize_ratio_range,\n                 input_size=300,\n                 scale_major=True):\n        super(LegacySSDAnchorGenerator,\n              self).__init__(strides, ratios, basesize_ratio_range, input_size,\n                             scale_major)\n        self.centers = [((stride - 1) / 2., (stride - 1) / 2.)\n                        for stride in strides]\n        self.base_anchors = self.gen_base_anchors()\n'"
mmdet/core/anchor/builder.py,0,"b""from mmcv.utils import Registry, build_from_cfg\n\nANCHOR_GENERATORS = Registry('Anchor generator')\n\n\ndef build_anchor_generator(cfg, default_args=None):\n    return build_from_cfg(cfg, ANCHOR_GENERATORS, default_args)\n"""
mmdet/core/anchor/point_generator.py,5,"b""import torch\n\nfrom .builder import ANCHOR_GENERATORS\n\n\n@ANCHOR_GENERATORS.register_module()\nclass PointGenerator(object):\n\n    def _meshgrid(self, x, y, row_major=True):\n        xx = x.repeat(len(y))\n        yy = y.view(-1, 1).repeat(1, len(x)).view(-1)\n        if row_major:\n            return xx, yy\n        else:\n            return yy, xx\n\n    def grid_points(self, featmap_size, stride=16, device='cuda'):\n        feat_h, feat_w = featmap_size\n        shift_x = torch.arange(0., feat_w, device=device) * stride\n        shift_y = torch.arange(0., feat_h, device=device) * stride\n        shift_xx, shift_yy = self._meshgrid(shift_x, shift_y)\n        stride = shift_x.new_full((shift_xx.shape[0], ), stride)\n        shifts = torch.stack([shift_xx, shift_yy, stride], dim=-1)\n        all_points = shifts.to(device)\n        return all_points\n\n    def valid_flags(self, featmap_size, valid_size, device='cuda'):\n        feat_h, feat_w = featmap_size\n        valid_h, valid_w = valid_size\n        assert valid_h <= feat_h and valid_w <= feat_w\n        valid_x = torch.zeros(feat_w, dtype=torch.bool, device=device)\n        valid_y = torch.zeros(feat_h, dtype=torch.bool, device=device)\n        valid_x[:valid_w] = 1\n        valid_y[:valid_h] = 1\n        valid_xx, valid_yy = self._meshgrid(valid_x, valid_y)\n        valid = valid_xx & valid_yy\n        return valid\n"""
mmdet/core/anchor/utils.py,5,"b'import torch\n\n\ndef images_to_levels(target, num_levels):\n    """"""Convert targets by image to targets by feature level.\n\n    [target_img0, target_img1] -> [target_level0, target_level1, ...]\n    """"""\n    target = torch.stack(target, 0)\n    level_targets = []\n    start = 0\n    for n in num_levels:\n        end = start + n\n        # level_targets.append(target[:, start:end].squeeze(0))\n        level_targets.append(target[:, start:end])\n        start = end\n    return level_targets\n\n\ndef anchor_inside_flags(flat_anchors,\n                        valid_flags,\n                        img_shape,\n                        allowed_border=0):\n    img_h, img_w = img_shape[:2]\n    if allowed_border >= 0:\n        inside_flags = valid_flags & \\\n            (flat_anchors[:, 0] >= -allowed_border) & \\\n            (flat_anchors[:, 1] >= -allowed_border) & \\\n            (flat_anchors[:, 2] < img_w + allowed_border) & \\\n            (flat_anchors[:, 3] < img_h + allowed_border)\n    else:\n        inside_flags = valid_flags\n    return inside_flags\n\n\ndef calc_region(bbox, ratio, featmap_size=None):\n    """"""Calculate a proportional bbox region.\n\n    The bbox center are fixed and the new h\' and w\' is h * ratio and w * ratio.\n\n    Args:\n        bbox (Tensor): Bboxes to calculate regions, shape (n, 4)\n        ratio (float): Ratio of the output region.\n        featmap_size (tuple): Feature map size used for clipping the boundary.\n\n    Returns:\n        tuple: x1, y1, x2, y2\n    """"""\n    x1 = torch.round((1 - ratio) * bbox[0] + ratio * bbox[2]).long()\n    y1 = torch.round((1 - ratio) * bbox[1] + ratio * bbox[3]).long()\n    x2 = torch.round(ratio * bbox[0] + (1 - ratio) * bbox[2]).long()\n    y2 = torch.round(ratio * bbox[1] + (1 - ratio) * bbox[3]).long()\n    if featmap_size is not None:\n        x1 = x1.clamp(min=0, max=featmap_size[1])\n        y1 = y1.clamp(min=0, max=featmap_size[0])\n        x2 = x2.clamp(min=0, max=featmap_size[1])\n        y2 = y2.clamp(min=0, max=featmap_size[0])\n    return (x1, y1, x2, y2)\n'"
mmdet/core/bbox/__init__.py,0,"b""from .assigners import (AssignResult, BaseAssigner, CenterRegionAssigner,\n                        MaxIoUAssigner)\nfrom .builder import build_assigner, build_bbox_coder, build_sampler\nfrom .coder import (BaseBBoxCoder, DeltaXYWHBBoxCoder, PseudoBBoxCoder,\n                    TBLRBBoxCoder)\nfrom .iou_calculators import BboxOverlaps2D, bbox_overlaps\nfrom .samplers import (BaseSampler, CombinedSampler,\n                       InstanceBalancedPosSampler, IoUBalancedNegSampler,\n                       PseudoSampler, RandomSampler, SamplingResult)\nfrom .transforms import (bbox2result, bbox2roi, bbox_flip, bbox_mapping,\n                         bbox_mapping_back, distance2bbox, roi2bbox)\n\n__all__ = [\n    'bbox_overlaps', 'BboxOverlaps2D', 'BaseAssigner', 'MaxIoUAssigner',\n    'AssignResult', 'BaseSampler', 'PseudoSampler', 'RandomSampler',\n    'InstanceBalancedPosSampler', 'IoUBalancedNegSampler', 'CombinedSampler',\n    'SamplingResult', 'build_assigner', 'build_sampler', 'bbox_flip',\n    'bbox_mapping', 'bbox_mapping_back', 'bbox2roi', 'roi2bbox', 'bbox2result',\n    'distance2bbox', 'build_bbox_coder', 'BaseBBoxCoder', 'PseudoBBoxCoder',\n    'DeltaXYWHBBoxCoder', 'TBLRBBoxCoder', 'CenterRegionAssigner'\n]\n"""
mmdet/core/bbox/builder.py,0,"b""from mmcv.utils import Registry, build_from_cfg\n\nBBOX_ASSIGNERS = Registry('bbox_assigner')\nBBOX_SAMPLERS = Registry('bbox_sampler')\nBBOX_CODERS = Registry('bbox_coder')\n\n\ndef build_assigner(cfg, **default_args):\n    return build_from_cfg(cfg, BBOX_ASSIGNERS, default_args)\n\n\ndef build_sampler(cfg, **default_args):\n    return build_from_cfg(cfg, BBOX_SAMPLERS, default_args)\n\n\ndef build_bbox_coder(cfg, **default_args):\n    return build_from_cfg(cfg, BBOX_CODERS, default_args)\n"""
mmdet/core/bbox/demodata.py,1,"b'import numpy as np\nimport torch\n\n\ndef ensure_rng(rng=None):\n    """"""\n    Simple version of the ``kwarray.ensure_rng``\n\n    Args:\n        rng (int | numpy.random.RandomState | None):\n            if None, then defaults to the global rng. Otherwise this can be an\n            integer or a RandomState class\n    Returns:\n        (numpy.random.RandomState) : rng -\n            a numpy random number generator\n\n    References:\n        https://gitlab.kitware.com/computer-vision/kwarray/blob/master/kwarray/util_random.py#L270\n    """"""\n\n    if rng is None:\n        rng = np.random.mtrand._rand\n    elif isinstance(rng, int):\n        rng = np.random.RandomState(rng)\n    else:\n        rng = rng\n    return rng\n\n\ndef random_boxes(num=1, scale=1, rng=None):\n    """"""\n    Simple version of ``kwimage.Boxes.random``\n\n    Returns:\n        Tensor: shape (n, 4) in x1, y1, x2, y2 format.\n\n    References:\n        https://gitlab.kitware.com/computer-vision/kwimage/blob/master/kwimage/structs/boxes.py#L1390\n\n    Example:\n        >>> num = 3\n        >>> scale = 512\n        >>> rng = 0\n        >>> boxes = random_boxes(num, scale, rng)\n        >>> print(boxes)\n        tensor([[280.9925, 278.9802, 308.6148, 366.1769],\n                [216.9113, 330.6978, 224.0446, 456.5878],\n                [405.3632, 196.3221, 493.3953, 270.7942]])\n    """"""\n    rng = ensure_rng(rng)\n\n    tlbr = rng.rand(num, 4).astype(np.float32)\n\n    tl_x = np.minimum(tlbr[:, 0], tlbr[:, 2])\n    tl_y = np.minimum(tlbr[:, 1], tlbr[:, 3])\n    br_x = np.maximum(tlbr[:, 0], tlbr[:, 2])\n    br_y = np.maximum(tlbr[:, 1], tlbr[:, 3])\n\n    tlbr[:, 0] = tl_x * scale\n    tlbr[:, 1] = tl_y * scale\n    tlbr[:, 2] = br_x * scale\n    tlbr[:, 3] = br_y * scale\n\n    boxes = torch.from_numpy(tlbr)\n    return boxes\n'"
mmdet/core/bbox/transforms.py,4,"b'import numpy as np\nimport torch\n\n\ndef bbox_flip(bboxes, img_shape, direction=\'horizontal\'):\n    """"""Flip bboxes horizontally or vertically.\n\n    Args:\n        bboxes (Tensor): Shape (..., 4*k)\n        img_shape (tuple): Image shape.\n        direction (str): Flip direction, options are ""horizontal"" and\n            ""vertical"". Default: ""horizontal""\n\n\n    Returns:\n        Tensor: Flipped bboxes.\n    """"""\n    assert bboxes.shape[-1] % 4 == 0\n    assert direction in [\'horizontal\', \'vertical\']\n    flipped = bboxes.clone()\n    if direction == \'vertical\':\n        flipped[..., 1::4] = img_shape[0] - bboxes[..., 3::4]\n        flipped[..., 3::4] = img_shape[0] - bboxes[..., 1::4]\n    else:\n        flipped[:, 0::4] = img_shape[1] - bboxes[:, 2::4]\n        flipped[:, 2::4] = img_shape[1] - bboxes[:, 0::4]\n    return flipped\n\n\ndef bbox_mapping(bboxes,\n                 img_shape,\n                 scale_factor,\n                 flip,\n                 flip_direction=\'horizontal\'):\n    """"""Map bboxes from the original image scale to testing scale""""""\n    new_bboxes = bboxes * bboxes.new_tensor(scale_factor)\n    if flip:\n        new_bboxes = bbox_flip(new_bboxes, img_shape, flip_direction)\n    return new_bboxes\n\n\ndef bbox_mapping_back(bboxes,\n                      img_shape,\n                      scale_factor,\n                      flip,\n                      flip_direction=\'horizontal\'):\n    """"""Map bboxes from testing scale to original image scale""""""\n    new_bboxes = bbox_flip(bboxes, img_shape,\n                           flip_direction) if flip else bboxes\n    new_bboxes = new_bboxes.view(-1, 4) / new_bboxes.new_tensor(scale_factor)\n    return new_bboxes.view(bboxes.shape)\n\n\ndef bbox2roi(bbox_list):\n    """"""Convert a list of bboxes to roi format.\n\n    Args:\n        bbox_list (list[Tensor]): a list of bboxes corresponding to a batch\n            of images.\n\n    Returns:\n        Tensor: shape (n, 5), [batch_ind, x1, y1, x2, y2]\n    """"""\n    rois_list = []\n    for img_id, bboxes in enumerate(bbox_list):\n        if bboxes.size(0) > 0:\n            img_inds = bboxes.new_full((bboxes.size(0), 1), img_id)\n            rois = torch.cat([img_inds, bboxes[:, :4]], dim=-1)\n        else:\n            rois = bboxes.new_zeros((0, 5))\n        rois_list.append(rois)\n    rois = torch.cat(rois_list, 0)\n    return rois\n\n\ndef roi2bbox(rois):\n    bbox_list = []\n    img_ids = torch.unique(rois[:, 0].cpu(), sorted=True)\n    for img_id in img_ids:\n        inds = (rois[:, 0] == img_id.item())\n        bbox = rois[inds, 1:]\n        bbox_list.append(bbox)\n    return bbox_list\n\n\ndef bbox2result(bboxes, labels, num_classes):\n    """"""Convert detection results to a list of numpy arrays.\n\n    Args:\n        bboxes (Tensor): shape (n, 5)\n        labels (Tensor): shape (n, )\n        num_classes (int): class number, including background class\n\n    Returns:\n        list(ndarray): bbox results of each class\n    """"""\n    if bboxes.shape[0] == 0:\n        return [np.zeros((0, 5), dtype=np.float32) for i in range(num_classes)]\n    else:\n        bboxes = bboxes.cpu().numpy()\n        labels = labels.cpu().numpy()\n        return [bboxes[labels == i, :] for i in range(num_classes)]\n\n\ndef distance2bbox(points, distance, max_shape=None):\n    """"""Decode distance prediction to bounding box.\n\n    Args:\n        points (Tensor): Shape (n, 2), [x, y].\n        distance (Tensor): Distance from the given point to 4\n            boundaries (left, top, right, bottom).\n        max_shape (tuple): Shape of the image.\n\n    Returns:\n        Tensor: Decoded bboxes.\n    """"""\n    x1 = points[:, 0] - distance[:, 0]\n    y1 = points[:, 1] - distance[:, 1]\n    x2 = points[:, 0] + distance[:, 2]\n    y2 = points[:, 1] + distance[:, 3]\n    if max_shape is not None:\n        x1 = x1.clamp(min=0, max=max_shape[1])\n        y1 = y1.clamp(min=0, max=max_shape[0])\n        x2 = x2.clamp(min=0, max=max_shape[1])\n        y2 = y2.clamp(min=0, max=max_shape[0])\n    return torch.stack([x1, y1, x2, y2], -1)\n'"
mmdet/core/evaluation/__init__.py,0,"b""from .class_names import (cityscapes_classes, coco_classes, dataset_aliases,\n                          get_classes, imagenet_det_classes,\n                          imagenet_vid_classes, voc_classes)\nfrom .eval_hooks import DistEvalHook, EvalHook\nfrom .mean_ap import average_precision, eval_map, print_map_summary\nfrom .recall import (eval_recalls, plot_iou_recall, plot_num_recall,\n                     print_recall_summary)\n\n__all__ = [\n    'voc_classes', 'imagenet_det_classes', 'imagenet_vid_classes',\n    'coco_classes', 'cityscapes_classes', 'dataset_aliases', 'get_classes',\n    'DistEvalHook', 'EvalHook', 'average_precision', 'eval_map',\n    'print_map_summary', 'eval_recalls', 'print_recall_summary',\n    'plot_num_recall', 'plot_iou_recall'\n]\n"""
mmdet/core/evaluation/bbox_overlaps.py,0,"b'import numpy as np\n\n\ndef bbox_overlaps(bboxes1, bboxes2, mode=\'iou\'):\n    """"""Calculate the ious between each bbox of bboxes1 and bboxes2.\n\n    Args:\n        bboxes1(ndarray): shape (n, 4)\n        bboxes2(ndarray): shape (k, 4)\n        mode(str): iou (intersection over union) or iof (intersection\n            over foreground)\n\n    Returns:\n        ious(ndarray): shape (n, k)\n    """"""\n\n    assert mode in [\'iou\', \'iof\']\n\n    bboxes1 = bboxes1.astype(np.float32)\n    bboxes2 = bboxes2.astype(np.float32)\n    rows = bboxes1.shape[0]\n    cols = bboxes2.shape[0]\n    ious = np.zeros((rows, cols), dtype=np.float32)\n    if rows * cols == 0:\n        return ious\n    exchange = False\n    if bboxes1.shape[0] > bboxes2.shape[0]:\n        bboxes1, bboxes2 = bboxes2, bboxes1\n        ious = np.zeros((cols, rows), dtype=np.float32)\n        exchange = True\n    area1 = (bboxes1[:, 2] - bboxes1[:, 0]) * (bboxes1[:, 3] - bboxes1[:, 1])\n    area2 = (bboxes2[:, 2] - bboxes2[:, 0]) * (bboxes2[:, 3] - bboxes2[:, 1])\n    for i in range(bboxes1.shape[0]):\n        x_start = np.maximum(bboxes1[i, 0], bboxes2[:, 0])\n        y_start = np.maximum(bboxes1[i, 1], bboxes2[:, 1])\n        x_end = np.minimum(bboxes1[i, 2], bboxes2[:, 2])\n        y_end = np.minimum(bboxes1[i, 3], bboxes2[:, 3])\n        overlap = np.maximum(x_end - x_start, 0) * np.maximum(\n            y_end - y_start, 0)\n        if mode == \'iou\':\n            union = area1[i] + area2 - overlap\n        else:\n            union = area1[i] if not exchange else area2\n        ious[i, :] = overlap / union\n    if exchange:\n        ious = ious.T\n    return ious\n'"
mmdet/core/evaluation/class_names.py,0,"b'import mmcv\n\n\ndef wider_face_classes():\n    return [\'face\']\n\n\ndef voc_classes():\n    return [\n        \'aeroplane\', \'bicycle\', \'bird\', \'boat\', \'bottle\', \'bus\', \'car\', \'cat\',\n        \'chair\', \'cow\', \'diningtable\', \'dog\', \'horse\', \'motorbike\', \'person\',\n        \'pottedplant\', \'sheep\', \'sofa\', \'train\', \'tvmonitor\'\n    ]\n\n\ndef imagenet_det_classes():\n    return [\n        \'accordion\', \'airplane\', \'ant\', \'antelope\', \'apple\', \'armadillo\',\n        \'artichoke\', \'axe\', \'baby_bed\', \'backpack\', \'bagel\', \'balance_beam\',\n        \'banana\', \'band_aid\', \'banjo\', \'baseball\', \'basketball\', \'bathing_cap\',\n        \'beaker\', \'bear\', \'bee\', \'bell_pepper\', \'bench\', \'bicycle\', \'binder\',\n        \'bird\', \'bookshelf\', \'bow_tie\', \'bow\', \'bowl\', \'brassiere\', \'burrito\',\n        \'bus\', \'butterfly\', \'camel\', \'can_opener\', \'car\', \'cart\', \'cattle\',\n        \'cello\', \'centipede\', \'chain_saw\', \'chair\', \'chime\', \'cocktail_shaker\',\n        \'coffee_maker\', \'computer_keyboard\', \'computer_mouse\', \'corkscrew\',\n        \'cream\', \'croquet_ball\', \'crutch\', \'cucumber\', \'cup_or_mug\', \'diaper\',\n        \'digital_clock\', \'dishwasher\', \'dog\', \'domestic_cat\', \'dragonfly\',\n        \'drum\', \'dumbbell\', \'electric_fan\', \'elephant\', \'face_powder\', \'fig\',\n        \'filing_cabinet\', \'flower_pot\', \'flute\', \'fox\', \'french_horn\', \'frog\',\n        \'frying_pan\', \'giant_panda\', \'goldfish\', \'golf_ball\', \'golfcart\',\n        \'guacamole\', \'guitar\', \'hair_dryer\', \'hair_spray\', \'hamburger\',\n        \'hammer\', \'hamster\', \'harmonica\', \'harp\', \'hat_with_a_wide_brim\',\n        \'head_cabbage\', \'helmet\', \'hippopotamus\', \'horizontal_bar\', \'horse\',\n        \'hotdog\', \'iPod\', \'isopod\', \'jellyfish\', \'koala_bear\', \'ladle\',\n        \'ladybug\', \'lamp\', \'laptop\', \'lemon\', \'lion\', \'lipstick\', \'lizard\',\n        \'lobster\', \'maillot\', \'maraca\', \'microphone\', \'microwave\', \'milk_can\',\n        \'miniskirt\', \'monkey\', \'motorcycle\', \'mushroom\', \'nail\', \'neck_brace\',\n        \'oboe\', \'orange\', \'otter\', \'pencil_box\', \'pencil_sharpener\', \'perfume\',\n        \'person\', \'piano\', \'pineapple\', \'ping-pong_ball\', \'pitcher\', \'pizza\',\n        \'plastic_bag\', \'plate_rack\', \'pomegranate\', \'popsicle\', \'porcupine\',\n        \'power_drill\', \'pretzel\', \'printer\', \'puck\', \'punching_bag\', \'purse\',\n        \'rabbit\', \'racket\', \'ray\', \'red_panda\', \'refrigerator\',\n        \'remote_control\', \'rubber_eraser\', \'rugby_ball\', \'ruler\',\n        \'salt_or_pepper_shaker\', \'saxophone\', \'scorpion\', \'screwdriver\',\n        \'seal\', \'sheep\', \'ski\', \'skunk\', \'snail\', \'snake\', \'snowmobile\',\n        \'snowplow\', \'soap_dispenser\', \'soccer_ball\', \'sofa\', \'spatula\',\n        \'squirrel\', \'starfish\', \'stethoscope\', \'stove\', \'strainer\',\n        \'strawberry\', \'stretcher\', \'sunglasses\', \'swimming_trunks\', \'swine\',\n        \'syringe\', \'table\', \'tape_player\', \'tennis_ball\', \'tick\', \'tie\',\n        \'tiger\', \'toaster\', \'traffic_light\', \'train\', \'trombone\', \'trumpet\',\n        \'turtle\', \'tv_or_monitor\', \'unicycle\', \'vacuum\', \'violin\',\n        \'volleyball\', \'waffle_iron\', \'washer\', \'water_bottle\', \'watercraft\',\n        \'whale\', \'wine_bottle\', \'zebra\'\n    ]\n\n\ndef imagenet_vid_classes():\n    return [\n        \'airplane\', \'antelope\', \'bear\', \'bicycle\', \'bird\', \'bus\', \'car\',\n        \'cattle\', \'dog\', \'domestic_cat\', \'elephant\', \'fox\', \'giant_panda\',\n        \'hamster\', \'horse\', \'lion\', \'lizard\', \'monkey\', \'motorcycle\', \'rabbit\',\n        \'red_panda\', \'sheep\', \'snake\', \'squirrel\', \'tiger\', \'train\', \'turtle\',\n        \'watercraft\', \'whale\', \'zebra\'\n    ]\n\n\ndef coco_classes():\n    return [\n        \'person\', \'bicycle\', \'car\', \'motorcycle\', \'airplane\', \'bus\', \'train\',\n        \'truck\', \'boat\', \'traffic_light\', \'fire_hydrant\', \'stop_sign\',\n        \'parking_meter\', \'bench\', \'bird\', \'cat\', \'dog\', \'horse\', \'sheep\',\n        \'cow\', \'elephant\', \'bear\', \'zebra\', \'giraffe\', \'backpack\', \'umbrella\',\n        \'handbag\', \'tie\', \'suitcase\', \'frisbee\', \'skis\', \'snowboard\',\n        \'sports_ball\', \'kite\', \'baseball_bat\', \'baseball_glove\', \'skateboard\',\n        \'surfboard\', \'tennis_racket\', \'bottle\', \'wine_glass\', \'cup\', \'fork\',\n        \'knife\', \'spoon\', \'bowl\', \'banana\', \'apple\', \'sandwich\', \'orange\',\n        \'broccoli\', \'carrot\', \'hot_dog\', \'pizza\', \'donut\', \'cake\', \'chair\',\n        \'couch\', \'potted_plant\', \'bed\', \'dining_table\', \'toilet\', \'tv\',\n        \'laptop\', \'mouse\', \'remote\', \'keyboard\', \'cell_phone\', \'microwave\',\n        \'oven\', \'toaster\', \'sink\', \'refrigerator\', \'book\', \'clock\', \'vase\',\n        \'scissors\', \'teddy_bear\', \'hair_drier\', \'toothbrush\'\n    ]\n\n\ndef cityscapes_classes():\n    return [\n        \'person\', \'rider\', \'car\', \'truck\', \'bus\', \'train\', \'motorcycle\',\n        \'bicycle\'\n    ]\n\n\ndataset_aliases = {\n    \'voc\': [\'voc\', \'pascal_voc\', \'voc07\', \'voc12\'],\n    \'imagenet_det\': [\'det\', \'imagenet_det\', \'ilsvrc_det\'],\n    \'imagenet_vid\': [\'vid\', \'imagenet_vid\', \'ilsvrc_vid\'],\n    \'coco\': [\'coco\', \'mscoco\', \'ms_coco\'],\n    \'wider_face\': [\'WIDERFaceDataset\', \'wider_face\', \'WDIERFace\'],\n    \'cityscapes\': [\'cityscapes\']\n}\n\n\ndef get_classes(dataset):\n    """"""Get class names of a dataset.""""""\n    alias2name = {}\n    for name, aliases in dataset_aliases.items():\n        for alias in aliases:\n            alias2name[alias] = name\n\n    if mmcv.is_str(dataset):\n        if dataset in alias2name:\n            labels = eval(alias2name[dataset] + \'_classes()\')\n        else:\n            raise ValueError(f\'Unrecognized dataset: {dataset}\')\n    else:\n        raise TypeError(f\'dataset must a str, but got {type(dataset)}\')\n    return labels\n'"
mmdet/core/evaluation/eval_hooks.py,1,"b'import os.path as osp\n\nfrom mmcv.runner import Hook\nfrom torch.utils.data import DataLoader\n\n\nclass EvalHook(Hook):\n    """"""Evaluation hook.\n\n    Attributes:\n        dataloader (DataLoader): A PyTorch dataloader.\n        interval (int): Evaluation interval (by epochs). Default: 1.\n    """"""\n\n    def __init__(self, dataloader, interval=1, **eval_kwargs):\n        if not isinstance(dataloader, DataLoader):\n            raise TypeError(\'dataloader must be a pytorch DataLoader, but got\'\n                            f\' {type(dataloader)}\')\n        self.dataloader = dataloader\n        self.interval = interval\n        self.eval_kwargs = eval_kwargs\n\n    def after_train_epoch(self, runner):\n        if not self.every_n_epochs(runner, self.interval):\n            return\n        from mmdet.apis import single_gpu_test\n        results = single_gpu_test(runner.model, self.dataloader, show=False)\n        self.evaluate(runner, results)\n\n    def evaluate(self, runner, results):\n        eval_res = self.dataloader.dataset.evaluate(\n            results, logger=runner.logger, **self.eval_kwargs)\n        for name, val in eval_res.items():\n            runner.log_buffer.output[name] = val\n        runner.log_buffer.ready = True\n\n\nclass DistEvalHook(EvalHook):\n    """"""Distributed evaluation hook.\n\n    Attributes:\n        dataloader (DataLoader): A PyTorch dataloader.\n        interval (int): Evaluation interval (by epochs). Default: 1.\n        tmpdir (str | None): Temporary directory to save the results of all\n            processes. Default: None.\n        gpu_collect (bool): Whether to use gpu or cpu to collect results.\n            Default: False.\n    """"""\n\n    def __init__(self,\n                 dataloader,\n                 interval=1,\n                 gpu_collect=False,\n                 **eval_kwargs):\n        if not isinstance(dataloader, DataLoader):\n            raise TypeError(\'dataloader must be a pytorch DataLoader, but got \'\n                            f\'{type(dataloader)}\')\n        self.dataloader = dataloader\n        self.interval = interval\n        self.gpu_collect = gpu_collect\n        self.eval_kwargs = eval_kwargs\n\n    def after_train_epoch(self, runner):\n        if not self.every_n_epochs(runner, self.interval):\n            return\n        from mmdet.apis import multi_gpu_test\n        results = multi_gpu_test(\n            runner.model,\n            self.dataloader,\n            tmpdir=osp.join(runner.work_dir, \'.eval_hook\'),\n            gpu_collect=self.gpu_collect)\n        if runner.rank == 0:\n            print(\'\\n\')\n            self.evaluate(runner, results)\n'"
mmdet/core/evaluation/mean_ap.py,0,"b'from multiprocessing import Pool\n\nimport mmcv\nimport numpy as np\nfrom mmcv.utils import print_log\nfrom terminaltables import AsciiTable\n\nfrom .bbox_overlaps import bbox_overlaps\nfrom .class_names import get_classes\n\n\ndef average_precision(recalls, precisions, mode=\'area\'):\n    """"""Calculate average precision (for single or multiple scales).\n\n    Args:\n        recalls (ndarray): shape (num_scales, num_dets) or (num_dets, )\n        precisions (ndarray): shape (num_scales, num_dets) or (num_dets, )\n        mode (str): \'area\' or \'11points\', \'area\' means calculating the area\n            under precision-recall curve, \'11points\' means calculating\n            the average precision of recalls at [0, 0.1, ..., 1]\n\n    Returns:\n        float or ndarray: calculated average precision\n    """"""\n    no_scale = False\n    if recalls.ndim == 1:\n        no_scale = True\n        recalls = recalls[np.newaxis, :]\n        precisions = precisions[np.newaxis, :]\n    assert recalls.shape == precisions.shape and recalls.ndim == 2\n    num_scales = recalls.shape[0]\n    ap = np.zeros(num_scales, dtype=np.float32)\n    if mode == \'area\':\n        zeros = np.zeros((num_scales, 1), dtype=recalls.dtype)\n        ones = np.ones((num_scales, 1), dtype=recalls.dtype)\n        mrec = np.hstack((zeros, recalls, ones))\n        mpre = np.hstack((zeros, precisions, zeros))\n        for i in range(mpre.shape[1] - 1, 0, -1):\n            mpre[:, i - 1] = np.maximum(mpre[:, i - 1], mpre[:, i])\n        for i in range(num_scales):\n            ind = np.where(mrec[i, 1:] != mrec[i, :-1])[0]\n            ap[i] = np.sum(\n                (mrec[i, ind + 1] - mrec[i, ind]) * mpre[i, ind + 1])\n    elif mode == \'11points\':\n        for i in range(num_scales):\n            for thr in np.arange(0, 1 + 1e-3, 0.1):\n                precs = precisions[i, recalls[i, :] >= thr]\n                prec = precs.max() if precs.size > 0 else 0\n                ap[i] += prec\n            ap /= 11\n    else:\n        raise ValueError(\n            \'Unrecognized mode, only ""area"" and ""11points"" are supported\')\n    if no_scale:\n        ap = ap[0]\n    return ap\n\n\ndef tpfp_imagenet(det_bboxes,\n                  gt_bboxes,\n                  gt_bboxes_ignore=None,\n                  default_iou_thr=0.5,\n                  area_ranges=None):\n    """"""Check if detected bboxes are true positive or false positive.\n\n    Args:\n        det_bbox (ndarray): Detected bboxes of this image, of shape (m, 5).\n        gt_bboxes (ndarray): GT bboxes of this image, of shape (n, 4).\n        gt_bboxes_ignore (ndarray): Ignored gt bboxes of this image,\n            of shape (k, 4). Default: None\n        default_iou_thr (float): IoU threshold to be considered as matched for\n            medium and large bboxes (small ones have special rules).\n            Default: 0.5.\n        area_ranges (list[tuple] | None): Range of bbox areas to be evaluated,\n            in the format [(min1, max1), (min2, max2), ...]. Default: None.\n\n    Returns:\n        tuple[np.ndarray]: (tp, fp) whose elements are 0 and 1. The shape of\n            each array is (num_scales, m).\n    """"""\n    # an indicator of ignored gts\n    gt_ignore_inds = np.concatenate(\n        (np.zeros(gt_bboxes.shape[0], dtype=np.bool),\n         np.ones(gt_bboxes_ignore.shape[0], dtype=np.bool)))\n    # stack gt_bboxes and gt_bboxes_ignore for convenience\n    gt_bboxes = np.vstack((gt_bboxes, gt_bboxes_ignore))\n\n    num_dets = det_bboxes.shape[0]\n    num_gts = gt_bboxes.shape[0]\n    if area_ranges is None:\n        area_ranges = [(None, None)]\n    num_scales = len(area_ranges)\n    # tp and fp are of shape (num_scales, num_gts), each row is tp or fp\n    # of a certain scale.\n    tp = np.zeros((num_scales, num_dets), dtype=np.float32)\n    fp = np.zeros((num_scales, num_dets), dtype=np.float32)\n    if gt_bboxes.shape[0] == 0:\n        if area_ranges == [(None, None)]:\n            fp[...] = 1\n        else:\n            det_areas = (det_bboxes[:, 2] - det_bboxes[:, 0]) * (\n                det_bboxes[:, 3] - det_bboxes[:, 1])\n            for i, (min_area, max_area) in enumerate(area_ranges):\n                fp[i, (det_areas >= min_area) & (det_areas < max_area)] = 1\n        return tp, fp\n    ious = bbox_overlaps(det_bboxes, gt_bboxes - 1)\n    gt_w = gt_bboxes[:, 2] - gt_bboxes[:, 0]\n    gt_h = gt_bboxes[:, 3] - gt_bboxes[:, 1]\n    iou_thrs = np.minimum((gt_w * gt_h) / ((gt_w + 10.0) * (gt_h + 10.0)),\n                          default_iou_thr)\n    # sort all detections by scores in descending order\n    sort_inds = np.argsort(-det_bboxes[:, -1])\n    for k, (min_area, max_area) in enumerate(area_ranges):\n        gt_covered = np.zeros(num_gts, dtype=bool)\n        # if no area range is specified, gt_area_ignore is all False\n        if min_area is None:\n            gt_area_ignore = np.zeros_like(gt_ignore_inds, dtype=bool)\n        else:\n            gt_areas = gt_w * gt_h\n            gt_area_ignore = (gt_areas < min_area) | (gt_areas >= max_area)\n        for i in sort_inds:\n            max_iou = -1\n            matched_gt = -1\n            # find best overlapped available gt\n            for j in range(num_gts):\n                # different from PASCAL VOC: allow finding other gts if the\n                # best overlaped ones are already matched by other det bboxes\n                if gt_covered[j]:\n                    continue\n                elif ious[i, j] >= iou_thrs[j] and ious[i, j] > max_iou:\n                    max_iou = ious[i, j]\n                    matched_gt = j\n            # there are 4 cases for a det bbox:\n            # 1. it matches a gt, tp = 1, fp = 0\n            # 2. it matches an ignored gt, tp = 0, fp = 0\n            # 3. it matches no gt and within area range, tp = 0, fp = 1\n            # 4. it matches no gt but is beyond area range, tp = 0, fp = 0\n            if matched_gt >= 0:\n                gt_covered[matched_gt] = 1\n                if not (gt_ignore_inds[matched_gt]\n                        or gt_area_ignore[matched_gt]):\n                    tp[k, i] = 1\n            elif min_area is None:\n                fp[k, i] = 1\n            else:\n                bbox = det_bboxes[i, :4]\n                area = (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])\n                if area >= min_area and area < max_area:\n                    fp[k, i] = 1\n    return tp, fp\n\n\ndef tpfp_default(det_bboxes,\n                 gt_bboxes,\n                 gt_bboxes_ignore=None,\n                 iou_thr=0.5,\n                 area_ranges=None):\n    """"""Check if detected bboxes are true positive or false positive.\n\n    Args:\n        det_bbox (ndarray): Detected bboxes of this image, of shape (m, 5).\n        gt_bboxes (ndarray): GT bboxes of this image, of shape (n, 4).\n        gt_bboxes_ignore (ndarray): Ignored gt bboxes of this image,\n            of shape (k, 4). Default: None\n        iou_thr (float): IoU threshold to be considered as matched.\n            Default: 0.5.\n        area_ranges (list[tuple] | None): Range of bbox areas to be evaluated,\n            in the format [(min1, max1), (min2, max2), ...]. Default: None.\n\n    Returns:\n        tuple[np.ndarray]: (tp, fp) whose elements are 0 and 1. The shape of\n            each array is (num_scales, m).\n    """"""\n    # an indicator of ignored gts\n    gt_ignore_inds = np.concatenate(\n        (np.zeros(gt_bboxes.shape[0], dtype=np.bool),\n         np.ones(gt_bboxes_ignore.shape[0], dtype=np.bool)))\n    # stack gt_bboxes and gt_bboxes_ignore for convenience\n    gt_bboxes = np.vstack((gt_bboxes, gt_bboxes_ignore))\n\n    num_dets = det_bboxes.shape[0]\n    num_gts = gt_bboxes.shape[0]\n    if area_ranges is None:\n        area_ranges = [(None, None)]\n    num_scales = len(area_ranges)\n    # tp and fp are of shape (num_scales, num_gts), each row is tp or fp of\n    # a certain scale\n    tp = np.zeros((num_scales, num_dets), dtype=np.float32)\n    fp = np.zeros((num_scales, num_dets), dtype=np.float32)\n\n    # if there is no gt bboxes in this image, then all det bboxes\n    # within area range are false positives\n    if gt_bboxes.shape[0] == 0:\n        if area_ranges == [(None, None)]:\n            fp[...] = 1\n        else:\n            det_areas = (det_bboxes[:, 2] - det_bboxes[:, 0]) * (\n                det_bboxes[:, 3] - det_bboxes[:, 1])\n            for i, (min_area, max_area) in enumerate(area_ranges):\n                fp[i, (det_areas >= min_area) & (det_areas < max_area)] = 1\n        return tp, fp\n\n    ious = bbox_overlaps(det_bboxes, gt_bboxes)\n    # for each det, the max iou with all gts\n    ious_max = ious.max(axis=1)\n    # for each det, which gt overlaps most with it\n    ious_argmax = ious.argmax(axis=1)\n    # sort all dets in descending order by scores\n    sort_inds = np.argsort(-det_bboxes[:, -1])\n    for k, (min_area, max_area) in enumerate(area_ranges):\n        gt_covered = np.zeros(num_gts, dtype=bool)\n        # if no area range is specified, gt_area_ignore is all False\n        if min_area is None:\n            gt_area_ignore = np.zeros_like(gt_ignore_inds, dtype=bool)\n        else:\n            gt_areas = (gt_bboxes[:, 2] - gt_bboxes[:, 0]) * (\n                gt_bboxes[:, 3] - gt_bboxes[:, 1])\n            gt_area_ignore = (gt_areas < min_area) | (gt_areas >= max_area)\n        for i in sort_inds:\n            if ious_max[i] >= iou_thr:\n                matched_gt = ious_argmax[i]\n                if not (gt_ignore_inds[matched_gt]\n                        or gt_area_ignore[matched_gt]):\n                    if not gt_covered[matched_gt]:\n                        gt_covered[matched_gt] = True\n                        tp[k, i] = 1\n                    else:\n                        fp[k, i] = 1\n                # otherwise ignore this detected bbox, tp = 0, fp = 0\n            elif min_area is None:\n                fp[k, i] = 1\n            else:\n                bbox = det_bboxes[i, :4]\n                area = (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])\n                if area >= min_area and area < max_area:\n                    fp[k, i] = 1\n    return tp, fp\n\n\ndef get_cls_results(det_results, annotations, class_id):\n    """"""Get det results and gt information of a certain class.\n\n    Args:\n        det_results (list[list]): Same as `eval_map()`.\n        annotations (list[dict]): Same as `eval_map()`.\n        class_id (int): ID of a specific class.\n\n    Returns:\n        tuple[list[np.ndarray]]: detected bboxes, gt bboxes, ignored gt bboxes\n    """"""\n    cls_dets = [img_res[class_id] for img_res in det_results]\n    cls_gts = []\n    cls_gts_ignore = []\n    for ann in annotations:\n        gt_inds = ann[\'labels\'] == class_id\n        cls_gts.append(ann[\'bboxes\'][gt_inds, :])\n\n        if ann.get(\'labels_ignore\', None) is not None:\n            ignore_inds = ann[\'labels_ignore\'] == class_id\n            cls_gts_ignore.append(ann[\'bboxes_ignore\'][ignore_inds, :])\n        else:\n            cls_gts_ignore.append(np.empty((0, 4), dtype=np.float32))\n\n    return cls_dets, cls_gts, cls_gts_ignore\n\n\ndef eval_map(det_results,\n             annotations,\n             scale_ranges=None,\n             iou_thr=0.5,\n             dataset=None,\n             logger=None,\n             nproc=4):\n    """"""Evaluate mAP of a dataset.\n\n    Args:\n        det_results (list[list]): [[cls1_det, cls2_det, ...], ...].\n            The outer list indicates images, and the inner list indicates\n            per-class detected bboxes.\n        annotations (list[dict]): Ground truth annotations where each item of\n            the list indicates an image. Keys of annotations are:\n\n            - `bboxes`: numpy array of shape (n, 4)\n            - `labels`: numpy array of shape (n, )\n            - `bboxes_ignore` (optional): numpy array of shape (k, 4)\n            - `labels_ignore` (optional): numpy array of shape (k, )\n        scale_ranges (list[tuple] | None): Range of scales to be evaluated,\n            in the format [(min1, max1), (min2, max2), ...]. A range of\n            (32, 64) means the area range between (32**2, 64**2).\n            Default: None.\n        iou_thr (float): IoU threshold to be considered as matched.\n            Default: 0.5.\n        dataset (list[str] | str | None): Dataset name or dataset classes,\n            there are minor differences in metrics for different datsets, e.g.\n            ""voc07"", ""imagenet_det"", etc. Default: None.\n        logger (logging.Logger | str | None): The way to print the mAP\n            summary. See `mmdet.utils.print_log()` for details. Default: None.\n        nproc (int): Processes used for computing TP and FP.\n            Default: 4.\n\n    Returns:\n        tuple: (mAP, [dict, dict, ...])\n    """"""\n    assert len(det_results) == len(annotations)\n\n    num_imgs = len(det_results)\n    num_scales = len(scale_ranges) if scale_ranges is not None else 1\n    num_classes = len(det_results[0])  # positive class num\n    area_ranges = ([(rg[0]**2, rg[1]**2) for rg in scale_ranges]\n                   if scale_ranges is not None else None)\n\n    pool = Pool(nproc)\n    eval_results = []\n    for i in range(num_classes):\n        # get gt and det bboxes of this class\n        cls_dets, cls_gts, cls_gts_ignore = get_cls_results(\n            det_results, annotations, i)\n        # choose proper function according to datasets to compute tp and fp\n        if dataset in [\'det\', \'vid\']:\n            tpfp_func = tpfp_imagenet\n        else:\n            tpfp_func = tpfp_default\n        # compute tp and fp for each image with multiple processes\n        tpfp = pool.starmap(\n            tpfp_func,\n            zip(cls_dets, cls_gts, cls_gts_ignore,\n                [iou_thr for _ in range(num_imgs)],\n                [area_ranges for _ in range(num_imgs)]))\n        tp, fp = tuple(zip(*tpfp))\n        # calculate gt number of each scale\n        # ignored gts or gts beyond the specific scale are not counted\n        num_gts = np.zeros(num_scales, dtype=int)\n        for j, bbox in enumerate(cls_gts):\n            if area_ranges is None:\n                num_gts[0] += bbox.shape[0]\n            else:\n                gt_areas = (bbox[:, 2] - bbox[:, 0]) * (\n                    bbox[:, 3] - bbox[:, 1])\n                for k, (min_area, max_area) in enumerate(area_ranges):\n                    num_gts[k] += np.sum((gt_areas >= min_area)\n                                         & (gt_areas < max_area))\n        # sort all det bboxes by score, also sort tp and fp\n        cls_dets = np.vstack(cls_dets)\n        num_dets = cls_dets.shape[0]\n        sort_inds = np.argsort(-cls_dets[:, -1])\n        tp = np.hstack(tp)[:, sort_inds]\n        fp = np.hstack(fp)[:, sort_inds]\n        # calculate recall and precision with tp and fp\n        tp = np.cumsum(tp, axis=1)\n        fp = np.cumsum(fp, axis=1)\n        eps = np.finfo(np.float32).eps\n        recalls = tp / np.maximum(num_gts[:, np.newaxis], eps)\n        precisions = tp / np.maximum((tp + fp), eps)\n        # calculate AP\n        if scale_ranges is None:\n            recalls = recalls[0, :]\n            precisions = precisions[0, :]\n            num_gts = num_gts.item()\n        mode = \'area\' if dataset != \'voc07\' else \'11points\'\n        ap = average_precision(recalls, precisions, mode)\n        eval_results.append({\n            \'num_gts\': num_gts,\n            \'num_dets\': num_dets,\n            \'recall\': recalls,\n            \'precision\': precisions,\n            \'ap\': ap\n        })\n    pool.close()\n    if scale_ranges is not None:\n        # shape (num_classes, num_scales)\n        all_ap = np.vstack([cls_result[\'ap\'] for cls_result in eval_results])\n        all_num_gts = np.vstack(\n            [cls_result[\'num_gts\'] for cls_result in eval_results])\n        mean_ap = []\n        for i in range(num_scales):\n            if np.any(all_num_gts[:, i] > 0):\n                mean_ap.append(all_ap[all_num_gts[:, i] > 0, i].mean())\n            else:\n                mean_ap.append(0.0)\n    else:\n        aps = []\n        for cls_result in eval_results:\n            if cls_result[\'num_gts\'] > 0:\n                aps.append(cls_result[\'ap\'])\n        mean_ap = np.array(aps).mean().item() if aps else 0.0\n\n    print_map_summary(\n        mean_ap, eval_results, dataset, area_ranges, logger=logger)\n\n    return mean_ap, eval_results\n\n\ndef print_map_summary(mean_ap,\n                      results,\n                      dataset=None,\n                      scale_ranges=None,\n                      logger=None):\n    """"""Print mAP and results of each class.\n\n    A table will be printed to show the gts/dets/recall/AP of each class and\n    the mAP.\n\n    Args:\n        mean_ap (float): Calculated from `eval_map()`.\n        results (list[dict]): Calculated from `eval_map()`.\n        dataset (list[str] | str | None): Dataset name or dataset classes.\n        scale_ranges (list[tuple] | None): Range of scales to be evaluated.\n        logger (logging.Logger | str | None): The way to print the mAP\n            summary. See `mmdet.utils.print_log()` for details. Default: None.\n    """"""\n\n    if logger == \'silent\':\n        return\n\n    if isinstance(results[0][\'ap\'], np.ndarray):\n        num_scales = len(results[0][\'ap\'])\n    else:\n        num_scales = 1\n\n    if scale_ranges is not None:\n        assert len(scale_ranges) == num_scales\n\n    num_classes = len(results)\n\n    recalls = np.zeros((num_scales, num_classes), dtype=np.float32)\n    aps = np.zeros((num_scales, num_classes), dtype=np.float32)\n    num_gts = np.zeros((num_scales, num_classes), dtype=int)\n    for i, cls_result in enumerate(results):\n        if cls_result[\'recall\'].size > 0:\n            recalls[:, i] = np.array(cls_result[\'recall\'], ndmin=2)[:, -1]\n        aps[:, i] = cls_result[\'ap\']\n        num_gts[:, i] = cls_result[\'num_gts\']\n\n    if dataset is None:\n        label_names = [str(i) for i in range(num_classes)]\n    elif mmcv.is_str(dataset):\n        label_names = get_classes(dataset)\n    else:\n        label_names = dataset\n\n    if not isinstance(mean_ap, list):\n        mean_ap = [mean_ap]\n\n    header = [\'class\', \'gts\', \'dets\', \'recall\', \'ap\']\n    for i in range(num_scales):\n        if scale_ranges is not None:\n            print_log(f\'Scale range {scale_ranges[i]}\', logger=logger)\n        table_data = [header]\n        for j in range(num_classes):\n            row_data = [\n                label_names[j], num_gts[i, j], results[j][\'num_dets\'],\n                f\'{recalls[i, j]:.3f}\', f\'{aps[i, j]:.3f}\'\n            ]\n            table_data.append(row_data)\n        table_data.append([\'mAP\', \'\', \'\', \'\', f\'{mean_ap[i]:.3f}\'])\n        table = AsciiTable(table_data)\n        table.inner_footing_row_border = True\n        print_log(\'\\n\' + table.table, logger=logger)\n'"
mmdet/core/evaluation/recall.py,0,"b'from collections.abc import Sequence\n\nimport numpy as np\nfrom mmcv.utils import print_log\nfrom terminaltables import AsciiTable\n\nfrom .bbox_overlaps import bbox_overlaps\n\n\ndef _recalls(all_ious, proposal_nums, thrs):\n\n    img_num = all_ious.shape[0]\n    total_gt_num = sum([ious.shape[0] for ious in all_ious])\n\n    _ious = np.zeros((proposal_nums.size, total_gt_num), dtype=np.float32)\n    for k, proposal_num in enumerate(proposal_nums):\n        tmp_ious = np.zeros(0)\n        for i in range(img_num):\n            ious = all_ious[i][:, :proposal_num].copy()\n            gt_ious = np.zeros((ious.shape[0]))\n            if ious.size == 0:\n                tmp_ious = np.hstack((tmp_ious, gt_ious))\n                continue\n            for j in range(ious.shape[0]):\n                gt_max_overlaps = ious.argmax(axis=1)\n                max_ious = ious[np.arange(0, ious.shape[0]), gt_max_overlaps]\n                gt_idx = max_ious.argmax()\n                gt_ious[j] = max_ious[gt_idx]\n                box_idx = gt_max_overlaps[gt_idx]\n                ious[gt_idx, :] = -1\n                ious[:, box_idx] = -1\n            tmp_ious = np.hstack((tmp_ious, gt_ious))\n        _ious[k, :] = tmp_ious\n\n    _ious = np.fliplr(np.sort(_ious, axis=1))\n    recalls = np.zeros((proposal_nums.size, thrs.size))\n    for i, thr in enumerate(thrs):\n        recalls[:, i] = (_ious >= thr).sum(axis=1) / float(total_gt_num)\n\n    return recalls\n\n\ndef set_recall_param(proposal_nums, iou_thrs):\n    """"""Check proposal_nums and iou_thrs and set correct format.\n    """"""\n    if isinstance(proposal_nums, Sequence):\n        _proposal_nums = np.array(proposal_nums)\n    elif isinstance(proposal_nums, int):\n        _proposal_nums = np.array([proposal_nums])\n    else:\n        _proposal_nums = proposal_nums\n\n    if iou_thrs is None:\n        _iou_thrs = np.array([0.5])\n    elif isinstance(iou_thrs, Sequence):\n        _iou_thrs = np.array(iou_thrs)\n    elif isinstance(iou_thrs, float):\n        _iou_thrs = np.array([iou_thrs])\n    else:\n        _iou_thrs = iou_thrs\n\n    return _proposal_nums, _iou_thrs\n\n\ndef eval_recalls(gts,\n                 proposals,\n                 proposal_nums=None,\n                 iou_thrs=0.5,\n                 logger=None):\n    """"""Calculate recalls.\n\n    Args:\n        gts (list[ndarray]): a list of arrays of shape (n, 4)\n        proposals (list[ndarray]): a list of arrays of shape (k, 4) or (k, 5)\n        proposal_nums (int | Sequence[int]): Top N proposals to be evaluated.\n        iou_thrs (float | Sequence[float]): IoU thresholds. Default: 0.5.\n        logger (logging.Logger | str | None): The way to print the recall\n            summary. See `mmdet.utils.print_log()` for details. Default: None.\n\n    Returns:\n        ndarray: recalls of different ious and proposal nums\n    """"""\n\n    img_num = len(gts)\n    assert img_num == len(proposals)\n\n    proposal_nums, iou_thrs = set_recall_param(proposal_nums, iou_thrs)\n\n    all_ious = []\n    for i in range(img_num):\n        if proposals[i].ndim == 2 and proposals[i].shape[1] == 5:\n            scores = proposals[i][:, 4]\n            sort_idx = np.argsort(scores)[::-1]\n            img_proposal = proposals[i][sort_idx, :]\n        else:\n            img_proposal = proposals[i]\n        prop_num = min(img_proposal.shape[0], proposal_nums[-1])\n        if gts[i] is None or gts[i].shape[0] == 0:\n            ious = np.zeros((0, img_proposal.shape[0]), dtype=np.float32)\n        else:\n            ious = bbox_overlaps(gts[i], img_proposal[:prop_num, :4])\n        all_ious.append(ious)\n    all_ious = np.array(all_ious)\n    recalls = _recalls(all_ious, proposal_nums, iou_thrs)\n\n    print_recall_summary(recalls, proposal_nums, iou_thrs, logger=logger)\n    return recalls\n\n\ndef print_recall_summary(recalls,\n                         proposal_nums,\n                         iou_thrs,\n                         row_idxs=None,\n                         col_idxs=None,\n                         logger=None):\n    """"""Print recalls in a table.\n\n    Args:\n        recalls (ndarray): calculated from `bbox_recalls`\n        proposal_nums (ndarray or list): top N proposals\n        iou_thrs (ndarray or list): iou thresholds\n        row_idxs (ndarray): which rows(proposal nums) to print\n        col_idxs (ndarray): which cols(iou thresholds) to print\n        logger (logging.Logger | str | None): The way to print the recall\n            summary. See `mmdet.utils.print_log()` for details. Default: None.\n    """"""\n    proposal_nums = np.array(proposal_nums, dtype=np.int32)\n    iou_thrs = np.array(iou_thrs)\n    if row_idxs is None:\n        row_idxs = np.arange(proposal_nums.size)\n    if col_idxs is None:\n        col_idxs = np.arange(iou_thrs.size)\n    row_header = [\'\'] + iou_thrs[col_idxs].tolist()\n    table_data = [row_header]\n    for i, num in enumerate(proposal_nums[row_idxs]):\n        row = [f\'{val:.3f}\' for val in recalls[row_idxs[i], col_idxs].tolist()]\n        row.insert(0, num)\n        table_data.append(row)\n    table = AsciiTable(table_data)\n    print_log(\'\\n\' + table.table, logger=logger)\n\n\ndef plot_num_recall(recalls, proposal_nums):\n    """"""Plot Proposal_num-Recalls curve.\n\n    Args:\n        recalls(ndarray or list): shape (k,)\n        proposal_nums(ndarray or list): same shape as `recalls`\n    """"""\n    if isinstance(proposal_nums, np.ndarray):\n        _proposal_nums = proposal_nums.tolist()\n    else:\n        _proposal_nums = proposal_nums\n    if isinstance(recalls, np.ndarray):\n        _recalls = recalls.tolist()\n    else:\n        _recalls = recalls\n\n    import matplotlib.pyplot as plt\n    f = plt.figure()\n    plt.plot([0] + _proposal_nums, [0] + _recalls)\n    plt.xlabel(\'Proposal num\')\n    plt.ylabel(\'Recall\')\n    plt.axis([0, proposal_nums.max(), 0, 1])\n    f.show()\n\n\ndef plot_iou_recall(recalls, iou_thrs):\n    """"""Plot IoU-Recalls curve.\n\n    Args:\n        recalls(ndarray or list): shape (k,)\n        iou_thrs(ndarray or list): same shape as `recalls`\n    """"""\n    if isinstance(iou_thrs, np.ndarray):\n        _iou_thrs = iou_thrs.tolist()\n    else:\n        _iou_thrs = iou_thrs\n    if isinstance(recalls, np.ndarray):\n        _recalls = recalls.tolist()\n    else:\n        _recalls = recalls\n\n    import matplotlib.pyplot as plt\n    f = plt.figure()\n    plt.plot(_iou_thrs + [1.0], _recalls + [0.])\n    plt.xlabel(\'IoU\')\n    plt.ylabel(\'Recall\')\n    plt.axis([iou_thrs.min(), 1, 0, 1])\n    f.show()\n'"
mmdet/core/fp16/__init__.py,0,"b""from .decorators import auto_fp16, force_fp32\nfrom .hooks import Fp16OptimizerHook, wrap_fp16_model\n\n__all__ = ['auto_fp16', 'force_fp32', 'Fp16OptimizerHook', 'wrap_fp16_model']\n"""
mmdet/core/fp16/decorators.py,12,"b'import functools\nfrom inspect import getfullargspec\n\nimport torch\n\nfrom .utils import cast_tensor_type\n\n\ndef auto_fp16(apply_to=None, out_fp32=False):\n    """"""Decorator to enable fp16 training automatically.\n\n    This decorator is useful when you write custom modules and want to support\n    mixed precision training. If inputs arguments are fp32 tensors, they will\n    be converted to fp16 automatically. Arguments other than fp32 tensors are\n    ignored.\n\n    Args:\n        apply_to (Iterable, optional): The argument names to be converted.\n            `None` indicates all arguments.\n        out_fp32 (bool): Whether to convert the output back to fp32.\n\n    Example:\n\n        >>> import torch.nn as nn\n        >>> class MyModule1(nn.Module):\n        >>>\n        >>>     # Convert x and y to fp16\n        >>>     @auto_fp16()\n        >>>     def forward(self, x, y):\n        >>>         pass\n\n        >>> import torch.nn as nn\n        >>> class MyModule2(nn.Module):\n        >>>\n        >>>     # convert pred to fp16\n        >>>     @auto_fp16(apply_to=(\'pred\', ))\n        >>>     def do_something(self, pred, others):\n        >>>         pass\n    """"""\n\n    def auto_fp16_wrapper(old_func):\n\n        @functools.wraps(old_func)\n        def new_func(*args, **kwargs):\n            # check if the module has set the attribute `fp16_enabled`, if not,\n            # just fallback to the original method.\n            if not isinstance(args[0], torch.nn.Module):\n                raise TypeError(\'@auto_fp16 can only be used to decorate the \'\n                                \'method of nn.Module\')\n            if not (hasattr(args[0], \'fp16_enabled\') and args[0].fp16_enabled):\n                return old_func(*args, **kwargs)\n            # get the arg spec of the decorated method\n            args_info = getfullargspec(old_func)\n            # get the argument names to be casted\n            args_to_cast = args_info.args if apply_to is None else apply_to\n            # convert the args that need to be processed\n            new_args = []\n            # NOTE: default args are not taken into consideration\n            if args:\n                arg_names = args_info.args[:len(args)]\n                for i, arg_name in enumerate(arg_names):\n                    if arg_name in args_to_cast:\n                        new_args.append(\n                            cast_tensor_type(args[i], torch.float, torch.half))\n                    else:\n                        new_args.append(args[i])\n            # convert the kwargs that need to be processed\n            new_kwargs = {}\n            if kwargs:\n                for arg_name, arg_value in kwargs.items():\n                    if arg_name in args_to_cast:\n                        new_kwargs[arg_name] = cast_tensor_type(\n                            arg_value, torch.float, torch.half)\n                    else:\n                        new_kwargs[arg_name] = arg_value\n            # apply converted arguments to the decorated method\n            output = old_func(*new_args, **new_kwargs)\n            # cast the results back to fp32 if necessary\n            if out_fp32:\n                output = cast_tensor_type(output, torch.half, torch.float)\n            return output\n\n        return new_func\n\n    return auto_fp16_wrapper\n\n\ndef force_fp32(apply_to=None, out_fp16=False):\n    """"""Decorator to convert input arguments to fp32 in force.\n\n    This decorator is useful when you write custom modules and want to support\n    mixed precision training. If there are some inputs that must be processed\n    in fp32 mode, then this decorator can handle it. If inputs arguments are\n    fp16 tensors, they will be converted to fp32 automatically. Arguments other\n    than fp16 tensors are ignored.\n\n    Args:\n        apply_to (Iterable, optional): The argument names to be converted.\n            `None` indicates all arguments.\n        out_fp16 (bool): Whether to convert the output back to fp16.\n\n    Example:\n\n        >>> import torch.nn as nn\n        >>> class MyModule1(nn.Module):\n        >>>\n        >>>     # Convert x and y to fp32\n        >>>     @force_fp32()\n        >>>     def loss(self, x, y):\n        >>>         pass\n\n        >>> import torch.nn as nn\n        >>> class MyModule2(nn.Module):\n        >>>\n        >>>     # convert pred to fp32\n        >>>     @force_fp32(apply_to=(\'pred\', ))\n        >>>     def post_process(self, pred, others):\n        >>>         pass\n    """"""\n\n    def force_fp32_wrapper(old_func):\n\n        @functools.wraps(old_func)\n        def new_func(*args, **kwargs):\n            # check if the module has set the attribute `fp16_enabled`, if not,\n            # just fallback to the original method.\n            if not isinstance(args[0], torch.nn.Module):\n                raise TypeError(\'@force_fp32 can only be used to decorate the \'\n                                \'method of nn.Module\')\n            if not (hasattr(args[0], \'fp16_enabled\') and args[0].fp16_enabled):\n                return old_func(*args, **kwargs)\n            # get the arg spec of the decorated method\n            args_info = getfullargspec(old_func)\n            # get the argument names to be casted\n            args_to_cast = args_info.args if apply_to is None else apply_to\n            # convert the args that need to be processed\n            new_args = []\n            if args:\n                arg_names = args_info.args[:len(args)]\n                for i, arg_name in enumerate(arg_names):\n                    if arg_name in args_to_cast:\n                        new_args.append(\n                            cast_tensor_type(args[i], torch.half, torch.float))\n                    else:\n                        new_args.append(args[i])\n            # convert the kwargs that need to be processed\n            new_kwargs = dict()\n            if kwargs:\n                for arg_name, arg_value in kwargs.items():\n                    if arg_name in args_to_cast:\n                        new_kwargs[arg_name] = cast_tensor_type(\n                            arg_value, torch.half, torch.float)\n                    else:\n                        new_kwargs[arg_name] = arg_value\n            # apply converted arguments to the decorated method\n            output = old_func(*new_args, **new_kwargs)\n            # cast the results back to fp32 if necessary\n            if out_fp16:\n                output = cast_tensor_type(output, torch.float, torch.half)\n            return output\n\n        return new_func\n\n    return force_fp32_wrapper\n'"
mmdet/core/fp16/hooks.py,6,"b'import copy\n\nimport torch\nimport torch.nn as nn\nfrom mmcv.runner import OptimizerHook\n\nfrom ..utils.dist_utils import allreduce_grads\nfrom .utils import cast_tensor_type\n\n\nclass Fp16OptimizerHook(OptimizerHook):\n    """"""FP16 optimizer hook.\n\n    The steps of fp16 optimizer is as follows.\n    1. Scale the loss value.\n    2. BP in the fp16 model.\n    2. Copy gradients from fp16 model to fp32 weights.\n    3. Update fp32 weights.\n    4. Copy updated parameters from fp32 weights to fp16 model.\n\n    Refer to https://arxiv.org/abs/1710.03740 for more details.\n\n    Args:\n        loss_scale (float): Scale factor multiplied with loss.\n    """"""\n\n    def __init__(self,\n                 grad_clip=None,\n                 coalesce=True,\n                 bucket_size_mb=-1,\n                 loss_scale=512.,\n                 distributed=True):\n        self.grad_clip = grad_clip\n        self.coalesce = coalesce\n        self.bucket_size_mb = bucket_size_mb\n        self.loss_scale = loss_scale\n        self.distributed = distributed\n\n    def before_run(self, runner):\n        # keep a copy of fp32 weights\n        runner.optimizer.param_groups = copy.deepcopy(\n            runner.optimizer.param_groups)\n        # convert model to fp16\n        wrap_fp16_model(runner.model)\n\n    def copy_grads_to_fp32(self, fp16_net, fp32_weights):\n        """"""Copy gradients from fp16 model to fp32 weight copy.""""""\n        for fp32_param, fp16_param in zip(fp32_weights, fp16_net.parameters()):\n            if fp16_param.grad is not None:\n                if fp32_param.grad is None:\n                    fp32_param.grad = fp32_param.data.new(fp32_param.size())\n                fp32_param.grad.copy_(fp16_param.grad)\n\n    def copy_params_to_fp16(self, fp16_net, fp32_weights):\n        """"""Copy updated params from fp32 weight copy to fp16 model.""""""\n        for fp16_param, fp32_param in zip(fp16_net.parameters(), fp32_weights):\n            fp16_param.data.copy_(fp32_param.data)\n\n    def after_train_iter(self, runner):\n        # clear grads of last iteration\n        runner.model.zero_grad()\n        runner.optimizer.zero_grad()\n        # scale the loss value\n        scaled_loss = runner.outputs[\'loss\'] * self.loss_scale\n        scaled_loss.backward()\n        # copy fp16 grads in the model to fp32 params in the optimizer\n        fp32_weights = []\n        for param_group in runner.optimizer.param_groups:\n            fp32_weights += param_group[\'params\']\n        self.copy_grads_to_fp32(runner.model, fp32_weights)\n        # allreduce grads\n        if self.distributed:\n            allreduce_grads(fp32_weights, self.coalesce, self.bucket_size_mb)\n        # scale the gradients back\n        for param in fp32_weights:\n            if param.grad is not None:\n                param.grad.div_(self.loss_scale)\n        if self.grad_clip is not None:\n            self.clip_grads(fp32_weights)\n        # update fp32 params\n        runner.optimizer.step()\n        # copy fp32 params to the fp16 model\n        self.copy_params_to_fp16(runner.model, fp32_weights)\n\n\ndef wrap_fp16_model(model):\n    # convert model to fp16\n    model.half()\n    # patch the normalization layers to make it work in fp32 mode\n    patch_norm_fp32(model)\n    # set `fp16_enabled` flag\n    for m in model.modules():\n        if hasattr(m, \'fp16_enabled\'):\n            m.fp16_enabled = True\n\n\ndef patch_norm_fp32(module):\n    if isinstance(module, (nn.modules.batchnorm._BatchNorm, nn.GroupNorm)):\n        module.float()\n        if isinstance(module, nn.GroupNorm) or torch.__version__ < \'1.3\':\n            module.forward = patch_forward_method(module.forward, torch.half,\n                                                  torch.float)\n    for child in module.children():\n        patch_norm_fp32(child)\n    return module\n\n\ndef patch_forward_method(func, src_type, dst_type, convert_output=True):\n    """"""Patch the forward method of a module.\n\n    Args:\n        func (callable): The original forward method.\n        src_type (torch.dtype): Type of input arguments to be converted from.\n        dst_type (torch.dtype): Type of input arguments to be converted to.\n        convert_output (bool): Whether to convert the output back to src_type.\n\n    Returns:\n        callable: The patched forward method.\n    """"""\n\n    def new_forward(*args, **kwargs):\n        output = func(*cast_tensor_type(args, src_type, dst_type),\n                      **cast_tensor_type(kwargs, src_type, dst_type))\n        if convert_output:\n            output = cast_tensor_type(output, dst_type, src_type)\n        return output\n\n    return new_forward\n'"
mmdet/core/fp16/utils.py,1,"b'from collections import abc\n\nimport numpy as np\nimport torch\n\n\ndef cast_tensor_type(inputs, src_type, dst_type):\n    if isinstance(inputs, torch.Tensor):\n        return inputs.to(dst_type)\n    elif isinstance(inputs, str):\n        return inputs\n    elif isinstance(inputs, np.ndarray):\n        return inputs\n    elif isinstance(inputs, abc.Mapping):\n        return type(inputs)({\n            k: cast_tensor_type(v, src_type, dst_type)\n            for k, v in inputs.items()\n        })\n    elif isinstance(inputs, abc.Iterable):\n        return type(inputs)(\n            cast_tensor_type(item, src_type, dst_type) for item in inputs)\n    else:\n        return inputs\n'"
mmdet/core/mask/__init__.py,0,"b""from .mask_target import mask_target\nfrom .structures import BitmapMasks, PolygonMasks\nfrom .utils import encode_mask_results, split_combined_polys\n\n__all__ = [\n    'split_combined_polys', 'mask_target', 'BitmapMasks', 'PolygonMasks',\n    'encode_mask_results'\n]\n"""
mmdet/core/mask/mask_target.py,3,"b'import numpy as np\nimport torch\nfrom torch.nn.modules.utils import _pair\n\n\ndef mask_target(pos_proposals_list, pos_assigned_gt_inds_list, gt_masks_list,\n                cfg):\n    cfg_list = [cfg for _ in range(len(pos_proposals_list))]\n    mask_targets = map(mask_target_single, pos_proposals_list,\n                       pos_assigned_gt_inds_list, gt_masks_list, cfg_list)\n    mask_targets = list(mask_targets)\n    if len(mask_targets) > 0:\n        mask_targets = torch.cat(mask_targets)\n    return mask_targets\n\n\ndef mask_target_single(pos_proposals, pos_assigned_gt_inds, gt_masks, cfg):\n    device = pos_proposals.device\n    mask_size = _pair(cfg.mask_size)\n    num_pos = pos_proposals.size(0)\n    if num_pos > 0:\n        proposals_np = pos_proposals.cpu().numpy()\n        maxh, maxw = gt_masks.height, gt_masks.width\n        proposals_np[:, [0, 2]] = np.clip(proposals_np[:, [0, 2]], 0, maxw)\n        proposals_np[:, [1, 3]] = np.clip(proposals_np[:, [1, 3]], 0, maxh)\n        pos_assigned_gt_inds = pos_assigned_gt_inds.cpu().numpy()\n\n        mask_targets = gt_masks.crop_and_resize(\n            proposals_np, mask_size, device=device,\n            inds=pos_assigned_gt_inds).to_ndarray()\n\n        mask_targets = torch.from_numpy(mask_targets).float().to(device)\n    else:\n        mask_targets = pos_proposals.new_zeros((0, ) + mask_size)\n\n    return mask_targets\n'"
mmdet/core/mask/structures.py,8,"b'from abc import ABCMeta, abstractmethod\n\nimport mmcv\nimport numpy as np\nimport pycocotools.mask as maskUtils\nimport torch\n\nfrom mmdet.ops.roi_align import roi_align\n\n\nclass BaseInstanceMasks(metaclass=ABCMeta):\n\n    @abstractmethod\n    def rescale(self, scale, interpolation=\'nearest\'):\n        pass\n\n    @abstractmethod\n    def resize(self, out_shape, interpolation=\'nearest\'):\n        pass\n\n    @abstractmethod\n    def flip(self, flip_direction=\'horizontal\'):\n        pass\n\n    @abstractmethod\n    def pad(self, out_shape, pad_val):\n        pass\n\n    @abstractmethod\n    def crop(self, bbox):\n        pass\n\n    @abstractmethod\n    def crop_and_resize(self,\n                        bboxes,\n                        out_shape,\n                        inds,\n                        interpolation=\'bilinear\'):\n        pass\n\n    @abstractmethod\n    def expand(self, expanded_h, expanded_w, top, left):\n        pass\n\n    @property\n    @abstractmethod\n    def areas(self):\n        pass\n\n    @abstractmethod\n    def to_ndarray(self):\n        pass\n\n    @abstractmethod\n    def to_tensor(self, dtype, device):\n        pass\n\n\nclass BitmapMasks(BaseInstanceMasks):\n    """"""This class represents masks in the form of bitmaps.\n\n    Args:\n        masks (ndarray): ndarray of masks in shape (N, H, W), where N is\n            the number of objects.\n        height (int): height of masks\n        width (int): width of masks\n    """"""\n\n    def __init__(self, masks, height, width):\n        self.height = height\n        self.width = width\n        if len(masks) == 0:\n            self.masks = np.empty((0, self.height, self.width), dtype=np.uint8)\n        else:\n            assert isinstance(masks, (list, np.ndarray))\n            if isinstance(masks, list):\n                assert isinstance(masks[0], np.ndarray)\n                assert masks[0].ndim == 2  # (H, W)\n            else:\n                assert masks.ndim == 3  # (N, H, W)\n\n            self.masks = np.stack(masks).reshape(-1, height, width)\n            assert self.masks.shape[1] == self.height\n            assert self.masks.shape[2] == self.width\n\n    def __getitem__(self, index):\n        masks = self.masks[index].reshape(-1, self.height, self.width)\n        return BitmapMasks(masks, self.height, self.width)\n\n    def __iter__(self):\n        return iter(self.masks)\n\n    def __repr__(self):\n        s = self.__class__.__name__ + \'(\'\n        s += f\'num_masks={len(self.masks)}, \'\n        s += f\'height={self.height}, \'\n        s += f\'width={self.width})\'\n        return s\n\n    def __len__(self):\n        return len(self.masks)\n\n    def rescale(self, scale, interpolation=\'nearest\'):\n        """"""Rescale masks as large as possible while keeping the aspect ratio.\n        For details can refer to `mmcv.imrescale`\n\n        Args:\n            scale (tuple[int]): the maximum size (h, w) of rescaled mask\n            interpolation (str): same as :func:`mmcv.imrescale`\n\n        Returns:\n            BitmapMasks: the rescaled masks\n        """"""\n        if len(self.masks) == 0:\n            new_w, new_h = mmcv.rescale_size((self.width, self.height), scale)\n            rescaled_masks = np.empty((0, new_h, new_w), dtype=np.uint8)\n        else:\n            rescaled_masks = np.stack([\n                mmcv.imrescale(mask, scale, interpolation=interpolation)\n                for mask in self.masks\n            ])\n        height, width = rescaled_masks.shape[1:]\n        return BitmapMasks(rescaled_masks, height, width)\n\n    def resize(self, out_shape, interpolation=\'nearest\'):\n        """"""Resize masks to the given out_shape.\n\n        Args:\n            out_shape: target (h, w) of resized mask\n            interpolation (str): see `mmcv.imresize`\n\n        Returns:\n            BitmapMasks: the resized masks\n        """"""\n        if len(self.masks) == 0:\n            resized_masks = np.empty((0, *out_shape), dtype=np.uint8)\n        else:\n            resized_masks = np.stack([\n                mmcv.imresize(mask, out_shape, interpolation=interpolation)\n                for mask in self.masks\n            ])\n        return BitmapMasks(resized_masks, *out_shape)\n\n    def flip(self, flip_direction=\'horizontal\'):\n        """"""flip masks alone the given direction.\n\n        Args:\n            flip_direction (str): either \'horizontal\' or \'vertical\'\n\n        Returns:\n            BitmapMasks: the flipped masks\n        """"""\n        assert flip_direction in (\'horizontal\', \'vertical\')\n\n        if len(self.masks) == 0:\n            flipped_masks = self.masks\n        else:\n            flipped_masks = np.stack([\n                mmcv.imflip(mask, direction=flip_direction)\n                for mask in self.masks\n            ])\n        return BitmapMasks(flipped_masks, self.height, self.width)\n\n    def pad(self, out_shape, pad_val=0):\n        """"""Pad masks to the given size of (h, w).\n\n        Args:\n            out_shape (tuple[int]): target (h, w) of padded mask\n            pad_val (int): the padded value\n\n        Returns:\n            BitmapMasks: the padded masks\n        """"""\n        if len(self.masks) == 0:\n            padded_masks = np.empty((0, *out_shape), dtype=np.uint8)\n        else:\n            padded_masks = np.stack([\n                mmcv.impad(mask, out_shape, pad_val=pad_val)\n                for mask in self.masks\n            ])\n        return BitmapMasks(padded_masks, *out_shape)\n\n    def crop(self, bbox):\n        """"""Crop each mask by the given bbox.\n\n        Args:\n            bbox (ndarray): bbox in format [x1, y1, x2, y2], shape (4, )\n\n        Return:\n            BitmapMasks: the cropped masks.\n        """"""\n        assert isinstance(bbox, np.ndarray)\n        assert bbox.ndim == 1\n\n        # clip the boundary\n        bbox = bbox.copy()\n        bbox[0::2] = np.clip(bbox[0::2], 0, self.width)\n        bbox[1::2] = np.clip(bbox[1::2], 0, self.height)\n        x1, y1, x2, y2 = bbox\n        w = np.maximum(x2 - x1, 1)\n        h = np.maximum(y2 - y1, 1)\n\n        if len(self.masks) == 0:\n            cropped_masks = np.empty((0, h, w), dtype=np.uint8)\n        else:\n            cropped_masks = self.masks[:, y1:y1 + h, x1:x1 + w]\n        return BitmapMasks(cropped_masks, h, w)\n\n    def crop_and_resize(self,\n                        bboxes,\n                        out_shape,\n                        inds,\n                        device=\'cpu\',\n                        interpolation=\'bilinear\'):\n        """"""Crop and resize masks by the given bboxes.\n\n        This function is mainly used in mask targets computation.\n        It firstly align mask to bboxes by assigned_inds, then crop mask by the\n        assigned bbox and resize to the size of (mask_h, mask_w)\n\n        Args:\n            bboxes (Tensor): bboxes in format [x1, y1, x2, y2], shape (N, 4)\n            out_shape (tuple[int]): target (h, w) of resized mask\n            inds (ndarray): indexes to assign masks to each bbox\n            device (str): device of bboxes\n            interpolation (str): see `mmcv.imresize`\n\n        Return:\n            ndarray: the cropped and resized masks.\n        """"""\n        if len(self.masks) == 0:\n            empty_masks = np.empty((0, *out_shape), dtype=np.uint8)\n            return BitmapMasks(empty_masks, *out_shape)\n\n        # convert bboxes to tensor\n        if isinstance(bboxes, np.ndarray):\n            bboxes = torch.from_numpy(bboxes).to(device=device)\n        if isinstance(inds, np.ndarray):\n            inds = torch.from_numpy(inds).to(device=device)\n\n        num_bbox = bboxes.shape[0]\n        fake_inds = torch.arange(\n            num_bbox, device=device).to(dtype=bboxes.dtype)[:, None]\n        rois = torch.cat([fake_inds, bboxes], dim=1)  # Nx5\n        rois = rois.to(device=device)\n        if num_bbox > 0:\n            gt_masks_th = torch.from_numpy(self.masks).to(device).index_select(\n                0, inds).to(dtype=rois.dtype)\n            targets = roi_align(gt_masks_th[:, None, :, :], rois, out_shape,\n                                1.0, 0, True).squeeze(1)\n            resized_masks = (targets >= 0.5).cpu().numpy()\n        else:\n            resized_masks = []\n        return BitmapMasks(resized_masks, *out_shape)\n\n    def expand(self, expanded_h, expanded_w, top, left):\n        """"""see `transforms.Expand`.""""""\n        if len(self.masks) == 0:\n            expanded_mask = np.empty((0, expanded_h, expanded_w),\n                                     dtype=np.uint8)\n        else:\n            expanded_mask = np.zeros((len(self), expanded_h, expanded_w),\n                                     dtype=np.uint8)\n            expanded_mask[:, top:top + self.height,\n                          left:left + self.width] = self.masks\n        return BitmapMasks(expanded_mask, expanded_h, expanded_w)\n\n    @property\n    def areas(self):\n        """"""Compute area of each instance\n\n        Return:\n            ndarray: areas of each instance\n        """"""\n        return self.masks.sum((1, 2))\n\n    def to_ndarray(self):\n        return self.masks\n\n    def to_tensor(self, dtype, device):\n        return torch.tensor(self.masks, dtype=dtype, device=device)\n\n\nclass PolygonMasks(BaseInstanceMasks):\n    """"""This class represents masks in the form of polygons.\n\n    Polygons is a list of three levels. The first level of the list\n    corresponds to objects, the second level to the polys that compose the\n    object, the third level to the poly coordinates\n\n    Args:\n        masks (list[list[ndarray]]): The first level of the list\n            corresponds to objects, the second level to the polys that\n            compose the object, the third level to the poly coordinates\n        height (int): height of masks\n        width (int): width of masks\n    """"""\n\n    def __init__(self, masks, height, width):\n        assert isinstance(masks, list)\n        if len(masks) > 0:\n            assert isinstance(masks[0], list)\n            assert isinstance(masks[0][0], np.ndarray)\n\n        self.height = height\n        self.width = width\n        self.masks = masks\n\n    def __getitem__(self, index):\n        if isinstance(index, np.ndarray):\n            index = index.tolist()\n        if isinstance(index, list):\n            masks = [self.masks[i] for i in index]\n        else:\n            try:\n                masks = self.masks[index]\n            except Exception:\n                raise ValueError(\n                    f\'Unsupported input of type {type(index)} for indexing!\')\n        if isinstance(masks[0], np.ndarray):\n            masks = [masks]  # ensure a list of three levels\n        return PolygonMasks(masks, self.height, self.width)\n\n    def __iter__(self):\n        return iter(self.masks)\n\n    def __repr__(self):\n        s = self.__class__.__name__ + \'(\'\n        s += f\'num_masks={len(self.masks)}, \'\n        s += f\'height={self.height}, \'\n        s += f\'width={self.width})\'\n        return s\n\n    def __len__(self):\n        return len(self.masks)\n\n    def rescale(self, scale, interpolation=None):\n        """"""see BitmapMasks.rescale""""""\n        new_w, new_h = mmcv.rescale_size((self.width, self.height), scale)\n        if len(self.masks) == 0:\n            rescaled_masks = PolygonMasks([], new_h, new_w)\n        else:\n            rescaled_masks = self.resize((new_h, new_w))\n        return rescaled_masks\n\n    def resize(self, out_shape, interpolation=None):\n        """"""see BitmapMasks.resize""""""\n        if len(self.masks) == 0:\n            resized_masks = PolygonMasks([], *out_shape)\n        else:\n            h_scale = out_shape[0] / self.height\n            w_scale = out_shape[1] / self.width\n            resized_masks = []\n            for poly_per_obj in self.masks:\n                resized_poly = []\n                for p in poly_per_obj:\n                    p = p.copy()\n                    p[0::2] *= w_scale\n                    p[1::2] *= h_scale\n                    resized_poly.append(p)\n                resized_masks.append(resized_poly)\n            resized_masks = PolygonMasks(resized_masks, *out_shape)\n        return resized_masks\n\n    def flip(self, flip_direction=\'horizontal\'):\n        """"""see BitmapMasks.flip""""""\n        assert flip_direction in (\'horizontal\', \'vertical\')\n        if len(self.masks) == 0:\n            flipped_masks = PolygonMasks([], self.height, self.width)\n        else:\n            if flip_direction == \'horizontal\':\n                dim = self.width\n                idx = 0\n            else:\n                dim = self.height\n                idx = 1\n            flipped_masks = []\n            for poly_per_obj in self.masks:\n                flipped_poly_per_obj = []\n                for p in poly_per_obj:\n                    p = p.copy()\n                    p[idx::2] = dim - p[idx::2]\n                    flipped_poly_per_obj.append(p)\n                flipped_masks.append(flipped_poly_per_obj)\n            flipped_masks = PolygonMasks(flipped_masks, self.height,\n                                         self.width)\n        return flipped_masks\n\n    def crop(self, bbox):\n        """"""see BitmapMasks.crop""""""\n        assert isinstance(bbox, np.ndarray)\n        assert bbox.ndim == 1\n\n        # clip the boundary\n        bbox = bbox.copy()\n        bbox[0::2] = np.clip(bbox[0::2], 0, self.width)\n        bbox[1::2] = np.clip(bbox[1::2], 0, self.height)\n        x1, y1, x2, y2 = bbox\n        w = np.maximum(x2 - x1, 1)\n        h = np.maximum(y2 - y1, 1)\n\n        if len(self.masks) == 0:\n            cropped_masks = PolygonMasks([], h, w)\n        else:\n            cropped_masks = []\n            for poly_per_obj in self.masks:\n                cropped_poly_per_obj = []\n                for p in poly_per_obj:\n                    # pycocotools will clip the boundary\n                    p = p.copy()\n                    p[0::2] -= bbox[0]\n                    p[1::2] -= bbox[1]\n                    cropped_poly_per_obj.append(p)\n                cropped_masks.append(cropped_poly_per_obj)\n            cropped_masks = PolygonMasks(cropped_masks, h, w)\n        return cropped_masks\n\n    def pad(self, out_shape, pad_val=0):\n        """"""padding has no effect on polygons""""""\n        return PolygonMasks(self.masks, *out_shape)\n\n    def expand(self, *args, **kwargs):\n        raise NotImplementedError\n\n    def crop_and_resize(self,\n                        bboxes,\n                        out_shape,\n                        inds,\n                        device=\'cpu\',\n                        interpolation=\'bilinear\'):\n        """"""see BitmapMasks.crop_and_resize""""""\n        out_h, out_w = out_shape\n        if len(self.masks) == 0:\n            return PolygonMasks([], out_h, out_w)\n\n        resized_masks = []\n        for i in range(len(bboxes)):\n            mask = self.masks[inds[i]]\n            bbox = bboxes[i, :]\n            x1, y1, x2, y2 = bbox\n            w = np.maximum(x2 - x1, 1)\n            h = np.maximum(y2 - y1, 1)\n            h_scale = out_h / max(h, 0.1)  # avoid too large scale\n            w_scale = out_w / max(w, 0.1)\n\n            resized_mask = []\n            for p in mask:\n                p = p.copy()\n                # crop\n                # pycocotools will clip the boundary\n                p[0::2] -= bbox[0]\n                p[1::2] -= bbox[1]\n\n                # resize\n                p[0::2] *= w_scale\n                p[1::2] *= h_scale\n                resized_mask.append(p)\n            resized_masks.append(resized_mask)\n        return PolygonMasks(resized_masks, *out_shape)\n\n    def to_bitmap(self):\n        """"""convert polygon masks to bitmap masks""""""\n        bitmap_masks = self.to_ndarray()\n        return BitmapMasks(bitmap_masks, self.height, self.width)\n\n    @property\n    def areas(self):\n        """"""Compute areas of masks.\n\n        This func is modified from\n        https://github.com/facebookresearch/detectron2/blob/ffff8acc35ea88ad1cb1806ab0f00b4c1c5dbfd9/detectron2/structures/masks.py#L387\n        Only works with Polygons, using the shoelace formula\n\n        Return:\n            ndarray: areas of each instance\n        """"""  # noqa: W501\n        area = []\n        for polygons_per_obj in self.masks:\n            area_per_obj = 0\n            for p in polygons_per_obj:\n                area_per_obj += self._polygon_area(p[0::2], p[1::2])\n            area.append(area_per_obj)\n        return np.asarray(area)\n\n    def _polygon_area(self, x, y):\n        """"""Compute the area of a component of a polygon.\n\n        Using the shoelace formula:\n        https://stackoverflow.com/questions/24467972/calculate-area-of-polygon-given-x-y-coordinates\n\n        Args:\n            x (ndarray): x coordinates of the component\n            y (ndarray): y coordinates of the component\n\n        Return:\n            float: the are of the component\n        """"""  # noqa: 501\n        return 0.5 * np.abs(\n            np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n\n    def to_ndarray(self):\n        if len(self.masks) == 0:\n            return np.empty((0, self.height, self.width), dtype=np.uint8)\n        bitmap_masks = []\n        for poly_per_obj in self.masks:\n            bitmap_masks.append(\n                polygon_to_bitmap(poly_per_obj, self.height, self.width))\n        return np.stack(bitmap_masks)\n\n    def to_tensor(self, dtype, device):\n        if len(self.masks) == 0:\n            return torch.empty((0, self.height, self.width),\n                               dtype=dtype,\n                               device=device)\n        ndarray_masks = self.to_ndarray()\n        return torch.tensor(ndarray_masks, dtype=dtype, device=device)\n\n\ndef polygon_to_bitmap(polygons, height, width):\n    """"""Convert masks from the form of polygons to bitmaps.\n\n        Args:\n            polygons (list[ndarray]): masks in polygon representation\n            height (int): mask height\n            width (int): mask width\n\n        Return:\n            ndarray: the converted masks in bitmap representation\n    """"""\n    rles = maskUtils.frPyObjects(polygons, height, width)\n    rle = maskUtils.merge(rles)\n    bitmap_mask = maskUtils.decode(rle).astype(np.bool)\n    return bitmap_mask\n'"
mmdet/core/mask/utils.py,0,"b'import mmcv\nimport numpy as np\nimport pycocotools.mask as mask_util\n\n\ndef split_combined_polys(polys, poly_lens, polys_per_mask):\n    """"""Split the combined 1-D polys into masks.\n\n    A mask is represented as a list of polys, and a poly is represented as\n    a 1-D array. In dataset, all masks are concatenated into a single 1-D\n    tensor. Here we need to split the tensor into original representations.\n\n    Args:\n        polys (list): a list (length = image num) of 1-D tensors\n        poly_lens (list): a list (length = image num) of poly length\n        polys_per_mask (list): a list (length = image num) of poly number\n            of each mask\n\n    Returns:\n        list: a list (length = image num) of list (length = mask num) of\n            list (length = poly num) of numpy array\n    """"""\n    mask_polys_list = []\n    for img_id in range(len(polys)):\n        polys_single = polys[img_id]\n        polys_lens_single = poly_lens[img_id].tolist()\n        polys_per_mask_single = polys_per_mask[img_id].tolist()\n\n        split_polys = mmcv.slice_list(polys_single, polys_lens_single)\n        mask_polys = mmcv.slice_list(split_polys, polys_per_mask_single)\n        mask_polys_list.append(mask_polys)\n    return mask_polys_list\n\n\n# TODO: move this function to more proper place\ndef encode_mask_results(mask_results):\n    """"""Encode bitmap mask to RLE code.\n\n    Args:\n        mask_results (list | tuple[list]): bitmap mask results.\n            In mask scoring rcnn, mask_results is a tuple of (segm_results,\n            segm_cls_score).\n\n    Returns:\n        list | tuple: RLE encoded mask.\n    """"""\n    if isinstance(mask_results, tuple):  # mask scoring\n        cls_segms, cls_mask_scores = mask_results\n    else:\n        cls_segms = mask_results\n    num_classes = len(cls_segms)\n    encoded_mask_results = [[] for _ in range(num_classes)]\n    for i in range(len(cls_segms)):\n        for cls_segm in cls_segms[i]:\n            encoded_mask_results[i].append(\n                mask_util.encode(\n                    np.array(\n                        cls_segm[:, :, np.newaxis], order=\'F\',\n                        dtype=\'uint8\'))[0])  # encoded with RLE\n    if isinstance(mask_results, tuple):\n        return encoded_mask_results, cls_mask_scores\n    else:\n        return encoded_mask_results\n'"
mmdet/core/post_processing/__init__.py,0,"b""from .bbox_nms import multiclass_nms\nfrom .merge_augs import (merge_aug_bboxes, merge_aug_masks,\n                         merge_aug_proposals, merge_aug_scores)\n\n__all__ = [\n    'multiclass_nms', 'merge_aug_proposals', 'merge_aug_bboxes',\n    'merge_aug_scores', 'merge_aug_masks'\n]\n"""
mmdet/core/post_processing/bbox_nms.py,1,"b'import torch\n\nfrom mmdet.ops.nms import batched_nms\n\n\ndef multiclass_nms(multi_bboxes,\n                   multi_scores,\n                   score_thr,\n                   nms_cfg,\n                   max_num=-1,\n                   score_factors=None):\n    """"""NMS for multi-class bboxes.\n\n    Args:\n        multi_bboxes (Tensor): shape (n, #class*4) or (n, 4)\n        multi_scores (Tensor): shape (n, #class), where the last column\n            contains scores of the background class, but this will be ignored.\n        score_thr (float): bbox threshold, bboxes with scores lower than it\n            will not be considered.\n        nms_thr (float): NMS IoU threshold\n        max_num (int): if there are more than max_num bboxes after NMS,\n            only top max_num will be kept.\n        score_factors (Tensor): The factors multiplied to scores before\n            applying NMS\n\n    Returns:\n        tuple: (bboxes, labels), tensors of shape (k, 5) and (k, 1). Labels\n            are 0-based.\n    """"""\n    num_classes = multi_scores.size(1) - 1\n    # exclude background category\n    if multi_bboxes.shape[1] > 4:\n        bboxes = multi_bboxes.view(multi_scores.size(0), -1, 4)\n    else:\n        bboxes = multi_bboxes[:, None].expand(-1, num_classes, 4)\n    scores = multi_scores[:, :-1]\n\n    # filter out boxes with low scores\n    valid_mask = scores > score_thr\n    bboxes = bboxes[valid_mask]\n    if score_factors is not None:\n        scores = scores * score_factors[:, None]\n    scores = scores[valid_mask]\n    labels = valid_mask.nonzero()[:, 1]\n\n    if bboxes.numel() == 0:\n        bboxes = multi_bboxes.new_zeros((0, 5))\n        labels = multi_bboxes.new_zeros((0, ), dtype=torch.long)\n        return bboxes, labels\n\n    dets, keep = batched_nms(bboxes, scores, labels, nms_cfg)\n\n    if max_num > 0:\n        dets = dets[:max_num]\n        keep = keep[:max_num]\n\n    return dets, labels[keep]\n'"
mmdet/core/post_processing/merge_augs.py,5,"b'import numpy as np\nimport torch\n\nfrom mmdet.ops import nms\nfrom ..bbox import bbox_mapping_back\n\n\ndef merge_aug_proposals(aug_proposals, img_metas, rpn_test_cfg):\n    """"""Merge augmented proposals (multiscale, flip, etc.)\n\n    Args:\n        aug_proposals (list[Tensor]): proposals from different testing\n            schemes, shape (n, 5). Note that they are not rescaled to the\n            original image size.\n\n        img_metas (list[dict]): list of image info dict where each dict has:\n            \'img_shape\', \'scale_factor\', \'flip\', and my also contain\n            \'filename\', \'ori_shape\', \'pad_shape\', and \'img_norm_cfg\'.\n            For details on the values of these keys see\n            `mmdet/datasets/pipelines/formatting.py:Collect`.\n\n        rpn_test_cfg (dict): rpn test config.\n\n    Returns:\n        Tensor: shape (n, 4), proposals corresponding to original image scale.\n    """"""\n    recovered_proposals = []\n    for proposals, img_info in zip(aug_proposals, img_metas):\n        img_shape = img_info[\'img_shape\']\n        scale_factor = img_info[\'scale_factor\']\n        flip = img_info[\'flip\']\n        flip_direction = img_info[\'flip_direction\']\n        _proposals = proposals.clone()\n        _proposals[:, :4] = bbox_mapping_back(_proposals[:, :4], img_shape,\n                                              scale_factor, flip,\n                                              flip_direction)\n        recovered_proposals.append(_proposals)\n    aug_proposals = torch.cat(recovered_proposals, dim=0)\n    merged_proposals, _ = nms(aug_proposals, rpn_test_cfg.nms_thr)\n    scores = merged_proposals[:, 4]\n    _, order = scores.sort(0, descending=True)\n    num = min(rpn_test_cfg.max_num, merged_proposals.shape[0])\n    order = order[:num]\n    merged_proposals = merged_proposals[order, :]\n    return merged_proposals\n\n\ndef merge_aug_bboxes(aug_bboxes, aug_scores, img_metas, rcnn_test_cfg):\n    """"""Merge augmented detection bboxes and scores.\n\n    Args:\n        aug_bboxes (list[Tensor]): shape (n, 4*#class)\n        aug_scores (list[Tensor] or None): shape (n, #class)\n        img_shapes (list[Tensor]): shape (3, ).\n        rcnn_test_cfg (dict): rcnn test config.\n\n    Returns:\n        tuple: (bboxes, scores)\n    """"""\n    recovered_bboxes = []\n    for bboxes, img_info in zip(aug_bboxes, img_metas):\n        img_shape = img_info[0][\'img_shape\']\n        scale_factor = img_info[0][\'scale_factor\']\n        flip = img_info[0][\'flip\']\n        flip_direction = img_info[0][\'flip_direction\']\n        bboxes = bbox_mapping_back(bboxes, img_shape, scale_factor, flip,\n                                   flip_direction)\n        recovered_bboxes.append(bboxes)\n    bboxes = torch.stack(recovered_bboxes).mean(dim=0)\n    if aug_scores is None:\n        return bboxes\n    else:\n        scores = torch.stack(aug_scores).mean(dim=0)\n        return bboxes, scores\n\n\ndef merge_aug_scores(aug_scores):\n    """"""Merge augmented bbox scores.""""""\n    if isinstance(aug_scores[0], torch.Tensor):\n        return torch.mean(torch.stack(aug_scores), dim=0)\n    else:\n        return np.mean(aug_scores, axis=0)\n\n\ndef merge_aug_masks(aug_masks, img_metas, rcnn_test_cfg, weights=None):\n    """"""Merge augmented mask prediction.\n\n    Args:\n        aug_masks (list[ndarray]): shape (n, #class, h, w)\n        img_shapes (list[ndarray]): shape (3, ).\n        rcnn_test_cfg (dict): rcnn test config.\n\n    Returns:\n        tuple: (bboxes, scores)\n    """"""\n    recovered_masks = []\n    for mask, img_info in zip(aug_masks, img_metas):\n        flip = img_info[0][\'flip\']\n        flip_direction = img_info[0][\'flip_direction\']\n        if flip:\n            if flip_direction == \'horizontal\':\n                mask = mask[:, :, :, ::-1]\n            elif flip_direction == \'vertical\':\n                mask = mask[:, :, ::-1, :]\n            else:\n                raise ValueError(\n                    f""Invalid flipping direction \'{flip_direction}\'"")\n        recovered_masks.append(mask)\n\n    if weights is None:\n        merged_masks = np.mean(recovered_masks, axis=0)\n    else:\n        merged_masks = np.average(\n            np.array(recovered_masks), axis=0, weights=np.array(weights))\n    return merged_masks\n'"
mmdet/core/utils/__init__.py,0,"b""from .dist_utils import DistOptimizerHook, allreduce_grads\nfrom .misc import multi_apply, tensor2imgs, unmap\n\n__all__ = [\n    'allreduce_grads', 'DistOptimizerHook', 'tensor2imgs', 'multi_apply',\n    'unmap'\n]\n"""
mmdet/core/utils/dist_utils.py,2,"b'import warnings\nfrom collections import OrderedDict\n\nimport torch.distributed as dist\nfrom mmcv.runner import OptimizerHook\nfrom torch._utils import (_flatten_dense_tensors, _take_tensors,\n                          _unflatten_dense_tensors)\n\n\ndef _allreduce_coalesced(tensors, world_size, bucket_size_mb=-1):\n    if bucket_size_mb > 0:\n        bucket_size_bytes = bucket_size_mb * 1024 * 1024\n        buckets = _take_tensors(tensors, bucket_size_bytes)\n    else:\n        buckets = OrderedDict()\n        for tensor in tensors:\n            tp = tensor.type()\n            if tp not in buckets:\n                buckets[tp] = []\n            buckets[tp].append(tensor)\n        buckets = buckets.values()\n\n    for bucket in buckets:\n        flat_tensors = _flatten_dense_tensors(bucket)\n        dist.all_reduce(flat_tensors)\n        flat_tensors.div_(world_size)\n        for tensor, synced in zip(\n                bucket, _unflatten_dense_tensors(flat_tensors, bucket)):\n            tensor.copy_(synced)\n\n\ndef allreduce_grads(params, coalesce=True, bucket_size_mb=-1):\n    grads = [\n        param.grad.data for param in params\n        if param.requires_grad and param.grad is not None\n    ]\n    world_size = dist.get_world_size()\n    if coalesce:\n        _allreduce_coalesced(grads, world_size, bucket_size_mb)\n    else:\n        for tensor in grads:\n            dist.all_reduce(tensor.div_(world_size))\n\n\nclass DistOptimizerHook(OptimizerHook):\n    """"""Deprecated optimizer hook for distributed training""""""\n\n    def __init__(self, *args, **kwargs):\n        warnings.warn(\'""DistOptimizerHook"" is deprecated, please switch to\'\n                      \'""mmcv.runner.OptimizerHook"".\')\n        super().__init__(*args, **kwargs)\n'"
mmdet/core/utils/misc.py,2,"b'from functools import partial\n\nimport mmcv\nimport numpy as np\nimport torch\nfrom six.moves import map, zip\n\n\ndef tensor2imgs(tensor, mean=(0, 0, 0), std=(1, 1, 1), to_rgb=True):\n    num_imgs = tensor.size(0)\n    mean = np.array(mean, dtype=np.float32)\n    std = np.array(std, dtype=np.float32)\n    imgs = []\n    for img_id in range(num_imgs):\n        img = tensor[img_id, ...].cpu().numpy().transpose(1, 2, 0)\n        img = mmcv.imdenormalize(\n            img, mean, std, to_bgr=to_rgb).astype(np.uint8)\n        imgs.append(np.ascontiguousarray(img))\n    return imgs\n\n\ndef multi_apply(func, *args, **kwargs):\n    pfunc = partial(func, **kwargs) if kwargs else func\n    map_results = map(pfunc, *args)\n    return tuple(map(list, zip(*map_results)))\n\n\ndef unmap(data, count, inds, fill=0):\n    """""" Unmap a subset of item (data) back to the original set of items (of\n    size count) """"""\n    if data.dim() == 1:\n        ret = data.new_full((count, ), fill)\n        ret[inds.type(torch.bool)] = data\n    else:\n        new_size = (count, ) + data.size()[1:]\n        ret = data.new_full(new_size, fill)\n        ret[inds.type(torch.bool), :] = data\n    return ret\n'"
mmdet/datasets/pipelines/__init__.py,0,"b""from .compose import Compose\nfrom .formating import (Collect, ImageToTensor, ToDataContainer, ToTensor,\n                        Transpose, to_tensor)\nfrom .instaboost import InstaBoost\nfrom .loading import (LoadAnnotations, LoadImageFromFile,\n                      LoadMultiChannelImageFromFiles, LoadProposals)\nfrom .test_time_aug import MultiScaleFlipAug\nfrom .transforms import (Albu, Expand, MinIoURandomCrop, Normalize, Pad,\n                         PhotoMetricDistortion, RandomCrop, RandomFlip, Resize,\n                         SegRescale)\n\n__all__ = [\n    'Compose', 'to_tensor', 'ToTensor', 'ImageToTensor', 'ToDataContainer',\n    'Transpose', 'Collect', 'LoadAnnotations', 'LoadImageFromFile',\n    'LoadMultiChannelImageFromFiles', 'LoadProposals', 'MultiScaleFlipAug',\n    'Resize', 'RandomFlip', 'Pad', 'RandomCrop', 'Normalize', 'SegRescale',\n    'MinIoURandomCrop', 'Expand', 'PhotoMetricDistortion', 'Albu', 'InstaBoost'\n]\n"""
mmdet/datasets/pipelines/compose.py,0,"b""import collections\n\nfrom mmcv.utils import build_from_cfg\n\nfrom ..builder import PIPELINES\n\n\n@PIPELINES.register_module()\nclass Compose(object):\n\n    def __init__(self, transforms):\n        assert isinstance(transforms, collections.abc.Sequence)\n        self.transforms = []\n        for transform in transforms:\n            if isinstance(transform, dict):\n                transform = build_from_cfg(transform, PIPELINES)\n                self.transforms.append(transform)\n            elif callable(transform):\n                self.transforms.append(transform)\n            else:\n                raise TypeError('transform must be callable or a dict')\n\n    def __call__(self, data):\n        for t in self.transforms:\n            data = t(data)\n            if data is None:\n                return None\n        return data\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + '('\n        for t in self.transforms:\n            format_string += '\\n'\n            format_string += f'    {t}'\n        format_string += '\\n)'\n        return format_string\n"""
mmdet/datasets/pipelines/formating.py,7,"b'from collections.abc import Sequence\n\nimport mmcv\nimport numpy as np\nimport torch\nfrom mmcv.parallel import DataContainer as DC\n\nfrom ..builder import PIPELINES\n\n\ndef to_tensor(data):\n    """"""Convert objects of various python types to :obj:`torch.Tensor`.\n\n    Supported types are: :class:`numpy.ndarray`, :class:`torch.Tensor`,\n    :class:`Sequence`, :class:`int` and :class:`float`.\n    """"""\n    if isinstance(data, torch.Tensor):\n        return data\n    elif isinstance(data, np.ndarray):\n        return torch.from_numpy(data)\n    elif isinstance(data, Sequence) and not mmcv.is_str(data):\n        return torch.tensor(data)\n    elif isinstance(data, int):\n        return torch.LongTensor([data])\n    elif isinstance(data, float):\n        return torch.FloatTensor([data])\n    else:\n        raise TypeError(f\'type {type(data)} cannot be converted to tensor.\')\n\n\n@PIPELINES.register_module()\nclass ToTensor(object):\n\n    def __init__(self, keys):\n        self.keys = keys\n\n    def __call__(self, results):\n        for key in self.keys:\n            results[key] = to_tensor(results[key])\n        return results\n\n    def __repr__(self):\n        return self.__class__.__name__ + f\'(keys={self.keys})\'\n\n\n@PIPELINES.register_module()\nclass ImageToTensor(object):\n\n    def __init__(self, keys):\n        self.keys = keys\n\n    def __call__(self, results):\n        for key in self.keys:\n            img = results[key]\n            if len(img.shape) < 3:\n                img = np.expand_dims(img, -1)\n            results[key] = to_tensor(img.transpose(2, 0, 1))\n        return results\n\n    def __repr__(self):\n        return self.__class__.__name__ + f\'(keys={self.keys})\'\n\n\n@PIPELINES.register_module()\nclass Transpose(object):\n\n    def __init__(self, keys, order):\n        self.keys = keys\n        self.order = order\n\n    def __call__(self, results):\n        for key in self.keys:\n            results[key] = results[key].transpose(self.order)\n        return results\n\n    def __repr__(self):\n        return self.__class__.__name__ + \\\n            f\'(keys={self.keys}, order={self.order})\'\n\n\n@PIPELINES.register_module()\nclass ToDataContainer(object):\n\n    def __init__(self,\n                 fields=(dict(key=\'img\', stack=True), dict(key=\'gt_bboxes\'),\n                         dict(key=\'gt_labels\'))):\n        self.fields = fields\n\n    def __call__(self, results):\n        for field in self.fields:\n            field = field.copy()\n            key = field.pop(\'key\')\n            results[key] = DC(results[key], **field)\n        return results\n\n    def __repr__(self):\n        return self.__class__.__name__ + f\'(fields={self.fields})\'\n\n\n@PIPELINES.register_module()\nclass DefaultFormatBundle(object):\n    """"""Default formatting bundle.\n\n    It simplifies the pipeline of formatting common fields, including ""img"",\n    ""proposals"", ""gt_bboxes"", ""gt_labels"", ""gt_masks"" and ""gt_semantic_seg"".\n    These fields are formatted as follows.\n\n    - img: (1)transpose, (2)to tensor, (3)to DataContainer (stack=True)\n    - proposals: (1)to tensor, (2)to DataContainer\n    - gt_bboxes: (1)to tensor, (2)to DataContainer\n    - gt_bboxes_ignore: (1)to tensor, (2)to DataContainer\n    - gt_labels: (1)to tensor, (2)to DataContainer\n    - gt_masks: (1)to tensor, (2)to DataContainer (cpu_only=True)\n    - gt_semantic_seg: (1)unsqueeze dim-0 (2)to tensor,\n                       (3)to DataContainer (stack=True)\n    """"""\n\n    def __call__(self, results):\n        if \'img\' in results:\n            img = results[\'img\']\n            if len(img.shape) < 3:\n                img = np.expand_dims(img, -1)\n            img = np.ascontiguousarray(img.transpose(2, 0, 1))\n            results[\'img\'] = DC(to_tensor(img), stack=True)\n        for key in [\'proposals\', \'gt_bboxes\', \'gt_bboxes_ignore\', \'gt_labels\']:\n            if key not in results:\n                continue\n            results[key] = DC(to_tensor(results[key]))\n        if \'gt_masks\' in results:\n            results[\'gt_masks\'] = DC(results[\'gt_masks\'], cpu_only=True)\n        if \'gt_semantic_seg\' in results:\n            results[\'gt_semantic_seg\'] = DC(\n                to_tensor(results[\'gt_semantic_seg\'][None, ...]), stack=True)\n        return results\n\n    def __repr__(self):\n        return self.__class__.__name__\n\n\n@PIPELINES.register_module()\nclass Collect(object):\n    """"""\n    Collect data from the loader relevant to the specific task.\n\n    This is usually the last stage of the data loader pipeline. Typically keys\n    is set to some subset of ""img"", ""proposals"", ""gt_bboxes"",\n    ""gt_bboxes_ignore"", ""gt_labels"", and/or ""gt_masks"".\n\n    The ""img_meta"" item is always populated.  The contents of the ""img_meta""\n    dictionary depends on ""meta_keys"". By default this includes:\n\n        - ""img_shape"": shape of the image input to the network as a tuple\n            (h, w, c).  Note that images may be zero padded on the bottom/right\n            if the batch tensor is larger than this shape.\n\n        - ""scale_factor"": a float indicating the preprocessing scale\n\n        - ""flip"": a boolean indicating if image flip transform was used\n\n        - ""filename"": path to the image file\n\n        - ""ori_shape"": original shape of the image as a tuple (h, w, c)\n\n        - ""pad_shape"": image shape after padding\n\n        - ""img_norm_cfg"": a dict of normalization information:\n            - mean - per channel mean subtraction\n            - std - per channel std divisor\n            - to_rgb - bool indicating if bgr was converted to rgb\n    """"""\n\n    def __init__(self,\n                 keys,\n                 meta_keys=(\'filename\', \'ori_filename\', \'ori_shape\',\n                            \'img_shape\', \'pad_shape\', \'scale_factor\', \'flip\',\n                            \'flip_direction\', \'img_norm_cfg\')):\n        self.keys = keys\n        self.meta_keys = meta_keys\n\n    def __call__(self, results):\n        data = {}\n        img_meta = {}\n        for key in self.meta_keys:\n            img_meta[key] = results[key]\n        data[\'img_metas\'] = DC(img_meta, cpu_only=True)\n        for key in self.keys:\n            data[key] = results[key]\n        return data\n\n    def __repr__(self):\n        return self.__class__.__name__ + \\\n            f\'(keys={self.keys}, meta_keys={self.meta_keys})\'\n\n\n@PIPELINES.register_module()\nclass WrapFieldsToLists(object):\n    """"""\n    Wrap fields of the data dictionary into lists for evaluation.\n\n    This class can be used as a last step of a test or validation\n    pipeline for single image evaluation or inference.\n\n    Example:\n        >>> test_pipeline = [\n        >>>    dict(type=\'LoadImageFromFile\'),\n        >>>    dict(type=\'Normalize\',\n                    mean=[123.675, 116.28, 103.53],\n                    std=[58.395, 57.12, 57.375],\n                    to_rgb=True),\n        >>>    dict(type=\'Pad\', size_divisor=32),\n        >>>    dict(type=\'ImageToTensor\', keys=[\'img\']),\n        >>>    dict(type=\'Collect\', keys=[\'img\']),\n        >>>    dict(type=\'WrapIntoLists\')\n        >>> ]\n    """"""\n\n    def __call__(self, results):\n        # Wrap dict fields into lists\n        for key, val in results.items():\n            results[key] = [val]\n        return results\n\n    def __repr__(self):\n        return f\'{self.__class__.__name__}()\'\n'"
mmdet/datasets/pipelines/instaboost.py,0,"b'import numpy as np\n\nfrom ..builder import PIPELINES\n\n\n@PIPELINES.register_module()\nclass InstaBoost(object):\n    """"""\n    Data augmentation method in paper ""InstaBoost: Boosting Instance\n    Segmentation Via Probability Map Guided Copy-Pasting""\n    Implementation details can refer to https://github.com/GothicAi/Instaboost.\n    """"""\n\n    def __init__(self,\n                 action_candidate=(\'normal\', \'horizontal\', \'skip\'),\n                 action_prob=(1, 0, 0),\n                 scale=(0.8, 1.2),\n                 dx=15,\n                 dy=15,\n                 theta=(-1, 1),\n                 color_prob=0.5,\n                 hflag=False,\n                 aug_ratio=0.5):\n        try:\n            import instaboostfast as instaboost\n        except ImportError:\n            raise ImportError(\n                \'Please run ""pip install instaboostfast"" \'\n                \'to install instaboostfast first for instaboost augmentation.\')\n        self.cfg = instaboost.InstaBoostConfig(action_candidate, action_prob,\n                                               scale, dx, dy, theta,\n                                               color_prob, hflag)\n        self.aug_ratio = aug_ratio\n\n    def _load_anns(self, results):\n        labels = results[\'ann_info\'][\'labels\']\n        masks = results[\'ann_info\'][\'masks\']\n        bboxes = results[\'ann_info\'][\'bboxes\']\n        n = len(labels)\n\n        anns = []\n        for i in range(n):\n            label = labels[i]\n            bbox = bboxes[i]\n            mask = masks[i]\n            x1, y1, x2, y2 = bbox\n            # assert (x2 - x1) >= 1 and (y2 - y1) >= 1\n            bbox = [x1, y1, x2 - x1, y2 - y1]\n            anns.append({\n                \'category_id\': label,\n                \'segmentation\': mask,\n                \'bbox\': bbox\n            })\n\n        return anns\n\n    def _parse_anns(self, results, anns, img):\n        gt_bboxes = []\n        gt_labels = []\n        gt_masks_ann = []\n        for ann in anns:\n            x1, y1, w, h = ann[\'bbox\']\n            # TODO: more essential bug need to be fixed in instaboost\n            if w <= 0 or h <= 0:\n                continue\n            bbox = [x1, y1, x1 + w, y1 + h]\n            gt_bboxes.append(bbox)\n            gt_labels.append(ann[\'category_id\'])\n            gt_masks_ann.append(ann[\'segmentation\'])\n        gt_bboxes = np.array(gt_bboxes, dtype=np.float32)\n        gt_labels = np.array(gt_labels, dtype=np.int64)\n        results[\'ann_info\'][\'labels\'] = gt_labels\n        results[\'ann_info\'][\'bboxes\'] = gt_bboxes\n        results[\'ann_info\'][\'masks\'] = gt_masks_ann\n        results[\'img\'] = img\n        return results\n\n    def __call__(self, results):\n        img = results[\'img\']\n        orig_type = img.dtype\n        anns = self._load_anns(results)\n        if np.random.choice([0, 1], p=[1 - self.aug_ratio, self.aug_ratio]):\n            try:\n                import instaboostfast as instaboost\n            except ImportError:\n                raise ImportError(\'Please run ""pip install instaboostfast"" \'\n                                  \'to install instaboostfast first.\')\n            anns, img = instaboost.get_new_data(\n                anns, img.astype(np.uint8), self.cfg, background=None)\n\n        results = self._parse_anns(results, anns, img.astype(orig_type))\n        return results\n\n    def __repr__(self):\n        repr_str = self.__class__.__name__\n        repr_str += f\'(cfg={self.cfg}, aug_ratio={self.aug_ratio})\'\n        return repr_str\n'"
mmdet/datasets/pipelines/loading.py,0,"b'import os.path as osp\n\nimport mmcv\nimport numpy as np\nimport pycocotools.mask as maskUtils\n\nfrom mmdet.core import BitmapMasks, PolygonMasks\nfrom ..builder import PIPELINES\n\n\n@PIPELINES.register_module()\nclass LoadImageFromFile(object):\n    """"""Load an image from file.\n\n    Required keys are ""img_prefix"" and ""img_info"" (a dict that must contain the\n    key ""filename""). Added or updated keys are ""filename"", ""img"", ""img_shape"",\n    ""ori_shape"" (same as `img_shape`), ""pad_shape"" (same as `img_shape`),\n    ""scale_factor"" (1.0) and ""img_norm_cfg"" (means=0 and stds=1).\n\n    Args:\n        to_float32 (bool): Whether to convert the loaded image to a float32\n            numpy array. If set to False, the loaded image is an uint8 array.\n            Defaults to False.\n        color_type (str): The flag argument for :func:`mmcv.imfrombytes()`.\n            Defaults to \'color\'.\n        file_client_args (dict): Arguments to instantiate a FileClient.\n            See :class:`mmcv.fileio.FileClient` for details.\n            Defaults to ``dict(backend=\'disk\')``.\n    """"""\n\n    def __init__(self,\n                 to_float32=False,\n                 color_type=\'color\',\n                 file_client_args=dict(backend=\'disk\')):\n        self.to_float32 = to_float32\n        self.color_type = color_type\n        self.file_client_args = file_client_args.copy()\n        self.file_client = None\n\n    def __call__(self, results):\n        if self.file_client is None:\n            self.file_client = mmcv.FileClient(**self.file_client_args)\n\n        if results[\'img_prefix\'] is not None:\n            filename = osp.join(results[\'img_prefix\'],\n                                results[\'img_info\'][\'filename\'])\n        else:\n            filename = results[\'img_info\'][\'filename\']\n\n        img_bytes = self.file_client.get(filename)\n        img = mmcv.imfrombytes(img_bytes, flag=self.color_type)\n        if self.to_float32:\n            img = img.astype(np.float32)\n\n        results[\'filename\'] = filename\n        results[\'ori_filename\'] = results[\'img_info\'][\'filename\']\n        results[\'img\'] = img\n        results[\'img_shape\'] = img.shape\n        results[\'ori_shape\'] = img.shape\n        # Set initial values for default meta_keys\n        results[\'pad_shape\'] = img.shape\n        results[\'scale_factor\'] = 1.0\n        num_channels = 1 if len(img.shape) < 3 else img.shape[2]\n        results[\'img_norm_cfg\'] = dict(\n            mean=np.zeros(num_channels, dtype=np.float32),\n            std=np.ones(num_channels, dtype=np.float32),\n            to_rgb=False)\n        results[\'img_fields\'] = [\'img\']\n        return results\n\n    def __repr__(self):\n        repr_str = (f\'{self.__class__.__name__}(\'\n                    f\'to_float32={self.to_float32}, \'\n                    f""color_type=\'{self.color_type}\', ""\n                    f\'file_client_args={self.file_client_args})\')\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass LoadMultiChannelImageFromFiles(object):\n    """"""Load multi-channel images from a list of separate channel files.\n\n    Required keys are ""img_prefix"" and ""img_info"" (a dict that must contain the\n    key ""filename"", which is expected to be a list of filenames).\n    Added or updated keys are ""filename"", ""img"", ""img_shape"",\n    ""ori_shape"" (same as `img_shape`), ""pad_shape"" (same as `img_shape`),\n    ""scale_factor"" (1.0) and ""img_norm_cfg"" (means=0 and stds=1).\n\n    Args:\n        to_float32 (bool): Whether to convert the loaded image to a float32\n            numpy array. If set to False, the loaded image is an uint8 array.\n            Defaults to False.\n        color_type (str): The flag argument for :func:`mmcv.imfrombytes()`.\n            Defaults to \'color\'.\n        file_client_args (dict): Arguments to instantiate a FileClient.\n            See :class:`mmcv.fileio.FileClient` for details.\n            Defaults to ``dict(backend=\'disk\')``.\n    """"""\n\n    def __init__(self,\n                 to_float32=False,\n                 color_type=\'unchanged\',\n                 file_client_args=dict(backend=\'disk\')):\n        self.to_float32 = to_float32\n        self.color_type = color_type\n        self.file_client_args = file_client_args.copy()\n        self.file_client = None\n\n    def __call__(self, results):\n        if self.file_client is None:\n            self.file_client = mmcv.FileClient(**self.file_client_args)\n\n        if results[\'img_prefix\'] is not None:\n            filename = [\n                osp.join(results[\'img_prefix\'], fname)\n                for fname in results[\'img_info\'][\'filename\']\n            ]\n        else:\n            filename = results[\'img_info\'][\'filename\']\n\n        img = []\n        for name in filename:\n            img_bytes = self.file_client.get(name)\n            img.append(mmcv.imfrombytes(img_bytes, flag=self.color_type))\n        img = np.stack(img, axis=-1)\n        if self.to_float32:\n            img = img.astype(np.float32)\n\n        results[\'filename\'] = filename\n        results[\'ori_filename\'] = results[\'img_info\'][\'filename\']\n        results[\'img\'] = img\n        results[\'img_shape\'] = img.shape\n        results[\'ori_shape\'] = img.shape\n        # Set initial values for default meta_keys\n        results[\'pad_shape\'] = img.shape\n        results[\'scale_factor\'] = 1.0\n        num_channels = 1 if len(img.shape) < 3 else img.shape[2]\n        results[\'img_norm_cfg\'] = dict(\n            mean=np.zeros(num_channels, dtype=np.float32),\n            std=np.ones(num_channels, dtype=np.float32),\n            to_rgb=False)\n        return results\n\n    def __repr__(self):\n        repr_str = (f\'{self.__class__.__name__}(\'\n                    f\'to_float32={self.to_float32}, \'\n                    f""color_type=\'{self.color_type}\', ""\n                    f\'file_client_args={self.file_client_args})\')\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass LoadAnnotations(object):\n    """"""Load annotations.\n\n    Args:\n        with_bbox (bool): Whether to parse and load the bbox annotation.\n             Default: True.\n        with_label (bool): Whether to parse and load the label annotation.\n            Default: True.\n        with_mask (bool): Whether to parse and load the mask annotation.\n             Default: False.\n        with_seg (bool): Whether to parse and load the semantic segmentation\n            annotation. Default: False.\n        poly2mask (bool): Whether to convert the instance masks from polygons\n            to bitmaps. Default: True.\n        file_client_args (dict): Arguments to instantiate a FileClient.\n            See :class:`mmcv.fileio.FileClient` for details.\n            Defaults to ``dict(backend=\'disk\')``.\n    """"""\n\n    def __init__(self,\n                 with_bbox=True,\n                 with_label=True,\n                 with_mask=False,\n                 with_seg=False,\n                 poly2mask=True,\n                 file_client_args=dict(backend=\'disk\')):\n        self.with_bbox = with_bbox\n        self.with_label = with_label\n        self.with_mask = with_mask\n        self.with_seg = with_seg\n        self.poly2mask = poly2mask\n        self.file_client_args = file_client_args.copy()\n        self.file_client = None\n\n    def _load_bboxes(self, results):\n        ann_info = results[\'ann_info\']\n        results[\'gt_bboxes\'] = ann_info[\'bboxes\']\n\n        gt_bboxes_ignore = ann_info.get(\'bboxes_ignore\', None)\n        if gt_bboxes_ignore is not None:\n            results[\'gt_bboxes_ignore\'] = gt_bboxes_ignore\n            results[\'bbox_fields\'].append(\'gt_bboxes_ignore\')\n        results[\'bbox_fields\'].append(\'gt_bboxes\')\n        return results\n\n    def _load_labels(self, results):\n        results[\'gt_labels\'] = results[\'ann_info\'][\'labels\']\n        return results\n\n    def _poly2mask(self, mask_ann, img_h, img_w):\n        if isinstance(mask_ann, list):\n            # polygon -- a single object might consist of multiple parts\n            # we merge all parts into one mask rle code\n            rles = maskUtils.frPyObjects(mask_ann, img_h, img_w)\n            rle = maskUtils.merge(rles)\n        elif isinstance(mask_ann[\'counts\'], list):\n            # uncompressed RLE\n            rle = maskUtils.frPyObjects(mask_ann, img_h, img_w)\n        else:\n            # rle\n            rle = mask_ann\n        mask = maskUtils.decode(rle)\n        return mask\n\n    def process_polygons(self, polygons):\n        """"""Convert polygons to list of ndarray and filter invalid polygons.\n\n        Args:\n            polygons (list[list]): polygons of one instance.\n\n        Returns:\n            list[ndarray]: processed polygons.\n        """"""\n        polygons = [np.array(p) for p in polygons]\n        valid_polygons = []\n        for polygon in polygons:\n            if len(polygon) % 2 == 0 and len(polygon) >= 6:\n                valid_polygons.append(polygon)\n        return valid_polygons\n\n    def _load_masks(self, results):\n        h, w = results[\'img_info\'][\'height\'], results[\'img_info\'][\'width\']\n        gt_masks = results[\'ann_info\'][\'masks\']\n        if self.poly2mask:\n            gt_masks = BitmapMasks(\n                [self._poly2mask(mask, h, w) for mask in gt_masks], h, w)\n        else:\n            gt_masks = PolygonMasks(\n                [self.process_polygons(polygons) for polygons in gt_masks], h,\n                w)\n        results[\'gt_masks\'] = gt_masks\n        results[\'mask_fields\'].append(\'gt_masks\')\n        return results\n\n    def _load_semantic_seg(self, results):\n        if self.file_client is None:\n            self.file_client = mmcv.FileClient(**self.file_client_args)\n\n        filename = osp.join(results[\'seg_prefix\'],\n                            results[\'ann_info\'][\'seg_map\'])\n        img_bytes = self.file_client.get(filename)\n        results[\'gt_semantic_seg\'] = mmcv.imfrombytes(\n            img_bytes, flag=\'unchanged\').squeeze()\n        results[\'seg_fields\'].append(\'gt_semantic_seg\')\n        return results\n\n    def __call__(self, results):\n        if self.with_bbox:\n            results = self._load_bboxes(results)\n            if results is None:\n                return None\n        if self.with_label:\n            results = self._load_labels(results)\n        if self.with_mask:\n            results = self._load_masks(results)\n        if self.with_seg:\n            results = self._load_semantic_seg(results)\n        return results\n\n    def __repr__(self):\n        repr_str = self.__class__.__name__\n        repr_str += f\'(with_bbox={self.with_bbox}, \'\n        repr_str += f\'with_label={self.with_label}, \'\n        repr_str += f\'with_mask={self.with_mask}, \'\n        repr_str += f\'with_seg={self.with_seg})\'\n        repr_str += f\'poly2mask={self.poly2mask})\'\n        repr_str += f\'poly2mask={self.file_client_args})\'\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass LoadProposals(object):\n\n    def __init__(self, num_max_proposals=None):\n        self.num_max_proposals = num_max_proposals\n\n    def __call__(self, results):\n        proposals = results[\'proposals\']\n        if proposals.shape[1] not in (4, 5):\n            raise AssertionError(\n                \'proposals should have shapes (n, 4) or (n, 5), \'\n                f\'but found {proposals.shape}\')\n        proposals = proposals[:, :4]\n\n        if self.num_max_proposals is not None:\n            proposals = proposals[:self.num_max_proposals]\n\n        if len(proposals) == 0:\n            proposals = np.array([[0, 0, 0, 0]], dtype=np.float32)\n        results[\'proposals\'] = proposals\n        results[\'bbox_fields\'].append(\'proposals\')\n        return results\n\n    def __repr__(self):\n        return self.__class__.__name__ + \\\n            f\'(num_max_proposals={self.num_max_proposals})\'\n'"
mmdet/datasets/pipelines/test_time_aug.py,0,"b'import warnings\n\nimport mmcv\n\nfrom ..builder import PIPELINES\nfrom .compose import Compose\nfrom .transforms import RandomFlip\n\n\n@PIPELINES.register_module()\nclass MultiScaleFlipAug(object):\n    """"""Test-time augmentation with multiple scales and flipping\n\n    Args:\n        transforms (list[dict]): Transforms to apply in each augmentation.\n        img_scale (tuple | list[tuple]: Images scales for resizing.\n        flip (bool): Whether apply flip augmentation. Default: False.\n        flip_direction (str | list[str]): Flip augmentation directions,\n            options are ""horizontal"" and ""vertical"". If flip_direction is list,\n            multiple flip augmentations will be applied.\n            It has no effect when flip == False. Default: ""horizontal"".\n    """"""\n\n    def __init__(self,\n                 transforms,\n                 img_scale,\n                 flip=False,\n                 flip_direction=\'horizontal\'):\n        self.transforms = Compose(transforms)\n        self.img_scale = img_scale if isinstance(img_scale,\n                                                 list) else [img_scale]\n        assert mmcv.is_list_of(self.img_scale, tuple)\n        self.flip = flip\n        self.flip_direction = flip_direction if isinstance(\n            flip_direction, list) else [flip_direction]\n        assert mmcv.is_list_of(self.flip_direction, str)\n        if not self.flip and self.flip_direction != [\'horizontal\']:\n            warnings.warn(\n                \'flip_direction has no effect when flip is set to False\')\n        if (self.flip and\n                not any([isinstance(_, RandomFlip) for _ in self.transforms])):\n            warnings.warn(\n                \'flip has no effect when RandFlip is not in transforms\')\n\n    def __call__(self, results):\n        aug_data = []\n        flip_aug = [False, True] if self.flip else [False]\n        for scale in self.img_scale:\n            for flip in flip_aug:\n                for direction in self.flip_direction:\n                    _results = results.copy()\n                    _results[\'scale\'] = scale\n                    _results[\'flip\'] = flip\n                    _results[\'flip_direction\'] = direction\n                    data = self.transforms(_results)\n                    aug_data.append(data)\n        # list of dict to dict of list\n        aug_data_dict = {key: [] for key in aug_data[0]}\n        for data in aug_data:\n            for key, val in data.items():\n                aug_data_dict[key].append(val)\n        return aug_data_dict\n\n    def __repr__(self):\n        repr_str = self.__class__.__name__\n        repr_str += f\'(transforms={self.transforms}, \'\n        repr_str += f\'img_scale={self.img_scale}, flip={self.flip})\'\n        repr_str += f\'flip_direction={self.flip_direction}\'\n        return repr_str\n'"
mmdet/datasets/pipelines/transforms.py,0,"b'import inspect\n\nimport mmcv\nimport numpy as np\nfrom numpy import random\n\nfrom mmdet.core import PolygonMasks\nfrom mmdet.core.evaluation.bbox_overlaps import bbox_overlaps\nfrom ..builder import PIPELINES\n\ntry:\n    from imagecorruptions import corrupt\nexcept ImportError:\n    corrupt = None\n\ntry:\n    import albumentations\n    from albumentations import Compose\nexcept ImportError:\n    albumentations = None\n    Compose = None\n\n\n@PIPELINES.register_module()\nclass Resize(object):\n    """"""Resize images & bbox & mask.\n\n    This transform resizes the input image to some scale. Bboxes and masks are\n    then resized with the same scale factor. If the input dict contains the key\n    ""scale"", then the scale in the input dict is used, otherwise the specified\n    scale in the init method is used.\n\n    `img_scale` can either be a tuple (single-scale) or a list of tuple\n    (multi-scale). There are 3 multiscale modes:\n\n    - ``ratio_range is not None``: randomly sample a ratio from the ratio range\n      and multiply it with the image scale.\n    - ``ratio_range is None`` and ``multiscale_mode == ""range""``: randomly\n      sample a scale from the multiscale range.\n    - ``ratio_range is None`` and ``multiscale_mode == ""value""``: randomly\n      sample a scale from multiple scales.\n\n    Args:\n        img_scale (tuple or list[tuple]): Images scales for resizing.\n        multiscale_mode (str): Either ""range"" or ""value"".\n        ratio_range (tuple[float]): (min_ratio, max_ratio)\n        keep_ratio (bool): Whether to keep the aspect ratio when resizing the\n            image.\n    """"""\n\n    def __init__(self,\n                 img_scale=None,\n                 multiscale_mode=\'range\',\n                 ratio_range=None,\n                 keep_ratio=True):\n        if img_scale is None:\n            self.img_scale = None\n        else:\n            if isinstance(img_scale, list):\n                self.img_scale = img_scale\n            else:\n                self.img_scale = [img_scale]\n            assert mmcv.is_list_of(self.img_scale, tuple)\n\n        if ratio_range is not None:\n            # mode 1: given a scale and a range of image ratio\n            assert len(self.img_scale) == 1\n        else:\n            # mode 2: given multiple scales or a range of scales\n            assert multiscale_mode in [\'value\', \'range\']\n\n        self.multiscale_mode = multiscale_mode\n        self.ratio_range = ratio_range\n        self.keep_ratio = keep_ratio\n\n    @staticmethod\n    def random_select(img_scales):\n        assert mmcv.is_list_of(img_scales, tuple)\n        scale_idx = np.random.randint(len(img_scales))\n        img_scale = img_scales[scale_idx]\n        return img_scale, scale_idx\n\n    @staticmethod\n    def random_sample(img_scales):\n        assert mmcv.is_list_of(img_scales, tuple) and len(img_scales) == 2\n        img_scale_long = [max(s) for s in img_scales]\n        img_scale_short = [min(s) for s in img_scales]\n        long_edge = np.random.randint(\n            min(img_scale_long),\n            max(img_scale_long) + 1)\n        short_edge = np.random.randint(\n            min(img_scale_short),\n            max(img_scale_short) + 1)\n        img_scale = (long_edge, short_edge)\n        return img_scale, None\n\n    @staticmethod\n    def random_sample_ratio(img_scale, ratio_range):\n        assert isinstance(img_scale, tuple) and len(img_scale) == 2\n        min_ratio, max_ratio = ratio_range\n        assert min_ratio <= max_ratio\n        ratio = np.random.random_sample() * (max_ratio - min_ratio) + min_ratio\n        scale = int(img_scale[0] * ratio), int(img_scale[1] * ratio)\n        return scale, None\n\n    def _random_scale(self, results):\n        if self.ratio_range is not None:\n            scale, scale_idx = self.random_sample_ratio(\n                self.img_scale[0], self.ratio_range)\n        elif len(self.img_scale) == 1:\n            scale, scale_idx = self.img_scale[0], 0\n        elif self.multiscale_mode == \'range\':\n            scale, scale_idx = self.random_sample(self.img_scale)\n        elif self.multiscale_mode == \'value\':\n            scale, scale_idx = self.random_select(self.img_scale)\n        else:\n            raise NotImplementedError\n\n        results[\'scale\'] = scale\n        results[\'scale_idx\'] = scale_idx\n\n    def _resize_img(self, results):\n        for key in results.get(\'img_fields\', [\'img\']):\n            if self.keep_ratio:\n                img, scale_factor = mmcv.imrescale(\n                    results[key], results[\'scale\'], return_scale=True)\n                # the w_scale and h_scale has minor difference\n                # a real fix should be done in the mmcv.imrescale in the future\n                new_h, new_w = img.shape[:2]\n                h, w = results[key].shape[:2]\n                w_scale = new_w / w\n                h_scale = new_h / h\n            else:\n                img, w_scale, h_scale = mmcv.imresize(\n                    results[key], results[\'scale\'], return_scale=True)\n            results[key] = img\n\n            scale_factor = np.array([w_scale, h_scale, w_scale, h_scale],\n                                    dtype=np.float32)\n            results[\'img_shape\'] = img.shape\n            # in case that there is no padding\n            results[\'pad_shape\'] = img.shape\n            results[\'scale_factor\'] = scale_factor\n            results[\'keep_ratio\'] = self.keep_ratio\n\n    def _resize_bboxes(self, results):\n        img_shape = results[\'img_shape\']\n        for key in results.get(\'bbox_fields\', []):\n            bboxes = results[key] * results[\'scale_factor\']\n            bboxes[:, 0::2] = np.clip(bboxes[:, 0::2], 0, img_shape[1])\n            bboxes[:, 1::2] = np.clip(bboxes[:, 1::2], 0, img_shape[0])\n            results[key] = bboxes\n\n    def _resize_masks(self, results):\n        for key in results.get(\'mask_fields\', []):\n            if results[key] is None:\n                continue\n            if self.keep_ratio:\n                results[key] = results[key].rescale(results[\'scale\'])\n            else:\n                results[key] = results[key].resize(results[\'img_shape\'][:2])\n\n    def _resize_seg(self, results):\n        for key in results.get(\'seg_fields\', []):\n            if self.keep_ratio:\n                gt_seg = mmcv.imrescale(\n                    results[key], results[\'scale\'], interpolation=\'nearest\')\n            else:\n                gt_seg = mmcv.imresize(\n                    results[key], results[\'scale\'], interpolation=\'nearest\')\n            results[\'gt_semantic_seg\'] = gt_seg\n\n    def __call__(self, results):\n        if \'scale\' not in results:\n            self._random_scale(results)\n        self._resize_img(results)\n        self._resize_bboxes(results)\n        self._resize_masks(results)\n        self._resize_seg(results)\n        return results\n\n    def __repr__(self):\n        repr_str = self.__class__.__name__\n        repr_str += f\'(img_scale={self.img_scale}, \'\n        repr_str += f\'multiscale_mode={self.multiscale_mode}, \'\n        repr_str += f\'ratio_range={self.ratio_range}, \'\n        repr_str += f\'keep_ratio={self.keep_ratio})\'\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass RandomFlip(object):\n    """"""Flip the image & bbox & mask.\n\n    If the input dict contains the key ""flip"", then the flag will be used,\n    otherwise it will be randomly decided by a ratio specified in the init\n    method.\n\n    Args:\n        flip_ratio (float, optional): The flipping probability.\n    """"""\n\n    def __init__(self, flip_ratio=None, direction=\'horizontal\'):\n        self.flip_ratio = flip_ratio\n        self.direction = direction\n        if flip_ratio is not None:\n            assert flip_ratio >= 0 and flip_ratio <= 1\n        assert direction in [\'horizontal\', \'vertical\']\n\n    def bbox_flip(self, bboxes, img_shape, direction):\n        """"""Flip bboxes horizontally.\n\n        Args:\n            bboxes(ndarray): shape (..., 4*k)\n            img_shape(tuple): (height, width)\n        """"""\n        assert bboxes.shape[-1] % 4 == 0\n        flipped = bboxes.copy()\n        if direction == \'horizontal\':\n            w = img_shape[1]\n            flipped[..., 0::4] = w - bboxes[..., 2::4]\n            flipped[..., 2::4] = w - bboxes[..., 0::4]\n        elif direction == \'vertical\':\n            h = img_shape[0]\n            flipped[..., 1::4] = h - bboxes[..., 3::4]\n            flipped[..., 3::4] = h - bboxes[..., 1::4]\n        else:\n            raise ValueError(f""Invalid flipping direction \'{direction}\'"")\n        return flipped\n\n    def __call__(self, results):\n        if \'flip\' not in results:\n            flip = True if np.random.rand() < self.flip_ratio else False\n            results[\'flip\'] = flip\n        if \'flip_direction\' not in results:\n            results[\'flip_direction\'] = self.direction\n        if results[\'flip\']:\n            # flip image\n            for key in results.get(\'img_fields\', [\'img\']):\n                results[key] = mmcv.imflip(\n                    results[key], direction=results[\'flip_direction\'])\n            # flip bboxes\n            for key in results.get(\'bbox_fields\', []):\n                results[key] = self.bbox_flip(results[key],\n                                              results[\'img_shape\'],\n                                              results[\'flip_direction\'])\n            # flip masks\n            for key in results.get(\'mask_fields\', []):\n                results[key] = results[key].flip(results[\'flip_direction\'])\n\n            # flip segs\n            for key in results.get(\'seg_fields\', []):\n                results[key] = mmcv.imflip(\n                    results[key], direction=results[\'flip_direction\'])\n        return results\n\n    def __repr__(self):\n        return self.__class__.__name__ + f\'(flip_ratio={self.flip_ratio})\'\n\n\n@PIPELINES.register_module()\nclass Pad(object):\n    """"""Pad the image & mask.\n\n    There are two padding modes: (1) pad to a fixed size and (2) pad to the\n    minimum size that is divisible by some number.\n\n    Args:\n        size (tuple, optional): Fixed padding size.\n        size_divisor (int, optional): The divisor of padded size.\n        pad_val (float, optional): Padding value, 0 by default.\n    """"""\n\n    def __init__(self, size=None, size_divisor=None, pad_val=0):\n        self.size = size\n        self.size_divisor = size_divisor\n        self.pad_val = pad_val\n        # only one of size and size_divisor should be valid\n        assert size is not None or size_divisor is not None\n        assert size is None or size_divisor is None\n\n    def _pad_img(self, results):\n        for key in results.get(\'img_fields\', [\'img\']):\n            if self.size is not None:\n                padded_img = mmcv.impad(results[key], self.size, self.pad_val)\n            elif self.size_divisor is not None:\n                padded_img = mmcv.impad_to_multiple(\n                    results[key], self.size_divisor, pad_val=self.pad_val)\n            results[key] = padded_img\n        results[\'pad_shape\'] = padded_img.shape\n        results[\'pad_fixed_size\'] = self.size\n        results[\'pad_size_divisor\'] = self.size_divisor\n\n    def _pad_masks(self, results):\n        pad_shape = results[\'pad_shape\'][:2]\n        for key in results.get(\'mask_fields\', []):\n            results[key] = results[key].pad(pad_shape, pad_val=self.pad_val)\n\n    def _pad_seg(self, results):\n        for key in results.get(\'seg_fields\', []):\n            results[key] = mmcv.impad(results[key], results[\'pad_shape\'][:2])\n\n    def __call__(self, results):\n        self._pad_img(results)\n        self._pad_masks(results)\n        self._pad_seg(results)\n        return results\n\n    def __repr__(self):\n        repr_str = self.__class__.__name__\n        repr_str += f\'(size={self.size}, \'\n        repr_str += f\'size_divisor={self.size_divisor}, \'\n        repr_str += f\'pad_val={self.pad_val})\'\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass Normalize(object):\n    """"""Normalize the image.\n\n    Args:\n        mean (sequence): Mean values of 3 channels.\n        std (sequence): Std values of 3 channels.\n        to_rgb (bool): Whether to convert the image from BGR to RGB,\n            default is true.\n    """"""\n\n    def __init__(self, mean, std, to_rgb=True):\n        self.mean = np.array(mean, dtype=np.float32)\n        self.std = np.array(std, dtype=np.float32)\n        self.to_rgb = to_rgb\n\n    def __call__(self, results):\n        for key in results.get(\'img_fields\', [\'img\']):\n            results[key] = mmcv.imnormalize(results[key], self.mean, self.std,\n                                            self.to_rgb)\n        results[\'img_norm_cfg\'] = dict(\n            mean=self.mean, std=self.std, to_rgb=self.to_rgb)\n        return results\n\n    def __repr__(self):\n        repr_str = self.__class__.__name__\n        repr_str += f\'(mean={self.mean}, std={self.std}, to_rgb={self.to_rgb})\'\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass RandomCrop(object):\n    """"""Random crop the image & bboxes & masks.\n\n    Args:\n        crop_size (tuple): Expected size after cropping, (h, w).\n\n    Notes:\n        - If the image is smaller than the crop size, return the original image\n        - The keys for bboxes, labels and masks must be aligned. That is,\n          `gt_bboxes` corresponds to `gt_labels` and `gt_masks`, and\n          `gt_bboxes_ignore` corresponds to `gt_labels_ignore` and\n          `gt_masks_ignore`.\n        - If there are gt bboxes in an image and the cropping area does not\n          have intersection with any gt bbox, this image is skipped.\n    """"""\n\n    def __init__(self, crop_size):\n        assert crop_size[0] > 0 and crop_size[1] > 0\n        self.crop_size = crop_size\n        # The key correspondence from bboxes to labels and masks.\n        self.bbox2label = {\n            \'gt_bboxes\': \'gt_labels\',\n            \'gt_bboxes_ignore\': \'gt_labels_ignore\'\n        }\n        self.bbox2mask = {\n            \'gt_bboxes\': \'gt_masks\',\n            \'gt_bboxes_ignore\': \'gt_masks_ignore\'\n        }\n\n    def __call__(self, results):\n        for key in results.get(\'img_fields\', [\'img\']):\n            img = results[key]\n            margin_h = max(img.shape[0] - self.crop_size[0], 0)\n            margin_w = max(img.shape[1] - self.crop_size[1], 0)\n            offset_h = np.random.randint(0, margin_h + 1)\n            offset_w = np.random.randint(0, margin_w + 1)\n            crop_y1, crop_y2 = offset_h, offset_h + self.crop_size[0]\n            crop_x1, crop_x2 = offset_w, offset_w + self.crop_size[1]\n\n            # crop the image\n            img = img[crop_y1:crop_y2, crop_x1:crop_x2, ...]\n            img_shape = img.shape\n            results[key] = img\n        results[\'img_shape\'] = img_shape\n\n        valid_flag = False\n        # crop bboxes accordingly and clip to the image boundary\n        for key in results.get(\'bbox_fields\', []):\n            # e.g. gt_bboxes and gt_bboxes_ignore\n            bbox_offset = np.array([offset_w, offset_h, offset_w, offset_h],\n                                   dtype=np.float32)\n            bboxes = results[key] - bbox_offset\n            bboxes[:, 0::2] = np.clip(bboxes[:, 0::2], 0, img_shape[1])\n            bboxes[:, 1::2] = np.clip(bboxes[:, 1::2], 0, img_shape[0])\n            valid_inds = (bboxes[:, 2] > bboxes[:, 0]) & (\n                bboxes[:, 3] > bboxes[:, 1])\n            # When there is no gt bbox, cropping is conducted.\n            # When the crop is valid, cropping is conducted.\n            if len(valid_inds) == 0 or valid_inds.any():\n                valid_flag = True\n            results[key] = bboxes[valid_inds, :]\n            # label fields. e.g. gt_labels and gt_labels_ignore\n            label_key = self.bbox2label.get(key)\n            if label_key in results:\n                results[label_key] = results[label_key][valid_inds]\n\n            # mask fields, e.g. gt_masks and gt_masks_ignore\n            mask_key = self.bbox2mask.get(key)\n            if mask_key in results:\n                results[mask_key] = results[mask_key][\n                    valid_inds.nonzero()[0]].crop(\n                        np.asarray([crop_x1, crop_y1, crop_x2, crop_y2]))\n\n        # if no gt bbox remains after cropping, just skip this image\n        # TODO: check whether we can keep the image regardless of the crop.\n        if \'bbox_fields\' in results and not valid_flag:\n            return None\n\n        # crop semantic seg\n        for key in results.get(\'seg_fields\', []):\n            results[key] = results[key][crop_y1:crop_y2, crop_x1:crop_x2]\n\n        return results\n\n    def __repr__(self):\n        return self.__class__.__name__ + f\'(crop_size={self.crop_size})\'\n\n\n@PIPELINES.register_module()\nclass SegRescale(object):\n    """"""Rescale semantic segmentation maps.\n\n    Args:\n        scale_factor (float): The scale factor of the final output.\n    """"""\n\n    def __init__(self, scale_factor=1):\n        self.scale_factor = scale_factor\n\n    def __call__(self, results):\n        for key in results.get(\'seg_fields\', []):\n            if self.scale_factor != 1:\n                results[key] = mmcv.imrescale(\n                    results[key], self.scale_factor, interpolation=\'nearest\')\n        return results\n\n    def __repr__(self):\n        return self.__class__.__name__ + f\'(scale_factor={self.scale_factor})\'\n\n\n@PIPELINES.register_module()\nclass PhotoMetricDistortion(object):\n    """"""Apply photometric distortion to image sequentially, every transformation\n    is applied with a probability of 0.5. The position of random contrast is in\n    second or second to last.\n\n    1. random brightness\n    2. random contrast (mode 0)\n    3. convert color from BGR to HSV\n    4. random saturation\n    5. random hue\n    6. convert color from HSV to BGR\n    7. random contrast (mode 1)\n    8. randomly swap channels\n\n    Args:\n        brightness_delta (int): delta of brightness.\n        contrast_range (tuple): range of contrast.\n        saturation_range (tuple): range of saturation.\n        hue_delta (int): delta of hue.\n    """"""\n\n    def __init__(self,\n                 brightness_delta=32,\n                 contrast_range=(0.5, 1.5),\n                 saturation_range=(0.5, 1.5),\n                 hue_delta=18):\n        self.brightness_delta = brightness_delta\n        self.contrast_lower, self.contrast_upper = contrast_range\n        self.saturation_lower, self.saturation_upper = saturation_range\n        self.hue_delta = hue_delta\n\n    def __call__(self, results):\n        if \'img_fields\' in results:\n            assert results[\'img_fields\'] == [\'img\'], \\\n                \'Only single img_fields is allowed\'\n        img = results[\'img\']\n        assert img.dtype == np.float32, \\\n            \'PhotoMetricDistortion needs the input image of dtype np.float32,\'\\\n            \' please set ""to_float32=True"" in ""LoadImageFromFile"" pipeline\'\n        # random brightness\n        if random.randint(2):\n            delta = random.uniform(-self.brightness_delta,\n                                   self.brightness_delta)\n            img += delta\n\n        # mode == 0 --> do random contrast first\n        # mode == 1 --> do random contrast last\n        mode = random.randint(2)\n        if mode == 1:\n            if random.randint(2):\n                alpha = random.uniform(self.contrast_lower,\n                                       self.contrast_upper)\n                img *= alpha\n\n        # convert color from BGR to HSV\n        img = mmcv.bgr2hsv(img)\n\n        # random saturation\n        if random.randint(2):\n            img[..., 1] *= random.uniform(self.saturation_lower,\n                                          self.saturation_upper)\n\n        # random hue\n        if random.randint(2):\n            img[..., 0] += random.uniform(-self.hue_delta, self.hue_delta)\n            img[..., 0][img[..., 0] > 360] -= 360\n            img[..., 0][img[..., 0] < 0] += 360\n\n        # convert color from HSV to BGR\n        img = mmcv.hsv2bgr(img)\n\n        # random contrast\n        if mode == 0:\n            if random.randint(2):\n                alpha = random.uniform(self.contrast_lower,\n                                       self.contrast_upper)\n                img *= alpha\n\n        # randomly swap channels\n        if random.randint(2):\n            img = img[..., random.permutation(3)]\n\n        results[\'img\'] = img\n        return results\n\n    def __repr__(self):\n        repr_str = self.__class__.__name__\n        repr_str += f\'(\\nbrightness_delta={self.brightness_delta},\\n\'\n        repr_str += \'contrast_range=\'\n        repr_str += f\'{(self.contrast_lower, self.contrast_upper)},\\n\'\n        repr_str += \'saturation_range=\'\n        repr_str += f\'{(self.saturation_lower, self.saturation_upper)},\\n\'\n        repr_str += f\'hue_delta={self.hue_delta})\'\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass Expand(object):\n    """"""Random expand the image & bboxes.\n\n    Randomly place the original image on a canvas of \'ratio\' x original image\n    size filled with mean values. The ratio is in the range of ratio_range.\n\n    Args:\n        mean (tuple): mean value of dataset.\n        to_rgb (bool): if need to convert the order of mean to align with RGB.\n        ratio_range (tuple): range of expand ratio.\n        prob (float): probability of applying this transformation\n    """"""\n\n    def __init__(self,\n                 mean=(0, 0, 0),\n                 to_rgb=True,\n                 ratio_range=(1, 4),\n                 seg_ignore_label=None,\n                 prob=0.5):\n        self.to_rgb = to_rgb\n        self.ratio_range = ratio_range\n        if to_rgb:\n            self.mean = mean[::-1]\n        else:\n            self.mean = mean\n        self.min_ratio, self.max_ratio = ratio_range\n        self.seg_ignore_label = seg_ignore_label\n        self.prob = prob\n\n    def __call__(self, results):\n        if random.uniform(0, 1) > self.prob:\n            return results\n\n        if \'img_fields\' in results:\n            assert results[\'img_fields\'] == [\'img\'], \\\n                \'Only single img_fields is allowed\'\n        img = results[\'img\']\n\n        h, w, c = img.shape\n        ratio = random.uniform(self.min_ratio, self.max_ratio)\n        expand_img = np.full((int(h * ratio), int(w * ratio), c),\n                             self.mean,\n                             dtype=img.dtype)\n        left = int(random.uniform(0, w * ratio - w))\n        top = int(random.uniform(0, h * ratio - h))\n        expand_img[top:top + h, left:left + w] = img\n\n        results[\'img\'] = expand_img\n        # expand bboxes\n        for key in results.get(\'bbox_fields\', []):\n            results[key] += np.tile((left, top), 2).astype(results[key].dtype)\n\n        # expand masks\n        for key in results.get(\'mask_fields\', []):\n            results[key] = results[key].expand(\n                int(h * ratio), int(w * ratio), top, left)\n\n        # expand segs\n        for key in results.get(\'seg_fields\', []):\n            gt_seg = results[key]\n            expand_gt_seg = np.full((int(h * ratio), int(w * ratio)),\n                                    self.seg_ignore_label,\n                                    dtype=gt_seg.dtype)\n            expand_gt_seg[top:top + h, left:left + w] = gt_seg\n            results[key] = expand_gt_seg\n        return results\n\n    def __repr__(self):\n        repr_str = self.__class__.__name__\n        repr_str += f\'(mean={self.mean}, to_rgb={self.to_rgb}, \'\n        repr_str += f\'ratio_range={self.ratio_range}, \'\n        repr_str += f\'seg_ignore_label={self.seg_ignore_label})\'\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass MinIoURandomCrop(object):\n    """"""Random crop the image & bboxes, the cropped patches have minimum IoU\n    requirement with original image & bboxes, the IoU threshold is randomly\n    selected from min_ious.\n\n    Args:\n        min_ious (tuple): minimum IoU threshold for all intersections with\n        bounding boxes\n        min_crop_size (float): minimum crop\'s size (i.e. h,w := a*h, a*w,\n        where a >= min_crop_size).\n\n    Notes:\n        The keys for bboxes, labels and masks should be paired. That is,\n        `gt_bboxes` corresponds to `gt_labels` and `gt_masks`, and\n        `gt_bboxes_ignore` to `gt_labels_ignore` and `gt_masks_ignore`.\n    """"""\n\n    def __init__(self, min_ious=(0.1, 0.3, 0.5, 0.7, 0.9), min_crop_size=0.3):\n        # 1: return ori img\n        self.min_ious = min_ious\n        self.sample_mode = (1, *min_ious, 0)\n        self.min_crop_size = min_crop_size\n        self.bbox2label = {\n            \'gt_bboxes\': \'gt_labels\',\n            \'gt_bboxes_ignore\': \'gt_labels_ignore\'\n        }\n        self.bbox2mask = {\n            \'gt_bboxes\': \'gt_masks\',\n            \'gt_bboxes_ignore\': \'gt_masks_ignore\'\n        }\n\n    def __call__(self, results):\n        if \'img_fields\' in results:\n            assert results[\'img_fields\'] == [\'img\'], \\\n                \'Only single img_fields is allowed\'\n        img = results[\'img\']\n        assert \'bbox_fields\' in results\n        boxes = [results[key] for key in results[\'bbox_fields\']]\n        boxes = np.concatenate(boxes, 0)\n        h, w, c = img.shape\n        while True:\n            mode = random.choice(self.sample_mode)\n            self.mode = mode\n            if mode == 1:\n                return results\n\n            min_iou = mode\n            for i in range(50):\n                new_w = random.uniform(self.min_crop_size * w, w)\n                new_h = random.uniform(self.min_crop_size * h, h)\n\n                # h / w in [0.5, 2]\n                if new_h / new_w < 0.5 or new_h / new_w > 2:\n                    continue\n\n                left = random.uniform(w - new_w)\n                top = random.uniform(h - new_h)\n\n                patch = np.array(\n                    (int(left), int(top), int(left + new_w), int(top + new_h)))\n                # Line or point crop is not allowed\n                if patch[2] == patch[0] or patch[3] == patch[1]:\n                    continue\n                overlaps = bbox_overlaps(\n                    patch.reshape(-1, 4), boxes.reshape(-1, 4)).reshape(-1)\n                if len(overlaps) > 0 and overlaps.min() < min_iou:\n                    continue\n\n                # center of boxes should inside the crop img\n                # only adjust boxes and instance masks when the gt is not empty\n                if len(overlaps) > 0:\n                    # adjust boxes\n                    def is_center_of_bboxes_in_patch(boxes, patch):\n                        center = (boxes[:, :2] + boxes[:, 2:]) / 2\n                        mask = ((center[:, 0] > patch[0]) *\n                                (center[:, 1] > patch[1]) *\n                                (center[:, 0] < patch[2]) *\n                                (center[:, 1] < patch[3]))\n                        return mask\n\n                    mask = is_center_of_bboxes_in_patch(boxes, patch)\n                    if not mask.any():\n                        continue\n                    for key in results.get(\'bbox_fields\', []):\n                        boxes = results[key]\n                        mask = is_center_of_bboxes_in_patch(boxes, patch)\n                        boxes = boxes[mask]\n                        boxes[:, 2:] = boxes[:, 2:].clip(max=patch[2:])\n                        boxes[:, :2] = boxes[:, :2].clip(min=patch[:2])\n                        boxes -= np.tile(patch[:2], 2)\n\n                        results[key] = boxes\n                        # labels\n                        label_key = self.bbox2label.get(key)\n                        if label_key in results:\n                            results[label_key] = results[label_key][mask]\n\n                        # mask fields\n                        mask_key = self.bbox2mask.get(key)\n                        if mask_key in results:\n                            results[mask_key] = results[mask_key][\n                                mask.nonzero()[0]].crop(patch)\n                # adjust the img no matter whether the gt is empty before crop\n                img = img[patch[1]:patch[3], patch[0]:patch[2]]\n                results[\'img\'] = img\n                results[\'img_shape\'] = img.shape\n\n                # seg fields\n                for key in results.get(\'seg_fields\', []):\n                    results[key] = results[key][patch[1]:patch[3],\n                                                patch[0]:patch[2]]\n                return results\n\n    def __repr__(self):\n        repr_str = self.__class__.__name__\n        repr_str += f\'(min_ious={self.min_ious}, \'\n        repr_str += f\'min_crop_size={self.min_crop_size})\'\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass Corrupt(object):\n\n    def __init__(self, corruption, severity=1):\n        self.corruption = corruption\n        self.severity = severity\n\n    def __call__(self, results):\n        if corrupt is None:\n            raise RuntimeError(\'imagecorruptions is not installed\')\n        if \'img_fields\' in results:\n            assert results[\'img_fields\'] == [\'img\'], \\\n                \'Only single img_fields is allowed\'\n        results[\'img\'] = corrupt(\n            results[\'img\'].astype(np.uint8),\n            corruption_name=self.corruption,\n            severity=self.severity)\n        return results\n\n    def __repr__(self):\n        repr_str = self.__class__.__name__\n        repr_str += f\'(corruption={self.corruption}, \'\n        repr_str += f\'severity={self.severity})\'\n        return repr_str\n\n\n@PIPELINES.register_module()\nclass Albu(object):\n\n    def __init__(self,\n                 transforms,\n                 bbox_params=None,\n                 keymap=None,\n                 update_pad_shape=False,\n                 skip_img_without_anno=False):\n        """"""\n        Adds custom transformations from Albumentations lib.\n        Please, visit `https://albumentations.readthedocs.io`\n        to get more information.\n\n        transforms (list): list of albu transformations\n        bbox_params (dict): bbox_params for albumentation `Compose`\n        keymap (dict): contains {\'input key\':\'albumentation-style key\'}\n        skip_img_without_anno (bool): whether to skip the image\n                                      if no ann left after aug\n        """"""\n        if Compose is None:\n            raise RuntimeError(\'albumentations is not installed\')\n\n        self.transforms = transforms\n        self.filter_lost_elements = False\n        self.update_pad_shape = update_pad_shape\n        self.skip_img_without_anno = skip_img_without_anno\n\n        # A simple workaround to remove masks without boxes\n        if (isinstance(bbox_params, dict) and \'label_fields\' in bbox_params\n                and \'filter_lost_elements\' in bbox_params):\n            self.filter_lost_elements = True\n            self.origin_label_fields = bbox_params[\'label_fields\']\n            bbox_params[\'label_fields\'] = [\'idx_mapper\']\n            del bbox_params[\'filter_lost_elements\']\n\n        self.bbox_params = (\n            self.albu_builder(bbox_params) if bbox_params else None)\n        self.aug = Compose([self.albu_builder(t) for t in self.transforms],\n                           bbox_params=self.bbox_params)\n\n        if not keymap:\n            self.keymap_to_albu = {\n                \'img\': \'image\',\n                \'gt_masks\': \'masks\',\n                \'gt_bboxes\': \'bboxes\'\n            }\n        else:\n            self.keymap_to_albu = keymap\n        self.keymap_back = {v: k for k, v in self.keymap_to_albu.items()}\n\n    def albu_builder(self, cfg):\n        """"""Import a module from albumentations.\n        Inherits some of `build_from_cfg` logic.\n\n        Args:\n            cfg (dict): Config dict. It should at least contain the key ""type"".\n        Returns:\n            obj: The constructed object.\n        """"""\n        assert isinstance(cfg, dict) and \'type\' in cfg\n        args = cfg.copy()\n\n        obj_type = args.pop(\'type\')\n        if mmcv.is_str(obj_type):\n            if albumentations is None:\n                raise RuntimeError(\'albumentations is not installed\')\n            obj_cls = getattr(albumentations, obj_type)\n        elif inspect.isclass(obj_type):\n            obj_cls = obj_type\n        else:\n            raise TypeError(\n                f\'type must be a str or valid type, but got {type(obj_type)}\')\n\n        if \'transforms\' in args:\n            args[\'transforms\'] = [\n                self.albu_builder(transform)\n                for transform in args[\'transforms\']\n            ]\n\n        return obj_cls(**args)\n\n    @staticmethod\n    def mapper(d, keymap):\n        """"""\n        Dictionary mapper.\n        Renames keys according to keymap provided.\n\n        Args:\n            d (dict): old dict\n            keymap (dict): {\'old_key\':\'new_key\'}\n        Returns:\n            dict: new dict.\n        """"""\n        updated_dict = {}\n        for k, v in zip(d.keys(), d.values()):\n            new_k = keymap.get(k, k)\n            updated_dict[new_k] = d[k]\n        return updated_dict\n\n    def __call__(self, results):\n        # dict to albumentations format\n        results = self.mapper(results, self.keymap_to_albu)\n        # TODO: add bbox_fields\n        if \'bboxes\' in results:\n            # to list of boxes\n            if isinstance(results[\'bboxes\'], np.ndarray):\n                results[\'bboxes\'] = [x for x in results[\'bboxes\']]\n            # add pseudo-field for filtration\n            if self.filter_lost_elements:\n                results[\'idx_mapper\'] = np.arange(len(results[\'bboxes\']))\n\n        # TODO: Support mask structure in albu\n        if \'masks\' in results:\n            if isinstance(results[\'masks\'], PolygonMasks):\n                raise NotImplementedError(\n                    \'Albu only supports BitMap masks now\')\n            ori_masks = results[\'masks\']\n            results[\'masks\'] = results[\'masks\'].masks\n\n        results = self.aug(**results)\n\n        if \'bboxes\' in results:\n            if isinstance(results[\'bboxes\'], list):\n                results[\'bboxes\'] = np.array(\n                    results[\'bboxes\'], dtype=np.float32)\n            results[\'bboxes\'] = results[\'bboxes\'].reshape(-1, 4)\n\n            # filter label_fields\n            if self.filter_lost_elements:\n\n                for label in self.origin_label_fields:\n                    results[label] = np.array(\n                        [results[label][i] for i in results[\'idx_mapper\']])\n                if \'masks\' in results:\n                    results[\'masks\'] = np.array(\n                        [results[\'masks\'][i] for i in results[\'idx_mapper\']])\n                    results[\'masks\'] = ori_masks.__class__(\n                        results[\'masks\'], results[\'image\'].shape[0],\n                        results[\'image\'].shape[1])\n\n                if (not len(results[\'idx_mapper\'])\n                        and self.skip_img_without_anno):\n                    return None\n\n        if \'gt_labels\' in results:\n            if isinstance(results[\'gt_labels\'], list):\n                results[\'gt_labels\'] = np.array(results[\'gt_labels\'])\n            results[\'gt_labels\'] = results[\'gt_labels\'].astype(np.int64)\n\n        # back to the original format\n        results = self.mapper(results, self.keymap_back)\n\n        # update final shape\n        if self.update_pad_shape:\n            results[\'pad_shape\'] = results[\'img\'].shape\n\n        return results\n\n    def __repr__(self):\n        repr_str = self.__class__.__name__ + f\'(transforms={self.transforms})\'\n        return repr_str\n'"
mmdet/datasets/samplers/__init__.py,0,"b""from .distributed_sampler import DistributedSampler\nfrom .group_sampler import DistributedGroupSampler, GroupSampler\n\n__all__ = ['DistributedSampler', 'DistributedGroupSampler', 'GroupSampler']\n"""
mmdet/datasets/samplers/distributed_sampler.py,4,"b'import torch\nfrom torch.utils.data import DistributedSampler as _DistributedSampler\n\n\nclass DistributedSampler(_DistributedSampler):\n\n    def __init__(self, dataset, num_replicas=None, rank=None, shuffle=True):\n        super().__init__(dataset, num_replicas=num_replicas, rank=rank)\n        self.shuffle = shuffle\n\n    def __iter__(self):\n        # deterministically shuffle based on epoch\n        if self.shuffle:\n            g = torch.Generator()\n            g.manual_seed(self.epoch)\n            indices = torch.randperm(len(self.dataset), generator=g).tolist()\n        else:\n            indices = torch.arange(len(self.dataset)).tolist()\n\n        # add extra samples to make it evenly divisible\n        indices += indices[:(self.total_size - len(indices))]\n        assert len(indices) == self.total_size\n\n        # subsample\n        indices = indices[self.rank:self.total_size:self.num_replicas]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n'"
mmdet/datasets/samplers/group_sampler.py,5,"b'from __future__ import division\nimport math\n\nimport numpy as np\nimport torch\nfrom mmcv.runner import get_dist_info\nfrom torch.utils.data import Sampler\n\n\nclass GroupSampler(Sampler):\n\n    def __init__(self, dataset, samples_per_gpu=1):\n        assert hasattr(dataset, \'flag\')\n        self.dataset = dataset\n        self.samples_per_gpu = samples_per_gpu\n        self.flag = dataset.flag.astype(np.int64)\n        self.group_sizes = np.bincount(self.flag)\n        self.num_samples = 0\n        for i, size in enumerate(self.group_sizes):\n            self.num_samples += int(np.ceil(\n                size / self.samples_per_gpu)) * self.samples_per_gpu\n\n    def __iter__(self):\n        indices = []\n        for i, size in enumerate(self.group_sizes):\n            if size == 0:\n                continue\n            indice = np.where(self.flag == i)[0]\n            assert len(indice) == size\n            np.random.shuffle(indice)\n            num_extra = int(np.ceil(size / self.samples_per_gpu)\n                            ) * self.samples_per_gpu - len(indice)\n            indice = np.concatenate(\n                [indice, np.random.choice(indice, num_extra)])\n            indices.append(indice)\n        indices = np.concatenate(indices)\n        indices = [\n            indices[i * self.samples_per_gpu:(i + 1) * self.samples_per_gpu]\n            for i in np.random.permutation(\n                range(len(indices) // self.samples_per_gpu))\n        ]\n        indices = np.concatenate(indices)\n        indices = indices.astype(np.int64).tolist()\n        assert len(indices) == self.num_samples\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_samples\n\n\nclass DistributedGroupSampler(Sampler):\n    """"""Sampler that restricts data loading to a subset of the dataset.\n\n    It is especially useful in conjunction with\n    :class:`torch.nn.parallel.DistributedDataParallel`. In such case, each\n    process can pass a DistributedSampler instance as a DataLoader sampler,\n    and load a subset of the original dataset that is exclusive to it.\n\n    .. note::\n        Dataset is assumed to be of constant size.\n\n    Arguments:\n        dataset: Dataset used for sampling.\n        num_replicas (optional): Number of processes participating in\n            distributed training.\n        rank (optional): Rank of the current process within num_replicas.\n    """"""\n\n    def __init__(self,\n                 dataset,\n                 samples_per_gpu=1,\n                 num_replicas=None,\n                 rank=None):\n        _rank, _num_replicas = get_dist_info()\n        if num_replicas is None:\n            num_replicas = _num_replicas\n        if rank is None:\n            rank = _rank\n        self.dataset = dataset\n        self.samples_per_gpu = samples_per_gpu\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n\n        assert hasattr(self.dataset, \'flag\')\n        self.flag = self.dataset.flag\n        self.group_sizes = np.bincount(self.flag)\n\n        self.num_samples = 0\n        for i, j in enumerate(self.group_sizes):\n            self.num_samples += int(\n                math.ceil(self.group_sizes[i] * 1.0 / self.samples_per_gpu /\n                          self.num_replicas)) * self.samples_per_gpu\n        self.total_size = self.num_samples * self.num_replicas\n\n    def __iter__(self):\n        # deterministically shuffle based on epoch\n        g = torch.Generator()\n        g.manual_seed(self.epoch)\n\n        indices = []\n        for i, size in enumerate(self.group_sizes):\n            if size > 0:\n                indice = np.where(self.flag == i)[0]\n                assert len(indice) == size\n                indice = indice[list(torch.randperm(int(size),\n                                                    generator=g))].tolist()\n                extra = int(\n                    math.ceil(\n                        size * 1.0 / self.samples_per_gpu / self.num_replicas)\n                ) * self.samples_per_gpu * self.num_replicas - len(indice)\n                # pad indice\n                tmp = indice.copy()\n                for _ in range(extra // size):\n                    indice.extend(tmp)\n                indice.extend(tmp[:extra % size])\n                indices.extend(indice)\n\n        assert len(indices) == self.total_size\n\n        indices = [\n            indices[j] for i in list(\n                torch.randperm(\n                    len(indices) // self.samples_per_gpu, generator=g))\n            for j in range(i * self.samples_per_gpu, (i + 1) *\n                           self.samples_per_gpu)\n        ]\n\n        # subsample\n        offset = self.num_samples * self.rank\n        indices = indices[offset:offset + self.num_samples]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_samples\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch\n'"
mmdet/models/backbones/__init__.py,0,"b""from .hrnet import HRNet\nfrom .regnet import RegNet\nfrom .res2net import Res2Net\nfrom .resnet import ResNet, ResNetV1d\nfrom .resnext import ResNeXt\nfrom .ssd_vgg import SSDVGG\n\n__all__ = [\n    'RegNet', 'ResNet', 'ResNetV1d', 'ResNeXt', 'SSDVGG', 'HRNet', 'Res2Net'\n]\n"""
mmdet/models/backbones/hrnet.py,3,"b'import torch.nn as nn\nfrom mmcv.cnn import (build_conv_layer, build_norm_layer, constant_init,\n                      kaiming_init)\nfrom mmcv.runner import load_checkpoint\nfrom torch.nn.modules.batchnorm import _BatchNorm\n\nfrom mmdet.utils import get_root_logger\nfrom ..builder import BACKBONES\nfrom .resnet import BasicBlock, Bottleneck\n\n\nclass HRModule(nn.Module):\n    """""" High-Resolution Module for HRNet. In this module, every branch\n    has 4 BasicBlocks/Bottlenecks. Fusion/Exchange is in this module.\n    """"""\n\n    def __init__(self,\n                 num_branches,\n                 blocks,\n                 num_blocks,\n                 in_channels,\n                 num_channels,\n                 multiscale_output=True,\n                 with_cp=False,\n                 conv_cfg=None,\n                 norm_cfg=dict(type=\'BN\')):\n        super(HRModule, self).__init__()\n        self._check_branches(num_branches, num_blocks, in_channels,\n                             num_channels)\n\n        self.in_channels = in_channels\n        self.num_branches = num_branches\n\n        self.multiscale_output = multiscale_output\n        self.norm_cfg = norm_cfg\n        self.conv_cfg = conv_cfg\n        self.with_cp = with_cp\n        self.branches = self._make_branches(num_branches, blocks, num_blocks,\n                                            num_channels)\n        self.fuse_layers = self._make_fuse_layers()\n        self.relu = nn.ReLU(inplace=False)\n\n    def _check_branches(self, num_branches, num_blocks, in_channels,\n                        num_channels):\n        if num_branches != len(num_blocks):\n            error_msg = f\'NUM_BRANCHES({num_branches}) \' \\\n                f\'!= NUM_BLOCKS({len(num_blocks)})\'\n            raise ValueError(error_msg)\n\n        if num_branches != len(num_channels):\n            error_msg = f\'NUM_BRANCHES({num_branches}) \' \\\n                f\'!= NUM_CHANNELS({len(num_channels)})\'\n            raise ValueError(error_msg)\n\n        if num_branches != len(in_channels):\n            error_msg = f\'NUM_BRANCHES({num_branches}) \' \\\n                f\'!= NUM_INCHANNELS({len(in_channels)})\'\n            raise ValueError(error_msg)\n\n    def _make_one_branch(self,\n                         branch_index,\n                         block,\n                         num_blocks,\n                         num_channels,\n                         stride=1):\n        downsample = None\n        if stride != 1 or \\\n                self.in_channels[branch_index] != \\\n                num_channels[branch_index] * block.expansion:\n            downsample = nn.Sequential(\n                build_conv_layer(\n                    self.conv_cfg,\n                    self.in_channels[branch_index],\n                    num_channels[branch_index] * block.expansion,\n                    kernel_size=1,\n                    stride=stride,\n                    bias=False),\n                build_norm_layer(self.norm_cfg, num_channels[branch_index] *\n                                 block.expansion)[1])\n\n        layers = []\n        layers.append(\n            block(\n                self.in_channels[branch_index],\n                num_channels[branch_index],\n                stride,\n                downsample=downsample,\n                with_cp=self.with_cp,\n                norm_cfg=self.norm_cfg,\n                conv_cfg=self.conv_cfg))\n        self.in_channels[branch_index] = \\\n            num_channels[branch_index] * block.expansion\n        for i in range(1, num_blocks[branch_index]):\n            layers.append(\n                block(\n                    self.in_channels[branch_index],\n                    num_channels[branch_index],\n                    with_cp=self.with_cp,\n                    norm_cfg=self.norm_cfg,\n                    conv_cfg=self.conv_cfg))\n\n        return nn.Sequential(*layers)\n\n    def _make_branches(self, num_branches, block, num_blocks, num_channels):\n        branches = []\n\n        for i in range(num_branches):\n            branches.append(\n                self._make_one_branch(i, block, num_blocks, num_channels))\n\n        return nn.ModuleList(branches)\n\n    def _make_fuse_layers(self):\n        if self.num_branches == 1:\n            return None\n\n        num_branches = self.num_branches\n        in_channels = self.in_channels\n        fuse_layers = []\n        num_out_branches = num_branches if self.multiscale_output else 1\n        for i in range(num_out_branches):\n            fuse_layer = []\n            for j in range(num_branches):\n                if j > i:\n                    fuse_layer.append(\n                        nn.Sequential(\n                            build_conv_layer(\n                                self.conv_cfg,\n                                in_channels[j],\n                                in_channels[i],\n                                kernel_size=1,\n                                stride=1,\n                                padding=0,\n                                bias=False),\n                            build_norm_layer(self.norm_cfg, in_channels[i])[1],\n                            nn.Upsample(\n                                scale_factor=2**(j - i), mode=\'nearest\')))\n                elif j == i:\n                    fuse_layer.append(None)\n                else:\n                    conv_downsamples = []\n                    for k in range(i - j):\n                        if k == i - j - 1:\n                            conv_downsamples.append(\n                                nn.Sequential(\n                                    build_conv_layer(\n                                        self.conv_cfg,\n                                        in_channels[j],\n                                        in_channels[i],\n                                        kernel_size=3,\n                                        stride=2,\n                                        padding=1,\n                                        bias=False),\n                                    build_norm_layer(self.norm_cfg,\n                                                     in_channels[i])[1]))\n                        else:\n                            conv_downsamples.append(\n                                nn.Sequential(\n                                    build_conv_layer(\n                                        self.conv_cfg,\n                                        in_channels[j],\n                                        in_channels[j],\n                                        kernel_size=3,\n                                        stride=2,\n                                        padding=1,\n                                        bias=False),\n                                    build_norm_layer(self.norm_cfg,\n                                                     in_channels[j])[1],\n                                    nn.ReLU(inplace=False)))\n                    fuse_layer.append(nn.Sequential(*conv_downsamples))\n            fuse_layers.append(nn.ModuleList(fuse_layer))\n\n        return nn.ModuleList(fuse_layers)\n\n    def forward(self, x):\n        if self.num_branches == 1:\n            return [self.branches[0](x[0])]\n\n        for i in range(self.num_branches):\n            x[i] = self.branches[i](x[i])\n\n        x_fuse = []\n        for i in range(len(self.fuse_layers)):\n            y = 0\n            for j in range(self.num_branches):\n                if i == j:\n                    y += x[j]\n                else:\n                    y += self.fuse_layers[i][j](x[j])\n            x_fuse.append(self.relu(y))\n        return x_fuse\n\n\n@BACKBONES.register_module()\nclass HRNet(nn.Module):\n    """"""HRNet backbone.\n\n    High-Resolution Representations for Labeling Pixels and Regions\n    arXiv: https://arxiv.org/abs/1904.04514\n\n    Args:\n        extra (dict): detailed configuration for each stage of HRNet.\n        in_channels (int): Number of input image channels. Default: 3.\n        conv_cfg (dict): dictionary to construct and config conv layer.\n        norm_cfg (dict): dictionary to construct and config norm layer.\n        norm_eval (bool): Whether to set norm layers to eval mode, namely,\n            freeze running stats (mean and var). Note: Effect on Batch Norm\n            and its variants only.\n        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n            memory while slowing down the training speed.\n        zero_init_residual (bool): whether to use zero init for last norm layer\n            in resblocks to let them behave as identity.\n\n    Example:\n        >>> from mmdet.models import HRNet\n        >>> import torch\n        >>> extra = dict(\n        >>>     stage1=dict(\n        >>>         num_modules=1,\n        >>>         num_branches=1,\n        >>>         block=\'BOTTLENECK\',\n        >>>         num_blocks=(4, ),\n        >>>         num_channels=(64, )),\n        >>>     stage2=dict(\n        >>>         num_modules=1,\n        >>>         num_branches=2,\n        >>>         block=\'BASIC\',\n        >>>         num_blocks=(4, 4),\n        >>>         num_channels=(32, 64)),\n        >>>     stage3=dict(\n        >>>         num_modules=4,\n        >>>         num_branches=3,\n        >>>         block=\'BASIC\',\n        >>>         num_blocks=(4, 4, 4),\n        >>>         num_channels=(32, 64, 128)),\n        >>>     stage4=dict(\n        >>>         num_modules=3,\n        >>>         num_branches=4,\n        >>>         block=\'BASIC\',\n        >>>         num_blocks=(4, 4, 4, 4),\n        >>>         num_channels=(32, 64, 128, 256)))\n        >>> self = HRNet(extra, in_channels=1)\n        >>> self.eval()\n        >>> inputs = torch.rand(1, 1, 32, 32)\n        >>> level_outputs = self.forward(inputs)\n        >>> for level_out in level_outputs:\n        ...     print(tuple(level_out.shape))\n        (1, 32, 8, 8)\n        (1, 64, 4, 4)\n        (1, 128, 2, 2)\n        (1, 256, 1, 1)\n    """"""\n\n    blocks_dict = {\'BASIC\': BasicBlock, \'BOTTLENECK\': Bottleneck}\n\n    def __init__(self,\n                 extra,\n                 in_channels=3,\n                 conv_cfg=None,\n                 norm_cfg=dict(type=\'BN\'),\n                 norm_eval=True,\n                 with_cp=False,\n                 zero_init_residual=False):\n        super(HRNet, self).__init__()\n        self.extra = extra\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        self.norm_eval = norm_eval\n        self.with_cp = with_cp\n        self.zero_init_residual = zero_init_residual\n\n        # stem net\n        self.norm1_name, norm1 = build_norm_layer(self.norm_cfg, 64, postfix=1)\n        self.norm2_name, norm2 = build_norm_layer(self.norm_cfg, 64, postfix=2)\n\n        self.conv1 = build_conv_layer(\n            self.conv_cfg,\n            in_channels,\n            64,\n            kernel_size=3,\n            stride=2,\n            padding=1,\n            bias=False)\n\n        self.add_module(self.norm1_name, norm1)\n        self.conv2 = build_conv_layer(\n            self.conv_cfg,\n            64,\n            64,\n            kernel_size=3,\n            stride=2,\n            padding=1,\n            bias=False)\n\n        self.add_module(self.norm2_name, norm2)\n        self.relu = nn.ReLU(inplace=True)\n\n        # stage 1\n        self.stage1_cfg = self.extra[\'stage1\']\n        num_channels = self.stage1_cfg[\'num_channels\'][0]\n        block_type = self.stage1_cfg[\'block\']\n        num_blocks = self.stage1_cfg[\'num_blocks\'][0]\n\n        block = self.blocks_dict[block_type]\n        stage1_out_channels = num_channels * block.expansion\n        self.layer1 = self._make_layer(block, 64, num_channels, num_blocks)\n\n        # stage 2\n        self.stage2_cfg = self.extra[\'stage2\']\n        num_channels = self.stage2_cfg[\'num_channels\']\n        block_type = self.stage2_cfg[\'block\']\n\n        block = self.blocks_dict[block_type]\n        num_channels = [channel * block.expansion for channel in num_channels]\n        self.transition1 = self._make_transition_layer([stage1_out_channels],\n                                                       num_channels)\n        self.stage2, pre_stage_channels = self._make_stage(\n            self.stage2_cfg, num_channels)\n\n        # stage 3\n        self.stage3_cfg = self.extra[\'stage3\']\n        num_channels = self.stage3_cfg[\'num_channels\']\n        block_type = self.stage3_cfg[\'block\']\n\n        block = self.blocks_dict[block_type]\n        num_channels = [channel * block.expansion for channel in num_channels]\n        self.transition2 = self._make_transition_layer(pre_stage_channels,\n                                                       num_channels)\n        self.stage3, pre_stage_channels = self._make_stage(\n            self.stage3_cfg, num_channels)\n\n        # stage 4\n        self.stage4_cfg = self.extra[\'stage4\']\n        num_channels = self.stage4_cfg[\'num_channels\']\n        block_type = self.stage4_cfg[\'block\']\n\n        block = self.blocks_dict[block_type]\n        num_channels = [channel * block.expansion for channel in num_channels]\n        self.transition3 = self._make_transition_layer(pre_stage_channels,\n                                                       num_channels)\n        self.stage4, pre_stage_channels = self._make_stage(\n            self.stage4_cfg, num_channels)\n\n    @property\n    def norm1(self):\n        return getattr(self, self.norm1_name)\n\n    @property\n    def norm2(self):\n        return getattr(self, self.norm2_name)\n\n    def _make_transition_layer(self, num_channels_pre_layer,\n                               num_channels_cur_layer):\n        num_branches_cur = len(num_channels_cur_layer)\n        num_branches_pre = len(num_channels_pre_layer)\n\n        transition_layers = []\n        for i in range(num_branches_cur):\n            if i < num_branches_pre:\n                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n                    transition_layers.append(\n                        nn.Sequential(\n                            build_conv_layer(\n                                self.conv_cfg,\n                                num_channels_pre_layer[i],\n                                num_channels_cur_layer[i],\n                                kernel_size=3,\n                                stride=1,\n                                padding=1,\n                                bias=False),\n                            build_norm_layer(self.norm_cfg,\n                                             num_channels_cur_layer[i])[1],\n                            nn.ReLU(inplace=True)))\n                else:\n                    transition_layers.append(None)\n            else:\n                conv_downsamples = []\n                for j in range(i + 1 - num_branches_pre):\n                    in_channels = num_channels_pre_layer[-1]\n                    out_channels = num_channels_cur_layer[i] \\\n                        if j == i - num_branches_pre else in_channels\n                    conv_downsamples.append(\n                        nn.Sequential(\n                            build_conv_layer(\n                                self.conv_cfg,\n                                in_channels,\n                                out_channels,\n                                kernel_size=3,\n                                stride=2,\n                                padding=1,\n                                bias=False),\n                            build_norm_layer(self.norm_cfg, out_channels)[1],\n                            nn.ReLU(inplace=True)))\n                transition_layers.append(nn.Sequential(*conv_downsamples))\n\n        return nn.ModuleList(transition_layers)\n\n    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                build_conv_layer(\n                    self.conv_cfg,\n                    inplanes,\n                    planes * block.expansion,\n                    kernel_size=1,\n                    stride=stride,\n                    bias=False),\n                build_norm_layer(self.norm_cfg, planes * block.expansion)[1])\n\n        layers = []\n        layers.append(\n            block(\n                inplanes,\n                planes,\n                stride,\n                downsample=downsample,\n                with_cp=self.with_cp,\n                norm_cfg=self.norm_cfg,\n                conv_cfg=self.conv_cfg))\n        inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(\n                block(\n                    inplanes,\n                    planes,\n                    with_cp=self.with_cp,\n                    norm_cfg=self.norm_cfg,\n                    conv_cfg=self.conv_cfg))\n\n        return nn.Sequential(*layers)\n\n    def _make_stage(self, layer_config, in_channels, multiscale_output=True):\n        num_modules = layer_config[\'num_modules\']\n        num_branches = layer_config[\'num_branches\']\n        num_blocks = layer_config[\'num_blocks\']\n        num_channels = layer_config[\'num_channels\']\n        block = self.blocks_dict[layer_config[\'block\']]\n\n        hr_modules = []\n        for i in range(num_modules):\n            # multi_scale_output is only used for the last module\n            if not multiscale_output and i == num_modules - 1:\n                reset_multiscale_output = False\n            else:\n                reset_multiscale_output = True\n\n            hr_modules.append(\n                HRModule(\n                    num_branches,\n                    block,\n                    num_blocks,\n                    in_channels,\n                    num_channels,\n                    reset_multiscale_output,\n                    with_cp=self.with_cp,\n                    norm_cfg=self.norm_cfg,\n                    conv_cfg=self.conv_cfg))\n\n        return nn.Sequential(*hr_modules), in_channels\n\n    def init_weights(self, pretrained=None):\n        if isinstance(pretrained, str):\n            logger = get_root_logger()\n            load_checkpoint(self, pretrained, strict=False, logger=logger)\n        elif pretrained is None:\n            for m in self.modules():\n                if isinstance(m, nn.Conv2d):\n                    kaiming_init(m)\n                elif isinstance(m, (_BatchNorm, nn.GroupNorm)):\n                    constant_init(m, 1)\n\n            if self.zero_init_residual:\n                for m in self.modules():\n                    if isinstance(m, Bottleneck):\n                        constant_init(m.norm3, 0)\n                    elif isinstance(m, BasicBlock):\n                        constant_init(m.norm2, 0)\n        else:\n            raise TypeError(\'pretrained must be a str or None\')\n\n    def forward(self, x):\n\n        x = self.conv1(x)\n        x = self.norm1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.norm2(x)\n        x = self.relu(x)\n        x = self.layer1(x)\n\n        x_list = []\n        for i in range(self.stage2_cfg[\'num_branches\']):\n            if self.transition1[i] is not None:\n                x_list.append(self.transition1[i](x))\n            else:\n                x_list.append(x)\n        y_list = self.stage2(x_list)\n\n        x_list = []\n        for i in range(self.stage3_cfg[\'num_branches\']):\n            if self.transition2[i] is not None:\n                x_list.append(self.transition2[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y_list = self.stage3(x_list)\n\n        x_list = []\n        for i in range(self.stage4_cfg[\'num_branches\']):\n            if self.transition3[i] is not None:\n                x_list.append(self.transition3[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y_list = self.stage4(x_list)\n\n        return y_list\n\n    def train(self, mode=True):\n        super(HRNet, self).train(mode)\n        if mode and self.norm_eval:\n            for m in self.modules():\n                # trick: eval have effect on BatchNorm only\n                if isinstance(m, _BatchNorm):\n                    m.eval()\n'"
mmdet/models/backbones/regnet.py,2,"b'import numpy as np\nimport torch.nn as nn\nfrom mmcv.cnn import build_conv_layer, build_norm_layer\n\nfrom ..builder import BACKBONES\nfrom .resnet import ResNet\nfrom .resnext import Bottleneck\n\n\n@BACKBONES.register_module()\nclass RegNet(ResNet):\n    """"""RegNet backbone.\n\n    More details can be found in `paper <https://arxiv.org/abs/2003.13678>`_ .\n\n    Args:\n        arch (dict): The parameter of RegNets.\n            - w0 (int): initial width\n            - wa (float): slope of width\n            - wm (float): quantization parameter to quantize the width\n            - depth (int): depth of the backbone\n            - group_w (int): width of group\n            - bot_mul (float): bottleneck ratio, i.e. expansion of bottlneck.\n        strides (Sequence[int]): Strides of the first block of each stage.\n        base_channels (int): Base channels after stem layer.\n        in_channels (int): Number of input image channels. Default: 3.\n        dilations (Sequence[int]): Dilation of each stage.\n        out_indices (Sequence[int]): Output from which stages.\n        style (str): `pytorch` or `caffe`. If set to ""pytorch"", the stride-two\n            layer is the 3x3 conv layer, otherwise the stride-two layer is\n            the first 1x1 conv layer.\n        frozen_stages (int): Stages to be frozen (all param fixed). -1 means\n            not freezing any parameters.\n        norm_cfg (dict): dictionary to construct and config norm layer.\n        norm_eval (bool): Whether to set norm layers to eval mode, namely,\n            freeze running stats (mean and var). Note: Effect on Batch Norm\n            and its variants only.\n        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n            memory while slowing down the training speed.\n        zero_init_residual (bool): whether to use zero init for last norm layer\n            in resblocks to let them behave as identity.\n\n    Example:\n        >>> from mmdet.models import RegNet\n        >>> import torch\n        >>> self = RegNet(\n                arch=dict(\n                    w0=88,\n                    wa=26.31,\n                    wm=2.25,\n                    group_w=48,\n                    depth=25,\n                    bot_mul=1.0))\n        >>> self.eval()\n        >>> inputs = torch.rand(1, 3, 32, 32)\n        >>> level_outputs = self.forward(inputs)\n        >>> for level_out in level_outputs:\n        ...     print(tuple(level_out.shape))\n        (1, 96, 8, 8)\n        (1, 192, 4, 4)\n        (1, 432, 2, 2)\n        (1, 1008, 1, 1)\n    """"""\n    arch_settings = {\n        \'regnetx_800mf\':\n        dict(w0=56, wa=35.73, wm=2.28, group_w=16, depth=16, bot_mul=1.0),\n        \'regnetx_1.6gf\':\n        dict(w0=80, wa=34.01, wm=2.25, group_w=24, depth=18, bot_mul=1.0),\n        \'regnetx_3.2gf\':\n        dict(w0=88, wa=26.31, wm=2.25, group_w=48, depth=25, bot_mul=1.0),\n        \'regnetx_4.0gf\':\n        dict(w0=96, wa=38.65, wm=2.43, group_w=40, depth=23, bot_mul=1.0),\n        \'regnetx_6.4gf\':\n        dict(w0=184, wa=60.83, wm=2.07, group_w=56, depth=17, bot_mul=1.0),\n        \'regnetx_8.0gf\':\n        dict(w0=80, wa=49.56, wm=2.88, group_w=120, depth=23, bot_mul=1.0),\n        \'regnetx_12gf\':\n        dict(w0=168, wa=73.36, wm=2.37, group_w=112, depth=19, bot_mul=1.0),\n    }\n\n    def __init__(self,\n                 arch,\n                 in_channels=3,\n                 stem_channels=32,\n                 base_channels=32,\n                 strides=(2, 2, 2, 2),\n                 dilations=(1, 1, 1, 1),\n                 out_indices=(0, 1, 2, 3),\n                 style=\'pytorch\',\n                 deep_stem=False,\n                 avg_down=False,\n                 frozen_stages=-1,\n                 conv_cfg=None,\n                 norm_cfg=dict(type=\'BN\', requires_grad=True),\n                 norm_eval=True,\n                 dcn=None,\n                 stage_with_dcn=(False, False, False, False),\n                 plugins=None,\n                 with_cp=False,\n                 zero_init_residual=True):\n        super(ResNet, self).__init__()\n\n        # Generate RegNet parameters first\n        if isinstance(arch, str):\n            assert arch in self.arch_settings, \\\n                f\'""arch"": ""{arch}"" is not one of the\' \\\n                \' arch_settings\'\n            arch = self.arch_settings[arch]\n        elif not isinstance(arch, dict):\n            raise ValueError(\'Expect ""arch"" to be either a string \'\n                             f\'or a dict, got {type(arch)}\')\n\n        widths, num_stages = self.generate_regnet(\n            arch[\'w0\'],\n            arch[\'wa\'],\n            arch[\'wm\'],\n            arch[\'depth\'],\n        )\n        # Convert to per stage format\n        stage_widths, stage_blocks = self.get_stages_from_blocks(widths)\n        # Generate group widths and bot muls\n        group_widths = [arch[\'group_w\'] for _ in range(num_stages)]\n        self.bottleneck_ratio = [arch[\'bot_mul\'] for _ in range(num_stages)]\n        # Adjust the compatibility of stage_widths and group_widths\n        stage_widths, group_widths = self.adjust_width_group(\n            stage_widths, self.bottleneck_ratio, group_widths)\n\n        # Group params by stage\n        self.stage_widths = stage_widths\n        self.group_widths = group_widths\n        self.depth = sum(stage_blocks)\n        self.stem_channels = stem_channels\n        self.base_channels = base_channels\n        self.num_stages = num_stages\n        assert num_stages >= 1 and num_stages <= 4\n        self.strides = strides\n        self.dilations = dilations\n        assert len(strides) == len(dilations) == num_stages\n        self.out_indices = out_indices\n        assert max(out_indices) < num_stages\n        self.style = style\n        self.deep_stem = deep_stem\n        self.avg_down = avg_down\n        self.frozen_stages = frozen_stages\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        self.with_cp = with_cp\n        self.norm_eval = norm_eval\n        self.dcn = dcn\n        self.stage_with_dcn = stage_with_dcn\n        if dcn is not None:\n            assert len(stage_with_dcn) == num_stages\n        self.plugins = plugins\n        self.zero_init_residual = zero_init_residual\n        self.block = Bottleneck\n        self.block.expansion = 1\n        self.stage_blocks = stage_blocks[:num_stages]\n\n        self._make_stem_layer(in_channels, stem_channels)\n\n        self.inplanes = stem_channels\n        self.res_layers = []\n        for i, num_blocks in enumerate(self.stage_blocks):\n            stride = self.strides[i]\n            dilation = self.dilations[i]\n            group_width = self.group_widths[i]\n            width = int(round(self.stage_widths[i] * self.bottleneck_ratio[i]))\n            stage_groups = width // group_width\n\n            dcn = self.dcn if self.stage_with_dcn[i] else None\n            if self.plugins is not None:\n                stage_plugins = self.make_stage_plugins(self.plugins, i)\n            else:\n                stage_plugins = None\n\n            res_layer = self.make_res_layer(\n                block=self.block,\n                inplanes=self.inplanes,\n                planes=self.stage_widths[i],\n                num_blocks=num_blocks,\n                stride=stride,\n                dilation=dilation,\n                style=self.style,\n                avg_down=self.avg_down,\n                with_cp=self.with_cp,\n                conv_cfg=self.conv_cfg,\n                norm_cfg=self.norm_cfg,\n                dcn=dcn,\n                plugins=stage_plugins,\n                groups=stage_groups,\n                base_width=group_width,\n                base_channels=self.stage_widths[i])\n            self.inplanes = self.stage_widths[i]\n            layer_name = f\'layer{i + 1}\'\n            self.add_module(layer_name, res_layer)\n            self.res_layers.append(layer_name)\n\n        self._freeze_stages()\n\n        self.feat_dim = stage_widths[-1]\n\n    def _make_stem_layer(self, in_channels, base_channels):\n        self.conv1 = build_conv_layer(\n            self.conv_cfg,\n            in_channels,\n            base_channels,\n            kernel_size=3,\n            stride=2,\n            padding=1,\n            bias=False)\n        self.norm1_name, norm1 = build_norm_layer(\n            self.norm_cfg, base_channels, postfix=1)\n        self.add_module(self.norm1_name, norm1)\n        self.relu = nn.ReLU(inplace=True)\n\n    def generate_regnet(self,\n                        initial_width,\n                        width_slope,\n                        width_parameter,\n                        depth,\n                        divisor=8):\n        """"""Generates per block width from RegNet parameters.\n\n        Args:\n            initial_width ([int]): Initial width of the backbone\n            width_slope ([float]): Slope of the quantized linear function\n            width_parameter ([int]): Parameter used to quantize the width.\n            depth ([int]): Depth of the backbone.\n            divisor (int, optional): The divisor of channels. Defaults to 8.\n\n        Returns:\n            list, int: return a list of widths of each stage and the number of\n                stages\n        """"""\n        assert width_slope >= 0\n        assert initial_width > 0\n        assert width_parameter > 1\n        assert initial_width % divisor == 0\n        widths_cont = np.arange(depth) * width_slope + initial_width\n        ks = np.round(\n            np.log(widths_cont / initial_width) / np.log(width_parameter))\n        widths = initial_width * np.power(width_parameter, ks)\n        widths = np.round(np.divide(widths, divisor)) * divisor\n        num_stages = len(np.unique(widths))\n        widths, widths_cont = widths.astype(int).tolist(), widths_cont.tolist()\n        return widths, num_stages\n\n    @staticmethod\n    def quantize_float(number, divisor):\n        """"""Converts a float to closest non-zero int divisible by divior.\n\n        Args:\n            number (int): Original number to be quantized.\n            divisor (int): Divisor used to quantize the number.\n\n        Returns:\n            int: quantized number that is divisible by devisor.\n        """"""\n        return int(round(number / divisor) * divisor)\n\n    def adjust_width_group(self, widths, bottleneck_ratio, groups):\n        """"""Adjusts the compatibility of widths and groups.\n\n        Args:\n            widths (list[int]): Width of each stage.\n            bottleneck_ratio (float): Bottleneck ratio.\n            groups (int): number of groups in each stage\n\n        Returns:\n            tuple(list): The adjusted widths and groups of each stage.\n        """"""\n        bottleneck_width = [\n            int(w * b) for w, b in zip(widths, bottleneck_ratio)\n        ]\n        groups = [min(g, w_bot) for g, w_bot in zip(groups, bottleneck_width)]\n        bottleneck_width = [\n            self.quantize_float(w_bot, g)\n            for w_bot, g in zip(bottleneck_width, groups)\n        ]\n        widths = [\n            int(w_bot / b)\n            for w_bot, b in zip(bottleneck_width, bottleneck_ratio)\n        ]\n        return widths, groups\n\n    def get_stages_from_blocks(self, widths):\n        """"""Gets widths/stage_blocks of network at each stage\n\n        Args:\n            widths (list[int]): Width in each stage.\n\n        Returns:\n            tuple(list): width and depth of each stage\n        """"""\n        width_diff = [\n            width != width_prev\n            for width, width_prev in zip(widths + [0], [0] + widths)\n        ]\n        stage_widths = [\n            width for width, diff in zip(widths, width_diff[:-1]) if diff\n        ]\n        stage_blocks = np.diff([\n            depth for depth, diff in zip(range(len(width_diff)), width_diff)\n            if diff\n        ]).tolist()\n        return stage_widths, stage_blocks\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.norm1(x)\n        x = self.relu(x)\n\n        outs = []\n        for i, layer_name in enumerate(self.res_layers):\n            res_layer = getattr(self, layer_name)\n            x = res_layer(x)\n            if i in self.out_indices:\n                outs.append(x)\n        return tuple(outs)\n'"
mmdet/models/backbones/res2net.py,7,"b'import math\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.checkpoint as cp\nfrom mmcv.cnn import build_conv_layer, build_norm_layer\n\nfrom ..builder import BACKBONES\nfrom .resnet import Bottleneck as _Bottleneck\nfrom .resnet import ResNet\n\n\nclass Bottle2neck(_Bottleneck):\n    expansion = 4\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 scales=4,\n                 base_width=26,\n                 base_channels=64,\n                 stage_type=\'normal\',\n                 **kwargs):\n        """"""Bottle2neck block for Res2Net.\n        If style is ""pytorch"", the stride-two layer is the 3x3 conv layer,\n        if it is ""caffe"", the stride-two layer is the first 1x1 conv layer.\n        """"""\n        super(Bottle2neck, self).__init__(inplanes, planes, **kwargs)\n        assert scales > 1, \'Res2Net degenerates to ResNet when scales = 1.\'\n        width = int(math.floor(self.planes * (base_width / base_channels)))\n\n        self.norm1_name, norm1 = build_norm_layer(\n            self.norm_cfg, width * scales, postfix=1)\n        self.norm3_name, norm3 = build_norm_layer(\n            self.norm_cfg, self.planes * self.expansion, postfix=3)\n\n        self.conv1 = build_conv_layer(\n            self.conv_cfg,\n            self.inplanes,\n            width * scales,\n            kernel_size=1,\n            stride=self.conv1_stride,\n            bias=False)\n        self.add_module(self.norm1_name, norm1)\n\n        if stage_type == \'stage\' and self.conv2_stride != 1:\n            self.pool = nn.AvgPool2d(\n                kernel_size=3, stride=self.conv2_stride, padding=1)\n        convs = []\n        bns = []\n\n        fallback_on_stride = False\n        if self.with_dcn:\n            fallback_on_stride = self.dcn.pop(\'fallback_on_stride\', False)\n        if not self.with_dcn or fallback_on_stride:\n            for i in range(scales - 1):\n                convs.append(\n                    build_conv_layer(\n                        self.conv_cfg,\n                        width,\n                        width,\n                        kernel_size=3,\n                        stride=self.conv2_stride,\n                        padding=self.dilation,\n                        dilation=self.dilation,\n                        bias=False))\n                bns.append(\n                    build_norm_layer(self.norm_cfg, width, postfix=i + 1)[1])\n            self.convs = nn.ModuleList(convs)\n            self.bns = nn.ModuleList(bns)\n        else:\n            assert self.conv_cfg is None, \'conv_cfg must be None for DCN\'\n            for i in range(scales - 1):\n                convs.append(\n                    build_conv_layer(\n                        self.dcn,\n                        width,\n                        width,\n                        kernel_size=3,\n                        stride=self.conv2_stride,\n                        padding=self.dilation,\n                        dilation=self.dilation,\n                        bias=False))\n                bns.append(\n                    build_norm_layer(self.norm_cfg, width, postfix=i + 1)[1])\n            self.convs = nn.ModuleList(convs)\n            self.bns = nn.ModuleList(bns)\n\n        self.conv3 = build_conv_layer(\n            self.conv_cfg,\n            width * scales,\n            self.planes * self.expansion,\n            kernel_size=1,\n            bias=False)\n        self.add_module(self.norm3_name, norm3)\n\n        self.stage_type = stage_type\n        self.scales = scales\n        self.width = width\n        delattr(self, \'conv2\')\n        delattr(self, self.norm2_name)\n\n    def forward(self, x):\n\n        def _inner_forward(x):\n            identity = x\n\n            out = self.conv1(x)\n            out = self.norm1(out)\n            out = self.relu(out)\n\n            if self.with_plugins:\n                out = self.forward_plugin(out, self.after_conv1_plugin_names)\n\n            spx = torch.split(out, self.width, 1)\n            sp = self.convs[0](spx[0])\n            sp = self.relu(self.bns[0](sp))\n            out = sp\n            for i in range(1, self.scales - 1):\n                if self.stage_type == \'stage\':\n                    sp = spx[i]\n                else:\n                    sp = sp + spx[i]\n                sp = self.convs[i](sp)\n                sp = self.relu(self.bns[i](sp))\n                out = torch.cat((out, sp), 1)\n\n            if self.stage_type == \'normal\' or self.conv2_stride == 1:\n                out = torch.cat((out, spx[self.scales - 1]), 1)\n            elif self.stage_type == \'stage\':\n                out = torch.cat((out, self.pool(spx[self.scales - 1])), 1)\n\n            if self.with_plugins:\n                out = self.forward_plugin(out, self.after_conv2_plugin_names)\n\n            out = self.conv3(out)\n            out = self.norm3(out)\n\n            if self.with_plugins:\n                out = self.forward_plugin(out, self.after_conv3_plugin_names)\n\n            if self.downsample is not None:\n                identity = self.downsample(x)\n\n            out += identity\n\n            return out\n\n        if self.with_cp and x.requires_grad:\n            out = cp.checkpoint(_inner_forward, x)\n        else:\n            out = _inner_forward(x)\n\n        out = self.relu(out)\n\n        return out\n\n\nclass Res2Layer(nn.Sequential):\n    """"""Res2Layer to build Res2Net style backbone.\n\n    Args:\n        block (nn.Module): block used to build ResLayer.\n        inplanes (int): inplanes of block.\n        planes (int): planes of block.\n        num_blocks (int): number of blocks.\n        stride (int): stride of the first block. Default: 1\n        avg_down (bool): Use AvgPool instead of stride conv when\n            downsampling in the bottle2neck. Default: False\n        conv_cfg (dict): dictionary to construct and config conv layer.\n            Default: None\n        norm_cfg (dict): dictionary to construct and config norm layer.\n            Default: dict(type=\'BN\')\n        scales (int): Scales used in Res2Net. Default: 4\n        base_width (int): Basic width of each scale. Default: 26\n    """"""\n\n    def __init__(self,\n                 block,\n                 inplanes,\n                 planes,\n                 num_blocks,\n                 stride=1,\n                 avg_down=True,\n                 conv_cfg=None,\n                 norm_cfg=dict(type=\'BN\'),\n                 scales=4,\n                 base_width=26,\n                 **kwargs):\n        self.block = block\n\n        downsample = None\n        if stride != 1 or inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.AvgPool2d(\n                    kernel_size=stride,\n                    stride=stride,\n                    ceil_mode=True,\n                    count_include_pad=False),\n                build_conv_layer(\n                    conv_cfg,\n                    inplanes,\n                    planes * block.expansion,\n                    kernel_size=1,\n                    stride=1,\n                    bias=False),\n                build_norm_layer(norm_cfg, planes * block.expansion)[1],\n            )\n\n        layers = []\n        layers.append(\n            block(\n                inplanes=inplanes,\n                planes=planes,\n                stride=stride,\n                downsample=downsample,\n                conv_cfg=conv_cfg,\n                norm_cfg=norm_cfg,\n                scales=scales,\n                base_width=base_width,\n                stage_type=\'stage\',\n                **kwargs))\n        inplanes = planes * block.expansion\n        for i in range(1, num_blocks):\n            layers.append(\n                block(\n                    inplanes=inplanes,\n                    planes=planes,\n                    stride=1,\n                    conv_cfg=conv_cfg,\n                    norm_cfg=norm_cfg,\n                    scales=scales,\n                    base_width=base_width,\n                    **kwargs))\n        super(Res2Layer, self).__init__(*layers)\n\n\n@BACKBONES.register_module()\nclass Res2Net(ResNet):\n    """"""Res2Net backbone.\n\n    Args:\n        scales (int): Scales used in Res2Net. Default: 4\n        base_width (int): Basic width of each scale. Default: 26\n        depth (int): Depth of res2net, from {50, 101, 152}.\n        in_channels (int): Number of input image channels. Default: 3.\n        num_stages (int): Res2net stages. Default: 4.\n        strides (Sequence[int]): Strides of the first block of each stage.\n        dilations (Sequence[int]): Dilation of each stage.\n        out_indices (Sequence[int]): Output from which stages.\n        style (str): `pytorch` or `caffe`. If set to ""pytorch"", the stride-two\n            layer is the 3x3 conv layer, otherwise the stride-two layer is\n            the first 1x1 conv layer.\n        deep_stem (bool): Replace 7x7 conv in input stem with 3 3x3 conv\n        avg_down (bool): Use AvgPool instead of stride conv when\n            downsampling in the bottle2neck.\n        frozen_stages (int): Stages to be frozen (stop grad and set eval mode).\n            -1 means not freezing any parameters.\n        norm_cfg (dict): Dictionary to construct and config norm layer.\n        norm_eval (bool): Whether to set norm layers to eval mode, namely,\n            freeze running stats (mean and var). Note: Effect on Batch Norm\n            and its variants only.\n        plugins (list[dict]): List of plugins for stages, each dict contains:\n\n            - cfg (dict, required): Cfg dict to build plugin.\n            - position (str, required): Position inside block to insert\n              plugin, options are \'after_conv1\', \'after_conv2\', \'after_conv3\'.\n            - stages (tuple[bool], optional): Stages to apply plugin, length\n              should be same as \'num_stages\'.\n        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n            memory while slowing down the training speed.\n        zero_init_residual (bool): Whether to use zero init for last norm layer\n            in resblocks to let them behave as identity.\n\n    Example:\n        >>> from mmdet.models import Res2Net\n        >>> import torch\n        >>> self = Res2Net(depth=50, scales=4, base_width=26)\n        >>> self.eval()\n        >>> inputs = torch.rand(1, 3, 32, 32)\n        >>> level_outputs = self.forward(inputs)\n        >>> for level_out in level_outputs:\n        ...     print(tuple(level_out.shape))\n        (1, 256, 8, 8)\n        (1, 512, 4, 4)\n        (1, 1024, 2, 2)\n        (1, 2048, 1, 1)\n    """"""\n\n    arch_settings = {\n        50: (Bottle2neck, (3, 4, 6, 3)),\n        101: (Bottle2neck, (3, 4, 23, 3)),\n        152: (Bottle2neck, (3, 8, 36, 3))\n    }\n\n    def __init__(self,\n                 scales=4,\n                 base_width=26,\n                 style=\'pytorch\',\n                 deep_stem=True,\n                 avg_down=True,\n                 **kwargs):\n        self.scales = scales\n        self.base_width = base_width\n        super(Res2Net, self).__init__(\n            style=\'pytorch\', deep_stem=True, avg_down=True, **kwargs)\n\n    def make_res_layer(self, **kwargs):\n        return Res2Layer(\n            scales=self.scales,\n            base_width=self.base_width,\n            base_channels=self.base_channels,\n            **kwargs)\n'"
mmdet/models/backbones/resnet.py,4,"b'import torch.nn as nn\nimport torch.utils.checkpoint as cp\nfrom mmcv.cnn import (build_conv_layer, build_norm_layer, constant_init,\n                      kaiming_init)\nfrom mmcv.runner import load_checkpoint\nfrom torch.nn.modules.batchnorm import _BatchNorm\n\nfrom mmdet.ops import build_plugin_layer\nfrom mmdet.utils import get_root_logger\nfrom ..builder import BACKBONES\nfrom ..utils import ResLayer\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 stride=1,\n                 dilation=1,\n                 downsample=None,\n                 style=\'pytorch\',\n                 with_cp=False,\n                 conv_cfg=None,\n                 norm_cfg=dict(type=\'BN\'),\n                 dcn=None,\n                 plugins=None):\n        super(BasicBlock, self).__init__()\n        assert dcn is None, \'Not implemented yet.\'\n        assert plugins is None, \'Not implemented yet.\'\n\n        self.norm1_name, norm1 = build_norm_layer(norm_cfg, planes, postfix=1)\n        self.norm2_name, norm2 = build_norm_layer(norm_cfg, planes, postfix=2)\n\n        self.conv1 = build_conv_layer(\n            conv_cfg,\n            inplanes,\n            planes,\n            3,\n            stride=stride,\n            padding=dilation,\n            dilation=dilation,\n            bias=False)\n        self.add_module(self.norm1_name, norm1)\n        self.conv2 = build_conv_layer(\n            conv_cfg, planes, planes, 3, padding=1, bias=False)\n        self.add_module(self.norm2_name, norm2)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.dilation = dilation\n        assert not with_cp\n\n    @property\n    def norm1(self):\n        return getattr(self, self.norm1_name)\n\n    @property\n    def norm2(self):\n        return getattr(self, self.norm2_name)\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.norm1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.norm2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 stride=1,\n                 dilation=1,\n                 downsample=None,\n                 style=\'pytorch\',\n                 with_cp=False,\n                 conv_cfg=None,\n                 norm_cfg=dict(type=\'BN\'),\n                 dcn=None,\n                 plugins=None):\n        """"""Bottleneck block for ResNet.\n        If style is ""pytorch"", the stride-two layer is the 3x3 conv layer,\n        if it is ""caffe"", the stride-two layer is the first 1x1 conv layer.\n        """"""\n        super(Bottleneck, self).__init__()\n        assert style in [\'pytorch\', \'caffe\']\n        assert dcn is None or isinstance(dcn, dict)\n        assert plugins is None or isinstance(plugins, list)\n        if plugins is not None:\n            allowed_position = [\'after_conv1\', \'after_conv2\', \'after_conv3\']\n            assert all(p[\'position\'] in allowed_position for p in plugins)\n\n        self.inplanes = inplanes\n        self.planes = planes\n        self.stride = stride\n        self.dilation = dilation\n        self.style = style\n        self.with_cp = with_cp\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        self.dcn = dcn\n        self.with_dcn = dcn is not None\n        self.plugins = plugins\n        self.with_plugins = plugins is not None\n\n        if self.with_plugins:\n            # collect plugins for conv1/conv2/conv3\n            self.after_conv1_plugins = [\n                plugin[\'cfg\'] for plugin in plugins\n                if plugin[\'position\'] == \'after_conv1\'\n            ]\n            self.after_conv2_plugins = [\n                plugin[\'cfg\'] for plugin in plugins\n                if plugin[\'position\'] == \'after_conv2\'\n            ]\n            self.after_conv3_plugins = [\n                plugin[\'cfg\'] for plugin in plugins\n                if plugin[\'position\'] == \'after_conv3\'\n            ]\n\n        if self.style == \'pytorch\':\n            self.conv1_stride = 1\n            self.conv2_stride = stride\n        else:\n            self.conv1_stride = stride\n            self.conv2_stride = 1\n\n        self.norm1_name, norm1 = build_norm_layer(norm_cfg, planes, postfix=1)\n        self.norm2_name, norm2 = build_norm_layer(norm_cfg, planes, postfix=2)\n        self.norm3_name, norm3 = build_norm_layer(\n            norm_cfg, planes * self.expansion, postfix=3)\n\n        self.conv1 = build_conv_layer(\n            conv_cfg,\n            inplanes,\n            planes,\n            kernel_size=1,\n            stride=self.conv1_stride,\n            bias=False)\n        self.add_module(self.norm1_name, norm1)\n        fallback_on_stride = False\n        if self.with_dcn:\n            fallback_on_stride = dcn.pop(\'fallback_on_stride\', False)\n        if not self.with_dcn or fallback_on_stride:\n            self.conv2 = build_conv_layer(\n                conv_cfg,\n                planes,\n                planes,\n                kernel_size=3,\n                stride=self.conv2_stride,\n                padding=dilation,\n                dilation=dilation,\n                bias=False)\n        else:\n            assert self.conv_cfg is None, \'conv_cfg must be None for DCN\'\n            self.conv2 = build_conv_layer(\n                dcn,\n                planes,\n                planes,\n                kernel_size=3,\n                stride=self.conv2_stride,\n                padding=dilation,\n                dilation=dilation,\n                bias=False)\n\n        self.add_module(self.norm2_name, norm2)\n        self.conv3 = build_conv_layer(\n            conv_cfg,\n            planes,\n            planes * self.expansion,\n            kernel_size=1,\n            bias=False)\n        self.add_module(self.norm3_name, norm3)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n\n        if self.with_plugins:\n            self.after_conv1_plugin_names = self.make_block_plugins(\n                planes, self.after_conv1_plugins)\n            self.after_conv2_plugin_names = self.make_block_plugins(\n                planes, self.after_conv2_plugins)\n            self.after_conv3_plugin_names = self.make_block_plugins(\n                planes * self.expansion, self.after_conv3_plugins)\n\n    def make_block_plugins(self, in_channels, plugins):\n        """""" make plugins for block\n\n        Args:\n            in_channels (int): Input channels of plugin.\n            plugins (list[dict]): List of plugins cfg to build.\n\n        Returns:\n            list[str]: List of the names of plugin.\n\n        """"""\n        assert isinstance(plugins, list)\n        plugin_names = []\n        for plugin in plugins:\n            plugin = plugin.copy()\n            name, layer = build_plugin_layer(\n                plugin,\n                in_channels=in_channels,\n                postfix=plugin.pop(\'postfix\', \'\'))\n            assert not hasattr(self, name), f\'duplicate plugin {name}\'\n            self.add_module(name, layer)\n            plugin_names.append(name)\n        return plugin_names\n\n    def forward_plugin(self, x, plugin_names):\n        out = x\n        for name in plugin_names:\n            out = getattr(self, name)(x)\n        return out\n\n    @property\n    def norm1(self):\n        return getattr(self, self.norm1_name)\n\n    @property\n    def norm2(self):\n        return getattr(self, self.norm2_name)\n\n    @property\n    def norm3(self):\n        return getattr(self, self.norm3_name)\n\n    def forward(self, x):\n\n        def _inner_forward(x):\n            identity = x\n\n            out = self.conv1(x)\n            out = self.norm1(out)\n            out = self.relu(out)\n\n            if self.with_plugins:\n                out = self.forward_plugin(out, self.after_conv1_plugin_names)\n\n            out = self.conv2(out)\n            out = self.norm2(out)\n            out = self.relu(out)\n\n            if self.with_plugins:\n                out = self.forward_plugin(out, self.after_conv2_plugin_names)\n\n            out = self.conv3(out)\n            out = self.norm3(out)\n\n            if self.with_plugins:\n                out = self.forward_plugin(out, self.after_conv3_plugin_names)\n\n            if self.downsample is not None:\n                identity = self.downsample(x)\n\n            out += identity\n\n            return out\n\n        if self.with_cp and x.requires_grad:\n            out = cp.checkpoint(_inner_forward, x)\n        else:\n            out = _inner_forward(x)\n\n        out = self.relu(out)\n\n        return out\n\n\n@BACKBONES.register_module()\nclass ResNet(nn.Module):\n    """"""ResNet backbone.\n\n    Args:\n        depth (int): Depth of resnet, from {18, 34, 50, 101, 152}.\n        stem_channels (int): Number of stem channels. Default: 64.\n        base_channels (int): Number of base channels of res layer. Default: 64.\n        in_channels (int): Number of input image channels. Default: 3.\n        num_stages (int): Resnet stages. Default: 4.\n        strides (Sequence[int]): Strides of the first block of each stage.\n        dilations (Sequence[int]): Dilation of each stage.\n        out_indices (Sequence[int]): Output from which stages.\n        style (str): `pytorch` or `caffe`. If set to ""pytorch"", the stride-two\n            layer is the 3x3 conv layer, otherwise the stride-two layer is\n            the first 1x1 conv layer.\n        deep_stem (bool): Replace 7x7 conv in input stem with 3 3x3 conv\n        avg_down (bool): Use AvgPool instead of stride conv when\n            downsampling in the bottleneck.\n        frozen_stages (int): Stages to be frozen (stop grad and set eval mode).\n            -1 means not freezing any parameters.\n        norm_cfg (dict): Dictionary to construct and config norm layer.\n        norm_eval (bool): Whether to set norm layers to eval mode, namely,\n            freeze running stats (mean and var). Note: Effect on Batch Norm\n            and its variants only.\n        plugins (list[dict]): List of plugins for stages, each dict contains:\n\n            - cfg (dict, required): Cfg dict to build plugin.\n            - position (str, required): Position inside block to insert\n              plugin, options are \'after_conv1\', \'after_conv2\', \'after_conv3\'.\n            - stages (tuple[bool], optional): Stages to apply plugin, length\n              should be same as \'num_stages\'.\n        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n            memory while slowing down the training speed.\n        zero_init_residual (bool): Whether to use zero init for last norm layer\n            in resblocks to let them behave as identity.\n\n    Example:\n        >>> from mmdet.models import ResNet\n        >>> import torch\n        >>> self = ResNet(depth=18)\n        >>> self.eval()\n        >>> inputs = torch.rand(1, 3, 32, 32)\n        >>> level_outputs = self.forward(inputs)\n        >>> for level_out in level_outputs:\n        ...     print(tuple(level_out.shape))\n        (1, 64, 8, 8)\n        (1, 128, 4, 4)\n        (1, 256, 2, 2)\n        (1, 512, 1, 1)\n    """"""\n\n    arch_settings = {\n        18: (BasicBlock, (2, 2, 2, 2)),\n        34: (BasicBlock, (3, 4, 6, 3)),\n        50: (Bottleneck, (3, 4, 6, 3)),\n        101: (Bottleneck, (3, 4, 23, 3)),\n        152: (Bottleneck, (3, 8, 36, 3))\n    }\n\n    def __init__(self,\n                 depth,\n                 in_channels=3,\n                 stem_channels=64,\n                 base_channels=64,\n                 num_stages=4,\n                 strides=(1, 2, 2, 2),\n                 dilations=(1, 1, 1, 1),\n                 out_indices=(0, 1, 2, 3),\n                 style=\'pytorch\',\n                 deep_stem=False,\n                 avg_down=False,\n                 frozen_stages=-1,\n                 conv_cfg=None,\n                 norm_cfg=dict(type=\'BN\', requires_grad=True),\n                 norm_eval=True,\n                 dcn=None,\n                 stage_with_dcn=(False, False, False, False),\n                 plugins=None,\n                 with_cp=False,\n                 zero_init_residual=True):\n        super(ResNet, self).__init__()\n        if depth not in self.arch_settings:\n            raise KeyError(f\'invalid depth {depth} for resnet\')\n        self.depth = depth\n        self.stem_channels = stem_channels\n        self.base_channels = base_channels\n        self.num_stages = num_stages\n        assert num_stages >= 1 and num_stages <= 4\n        self.strides = strides\n        self.dilations = dilations\n        assert len(strides) == len(dilations) == num_stages\n        self.out_indices = out_indices\n        assert max(out_indices) < num_stages\n        self.style = style\n        self.deep_stem = deep_stem\n        self.avg_down = avg_down\n        self.frozen_stages = frozen_stages\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        self.with_cp = with_cp\n        self.norm_eval = norm_eval\n        self.dcn = dcn\n        self.stage_with_dcn = stage_with_dcn\n        if dcn is not None:\n            assert len(stage_with_dcn) == num_stages\n        self.plugins = plugins\n        self.zero_init_residual = zero_init_residual\n        self.block, stage_blocks = self.arch_settings[depth]\n        self.stage_blocks = stage_blocks[:num_stages]\n        self.inplanes = stem_channels\n\n        self._make_stem_layer(in_channels, stem_channels)\n\n        self.res_layers = []\n        for i, num_blocks in enumerate(self.stage_blocks):\n            stride = strides[i]\n            dilation = dilations[i]\n            dcn = self.dcn if self.stage_with_dcn[i] else None\n            if plugins is not None:\n                stage_plugins = self.make_stage_plugins(plugins, i)\n            else:\n                stage_plugins = None\n            planes = base_channels * 2**i\n            res_layer = self.make_res_layer(\n                block=self.block,\n                inplanes=self.inplanes,\n                planes=planes,\n                num_blocks=num_blocks,\n                stride=stride,\n                dilation=dilation,\n                style=self.style,\n                avg_down=self.avg_down,\n                with_cp=with_cp,\n                conv_cfg=conv_cfg,\n                norm_cfg=norm_cfg,\n                dcn=dcn,\n                plugins=stage_plugins)\n            self.inplanes = planes * self.block.expansion\n            layer_name = f\'layer{i + 1}\'\n            self.add_module(layer_name, res_layer)\n            self.res_layers.append(layer_name)\n\n        self._freeze_stages()\n\n        self.feat_dim = self.block.expansion * base_channels * 2**(\n            len(self.stage_blocks) - 1)\n\n    def make_stage_plugins(self, plugins, stage_idx):\n        """""" make plugins for ResNet \'stage_idx\'th stage .\n\n        Currently we support to insert \'context_block\',\n        \'empirical_attention_block\', \'nonlocal_block\' into the backbone like\n        ResNet/ResNeXt. They could be inserted after conv1/conv2/conv3 of\n        Bottleneck.\n        An example of plugins format could be:\n\n        >>> plugins=[\n        ...     dict(cfg=dict(type=\'xxx\', arg1=\'xxx\'),\n        ...          stages=(False, True, True, True),\n        ...          position=\'after_conv2\'),\n        ...     dict(cfg=dict(type=\'yyy\'),\n        ...          stages=(True, True, True, True),\n        ...          position=\'after_conv3\'),\n        ...     dict(cfg=dict(type=\'zzz\', postfix=\'1\'),\n        ...          stages=(True, True, True, True),\n        ...          position=\'after_conv3\'),\n        ...     dict(cfg=dict(type=\'zzz\', postfix=\'2\'),\n        ...          stages=(True, True, True, True),\n        ...          position=\'after_conv3\')\n        ... ]\n        >>> self = ResNet(depth=18)\n        >>> stage_plugins = self.make_stage_plugins(plugins, 0)\n        >>> assert len(stage_plugins) == 3\n\n        Suppose \'stage_idx=0\', the structure of blocks in the stage would be:\n\n        .. code-block:: none\n\n            conv1-> conv2->conv3->yyy->zzz1->zzz2\n\n        Suppose \'stage_idx=1\', the structure of blocks in the stage would be:\n\n        .. code-block:: none\n\n            conv1-> conv2->xxx->conv3->yyy->zzz1->zzz2\n\n        If stages is missing, the plugin would be applied to all stages.\n\n        Args:\n            plugins (list[dict]): List of plugins cfg to build. The postfix is\n                required if multiple same type plugins are inserted.\n            stage_idx (int): Index of stage to build\n\n        Returns:\n            list[dict]: Plugins for current stage\n        """"""\n        stage_plugins = []\n        for plugin in plugins:\n            plugin = plugin.copy()\n            stages = plugin.pop(\'stages\', None)\n            assert stages is None or len(stages) == self.num_stages\n            # whether to insert plugin into current stage\n            if stages is None or stages[stage_idx]:\n                stage_plugins.append(plugin)\n\n        return stage_plugins\n\n    def make_res_layer(self, **kwargs):\n        return ResLayer(**kwargs)\n\n    @property\n    def norm1(self):\n        return getattr(self, self.norm1_name)\n\n    def _make_stem_layer(self, in_channels, stem_channels):\n        if self.deep_stem:\n            self.stem = nn.Sequential(\n                build_conv_layer(\n                    self.conv_cfg,\n                    in_channels,\n                    stem_channels // 2,\n                    kernel_size=3,\n                    stride=2,\n                    padding=1,\n                    bias=False),\n                build_norm_layer(self.norm_cfg, stem_channels // 2)[1],\n                nn.ReLU(inplace=True),\n                build_conv_layer(\n                    self.conv_cfg,\n                    stem_channels // 2,\n                    stem_channels // 2,\n                    kernel_size=3,\n                    stride=1,\n                    padding=1,\n                    bias=False),\n                build_norm_layer(self.norm_cfg, stem_channels // 2)[1],\n                nn.ReLU(inplace=True),\n                build_conv_layer(\n                    self.conv_cfg,\n                    stem_channels // 2,\n                    stem_channels,\n                    kernel_size=3,\n                    stride=1,\n                    padding=1,\n                    bias=False),\n                build_norm_layer(self.norm_cfg, stem_channels)[1],\n                nn.ReLU(inplace=True))\n        else:\n            self.conv1 = build_conv_layer(\n                self.conv_cfg,\n                in_channels,\n                stem_channels,\n                kernel_size=7,\n                stride=2,\n                padding=3,\n                bias=False)\n            self.norm1_name, norm1 = build_norm_layer(\n                self.norm_cfg, stem_channels, postfix=1)\n            self.add_module(self.norm1_name, norm1)\n            self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n    def _freeze_stages(self):\n        if self.frozen_stages >= 0:\n            if self.deep_stem:\n                self.stem.eval()\n                for param in self.stem.parameters():\n                    param.requires_grad = False\n            else:\n                self.norm1.eval()\n                for m in [self.conv1, self.norm1]:\n                    for param in m.parameters():\n                        param.requires_grad = False\n\n        for i in range(1, self.frozen_stages + 1):\n            m = getattr(self, f\'layer{i}\')\n            m.eval()\n            for param in m.parameters():\n                param.requires_grad = False\n\n    def init_weights(self, pretrained=None):\n        if isinstance(pretrained, str):\n            logger = get_root_logger()\n            load_checkpoint(self, pretrained, strict=False, logger=logger)\n        elif pretrained is None:\n            for m in self.modules():\n                if isinstance(m, nn.Conv2d):\n                    kaiming_init(m)\n                elif isinstance(m, (_BatchNorm, nn.GroupNorm)):\n                    constant_init(m, 1)\n\n            if self.dcn is not None:\n                for m in self.modules():\n                    if isinstance(m, Bottleneck) and hasattr(\n                            m.conv2, \'conv_offset\'):\n                        constant_init(m.conv2.conv_offset, 0)\n\n            if self.zero_init_residual:\n                for m in self.modules():\n                    if isinstance(m, Bottleneck):\n                        constant_init(m.norm3, 0)\n                    elif isinstance(m, BasicBlock):\n                        constant_init(m.norm2, 0)\n        else:\n            raise TypeError(\'pretrained must be a str or None\')\n\n    def forward(self, x):\n        if self.deep_stem:\n            x = self.stem(x)\n        else:\n            x = self.conv1(x)\n            x = self.norm1(x)\n            x = self.relu(x)\n        x = self.maxpool(x)\n        outs = []\n        for i, layer_name in enumerate(self.res_layers):\n            res_layer = getattr(self, layer_name)\n            x = res_layer(x)\n            if i in self.out_indices:\n                outs.append(x)\n        return tuple(outs)\n\n    def train(self, mode=True):\n        super(ResNet, self).train(mode)\n        self._freeze_stages()\n        if mode and self.norm_eval:\n            for m in self.modules():\n                # trick: eval have effect on BatchNorm only\n                if isinstance(m, _BatchNorm):\n                    m.eval()\n\n\n@BACKBONES.register_module()\nclass ResNetV1d(ResNet):\n    """"""ResNetV1d variant described in\n    `Bag of Tricks <https://arxiv.org/pdf/1812.01187.pdf>`_.\n\n    Compared with default ResNet(ResNetV1b), ResNetV1d replaces the 7x7 conv\n    in the input stem with three 3x3 convs. And in the downsampling block,\n    a 2x2 avg_pool with stride 2 is added before conv, whose stride is\n    changed to 1.\n    """"""\n\n    def __init__(self, **kwargs):\n        super(ResNetV1d, self).__init__(\n            deep_stem=True, avg_down=True, **kwargs)\n'"
mmdet/models/backbones/resnext.py,0,"b'import math\n\nfrom mmcv.cnn import build_conv_layer, build_norm_layer\n\nfrom ..builder import BACKBONES\nfrom ..utils import ResLayer\nfrom .resnet import Bottleneck as _Bottleneck\nfrom .resnet import ResNet\n\n\nclass Bottleneck(_Bottleneck):\n    expansion = 4\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 groups=1,\n                 base_width=4,\n                 base_channels=64,\n                 **kwargs):\n        """"""Bottleneck block for ResNeXt.\n        If style is ""pytorch"", the stride-two layer is the 3x3 conv layer,\n        if it is ""caffe"", the stride-two layer is the first 1x1 conv layer.\n        """"""\n        super(Bottleneck, self).__init__(inplanes, planes, **kwargs)\n\n        if groups == 1:\n            width = self.planes\n        else:\n            width = math.floor(self.planes *\n                               (base_width / base_channels)) * groups\n\n        self.norm1_name, norm1 = build_norm_layer(\n            self.norm_cfg, width, postfix=1)\n        self.norm2_name, norm2 = build_norm_layer(\n            self.norm_cfg, width, postfix=2)\n        self.norm3_name, norm3 = build_norm_layer(\n            self.norm_cfg, self.planes * self.expansion, postfix=3)\n\n        self.conv1 = build_conv_layer(\n            self.conv_cfg,\n            self.inplanes,\n            width,\n            kernel_size=1,\n            stride=self.conv1_stride,\n            bias=False)\n        self.add_module(self.norm1_name, norm1)\n        fallback_on_stride = False\n        self.with_modulated_dcn = False\n        if self.with_dcn:\n            fallback_on_stride = self.dcn.pop(\'fallback_on_stride\', False)\n        if not self.with_dcn or fallback_on_stride:\n            self.conv2 = build_conv_layer(\n                self.conv_cfg,\n                width,\n                width,\n                kernel_size=3,\n                stride=self.conv2_stride,\n                padding=self.dilation,\n                dilation=self.dilation,\n                groups=groups,\n                bias=False)\n        else:\n            assert self.conv_cfg is None, \'conv_cfg must be None for DCN\'\n            self.conv2 = build_conv_layer(\n                self.dcn,\n                width,\n                width,\n                kernel_size=3,\n                stride=self.conv2_stride,\n                padding=self.dilation,\n                dilation=self.dilation,\n                groups=groups,\n                bias=False)\n\n        self.add_module(self.norm2_name, norm2)\n        self.conv3 = build_conv_layer(\n            self.conv_cfg,\n            width,\n            self.planes * self.expansion,\n            kernel_size=1,\n            bias=False)\n        self.add_module(self.norm3_name, norm3)\n\n\n@BACKBONES.register_module()\nclass ResNeXt(ResNet):\n    """"""ResNeXt backbone.\n\n    Args:\n        depth (int): Depth of resnet, from {18, 34, 50, 101, 152}.\n        in_channels (int): Number of input image channels. Default: 3.\n        num_stages (int): Resnet stages. Default: 4.\n        groups (int): Group of resnext.\n        base_width (int): Base width of resnext.\n        strides (Sequence[int]): Strides of the first block of each stage.\n        dilations (Sequence[int]): Dilation of each stage.\n        out_indices (Sequence[int]): Output from which stages.\n        style (str): `pytorch` or `caffe`. If set to ""pytorch"", the stride-two\n            layer is the 3x3 conv layer, otherwise the stride-two layer is\n            the first 1x1 conv layer.\n        frozen_stages (int): Stages to be frozen (all param fixed). -1 means\n            not freezing any parameters.\n        norm_cfg (dict): dictionary to construct and config norm layer.\n        norm_eval (bool): Whether to set norm layers to eval mode, namely,\n            freeze running stats (mean and var). Note: Effect on Batch Norm\n            and its variants only.\n        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n            memory while slowing down the training speed.\n        zero_init_residual (bool): whether to use zero init for last norm layer\n            in resblocks to let them behave as identity.\n    """"""\n\n    arch_settings = {\n        50: (Bottleneck, (3, 4, 6, 3)),\n        101: (Bottleneck, (3, 4, 23, 3)),\n        152: (Bottleneck, (3, 8, 36, 3))\n    }\n\n    def __init__(self, groups=1, base_width=4, **kwargs):\n        self.groups = groups\n        self.base_width = base_width\n        super(ResNeXt, self).__init__(**kwargs)\n\n    def make_res_layer(self, **kwargs):\n        return ResLayer(\n            groups=self.groups,\n            base_width=self.base_width,\n            base_channels=self.base_channels,\n            **kwargs)\n'"
mmdet/models/backbones/ssd_vgg.py,4,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import VGG, constant_init, kaiming_init, normal_init, xavier_init\nfrom mmcv.runner import load_checkpoint\n\nfrom mmdet.utils import get_root_logger\nfrom ..builder import BACKBONES\n\n\n@BACKBONES.register_module()\nclass SSDVGG(VGG):\n    """"""VGG Backbone network for single-shot-detection\n\n    Args:\n        input_size (int): width and height of input, from {300, 512}.\n        depth (int): Depth of vgg, from {11, 13, 16, 19}.\n        out_indices (Sequence[int]): Output from which stages.\n\n    Example:\n        >>> self = SSDVGG(input_size=300, depth=11)\n        >>> self.eval()\n        >>> inputs = torch.rand(1, 3, 300, 300)\n        >>> level_outputs = self.forward(inputs)\n        >>> for level_out in level_outputs:\n        ...     print(tuple(level_out.shape))\n        (1, 1024, 19, 19)\n        (1, 512, 10, 10)\n        (1, 256, 5, 5)\n        (1, 256, 3, 3)\n        (1, 256, 1, 1)\n    """"""\n    extra_setting = {\n        300: (256, \'S\', 512, 128, \'S\', 256, 128, 256, 128, 256),\n        512: (256, \'S\', 512, 128, \'S\', 256, 128, \'S\', 256, 128, \'S\', 256, 128),\n    }\n\n    def __init__(self,\n                 input_size,\n                 depth,\n                 with_last_pool=False,\n                 ceil_mode=True,\n                 out_indices=(3, 4),\n                 out_feature_indices=(22, 34),\n                 l2_norm_scale=20.):\n        # TODO: in_channels for mmcv.VGG\n        super(SSDVGG, self).__init__(\n            depth,\n            with_last_pool=with_last_pool,\n            ceil_mode=ceil_mode,\n            out_indices=out_indices)\n        assert input_size in (300, 512)\n        self.input_size = input_size\n\n        self.features.add_module(\n            str(len(self.features)),\n            nn.MaxPool2d(kernel_size=3, stride=1, padding=1))\n        self.features.add_module(\n            str(len(self.features)),\n            nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6))\n        self.features.add_module(\n            str(len(self.features)), nn.ReLU(inplace=True))\n        self.features.add_module(\n            str(len(self.features)), nn.Conv2d(1024, 1024, kernel_size=1))\n        self.features.add_module(\n            str(len(self.features)), nn.ReLU(inplace=True))\n        self.out_feature_indices = out_feature_indices\n\n        self.inplanes = 1024\n        self.extra = self._make_extra_layers(self.extra_setting[input_size])\n        self.l2_norm = L2Norm(\n            self.features[out_feature_indices[0] - 1].out_channels,\n            l2_norm_scale)\n\n    def init_weights(self, pretrained=None):\n        if isinstance(pretrained, str):\n            logger = get_root_logger()\n            load_checkpoint(self, pretrained, strict=False, logger=logger)\n        elif pretrained is None:\n            for m in self.features.modules():\n                if isinstance(m, nn.Conv2d):\n                    kaiming_init(m)\n                elif isinstance(m, nn.BatchNorm2d):\n                    constant_init(m, 1)\n                elif isinstance(m, nn.Linear):\n                    normal_init(m, std=0.01)\n        else:\n            raise TypeError(\'pretrained must be a str or None\')\n\n        for m in self.extra.modules():\n            if isinstance(m, nn.Conv2d):\n                xavier_init(m, distribution=\'uniform\')\n\n        constant_init(self.l2_norm, self.l2_norm.scale)\n\n    def forward(self, x):\n        outs = []\n        for i, layer in enumerate(self.features):\n            x = layer(x)\n            if i in self.out_feature_indices:\n                outs.append(x)\n        for i, layer in enumerate(self.extra):\n            x = F.relu(layer(x), inplace=True)\n            if i % 2 == 1:\n                outs.append(x)\n        outs[0] = self.l2_norm(outs[0])\n        if len(outs) == 1:\n            return outs[0]\n        else:\n            return tuple(outs)\n\n    def _make_extra_layers(self, outplanes):\n        layers = []\n        kernel_sizes = (1, 3)\n        num_layers = 0\n        outplane = None\n        for i in range(len(outplanes)):\n            if self.inplanes == \'S\':\n                self.inplanes = outplane\n                continue\n            k = kernel_sizes[num_layers % 2]\n            if outplanes[i] == \'S\':\n                outplane = outplanes[i + 1]\n                conv = nn.Conv2d(\n                    self.inplanes, outplane, k, stride=2, padding=1)\n            else:\n                outplane = outplanes[i]\n                conv = nn.Conv2d(\n                    self.inplanes, outplane, k, stride=1, padding=0)\n            layers.append(conv)\n            self.inplanes = outplanes[i]\n            num_layers += 1\n        if self.input_size == 512:\n            layers.append(nn.Conv2d(self.inplanes, 256, 4, padding=1))\n\n        return nn.Sequential(*layers)\n\n\nclass L2Norm(nn.Module):\n\n    def __init__(self, n_dims, scale=20., eps=1e-10):\n        super(L2Norm, self).__init__()\n        self.n_dims = n_dims\n        self.weight = nn.Parameter(torch.Tensor(self.n_dims))\n        self.eps = eps\n        self.scale = scale\n\n    def forward(self, x):\n        # normalization layer convert to FP32 in FP16 training\n        x_float = x.float()\n        norm = x_float.pow(2).sum(1, keepdim=True).sqrt() + self.eps\n        return (self.weight[None, :, None, None].float().expand_as(x_float) *\n                x_float / norm).type_as(x)\n'"
mmdet/models/dense_heads/__init__.py,0,"b""from .anchor_head import AnchorHead\nfrom .atss_head import ATSSHead\nfrom .fcos_head import FCOSHead\nfrom .fovea_head import FoveaHead\nfrom .free_anchor_retina_head import FreeAnchorRetinaHead\nfrom .fsaf_head import FSAFHead\nfrom .ga_retina_head import GARetinaHead\nfrom .ga_rpn_head import GARPNHead\nfrom .guided_anchor_head import FeatureAdaption, GuidedAnchorHead\nfrom .nasfcos_head import NASFCOSHead\nfrom .pisa_retinanet_head import PISARetinaHead\nfrom .pisa_ssd_head import PISASSDHead\nfrom .reppoints_head import RepPointsHead\nfrom .retina_head import RetinaHead\nfrom .retina_sepbn_head import RetinaSepBNHead\nfrom .rpn_head import RPNHead\nfrom .ssd_head import SSDHead\n\n__all__ = [\n    'AnchorHead', 'GuidedAnchorHead', 'FeatureAdaption', 'RPNHead',\n    'GARPNHead', 'RetinaHead', 'RetinaSepBNHead', 'GARetinaHead', 'SSDHead',\n    'FCOSHead', 'RepPointsHead', 'FoveaHead', 'FreeAnchorRetinaHead',\n    'ATSSHead', 'FSAFHead', 'NASFCOSHead', 'PISARetinaHead', 'PISASSDHead'\n]\n"""
mmdet/models/dense_heads/anchor_head.py,13,"b'import torch\nimport torch.nn as nn\nfrom mmcv.cnn import normal_init\n\nfrom mmdet.core import (anchor_inside_flags, build_anchor_generator,\n                        build_assigner, build_bbox_coder, build_sampler,\n                        force_fp32, images_to_levels, multi_apply,\n                        multiclass_nms, unmap)\nfrom ..builder import HEADS, build_loss\n\n\n@HEADS.register_module()\nclass AnchorHead(nn.Module):\n    """"""Anchor-based head (RPN, RetinaNet, SSD, etc.).\n\n    Args:\n        num_classes (int): Number of categories excluding the background\n            category.\n        in_channels (int): Number of channels in the input feature map.\n        feat_channels (int): Number of hidden channels. Used in child classes.\n        anchor_generator (dict): Config dict for anchor generator\n        bbox_coder (dict): Config of bounding box coder.\n        reg_decoded_bbox (bool): If true, the regression loss would be\n            applied on decoded bounding boxes. Default: False\n        background_label (int | None): Label ID of background, set as 0 for\n            RPN and num_classes for other heads. It will automatically set as\n            num_classes if None is given.\n        loss_cls (dict): Config of classification loss.\n        loss_bbox (dict): Config of localization loss.\n        train_cfg (dict): Training config of anchor head.\n        test_cfg (dict): Testing config of anchor head.\n    """"""  # noqa: W605\n\n    def __init__(self,\n                 num_classes,\n                 in_channels,\n                 feat_channels=256,\n                 anchor_generator=dict(\n                     type=\'AnchorGenerator\',\n                     scales=[8, 16, 32],\n                     ratios=[0.5, 1.0, 2.0],\n                     strides=[4, 8, 16, 32, 64]),\n                 bbox_coder=dict(\n                     type=\'DeltaXYWHBBoxCoder\',\n                     target_means=(.0, .0, .0, .0),\n                     target_stds=(1.0, 1.0, 1.0, 1.0)),\n                 reg_decoded_bbox=False,\n                 background_label=None,\n                 loss_cls=dict(\n                     type=\'CrossEntropyLoss\',\n                     use_sigmoid=True,\n                     loss_weight=1.0),\n                 loss_bbox=dict(\n                     type=\'SmoothL1Loss\', beta=1.0 / 9.0, loss_weight=1.0),\n                 train_cfg=None,\n                 test_cfg=None):\n        super(AnchorHead, self).__init__()\n        self.in_channels = in_channels\n        self.num_classes = num_classes\n        self.feat_channels = feat_channels\n        self.use_sigmoid_cls = loss_cls.get(\'use_sigmoid\', False)\n        # TODO better way to determine whether sample or not\n        self.sampling = loss_cls[\'type\'] not in [\'FocalLoss\', \'GHMC\']\n        if self.use_sigmoid_cls:\n            self.cls_out_channels = num_classes\n        else:\n            self.cls_out_channels = num_classes + 1\n\n        if self.cls_out_channels <= 0:\n            raise ValueError(f\'num_classes={num_classes} is too small\')\n        self.reg_decoded_bbox = reg_decoded_bbox\n\n        self.background_label = (\n            num_classes if background_label is None else background_label)\n        # background_label should be either 0 or num_classes\n        assert (self.background_label == 0\n                or self.background_label == num_classes)\n\n        self.bbox_coder = build_bbox_coder(bbox_coder)\n        self.loss_cls = build_loss(loss_cls)\n        self.loss_bbox = build_loss(loss_bbox)\n        self.train_cfg = train_cfg\n        self.test_cfg = test_cfg\n        if self.train_cfg:\n            self.assigner = build_assigner(self.train_cfg.assigner)\n            # use PseudoSampler when sampling is False\n            if self.sampling and hasattr(self.train_cfg, \'sampler\'):\n                sampler_cfg = self.train_cfg.sampler\n            else:\n                sampler_cfg = dict(type=\'PseudoSampler\')\n            self.sampler = build_sampler(sampler_cfg, context=self)\n        self.fp16_enabled = False\n\n        self.anchor_generator = build_anchor_generator(anchor_generator)\n        # usually the numbers of anchors for each level are the same\n        # except SSD detectors\n        self.num_anchors = self.anchor_generator.num_base_anchors[0]\n        self._init_layers()\n\n    def _init_layers(self):\n        self.conv_cls = nn.Conv2d(self.in_channels,\n                                  self.num_anchors * self.cls_out_channels, 1)\n        self.conv_reg = nn.Conv2d(self.in_channels, self.num_anchors * 4, 1)\n\n    def init_weights(self):\n        normal_init(self.conv_cls, std=0.01)\n        normal_init(self.conv_reg, std=0.01)\n\n    def forward_single(self, x):\n        cls_score = self.conv_cls(x)\n        bbox_pred = self.conv_reg(x)\n        return cls_score, bbox_pred\n\n    def forward(self, feats):\n        return multi_apply(self.forward_single, feats)\n\n    def get_anchors(self, featmap_sizes, img_metas, device=\'cuda\'):\n        """"""Get anchors according to feature map sizes.\n\n        Args:\n            featmap_sizes (list[tuple]): Multi-level feature map sizes.\n            img_metas (list[dict]): Image meta info.\n            device (torch.device | str): Device for returned tensors\n\n        Returns:\n            tuple:\n                anchor_list (list[Tensor]): Anchors of each image\n                valid_flag_list (list[Tensor]): Valid flags of each image\n        """"""\n        num_imgs = len(img_metas)\n\n        # since feature map sizes of all images are the same, we only compute\n        # anchors for one time\n        multi_level_anchors = self.anchor_generator.grid_anchors(\n            featmap_sizes, device)\n        anchor_list = [multi_level_anchors for _ in range(num_imgs)]\n\n        # for each image, we compute valid flags of multi level anchors\n        valid_flag_list = []\n        for img_id, img_meta in enumerate(img_metas):\n            multi_level_flags = self.anchor_generator.valid_flags(\n                featmap_sizes, img_meta[\'pad_shape\'], device)\n            valid_flag_list.append(multi_level_flags)\n\n        return anchor_list, valid_flag_list\n\n    def _get_targets_single(self,\n                            flat_anchors,\n                            valid_flags,\n                            gt_bboxes,\n                            gt_bboxes_ignore,\n                            gt_labels,\n                            img_meta,\n                            label_channels=1,\n                            unmap_outputs=True):\n        """"""Compute regression and classification targets for anchors in\n            a single image.\n\n        Args:\n            flat_anchors (Tensor): Multi-level anchors of the image, which are\n                concatenated into a single tensor of shape (num_anchors ,4)\n            valid_flags (Tensor): Multi level valid flags of the image,\n                which are concatenated into a single tensor of\n                    shape (num_anchors,).\n            gt_bboxes (Tensor): Ground truth bboxes of the image,\n                shape (num_gts, 4).\n            img_meta (dict): Meta info of the image.\n            gt_bboxes_ignore (Tensor): Ground truth bboxes to be\n                ignored, shape (num_ignored_gts, 4).\n            img_meta (dict): Meta info of the image.\n            gt_labels (Tensor): Ground truth labels of each box,\n                shape (num_gts,).\n            label_channels (int): Channel of label.\n            unmap_outputs (bool): Whether to map outputs back to the original\n                set of anchors.\n\n        Returns:\n            tuple:\n                labels_list (list[Tensor]): Labels of each level\n                label_weights_list (list[Tensor]): Label weights of each level\n                bbox_targets_list (list[Tensor]): BBox targets of each level\n                bbox_weights_list (list[Tensor]): BBox weights of each level\n                num_total_pos (int): Number of positive samples in all images\n                num_total_neg (int): Number of negative samples in all images\n        """"""\n        inside_flags = anchor_inside_flags(flat_anchors, valid_flags,\n                                           img_meta[\'img_shape\'][:2],\n                                           self.train_cfg.allowed_border)\n        if not inside_flags.any():\n            return (None, ) * 6\n        # assign gt and sample anchors\n        anchors = flat_anchors[inside_flags, :]\n\n        assign_result = self.assigner.assign(\n            anchors, gt_bboxes, gt_bboxes_ignore,\n            None if self.sampling else gt_labels)\n        sampling_result = self.sampler.sample(assign_result, anchors,\n                                              gt_bboxes)\n\n        num_valid_anchors = anchors.shape[0]\n        bbox_targets = torch.zeros_like(anchors)\n        bbox_weights = torch.zeros_like(anchors)\n        labels = anchors.new_full((num_valid_anchors, ),\n                                  self.background_label,\n                                  dtype=torch.long)\n        label_weights = anchors.new_zeros(num_valid_anchors, dtype=torch.float)\n\n        pos_inds = sampling_result.pos_inds\n        neg_inds = sampling_result.neg_inds\n        if len(pos_inds) > 0:\n            if not self.reg_decoded_bbox:\n                pos_bbox_targets = self.bbox_coder.encode(\n                    sampling_result.pos_bboxes, sampling_result.pos_gt_bboxes)\n            else:\n                pos_bbox_targets = sampling_result.pos_gt_bboxes\n            bbox_targets[pos_inds, :] = pos_bbox_targets\n            bbox_weights[pos_inds, :] = 1.0\n            if gt_labels is None:\n                # only rpn gives gt_labels as None, this time FG is 1\n                labels[pos_inds] = 1\n            else:\n                labels[pos_inds] = gt_labels[\n                    sampling_result.pos_assigned_gt_inds]\n            if self.train_cfg.pos_weight <= 0:\n                label_weights[pos_inds] = 1.0\n            else:\n                label_weights[pos_inds] = self.train_cfg.pos_weight\n        if len(neg_inds) > 0:\n            label_weights[neg_inds] = 1.0\n\n        # map up to original set of anchors\n        if unmap_outputs:\n            num_total_anchors = flat_anchors.size(0)\n            labels = unmap(\n                labels, num_total_anchors, inside_flags,\n                fill=self.num_classes)  # fill bg label\n            label_weights = unmap(label_weights, num_total_anchors,\n                                  inside_flags)\n            bbox_targets = unmap(bbox_targets, num_total_anchors, inside_flags)\n            bbox_weights = unmap(bbox_weights, num_total_anchors, inside_flags)\n\n        return (labels, label_weights, bbox_targets, bbox_weights, pos_inds,\n                neg_inds, sampling_result)\n\n    def get_targets(self,\n                    anchor_list,\n                    valid_flag_list,\n                    gt_bboxes_list,\n                    img_metas,\n                    gt_bboxes_ignore_list=None,\n                    gt_labels_list=None,\n                    label_channels=1,\n                    unmap_outputs=True,\n                    return_sampling_results=False):\n        """"""Compute regression and classification targets for anchors in\n            multiple images.\n\n        Args:\n            anchor_list (list[list[Tensor]]): Multi level anchors of each\n                image. The outer list indicates images, and the inner list\n                corresponds to feature levels of the image. Each element of\n                the inner list is a tensor of shape (num_anchors, 4).\n            valid_flag_list (list[list[Tensor]]): Multi level valid flags of\n                each image. The outer list indicates images, and the inner list\n                corresponds to feature levels of the image. Each element of\n                the inner list is a tensor of shape (num_anchors, )\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes of each image.\n            img_metas (list[dict]): Meta info of each image.\n            gt_bboxes_ignore_list (list[Tensor]): Ground truth bboxes to be\n                ignored.\n            gt_labels_list (list[Tensor]): Ground truth labels of each box.\n            label_channels (int): Channel of label.\n            unmap_outputs (bool): Whether to map outputs back to the original\n                set of anchors.\n\n        Returns:\n            tuple:\n                labels_list (list[Tensor]): Labels of each level\n                label_weights_list (list[Tensor]): Label weights of each level\n                bbox_targets_list (list[Tensor]): BBox targets of each level\n                bbox_weights_list (list[Tensor]): BBox weights of each level\n                num_total_pos (int): Number of positive samples in all images\n                num_total_neg (int): Number of negative samples in all images\n            additional_returns: This function enables user-defined returns from\n                `self._get_targets_single`. These returns are currently refined\n                to properties at each feature map (i.e. having HxW dimension).\n                The results will be concatenated after the end\n\n        """"""\n        num_imgs = len(img_metas)\n        assert len(anchor_list) == len(valid_flag_list) == num_imgs\n\n        # anchor number of multi levels\n        num_level_anchors = [anchors.size(0) for anchors in anchor_list[0]]\n        # concat all level anchors to a single tensor\n        concat_anchor_list = []\n        concat_valid_flag_list = []\n        for i in range(num_imgs):\n            assert len(anchor_list[i]) == len(valid_flag_list[i])\n            concat_anchor_list.append(torch.cat(anchor_list[i]))\n            concat_valid_flag_list.append(torch.cat(valid_flag_list[i]))\n\n        # compute targets for each image\n        if gt_bboxes_ignore_list is None:\n            gt_bboxes_ignore_list = [None for _ in range(num_imgs)]\n        if gt_labels_list is None:\n            gt_labels_list = [None for _ in range(num_imgs)]\n        results = multi_apply(\n            self._get_targets_single,\n            concat_anchor_list,\n            concat_valid_flag_list,\n            gt_bboxes_list,\n            gt_bboxes_ignore_list,\n            gt_labels_list,\n            img_metas,\n            label_channels=label_channels,\n            unmap_outputs=unmap_outputs)\n        (all_labels, all_label_weights, all_bbox_targets, all_bbox_weights,\n         pos_inds_list, neg_inds_list, sampling_results_list) = results[:7]\n        rest_results = list(results[7:])  # user-added return values\n        # no valid anchors\n        if any([labels is None for labels in all_labels]):\n            return None\n        # sampled anchors of all images\n        num_total_pos = sum([max(inds.numel(), 1) for inds in pos_inds_list])\n        num_total_neg = sum([max(inds.numel(), 1) for inds in neg_inds_list])\n        # split targets to a list w.r.t. multiple levels\n        labels_list = images_to_levels(all_labels, num_level_anchors)\n        label_weights_list = images_to_levels(all_label_weights,\n                                              num_level_anchors)\n        bbox_targets_list = images_to_levels(all_bbox_targets,\n                                             num_level_anchors)\n        bbox_weights_list = images_to_levels(all_bbox_weights,\n                                             num_level_anchors)\n        res = (labels_list, label_weights_list, bbox_targets_list,\n               bbox_weights_list, num_total_pos, num_total_neg)\n        if return_sampling_results:\n            res = res + (sampling_results_list, )\n        for i, r in enumerate(rest_results):  # user-added return values\n            rest_results[i] = images_to_levels(r, num_level_anchors)\n\n        return res + tuple(rest_results)\n\n    def loss_single(self, cls_score, bbox_pred, anchors, labels, label_weights,\n                    bbox_targets, bbox_weights, num_total_samples):\n        # classification loss\n        labels = labels.reshape(-1)\n        label_weights = label_weights.reshape(-1)\n        cls_score = cls_score.permute(0, 2, 3,\n                                      1).reshape(-1, self.cls_out_channels)\n        loss_cls = self.loss_cls(\n            cls_score, labels, label_weights, avg_factor=num_total_samples)\n        # regression loss\n        bbox_targets = bbox_targets.reshape(-1, 4)\n        bbox_weights = bbox_weights.reshape(-1, 4)\n        bbox_pred = bbox_pred.permute(0, 2, 3, 1).reshape(-1, 4)\n        if self.reg_decoded_bbox:\n            anchors = anchors.reshape(-1, 4)\n            bbox_pred = self.bbox_coder.decode(anchors, bbox_pred)\n        loss_bbox = self.loss_bbox(\n            bbox_pred,\n            bbox_targets,\n            bbox_weights,\n            avg_factor=num_total_samples)\n        return loss_cls, loss_bbox\n\n    @force_fp32(apply_to=(\'cls_scores\', \'bbox_preds\'))\n    def loss(self,\n             cls_scores,\n             bbox_preds,\n             gt_bboxes,\n             gt_labels,\n             img_metas,\n             gt_bboxes_ignore=None):\n        featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n        assert len(featmap_sizes) == self.anchor_generator.num_levels\n\n        device = cls_scores[0].device\n\n        anchor_list, valid_flag_list = self.get_anchors(\n            featmap_sizes, img_metas, device=device)\n        label_channels = self.cls_out_channels if self.use_sigmoid_cls else 1\n        cls_reg_targets = self.get_targets(\n            anchor_list,\n            valid_flag_list,\n            gt_bboxes,\n            img_metas,\n            gt_bboxes_ignore_list=gt_bboxes_ignore,\n            gt_labels_list=gt_labels,\n            label_channels=label_channels)\n        if cls_reg_targets is None:\n            return None\n        (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list,\n         num_total_pos, num_total_neg) = cls_reg_targets\n        num_total_samples = (\n            num_total_pos + num_total_neg if self.sampling else num_total_pos)\n\n        # anchor number of multi levels\n        num_level_anchors = [anchors.size(0) for anchors in anchor_list[0]]\n        # concat all level anchors and flags to a single tensor\n        concat_anchor_list = []\n        for i in range(len(anchor_list)):\n            concat_anchor_list.append(torch.cat(anchor_list[i]))\n        all_anchor_list = images_to_levels(concat_anchor_list,\n                                           num_level_anchors)\n\n        losses_cls, losses_bbox = multi_apply(\n            self.loss_single,\n            cls_scores,\n            bbox_preds,\n            all_anchor_list,\n            labels_list,\n            label_weights_list,\n            bbox_targets_list,\n            bbox_weights_list,\n            num_total_samples=num_total_samples)\n        return dict(loss_cls=losses_cls, loss_bbox=losses_bbox)\n\n    @force_fp32(apply_to=(\'cls_scores\', \'bbox_preds\'))\n    def get_bboxes(self,\n                   cls_scores,\n                   bbox_preds,\n                   img_metas,\n                   cfg=None,\n                   rescale=False):\n        """"""\n        Transform network output for a batch into labeled boxes.\n\n        Args:\n            cls_scores (list[Tensor]): Box scores for each scale level\n                Has shape (N, num_anchors * num_classes, H, W)\n            bbox_preds (list[Tensor]): Box energies / deltas for each scale\n                level with shape (N, num_anchors * 4, H, W)\n            img_metas (list[dict]): Size / scale info for each image\n            cfg (mmcv.Config): Test / postprocessing configuration,\n                if None, test_cfg would be used\n            rescale (bool): If True, return boxes in original image space\n\n        Returns:\n            list[tuple[Tensor, Tensor]]: Each item in result_list is 2-tuple.\n                The first item is an (n, 5) tensor, where the first 4 columns\n                are bounding box positions (tl_x, tl_y, br_x, br_y) and the\n                5-th column is a score between 0 and 1. The second item is a\n                (n,) tensor where each item is the class index of the\n                corresponding box.\n\n        Example:\n            >>> import mmcv\n            >>> self = AnchorHead(\n            >>>     num_classes=9,\n            >>>     in_channels=1,\n            >>>     anchor_generator=dict(\n            >>>         type=\'AnchorGenerator\',\n            >>>         scales=[8],\n            >>>         ratios=[0.5, 1.0, 2.0],\n            >>>         strides=[4,]))\n            >>> img_metas = [{\'img_shape\': (32, 32, 3), \'scale_factor\': 1}]\n            >>> cfg = mmcv.Config(dict(\n            >>>     score_thr=0.00,\n            >>>     nms=dict(type=\'nms\', iou_thr=1.0),\n            >>>     max_per_img=10))\n            >>> feat = torch.rand(1, 1, 3, 3)\n            >>> cls_score, bbox_pred = self.forward_single(feat)\n            >>> # note the input lists are over different levels, not images\n            >>> cls_scores, bbox_preds = [cls_score], [bbox_pred]\n            >>> result_list = self.get_bboxes(cls_scores, bbox_preds,\n            >>>                               img_metas, cfg)\n            >>> det_bboxes, det_labels = result_list[0]\n            >>> assert len(result_list) == 1\n            >>> assert det_bboxes.shape[1] == 5\n            >>> assert len(det_bboxes) == len(det_labels) == cfg.max_per_img\n        """"""\n        assert len(cls_scores) == len(bbox_preds)\n        num_levels = len(cls_scores)\n\n        device = cls_scores[0].device\n        featmap_sizes = [cls_scores[i].shape[-2:] for i in range(num_levels)]\n        mlvl_anchors = self.anchor_generator.grid_anchors(\n            featmap_sizes, device=device)\n\n        result_list = []\n        for img_id in range(len(img_metas)):\n            cls_score_list = [\n                cls_scores[i][img_id].detach() for i in range(num_levels)\n            ]\n            bbox_pred_list = [\n                bbox_preds[i][img_id].detach() for i in range(num_levels)\n            ]\n            img_shape = img_metas[img_id][\'img_shape\']\n            scale_factor = img_metas[img_id][\'scale_factor\']\n            proposals = self._get_bboxes_single(cls_score_list, bbox_pred_list,\n                                                mlvl_anchors, img_shape,\n                                                scale_factor, cfg, rescale)\n            result_list.append(proposals)\n        return result_list\n\n    def _get_bboxes_single(self,\n                           cls_score_list,\n                           bbox_pred_list,\n                           mlvl_anchors,\n                           img_shape,\n                           scale_factor,\n                           cfg,\n                           rescale=False):\n        """"""\n        Transform outputs for a single batch item into labeled boxes.\n        """"""\n        cfg = self.test_cfg if cfg is None else cfg\n        assert len(cls_score_list) == len(bbox_pred_list) == len(mlvl_anchors)\n        mlvl_bboxes = []\n        mlvl_scores = []\n        for cls_score, bbox_pred, anchors in zip(cls_score_list,\n                                                 bbox_pred_list, mlvl_anchors):\n            assert cls_score.size()[-2:] == bbox_pred.size()[-2:]\n            cls_score = cls_score.permute(1, 2,\n                                          0).reshape(-1, self.cls_out_channels)\n            if self.use_sigmoid_cls:\n                scores = cls_score.sigmoid()\n            else:\n                scores = cls_score.softmax(-1)\n            bbox_pred = bbox_pred.permute(1, 2, 0).reshape(-1, 4)\n            nms_pre = cfg.get(\'nms_pre\', -1)\n            if nms_pre > 0 and scores.shape[0] > nms_pre:\n                # Get maximum scores for foreground classes.\n                if self.use_sigmoid_cls:\n                    max_scores, _ = scores.max(dim=1)\n                else:\n                    # remind that we set FG labels to [0, num_class-1]\n                    # since mmdet v2.0\n                    # BG cat_id: num_class\n                    max_scores, _ = scores[:, :-1].max(dim=1)\n                _, topk_inds = max_scores.topk(nms_pre)\n                anchors = anchors[topk_inds, :]\n                bbox_pred = bbox_pred[topk_inds, :]\n                scores = scores[topk_inds, :]\n            bboxes = self.bbox_coder.decode(\n                anchors, bbox_pred, max_shape=img_shape)\n            mlvl_bboxes.append(bboxes)\n            mlvl_scores.append(scores)\n        mlvl_bboxes = torch.cat(mlvl_bboxes)\n        if rescale:\n            mlvl_bboxes /= mlvl_bboxes.new_tensor(scale_factor)\n        mlvl_scores = torch.cat(mlvl_scores)\n        if self.use_sigmoid_cls:\n            # Add a dummy background class to the backend when using sigmoid\n            # remind that we set FG labels to [0, num_class-1] since mmdet v2.0\n            # BG cat_id: num_class\n            padding = mlvl_scores.new_zeros(mlvl_scores.shape[0], 1)\n            mlvl_scores = torch.cat([mlvl_scores, padding], dim=1)\n        det_bboxes, det_labels = multiclass_nms(mlvl_bboxes, mlvl_scores,\n                                                cfg.score_thr, cfg.nms,\n                                                cfg.max_per_img)\n        return det_bboxes, det_labels\n'"
mmdet/models/dense_heads/atss_head.py,19,"b'import torch\nimport torch.distributed as dist\nimport torch.nn as nn\nfrom mmcv.cnn import ConvModule, Scale, bias_init_with_prob, normal_init\n\nfrom mmdet.core import (anchor_inside_flags, build_assigner, build_sampler,\n                        force_fp32, images_to_levels, multi_apply,\n                        multiclass_nms, unmap)\nfrom ..builder import HEADS, build_loss\nfrom .anchor_head import AnchorHead\n\n\ndef reduce_mean(tensor):\n    if not (dist.is_available() and dist.is_initialized()):\n        return tensor\n    tensor = tensor.clone()\n    dist.all_reduce(tensor.div_(dist.get_world_size()), op=dist.ReduceOp.SUM)\n    return tensor\n\n\n@HEADS.register_module()\nclass ATSSHead(AnchorHead):\n    """"""\n    Bridging the Gap Between Anchor-based and Anchor-free Detection via\n    Adaptive Training Sample Selection\n\n    ATSS head structure is similar with FCOS, however ATSS use anchor boxes\n    and assign label by Adaptive Training Sample Selection instead max-iou.\n\n    https://arxiv.org/abs/1912.02424\n    """"""\n\n    def __init__(self,\n                 num_classes,\n                 in_channels,\n                 stacked_convs=4,\n                 conv_cfg=None,\n                 norm_cfg=dict(type=\'GN\', num_groups=32, requires_grad=True),\n                 loss_centerness=dict(\n                     type=\'CrossEntropyLoss\',\n                     use_sigmoid=True,\n                     loss_weight=1.0),\n                 **kwargs):\n        self.stacked_convs = stacked_convs\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        super(ATSSHead, self).__init__(num_classes, in_channels, **kwargs)\n\n        self.sampling = False\n        if self.train_cfg:\n            self.assigner = build_assigner(self.train_cfg.assigner)\n            # SSD sampling=False so use PseudoSampler\n            sampler_cfg = dict(type=\'PseudoSampler\')\n            self.sampler = build_sampler(sampler_cfg, context=self)\n        self.loss_centerness = build_loss(loss_centerness)\n\n    def _init_layers(self):\n        self.relu = nn.ReLU(inplace=True)\n        self.cls_convs = nn.ModuleList()\n        self.reg_convs = nn.ModuleList()\n        for i in range(self.stacked_convs):\n            chn = self.in_channels if i == 0 else self.feat_channels\n            self.cls_convs.append(\n                ConvModule(\n                    chn,\n                    self.feat_channels,\n                    3,\n                    stride=1,\n                    padding=1,\n                    conv_cfg=self.conv_cfg,\n                    norm_cfg=self.norm_cfg))\n            self.reg_convs.append(\n                ConvModule(\n                    chn,\n                    self.feat_channels,\n                    3,\n                    stride=1,\n                    padding=1,\n                    conv_cfg=self.conv_cfg,\n                    norm_cfg=self.norm_cfg))\n        self.atss_cls = nn.Conv2d(\n            self.feat_channels,\n            self.num_anchors * self.cls_out_channels,\n            3,\n            padding=1)\n        self.atss_reg = nn.Conv2d(\n            self.feat_channels, self.num_anchors * 4, 3, padding=1)\n        self.atss_centerness = nn.Conv2d(\n            self.feat_channels, self.num_anchors * 1, 3, padding=1)\n        self.scales = nn.ModuleList(\n            [Scale(1.0) for _ in self.anchor_generator.strides])\n\n    def init_weights(self):\n        for m in self.cls_convs:\n            normal_init(m.conv, std=0.01)\n        for m in self.reg_convs:\n            normal_init(m.conv, std=0.01)\n        bias_cls = bias_init_with_prob(0.01)\n        normal_init(self.atss_cls, std=0.01, bias=bias_cls)\n        normal_init(self.atss_reg, std=0.01)\n        normal_init(self.atss_centerness, std=0.01)\n\n    def forward(self, feats):\n        return multi_apply(self.forward_single, feats, self.scales)\n\n    def forward_single(self, x, scale):\n        cls_feat = x\n        reg_feat = x\n        for cls_conv in self.cls_convs:\n            cls_feat = cls_conv(cls_feat)\n        for reg_conv in self.reg_convs:\n            reg_feat = reg_conv(reg_feat)\n        cls_score = self.atss_cls(cls_feat)\n        # we just follow atss, not apply exp in bbox_pred\n        bbox_pred = scale(self.atss_reg(reg_feat)).float()\n        centerness = self.atss_centerness(reg_feat)\n        return cls_score, bbox_pred, centerness\n\n    def loss_single(self, anchors, cls_score, bbox_pred, centerness, labels,\n                    label_weights, bbox_targets, num_total_samples):\n\n        anchors = anchors.reshape(-1, 4)\n        cls_score = cls_score.permute(0, 2, 3,\n                                      1).reshape(-1, self.cls_out_channels)\n        bbox_pred = bbox_pred.permute(0, 2, 3, 1).reshape(-1, 4)\n        centerness = centerness.permute(0, 2, 3, 1).reshape(-1)\n        bbox_targets = bbox_targets.reshape(-1, 4)\n        labels = labels.reshape(-1)\n        label_weights = label_weights.reshape(-1)\n\n        # classification loss\n        loss_cls = self.loss_cls(\n            cls_score, labels, label_weights, avg_factor=num_total_samples)\n\n        # FG cat_id: [0, num_classes -1], BG cat_id: num_classes\n        bg_class_ind = self.num_classes\n        pos_inds = ((labels >= 0)\n                    & (labels < bg_class_ind)).nonzero().squeeze(1)\n\n        if len(pos_inds) > 0:\n            pos_bbox_targets = bbox_targets[pos_inds]\n            pos_bbox_pred = bbox_pred[pos_inds]\n            pos_anchors = anchors[pos_inds]\n            pos_centerness = centerness[pos_inds]\n\n            centerness_targets = self.centerness_target(\n                pos_anchors, pos_bbox_targets)\n            pos_decode_bbox_pred = self.bbox_coder.decode(\n                pos_anchors, pos_bbox_pred)\n            pos_decode_bbox_targets = self.bbox_coder.decode(\n                pos_anchors, pos_bbox_targets)\n\n            # regression loss\n            loss_bbox = self.loss_bbox(\n                pos_decode_bbox_pred,\n                pos_decode_bbox_targets,\n                weight=centerness_targets,\n                avg_factor=1.0)\n\n            # centerness loss\n            loss_centerness = self.loss_centerness(\n                pos_centerness,\n                centerness_targets,\n                avg_factor=num_total_samples)\n\n        else:\n            loss_bbox = bbox_pred.sum() * 0\n            loss_centerness = centerness.sum() * 0\n            centerness_targets = torch.tensor(0).cuda()\n\n        return loss_cls, loss_bbox, loss_centerness, centerness_targets.sum()\n\n    @force_fp32(apply_to=(\'cls_scores\', \'bbox_preds\', \'centernesses\'))\n    def loss(self,\n             cls_scores,\n             bbox_preds,\n             centernesses,\n             gt_bboxes,\n             gt_labels,\n             img_metas,\n             gt_bboxes_ignore=None):\n\n        featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n        assert len(featmap_sizes) == self.anchor_generator.num_levels\n\n        device = cls_scores[0].device\n        anchor_list, valid_flag_list = self.get_anchors(\n            featmap_sizes, img_metas, device=device)\n        label_channels = self.cls_out_channels if self.use_sigmoid_cls else 1\n\n        cls_reg_targets = self.get_targets(\n            anchor_list,\n            valid_flag_list,\n            gt_bboxes,\n            img_metas,\n            gt_bboxes_ignore_list=gt_bboxes_ignore,\n            gt_labels_list=gt_labels,\n            label_channels=label_channels)\n        if cls_reg_targets is None:\n            return None\n\n        (anchor_list, labels_list, label_weights_list, bbox_targets_list,\n         bbox_weights_list, num_total_pos, num_total_neg) = cls_reg_targets\n\n        num_total_samples = reduce_mean(\n            torch.tensor(num_total_pos).cuda()).item()\n        num_total_samples = max(num_total_samples, 1.0)\n\n        losses_cls, losses_bbox, loss_centerness,\\\n            bbox_avg_factor = multi_apply(\n                self.loss_single,\n                anchor_list,\n                cls_scores,\n                bbox_preds,\n                centernesses,\n                labels_list,\n                label_weights_list,\n                bbox_targets_list,\n                num_total_samples=num_total_samples)\n\n        bbox_avg_factor = sum(bbox_avg_factor)\n        bbox_avg_factor = reduce_mean(bbox_avg_factor).item()\n        losses_bbox = list(map(lambda x: x / bbox_avg_factor, losses_bbox))\n        return dict(\n            loss_cls=losses_cls,\n            loss_bbox=losses_bbox,\n            loss_centerness=loss_centerness)\n\n    def centerness_target(self, anchors, bbox_targets):\n        # only calculate pos centerness targets, otherwise there may be nan\n        gts = self.bbox_coder.decode(anchors, bbox_targets)\n        anchors_cx = (anchors[:, 2] + anchors[:, 0]) / 2\n        anchors_cy = (anchors[:, 3] + anchors[:, 1]) / 2\n        l_ = anchors_cx - gts[:, 0]\n        t_ = anchors_cy - gts[:, 1]\n        r_ = gts[:, 2] - anchors_cx\n        b_ = gts[:, 3] - anchors_cy\n\n        left_right = torch.stack([l_, r_], dim=1)\n        top_bottom = torch.stack([t_, b_], dim=1)\n        centerness = torch.sqrt(\n            (left_right.min(dim=-1)[0] / left_right.max(dim=-1)[0]) *\n            (top_bottom.min(dim=-1)[0] / top_bottom.max(dim=-1)[0]))\n        assert not torch.isnan(centerness).any()\n        return centerness\n\n    @force_fp32(apply_to=(\'cls_scores\', \'bbox_preds\', \'centernesses\'))\n    def get_bboxes(self,\n                   cls_scores,\n                   bbox_preds,\n                   centernesses,\n                   img_metas,\n                   cfg=None,\n                   rescale=False):\n        cfg = self.test_cfg if cfg is None else cfg\n        assert len(cls_scores) == len(bbox_preds)\n        num_levels = len(cls_scores)\n        device = cls_scores[0].device\n        featmap_sizes = [cls_scores[i].shape[-2:] for i in range(num_levels)]\n        mlvl_anchors = self.anchor_generator.grid_anchors(\n            featmap_sizes, device=device)\n\n        result_list = []\n        for img_id in range(len(img_metas)):\n            cls_score_list = [\n                cls_scores[i][img_id].detach() for i in range(num_levels)\n            ]\n            bbox_pred_list = [\n                bbox_preds[i][img_id].detach() for i in range(num_levels)\n            ]\n            centerness_pred_list = [\n                centernesses[i][img_id].detach() for i in range(num_levels)\n            ]\n            img_shape = img_metas[img_id][\'img_shape\']\n            scale_factor = img_metas[img_id][\'scale_factor\']\n            proposals = self._get_bboxes_single(cls_score_list, bbox_pred_list,\n                                                centerness_pred_list,\n                                                mlvl_anchors, img_shape,\n                                                scale_factor, cfg, rescale)\n            result_list.append(proposals)\n        return result_list\n\n    def _get_bboxes_single(self,\n                           cls_scores,\n                           bbox_preds,\n                           centernesses,\n                           mlvl_anchors,\n                           img_shape,\n                           scale_factor,\n                           cfg,\n                           rescale=False):\n        assert len(cls_scores) == len(bbox_preds) == len(mlvl_anchors)\n        mlvl_bboxes = []\n        mlvl_scores = []\n        mlvl_centerness = []\n        for cls_score, bbox_pred, centerness, anchors in zip(\n                cls_scores, bbox_preds, centernesses, mlvl_anchors):\n            assert cls_score.size()[-2:] == bbox_pred.size()[-2:]\n\n            scores = cls_score.permute(1, 2, 0).reshape(\n                -1, self.cls_out_channels).sigmoid()\n            bbox_pred = bbox_pred.permute(1, 2, 0).reshape(-1, 4)\n            centerness = centerness.permute(1, 2, 0).reshape(-1).sigmoid()\n\n            nms_pre = cfg.get(\'nms_pre\', -1)\n            if nms_pre > 0 and scores.shape[0] > nms_pre:\n                max_scores, _ = (scores * centerness[:, None]).max(dim=1)\n                _, topk_inds = max_scores.topk(nms_pre)\n                anchors = anchors[topk_inds, :]\n                bbox_pred = bbox_pred[topk_inds, :]\n                scores = scores[topk_inds, :]\n                centerness = centerness[topk_inds]\n\n            bboxes = self.bbox_coder.decode(\n                anchors, bbox_pred, max_shape=img_shape)\n            mlvl_bboxes.append(bboxes)\n            mlvl_scores.append(scores)\n            mlvl_centerness.append(centerness)\n\n        mlvl_bboxes = torch.cat(mlvl_bboxes)\n        if rescale:\n            mlvl_bboxes /= mlvl_bboxes.new_tensor(scale_factor)\n\n        mlvl_scores = torch.cat(mlvl_scores)\n        # Add a dummy background class to the backend when using sigmoid\n        # remind that we set FG labels to [0, num_class-1] since mmdet v2.0\n        # BG cat_id: num_class\n        padding = mlvl_scores.new_zeros(mlvl_scores.shape[0], 1)\n        mlvl_scores = torch.cat([mlvl_scores, padding], dim=1)\n        mlvl_centerness = torch.cat(mlvl_centerness)\n\n        det_bboxes, det_labels = multiclass_nms(\n            mlvl_bboxes,\n            mlvl_scores,\n            cfg.score_thr,\n            cfg.nms,\n            cfg.max_per_img,\n            score_factors=mlvl_centerness)\n        return det_bboxes, det_labels\n\n    def get_targets(self,\n                    anchor_list,\n                    valid_flag_list,\n                    gt_bboxes_list,\n                    img_metas,\n                    gt_bboxes_ignore_list=None,\n                    gt_labels_list=None,\n                    label_channels=1,\n                    unmap_outputs=True):\n        """"""Get targets for ATSS head.\n\n        This method is almost the same as `AnchorHead.get_targets()`. Besides\n        returning the targets as the parent method does, it also returns the\n        anchors as the first element of the returned tuple.\n        """"""\n        num_imgs = len(img_metas)\n        assert len(anchor_list) == len(valid_flag_list) == num_imgs\n\n        # anchor number of multi levels\n        num_level_anchors = [anchors.size(0) for anchors in anchor_list[0]]\n        num_level_anchors_list = [num_level_anchors] * num_imgs\n\n        # concat all level anchors and flags to a single tensor\n        for i in range(num_imgs):\n            assert len(anchor_list[i]) == len(valid_flag_list[i])\n            anchor_list[i] = torch.cat(anchor_list[i])\n            valid_flag_list[i] = torch.cat(valid_flag_list[i])\n\n        # compute targets for each image\n        if gt_bboxes_ignore_list is None:\n            gt_bboxes_ignore_list = [None for _ in range(num_imgs)]\n        if gt_labels_list is None:\n            gt_labels_list = [None for _ in range(num_imgs)]\n        (all_anchors, all_labels, all_label_weights, all_bbox_targets,\n         all_bbox_weights, pos_inds_list, neg_inds_list) = multi_apply(\n             self._get_target_single,\n             anchor_list,\n             valid_flag_list,\n             num_level_anchors_list,\n             gt_bboxes_list,\n             gt_bboxes_ignore_list,\n             gt_labels_list,\n             img_metas,\n             label_channels=label_channels,\n             unmap_outputs=unmap_outputs)\n        # no valid anchors\n        if any([labels is None for labels in all_labels]):\n            return None\n        # sampled anchors of all images\n        num_total_pos = sum([max(inds.numel(), 1) for inds in pos_inds_list])\n        num_total_neg = sum([max(inds.numel(), 1) for inds in neg_inds_list])\n        # split targets to a list w.r.t. multiple levels\n        anchors_list = images_to_levels(all_anchors, num_level_anchors)\n        labels_list = images_to_levels(all_labels, num_level_anchors)\n        label_weights_list = images_to_levels(all_label_weights,\n                                              num_level_anchors)\n        bbox_targets_list = images_to_levels(all_bbox_targets,\n                                             num_level_anchors)\n        bbox_weights_list = images_to_levels(all_bbox_weights,\n                                             num_level_anchors)\n        return (anchors_list, labels_list, label_weights_list,\n                bbox_targets_list, bbox_weights_list, num_total_pos,\n                num_total_neg)\n\n    def _get_target_single(self,\n                           flat_anchors,\n                           valid_flags,\n                           num_level_anchors,\n                           gt_bboxes,\n                           gt_bboxes_ignore,\n                           gt_labels,\n                           img_meta,\n                           label_channels=1,\n                           unmap_outputs=True):\n        inside_flags = anchor_inside_flags(flat_anchors, valid_flags,\n                                           img_meta[\'img_shape\'][:2],\n                                           self.train_cfg.allowed_border)\n        if not inside_flags.any():\n            return (None, ) * 6\n        # assign gt and sample anchors\n        anchors = flat_anchors[inside_flags, :]\n\n        num_level_anchors_inside = self.get_num_level_anchors_inside(\n            num_level_anchors, inside_flags)\n        assign_result = self.assigner.assign(anchors, num_level_anchors_inside,\n                                             gt_bboxes, gt_bboxes_ignore,\n                                             gt_labels)\n\n        sampling_result = self.sampler.sample(assign_result, anchors,\n                                              gt_bboxes)\n\n        num_valid_anchors = anchors.shape[0]\n        bbox_targets = torch.zeros_like(anchors)\n        bbox_weights = torch.zeros_like(anchors)\n        labels = anchors.new_full((num_valid_anchors, ),\n                                  self.background_label,\n                                  dtype=torch.long)\n        label_weights = anchors.new_zeros(num_valid_anchors, dtype=torch.float)\n\n        pos_inds = sampling_result.pos_inds\n        neg_inds = sampling_result.neg_inds\n        if len(pos_inds) > 0:\n            pos_bbox_targets = self.bbox_coder.encode(\n                sampling_result.pos_bboxes, sampling_result.pos_gt_bboxes)\n            bbox_targets[pos_inds, :] = pos_bbox_targets\n            bbox_weights[pos_inds, :] = 1.0\n            if gt_labels is None:\n                labels[pos_inds] = 1\n            else:\n                labels[pos_inds] = gt_labels[\n                    sampling_result.pos_assigned_gt_inds]\n            if self.train_cfg.pos_weight <= 0:\n                label_weights[pos_inds] = 1.0\n            else:\n                label_weights[pos_inds] = self.train_cfg.pos_weight\n        if len(neg_inds) > 0:\n            label_weights[neg_inds] = 1.0\n\n        # map up to original set of anchors\n        if unmap_outputs:\n            num_total_anchors = flat_anchors.size(0)\n            anchors = unmap(anchors, num_total_anchors, inside_flags)\n            labels = unmap(\n                labels, num_total_anchors, inside_flags, fill=self.num_classes)\n            label_weights = unmap(label_weights, num_total_anchors,\n                                  inside_flags)\n            bbox_targets = unmap(bbox_targets, num_total_anchors, inside_flags)\n            bbox_weights = unmap(bbox_weights, num_total_anchors, inside_flags)\n\n        return (anchors, labels, label_weights, bbox_targets, bbox_weights,\n                pos_inds, neg_inds)\n\n    def get_num_level_anchors_inside(self, num_level_anchors, inside_flags):\n        split_inside_flags = torch.split(inside_flags, num_level_anchors)\n        num_level_anchors_inside = [\n            int(flags.sum()) for flags in split_inside_flags\n        ]\n        return num_level_anchors_inside\n'"
mmdet/models/dense_heads/fcos_head.py,30,"b'import torch\nimport torch.nn as nn\nfrom mmcv.cnn import ConvModule, Scale, bias_init_with_prob, normal_init\n\nfrom mmdet.core import distance2bbox, force_fp32, multi_apply, multiclass_nms\nfrom ..builder import HEADS, build_loss\n\nINF = 1e8\n\n\n@HEADS.register_module()\nclass FCOSHead(nn.Module):\n    """"""Anchor-free head used in `FCOS <https://arxiv.org/abs/1904.01355>`_.\n\n    The FCOS head does not use anchor boxes. Instead bounding boxes are\n    predicted at each pixel and a centerness measure is used to supress\n    low-quality predictions.\n\n    Example:\n        >>> self = FCOSHead(11, 7)\n        >>> feats = [torch.rand(1, 7, s, s) for s in [4, 8, 16, 32, 64]]\n        >>> cls_score, bbox_pred, centerness = self.forward(feats)\n        >>> assert len(cls_score) == len(self.scales)\n    """"""\n\n    def __init__(self,\n                 num_classes,\n                 in_channels,\n                 feat_channels=256,\n                 stacked_convs=4,\n                 strides=(4, 8, 16, 32, 64),\n                 regress_ranges=((-1, 64), (64, 128), (128, 256), (256, 512),\n                                 (512, INF)),\n                 center_sampling=False,\n                 center_sample_radius=1.5,\n                 background_label=None,\n                 loss_cls=dict(\n                     type=\'FocalLoss\',\n                     use_sigmoid=True,\n                     gamma=2.0,\n                     alpha=0.25,\n                     loss_weight=1.0),\n                 loss_bbox=dict(type=\'IoULoss\', loss_weight=1.0),\n                 loss_centerness=dict(\n                     type=\'CrossEntropyLoss\',\n                     use_sigmoid=True,\n                     loss_weight=1.0),\n                 conv_cfg=None,\n                 norm_cfg=dict(type=\'GN\', num_groups=32, requires_grad=True),\n                 train_cfg=None,\n                 test_cfg=None):\n        super(FCOSHead, self).__init__()\n        self.num_classes = num_classes\n        self.cls_out_channels = num_classes\n        self.in_channels = in_channels\n        self.feat_channels = feat_channels\n        self.stacked_convs = stacked_convs\n        self.strides = strides\n        self.regress_ranges = regress_ranges\n        self.loss_cls = build_loss(loss_cls)\n        self.loss_bbox = build_loss(loss_bbox)\n        self.loss_centerness = build_loss(loss_centerness)\n        self.train_cfg = train_cfg\n        self.test_cfg = test_cfg\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        self.fp16_enabled = False\n        self.center_sampling = center_sampling\n        self.center_sample_radius = center_sample_radius\n        self.background_label = (\n            num_classes if background_label is None else background_label)\n        # background_label should be either 0 or num_classes\n        assert (self.background_label == 0\n                or self.background_label == num_classes)\n\n        self._init_layers()\n\n    def _init_layers(self):\n        self.cls_convs = nn.ModuleList()\n        self.reg_convs = nn.ModuleList()\n        for i in range(self.stacked_convs):\n            chn = self.in_channels if i == 0 else self.feat_channels\n            self.cls_convs.append(\n                ConvModule(\n                    chn,\n                    self.feat_channels,\n                    3,\n                    stride=1,\n                    padding=1,\n                    conv_cfg=self.conv_cfg,\n                    norm_cfg=self.norm_cfg,\n                    bias=self.norm_cfg is None))\n            self.reg_convs.append(\n                ConvModule(\n                    chn,\n                    self.feat_channels,\n                    3,\n                    stride=1,\n                    padding=1,\n                    conv_cfg=self.conv_cfg,\n                    norm_cfg=self.norm_cfg,\n                    bias=self.norm_cfg is None))\n        self.fcos_cls = nn.Conv2d(\n            self.feat_channels, self.cls_out_channels, 3, padding=1)\n        self.fcos_reg = nn.Conv2d(self.feat_channels, 4, 3, padding=1)\n        self.fcos_centerness = nn.Conv2d(self.feat_channels, 1, 3, padding=1)\n\n        self.scales = nn.ModuleList([Scale(1.0) for _ in self.strides])\n\n    def init_weights(self):\n        for m in self.cls_convs:\n            normal_init(m.conv, std=0.01)\n        for m in self.reg_convs:\n            normal_init(m.conv, std=0.01)\n        bias_cls = bias_init_with_prob(0.01)\n        normal_init(self.fcos_cls, std=0.01, bias=bias_cls)\n        normal_init(self.fcos_reg, std=0.01)\n        normal_init(self.fcos_centerness, std=0.01)\n\n    def forward(self, feats):\n        return multi_apply(self.forward_single, feats, self.scales)\n\n    def forward_single(self, x, scale):\n        cls_feat = x\n        reg_feat = x\n\n        for cls_layer in self.cls_convs:\n            cls_feat = cls_layer(cls_feat)\n        cls_score = self.fcos_cls(cls_feat)\n        centerness = self.fcos_centerness(cls_feat)\n\n        for reg_layer in self.reg_convs:\n            reg_feat = reg_layer(reg_feat)\n        # scale the bbox_pred of different level\n        # float to avoid overflow when enabling FP16\n        bbox_pred = scale(self.fcos_reg(reg_feat)).float().exp()\n        return cls_score, bbox_pred, centerness\n\n    @force_fp32(apply_to=(\'cls_scores\', \'bbox_preds\', \'centernesses\'))\n    def loss(self,\n             cls_scores,\n             bbox_preds,\n             centernesses,\n             gt_bboxes,\n             gt_labels,\n             img_metas,\n             gt_bboxes_ignore=None):\n        assert len(cls_scores) == len(bbox_preds) == len(centernesses)\n        featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n        all_level_points = self.get_points(featmap_sizes, bbox_preds[0].dtype,\n                                           bbox_preds[0].device)\n        labels, bbox_targets = self.get_targets(all_level_points, gt_bboxes,\n                                                gt_labels)\n\n        num_imgs = cls_scores[0].size(0)\n        # flatten cls_scores, bbox_preds and centerness\n        flatten_cls_scores = [\n            cls_score.permute(0, 2, 3, 1).reshape(-1, self.cls_out_channels)\n            for cls_score in cls_scores\n        ]\n        flatten_bbox_preds = [\n            bbox_pred.permute(0, 2, 3, 1).reshape(-1, 4)\n            for bbox_pred in bbox_preds\n        ]\n        flatten_centerness = [\n            centerness.permute(0, 2, 3, 1).reshape(-1)\n            for centerness in centernesses\n        ]\n        flatten_cls_scores = torch.cat(flatten_cls_scores)\n        flatten_bbox_preds = torch.cat(flatten_bbox_preds)\n        flatten_centerness = torch.cat(flatten_centerness)\n        flatten_labels = torch.cat(labels)\n        flatten_bbox_targets = torch.cat(bbox_targets)\n        # repeat points to align with bbox_preds\n        flatten_points = torch.cat(\n            [points.repeat(num_imgs, 1) for points in all_level_points])\n\n        # FG cat_id: [0, num_classes -1], BG cat_id: num_classes\n        bg_class_ind = self.num_classes\n        pos_inds = ((flatten_labels >= 0)\n                    & (flatten_labels < bg_class_ind)).nonzero().reshape(-1)\n        num_pos = len(pos_inds)\n        loss_cls = self.loss_cls(\n            flatten_cls_scores, flatten_labels,\n            avg_factor=num_pos + num_imgs)  # avoid num_pos is 0\n\n        pos_bbox_preds = flatten_bbox_preds[pos_inds]\n        pos_centerness = flatten_centerness[pos_inds]\n\n        if num_pos > 0:\n            pos_bbox_targets = flatten_bbox_targets[pos_inds]\n            pos_centerness_targets = self.centerness_target(pos_bbox_targets)\n            pos_points = flatten_points[pos_inds]\n            pos_decoded_bbox_preds = distance2bbox(pos_points, pos_bbox_preds)\n            pos_decoded_target_preds = distance2bbox(pos_points,\n                                                     pos_bbox_targets)\n            # centerness weighted iou loss\n            loss_bbox = self.loss_bbox(\n                pos_decoded_bbox_preds,\n                pos_decoded_target_preds,\n                weight=pos_centerness_targets,\n                avg_factor=pos_centerness_targets.sum())\n            loss_centerness = self.loss_centerness(pos_centerness,\n                                                   pos_centerness_targets)\n        else:\n            loss_bbox = pos_bbox_preds.sum()\n            loss_centerness = pos_centerness.sum()\n\n        return dict(\n            loss_cls=loss_cls,\n            loss_bbox=loss_bbox,\n            loss_centerness=loss_centerness)\n\n    @force_fp32(apply_to=(\'cls_scores\', \'bbox_preds\', \'centernesses\'))\n    def get_bboxes(self,\n                   cls_scores,\n                   bbox_preds,\n                   centernesses,\n                   img_metas,\n                   cfg=None,\n                   rescale=None):\n        assert len(cls_scores) == len(bbox_preds)\n        num_levels = len(cls_scores)\n\n        featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n        mlvl_points = self.get_points(featmap_sizes, bbox_preds[0].dtype,\n                                      bbox_preds[0].device)\n        result_list = []\n        for img_id in range(len(img_metas)):\n            cls_score_list = [\n                cls_scores[i][img_id].detach() for i in range(num_levels)\n            ]\n            bbox_pred_list = [\n                bbox_preds[i][img_id].detach() for i in range(num_levels)\n            ]\n            centerness_pred_list = [\n                centernesses[i][img_id].detach() for i in range(num_levels)\n            ]\n            img_shape = img_metas[img_id][\'img_shape\']\n            scale_factor = img_metas[img_id][\'scale_factor\']\n            det_bboxes = self._get_bboxes_single(cls_score_list,\n                                                 bbox_pred_list,\n                                                 centerness_pred_list,\n                                                 mlvl_points, img_shape,\n                                                 scale_factor, cfg, rescale)\n            result_list.append(det_bboxes)\n        return result_list\n\n    def _get_bboxes_single(self,\n                           cls_scores,\n                           bbox_preds,\n                           centernesses,\n                           mlvl_points,\n                           img_shape,\n                           scale_factor,\n                           cfg,\n                           rescale=False):\n        cfg = self.test_cfg if cfg is None else cfg\n        assert len(cls_scores) == len(bbox_preds) == len(mlvl_points)\n        mlvl_bboxes = []\n        mlvl_scores = []\n        mlvl_centerness = []\n        for cls_score, bbox_pred, centerness, points in zip(\n                cls_scores, bbox_preds, centernesses, mlvl_points):\n            assert cls_score.size()[-2:] == bbox_pred.size()[-2:]\n            scores = cls_score.permute(1, 2, 0).reshape(\n                -1, self.cls_out_channels).sigmoid()\n            centerness = centerness.permute(1, 2, 0).reshape(-1).sigmoid()\n\n            bbox_pred = bbox_pred.permute(1, 2, 0).reshape(-1, 4)\n            nms_pre = cfg.get(\'nms_pre\', -1)\n            if nms_pre > 0 and scores.shape[0] > nms_pre:\n                max_scores, _ = (scores * centerness[:, None]).max(dim=1)\n                _, topk_inds = max_scores.topk(nms_pre)\n                points = points[topk_inds, :]\n                bbox_pred = bbox_pred[topk_inds, :]\n                scores = scores[topk_inds, :]\n                centerness = centerness[topk_inds]\n            bboxes = distance2bbox(points, bbox_pred, max_shape=img_shape)\n            mlvl_bboxes.append(bboxes)\n            mlvl_scores.append(scores)\n            mlvl_centerness.append(centerness)\n        mlvl_bboxes = torch.cat(mlvl_bboxes)\n        if rescale:\n            mlvl_bboxes /= mlvl_bboxes.new_tensor(scale_factor)\n        mlvl_scores = torch.cat(mlvl_scores)\n        padding = mlvl_scores.new_zeros(mlvl_scores.shape[0], 1)\n        # remind that we set FG labels to [0, num_class-1] since mmdet v2.0\n        # BG cat_id: num_class\n        mlvl_scores = torch.cat([mlvl_scores, padding], dim=1)\n        mlvl_centerness = torch.cat(mlvl_centerness)\n        det_bboxes, det_labels = multiclass_nms(\n            mlvl_bboxes,\n            mlvl_scores,\n            cfg.score_thr,\n            cfg.nms,\n            cfg.max_per_img,\n            score_factors=mlvl_centerness)\n        return det_bboxes, det_labels\n\n    def get_points(self, featmap_sizes, dtype, device):\n        """"""Get points according to feature map sizes.\n\n        Args:\n            featmap_sizes (list[tuple]): Multi-level feature map sizes.\n            dtype (torch.dtype): Type of points.\n            device (torch.device): Device of points.\n\n        Returns:\n            tuple: points of each image.\n        """"""\n        mlvl_points = []\n        for i in range(len(featmap_sizes)):\n            mlvl_points.append(\n                self._get_points_single(featmap_sizes[i], self.strides[i],\n                                        dtype, device))\n        return mlvl_points\n\n    def _get_points_single(self, featmap_size, stride, dtype, device):\n        h, w = featmap_size\n        x_range = torch.arange(\n            0, w * stride, stride, dtype=dtype, device=device)\n        y_range = torch.arange(\n            0, h * stride, stride, dtype=dtype, device=device)\n        y, x = torch.meshgrid(y_range, x_range)\n        points = torch.stack(\n            (x.reshape(-1), y.reshape(-1)), dim=-1) + stride // 2\n        return points\n\n    def get_targets(self, points, gt_bboxes_list, gt_labels_list):\n        assert len(points) == len(self.regress_ranges)\n        num_levels = len(points)\n        # expand regress ranges to align with points\n        expanded_regress_ranges = [\n            points[i].new_tensor(self.regress_ranges[i])[None].expand_as(\n                points[i]) for i in range(num_levels)\n        ]\n        # concat all levels points and regress ranges\n        concat_regress_ranges = torch.cat(expanded_regress_ranges, dim=0)\n        concat_points = torch.cat(points, dim=0)\n\n        # the number of points per img, per lvl\n        num_points = [center.size(0) for center in points]\n\n        # get labels and bbox_targets of each image\n        labels_list, bbox_targets_list = multi_apply(\n            self._get_target_single,\n            gt_bboxes_list,\n            gt_labels_list,\n            points=concat_points,\n            regress_ranges=concat_regress_ranges,\n            num_points_per_lvl=num_points)\n\n        # split to per img, per level\n        labels_list = [labels.split(num_points, 0) for labels in labels_list]\n        bbox_targets_list = [\n            bbox_targets.split(num_points, 0)\n            for bbox_targets in bbox_targets_list\n        ]\n\n        # concat per level image\n        concat_lvl_labels = []\n        concat_lvl_bbox_targets = []\n        for i in range(num_levels):\n            concat_lvl_labels.append(\n                torch.cat([labels[i] for labels in labels_list]))\n            concat_lvl_bbox_targets.append(\n                torch.cat(\n                    [bbox_targets[i] for bbox_targets in bbox_targets_list]))\n        return concat_lvl_labels, concat_lvl_bbox_targets\n\n    def _get_target_single(self, gt_bboxes, gt_labels, points, regress_ranges,\n                           num_points_per_lvl):\n        num_points = points.size(0)\n        num_gts = gt_labels.size(0)\n        if num_gts == 0:\n            return gt_labels.new_full((num_points, ), self.background_label), \\\n                   gt_bboxes.new_zeros((num_points, 4))\n\n        areas = (gt_bboxes[:, 2] - gt_bboxes[:, 0]) * (\n            gt_bboxes[:, 3] - gt_bboxes[:, 1])\n        # TODO: figure out why these two are different\n        # areas = areas[None].expand(num_points, num_gts)\n        areas = areas[None].repeat(num_points, 1)\n        regress_ranges = regress_ranges[:, None, :].expand(\n            num_points, num_gts, 2)\n        gt_bboxes = gt_bboxes[None].expand(num_points, num_gts, 4)\n        xs, ys = points[:, 0], points[:, 1]\n        xs = xs[:, None].expand(num_points, num_gts)\n        ys = ys[:, None].expand(num_points, num_gts)\n\n        left = xs - gt_bboxes[..., 0]\n        right = gt_bboxes[..., 2] - xs\n        top = ys - gt_bboxes[..., 1]\n        bottom = gt_bboxes[..., 3] - ys\n        bbox_targets = torch.stack((left, top, right, bottom), -1)\n\n        if self.center_sampling:\n            # condition1: inside a `center bbox`\n            radius = self.center_sample_radius\n            center_xs = (gt_bboxes[..., 0] + gt_bboxes[..., 2]) / 2\n            center_ys = (gt_bboxes[..., 1] + gt_bboxes[..., 3]) / 2\n            center_gts = torch.zeros_like(gt_bboxes)\n            stride = center_xs.new_zeros(center_xs.shape)\n\n            # project the points on current lvl back to the `original` sizes\n            lvl_begin = 0\n            for lvl_idx, num_points_lvl in enumerate(num_points_per_lvl):\n                lvl_end = lvl_begin + num_points_lvl\n                stride[lvl_begin:lvl_end] = self.strides[lvl_idx] * radius\n                lvl_begin = lvl_end\n\n            x_mins = center_xs - stride\n            y_mins = center_ys - stride\n            x_maxs = center_xs + stride\n            y_maxs = center_ys + stride\n            center_gts[..., 0] = torch.where(x_mins > gt_bboxes[..., 0],\n                                             x_mins, gt_bboxes[..., 0])\n            center_gts[..., 1] = torch.where(y_mins > gt_bboxes[..., 1],\n                                             y_mins, gt_bboxes[..., 1])\n            center_gts[..., 2] = torch.where(x_maxs > gt_bboxes[..., 2],\n                                             gt_bboxes[..., 2], x_maxs)\n            center_gts[..., 3] = torch.where(y_maxs > gt_bboxes[..., 3],\n                                             gt_bboxes[..., 3], y_maxs)\n\n            cb_dist_left = xs - center_gts[..., 0]\n            cb_dist_right = center_gts[..., 2] - xs\n            cb_dist_top = ys - center_gts[..., 1]\n            cb_dist_bottom = center_gts[..., 3] - ys\n            center_bbox = torch.stack(\n                (cb_dist_left, cb_dist_top, cb_dist_right, cb_dist_bottom), -1)\n            inside_gt_bbox_mask = center_bbox.min(-1)[0] > 0\n        else:\n            # condition1: inside a gt bbox\n            inside_gt_bbox_mask = bbox_targets.min(-1)[0] > 0\n\n        # condition2: limit the regression range for each location\n        max_regress_distance = bbox_targets.max(-1)[0]\n        inside_regress_range = (\n            max_regress_distance >= regress_ranges[..., 0]) & (\n                max_regress_distance <= regress_ranges[..., 1])\n\n        # if there are still more than one objects for a location,\n        # we choose the one with minimal area\n        areas[inside_gt_bbox_mask == 0] = INF\n        areas[inside_regress_range == 0] = INF\n        min_area, min_area_inds = areas.min(dim=1)\n\n        labels = gt_labels[min_area_inds]\n        labels[min_area == INF] = self.background_label  # set as BG\n        bbox_targets = bbox_targets[range(num_points), min_area_inds]\n\n        return labels, bbox_targets\n\n    def centerness_target(self, pos_bbox_targets):\n        # only calculate pos centerness targets, otherwise there may be nan\n        left_right = pos_bbox_targets[:, [0, 2]]\n        top_bottom = pos_bbox_targets[:, [1, 3]]\n        centerness_targets = (\n            left_right.min(dim=-1)[0] / left_right.max(dim=-1)[0]) * (\n                top_bottom.min(dim=-1)[0] / top_bottom.max(dim=-1)[0])\n        return torch.sqrt(centerness_targets)\n'"
mmdet/models/dense_heads/fovea_head.py,23,"b'import torch\nimport torch.nn as nn\nfrom mmcv.cnn import ConvModule, bias_init_with_prob, normal_init\n\nfrom mmdet.core import multi_apply, multiclass_nms\nfrom mmdet.ops import DeformConv\nfrom ..builder import HEADS, build_loss\n\nINF = 1e8\n\n\nclass FeatureAlign(nn.Module):\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size=3,\n                 deformable_groups=4):\n        super(FeatureAlign, self).__init__()\n        offset_channels = kernel_size * kernel_size * 2\n        self.conv_offset = nn.Conv2d(\n            4, deformable_groups * offset_channels, 1, bias=False)\n        self.conv_adaption = DeformConv(\n            in_channels,\n            out_channels,\n            kernel_size=kernel_size,\n            padding=(kernel_size - 1) // 2,\n            deformable_groups=deformable_groups)\n        self.relu = nn.ReLU(inplace=True)\n\n    def init_weights(self):\n        normal_init(self.conv_offset, std=0.1)\n        normal_init(self.conv_adaption, std=0.01)\n\n    def forward(self, x, shape):\n        offset = self.conv_offset(shape)\n        x = self.relu(self.conv_adaption(x, offset))\n        return x\n\n\n@HEADS.register_module()\nclass FoveaHead(nn.Module):\n    """"""FoveaBox: Beyond Anchor-based Object Detector\n    https://arxiv.org/abs/1904.03797\n    """"""\n\n    def __init__(self,\n                 num_classes,\n                 in_channels,\n                 feat_channels=256,\n                 stacked_convs=4,\n                 strides=(4, 8, 16, 32, 64),\n                 base_edge_list=(16, 32, 64, 128, 256),\n                 scale_ranges=((8, 32), (16, 64), (32, 128), (64, 256), (128,\n                                                                         512)),\n                 sigma=0.4,\n                 with_deform=False,\n                 deformable_groups=4,\n                 background_label=None,\n                 loss_cls=None,\n                 loss_bbox=None,\n                 conv_cfg=None,\n                 norm_cfg=None,\n                 train_cfg=None,\n                 test_cfg=None):\n        super(FoveaHead, self).__init__()\n        self.num_classes = num_classes\n        self.cls_out_channels = num_classes\n        self.in_channels = in_channels\n        self.feat_channels = feat_channels\n        self.stacked_convs = stacked_convs\n        self.strides = strides\n        self.base_edge_list = base_edge_list\n        self.scale_ranges = scale_ranges\n        self.sigma = sigma\n        self.with_deform = with_deform\n        self.deformable_groups = deformable_groups\n        self.background_label = (\n            num_classes if background_label is None else background_label)\n        # background_label should be either 0 or num_classes\n        assert (self.background_label == 0\n                or self.background_label == num_classes)\n        self.loss_cls = build_loss(loss_cls)\n        self.loss_bbox = build_loss(loss_bbox)\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        self.train_cfg = train_cfg\n        self.test_cfg = test_cfg\n        self._init_layers()\n\n    def _init_layers(self):\n        self.cls_convs = nn.ModuleList()\n        self.reg_convs = nn.ModuleList()\n        # box branch\n        for i in range(self.stacked_convs):\n            chn = self.in_channels if i == 0 else self.feat_channels\n            self.reg_convs.append(\n                ConvModule(\n                    chn,\n                    self.feat_channels,\n                    3,\n                    stride=1,\n                    padding=1,\n                    conv_cfg=self.conv_cfg,\n                    norm_cfg=self.norm_cfg,\n                    bias=self.norm_cfg is None))\n        self.fovea_reg = nn.Conv2d(self.feat_channels, 4, 3, padding=1)\n        # cls branch\n        if not self.with_deform:\n            for i in range(self.stacked_convs):\n                chn = self.in_channels if i == 0 else self.feat_channels\n                self.cls_convs.append(\n                    ConvModule(\n                        chn,\n                        self.feat_channels,\n                        3,\n                        stride=1,\n                        padding=1,\n                        conv_cfg=self.conv_cfg,\n                        norm_cfg=self.norm_cfg,\n                        bias=self.norm_cfg is None))\n            self.fovea_cls = nn.Conv2d(\n                self.feat_channels, self.cls_out_channels, 3, padding=1)\n        else:\n            self.cls_convs.append(\n                ConvModule(\n                    self.feat_channels, (self.feat_channels * 4),\n                    3,\n                    stride=1,\n                    padding=1,\n                    conv_cfg=self.conv_cfg,\n                    norm_cfg=self.norm_cfg,\n                    bias=self.norm_cfg is None))\n            self.cls_convs.append(\n                ConvModule((self.feat_channels * 4), (self.feat_channels * 4),\n                           1,\n                           stride=1,\n                           padding=0,\n                           conv_cfg=self.conv_cfg,\n                           norm_cfg=self.norm_cfg,\n                           bias=self.norm_cfg is None))\n            self.feature_adaption = FeatureAlign(\n                self.feat_channels,\n                self.feat_channels,\n                kernel_size=3,\n                deformable_groups=self.deformable_groups)\n            self.fovea_cls = nn.Conv2d(\n                int(self.feat_channels * 4),\n                self.cls_out_channels,\n                3,\n                padding=1)\n\n    def init_weights(self):\n        for m in self.cls_convs:\n            normal_init(m.conv, std=0.01)\n        for m in self.reg_convs:\n            normal_init(m.conv, std=0.01)\n        bias_cls = bias_init_with_prob(0.01)\n        normal_init(self.fovea_cls, std=0.01, bias=bias_cls)\n        normal_init(self.fovea_reg, std=0.01)\n        if self.with_deform:\n            self.feature_adaption.init_weights()\n\n    def forward(self, feats):\n        return multi_apply(self.forward_single, feats)\n\n    def forward_single(self, x):\n        cls_feat = x\n        reg_feat = x\n        for reg_layer in self.reg_convs:\n            reg_feat = reg_layer(reg_feat)\n        bbox_pred = self.fovea_reg(reg_feat)\n        if self.with_deform:\n            cls_feat = self.feature_adaption(cls_feat, bbox_pred.exp())\n        for cls_layer in self.cls_convs:\n            cls_feat = cls_layer(cls_feat)\n        cls_score = self.fovea_cls(cls_feat)\n        return cls_score, bbox_pred\n\n    def get_points(self, featmap_sizes, dtype, device, flatten=False):\n        points = []\n        for featmap_size in featmap_sizes:\n            x_range = torch.arange(\n                featmap_size[1], dtype=dtype, device=device) + 0.5\n            y_range = torch.arange(\n                featmap_size[0], dtype=dtype, device=device) + 0.5\n            y, x = torch.meshgrid(y_range, x_range)\n            if flatten:\n                points.append((y.flatten(), x.flatten()))\n            else:\n                points.append((y, x))\n        return points\n\n    def loss(self,\n             cls_scores,\n             bbox_preds,\n             gt_bbox_list,\n             gt_label_list,\n             img_metas,\n             gt_bboxes_ignore=None):\n        assert len(cls_scores) == len(bbox_preds)\n\n        featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n        points = self.get_points(featmap_sizes, bbox_preds[0].dtype,\n                                 bbox_preds[0].device)\n        num_imgs = cls_scores[0].size(0)\n        flatten_cls_scores = [\n            cls_score.permute(0, 2, 3, 1).reshape(-1, self.cls_out_channels)\n            for cls_score in cls_scores\n        ]\n        flatten_bbox_preds = [\n            bbox_pred.permute(0, 2, 3, 1).reshape(-1, 4)\n            for bbox_pred in bbox_preds\n        ]\n        flatten_cls_scores = torch.cat(flatten_cls_scores)\n        flatten_bbox_preds = torch.cat(flatten_bbox_preds)\n        flatten_labels, flatten_bbox_targets = self.get_targets(\n            gt_bbox_list, gt_label_list, featmap_sizes, points)\n\n        # FG cat_id: [0, num_classes -1], BG cat_id: num_classes\n        pos_inds = (\n            (flatten_labels >= 0)\n            & (flatten_labels < self.background_label)).nonzero().view(-1)\n        num_pos = len(pos_inds)\n\n        loss_cls = self.loss_cls(\n            flatten_cls_scores, flatten_labels, avg_factor=num_pos + num_imgs)\n        if num_pos > 0:\n            pos_bbox_preds = flatten_bbox_preds[pos_inds]\n            pos_bbox_targets = flatten_bbox_targets[pos_inds]\n            pos_weights = pos_bbox_targets.new_zeros(\n                pos_bbox_targets.size()) + 1.0\n            loss_bbox = self.loss_bbox(\n                pos_bbox_preds,\n                pos_bbox_targets,\n                pos_weights,\n                avg_factor=num_pos)\n        else:\n            loss_bbox = torch.tensor(\n                0,\n                dtype=flatten_bbox_preds.dtype,\n                device=flatten_bbox_preds.device)\n        return dict(loss_cls=loss_cls, loss_bbox=loss_bbox)\n\n    def get_targets(self, gt_bbox_list, gt_label_list, featmap_sizes, points):\n        label_list, bbox_target_list = multi_apply(\n            self._get_target_single,\n            gt_bbox_list,\n            gt_label_list,\n            featmap_size_list=featmap_sizes,\n            point_list=points)\n        flatten_labels = [\n            torch.cat([\n                labels_level_img.flatten() for labels_level_img in labels_level\n            ]) for labels_level in zip(*label_list)\n        ]\n        flatten_bbox_targets = [\n            torch.cat([\n                bbox_targets_level_img.reshape(-1, 4)\n                for bbox_targets_level_img in bbox_targets_level\n            ]) for bbox_targets_level in zip(*bbox_target_list)\n        ]\n        flatten_labels = torch.cat(flatten_labels)\n        flatten_bbox_targets = torch.cat(flatten_bbox_targets)\n        return flatten_labels, flatten_bbox_targets\n\n    def _get_target_single(self,\n                           gt_bboxes_raw,\n                           gt_labels_raw,\n                           featmap_size_list=None,\n                           point_list=None):\n\n        gt_areas = torch.sqrt((gt_bboxes_raw[:, 2] - gt_bboxes_raw[:, 0]) *\n                              (gt_bboxes_raw[:, 3] - gt_bboxes_raw[:, 1]))\n        label_list = []\n        bbox_target_list = []\n        # for each pyramid, find the cls and box target\n        for base_len, (lower_bound, upper_bound), stride, featmap_size, \\\n            (y, x) in zip(self.base_edge_list, self.scale_ranges,\n                          self.strides, featmap_size_list, point_list):\n            # FG cat_id: [0, num_classes -1], BG cat_id: num_classes\n            labels = gt_labels_raw.new_zeros(featmap_size) + self.num_classes\n            bbox_targets = gt_bboxes_raw.new(featmap_size[0], featmap_size[1],\n                                             4) + 1\n            # scale assignment\n            hit_indices = ((gt_areas >= lower_bound) &\n                           (gt_areas <= upper_bound)).nonzero().flatten()\n            if len(hit_indices) == 0:\n                label_list.append(labels)\n                bbox_target_list.append(torch.log(bbox_targets))\n                continue\n            _, hit_index_order = torch.sort(-gt_areas[hit_indices])\n            hit_indices = hit_indices[hit_index_order]\n            gt_bboxes = gt_bboxes_raw[hit_indices, :] / stride\n            gt_labels = gt_labels_raw[hit_indices]\n            half_w = 0.5 * (gt_bboxes[:, 2] - gt_bboxes[:, 0])\n            half_h = 0.5 * (gt_bboxes[:, 3] - gt_bboxes[:, 1])\n            # valid fovea area: left, right, top, down\n            pos_left = torch.ceil(\n                gt_bboxes[:, 0] + (1 - self.sigma) * half_w - 0.5).long().\\\n                clamp(0, featmap_size[1] - 1)\n            pos_right = torch.floor(\n                gt_bboxes[:, 0] + (1 + self.sigma) * half_w - 0.5).long().\\\n                clamp(0, featmap_size[1] - 1)\n            pos_top = torch.ceil(\n                gt_bboxes[:, 1] + (1 - self.sigma) * half_h - 0.5).long().\\\n                clamp(0, featmap_size[0] - 1)\n            pos_down = torch.floor(\n                gt_bboxes[:, 1] + (1 + self.sigma) * half_h - 0.5).long().\\\n                clamp(0, featmap_size[0] - 1)\n            for px1, py1, px2, py2, label, (gt_x1, gt_y1, gt_x2, gt_y2) in \\\n                    zip(pos_left, pos_top, pos_right, pos_down, gt_labels,\n                        gt_bboxes_raw[hit_indices, :]):\n                labels[py1:py2 + 1, px1:px2 + 1] = label\n                bbox_targets[py1:py2 + 1, px1:px2 + 1, 0] = \\\n                    (stride * x[py1:py2 + 1, px1:px2 + 1] - gt_x1) / base_len\n                bbox_targets[py1:py2 + 1, px1:px2 + 1, 1] = \\\n                    (stride * y[py1:py2 + 1, px1:px2 + 1] - gt_y1) / base_len\n                bbox_targets[py1:py2 + 1, px1:px2 + 1, 2] = \\\n                    (gt_x2 - stride * x[py1:py2 + 1, px1:px2 + 1]) / base_len\n                bbox_targets[py1:py2 + 1, px1:px2 + 1, 3] = \\\n                    (gt_y2 - stride * y[py1:py2 + 1, px1:px2 + 1]) / base_len\n            bbox_targets = bbox_targets.clamp(min=1. / 16, max=16.)\n            label_list.append(labels)\n            bbox_target_list.append(torch.log(bbox_targets))\n        return label_list, bbox_target_list\n\n    def get_bboxes(self,\n                   cls_scores,\n                   bbox_preds,\n                   img_metas,\n                   cfg=None,\n                   rescale=None):\n        assert len(cls_scores) == len(bbox_preds)\n        num_levels = len(cls_scores)\n        featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n        points = self.get_points(\n            featmap_sizes,\n            bbox_preds[0].dtype,\n            bbox_preds[0].device,\n            flatten=True)\n        result_list = []\n        for img_id in range(len(img_metas)):\n            cls_score_list = [\n                cls_scores[i][img_id].detach() for i in range(num_levels)\n            ]\n            bbox_pred_list = [\n                bbox_preds[i][img_id].detach() for i in range(num_levels)\n            ]\n            img_shape = img_metas[img_id][\'img_shape\']\n            scale_factor = img_metas[img_id][\'scale_factor\']\n            det_bboxes = self._get_bboxes_single(cls_score_list,\n                                                 bbox_pred_list, featmap_sizes,\n                                                 points, img_shape,\n                                                 scale_factor, cfg, rescale)\n            result_list.append(det_bboxes)\n        return result_list\n\n    def _get_bboxes_single(self,\n                           cls_scores,\n                           bbox_preds,\n                           featmap_sizes,\n                           point_list,\n                           img_shape,\n                           scale_factor,\n                           cfg,\n                           rescale=False):\n        cfg = self.test_cfg if cfg is None else cfg\n        assert len(cls_scores) == len(bbox_preds) == len(point_list)\n        det_bboxes = []\n        det_scores = []\n        for cls_score, bbox_pred, featmap_size, stride, base_len, (y, x) \\\n                in zip(cls_scores, bbox_preds, featmap_sizes, self.strides,\n                       self.base_edge_list, point_list):\n            assert cls_score.size()[-2:] == bbox_pred.size()[-2:]\n            scores = cls_score.permute(1, 2, 0).reshape(\n                -1, self.cls_out_channels).sigmoid()\n            bbox_pred = bbox_pred.permute(1, 2, 0).reshape(-1, 4).exp()\n            nms_pre = cfg.get(\'nms_pre\', -1)\n            if (nms_pre > 0) and (scores.shape[0] > nms_pre):\n                max_scores, _ = scores.max(dim=1)\n                _, topk_inds = max_scores.topk(nms_pre)\n                bbox_pred = bbox_pred[topk_inds, :]\n                scores = scores[topk_inds, :]\n                y = y[topk_inds]\n                x = x[topk_inds]\n            x1 = (stride * x - base_len * bbox_pred[:, 0]).\\\n                clamp(min=0, max=img_shape[1] - 1)\n            y1 = (stride * y - base_len * bbox_pred[:, 1]).\\\n                clamp(min=0, max=img_shape[0] - 1)\n            x2 = (stride * x + base_len * bbox_pred[:, 2]).\\\n                clamp(min=0, max=img_shape[1] - 1)\n            y2 = (stride * y + base_len * bbox_pred[:, 3]).\\\n                clamp(min=0, max=img_shape[0] - 1)\n            bboxes = torch.stack([x1, y1, x2, y2], -1)\n            det_bboxes.append(bboxes)\n            det_scores.append(scores)\n        det_bboxes = torch.cat(det_bboxes)\n        if rescale:\n            det_bboxes /= det_bboxes.new_tensor(scale_factor)\n        det_scores = torch.cat(det_scores)\n        padding = det_scores.new_zeros(det_scores.shape[0], 1)\n        # remind that we set FG labels to [0, num_class-1] since mmdet v2.0\n        # BG cat_id: num_class\n        det_scores = torch.cat([det_scores, padding], dim=1)\n        det_bboxes, det_labels = multiclass_nms(det_bboxes, det_scores,\n                                                cfg.score_thr, cfg.nms,\n                                                cfg.max_per_img)\n        return det_bboxes, det_labels\n'"
mmdet/models/dense_heads/free_anchor_retina_head.py,24,"b'import torch\nimport torch.nn.functional as F\n\nfrom mmdet.core import bbox_overlaps\nfrom ..builder import HEADS\nfrom .retina_head import RetinaHead\n\n\n@HEADS.register_module()\nclass FreeAnchorRetinaHead(RetinaHead):\n\n    def __init__(self,\n                 num_classes,\n                 in_channels,\n                 stacked_convs=4,\n                 conv_cfg=None,\n                 norm_cfg=None,\n                 pre_anchor_topk=50,\n                 bbox_thr=0.6,\n                 gamma=2.0,\n                 alpha=0.5,\n                 **kwargs):\n        super(FreeAnchorRetinaHead,\n              self).__init__(num_classes, in_channels, stacked_convs, conv_cfg,\n                             norm_cfg, **kwargs)\n\n        self.pre_anchor_topk = pre_anchor_topk\n        self.bbox_thr = bbox_thr\n        self.gamma = gamma\n        self.alpha = alpha\n\n    def loss(self,\n             cls_scores,\n             bbox_preds,\n             gt_bboxes,\n             gt_labels,\n             img_metas,\n             gt_bboxes_ignore=None):\n        featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n        assert len(featmap_sizes) == len(self.anchor_generator.base_anchors)\n\n        anchor_list, _ = self.get_anchors(featmap_sizes, img_metas)\n        anchors = [torch.cat(anchor) for anchor in anchor_list]\n\n        # concatenate each level\n        cls_scores = [\n            cls.permute(0, 2, 3,\n                        1).reshape(cls.size(0), -1, self.cls_out_channels)\n            for cls in cls_scores\n        ]\n        bbox_preds = [\n            bbox_pred.permute(0, 2, 3, 1).reshape(bbox_pred.size(0), -1, 4)\n            for bbox_pred in bbox_preds\n        ]\n        cls_scores = torch.cat(cls_scores, dim=1)\n        bbox_preds = torch.cat(bbox_preds, dim=1)\n\n        cls_prob = torch.sigmoid(cls_scores)\n        box_prob = []\n        num_pos = 0\n        positive_losses = []\n        for _, (anchors_, gt_labels_, gt_bboxes_, cls_prob_,\n                bbox_preds_) in enumerate(\n                    zip(anchors, gt_labels, gt_bboxes, cls_prob, bbox_preds)):\n\n            with torch.no_grad():\n                # box_localization: a_{j}^{loc}, shape: [j, 4]\n                pred_boxes = self.bbox_coder.decode(anchors_, bbox_preds_)\n\n                # object_box_iou: IoU_{ij}^{loc}, shape: [i, j]\n                object_box_iou = bbox_overlaps(gt_bboxes_, pred_boxes)\n\n                # object_box_prob: P{a_{j} -> b_{i}}, shape: [i, j]\n                t1 = self.bbox_thr\n                t2 = object_box_iou.max(\n                    dim=1, keepdim=True).values.clamp(min=t1 + 1e-12)\n                object_box_prob = ((object_box_iou - t1) / (t2 - t1)).clamp(\n                    min=0, max=1)\n\n                # object_cls_box_prob: P{a_{j} -> b_{i}}, shape: [i, c, j]\n                num_obj = gt_labels_.size(0)\n                indices = torch.stack(\n                    [torch.arange(num_obj).type_as(gt_labels_), gt_labels_],\n                    dim=0)\n                object_cls_box_prob = torch.sparse_coo_tensor(\n                    indices, object_box_prob)\n\n                # image_box_iou: P{a_{j} \\in A_{+}}, shape: [c, j]\n                """"""\n                from ""start"" to ""end"" implement:\n                image_box_iou = torch.sparse.max(object_cls_box_prob,\n                                                 dim=0).t()\n\n                """"""\n                # start\n                box_cls_prob = torch.sparse.sum(\n                    object_cls_box_prob, dim=0).to_dense()\n\n                indices = torch.nonzero(box_cls_prob, as_tuple=False).t_()\n                if indices.numel() == 0:\n                    image_box_prob = torch.zeros(\n                        anchors_.size(0),\n                        self.cls_out_channels).type_as(object_box_prob)\n                else:\n                    nonzero_box_prob = torch.where(\n                        (gt_labels_.unsqueeze(dim=-1) == indices[0]),\n                        object_box_prob[:, indices[1]],\n                        torch.tensor(\n                            [0]).type_as(object_box_prob)).max(dim=0).values\n\n                    # upmap to shape [j, c]\n                    image_box_prob = torch.sparse_coo_tensor(\n                        indices.flip([0]),\n                        nonzero_box_prob,\n                        size=(anchors_.size(0),\n                              self.cls_out_channels)).to_dense()\n                # end\n\n                box_prob.append(image_box_prob)\n\n            # construct bags for objects\n            match_quality_matrix = bbox_overlaps(gt_bboxes_, anchors_)\n            _, matched = torch.topk(\n                match_quality_matrix,\n                self.pre_anchor_topk,\n                dim=1,\n                sorted=False)\n            del match_quality_matrix\n\n            # matched_cls_prob: P_{ij}^{cls}\n            matched_cls_prob = torch.gather(\n                cls_prob_[matched], 2,\n                gt_labels_.view(-1, 1, 1).repeat(1, self.pre_anchor_topk,\n                                                 1)).squeeze(2)\n\n            # matched_box_prob: P_{ij}^{loc}\n            matched_anchors = anchors_[matched]\n            matched_object_targets = self.bbox_coder.encode(\n                matched_anchors,\n                gt_bboxes_.unsqueeze(dim=1).expand_as(matched_anchors))\n            loss_bbox = self.loss_bbox(\n                bbox_preds_[matched],\n                matched_object_targets,\n                reduction_override=\'none\').sum(-1)\n            matched_box_prob = torch.exp(-loss_bbox)\n\n            # positive_losses: {-log( Mean-max(P_{ij}^{cls} * P_{ij}^{loc}) )}\n            num_pos += len(gt_bboxes_)\n            positive_losses.append(\n                self.positive_bag_loss(matched_cls_prob, matched_box_prob))\n        positive_loss = torch.cat(positive_losses).sum() / max(1, num_pos)\n\n        # box_prob: P{a_{j} \\in A_{+}}\n        box_prob = torch.stack(box_prob, dim=0)\n\n        # negative_loss:\n        # \\sum_{j}{ FL((1 - P{a_{j} \\in A_{+}}) * (1 - P_{j}^{bg})) } / n||B||\n        negative_loss = self.negative_bag_loss(cls_prob, box_prob).sum() / max(\n            1, num_pos * self.pre_anchor_topk)\n\n        losses = {\n            \'positive_bag_loss\': positive_loss,\n            \'negative_bag_loss\': negative_loss\n        }\n        return losses\n\n    def positive_bag_loss(self, matched_cls_prob, matched_box_prob):\n        # bag_prob = Mean-max(matched_prob)\n        matched_prob = matched_cls_prob * matched_box_prob\n        weight = 1 / torch.clamp(1 - matched_prob, 1e-12, None)\n        weight /= weight.sum(dim=1).unsqueeze(dim=-1)\n        bag_prob = (weight * matched_prob).sum(dim=1)\n        # positive_bag_loss = -self.alpha * log(bag_prob)\n        return self.alpha * F.binary_cross_entropy(\n            bag_prob, torch.ones_like(bag_prob), reduction=\'none\')\n\n    def negative_bag_loss(self, cls_prob, box_prob):\n        prob = cls_prob * (1 - box_prob)\n        negative_bag_loss = prob**self.gamma * F.binary_cross_entropy(\n            prob, torch.zeros_like(prob), reduction=\'none\')\n        return (1 - self.alpha) * negative_bag_loss\n'"
mmdet/models/dense_heads/fsaf_head.py,18,"b'import numpy as np\nimport torch\nfrom mmcv.cnn import normal_init\n\nfrom mmdet.core import (anchor_inside_flags, force_fp32, images_to_levels,\n                        multi_apply, unmap)\nfrom ..builder import HEADS\nfrom ..losses.utils import weight_reduce_loss\nfrom .retina_head import RetinaHead\n\n\n@HEADS.register_module()\nclass FSAFHead(RetinaHead):\n    """"""Anchor-free head used in `FSAF <https://arxiv.org/abs/1903.00621>`_.\n\n    The head contains two subnetworks. The first classifies anchor boxes and\n    the second regresses deltas for the anchors (num_anchors is 1 for anchor-\n    free methods)\n\n    Example:\n        >>> import torch\n        >>> self = FSAFHead(11, 7)\n        >>> x = torch.rand(1, 7, 32, 32)\n        >>> cls_score, bbox_pred = self.forward_single(x)\n        >>> # Each anchor predicts a score for each class except background\n        >>> cls_per_anchor = cls_score.shape[1] / self.num_anchors\n        >>> box_per_anchor = bbox_pred.shape[1] / self.num_anchors\n        >>> assert cls_per_anchor == self.num_classes\n        >>> assert box_per_anchor == 4\n    """"""\n\n    def forward_single(self, x):\n        cls_score, bbox_pred = super().forward_single(x)\n        # relu: TBLR encoder only accepts positive bbox_pred\n        return cls_score, self.relu(bbox_pred)\n\n    def init_weights(self):\n        super(FSAFHead, self).init_weights()\n        # The positive bias in self.retina_reg conv is to prevent predicted \\\n        #  bbox with 0 area\n        normal_init(self.retina_reg, std=0.01, bias=0.25)\n\n    def _get_targets_single(self,\n                            flat_anchors,\n                            valid_flags,\n                            gt_bboxes,\n                            gt_bboxes_ignore,\n                            gt_labels,\n                            img_meta,\n                            label_channels=1,\n                            unmap_outputs=True):\n        """"""Compute regression and classification targets for anchors in\n            a single image.\n\n        Most of the codes are the same with the base class\n          :obj: `AnchorHead`, except that it also collects and returns\n          the matched gt index in the image (from 0 to num_gt-1). If the\n          anchor bbox is not matched to any gt, the corresponding value in\n          pos_gt_inds is -1.\n        """"""\n        inside_flags = anchor_inside_flags(flat_anchors, valid_flags,\n                                           img_meta[\'img_shape\'][:2],\n                                           self.train_cfg.allowed_border)\n        if not inside_flags.any():\n            return (None, ) * 7\n        # Assign gt and sample anchors\n        anchors = flat_anchors[inside_flags.type(torch.bool), :]\n        assign_result = self.assigner.assign(\n            anchors, gt_bboxes, gt_bboxes_ignore,\n            None if self.sampling else gt_labels)\n\n        sampling_result = self.sampler.sample(assign_result, anchors,\n                                              gt_bboxes)\n\n        num_valid_anchors = anchors.shape[0]\n        bbox_targets = torch.zeros_like(anchors)\n        bbox_weights = torch.zeros_like(anchors)\n        labels = anchors.new_full((num_valid_anchors, ),\n                                  self.background_label,\n                                  dtype=torch.long)\n        label_weights = anchors.new_zeros((num_valid_anchors, label_channels),\n                                          dtype=torch.float)\n        pos_gt_inds = anchors.new_full((num_valid_anchors, ),\n                                       -1,\n                                       dtype=torch.long)\n\n        pos_inds = sampling_result.pos_inds\n        neg_inds = sampling_result.neg_inds\n\n        if len(pos_inds) > 0:\n            if not self.reg_decoded_bbox:\n                pos_bbox_targets = self.bbox_coder.encode(\n                    sampling_result.pos_bboxes, sampling_result.pos_gt_bboxes)\n            else:\n                pos_bbox_targets = sampling_result.pos_gt_bboxes\n            bbox_targets[pos_inds, :] = pos_bbox_targets\n            bbox_weights[pos_inds, :] = 1.0\n            # The assigned gt_index for each anchor. (0-based)\n            pos_gt_inds[pos_inds] = sampling_result.pos_assigned_gt_inds\n            if gt_labels is None:\n                # only rpn gives gt_labels as None, this time FG is 1\n                labels[pos_inds] = 1\n            else:\n                labels[pos_inds] = gt_labels[\n                    sampling_result.pos_assigned_gt_inds]\n            if self.train_cfg.pos_weight <= 0:\n                label_weights[pos_inds] = 1.0\n            else:\n                label_weights[pos_inds] = self.train_cfg.pos_weight\n\n        # shadowed_labels is a tensor composed of tuples\n        #  (anchor_inds, class_label) that indicate those anchors lying in the\n        #  outer region of a gt or overlapped by another gt with a smaller\n        #  area.\n        #\n        # Therefore, only the shadowed labels are ignored for loss calculation.\n        # the key `shadowed_labels` is defined in :obj:`CenterRegionAssigner`\n        shadowed_labels = assign_result.get_extra_property(\'shadowed_labels\')\n        if shadowed_labels is not None and shadowed_labels.numel():\n            if len(shadowed_labels.shape) == 2:\n                idx_, label_ = shadowed_labels[:, 0], shadowed_labels[:, 1]\n                assert (labels[idx_] != label_).all(), \\\n                    \'One label cannot be both positive and ignored\'\n                # If background_label is 0. Then all labels increase by 1\n                label_ += int(self.background_label == 0)\n                label_weights[idx_, label_] = 0\n            else:\n                label_weights[shadowed_labels] = 0\n\n        if len(neg_inds) > 0:\n            label_weights[neg_inds] = 1.0\n\n        # map up to original set of anchors\n        if unmap_outputs:\n            num_total_anchors = flat_anchors.size(0)\n            labels = unmap(labels, num_total_anchors, inside_flags)\n            label_weights = unmap(label_weights, num_total_anchors,\n                                  inside_flags)\n            bbox_targets = unmap(bbox_targets, num_total_anchors, inside_flags)\n            bbox_weights = unmap(bbox_weights, num_total_anchors, inside_flags)\n            pos_gt_inds = unmap(\n                pos_gt_inds, num_total_anchors, inside_flags, fill=-1)\n\n        return (labels, label_weights, bbox_targets, bbox_weights, pos_inds,\n                neg_inds, sampling_result, pos_gt_inds)\n\n    @force_fp32(apply_to=(\'cls_scores\', \'bbox_preds\'))\n    def loss(self,\n             cls_scores,\n             bbox_preds,\n             gt_bboxes,\n             gt_labels,\n             img_metas,\n             gt_bboxes_ignore=None):\n        for i in range(len(bbox_preds)):  # loop over fpn level\n            # avoid 0 area of the predicted bbox\n            bbox_preds[i] = bbox_preds[i].clamp(min=1e-4)\n        # TODO: It may directly use the base-class loss function.\n        featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n        assert len(featmap_sizes) == self.anchor_generator.num_levels\n        batch_size = len(gt_bboxes)\n        device = cls_scores[0].device\n        anchor_list, valid_flag_list = self.get_anchors(\n            featmap_sizes, img_metas, device=device)\n        label_channels = self.cls_out_channels if self.use_sigmoid_cls else 1\n        cls_reg_targets = self.get_targets(\n            anchor_list,\n            valid_flag_list,\n            gt_bboxes,\n            img_metas,\n            gt_bboxes_ignore_list=gt_bboxes_ignore,\n            gt_labels_list=gt_labels,\n            label_channels=label_channels)\n        if cls_reg_targets is None:\n            return None\n        (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list,\n         num_total_pos, num_total_neg,\n         pos_assigned_gt_inds_list) = cls_reg_targets\n\n        num_gts = np.array(list(map(len, gt_labels)))\n        num_total_samples = (\n            num_total_pos + num_total_neg if self.sampling else num_total_pos)\n        # anchor number of multi levels\n        num_level_anchors = [anchors.size(0) for anchors in anchor_list[0]]\n        # concat all level anchors and flags to a single tensor\n        concat_anchor_list = []\n        for i in range(len(anchor_list)):\n            concat_anchor_list.append(torch.cat(anchor_list[i]))\n        all_anchor_list = images_to_levels(concat_anchor_list,\n                                           num_level_anchors)\n        losses_cls, losses_bbox = multi_apply(\n            self.loss_single,\n            cls_scores,\n            bbox_preds,\n            all_anchor_list,\n            labels_list,\n            label_weights_list,\n            bbox_targets_list,\n            bbox_weights_list,\n            num_total_samples=num_total_samples)\n\n        # `pos_assigned_gt_inds_list` (length: fpn_levels) stores the assigned\n        # gt index of each anchor bbox in each fpn level.\n        cum_num_gts = list(np.cumsum(num_gts))  # length of batch_size\n        for i, assign in enumerate(pos_assigned_gt_inds_list):\n            # loop over fpn levels\n            for j in range(1, batch_size):\n                # loop over batch size\n                # Convert gt indices in each img to those in the batch\n                assign[j][assign[j] >= 0] += int(cum_num_gts[j - 1])\n            pos_assigned_gt_inds_list[i] = assign.flatten()\n            labels_list[i] = labels_list[i].flatten()\n        num_gts = sum(map(len, gt_labels))  # total number of gt in the batch\n        # The unique label index of each gt in the batch\n        label_sequence = torch.arange(num_gts, device=device)\n        # Collect the average loss of each gt in each level\n        with torch.no_grad():\n            loss_levels, = multi_apply(\n                self.collect_loss_level_single,\n                losses_cls,\n                losses_bbox,\n                pos_assigned_gt_inds_list,\n                labels_seq=label_sequence)\n            # Shape: (fpn_levels, num_gts). Loss of each gt at each fpn level\n            loss_levels = torch.stack(loss_levels, dim=0)\n            # Locate the best fpn level for loss back-propagation\n            if loss_levels.numel() == 0:  # zero gt\n                argmin = loss_levels.new_empty((num_gts, ), dtype=torch.long)\n            else:\n                _, argmin = loss_levels.min(dim=0)\n\n        # Reweight the loss of each (anchor, label) pair, so that only those\n        #  at the best gt level are back-propagated.\n        losses_cls, losses_bbox, pos_inds = multi_apply(\n            self.reweight_loss_single,\n            losses_cls,\n            losses_bbox,\n            pos_assigned_gt_inds_list,\n            labels_list,\n            list(range(len(losses_cls))),\n            min_levels=argmin)\n        num_pos = torch.cat(pos_inds, 0).sum().float()\n        acc = self.calculate_accuracy(cls_scores, labels_list, pos_inds)\n\n        if num_pos == 0:  # No gt\n            avg_factor = num_pos + float(num_total_neg)\n        else:\n            avg_factor = num_pos\n        for i in range(len(losses_cls)):\n            losses_cls[i] /= avg_factor\n            losses_bbox[i] /= avg_factor\n        return dict(\n            loss_cls=losses_cls,\n            loss_bbox=losses_bbox,\n            num_pos=num_pos / batch_size,\n            accuracy=acc)\n\n    def calculate_accuracy(self, cls_scores, labels_list, pos_inds):\n        with torch.no_grad():\n            num_pos = torch.cat(pos_inds, 0).sum().float().clamp(min=1e-3)\n            num_class = cls_scores[0].size(1)\n            scores = [\n                cls.permute(0, 2, 3, 1).reshape(-1, num_class)[pos]\n                for cls, pos in zip(cls_scores, pos_inds)\n            ]\n            labels = [\n                label.reshape(-1)[pos]\n                for label, pos in zip(labels_list, pos_inds)\n            ]\n\n            def argmax(x):\n                return x.argmax(1) if x.numel() > 0 else -100\n\n            num_correct = sum([(argmax(score) == label).sum()\n                               for score, label in zip(scores, labels)])\n            return num_correct.float() / num_pos\n\n    def collect_loss_level_single(self, cls_loss, reg_loss, assigned_gt_inds,\n                                  labels_seq):\n        """"""Get the average loss in each FPN level w.r.t. each gt label\n\n        Args:\n            cls_loss (Tensor): Classification loss of each feature map pixel,\n              shape (num_anchor, num_class)\n            reg_loss (Tensor): Regression loss of each feature map pixel,\n              shape (num_anchor, 4)\n            assigned_gt_inds (Tensor): It indicates which gt the prior is\n              assigned to (0-based, -1: no assignment). shape (num_anchor),\n            labels_seq: The rank of labels. shape (num_gt)\n\n        Returns:\n            shape: (num_gt), average loss of each gt in this level\n        """"""\n        if len(reg_loss.shape) == 2:  # iou loss has shape (num_prior, 4)\n            reg_loss = reg_loss.sum(dim=-1)  # sum loss in tblr dims\n        if len(cls_loss.shape) == 2:\n            cls_loss = cls_loss.sum(dim=-1)  # sum loss in class dims\n        loss = cls_loss + reg_loss\n        assert loss.size(0) == assigned_gt_inds.size(0)\n        # Default loss value is 1e6 for a layer where no anchor is positive\n        #  to ensure it will not be chosen to back-propagate gradient\n        losses_ = loss.new_full(labels_seq.shape, 1e6)\n        for i, l in enumerate(labels_seq):\n            match = assigned_gt_inds == l\n            if match.any():\n                losses_[i] = loss[match].mean()\n        return losses_,\n\n    def reweight_loss_single(self, cls_loss, reg_loss, assigned_gt_inds,\n                             labels, level, min_levels):\n        """"""Reweight loss values at each level.\n\n        Reassign loss values at each level by masking those where the\n        pre-calculated loss is too large. Then return the reduced losses.\n\n        Args:\n            cls_loss (Tensor): Element-wise classification loss.\n              Shape: (num_anchors, num_classes)\n            reg_loss (Tensor): Element-wise regression loss.\n              Shape: (num_anchors, 4)\n            assigned_gt_inds (Tensor): The gt indices that each anchor bbox\n              is assigned to. -1 denotes a negative anchor, otherwise it is the\n              gt index (0-based). Shape: (num_anchors, ),\n            labels (Tensor): Label assigned to anchors. Shape: (num_anchors, ).\n            level (int): The current level index in the pyramid\n              (0-4 for RetinaNet)\n            min_levels (Tensor): The best-matching level for each gt.\n              Shape: (num_gts, ),\n\n        Returns:\n            tuple:\n                - cls_loss: Reduced corrected classification loss. Scalar.\n                - reg_loss: Reduced corrected regression loss. Scalar.\n                - pos_flags (Tensor): Corrected bool tensor indicating the\n                  final postive anchors. Shape: (num_anchors, ).\n        """"""\n        loc_weight = torch.ones_like(reg_loss)\n        cls_weight = torch.ones_like(cls_loss)\n        pos_flags = assigned_gt_inds >= 0  # positive pixel flag\n        pos_indices = torch.nonzero(pos_flags, as_tuple=False).flatten()\n\n        if pos_flags.any():  # pos pixels exist\n            pos_assigned_gt_inds = assigned_gt_inds[pos_flags]\n            zeroing_indices = (min_levels[pos_assigned_gt_inds] != level)\n            neg_indices = pos_indices[zeroing_indices]\n\n            if neg_indices.numel():\n                pos_flags[neg_indices] = 0\n                loc_weight[neg_indices] = 0\n                # Only the weight corresponding to the label is\n                #  zeroed out if not selected\n                zeroing_labels = labels[neg_indices]\n                assert (zeroing_labels >= 0).all()\n                cls_weight[neg_indices, zeroing_labels] = 0\n\n        # Weighted loss for both cls and reg loss\n        cls_loss = weight_reduce_loss(cls_loss, cls_weight, reduction=\'sum\')\n        reg_loss = weight_reduce_loss(reg_loss, loc_weight, reduction=\'sum\')\n\n        return cls_loss, reg_loss, pos_flags\n'"
mmdet/models/dense_heads/ga_retina_head.py,1,"b'import torch.nn as nn\nfrom mmcv.cnn import ConvModule, bias_init_with_prob, normal_init\n\nfrom mmdet.ops import MaskedConv2d\nfrom ..builder import HEADS\nfrom .guided_anchor_head import FeatureAdaption, GuidedAnchorHead\n\n\n@HEADS.register_module()\nclass GARetinaHead(GuidedAnchorHead):\n    """"""Guided-Anchor-based RetinaNet head.""""""\n\n    def __init__(self,\n                 num_classes,\n                 in_channels,\n                 stacked_convs=4,\n                 conv_cfg=None,\n                 norm_cfg=None,\n                 **kwargs):\n        self.stacked_convs = stacked_convs\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        super(GARetinaHead, self).__init__(num_classes, in_channels, **kwargs)\n\n    def _init_layers(self):\n        self.relu = nn.ReLU(inplace=True)\n        self.cls_convs = nn.ModuleList()\n        self.reg_convs = nn.ModuleList()\n        for i in range(self.stacked_convs):\n            chn = self.in_channels if i == 0 else self.feat_channels\n            self.cls_convs.append(\n                ConvModule(\n                    chn,\n                    self.feat_channels,\n                    3,\n                    stride=1,\n                    padding=1,\n                    conv_cfg=self.conv_cfg,\n                    norm_cfg=self.norm_cfg))\n            self.reg_convs.append(\n                ConvModule(\n                    chn,\n                    self.feat_channels,\n                    3,\n                    stride=1,\n                    padding=1,\n                    conv_cfg=self.conv_cfg,\n                    norm_cfg=self.norm_cfg))\n\n        self.conv_loc = nn.Conv2d(self.feat_channels, 1, 1)\n        self.conv_shape = nn.Conv2d(self.feat_channels, self.num_anchors * 2,\n                                    1)\n        self.feature_adaption_cls = FeatureAdaption(\n            self.feat_channels,\n            self.feat_channels,\n            kernel_size=3,\n            deformable_groups=self.deformable_groups)\n        self.feature_adaption_reg = FeatureAdaption(\n            self.feat_channels,\n            self.feat_channels,\n            kernel_size=3,\n            deformable_groups=self.deformable_groups)\n        self.retina_cls = MaskedConv2d(\n            self.feat_channels,\n            self.num_anchors * self.cls_out_channels,\n            3,\n            padding=1)\n        self.retina_reg = MaskedConv2d(\n            self.feat_channels, self.num_anchors * 4, 3, padding=1)\n\n    def init_weights(self):\n        for m in self.cls_convs:\n            normal_init(m.conv, std=0.01)\n        for m in self.reg_convs:\n            normal_init(m.conv, std=0.01)\n\n        self.feature_adaption_cls.init_weights()\n        self.feature_adaption_reg.init_weights()\n\n        bias_cls = bias_init_with_prob(0.01)\n        normal_init(self.conv_loc, std=0.01, bias=bias_cls)\n        normal_init(self.conv_shape, std=0.01)\n        normal_init(self.retina_cls, std=0.01, bias=bias_cls)\n        normal_init(self.retina_reg, std=0.01)\n\n    def forward_single(self, x):\n        cls_feat = x\n        reg_feat = x\n        for cls_conv in self.cls_convs:\n            cls_feat = cls_conv(cls_feat)\n        for reg_conv in self.reg_convs:\n            reg_feat = reg_conv(reg_feat)\n\n        loc_pred = self.conv_loc(cls_feat)\n        shape_pred = self.conv_shape(reg_feat)\n\n        cls_feat = self.feature_adaption_cls(cls_feat, shape_pred)\n        reg_feat = self.feature_adaption_reg(reg_feat, shape_pred)\n\n        if not self.training:\n            mask = loc_pred.sigmoid()[0] >= self.loc_filter_thr\n        else:\n            mask = None\n        cls_score = self.retina_cls(cls_feat, mask)\n        bbox_pred = self.retina_reg(reg_feat, mask)\n        return cls_score, bbox_pred, shape_pred, loc_pred\n'"
mmdet/models/dense_heads/ga_rpn_head.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import normal_init\n\nfrom mmdet.ops import nms\nfrom ..builder import HEADS\nfrom .guided_anchor_head import GuidedAnchorHead\n\n\n@HEADS.register_module()\nclass GARPNHead(GuidedAnchorHead):\n    """"""Guided-Anchor-based RPN head.""""""\n\n    def __init__(self, in_channels, **kwargs):\n        super(GARPNHead, self).__init__(\n            1, in_channels, background_label=0, **kwargs)\n\n    def _init_layers(self):\n        self.rpn_conv = nn.Conv2d(\n            self.in_channels, self.feat_channels, 3, padding=1)\n        super(GARPNHead, self)._init_layers()\n\n    def init_weights(self):\n        normal_init(self.rpn_conv, std=0.01)\n        super(GARPNHead, self).init_weights()\n\n    def forward_single(self, x):\n        x = self.rpn_conv(x)\n        x = F.relu(x, inplace=True)\n        (cls_score, bbox_pred, shape_pred,\n         loc_pred) = super(GARPNHead, self).forward_single(x)\n        return cls_score, bbox_pred, shape_pred, loc_pred\n\n    def loss(self,\n             cls_scores,\n             bbox_preds,\n             shape_preds,\n             loc_preds,\n             gt_bboxes,\n             img_metas,\n             gt_bboxes_ignore=None):\n        losses = super(GARPNHead, self).loss(\n            cls_scores,\n            bbox_preds,\n            shape_preds,\n            loc_preds,\n            gt_bboxes,\n            None,\n            img_metas,\n            gt_bboxes_ignore=gt_bboxes_ignore)\n        return dict(\n            loss_rpn_cls=losses[\'loss_cls\'],\n            loss_rpn_bbox=losses[\'loss_bbox\'],\n            loss_anchor_shape=losses[\'loss_shape\'],\n            loss_anchor_loc=losses[\'loss_loc\'])\n\n    def _get_bboxes_single(self,\n                           cls_scores,\n                           bbox_preds,\n                           mlvl_anchors,\n                           mlvl_masks,\n                           img_shape,\n                           scale_factor,\n                           cfg,\n                           rescale=False):\n        cfg = self.test_cfg if cfg is None else cfg\n        mlvl_proposals = []\n        for idx in range(len(cls_scores)):\n            rpn_cls_score = cls_scores[idx]\n            rpn_bbox_pred = bbox_preds[idx]\n            anchors = mlvl_anchors[idx]\n            mask = mlvl_masks[idx]\n            assert rpn_cls_score.size()[-2:] == rpn_bbox_pred.size()[-2:]\n            # if no location is kept, end.\n            if mask.sum() == 0:\n                continue\n            rpn_cls_score = rpn_cls_score.permute(1, 2, 0)\n            if self.use_sigmoid_cls:\n                rpn_cls_score = rpn_cls_score.reshape(-1)\n                scores = rpn_cls_score.sigmoid()\n            else:\n                rpn_cls_score = rpn_cls_score.reshape(-1, 2)\n                # remind that we set FG labels to [0, num_class-1]\n                # since mmdet v2.0\n                # BG cat_id: num_class\n                scores = rpn_cls_score.softmax(dim=1)[:, :-1]\n            # filter scores, bbox_pred w.r.t. mask.\n            # anchors are filtered in get_anchors() beforehand.\n            scores = scores[mask]\n            rpn_bbox_pred = rpn_bbox_pred.permute(1, 2, 0).reshape(-1,\n                                                                   4)[mask, :]\n            if scores.dim() == 0:\n                rpn_bbox_pred = rpn_bbox_pred.unsqueeze(0)\n                anchors = anchors.unsqueeze(0)\n                scores = scores.unsqueeze(0)\n            # filter anchors, bbox_pred, scores w.r.t. scores\n            if cfg.nms_pre > 0 and scores.shape[0] > cfg.nms_pre:\n                _, topk_inds = scores.topk(cfg.nms_pre)\n                rpn_bbox_pred = rpn_bbox_pred[topk_inds, :]\n                anchors = anchors[topk_inds, :]\n                scores = scores[topk_inds]\n            # get proposals w.r.t. anchors and rpn_bbox_pred\n            proposals = self.bbox_coder.decode(\n                anchors, rpn_bbox_pred, max_shape=img_shape)\n            # filter out too small bboxes\n            if cfg.min_bbox_size > 0:\n                w = proposals[:, 2] - proposals[:, 0]\n                h = proposals[:, 3] - proposals[:, 1]\n                valid_inds = torch.nonzero(\n                    (w >= cfg.min_bbox_size) & (h >= cfg.min_bbox_size),\n                    as_tuple=False).squeeze()\n                proposals = proposals[valid_inds, :]\n                scores = scores[valid_inds]\n            proposals = torch.cat([proposals, scores.unsqueeze(-1)], dim=-1)\n            # NMS in current level\n            proposals, _ = nms(proposals, cfg.nms_thr)\n            proposals = proposals[:cfg.nms_post, :]\n            mlvl_proposals.append(proposals)\n        proposals = torch.cat(mlvl_proposals, 0)\n        if cfg.nms_across_levels:\n            # NMS across multi levels\n            proposals, _ = nms(proposals, cfg.nms_thr)\n            proposals = proposals[:cfg.max_num, :]\n        else:\n            scores = proposals[:, 4]\n            num = min(cfg.max_num, proposals.shape[0])\n            _, topk_inds = scores.topk(num)\n            proposals = proposals[topk_inds, :]\n        return proposals\n'"
mmdet/models/dense_heads/guided_anchor_head.py,22,"b'import torch\nimport torch.nn as nn\nfrom mmcv.cnn import bias_init_with_prob, normal_init\n\nfrom mmdet.core import (anchor_inside_flags, build_anchor_generator,\n                        build_assigner, build_bbox_coder, build_sampler,\n                        calc_region, force_fp32, images_to_levels, multi_apply,\n                        multiclass_nms, unmap)\nfrom mmdet.ops import DeformConv, MaskedConv2d\nfrom ..builder import HEADS, build_loss\nfrom .anchor_head import AnchorHead\n\n\nclass FeatureAdaption(nn.Module):\n    """"""Feature Adaption Module.\n\n    Feature Adaption Module is implemented based on DCN v1.\n    It uses anchor shape prediction rather than feature map to\n    predict offsets of deformable conv layer.\n\n    Args:\n        in_channels (int): Number of channels in the input feature map.\n        out_channels (int): Number of channels in the output feature map.\n        kernel_size (int): Deformable conv kernel size.\n        deformable_groups (int): Deformable conv group size.\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size=3,\n                 deformable_groups=4):\n        super(FeatureAdaption, self).__init__()\n        offset_channels = kernel_size * kernel_size * 2\n        self.conv_offset = nn.Conv2d(\n            2, deformable_groups * offset_channels, 1, bias=False)\n        self.conv_adaption = DeformConv(\n            in_channels,\n            out_channels,\n            kernel_size=kernel_size,\n            padding=(kernel_size - 1) // 2,\n            deformable_groups=deformable_groups)\n        self.relu = nn.ReLU(inplace=True)\n\n    def init_weights(self):\n        normal_init(self.conv_offset, std=0.1)\n        normal_init(self.conv_adaption, std=0.01)\n\n    def forward(self, x, shape):\n        offset = self.conv_offset(shape.detach())\n        x = self.relu(self.conv_adaption(x, offset))\n        return x\n\n\n@HEADS.register_module()\nclass GuidedAnchorHead(AnchorHead):\n    """"""Guided-Anchor-based head (GA-RPN, GA-RetinaNet, etc.).\n\n    This GuidedAnchorHead will predict high-quality feature guided\n    anchors and locations where anchors will be kept in inference.\n    There are mainly 3 categories of bounding-boxes.\n\n    - Sampled 9 pairs for target assignment. (approxes)\n    - The square boxes where the predicted anchors are based on. (squares)\n    - Guided anchors.\n\n    Please refer to https://arxiv.org/abs/1901.03278 for more details.\n\n    Args:\n        num_classes (int): Number of classes.\n        in_channels (int): Number of channels in the input feature map.\n        feat_channels (int): Number of hidden channels.\n        approx_anchor_generator (dict): Config dict for approx generator\n        square_anchor_generator (dict): Config dict for square generator\n        anchor_coder (dict): Config dict for anchor coder\n        bbox_coder (dict): Config dict for bbox coder\n        deformable_groups: (int): Group number of DCN in\n            FeatureAdaption module.\n        loc_filter_thr (float): Threshold to filter out unconcerned regions.\n        background_label (int | None): Label ID of background, set as 0 for\n            RPN and num_classes for other heads. It will automatically set as\n            num_classes if None is given.\n        loss_loc (dict): Config of location loss.\n        loss_shape (dict): Config of anchor shape loss.\n        loss_cls (dict): Config of classification loss.\n        loss_bbox (dict): Config of bbox regression loss.\n    """"""\n\n    def __init__(\n        self,\n        num_classes,\n        in_channels,\n        feat_channels=256,\n        approx_anchor_generator=dict(\n            type=\'AnchorGenerator\',\n            octave_base_scale=8,\n            scales_per_octave=3,\n            ratios=[0.5, 1.0, 2.0],\n            strides=[4, 8, 16, 32, 64]),\n        square_anchor_generator=dict(\n            type=\'AnchorGenerator\',\n            ratios=[1.0],\n            scales=[8],\n            strides=[4, 8, 16, 32, 64]),\n        anchor_coder=dict(\n            type=\'DeltaXYWHBBoxCoder\',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[1.0, 1.0, 1.0, 1.0]\n        ),\n        bbox_coder=dict(\n            type=\'DeltaXYWHBBoxCoder\',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[1.0, 1.0, 1.0, 1.0]\n        ),\n        reg_decoded_bbox=False,\n        deformable_groups=4,\n        loc_filter_thr=0.01,\n        background_label=None,\n        train_cfg=None,\n        test_cfg=None,\n        loss_loc=dict(\n            type=\'FocalLoss\',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_shape=dict(type=\'BoundedIoULoss\', beta=0.2, loss_weight=1.0),\n        loss_cls=dict(\n            type=\'CrossEntropyLoss\', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type=\'SmoothL1Loss\', beta=1.0,\n                       loss_weight=1.0)):  # yapf: disable\n        super(AnchorHead, self).__init__()\n        self.in_channels = in_channels\n        self.num_classes = num_classes\n        self.feat_channels = feat_channels\n        self.deformable_groups = deformable_groups\n        self.loc_filter_thr = loc_filter_thr\n\n        # build approx_anchor_generator and square_anchor_generator\n        assert (approx_anchor_generator[\'octave_base_scale\'] ==\n                square_anchor_generator[\'scales\'][0])\n        assert (approx_anchor_generator[\'strides\'] ==\n                square_anchor_generator[\'strides\'])\n        self.approx_anchor_generator = build_anchor_generator(\n            approx_anchor_generator)\n        self.square_anchor_generator = build_anchor_generator(\n            square_anchor_generator)\n        self.approxs_per_octave = self.approx_anchor_generator \\\n            .num_base_anchors[0]\n\n        self.reg_decoded_bbox = reg_decoded_bbox\n\n        self.background_label = (\n            num_classes if background_label is None else background_label)\n        # background_label should be either 0 or num_classes\n        assert (self.background_label == 0\n                or self.background_label == num_classes)\n\n        # one anchor per location\n        self.num_anchors = 1\n        self.use_sigmoid_cls = loss_cls.get(\'use_sigmoid\', False)\n        self.loc_focal_loss = loss_loc[\'type\'] in [\'FocalLoss\']\n        self.sampling = loss_cls[\'type\'] not in [\'FocalLoss\']\n        self.ga_sampling = train_cfg is not None and hasattr(\n            train_cfg, \'ga_sampler\')\n        if self.use_sigmoid_cls:\n            self.cls_out_channels = self.num_classes\n        else:\n            self.cls_out_channels = self.num_classes + 1\n\n        # build bbox_coder\n        self.anchor_coder = build_bbox_coder(anchor_coder)\n        self.bbox_coder = build_bbox_coder(bbox_coder)\n\n        # build losses\n        self.loss_loc = build_loss(loss_loc)\n        self.loss_shape = build_loss(loss_shape)\n        self.loss_cls = build_loss(loss_cls)\n        self.loss_bbox = build_loss(loss_bbox)\n\n        self.train_cfg = train_cfg\n        self.test_cfg = test_cfg\n\n        if self.train_cfg:\n            self.assigner = build_assigner(self.train_cfg.assigner)\n            # use PseudoSampler when sampling is False\n            if self.sampling and hasattr(self.train_cfg, \'sampler\'):\n                sampler_cfg = self.train_cfg.sampler\n            else:\n                sampler_cfg = dict(type=\'PseudoSampler\')\n            self.sampler = build_sampler(sampler_cfg, context=self)\n\n            self.ga_assigner = build_assigner(self.train_cfg.ga_assigner)\n            if self.ga_sampling:\n                ga_sampler_cfg = self.train_cfg.ga_sampler\n            else:\n                ga_sampler_cfg = dict(type=\'PseudoSampler\')\n            self.ga_sampler = build_sampler(ga_sampler_cfg, context=self)\n\n        self.fp16_enabled = False\n\n        self._init_layers()\n\n    def _init_layers(self):\n        self.relu = nn.ReLU(inplace=True)\n        self.conv_loc = nn.Conv2d(self.in_channels, 1, 1)\n        self.conv_shape = nn.Conv2d(self.in_channels, self.num_anchors * 2, 1)\n        self.feature_adaption = FeatureAdaption(\n            self.in_channels,\n            self.feat_channels,\n            kernel_size=3,\n            deformable_groups=self.deformable_groups)\n        self.conv_cls = MaskedConv2d(self.feat_channels,\n                                     self.num_anchors * self.cls_out_channels,\n                                     1)\n        self.conv_reg = MaskedConv2d(self.feat_channels, self.num_anchors * 4,\n                                     1)\n\n    def init_weights(self):\n        normal_init(self.conv_cls, std=0.01)\n        normal_init(self.conv_reg, std=0.01)\n\n        bias_cls = bias_init_with_prob(0.01)\n        normal_init(self.conv_loc, std=0.01, bias=bias_cls)\n        normal_init(self.conv_shape, std=0.01)\n\n        self.feature_adaption.init_weights()\n\n    def forward_single(self, x):\n        loc_pred = self.conv_loc(x)\n        shape_pred = self.conv_shape(x)\n        x = self.feature_adaption(x, shape_pred)\n        # masked conv is only used during inference for speed-up\n        if not self.training:\n            mask = loc_pred.sigmoid()[0] >= self.loc_filter_thr\n        else:\n            mask = None\n        cls_score = self.conv_cls(x, mask)\n        bbox_pred = self.conv_reg(x, mask)\n        return cls_score, bbox_pred, shape_pred, loc_pred\n\n    def forward(self, feats):\n        return multi_apply(self.forward_single, feats)\n\n    def get_sampled_approxs(self, featmap_sizes, img_metas, device=\'cuda\'):\n        """"""Get sampled approxs and inside flags according to feature map sizes.\n\n        Args:\n            featmap_sizes (list[tuple]): Multi-level feature map sizes.\n            img_metas (list[dict]): Image meta info.\n            device (torch.device | str): device for returned tensors\n\n        Returns:\n            tuple: approxes of each image, inside flags of each image\n        """"""\n        num_imgs = len(img_metas)\n\n        # since feature map sizes of all images are the same, we only compute\n        # approxes for one time\n        multi_level_approxs = self.approx_anchor_generator.grid_anchors(\n            featmap_sizes, device=device)\n        approxs_list = [multi_level_approxs for _ in range(num_imgs)]\n\n        # for each image, we compute inside flags of multi level approxes\n        inside_flag_list = []\n        for img_id, img_meta in enumerate(img_metas):\n            multi_level_flags = []\n            multi_level_approxs = approxs_list[img_id]\n\n            # obtain valid flags for each approx first\n            multi_level_approx_flags = self.approx_anchor_generator \\\n                .valid_flags(featmap_sizes,\n                             img_meta[\'pad_shape\'],\n                             device=device)\n\n            for i, flags in enumerate(multi_level_approx_flags):\n                approxs = multi_level_approxs[i]\n                inside_flags_list = []\n                for i in range(self.approxs_per_octave):\n                    split_valid_flags = flags[i::self.approxs_per_octave]\n                    split_approxs = approxs[i::self.approxs_per_octave, :]\n                    inside_flags = anchor_inside_flags(\n                        split_approxs, split_valid_flags,\n                        img_meta[\'img_shape\'][:2],\n                        self.train_cfg.allowed_border)\n                    inside_flags_list.append(inside_flags)\n                # inside_flag for a position is true if any anchor in this\n                # position is true\n                inside_flags = (\n                    torch.stack(inside_flags_list, 0).sum(dim=0) > 0)\n                multi_level_flags.append(inside_flags)\n            inside_flag_list.append(multi_level_flags)\n        return approxs_list, inside_flag_list\n\n    def get_anchors(self,\n                    featmap_sizes,\n                    shape_preds,\n                    loc_preds,\n                    img_metas,\n                    use_loc_filter=False,\n                    device=\'cuda\'):\n        """"""Get squares according to feature map sizes and guided\n        anchors.\n\n        Args:\n            featmap_sizes (list[tuple]): Multi-level feature map sizes.\n            shape_preds (list[tensor]): Multi-level shape predictions.\n            loc_preds (list[tensor]): Multi-level location predictions.\n            img_metas (list[dict]): Image meta info.\n            use_loc_filter (bool): Use loc filter or not.\n            device (torch.device | str): device for returned tensors\n\n        Returns:\n            tuple: square approxs of each image, guided anchors of each image,\n                loc masks of each image\n        """"""\n        num_imgs = len(img_metas)\n        num_levels = len(featmap_sizes)\n\n        # since feature map sizes of all images are the same, we only compute\n        # squares for one time\n        multi_level_squares = self.square_anchor_generator.grid_anchors(\n            featmap_sizes, device=device)\n        squares_list = [multi_level_squares for _ in range(num_imgs)]\n\n        # for each image, we compute multi level guided anchors\n        guided_anchors_list = []\n        loc_mask_list = []\n        for img_id, img_meta in enumerate(img_metas):\n            multi_level_guided_anchors = []\n            multi_level_loc_mask = []\n            for i in range(num_levels):\n                squares = squares_list[img_id][i]\n                shape_pred = shape_preds[i][img_id]\n                loc_pred = loc_preds[i][img_id]\n                guided_anchors, loc_mask = self._get_guided_anchors_single(\n                    squares,\n                    shape_pred,\n                    loc_pred,\n                    use_loc_filter=use_loc_filter)\n                multi_level_guided_anchors.append(guided_anchors)\n                multi_level_loc_mask.append(loc_mask)\n            guided_anchors_list.append(multi_level_guided_anchors)\n            loc_mask_list.append(multi_level_loc_mask)\n        return squares_list, guided_anchors_list, loc_mask_list\n\n    def _get_guided_anchors_single(self,\n                                   squares,\n                                   shape_pred,\n                                   loc_pred,\n                                   use_loc_filter=False):\n        """"""Get guided anchors and loc masks for a single level.\n\n        Args:\n            square (tensor): Squares of a single level.\n            shape_pred (tensor): Shape predections of a single level.\n            loc_pred (tensor): Loc predections of a single level.\n            use_loc_filter (list[tensor]): Use loc filter or not.\n\n        Returns:\n            tuple: guided anchors, location masks\n        """"""\n        # calculate location filtering mask\n        loc_pred = loc_pred.sigmoid().detach()\n        if use_loc_filter:\n            loc_mask = loc_pred >= self.loc_filter_thr\n        else:\n            loc_mask = loc_pred >= 0.0\n        mask = loc_mask.permute(1, 2, 0).expand(-1, -1, self.num_anchors)\n        mask = mask.contiguous().view(-1)\n        # calculate guided anchors\n        squares = squares[mask]\n        anchor_deltas = shape_pred.permute(1, 2, 0).contiguous().view(\n            -1, 2).detach()[mask]\n        bbox_deltas = anchor_deltas.new_full(squares.size(), 0)\n        bbox_deltas[:, 2:] = anchor_deltas\n        guided_anchors = self.anchor_coder.decode(\n            squares, bbox_deltas, wh_ratio_clip=1e-6)\n        return guided_anchors, mask\n\n    def ga_loc_targets(self, gt_bboxes_list, featmap_sizes):\n        """"""Compute location targets for guided anchoring.\n\n        Each feature map is divided into positive, negative and ignore regions.\n        - positive regions: target 1, weight 1\n        - ignore regions: target 0, weight 0\n        - negative regions: target 0, weight 0.1\n\n        Args:\n            gt_bboxes_list (list[Tensor]): Gt bboxes of each image.\n            featmap_sizes (list[tuple]): Multi level sizes of each feature\n                maps.\n\n        Returns:\n            tuple\n        """"""\n        anchor_scale = self.approx_anchor_generator.octave_base_scale\n        anchor_strides = self.approx_anchor_generator.strides\n        # Currently only supports same stride in x and y direction.\n        for stride in anchor_strides:\n            assert (stride[0] == stride[1])\n        anchor_strides = [stride[0] for stride in anchor_strides]\n\n        center_ratio = self.train_cfg.center_ratio\n        ignore_ratio = self.train_cfg.ignore_ratio\n        img_per_gpu = len(gt_bboxes_list)\n        num_lvls = len(featmap_sizes)\n        r1 = (1 - center_ratio) / 2\n        r2 = (1 - ignore_ratio) / 2\n        all_loc_targets = []\n        all_loc_weights = []\n        all_ignore_map = []\n        for lvl_id in range(num_lvls):\n            h, w = featmap_sizes[lvl_id]\n            loc_targets = torch.zeros(\n                img_per_gpu,\n                1,\n                h,\n                w,\n                device=gt_bboxes_list[0].device,\n                dtype=torch.float32)\n            loc_weights = torch.full_like(loc_targets, -1)\n            ignore_map = torch.zeros_like(loc_targets)\n            all_loc_targets.append(loc_targets)\n            all_loc_weights.append(loc_weights)\n            all_ignore_map.append(ignore_map)\n        for img_id in range(img_per_gpu):\n            gt_bboxes = gt_bboxes_list[img_id]\n            scale = torch.sqrt((gt_bboxes[:, 2] - gt_bboxes[:, 0]) *\n                               (gt_bboxes[:, 3] - gt_bboxes[:, 1]))\n            min_anchor_size = scale.new_full(\n                (1, ), float(anchor_scale * anchor_strides[0]))\n            # assign gt bboxes to different feature levels w.r.t. their scales\n            target_lvls = torch.floor(\n                torch.log2(scale) - torch.log2(min_anchor_size) + 0.5)\n            target_lvls = target_lvls.clamp(min=0, max=num_lvls - 1).long()\n            for gt_id in range(gt_bboxes.size(0)):\n                lvl = target_lvls[gt_id].item()\n                # rescaled to corresponding feature map\n                gt_ = gt_bboxes[gt_id, :4] / anchor_strides[lvl]\n                # calculate ignore regions\n                ignore_x1, ignore_y1, ignore_x2, ignore_y2 = calc_region(\n                    gt_, r2, featmap_sizes[lvl])\n                # calculate positive (center) regions\n                ctr_x1, ctr_y1, ctr_x2, ctr_y2 = calc_region(\n                    gt_, r1, featmap_sizes[lvl])\n                all_loc_targets[lvl][img_id, 0, ctr_y1:ctr_y2 + 1,\n                                     ctr_x1:ctr_x2 + 1] = 1\n                all_loc_weights[lvl][img_id, 0, ignore_y1:ignore_y2 + 1,\n                                     ignore_x1:ignore_x2 + 1] = 0\n                all_loc_weights[lvl][img_id, 0, ctr_y1:ctr_y2 + 1,\n                                     ctr_x1:ctr_x2 + 1] = 1\n                # calculate ignore map on nearby low level feature\n                if lvl > 0:\n                    d_lvl = lvl - 1\n                    # rescaled to corresponding feature map\n                    gt_ = gt_bboxes[gt_id, :4] / anchor_strides[d_lvl]\n                    ignore_x1, ignore_y1, ignore_x2, ignore_y2 = calc_region(\n                        gt_, r2, featmap_sizes[d_lvl])\n                    all_ignore_map[d_lvl][img_id, 0, ignore_y1:ignore_y2 + 1,\n                                          ignore_x1:ignore_x2 + 1] = 1\n                # calculate ignore map on nearby high level feature\n                if lvl < num_lvls - 1:\n                    u_lvl = lvl + 1\n                    # rescaled to corresponding feature map\n                    gt_ = gt_bboxes[gt_id, :4] / anchor_strides[u_lvl]\n                    ignore_x1, ignore_y1, ignore_x2, ignore_y2 = calc_region(\n                        gt_, r2, featmap_sizes[u_lvl])\n                    all_ignore_map[u_lvl][img_id, 0, ignore_y1:ignore_y2 + 1,\n                                          ignore_x1:ignore_x2 + 1] = 1\n        for lvl_id in range(num_lvls):\n            # ignore negative regions w.r.t. ignore map\n            all_loc_weights[lvl_id][(all_loc_weights[lvl_id] < 0)\n                                    & (all_ignore_map[lvl_id] > 0)] = 0\n            # set negative regions with weight 0.1\n            all_loc_weights[lvl_id][all_loc_weights[lvl_id] < 0] = 0.1\n        # loc average factor to balance loss\n        loc_avg_factor = sum(\n            [t.size(0) * t.size(-1) * t.size(-2)\n             for t in all_loc_targets]) / 200\n        return all_loc_targets, all_loc_weights, loc_avg_factor\n\n    def _ga_shape_target_single(self,\n                                flat_approxs,\n                                inside_flags,\n                                flat_squares,\n                                gt_bboxes,\n                                gt_bboxes_ignore,\n                                img_meta,\n                                unmap_outputs=True):\n        """"""Compute guided anchoring targets.\n\n        This function returns sampled anchors and gt bboxes directly\n        rather than calculates regression targets.\n\n        Args:\n            flat_approxs (Tensor): flat approxs of a single image,\n                shape (n, 4)\n            inside_flags (Tensor): inside flags of a single image,\n                shape (n, ).\n            flat_squares (Tensor): flat squares of a single image,\n                shape (approxs_per_octave * n, 4)\n            gt_bboxes (Tensor): Ground truth bboxes of a single image.\n            img_meta (dict): Meta info of a single image.\n            approxs_per_octave (int): number of approxs per octave\n            cfg (dict): RPN train configs.\n            unmap_outputs (bool): unmap outputs or not.\n\n        Returns:\n            tuple\n        """"""\n        if not inside_flags.any():\n            return (None, ) * 5\n        # assign gt and sample anchors\n        expand_inside_flags = inside_flags[:, None].expand(\n            -1, self.approxs_per_octave).reshape(-1)\n        approxs = flat_approxs[expand_inside_flags, :]\n        squares = flat_squares[inside_flags, :]\n\n        assign_result = self.ga_assigner.assign(approxs, squares,\n                                                self.approxs_per_octave,\n                                                gt_bboxes, gt_bboxes_ignore)\n        sampling_result = self.ga_sampler.sample(assign_result, squares,\n                                                 gt_bboxes)\n\n        bbox_anchors = torch.zeros_like(squares)\n        bbox_gts = torch.zeros_like(squares)\n        bbox_weights = torch.zeros_like(squares)\n\n        pos_inds = sampling_result.pos_inds\n        neg_inds = sampling_result.neg_inds\n        if len(pos_inds) > 0:\n            bbox_anchors[pos_inds, :] = sampling_result.pos_bboxes\n            bbox_gts[pos_inds, :] = sampling_result.pos_gt_bboxes\n            bbox_weights[pos_inds, :] = 1.0\n\n        # map up to original set of anchors\n        if unmap_outputs:\n            num_total_anchors = flat_squares.size(0)\n            bbox_anchors = unmap(bbox_anchors, num_total_anchors, inside_flags)\n            bbox_gts = unmap(bbox_gts, num_total_anchors, inside_flags)\n            bbox_weights = unmap(bbox_weights, num_total_anchors, inside_flags)\n\n        return (bbox_anchors, bbox_gts, bbox_weights, pos_inds, neg_inds)\n\n    def ga_shape_targets(self,\n                         approx_list,\n                         inside_flag_list,\n                         square_list,\n                         gt_bboxes_list,\n                         img_metas,\n                         gt_bboxes_ignore_list=None,\n                         unmap_outputs=True):\n        """"""Compute guided anchoring targets.\n\n        Args:\n            approx_list (list[list]): Multi level approxs of each image.\n            inside_flag_list (list[list]): Multi level inside flags of each\n                image.\n            square_list (list[list]): Multi level squares of each image.\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes of each image.\n            img_metas (list[dict]): Meta info of each image.\n            gt_bboxes_ignore_list (list[Tensor]): ignore list of gt bboxes.\n            unmap_outputs (bool): unmap outputs or not.\n\n        Returns:\n            tuple\n        """"""\n        num_imgs = len(img_metas)\n        assert len(approx_list) == len(inside_flag_list) == len(\n            square_list) == num_imgs\n        # anchor number of multi levels\n        num_level_squares = [squares.size(0) for squares in square_list[0]]\n        # concat all level anchors and flags to a single tensor\n        inside_flag_flat_list = []\n        approx_flat_list = []\n        square_flat_list = []\n        for i in range(num_imgs):\n            assert len(square_list[i]) == len(inside_flag_list[i])\n            inside_flag_flat_list.append(torch.cat(inside_flag_list[i]))\n            approx_flat_list.append(torch.cat(approx_list[i]))\n            square_flat_list.append(torch.cat(square_list[i]))\n\n        # compute targets for each image\n        if gt_bboxes_ignore_list is None:\n            gt_bboxes_ignore_list = [None for _ in range(num_imgs)]\n        (all_bbox_anchors, all_bbox_gts, all_bbox_weights, pos_inds_list,\n         neg_inds_list) = multi_apply(\n             self._ga_shape_target_single,\n             approx_flat_list,\n             inside_flag_flat_list,\n             square_flat_list,\n             gt_bboxes_list,\n             gt_bboxes_ignore_list,\n             img_metas,\n             unmap_outputs=unmap_outputs)\n        # no valid anchors\n        if any([bbox_anchors is None for bbox_anchors in all_bbox_anchors]):\n            return None\n        # sampled anchors of all images\n        num_total_pos = sum([max(inds.numel(), 1) for inds in pos_inds_list])\n        num_total_neg = sum([max(inds.numel(), 1) for inds in neg_inds_list])\n        # split targets to a list w.r.t. multiple levels\n        bbox_anchors_list = images_to_levels(all_bbox_anchors,\n                                             num_level_squares)\n        bbox_gts_list = images_to_levels(all_bbox_gts, num_level_squares)\n        bbox_weights_list = images_to_levels(all_bbox_weights,\n                                             num_level_squares)\n        return (bbox_anchors_list, bbox_gts_list, bbox_weights_list,\n                num_total_pos, num_total_neg)\n\n    def loss_shape_single(self, shape_pred, bbox_anchors, bbox_gts,\n                          anchor_weights, anchor_total_num):\n        shape_pred = shape_pred.permute(0, 2, 3, 1).contiguous().view(-1, 2)\n        bbox_anchors = bbox_anchors.contiguous().view(-1, 4)\n        bbox_gts = bbox_gts.contiguous().view(-1, 4)\n        anchor_weights = anchor_weights.contiguous().view(-1, 4)\n        bbox_deltas = bbox_anchors.new_full(bbox_anchors.size(), 0)\n        bbox_deltas[:, 2:] += shape_pred\n        # filter out negative samples to speed-up weighted_bounded_iou_loss\n        inds = torch.nonzero(\n            anchor_weights[:, 0] > 0, as_tuple=False).squeeze(1)\n        bbox_deltas_ = bbox_deltas[inds]\n        bbox_anchors_ = bbox_anchors[inds]\n        bbox_gts_ = bbox_gts[inds]\n        anchor_weights_ = anchor_weights[inds]\n        pred_anchors_ = self.anchor_coder.decode(\n            bbox_anchors_, bbox_deltas_, wh_ratio_clip=1e-6)\n        loss_shape = self.loss_shape(\n            pred_anchors_,\n            bbox_gts_,\n            anchor_weights_,\n            avg_factor=anchor_total_num)\n        return loss_shape\n\n    def loss_loc_single(self, loc_pred, loc_target, loc_weight,\n                        loc_avg_factor):\n        loss_loc = self.loss_loc(\n            loc_pred.reshape(-1, 1),\n            loc_target.reshape(-1, 1).long(),\n            loc_weight.reshape(-1, 1),\n            avg_factor=loc_avg_factor)\n        return loss_loc\n\n    @force_fp32(\n        apply_to=(\'cls_scores\', \'bbox_preds\', \'shape_preds\', \'loc_preds\'))\n    def loss(self,\n             cls_scores,\n             bbox_preds,\n             shape_preds,\n             loc_preds,\n             gt_bboxes,\n             gt_labels,\n             img_metas,\n             gt_bboxes_ignore=None):\n        featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n        assert len(featmap_sizes) == self.approx_anchor_generator.num_levels\n\n        device = cls_scores[0].device\n\n        # get loc targets\n        loc_targets, loc_weights, loc_avg_factor = self.ga_loc_targets(\n            gt_bboxes, featmap_sizes)\n\n        # get sampled approxes\n        approxs_list, inside_flag_list = self.get_sampled_approxs(\n            featmap_sizes, img_metas, device=device)\n        # get squares and guided anchors\n        squares_list, guided_anchors_list, _ = self.get_anchors(\n            featmap_sizes, shape_preds, loc_preds, img_metas, device=device)\n\n        # get shape targets\n        shape_targets = self.ga_shape_targets(approxs_list, inside_flag_list,\n                                              squares_list, gt_bboxes,\n                                              img_metas)\n        if shape_targets is None:\n            return None\n        (bbox_anchors_list, bbox_gts_list, anchor_weights_list, anchor_fg_num,\n         anchor_bg_num) = shape_targets\n        anchor_total_num = (\n            anchor_fg_num if not self.ga_sampling else anchor_fg_num +\n            anchor_bg_num)\n\n        # get anchor targets\n        label_channels = self.cls_out_channels if self.use_sigmoid_cls else 1\n        cls_reg_targets = self.get_targets(\n            guided_anchors_list,\n            inside_flag_list,\n            gt_bboxes,\n            img_metas,\n            gt_bboxes_ignore_list=gt_bboxes_ignore,\n            gt_labels_list=gt_labels,\n            label_channels=label_channels)\n        if cls_reg_targets is None:\n            return None\n        (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list,\n         num_total_pos, num_total_neg) = cls_reg_targets\n        num_total_samples = (\n            num_total_pos + num_total_neg if self.sampling else num_total_pos)\n\n        # anchor number of multi levels\n        num_level_anchors = [\n            anchors.size(0) for anchors in guided_anchors_list[0]\n        ]\n        # concat all level anchors to a single tensor\n        concat_anchor_list = []\n        for i in range(len(guided_anchors_list)):\n            concat_anchor_list.append(torch.cat(guided_anchors_list[i]))\n        all_anchor_list = images_to_levels(concat_anchor_list,\n                                           num_level_anchors)\n\n        # get classification and bbox regression losses\n        losses_cls, losses_bbox = multi_apply(\n            self.loss_single,\n            cls_scores,\n            bbox_preds,\n            all_anchor_list,\n            labels_list,\n            label_weights_list,\n            bbox_targets_list,\n            bbox_weights_list,\n            num_total_samples=num_total_samples)\n\n        # get anchor location loss\n        losses_loc = []\n        for i in range(len(loc_preds)):\n            loss_loc = self.loss_loc_single(\n                loc_preds[i],\n                loc_targets[i],\n                loc_weights[i],\n                loc_avg_factor=loc_avg_factor)\n            losses_loc.append(loss_loc)\n\n        # get anchor shape loss\n        losses_shape = []\n        for i in range(len(shape_preds)):\n            loss_shape = self.loss_shape_single(\n                shape_preds[i],\n                bbox_anchors_list[i],\n                bbox_gts_list[i],\n                anchor_weights_list[i],\n                anchor_total_num=anchor_total_num)\n            losses_shape.append(loss_shape)\n\n        return dict(\n            loss_cls=losses_cls,\n            loss_bbox=losses_bbox,\n            loss_shape=losses_shape,\n            loss_loc=losses_loc)\n\n    @force_fp32(\n        apply_to=(\'cls_scores\', \'bbox_preds\', \'shape_preds\', \'loc_preds\'))\n    def get_bboxes(self,\n                   cls_scores,\n                   bbox_preds,\n                   shape_preds,\n                   loc_preds,\n                   img_metas,\n                   cfg=None,\n                   rescale=False):\n        assert len(cls_scores) == len(bbox_preds) == len(shape_preds) == len(\n            loc_preds)\n        num_levels = len(cls_scores)\n        featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n        device = cls_scores[0].device\n        # get guided anchors\n        _, guided_anchors, loc_masks = self.get_anchors(\n            featmap_sizes,\n            shape_preds,\n            loc_preds,\n            img_metas,\n            use_loc_filter=not self.training,\n            device=device)\n        result_list = []\n        for img_id in range(len(img_metas)):\n            cls_score_list = [\n                cls_scores[i][img_id].detach() for i in range(num_levels)\n            ]\n            bbox_pred_list = [\n                bbox_preds[i][img_id].detach() for i in range(num_levels)\n            ]\n            guided_anchor_list = [\n                guided_anchors[img_id][i].detach() for i in range(num_levels)\n            ]\n            loc_mask_list = [\n                loc_masks[img_id][i].detach() for i in range(num_levels)\n            ]\n            img_shape = img_metas[img_id][\'img_shape\']\n            scale_factor = img_metas[img_id][\'scale_factor\']\n            proposals = self._get_bboxes_single(cls_score_list, bbox_pred_list,\n                                                guided_anchor_list,\n                                                loc_mask_list, img_shape,\n                                                scale_factor, cfg, rescale)\n            result_list.append(proposals)\n        return result_list\n\n    def _get_bboxes_single(self,\n                           cls_scores,\n                           bbox_preds,\n                           mlvl_anchors,\n                           mlvl_masks,\n                           img_shape,\n                           scale_factor,\n                           cfg,\n                           rescale=False):\n        cfg = self.test_cfg if cfg is None else cfg\n        assert len(cls_scores) == len(bbox_preds) == len(mlvl_anchors)\n        mlvl_bboxes = []\n        mlvl_scores = []\n        for cls_score, bbox_pred, anchors, mask in zip(cls_scores, bbox_preds,\n                                                       mlvl_anchors,\n                                                       mlvl_masks):\n            assert cls_score.size()[-2:] == bbox_pred.size()[-2:]\n            # if no location is kept, end.\n            if mask.sum() == 0:\n                continue\n            # reshape scores and bbox_pred\n            cls_score = cls_score.permute(1, 2,\n                                          0).reshape(-1, self.cls_out_channels)\n            if self.use_sigmoid_cls:\n                scores = cls_score.sigmoid()\n            else:\n                scores = cls_score.softmax(-1)\n            bbox_pred = bbox_pred.permute(1, 2, 0).reshape(-1, 4)\n            # filter scores, bbox_pred w.r.t. mask.\n            # anchors are filtered in get_anchors() beforehand.\n            scores = scores[mask, :]\n            bbox_pred = bbox_pred[mask, :]\n            if scores.dim() == 0:\n                anchors = anchors.unsqueeze(0)\n                scores = scores.unsqueeze(0)\n                bbox_pred = bbox_pred.unsqueeze(0)\n            # filter anchors, bbox_pred, scores w.r.t. scores\n            nms_pre = cfg.get(\'nms_pre\', -1)\n            if nms_pre > 0 and scores.shape[0] > nms_pre:\n                if self.use_sigmoid_cls:\n                    max_scores, _ = scores.max(dim=1)\n                else:\n                    # remind that we set FG labels to [0, num_class-1]\n                    # since mmdet v2.0\n                    # BG cat_id: num_class\n                    max_scores, _ = scores[:, :-1].max(dim=1)\n                _, topk_inds = max_scores.topk(nms_pre)\n                anchors = anchors[topk_inds, :]\n                bbox_pred = bbox_pred[topk_inds, :]\n                scores = scores[topk_inds, :]\n            bboxes = self.bbox_coder.decode(\n                anchors, bbox_pred, max_shape=img_shape)\n            mlvl_bboxes.append(bboxes)\n            mlvl_scores.append(scores)\n        mlvl_bboxes = torch.cat(mlvl_bboxes)\n        if rescale:\n            mlvl_bboxes /= mlvl_bboxes.new_tensor(scale_factor)\n        mlvl_scores = torch.cat(mlvl_scores)\n        if self.use_sigmoid_cls:\n            # Add a dummy background class to the backend when using sigmoid\n            # remind that we set FG labels to [0, num_class-1] since mmdet v2.0\n            # BG cat_id: num_class\n            padding = mlvl_scores.new_zeros(mlvl_scores.shape[0], 1)\n            mlvl_scores = torch.cat([mlvl_scores, padding], dim=1)\n        # multi class NMS\n        det_bboxes, det_labels = multiclass_nms(mlvl_bboxes, mlvl_scores,\n                                                cfg.score_thr, cfg.nms,\n                                                cfg.max_per_img)\n        return det_bboxes, det_labels\n'"
mmdet/models/dense_heads/nasfcos_head.py,1,"b'import copy\n\nimport torch.nn as nn\nfrom mmcv.cnn import (ConvModule, Scale, bias_init_with_prob,\n                      caffe2_xavier_init, normal_init)\n\nfrom mmdet.models.dense_heads.fcos_head import FCOSHead\nfrom ..builder import HEADS\n\n\n@HEADS.register_module()\nclass NASFCOSHead(FCOSHead):\n    """"""Anchor-free head used in `NASFCOS <https://arxiv.org/abs/1906.04423>`_.\n\n    It is quite similar with FCOS head, except for the searched structure\n    of classification branch and bbox regression branch, where a structure\n    of ""dconv3x3, conv3x3, dconv3x3, conv1x1"" is utilized instead.\n\n    """"""\n\n    def _init_layers(self):\n        dconv3x3_config = dict(\n            type=\'DCNv2\',\n            kernel_size=3,\n            use_bias=True,\n            deformable_groups=2,\n            padding=1)\n        conv3x3_config = dict(type=\'Conv\', kernel_size=3, padding=1)\n        conv1x1_config = dict(type=\'Conv\', kernel_size=1)\n\n        self.arch_config = [\n            dconv3x3_config, conv3x3_config, dconv3x3_config, conv1x1_config\n        ]\n        self.cls_convs = nn.ModuleList()\n        self.reg_convs = nn.ModuleList()\n        for i, op_ in enumerate(self.arch_config):\n            op = copy.deepcopy(op_)\n            chn = self.in_channels if i == 0 else self.feat_channels\n            assert isinstance(op, dict)\n            use_bias = op.pop(\'use_bias\', False)\n            padding = op.pop(\'padding\', 0)\n            kernel_size = op.pop(\'kernel_size\')\n            module = ConvModule(\n                chn,\n                self.feat_channels,\n                kernel_size,\n                stride=1,\n                padding=padding,\n                norm_cfg=self.norm_cfg,\n                bias=use_bias,\n                conv_cfg=op)\n\n            self.cls_convs.append(copy.deepcopy(module))\n            self.reg_convs.append(copy.deepcopy(module))\n\n        self.fcos_cls = nn.Conv2d(\n            self.feat_channels, self.cls_out_channels, 3, padding=1)\n        self.fcos_reg = nn.Conv2d(self.feat_channels, 4, 3, padding=1)\n        self.fcos_centerness = nn.Conv2d(self.feat_channels, 1, 3, padding=1)\n\n        self.scales = nn.ModuleList([Scale(1.0) for _ in self.strides])\n\n    def init_weights(self):\n        # retinanet_bias_init\n        bias_cls = bias_init_with_prob(0.01)\n        normal_init(self.fcos_reg, std=0.01)\n        normal_init(self.fcos_centerness, std=0.01)\n        normal_init(self.fcos_cls, std=0.01, bias=bias_cls)\n\n        for branch in [self.cls_convs, self.reg_convs]:\n            for module in branch.modules():\n                if isinstance(module, ConvModule) \\\n                        and isinstance(module.conv, nn.Conv2d):\n                    caffe2_xavier_init(module.conv)\n'"
mmdet/models/dense_heads/pisa_retinanet_head.py,9,"b'import torch\n\nfrom mmdet.core import force_fp32, images_to_levels\nfrom ..builder import HEADS\nfrom ..losses import carl_loss, isr_p\nfrom .retina_head import RetinaHead\n\n\n@HEADS.register_module()\nclass PISARetinaHead(RetinaHead):\n    """"""PISA Retinanet Head.\n\n    The head owns the same structure with Retinanet Head, but differs in two\n        aspects:\n        1. Importance-based Sample Reweighting Positive (ISR-P) is applied to\n            change the positive loss weights.\n        2. Classification-aware regression loss is adopted as a third loss.\n    """"""\n\n    @force_fp32(apply_to=(\'cls_scores\', \'bbox_preds\'))\n    def loss(self,\n             cls_scores,\n             bbox_preds,\n             gt_bboxes,\n             gt_labels,\n             img_metas,\n             gt_bboxes_ignore=None):\n        featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n        assert len(featmap_sizes) == self.anchor_generator.num_levels\n\n        device = cls_scores[0].device\n\n        anchor_list, valid_flag_list = self.get_anchors(\n            featmap_sizes, img_metas, device=device)\n        label_channels = self.cls_out_channels if self.use_sigmoid_cls else 1\n        cls_reg_targets = self.get_targets(\n            anchor_list,\n            valid_flag_list,\n            gt_bboxes,\n            img_metas,\n            gt_bboxes_ignore_list=gt_bboxes_ignore,\n            gt_labels_list=gt_labels,\n            label_channels=label_channels,\n            return_sampling_results=True)\n        if cls_reg_targets is None:\n            return None\n        (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list,\n         num_total_pos, num_total_neg, sampling_results_list) = cls_reg_targets\n        num_total_samples = (\n            num_total_pos + num_total_neg if self.sampling else num_total_pos)\n\n        # anchor number of multi levels\n        num_level_anchors = [anchors.size(0) for anchors in anchor_list[0]]\n        # concat all level anchors and flags to a single tensor\n        concat_anchor_list = []\n        for i in range(len(anchor_list)):\n            concat_anchor_list.append(torch.cat(anchor_list[i]))\n        all_anchor_list = images_to_levels(concat_anchor_list,\n                                           num_level_anchors)\n\n        num_imgs = len(img_metas)\n        flatten_cls_scores = [\n            cls_score.permute(0, 2, 3, 1).reshape(num_imgs, -1, label_channels)\n            for cls_score in cls_scores\n        ]\n        flatten_cls_scores = torch.cat(\n            flatten_cls_scores, dim=1).reshape(-1,\n                                               flatten_cls_scores[0].size(-1))\n        flatten_bbox_preds = [\n            bbox_pred.permute(0, 2, 3, 1).reshape(num_imgs, -1, 4)\n            for bbox_pred in bbox_preds\n        ]\n        flatten_bbox_preds = torch.cat(\n            flatten_bbox_preds, dim=1).view(-1, flatten_bbox_preds[0].size(-1))\n        flatten_labels = torch.cat(labels_list, dim=1).reshape(-1)\n        flatten_label_weights = torch.cat(\n            label_weights_list, dim=1).reshape(-1)\n        flatten_anchors = torch.cat(all_anchor_list, dim=1).reshape(-1, 4)\n        flatten_bbox_targets = torch.cat(\n            bbox_targets_list, dim=1).reshape(-1, 4)\n        flatten_bbox_weights = torch.cat(\n            bbox_weights_list, dim=1).reshape(-1, 4)\n\n        # Apply ISR-P\n        isr_cfg = self.train_cfg.get(\'isr\', None)\n        if isr_cfg is not None:\n            all_targets = (flatten_labels, flatten_label_weights,\n                           flatten_bbox_targets, flatten_bbox_weights)\n            with torch.no_grad():\n                all_targets = isr_p(\n                    flatten_cls_scores,\n                    flatten_bbox_preds,\n                    all_targets,\n                    flatten_anchors,\n                    sampling_results_list,\n                    bbox_coder=self.bbox_coder,\n                    loss_cls=self.loss_cls,\n                    num_class=self.num_classes,\n                    **self.train_cfg.isr)\n            (flatten_labels, flatten_label_weights, flatten_bbox_targets,\n             flatten_bbox_weights) = all_targets\n\n        # For convenience we compute loss once instead separating by fpn level,\n        # so that we don\'t need to separate the weights by level again.\n        # The result should be the same\n        losses_cls = self.loss_cls(\n            flatten_cls_scores,\n            flatten_labels,\n            flatten_label_weights,\n            avg_factor=num_total_samples)\n        losses_bbox = self.loss_bbox(\n            flatten_bbox_preds,\n            flatten_bbox_targets,\n            flatten_bbox_weights,\n            avg_factor=num_total_samples)\n        loss_dict = dict(loss_cls=losses_cls, loss_bbox=losses_bbox)\n\n        # CARL Loss\n        carl_cfg = self.train_cfg.get(\'carl\', None)\n        if carl_cfg is not None:\n            loss_carl = carl_loss(\n                flatten_cls_scores,\n                flatten_labels,\n                flatten_bbox_preds,\n                flatten_bbox_targets,\n                self.loss_bbox,\n                **self.train_cfg.carl,\n                avg_factor=num_total_pos,\n                sigmoid=True,\n                num_class=self.num_classes)\n            loss_dict.update(loss_carl)\n\n        return loss_dict\n'"
mmdet/models/dense_heads/pisa_ssd_head.py,10,"b""import torch\n\nfrom mmdet.core import multi_apply\nfrom ..builder import HEADS\nfrom ..losses import CrossEntropyLoss, SmoothL1Loss, carl_loss, isr_p\nfrom .ssd_head import SSDHead\n\n\n# TODO: add loss evaluator for SSD\n@HEADS.register_module()\nclass PISASSDHead(SSDHead):\n\n    def loss(self,\n             cls_scores,\n             bbox_preds,\n             gt_bboxes,\n             gt_labels,\n             img_metas,\n             gt_bboxes_ignore=None):\n        featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n        assert len(featmap_sizes) == self.anchor_generator.num_levels\n\n        device = cls_scores[0].device\n\n        anchor_list, valid_flag_list = self.get_anchors(\n            featmap_sizes, img_metas, device=device)\n        cls_reg_targets = self.get_targets(\n            anchor_list,\n            valid_flag_list,\n            gt_bboxes,\n            img_metas,\n            gt_bboxes_ignore_list=gt_bboxes_ignore,\n            gt_labels_list=gt_labels,\n            label_channels=1,\n            unmap_outputs=False,\n            return_sampling_results=True)\n        if cls_reg_targets is None:\n            return None\n        (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list,\n         num_total_pos, num_total_neg, sampling_results_list) = cls_reg_targets\n\n        num_images = len(img_metas)\n        all_cls_scores = torch.cat([\n            s.permute(0, 2, 3, 1).reshape(\n                num_images, -1, self.cls_out_channels) for s in cls_scores\n        ], 1)\n        all_labels = torch.cat(labels_list, -1).view(num_images, -1)\n        all_label_weights = torch.cat(label_weights_list,\n                                      -1).view(num_images, -1)\n        all_bbox_preds = torch.cat([\n            b.permute(0, 2, 3, 1).reshape(num_images, -1, 4)\n            for b in bbox_preds\n        ], -2)\n        all_bbox_targets = torch.cat(bbox_targets_list,\n                                     -2).view(num_images, -1, 4)\n        all_bbox_weights = torch.cat(bbox_weights_list,\n                                     -2).view(num_images, -1, 4)\n\n        # concat all level anchors to a single tensor\n        all_anchors = []\n        for i in range(num_images):\n            all_anchors.append(torch.cat(anchor_list[i]))\n\n        isr_cfg = self.train_cfg.get('isr', None)\n        all_targets = (all_labels.view(-1), all_label_weights.view(-1),\n                       all_bbox_targets.view(-1,\n                                             4), all_bbox_weights.view(-1, 4))\n        # apply ISR-P\n        if isr_cfg is not None:\n            all_targets = isr_p(\n                all_cls_scores.view(-1, all_cls_scores.size(-1)),\n                all_bbox_preds.view(-1, 4),\n                all_targets,\n                torch.cat(all_anchors),\n                sampling_results_list,\n                loss_cls=CrossEntropyLoss(),\n                bbox_coder=self.bbox_coder,\n                **self.train_cfg.isr,\n                num_class=self.num_classes)\n            (new_labels, new_label_weights, new_bbox_targets,\n             new_bbox_weights) = all_targets\n            all_labels = new_labels.view(all_labels.shape)\n            all_label_weights = new_label_weights.view(all_label_weights.shape)\n            all_bbox_targets = new_bbox_targets.view(all_bbox_targets.shape)\n            all_bbox_weights = new_bbox_weights.view(all_bbox_weights.shape)\n\n        # add CARL loss\n        carl_loss_cfg = self.train_cfg.get('carl', None)\n        if carl_loss_cfg is not None:\n            loss_carl = carl_loss(\n                all_cls_scores.view(-1, all_cls_scores.size(-1)),\n                all_targets[0],\n                all_bbox_preds.view(-1, 4),\n                all_targets[2],\n                SmoothL1Loss(beta=1.),\n                **self.train_cfg.carl,\n                avg_factor=num_total_pos,\n                num_class=self.num_classes)\n\n        # check NaN and Inf\n        assert torch.isfinite(all_cls_scores).all().item(), \\\n            'classification scores become infinite or NaN!'\n        assert torch.isfinite(all_bbox_preds).all().item(), \\\n            'bbox predications become infinite or NaN!'\n\n        losses_cls, losses_bbox = multi_apply(\n            self.loss_single,\n            all_cls_scores,\n            all_bbox_preds,\n            all_anchors,\n            all_labels,\n            all_label_weights,\n            all_bbox_targets,\n            all_bbox_weights,\n            num_total_samples=num_total_pos)\n        loss_dict = dict(loss_cls=losses_cls, loss_bbox=losses_bbox)\n        if carl_loss_cfg is not None:\n            loss_dict.update(loss_carl)\n        return loss_dict\n"""
mmdet/models/dense_heads/reppoints_head.py,30,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nfrom mmcv.cnn import ConvModule, bias_init_with_prob, normal_init\n\nfrom mmdet.core import (PointGenerator, build_assigner, build_sampler,\n                        images_to_levels, multi_apply, multiclass_nms, unmap)\nfrom mmdet.ops import DeformConv\nfrom ..builder import HEADS, build_loss\n\n\n@HEADS.register_module()\nclass RepPointsHead(nn.Module):\n    """"""RepPoint head.\n\n    Args:\n        in_channels (int): Number of channels in the input feature map.\n        feat_channels (int): Number of channels of the feature map.\n        point_feat_channels (int): Number of channels of points features.\n        stacked_convs (int): How many conv layers are used.\n        gradient_mul (float): The multiplier to gradients from\n            points refinement and recognition.\n        point_strides (Iterable): points strides.\n        point_base_scale (int): bbox scale for assigning labels.\n        background_label (int | None): Label ID of background, set as 0 for\n            RPN and num_classes for other heads. It will automatically set as\n            num_classes if None is given.\n        loss_cls (dict): Config of classification loss.\n        loss_bbox_init (dict): Config of initial points loss.\n        loss_bbox_refine (dict): Config of points loss in refinement.\n        use_grid_points (bool): If we use bounding box representation, the\n        reppoints is represented as grid points on the bounding box.\n        center_init (bool): Whether to use center point assignment.\n        transform_method (str): The methods to transform RepPoints to bbox.\n    """"""  # noqa: W605\n\n    def __init__(self,\n                 num_classes,\n                 in_channels,\n                 feat_channels=256,\n                 point_feat_channels=256,\n                 stacked_convs=3,\n                 num_points=9,\n                 gradient_mul=0.1,\n                 point_strides=[8, 16, 32, 64, 128],\n                 point_base_scale=4,\n                 conv_cfg=None,\n                 norm_cfg=None,\n                 background_label=None,\n                 loss_cls=dict(\n                     type=\'FocalLoss\',\n                     use_sigmoid=True,\n                     gamma=2.0,\n                     alpha=0.25,\n                     loss_weight=1.0),\n                 loss_bbox_init=dict(\n                     type=\'SmoothL1Loss\', beta=1.0 / 9.0, loss_weight=0.5),\n                 loss_bbox_refine=dict(\n                     type=\'SmoothL1Loss\', beta=1.0 / 9.0, loss_weight=1.0),\n                 use_grid_points=False,\n                 center_init=True,\n                 transform_method=\'moment\',\n                 moment_mul=0.01,\n                 train_cfg=None,\n                 test_cfg=None):\n        super(RepPointsHead, self).__init__()\n        self.in_channels = in_channels\n        self.num_classes = num_classes\n        self.feat_channels = feat_channels\n        self.point_feat_channels = point_feat_channels\n        self.stacked_convs = stacked_convs\n        self.num_points = num_points\n        self.gradient_mul = gradient_mul\n        self.point_base_scale = point_base_scale\n        self.point_strides = point_strides\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        self.train_cfg = train_cfg\n        self.test_cfg = test_cfg\n\n        self.background_label = (\n            num_classes if background_label is None else background_label)\n        # background_label should be either 0 or num_classes\n        assert (self.background_label == 0\n                or self.background_label == num_classes)\n\n        self.use_sigmoid_cls = loss_cls.get(\'use_sigmoid\', False)\n        self.sampling = loss_cls[\'type\'] not in [\'FocalLoss\']\n        self.loss_cls = build_loss(loss_cls)\n        self.loss_bbox_init = build_loss(loss_bbox_init)\n        self.loss_bbox_refine = build_loss(loss_bbox_refine)\n        if self.train_cfg:\n            self.init_assigner = build_assigner(self.train_cfg.init.assigner)\n            self.refine_assigner = build_assigner(\n                self.train_cfg.refine.assigner)\n            # use PseudoSampler when sampling is False\n            if self.sampling and hasattr(self.train_cfg, \'sampler\'):\n                sampler_cfg = self.train_cfg.sampler\n            else:\n                sampler_cfg = dict(type=\'PseudoSampler\')\n            self.sampler = build_sampler(sampler_cfg, context=self)\n        self.use_grid_points = use_grid_points\n        self.center_init = center_init\n        self.transform_method = transform_method\n        if self.transform_method == \'moment\':\n            self.moment_transfer = nn.Parameter(\n                data=torch.zeros(2), requires_grad=True)\n            self.moment_mul = moment_mul\n        if self.use_sigmoid_cls:\n            self.cls_out_channels = self.num_classes\n        else:\n            self.cls_out_channels = self.num_classes + 1\n        self.point_generators = [PointGenerator() for _ in self.point_strides]\n        # we use deformable conv to extract points features\n        self.dcn_kernel = int(np.sqrt(num_points))\n        self.dcn_pad = int((self.dcn_kernel - 1) / 2)\n        assert self.dcn_kernel * self.dcn_kernel == num_points, \\\n            \'The points number should be a square number.\'\n        assert self.dcn_kernel % 2 == 1, \\\n            \'The points number should be an odd square number.\'\n        dcn_base = np.arange(-self.dcn_pad,\n                             self.dcn_pad + 1).astype(np.float64)\n        dcn_base_y = np.repeat(dcn_base, self.dcn_kernel)\n        dcn_base_x = np.tile(dcn_base, self.dcn_kernel)\n        dcn_base_offset = np.stack([dcn_base_y, dcn_base_x], axis=1).reshape(\n            (-1))\n        self.dcn_base_offset = torch.tensor(dcn_base_offset).view(1, -1, 1, 1)\n        self._init_layers()\n\n    def _init_layers(self):\n        self.relu = nn.ReLU(inplace=True)\n        self.cls_convs = nn.ModuleList()\n        self.reg_convs = nn.ModuleList()\n        for i in range(self.stacked_convs):\n            chn = self.in_channels if i == 0 else self.feat_channels\n            self.cls_convs.append(\n                ConvModule(\n                    chn,\n                    self.feat_channels,\n                    3,\n                    stride=1,\n                    padding=1,\n                    conv_cfg=self.conv_cfg,\n                    norm_cfg=self.norm_cfg))\n            self.reg_convs.append(\n                ConvModule(\n                    chn,\n                    self.feat_channels,\n                    3,\n                    stride=1,\n                    padding=1,\n                    conv_cfg=self.conv_cfg,\n                    norm_cfg=self.norm_cfg))\n        pts_out_dim = 4 if self.use_grid_points else 2 * self.num_points\n        self.reppoints_cls_conv = DeformConv(self.feat_channels,\n                                             self.point_feat_channels,\n                                             self.dcn_kernel, 1, self.dcn_pad)\n        self.reppoints_cls_out = nn.Conv2d(self.point_feat_channels,\n                                           self.cls_out_channels, 1, 1, 0)\n        self.reppoints_pts_init_conv = nn.Conv2d(self.feat_channels,\n                                                 self.point_feat_channels, 3,\n                                                 1, 1)\n        self.reppoints_pts_init_out = nn.Conv2d(self.point_feat_channels,\n                                                pts_out_dim, 1, 1, 0)\n        self.reppoints_pts_refine_conv = DeformConv(self.feat_channels,\n                                                    self.point_feat_channels,\n                                                    self.dcn_kernel, 1,\n                                                    self.dcn_pad)\n        self.reppoints_pts_refine_out = nn.Conv2d(self.point_feat_channels,\n                                                  pts_out_dim, 1, 1, 0)\n\n    def init_weights(self):\n        for m in self.cls_convs:\n            normal_init(m.conv, std=0.01)\n        for m in self.reg_convs:\n            normal_init(m.conv, std=0.01)\n        bias_cls = bias_init_with_prob(0.01)\n        normal_init(self.reppoints_cls_conv, std=0.01)\n        normal_init(self.reppoints_cls_out, std=0.01, bias=bias_cls)\n        normal_init(self.reppoints_pts_init_conv, std=0.01)\n        normal_init(self.reppoints_pts_init_out, std=0.01)\n        normal_init(self.reppoints_pts_refine_conv, std=0.01)\n        normal_init(self.reppoints_pts_refine_out, std=0.01)\n\n    def points2bbox(self, pts, y_first=True):\n        """"""Converting the points set into bounding box.\n\n        :param pts: the input points sets (fields), each points\n            set (fields) is represented as 2n scalar.\n        :param y_first: if y_fisrt=True, the point set is represented as\n            [y1, x1, y2, x2 ... yn, xn], otherwise the point set is\n            represented as [x1, y1, x2, y2 ... xn, yn].\n        :return: each points set is converting to a bbox [x1, y1, x2, y2].\n        """"""\n        pts_reshape = pts.view(pts.shape[0], -1, 2, *pts.shape[2:])\n        pts_y = pts_reshape[:, :, 0, ...] if y_first else pts_reshape[:, :, 1,\n                                                                      ...]\n        pts_x = pts_reshape[:, :, 1, ...] if y_first else pts_reshape[:, :, 0,\n                                                                      ...]\n        if self.transform_method == \'minmax\':\n            bbox_left = pts_x.min(dim=1, keepdim=True)[0]\n            bbox_right = pts_x.max(dim=1, keepdim=True)[0]\n            bbox_up = pts_y.min(dim=1, keepdim=True)[0]\n            bbox_bottom = pts_y.max(dim=1, keepdim=True)[0]\n            bbox = torch.cat([bbox_left, bbox_up, bbox_right, bbox_bottom],\n                             dim=1)\n        elif self.transform_method == \'partial_minmax\':\n            pts_y = pts_y[:, :4, ...]\n            pts_x = pts_x[:, :4, ...]\n            bbox_left = pts_x.min(dim=1, keepdim=True)[0]\n            bbox_right = pts_x.max(dim=1, keepdim=True)[0]\n            bbox_up = pts_y.min(dim=1, keepdim=True)[0]\n            bbox_bottom = pts_y.max(dim=1, keepdim=True)[0]\n            bbox = torch.cat([bbox_left, bbox_up, bbox_right, bbox_bottom],\n                             dim=1)\n        elif self.transform_method == \'moment\':\n            pts_y_mean = pts_y.mean(dim=1, keepdim=True)\n            pts_x_mean = pts_x.mean(dim=1, keepdim=True)\n            pts_y_std = torch.std(pts_y - pts_y_mean, dim=1, keepdim=True)\n            pts_x_std = torch.std(pts_x - pts_x_mean, dim=1, keepdim=True)\n            moment_transfer = (self.moment_transfer * self.moment_mul) + (\n                self.moment_transfer.detach() * (1 - self.moment_mul))\n            moment_width_transfer = moment_transfer[0]\n            moment_height_transfer = moment_transfer[1]\n            half_width = pts_x_std * torch.exp(moment_width_transfer)\n            half_height = pts_y_std * torch.exp(moment_height_transfer)\n            bbox = torch.cat([\n                pts_x_mean - half_width, pts_y_mean - half_height,\n                pts_x_mean + half_width, pts_y_mean + half_height\n            ],\n                             dim=1)\n        else:\n            raise NotImplementedError\n        return bbox\n\n    def gen_grid_from_reg(self, reg, previous_boxes):\n        """"""Base on the previous bboxes and regression values, we compute the\n        regressed bboxes and generate the grids on the bboxes.\n\n        :param reg: the regression value to previous bboxes.\n        :param previous_boxes: previous bboxes.\n        :return: generate grids on the regressed bboxes.\n        """"""\n        b, _, h, w = reg.shape\n        bxy = (previous_boxes[:, :2, ...] + previous_boxes[:, 2:, ...]) / 2.\n        bwh = (previous_boxes[:, 2:, ...] -\n               previous_boxes[:, :2, ...]).clamp(min=1e-6)\n        grid_topleft = bxy + bwh * reg[:, :2, ...] - 0.5 * bwh * torch.exp(\n            reg[:, 2:, ...])\n        grid_wh = bwh * torch.exp(reg[:, 2:, ...])\n        grid_left = grid_topleft[:, [0], ...]\n        grid_top = grid_topleft[:, [1], ...]\n        grid_width = grid_wh[:, [0], ...]\n        grid_height = grid_wh[:, [1], ...]\n        intervel = torch.linspace(0., 1., self.dcn_kernel).view(\n            1, self.dcn_kernel, 1, 1).type_as(reg)\n        grid_x = grid_left + grid_width * intervel\n        grid_x = grid_x.unsqueeze(1).repeat(1, self.dcn_kernel, 1, 1, 1)\n        grid_x = grid_x.view(b, -1, h, w)\n        grid_y = grid_top + grid_height * intervel\n        grid_y = grid_y.unsqueeze(2).repeat(1, 1, self.dcn_kernel, 1, 1)\n        grid_y = grid_y.view(b, -1, h, w)\n        grid_yx = torch.stack([grid_y, grid_x], dim=2)\n        grid_yx = grid_yx.view(b, -1, h, w)\n        regressed_bbox = torch.cat([\n            grid_left, grid_top, grid_left + grid_width, grid_top + grid_height\n        ], 1)\n        return grid_yx, regressed_bbox\n\n    def forward_single(self, x):\n        dcn_base_offset = self.dcn_base_offset.type_as(x)\n        # If we use center_init, the initial reppoints is from center points.\n        # If we use bounding bbox representation, the initial reppoints is\n        #   from regular grid placed on a pre-defined bbox.\n        if self.use_grid_points or not self.center_init:\n            scale = self.point_base_scale / 2\n            points_init = dcn_base_offset / dcn_base_offset.max() * scale\n            bbox_init = x.new_tensor([-scale, -scale, scale,\n                                      scale]).view(1, 4, 1, 1)\n        else:\n            points_init = 0\n        cls_feat = x\n        pts_feat = x\n        for cls_conv in self.cls_convs:\n            cls_feat = cls_conv(cls_feat)\n        for reg_conv in self.reg_convs:\n            pts_feat = reg_conv(pts_feat)\n        # initialize reppoints\n        pts_out_init = self.reppoints_pts_init_out(\n            self.relu(self.reppoints_pts_init_conv(pts_feat)))\n        if self.use_grid_points:\n            pts_out_init, bbox_out_init = self.gen_grid_from_reg(\n                pts_out_init, bbox_init.detach())\n        else:\n            pts_out_init = pts_out_init + points_init\n        # refine and classify reppoints\n        pts_out_init_grad_mul = (1 - self.gradient_mul) * pts_out_init.detach(\n        ) + self.gradient_mul * pts_out_init\n        dcn_offset = pts_out_init_grad_mul - dcn_base_offset\n        cls_out = self.reppoints_cls_out(\n            self.relu(self.reppoints_cls_conv(cls_feat, dcn_offset)))\n        pts_out_refine = self.reppoints_pts_refine_out(\n            self.relu(self.reppoints_pts_refine_conv(pts_feat, dcn_offset)))\n        if self.use_grid_points:\n            pts_out_refine, bbox_out_refine = self.gen_grid_from_reg(\n                pts_out_refine, bbox_out_init.detach())\n        else:\n            pts_out_refine = pts_out_refine + pts_out_init.detach()\n        return cls_out, pts_out_init, pts_out_refine\n\n    def forward(self, feats):\n        return multi_apply(self.forward_single, feats)\n\n    def get_points(self, featmap_sizes, img_metas):\n        """"""Get points according to feature map sizes.\n\n        Args:\n            featmap_sizes (list[tuple]): Multi-level feature map sizes.\n            img_metas (list[dict]): Image meta info.\n\n        Returns:\n            tuple: points of each image, valid flags of each image\n        """"""\n        num_imgs = len(img_metas)\n        num_levels = len(featmap_sizes)\n\n        # since feature map sizes of all images are the same, we only compute\n        # points center for one time\n        multi_level_points = []\n        for i in range(num_levels):\n            points = self.point_generators[i].grid_points(\n                featmap_sizes[i], self.point_strides[i])\n            multi_level_points.append(points)\n        points_list = [[point.clone() for point in multi_level_points]\n                       for _ in range(num_imgs)]\n\n        # for each image, we compute valid flags of multi level grids\n        valid_flag_list = []\n        for img_id, img_meta in enumerate(img_metas):\n            multi_level_flags = []\n            for i in range(num_levels):\n                point_stride = self.point_strides[i]\n                feat_h, feat_w = featmap_sizes[i]\n                h, w = img_meta[\'pad_shape\'][:2]\n                valid_feat_h = min(int(np.ceil(h / point_stride)), feat_h)\n                valid_feat_w = min(int(np.ceil(w / point_stride)), feat_w)\n                flags = self.point_generators[i].valid_flags(\n                    (feat_h, feat_w), (valid_feat_h, valid_feat_w))\n                multi_level_flags.append(flags)\n            valid_flag_list.append(multi_level_flags)\n\n        return points_list, valid_flag_list\n\n    def centers_to_bboxes(self, point_list):\n        """"""Get bboxes according to center points. Only used in MaxIOUAssigner.\n        """"""\n        bbox_list = []\n        for i_img, point in enumerate(point_list):\n            bbox = []\n            for i_lvl in range(len(self.point_strides)):\n                scale = self.point_base_scale * self.point_strides[i_lvl] * 0.5\n                bbox_shift = torch.Tensor([-scale, -scale, scale,\n                                           scale]).view(1, 4).type_as(point[0])\n                bbox_center = torch.cat(\n                    [point[i_lvl][:, :2], point[i_lvl][:, :2]], dim=1)\n                bbox.append(bbox_center + bbox_shift)\n            bbox_list.append(bbox)\n        return bbox_list\n\n    def offset_to_pts(self, center_list, pred_list):\n        """"""Change from point offset to point coordinate.\n        """"""\n        pts_list = []\n        for i_lvl in range(len(self.point_strides)):\n            pts_lvl = []\n            for i_img in range(len(center_list)):\n                pts_center = center_list[i_img][i_lvl][:, :2].repeat(\n                    1, self.num_points)\n                pts_shift = pred_list[i_lvl][i_img]\n                yx_pts_shift = pts_shift.permute(1, 2, 0).view(\n                    -1, 2 * self.num_points)\n                y_pts_shift = yx_pts_shift[..., 0::2]\n                x_pts_shift = yx_pts_shift[..., 1::2]\n                xy_pts_shift = torch.stack([x_pts_shift, y_pts_shift], -1)\n                xy_pts_shift = xy_pts_shift.view(*yx_pts_shift.shape[:-1], -1)\n                pts = xy_pts_shift * self.point_strides[i_lvl] + pts_center\n                pts_lvl.append(pts)\n            pts_lvl = torch.stack(pts_lvl, 0)\n            pts_list.append(pts_lvl)\n        return pts_list\n\n    def _point_target_single(self,\n                             flat_proposals,\n                             valid_flags,\n                             gt_bboxes,\n                             gt_bboxes_ignore,\n                             gt_labels,\n                             label_channels=1,\n                             stage=\'init\',\n                             unmap_outputs=True):\n        inside_flags = valid_flags\n        if not inside_flags.any():\n            return (None, ) * 7\n        # assign gt and sample proposals\n        proposals = flat_proposals[inside_flags, :]\n\n        if stage == \'init\':\n            assigner = self.init_assigner\n            pos_weight = self.train_cfg.init.pos_weight\n        else:\n            assigner = self.refine_assigner\n            pos_weight = self.train_cfg.refine.pos_weight\n        assign_result = assigner.assign(proposals, gt_bboxes, gt_bboxes_ignore,\n                                        None if self.sampling else gt_labels)\n        sampling_result = self.sampler.sample(assign_result, proposals,\n                                              gt_bboxes)\n\n        num_valid_proposals = proposals.shape[0]\n        bbox_gt = proposals.new_zeros([num_valid_proposals, 4])\n        pos_proposals = torch.zeros_like(proposals)\n        proposals_weights = proposals.new_zeros([num_valid_proposals, 4])\n        labels = proposals.new_full((num_valid_proposals, ),\n                                    self.background_label,\n                                    dtype=torch.long)\n        label_weights = proposals.new_zeros(\n            num_valid_proposals, dtype=torch.float)\n\n        pos_inds = sampling_result.pos_inds\n        neg_inds = sampling_result.neg_inds\n        if len(pos_inds) > 0:\n            pos_gt_bboxes = sampling_result.pos_gt_bboxes\n            bbox_gt[pos_inds, :] = pos_gt_bboxes\n            pos_proposals[pos_inds, :] = proposals[pos_inds, :]\n            proposals_weights[pos_inds, :] = 1.0\n            if gt_labels is None:\n                labels[pos_inds] = 1\n            else:\n                labels[pos_inds] = gt_labels[\n                    sampling_result.pos_assigned_gt_inds]\n            if pos_weight <= 0:\n                label_weights[pos_inds] = 1.0\n            else:\n                label_weights[pos_inds] = pos_weight\n        if len(neg_inds) > 0:\n            label_weights[neg_inds] = 1.0\n\n        # map up to original set of proposals\n        if unmap_outputs:\n            num_total_proposals = flat_proposals.size(0)\n            labels = unmap(labels, num_total_proposals, inside_flags)\n            label_weights = unmap(label_weights, num_total_proposals,\n                                  inside_flags)\n            bbox_gt = unmap(bbox_gt, num_total_proposals, inside_flags)\n            pos_proposals = unmap(pos_proposals, num_total_proposals,\n                                  inside_flags)\n            proposals_weights = unmap(proposals_weights, num_total_proposals,\n                                      inside_flags)\n\n        return (labels, label_weights, bbox_gt, pos_proposals,\n                proposals_weights, pos_inds, neg_inds)\n\n    def get_targets(self,\n                    proposals_list,\n                    valid_flag_list,\n                    gt_bboxes_list,\n                    img_metas,\n                    gt_bboxes_ignore_list=None,\n                    gt_labels_list=None,\n                    stage=\'init\',\n                    label_channels=1,\n                    unmap_outputs=True):\n        """"""Compute corresponding GT box and classification targets for\n        proposals.\n\n        Args:\n            proposals_list (list[list]): Multi level points/bboxes of each\n                image.\n            valid_flag_list (list[list]): Multi level valid flags of each\n                image.\n            gt_bboxes_list (list[Tensor]): Ground truth bboxes of each image.\n            img_metas (list[dict]): Meta info of each image.\n            gt_bboxes_ignore_list (list[Tensor]): Ground truth bboxes to be\n                ignored.\n            gt_bboxes_list (list[Tensor]): Ground truth labels of each box.\n            stage (str): `init` or `refine`. Generate target for init stage or\n                refine stage\n            label_channels (int): Channel of label.\n            unmap_outputs (bool): Whether to map outputs back to the original\n                set of anchors.\n\n        Returns:\n            tuple:\n                - labels_list (list[Tensor]): Labels of each level.\n                - label_weights_list (list[Tensor]): Label weights of each level.  # noqa: E501\n                - bbox_gt_list (list[Tensor]): Ground truth bbox of each level.\n                - proposal_list (list[Tensor]): Proposals(points/bboxes) of each level.  # noqa: E501\n                - proposal_weights_list (list[Tensor]): Proposal weights of each level.  # noqa: E501\n                - num_total_pos (int): Number of positive samples in all images.  # noqa: E501\n                - num_total_neg (int): Number of negative samples in all images.  # noqa: E501\n        """"""\n        assert stage in [\'init\', \'refine\']\n        num_imgs = len(img_metas)\n        assert len(proposals_list) == len(valid_flag_list) == num_imgs\n\n        # points number of multi levels\n        num_level_proposals = [points.size(0) for points in proposals_list[0]]\n\n        # concat all level points and flags to a single tensor\n        for i in range(num_imgs):\n            assert len(proposals_list[i]) == len(valid_flag_list[i])\n            proposals_list[i] = torch.cat(proposals_list[i])\n            valid_flag_list[i] = torch.cat(valid_flag_list[i])\n\n        # compute targets for each image\n        if gt_bboxes_ignore_list is None:\n            gt_bboxes_ignore_list = [None for _ in range(num_imgs)]\n        if gt_labels_list is None:\n            gt_labels_list = [None for _ in range(num_imgs)]\n        (all_labels, all_label_weights, all_bbox_gt, all_proposals,\n         all_proposal_weights, pos_inds_list, neg_inds_list) = multi_apply(\n             self._point_target_single,\n             proposals_list,\n             valid_flag_list,\n             gt_bboxes_list,\n             gt_bboxes_ignore_list,\n             gt_labels_list,\n             stage=stage,\n             label_channels=label_channels,\n             unmap_outputs=unmap_outputs)\n        # no valid points\n        if any([labels is None for labels in all_labels]):\n            return None\n        # sampled points of all images\n        num_total_pos = sum([max(inds.numel(), 1) for inds in pos_inds_list])\n        num_total_neg = sum([max(inds.numel(), 1) for inds in neg_inds_list])\n        labels_list = images_to_levels(all_labels, num_level_proposals)\n        label_weights_list = images_to_levels(all_label_weights,\n                                              num_level_proposals)\n        bbox_gt_list = images_to_levels(all_bbox_gt, num_level_proposals)\n        proposals_list = images_to_levels(all_proposals, num_level_proposals)\n        proposal_weights_list = images_to_levels(all_proposal_weights,\n                                                 num_level_proposals)\n        return (labels_list, label_weights_list, bbox_gt_list, proposals_list,\n                proposal_weights_list, num_total_pos, num_total_neg)\n\n    def loss_single(self, cls_score, pts_pred_init, pts_pred_refine, labels,\n                    label_weights, bbox_gt_init, bbox_weights_init,\n                    bbox_gt_refine, bbox_weights_refine, stride,\n                    num_total_samples_init, num_total_samples_refine):\n        # classification loss\n        labels = labels.reshape(-1)\n        label_weights = label_weights.reshape(-1)\n        cls_score = cls_score.permute(0, 2, 3,\n                                      1).reshape(-1, self.cls_out_channels)\n        loss_cls = self.loss_cls(\n            cls_score,\n            labels,\n            label_weights,\n            avg_factor=num_total_samples_refine)\n\n        # points loss\n        bbox_gt_init = bbox_gt_init.reshape(-1, 4)\n        bbox_weights_init = bbox_weights_init.reshape(-1, 4)\n        bbox_pred_init = self.points2bbox(\n            pts_pred_init.reshape(-1, 2 * self.num_points), y_first=False)\n        bbox_gt_refine = bbox_gt_refine.reshape(-1, 4)\n        bbox_weights_refine = bbox_weights_refine.reshape(-1, 4)\n        bbox_pred_refine = self.points2bbox(\n            pts_pred_refine.reshape(-1, 2 * self.num_points), y_first=False)\n        normalize_term = self.point_base_scale * stride\n        loss_pts_init = self.loss_bbox_init(\n            bbox_pred_init / normalize_term,\n            bbox_gt_init / normalize_term,\n            bbox_weights_init,\n            avg_factor=num_total_samples_init)\n        loss_pts_refine = self.loss_bbox_refine(\n            bbox_pred_refine / normalize_term,\n            bbox_gt_refine / normalize_term,\n            bbox_weights_refine,\n            avg_factor=num_total_samples_refine)\n        return loss_cls, loss_pts_init, loss_pts_refine\n\n    def loss(self,\n             cls_scores,\n             pts_preds_init,\n             pts_preds_refine,\n             gt_bboxes,\n             gt_labels,\n             img_metas,\n             gt_bboxes_ignore=None):\n        featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n        assert len(featmap_sizes) == len(self.point_generators)\n        label_channels = self.cls_out_channels if self.use_sigmoid_cls else 1\n\n        # target for initial stage\n        center_list, valid_flag_list = self.get_points(featmap_sizes,\n                                                       img_metas)\n        pts_coordinate_preds_init = self.offset_to_pts(center_list,\n                                                       pts_preds_init)\n        if self.train_cfg.init.assigner[\'type\'] == \'PointAssigner\':\n            # Assign target for center list\n            candidate_list = center_list\n        else:\n            # transform center list to bbox list and\n            #   assign target for bbox list\n            bbox_list = self.centers_to_bboxes(center_list)\n            candidate_list = bbox_list\n        cls_reg_targets_init = self.get_targets(\n            candidate_list,\n            valid_flag_list,\n            gt_bboxes,\n            img_metas,\n            gt_bboxes_ignore_list=gt_bboxes_ignore,\n            gt_labels_list=gt_labels,\n            stage=\'init\',\n            label_channels=label_channels)\n        (*_, bbox_gt_list_init, candidate_list_init, bbox_weights_list_init,\n         num_total_pos_init, num_total_neg_init) = cls_reg_targets_init\n        num_total_samples_init = (\n            num_total_pos_init +\n            num_total_neg_init if self.sampling else num_total_pos_init)\n\n        # target for refinement stage\n        center_list, valid_flag_list = self.get_points(featmap_sizes,\n                                                       img_metas)\n        pts_coordinate_preds_refine = self.offset_to_pts(\n            center_list, pts_preds_refine)\n        bbox_list = []\n        for i_img, center in enumerate(center_list):\n            bbox = []\n            for i_lvl in range(len(pts_preds_refine)):\n                bbox_preds_init = self.points2bbox(\n                    pts_preds_init[i_lvl].detach())\n                bbox_shift = bbox_preds_init * self.point_strides[i_lvl]\n                bbox_center = torch.cat(\n                    [center[i_lvl][:, :2], center[i_lvl][:, :2]], dim=1)\n                bbox.append(bbox_center +\n                            bbox_shift[i_img].permute(1, 2, 0).reshape(-1, 4))\n            bbox_list.append(bbox)\n        cls_reg_targets_refine = self.get_targets(\n            bbox_list,\n            valid_flag_list,\n            gt_bboxes,\n            img_metas,\n            gt_bboxes_ignore_list=gt_bboxes_ignore,\n            gt_labels_list=gt_labels,\n            stage=\'refine\',\n            label_channels=label_channels)\n        (labels_list, label_weights_list, bbox_gt_list_refine,\n         candidate_list_refine, bbox_weights_list_refine, num_total_pos_refine,\n         num_total_neg_refine) = cls_reg_targets_refine\n        num_total_samples_refine = (\n            num_total_pos_refine +\n            num_total_neg_refine if self.sampling else num_total_pos_refine)\n\n        # compute loss\n        losses_cls, losses_pts_init, losses_pts_refine = multi_apply(\n            self.loss_single,\n            cls_scores,\n            pts_coordinate_preds_init,\n            pts_coordinate_preds_refine,\n            labels_list,\n            label_weights_list,\n            bbox_gt_list_init,\n            bbox_weights_list_init,\n            bbox_gt_list_refine,\n            bbox_weights_list_refine,\n            self.point_strides,\n            num_total_samples_init=num_total_samples_init,\n            num_total_samples_refine=num_total_samples_refine)\n        loss_dict_all = {\n            \'loss_cls\': losses_cls,\n            \'loss_pts_init\': losses_pts_init,\n            \'loss_pts_refine\': losses_pts_refine\n        }\n        return loss_dict_all\n\n    def get_bboxes(self,\n                   cls_scores,\n                   pts_preds_init,\n                   pts_preds_refine,\n                   img_metas,\n                   cfg=None,\n                   rescale=False,\n                   nms=True):\n        assert len(cls_scores) == len(pts_preds_refine)\n        bbox_preds_refine = [\n            self.points2bbox(pts_pred_refine)\n            for pts_pred_refine in pts_preds_refine\n        ]\n        num_levels = len(cls_scores)\n        mlvl_points = [\n            self.point_generators[i].grid_points(cls_scores[i].size()[-2:],\n                                                 self.point_strides[i])\n            for i in range(num_levels)\n        ]\n        result_list = []\n        for img_id in range(len(img_metas)):\n            cls_score_list = [\n                cls_scores[i][img_id].detach() for i in range(num_levels)\n            ]\n            bbox_pred_list = [\n                bbox_preds_refine[i][img_id].detach()\n                for i in range(num_levels)\n            ]\n            img_shape = img_metas[img_id][\'img_shape\']\n            scale_factor = img_metas[img_id][\'scale_factor\']\n            proposals = self._get_bboxes_single(cls_score_list, bbox_pred_list,\n                                                mlvl_points, img_shape,\n                                                scale_factor, cfg, rescale,\n                                                nms)\n            result_list.append(proposals)\n        return result_list\n\n    def _get_bboxes_single(self,\n                           cls_scores,\n                           bbox_preds,\n                           mlvl_points,\n                           img_shape,\n                           scale_factor,\n                           cfg,\n                           rescale=False,\n                           nms=True):\n        cfg = self.test_cfg if cfg is None else cfg\n        assert len(cls_scores) == len(bbox_preds) == len(mlvl_points)\n        mlvl_bboxes = []\n        mlvl_scores = []\n        for i_lvl, (cls_score, bbox_pred, points) in enumerate(\n                zip(cls_scores, bbox_preds, mlvl_points)):\n            assert cls_score.size()[-2:] == bbox_pred.size()[-2:]\n            cls_score = cls_score.permute(1, 2,\n                                          0).reshape(-1, self.cls_out_channels)\n            if self.use_sigmoid_cls:\n                scores = cls_score.sigmoid()\n            else:\n                scores = cls_score.softmax(-1)\n            bbox_pred = bbox_pred.permute(1, 2, 0).reshape(-1, 4)\n            nms_pre = cfg.get(\'nms_pre\', -1)\n            if nms_pre > 0 and scores.shape[0] > nms_pre:\n                if self.use_sigmoid_cls:\n                    max_scores, _ = scores.max(dim=1)\n                else:\n                    # remind that we set FG labels to [0, num_class-1]\n                    # since mmdet v2.0\n                    # BG cat_id: num_class\n                    max_scores, _ = scores[:, :-1].max(dim=1)\n                _, topk_inds = max_scores.topk(nms_pre)\n                points = points[topk_inds, :]\n                bbox_pred = bbox_pred[topk_inds, :]\n                scores = scores[topk_inds, :]\n            bbox_pos_center = torch.cat([points[:, :2], points[:, :2]], dim=1)\n            bboxes = bbox_pred * self.point_strides[i_lvl] + bbox_pos_center\n            x1 = bboxes[:, 0].clamp(min=0, max=img_shape[1])\n            y1 = bboxes[:, 1].clamp(min=0, max=img_shape[0])\n            x2 = bboxes[:, 2].clamp(min=0, max=img_shape[1])\n            y2 = bboxes[:, 3].clamp(min=0, max=img_shape[0])\n            bboxes = torch.stack([x1, y1, x2, y2], dim=-1)\n            mlvl_bboxes.append(bboxes)\n            mlvl_scores.append(scores)\n        mlvl_bboxes = torch.cat(mlvl_bboxes)\n        if rescale:\n            mlvl_bboxes /= mlvl_bboxes.new_tensor(scale_factor)\n        mlvl_scores = torch.cat(mlvl_scores)\n        if self.use_sigmoid_cls:\n            # Add a dummy background class to the backend when using sigmoid\n            # remind that we set FG labels to [0, num_class-1] since mmdet v2.0\n            # BG cat_id: num_class\n            padding = mlvl_scores.new_zeros(mlvl_scores.shape[0], 1)\n            mlvl_scores = torch.cat([mlvl_scores, padding], dim=1)\n        if nms:\n            det_bboxes, det_labels = multiclass_nms(mlvl_bboxes, mlvl_scores,\n                                                    cfg.score_thr, cfg.nms,\n                                                    cfg.max_per_img)\n            return det_bboxes, det_labels\n        else:\n            return mlvl_bboxes, mlvl_scores\n'"
mmdet/models/dense_heads/retina_head.py,2,"b'import torch.nn as nn\nfrom mmcv.cnn import ConvModule, bias_init_with_prob, normal_init\n\nfrom ..builder import HEADS\nfrom .anchor_head import AnchorHead\n\n\n@HEADS.register_module()\nclass RetinaHead(AnchorHead):\n    """"""An anchor-based head used in\n    `RetinaNet <https://arxiv.org/pdf/1708.02002.pdf>`_.\n\n    The head contains two subnetworks. The first classifies anchor boxes and\n    the second regresses deltas for the anchors.\n\n    Example:\n        >>> import torch\n        >>> self = RetinaHead(11, 7)\n        >>> x = torch.rand(1, 7, 32, 32)\n        >>> cls_score, bbox_pred = self.forward_single(x)\n        >>> # Each anchor predicts a score for each class except background\n        >>> cls_per_anchor = cls_score.shape[1] / self.num_anchors\n        >>> box_per_anchor = bbox_pred.shape[1] / self.num_anchors\n        >>> assert cls_per_anchor == (self.num_classes)\n        >>> assert box_per_anchor == 4\n    """"""\n\n    def __init__(self,\n                 num_classes,\n                 in_channels,\n                 stacked_convs=4,\n                 conv_cfg=None,\n                 norm_cfg=None,\n                 anchor_generator=dict(\n                     type=\'AnchorGenerator\',\n                     octave_base_scale=4,\n                     scales_per_octave=3,\n                     ratios=[0.5, 1.0, 2.0],\n                     strides=[8, 16, 32, 64, 128]),\n                 **kwargs):\n        self.stacked_convs = stacked_convs\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        super(RetinaHead, self).__init__(\n            num_classes,\n            in_channels,\n            anchor_generator=anchor_generator,\n            **kwargs)\n\n    def _init_layers(self):\n        self.relu = nn.ReLU(inplace=True)\n        self.cls_convs = nn.ModuleList()\n        self.reg_convs = nn.ModuleList()\n        for i in range(self.stacked_convs):\n            chn = self.in_channels if i == 0 else self.feat_channels\n            self.cls_convs.append(\n                ConvModule(\n                    chn,\n                    self.feat_channels,\n                    3,\n                    stride=1,\n                    padding=1,\n                    conv_cfg=self.conv_cfg,\n                    norm_cfg=self.norm_cfg))\n            self.reg_convs.append(\n                ConvModule(\n                    chn,\n                    self.feat_channels,\n                    3,\n                    stride=1,\n                    padding=1,\n                    conv_cfg=self.conv_cfg,\n                    norm_cfg=self.norm_cfg))\n        self.retina_cls = nn.Conv2d(\n            self.feat_channels,\n            self.num_anchors * self.cls_out_channels,\n            3,\n            padding=1)\n        self.retina_reg = nn.Conv2d(\n            self.feat_channels, self.num_anchors * 4, 3, padding=1)\n\n    def init_weights(self):\n        for m in self.cls_convs:\n            normal_init(m.conv, std=0.01)\n        for m in self.reg_convs:\n            normal_init(m.conv, std=0.01)\n        bias_cls = bias_init_with_prob(0.01)\n        normal_init(self.retina_cls, std=0.01, bias=bias_cls)\n        normal_init(self.retina_reg, std=0.01)\n\n    def forward_single(self, x):\n        cls_feat = x\n        reg_feat = x\n        for cls_conv in self.cls_convs:\n            cls_feat = cls_conv(cls_feat)\n        for reg_conv in self.reg_convs:\n            reg_feat = reg_conv(reg_feat)\n        cls_score = self.retina_cls(cls_feat)\n        bbox_pred = self.retina_reg(reg_feat)\n        return cls_score, bbox_pred\n'"
mmdet/models/dense_heads/retina_sepbn_head.py,1,"b'import torch.nn as nn\nfrom mmcv.cnn import ConvModule, bias_init_with_prob, normal_init\n\nfrom ..builder import HEADS\nfrom .anchor_head import AnchorHead\n\n\n@HEADS.register_module()\nclass RetinaSepBNHead(AnchorHead):\n    """"""""RetinaHead with separate BN.\n\n    In RetinaHead, conv/norm layers are shared across different FPN levels,\n    while in RetinaSepBNHead, conv layers are shared across different FPN\n    levels, but BN layers are separated.\n    """"""\n\n    def __init__(self,\n                 num_classes,\n                 num_ins,\n                 in_channels,\n                 stacked_convs=4,\n                 conv_cfg=None,\n                 norm_cfg=None,\n                 **kwargs):\n        self.stacked_convs = stacked_convs\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        self.num_ins = num_ins\n        super(RetinaSepBNHead, self).__init__(num_classes, in_channels,\n                                              **kwargs)\n\n    def _init_layers(self):\n        self.relu = nn.ReLU(inplace=True)\n        self.cls_convs = nn.ModuleList()\n        self.reg_convs = nn.ModuleList()\n        for i in range(self.num_ins):\n            cls_convs = nn.ModuleList()\n            reg_convs = nn.ModuleList()\n            for i in range(self.stacked_convs):\n                chn = self.in_channels if i == 0 else self.feat_channels\n                cls_convs.append(\n                    ConvModule(\n                        chn,\n                        self.feat_channels,\n                        3,\n                        stride=1,\n                        padding=1,\n                        conv_cfg=self.conv_cfg,\n                        norm_cfg=self.norm_cfg))\n                reg_convs.append(\n                    ConvModule(\n                        chn,\n                        self.feat_channels,\n                        3,\n                        stride=1,\n                        padding=1,\n                        conv_cfg=self.conv_cfg,\n                        norm_cfg=self.norm_cfg))\n            self.cls_convs.append(cls_convs)\n            self.reg_convs.append(reg_convs)\n        for i in range(self.stacked_convs):\n            for j in range(1, self.num_ins):\n                self.cls_convs[j][i].conv = self.cls_convs[0][i].conv\n                self.reg_convs[j][i].conv = self.reg_convs[0][i].conv\n        self.retina_cls = nn.Conv2d(\n            self.feat_channels,\n            self.num_anchors * self.cls_out_channels,\n            3,\n            padding=1)\n        self.retina_reg = nn.Conv2d(\n            self.feat_channels, self.num_anchors * 4, 3, padding=1)\n\n    def init_weights(self):\n        for m in self.cls_convs[0]:\n            normal_init(m.conv, std=0.01)\n        for m in self.reg_convs[0]:\n            normal_init(m.conv, std=0.01)\n        bias_cls = bias_init_with_prob(0.01)\n        normal_init(self.retina_cls, std=0.01, bias=bias_cls)\n        normal_init(self.retina_reg, std=0.01)\n\n    def forward(self, feats):\n        cls_scores = []\n        bbox_preds = []\n        for i, x in enumerate(feats):\n            cls_feat = feats[i]\n            reg_feat = feats[i]\n            for cls_conv in self.cls_convs[i]:\n                cls_feat = cls_conv(cls_feat)\n            for reg_conv in self.reg_convs[i]:\n                reg_feat = reg_conv(reg_feat)\n            cls_score = self.retina_cls(cls_feat)\n            bbox_pred = self.retina_reg(reg_feat)\n            cls_scores.append(cls_score)\n            bbox_preds.append(bbox_pred)\n        return cls_scores, bbox_preds\n'"
mmdet/models/dense_heads/rpn_head.py,8,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import normal_init\n\nfrom mmdet.ops import batched_nms\nfrom ..builder import HEADS\nfrom .anchor_head import AnchorHead\n\n\n@HEADS.register_module()\nclass RPNHead(AnchorHead):\n\n    def __init__(self, in_channels, **kwargs):\n        super(RPNHead, self).__init__(\n            1, in_channels, background_label=0, **kwargs)\n\n    def _init_layers(self):\n        self.rpn_conv = nn.Conv2d(\n            self.in_channels, self.feat_channels, 3, padding=1)\n        self.rpn_cls = nn.Conv2d(self.feat_channels,\n                                 self.num_anchors * self.cls_out_channels, 1)\n        self.rpn_reg = nn.Conv2d(self.feat_channels, self.num_anchors * 4, 1)\n\n    def init_weights(self):\n        normal_init(self.rpn_conv, std=0.01)\n        normal_init(self.rpn_cls, std=0.01)\n        normal_init(self.rpn_reg, std=0.01)\n\n    def forward_single(self, x):\n        x = self.rpn_conv(x)\n        x = F.relu(x, inplace=True)\n        rpn_cls_score = self.rpn_cls(x)\n        rpn_bbox_pred = self.rpn_reg(x)\n        return rpn_cls_score, rpn_bbox_pred\n\n    def loss(self,\n             cls_scores,\n             bbox_preds,\n             gt_bboxes,\n             img_metas,\n             gt_bboxes_ignore=None):\n        losses = super(RPNHead, self).loss(\n            cls_scores,\n            bbox_preds,\n            gt_bboxes,\n            None,\n            img_metas,\n            gt_bboxes_ignore=gt_bboxes_ignore)\n        return dict(\n            loss_rpn_cls=losses['loss_cls'], loss_rpn_bbox=losses['loss_bbox'])\n\n    def _get_bboxes_single(self,\n                           cls_scores,\n                           bbox_preds,\n                           mlvl_anchors,\n                           img_shape,\n                           scale_factor,\n                           cfg,\n                           rescale=False):\n        cfg = self.test_cfg if cfg is None else cfg\n        # bboxes from different level should be independent during NMS,\n        # level_ids are used as labels for batched NMS to separate them\n        level_ids = []\n        mlvl_scores = []\n        mlvl_bbox_preds = []\n        mlvl_valid_anchors = []\n        for idx in range(len(cls_scores)):\n            rpn_cls_score = cls_scores[idx]\n            rpn_bbox_pred = bbox_preds[idx]\n            assert rpn_cls_score.size()[-2:] == rpn_bbox_pred.size()[-2:]\n            rpn_cls_score = rpn_cls_score.permute(1, 2, 0)\n            if self.use_sigmoid_cls:\n                rpn_cls_score = rpn_cls_score.reshape(-1)\n                scores = rpn_cls_score.sigmoid()\n            else:\n                rpn_cls_score = rpn_cls_score.reshape(-1, 2)\n                # remind that we set FG labels to [0, num_class-1]\n                # since mmdet v2.0\n                # BG cat_id: num_class\n                scores = rpn_cls_score.softmax(dim=1)[:, :-1]\n            rpn_bbox_pred = rpn_bbox_pred.permute(1, 2, 0).reshape(-1, 4)\n            anchors = mlvl_anchors[idx]\n            if cfg.nms_pre > 0 and scores.shape[0] > cfg.nms_pre:\n                # sort is faster than topk\n                # _, topk_inds = scores.topk(cfg.nms_pre)\n                ranked_scores, rank_inds = scores.sort(descending=True)\n                topk_inds = rank_inds[:cfg.nms_pre]\n                scores = ranked_scores[:cfg.nms_pre]\n                rpn_bbox_pred = rpn_bbox_pred[topk_inds, :]\n                anchors = anchors[topk_inds, :]\n            mlvl_scores.append(scores)\n            mlvl_bbox_preds.append(rpn_bbox_pred)\n            mlvl_valid_anchors.append(anchors)\n            level_ids.append(\n                scores.new_full((scores.size(0), ), idx, dtype=torch.long))\n\n        scores = torch.cat(mlvl_scores)\n        anchors = torch.cat(mlvl_valid_anchors)\n        rpn_bbox_pred = torch.cat(mlvl_bbox_preds)\n        proposals = self.bbox_coder.decode(\n            anchors, rpn_bbox_pred, max_shape=img_shape)\n        ids = torch.cat(level_ids)\n\n        if cfg.min_bbox_size > 0:\n            w = proposals[:, 2] - proposals[:, 0]\n            h = proposals[:, 3] - proposals[:, 1]\n            valid_inds = torch.nonzero(\n                (w >= cfg.min_bbox_size)\n                & (h >= cfg.min_bbox_size),\n                as_tuple=False).squeeze()\n            if valid_inds.sum().item() != len(proposals):\n                proposals = proposals[valid_inds, :]\n                scores = scores[valid_inds]\n                ids = ids[valid_inds]\n\n        # TODO: remove the hard coded nms type\n        nms_cfg = dict(type='nms', iou_thr=cfg.nms_thr)\n        dets, keep = batched_nms(proposals, scores, ids, nms_cfg)\n        return dets[:cfg.nms_post]\n"""
mmdet/models/dense_heads/ssd_head.py,11,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import xavier_init\n\nfrom mmdet.core import (build_anchor_generator, build_assigner,\n                        build_bbox_coder, build_sampler, multi_apply)\nfrom ..builder import HEADS\nfrom ..losses import smooth_l1_loss\nfrom .anchor_head import AnchorHead\n\n\n# TODO: add loss evaluator for SSD\n@HEADS.register_module()\nclass SSDHead(AnchorHead):\n\n    def __init__(self,\n                 num_classes=80,\n                 in_channels=(512, 1024, 512, 256, 256, 256),\n                 anchor_generator=dict(\n                     type='SSDAnchorGenerator',\n                     scale_major=False,\n                     input_size=300,\n                     strides=[8, 16, 32, 64, 100, 300],\n                     ratios=([2], [2, 3], [2, 3], [2, 3], [2], [2]),\n                     basesize_ratio_range=(0.1, 0.9)),\n                 background_label=None,\n                 bbox_coder=dict(\n                     type='DeltaXYWHBBoxCoder',\n                     target_means=[.0, .0, .0, .0],\n                     target_stds=[1.0, 1.0, 1.0, 1.0],\n                 ),\n                 reg_decoded_bbox=False,\n                 train_cfg=None,\n                 test_cfg=None):\n        super(AnchorHead, self).__init__()\n        self.num_classes = num_classes\n        self.in_channels = in_channels\n        self.cls_out_channels = num_classes + 1  # add background class\n        self.anchor_generator = build_anchor_generator(anchor_generator)\n        num_anchors = self.anchor_generator.num_base_anchors\n\n        reg_convs = []\n        cls_convs = []\n        for i in range(len(in_channels)):\n            reg_convs.append(\n                nn.Conv2d(\n                    in_channels[i],\n                    num_anchors[i] * 4,\n                    kernel_size=3,\n                    padding=1))\n            cls_convs.append(\n                nn.Conv2d(\n                    in_channels[i],\n                    num_anchors[i] * (num_classes + 1),\n                    kernel_size=3,\n                    padding=1))\n        self.reg_convs = nn.ModuleList(reg_convs)\n        self.cls_convs = nn.ModuleList(cls_convs)\n\n        self.background_label = (\n            num_classes if background_label is None else background_label)\n        # background_label should be either 0 or num_classes\n        assert (self.background_label == 0\n                or self.background_label == num_classes)\n\n        self.bbox_coder = build_bbox_coder(bbox_coder)\n        self.reg_decoded_bbox = reg_decoded_bbox\n        self.use_sigmoid_cls = False\n        self.cls_focal_loss = False\n        self.train_cfg = train_cfg\n        self.test_cfg = test_cfg\n        # set sampling=False for archor_target\n        self.sampling = False\n        if self.train_cfg:\n            self.assigner = build_assigner(self.train_cfg.assigner)\n            # SSD sampling=False so use PseudoSampler\n            sampler_cfg = dict(type='PseudoSampler')\n            self.sampler = build_sampler(sampler_cfg, context=self)\n        self.fp16_enabled = False\n\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                xavier_init(m, distribution='uniform', bias=0)\n\n    def forward(self, feats):\n        cls_scores = []\n        bbox_preds = []\n        for feat, reg_conv, cls_conv in zip(feats, self.reg_convs,\n                                            self.cls_convs):\n            cls_scores.append(cls_conv(feat))\n            bbox_preds.append(reg_conv(feat))\n        return cls_scores, bbox_preds\n\n    def loss_single(self, cls_score, bbox_pred, anchor, labels, label_weights,\n                    bbox_targets, bbox_weights, num_total_samples):\n        loss_cls_all = F.cross_entropy(\n            cls_score, labels, reduction='none') * label_weights\n        # FG cat_id: [0, num_classes -1], BG cat_id: num_classes\n        pos_inds = ((labels >= 0) &\n                    (labels < self.background_label)).nonzero().reshape(-1)\n        neg_inds = (labels == self.background_label).nonzero().view(-1)\n\n        num_pos_samples = pos_inds.size(0)\n        num_neg_samples = self.train_cfg.neg_pos_ratio * num_pos_samples\n        if num_neg_samples > neg_inds.size(0):\n            num_neg_samples = neg_inds.size(0)\n        topk_loss_cls_neg, _ = loss_cls_all[neg_inds].topk(num_neg_samples)\n        loss_cls_pos = loss_cls_all[pos_inds].sum()\n        loss_cls_neg = topk_loss_cls_neg.sum()\n        loss_cls = (loss_cls_pos + loss_cls_neg) / num_total_samples\n\n        if self.reg_decoded_bbox:\n            bbox_pred = self.bbox_coder.decode(anchor, bbox_pred)\n\n        loss_bbox = smooth_l1_loss(\n            bbox_pred,\n            bbox_targets,\n            bbox_weights,\n            beta=self.train_cfg.smoothl1_beta,\n            avg_factor=num_total_samples)\n        return loss_cls[None], loss_bbox\n\n    def loss(self,\n             cls_scores,\n             bbox_preds,\n             gt_bboxes,\n             gt_labels,\n             img_metas,\n             gt_bboxes_ignore=None):\n        featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n        assert len(featmap_sizes) == self.anchor_generator.num_levels\n\n        device = cls_scores[0].device\n\n        anchor_list, valid_flag_list = self.get_anchors(\n            featmap_sizes, img_metas, device=device)\n        cls_reg_targets = self.get_targets(\n            anchor_list,\n            valid_flag_list,\n            gt_bboxes,\n            img_metas,\n            gt_bboxes_ignore_list=gt_bboxes_ignore,\n            gt_labels_list=gt_labels,\n            label_channels=1,\n            unmap_outputs=False)\n        if cls_reg_targets is None:\n            return None\n        (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list,\n         num_total_pos, num_total_neg) = cls_reg_targets\n\n        num_images = len(img_metas)\n        all_cls_scores = torch.cat([\n            s.permute(0, 2, 3, 1).reshape(\n                num_images, -1, self.cls_out_channels) for s in cls_scores\n        ], 1)\n        all_labels = torch.cat(labels_list, -1).view(num_images, -1)\n        all_label_weights = torch.cat(label_weights_list,\n                                      -1).view(num_images, -1)\n        all_bbox_preds = torch.cat([\n            b.permute(0, 2, 3, 1).reshape(num_images, -1, 4)\n            for b in bbox_preds\n        ], -2)\n        all_bbox_targets = torch.cat(bbox_targets_list,\n                                     -2).view(num_images, -1, 4)\n        all_bbox_weights = torch.cat(bbox_weights_list,\n                                     -2).view(num_images, -1, 4)\n\n        # concat all level anchors to a single tensor\n        all_anchors = []\n        for i in range(num_images):\n            all_anchors.append(torch.cat(anchor_list[i]))\n\n        # check NaN and Inf\n        assert torch.isfinite(all_cls_scores).all().item(), \\\n            'classification scores become infinite or NaN!'\n        assert torch.isfinite(all_bbox_preds).all().item(), \\\n            'bbox predications become infinite or NaN!'\n\n        losses_cls, losses_bbox = multi_apply(\n            self.loss_single,\n            all_cls_scores,\n            all_bbox_preds,\n            all_anchors,\n            all_labels,\n            all_label_weights,\n            all_bbox_targets,\n            all_bbox_weights,\n            num_total_samples=num_total_pos)\n        return dict(loss_cls=losses_cls, loss_bbox=losses_bbox)\n"""
mmdet/models/detectors/__init__.py,0,"b""from .atss import ATSS\nfrom .base import BaseDetector\nfrom .cascade_rcnn import CascadeRCNN\nfrom .fast_rcnn import FastRCNN\nfrom .faster_rcnn import FasterRCNN\nfrom .fcos import FCOS\nfrom .fovea import FOVEA\nfrom .fsaf import FSAF\nfrom .grid_rcnn import GridRCNN\nfrom .htc import HybridTaskCascade\nfrom .mask_rcnn import MaskRCNN\nfrom .mask_scoring_rcnn import MaskScoringRCNN\nfrom .nasfcos import NASFCOS\nfrom .reppoints_detector import RepPointsDetector\nfrom .retinanet import RetinaNet\nfrom .rpn import RPN\nfrom .single_stage import SingleStageDetector\nfrom .two_stage import TwoStageDetector\n\n__all__ = [\n    'ATSS', 'BaseDetector', 'SingleStageDetector', 'TwoStageDetector', 'RPN',\n    'FastRCNN', 'FasterRCNN', 'MaskRCNN', 'CascadeRCNN', 'HybridTaskCascade',\n    'RetinaNet', 'FCOS', 'GridRCNN', 'MaskScoringRCNN', 'RepPointsDetector',\n    'FOVEA', 'FSAF', 'NASFCOS'\n]\n"""
mmdet/models/detectors/atss.py,0,"b'from ..builder import DETECTORS\nfrom .single_stage import SingleStageDetector\n\n\n@DETECTORS.register_module()\nclass ATSS(SingleStageDetector):\n\n    def __init__(self,\n                 backbone,\n                 neck,\n                 bbox_head,\n                 train_cfg=None,\n                 test_cfg=None,\n                 pretrained=None):\n        super(ATSS, self).__init__(backbone, neck, bbox_head, train_cfg,\n                                   test_cfg, pretrained)\n'"
mmdet/models/detectors/base.py,1,"b'import warnings\nfrom abc import ABCMeta, abstractmethod\n\nimport mmcv\nimport numpy as np\nimport torch.nn as nn\nfrom mmcv.utils import print_log\n\nfrom mmdet.core import auto_fp16\nfrom mmdet.utils import get_root_logger\n\n\nclass BaseDetector(nn.Module, metaclass=ABCMeta):\n    """"""Base class for detectors""""""\n\n    def __init__(self):\n        super(BaseDetector, self).__init__()\n        self.fp16_enabled = False\n\n    @property\n    def with_neck(self):\n        return hasattr(self, \'neck\') and self.neck is not None\n\n    # TODO: these properties need to be carefully handled\n    # for both single stage & two stage detectors\n    @property\n    def with_shared_head(self):\n        return hasattr(self.roi_head,\n                       \'shared_head\') and self.roi_head.shared_head is not None\n\n    @property\n    def with_bbox(self):\n        return ((hasattr(self.roi_head, \'bbox_head\')\n                 and self.roi_head.bbox_head is not None)\n                or (hasattr(self, \'bbox_head\') and self.bbox_head is not None))\n\n    @property\n    def with_mask(self):\n        return ((hasattr(self.roi_head, \'mask_head\')\n                 and self.roi_head.mask_head is not None)\n                or (hasattr(self, \'mask_head\') and self.mask_head is not None))\n\n    @abstractmethod\n    def extract_feat(self, imgs):\n        pass\n\n    def extract_feats(self, imgs):\n        assert isinstance(imgs, list)\n        return [self.extract_feat(img) for img in imgs]\n\n    @abstractmethod\n    def forward_train(self, imgs, img_metas, **kwargs):\n        """"""\n        Args:\n            img (list[Tensor]): List of tensors of shape (1, C, H, W).\n                Typically these should be mean centered and std scaled.\n            img_metas (list[dict]): List of image info dict where each dict\n                has: \'img_shape\', \'scale_factor\', \'flip\', and my also contain\n                \'filename\', \'ori_shape\', \'pad_shape\', and \'img_norm_cfg\'.\n                For details on the values of these keys, see\n                :class:`mmdet.datasets.pipelines.Collect`.\n            kwargs (keyword arguments): Specific to concrete implementation.\n        """"""\n        pass\n\n    async def async_simple_test(self, img, img_metas, **kwargs):\n        raise NotImplementedError\n\n    @abstractmethod\n    def simple_test(self, img, img_metas, **kwargs):\n        pass\n\n    @abstractmethod\n    def aug_test(self, imgs, img_metas, **kwargs):\n        pass\n\n    def init_weights(self, pretrained=None):\n        if pretrained is not None:\n            logger = get_root_logger()\n            print_log(f\'load model from: {pretrained}\', logger=logger)\n\n    async def aforward_test(self, *, img, img_metas, **kwargs):\n        for var, name in [(img, \'img\'), (img_metas, \'img_metas\')]:\n            if not isinstance(var, list):\n                raise TypeError(f\'{name} must be a list, but got {type(var)}\')\n\n        num_augs = len(img)\n        if num_augs != len(img_metas):\n            raise ValueError(f\'num of augmentations ({len(img)}) \'\n                             f\'!= num of image metas ({len(img_metas)})\')\n        # TODO: remove the restriction of samples_per_gpu == 1 when prepared\n        samples_per_gpu = img[0].size(0)\n        assert samples_per_gpu == 1\n\n        if num_augs == 1:\n            return await self.async_simple_test(img[0], img_metas[0], **kwargs)\n        else:\n            raise NotImplementedError\n\n    def forward_test(self, imgs, img_metas, **kwargs):\n        """"""\n        Args:\n            imgs (List[Tensor]): the outer list indicates test-time\n                augmentations and inner Tensor should have a shape NxCxHxW,\n                which contains all images in the batch.\n            img_metas (List[List[dict]]): the outer list indicates test-time\n                augs (multiscale, flip, etc.) and the inner list indicates\n                images in a batch.\n        """"""\n        for var, name in [(imgs, \'imgs\'), (img_metas, \'img_metas\')]:\n            if not isinstance(var, list):\n                raise TypeError(f\'{name} must be a list, but got {type(var)}\')\n\n        num_augs = len(imgs)\n        if num_augs != len(img_metas):\n            raise ValueError(f\'num of augmentations ({len(imgs)}) \'\n                             f\'!= num of image meta ({len(img_metas)})\')\n        # TODO: remove the restriction of samples_per_gpu == 1 when prepared\n        samples_per_gpu = imgs[0].size(0)\n        assert samples_per_gpu == 1\n\n        if num_augs == 1:\n            """"""\n            proposals (List[List[Tensor]]): the outer list indicates test-time\n                augs (multiscale, flip, etc.) and the inner list indicates\n                images in a batch. The Tensor should have a shape Px4, where\n                P is the number of proposals.\n            """"""\n            if \'proposals\' in kwargs:\n                kwargs[\'proposals\'] = kwargs[\'proposals\'][0]\n            return self.simple_test(imgs[0], img_metas[0], **kwargs)\n        else:\n            # TODO: support test augmentation for predefined proposals\n            assert \'proposals\' not in kwargs\n            return self.aug_test(imgs, img_metas, **kwargs)\n\n    @auto_fp16(apply_to=(\'img\', ))\n    def forward(self, img, img_metas, return_loss=True, **kwargs):\n        """"""\n        Calls either forward_train or forward_test depending on whether\n        return_loss=True. Note this setting will change the expected inputs.\n        When `return_loss=True`, img and img_meta are single-nested (i.e.\n        Tensor and List[dict]), and when `resturn_loss=False`, img and img_meta\n        should be double nested (i.e.  List[Tensor], List[List[dict]]), with\n        the outer list indicating test time augmentations.\n        """"""\n        if return_loss:\n            return self.forward_train(img, img_metas, **kwargs)\n        else:\n            return self.forward_test(img, img_metas, **kwargs)\n\n    def show_result(self,\n                    img,\n                    result,\n                    score_thr=0.3,\n                    bbox_color=\'green\',\n                    text_color=\'green\',\n                    thickness=1,\n                    font_scale=0.5,\n                    win_name=\'\',\n                    show=False,\n                    wait_time=0,\n                    out_file=None):\n        """"""Draw `result` over `img`.\n\n        Args:\n            img (str or Tensor): The image to be displayed.\n            result (Tensor or tuple): The results to draw over `img`\n                bbox_result or (bbox_result, segm_result).\n            score_thr (float, optional): Minimum score of bboxes to be shown.\n                Default: 0.3.\n            bbox_color (str or tuple or :obj:`Color`): Color of bbox lines.\n            text_color (str or tuple or :obj:`Color`): Color of texts.\n            thickness (int): Thickness of lines.\n            font_scale (float): Font scales of texts.\n            win_name (str): The window name.\n            wait_time (int): Value of waitKey param.\n                Default: 0.\n            show (bool): Whether to show the image.\n                Default: False.\n            out_file (str or None): The filename to write the image.\n                Default: None.\n\n        Returns:\n            img (Tensor): Only if not `show` or `out_file`\n        """"""\n        img = mmcv.imread(img)\n        img = img.copy()\n        if isinstance(result, tuple):\n            bbox_result, segm_result = result\n            if isinstance(segm_result, tuple):\n                segm_result = segm_result[0]  # ms rcnn\n        else:\n            bbox_result, segm_result = result, None\n        bboxes = np.vstack(bbox_result)\n        labels = [\n            np.full(bbox.shape[0], i, dtype=np.int32)\n            for i, bbox in enumerate(bbox_result)\n        ]\n        labels = np.concatenate(labels)\n        # draw segmentation masks\n        if segm_result is not None and len(labels) > 0:  # non empty\n            segms = mmcv.concat_list(segm_result)\n            inds = np.where(bboxes[:, -1] > score_thr)[0]\n            np.random.seed(42)\n            color_masks = [\n                np.random.randint(0, 256, (1, 3), dtype=np.uint8)\n                for _ in range(max(labels) + 1)\n            ]\n            for i in inds:\n                i = int(i)\n                color_mask = color_masks[labels[i]]\n                mask = segms[i]\n                img[mask] = img[mask] * 0.5 + color_mask * 0.5\n        # if out_file specified, do not show image in window\n        if out_file is not None:\n            show = False\n        # draw bounding boxes\n        mmcv.imshow_det_bboxes(\n            img,\n            bboxes,\n            labels,\n            class_names=self.CLASSES,\n            score_thr=score_thr,\n            bbox_color=bbox_color,\n            text_color=text_color,\n            thickness=thickness,\n            font_scale=font_scale,\n            win_name=win_name,\n            show=show,\n            wait_time=wait_time,\n            out_file=out_file)\n\n        if not (show or out_file):\n            warnings.warn(\'show==False and out_file is not specified, only \'\n                          \'result image will be returned\')\n            return img\n'"
mmdet/models/detectors/cascade_rcnn.py,0,"b""from ..builder import DETECTORS\nfrom .two_stage import TwoStageDetector\n\n\n@DETECTORS.register_module()\nclass CascadeRCNN(TwoStageDetector):\n\n    def __init__(self,\n                 backbone,\n                 neck=None,\n                 rpn_head=None,\n                 roi_head=None,\n                 train_cfg=None,\n                 test_cfg=None,\n                 pretrained=None):\n        super(CascadeRCNN, self).__init__(\n            backbone=backbone,\n            neck=neck,\n            rpn_head=rpn_head,\n            roi_head=roi_head,\n            train_cfg=train_cfg,\n            test_cfg=test_cfg,\n            pretrained=pretrained)\n\n    def show_result(self, data, result, **kwargs):\n        if self.with_mask:\n            ms_bbox_result, ms_segm_result = result\n            if isinstance(ms_bbox_result, dict):\n                result = (ms_bbox_result['ensemble'],\n                          ms_segm_result['ensemble'])\n        else:\n            if isinstance(result, dict):\n                result = result['ensemble']\n        return super(CascadeRCNN, self).show_result(data, result, **kwargs)\n"""
mmdet/models/detectors/fast_rcnn.py,0,"b'from ..builder import DETECTORS\nfrom .two_stage import TwoStageDetector\n\n\n@DETECTORS.register_module()\nclass FastRCNN(TwoStageDetector):\n\n    def __init__(self,\n                 backbone,\n                 roi_head,\n                 train_cfg,\n                 test_cfg,\n                 neck=None,\n                 pretrained=None):\n        super(FastRCNN, self).__init__(\n            backbone=backbone,\n            neck=neck,\n            roi_head=roi_head,\n            train_cfg=train_cfg,\n            test_cfg=test_cfg,\n            pretrained=pretrained)\n\n    def forward_test(self, imgs, img_metas, proposals, **kwargs):\n        """"""\n        Args:\n            imgs (List[Tensor]): the outer list indicates test-time\n                augmentations and inner Tensor should have a shape NxCxHxW,\n                which contains all images in the batch.\n            img_metas (List[List[dict]]): the outer list indicates test-time\n                augs (multiscale, flip, etc.) and the inner list indicates\n                images in a batch.\n            proposals (List[List[Tensor]]): the outer list indicates test-time\n                augs (multiscale, flip, etc.) and the inner list indicates\n                images in a batch. The Tensor should have a shape Px4, where\n                P is the number of proposals.\n        """"""\n        for var, name in [(imgs, \'imgs\'), (img_metas, \'img_metas\')]:\n            if not isinstance(var, list):\n                raise TypeError(f\'{name} must be a list, but got {type(var)}\')\n\n        num_augs = len(imgs)\n        if num_augs != len(img_metas):\n            raise ValueError(f\'num of augmentations ({len(imgs)}) \'\n                             f\'!= num of image meta ({len(img_metas)})\')\n        # TODO: remove the restriction of samples_per_gpu == 1 when prepared\n        samples_per_gpu = imgs[0].size(0)\n        assert samples_per_gpu == 1\n\n        if num_augs == 1:\n            return self.simple_test(imgs[0], img_metas[0], proposals[0],\n                                    **kwargs)\n        else:\n            # TODO: support test-time augmentation\n            assert NotImplementedError\n'"
mmdet/models/detectors/faster_rcnn.py,0,"b'from ..builder import DETECTORS\nfrom .two_stage import TwoStageDetector\n\n\n@DETECTORS.register_module()\nclass FasterRCNN(TwoStageDetector):\n\n    def __init__(self,\n                 backbone,\n                 rpn_head,\n                 roi_head,\n                 train_cfg,\n                 test_cfg,\n                 neck=None,\n                 pretrained=None):\n        super(FasterRCNN, self).__init__(\n            backbone=backbone,\n            neck=neck,\n            rpn_head=rpn_head,\n            roi_head=roi_head,\n            train_cfg=train_cfg,\n            test_cfg=test_cfg,\n            pretrained=pretrained)\n'"
mmdet/models/detectors/fcos.py,0,"b'from ..builder import DETECTORS\nfrom .single_stage import SingleStageDetector\n\n\n@DETECTORS.register_module()\nclass FCOS(SingleStageDetector):\n\n    def __init__(self,\n                 backbone,\n                 neck,\n                 bbox_head,\n                 train_cfg=None,\n                 test_cfg=None,\n                 pretrained=None):\n        super(FCOS, self).__init__(backbone, neck, bbox_head, train_cfg,\n                                   test_cfg, pretrained)\n'"
mmdet/models/detectors/fovea.py,0,"b'from ..builder import DETECTORS\nfrom .single_stage import SingleStageDetector\n\n\n@DETECTORS.register_module()\nclass FOVEA(SingleStageDetector):\n\n    def __init__(self,\n                 backbone,\n                 neck,\n                 bbox_head,\n                 train_cfg=None,\n                 test_cfg=None,\n                 pretrained=None):\n        super(FOVEA, self).__init__(backbone, neck, bbox_head, train_cfg,\n                                    test_cfg, pretrained)\n'"
mmdet/models/detectors/fsaf.py,0,"b'from ..builder import DETECTORS\nfrom .single_stage import SingleStageDetector\n\n\n@DETECTORS.register_module()\nclass FSAF(SingleStageDetector):\n\n    def __init__(self,\n                 backbone,\n                 neck,\n                 bbox_head,\n                 train_cfg=None,\n                 test_cfg=None,\n                 pretrained=None):\n        super(FSAF, self).__init__(backbone, neck, bbox_head, train_cfg,\n                                   test_cfg, pretrained)\n'"
mmdet/models/detectors/grid_rcnn.py,0,"b'from ..builder import DETECTORS\nfrom .two_stage import TwoStageDetector\n\n\n@DETECTORS.register_module()\nclass GridRCNN(TwoStageDetector):\n    """"""Grid R-CNN.\n\n    This detector is the implementation of:\n    - Grid R-CNN (https://arxiv.org/abs/1811.12030)\n    - Grid R-CNN Plus: Faster and Better (https://arxiv.org/abs/1906.05688)\n    """"""\n\n    def __init__(self,\n                 backbone,\n                 rpn_head,\n                 roi_head,\n                 train_cfg,\n                 test_cfg,\n                 neck=None,\n                 pretrained=None):\n        super(GridRCNN, self).__init__(\n            backbone=backbone,\n            neck=neck,\n            rpn_head=rpn_head,\n            roi_head=roi_head,\n            train_cfg=train_cfg,\n            test_cfg=test_cfg,\n            pretrained=pretrained)\n'"
mmdet/models/detectors/htc.py,0,"b'from ..builder import DETECTORS\nfrom .cascade_rcnn import CascadeRCNN\n\n\n@DETECTORS.register_module()\nclass HybridTaskCascade(CascadeRCNN):\n\n    def __init__(self, **kwargs):\n        super(HybridTaskCascade, self).__init__(**kwargs)\n\n    @property\n    def with_semantic(self):\n        return self.roi_head.with_semantic\n'"
mmdet/models/detectors/mask_rcnn.py,0,"b'from ..builder import DETECTORS\nfrom .two_stage import TwoStageDetector\n\n\n@DETECTORS.register_module()\nclass MaskRCNN(TwoStageDetector):\n\n    def __init__(self,\n                 backbone,\n                 rpn_head,\n                 roi_head,\n                 train_cfg,\n                 test_cfg,\n                 neck=None,\n                 pretrained=None):\n        super(MaskRCNN, self).__init__(\n            backbone=backbone,\n            neck=neck,\n            rpn_head=rpn_head,\n            roi_head=roi_head,\n            train_cfg=train_cfg,\n            test_cfg=test_cfg,\n            pretrained=pretrained)\n'"
mmdet/models/detectors/mask_scoring_rcnn.py,0,"b'from ..builder import DETECTORS\nfrom .two_stage import TwoStageDetector\n\n\n@DETECTORS.register_module()\nclass MaskScoringRCNN(TwoStageDetector):\n    """"""Mask Scoring RCNN.\n\n    https://arxiv.org/abs/1903.00241\n    """"""\n\n    def __init__(self,\n                 backbone,\n                 rpn_head,\n                 roi_head,\n                 train_cfg,\n                 test_cfg,\n                 neck=None,\n                 pretrained=None):\n        super(MaskScoringRCNN, self).__init__(\n            backbone=backbone,\n            neck=neck,\n            rpn_head=rpn_head,\n            roi_head=roi_head,\n            train_cfg=train_cfg,\n            test_cfg=test_cfg,\n            pretrained=pretrained)\n'"
mmdet/models/detectors/nasfcos.py,0,"b'from ..builder import DETECTORS\nfrom .single_stage import SingleStageDetector\n\n\n@DETECTORS.register_module()\nclass NASFCOS(SingleStageDetector):\n    """"""NAS-FCOS: Fast Neural Architecture Search for Object Detection.\n\n    https://arxiv.org/abs/1906.0442\n    """"""\n\n    def __init__(self,\n                 backbone,\n                 neck,\n                 bbox_head,\n                 train_cfg=None,\n                 test_cfg=None,\n                 pretrained=None):\n        super(NASFCOS, self).__init__(backbone, neck, bbox_head, train_cfg,\n                                      test_cfg, pretrained)\n'"
mmdet/models/detectors/reppoints_detector.py,2,"b'import torch\n\nfrom mmdet.core import bbox2result, bbox_mapping_back, multiclass_nms\nfrom ..builder import DETECTORS\nfrom .single_stage import SingleStageDetector\n\n\n@DETECTORS.register_module()\nclass RepPointsDetector(SingleStageDetector):\n    """"""RepPoints: Point Set Representation for Object Detection.\n\n        This detector is the implementation of:\n        - RepPoints detector (https://arxiv.org/pdf/1904.11490)\n    """"""\n\n    def __init__(self,\n                 backbone,\n                 neck,\n                 bbox_head,\n                 train_cfg=None,\n                 test_cfg=None,\n                 pretrained=None):\n        super(RepPointsDetector,\n              self).__init__(backbone, neck, bbox_head, train_cfg, test_cfg,\n                             pretrained)\n\n    def merge_aug_results(self, aug_bboxes, aug_scores, img_metas):\n        """"""Merge augmented detection bboxes and scores.\n\n        Args:\n            aug_bboxes (list[Tensor]): shape (n, 4*#class)\n            aug_scores (list[Tensor] or None): shape (n, #class)\n            img_shapes (list[Tensor]): shape (3, ).\n\n        Returns:\n            tuple: (bboxes, scores)\n        """"""\n        recovered_bboxes = []\n        for bboxes, img_info in zip(aug_bboxes, img_metas):\n            img_shape = img_info[0][\'img_shape\']\n            scale_factor = img_info[0][\'scale_factor\']\n            flip = img_info[0][\'flip\']\n            flip_direction = img_info[0][\'flip_direction\']\n            bboxes = bbox_mapping_back(bboxes, img_shape, scale_factor, flip,\n                                       flip_direction)\n            recovered_bboxes.append(bboxes)\n        bboxes = torch.cat(recovered_bboxes, dim=0)\n        if aug_scores is None:\n            return bboxes\n        else:\n            scores = torch.cat(aug_scores, dim=0)\n            return bboxes, scores\n\n    def aug_test(self, imgs, img_metas, rescale=False):\n        # recompute feats to save memory\n        feats = self.extract_feats(imgs)\n\n        aug_bboxes = []\n        aug_scores = []\n        for x, img_meta in zip(feats, img_metas):\n            # only one image in the batch\n            outs = self.bbox_head(x)\n            bbox_inputs = outs + (img_metas, self.test_cfg, False, False)\n            det_bboxes, det_scores = self.bbox_head.get_bboxes(*bbox_inputs)[0]\n            aug_bboxes.append(det_bboxes)\n            aug_scores.append(det_scores)\n\n        # after merging, bboxes will be rescaled to the original image size\n        merged_bboxes, merged_scores = self.merge_aug_results(\n            aug_bboxes, aug_scores, img_metas)\n        det_bboxes, det_labels = multiclass_nms(merged_bboxes, merged_scores,\n                                                self.test_cfg.score_thr,\n                                                self.test_cfg.nms,\n                                                self.test_cfg.max_per_img)\n\n        if rescale:\n            _det_bboxes = det_bboxes\n        else:\n            _det_bboxes = det_bboxes.clone()\n            _det_bboxes[:, :4] *= det_bboxes.new_tensor(\n                img_metas[0][0][\'scale_factor\'])\n        bbox_results = bbox2result(_det_bboxes, det_labels,\n                                   self.bbox_head.num_classes)\n        return bbox_results\n'"
mmdet/models/detectors/retinanet.py,0,"b'from ..builder import DETECTORS\nfrom .single_stage import SingleStageDetector\n\n\n@DETECTORS.register_module()\nclass RetinaNet(SingleStageDetector):\n\n    def __init__(self,\n                 backbone,\n                 neck,\n                 bbox_head,\n                 train_cfg=None,\n                 test_cfg=None,\n                 pretrained=None):\n        super(RetinaNet, self).__init__(backbone, neck, bbox_head, train_cfg,\n                                        test_cfg, pretrained)\n'"
mmdet/models/detectors/rpn.py,0,"b'import mmcv\n\nfrom mmdet.core import bbox_mapping, tensor2imgs\nfrom ..builder import DETECTORS, build_backbone, build_head, build_neck\nfrom .base import BaseDetector\nfrom .test_mixins import RPNTestMixin\n\n\n@DETECTORS.register_module()\nclass RPN(BaseDetector, RPNTestMixin):\n\n    def __init__(self,\n                 backbone,\n                 neck,\n                 rpn_head,\n                 train_cfg,\n                 test_cfg,\n                 pretrained=None):\n        super(RPN, self).__init__()\n        self.backbone = build_backbone(backbone)\n        self.neck = build_neck(neck) if neck is not None else None\n        rpn_train_cfg = train_cfg.rpn if train_cfg is not None else None\n        rpn_head.update(train_cfg=rpn_train_cfg)\n        rpn_head.update(test_cfg=test_cfg.rpn)\n        self.rpn_head = build_head(rpn_head)\n        self.train_cfg = train_cfg\n        self.test_cfg = test_cfg\n        self.init_weights(pretrained=pretrained)\n\n    def init_weights(self, pretrained=None):\n        super(RPN, self).init_weights(pretrained)\n        self.backbone.init_weights(pretrained=pretrained)\n        if self.with_neck:\n            self.neck.init_weights()\n        self.rpn_head.init_weights()\n\n    def extract_feat(self, img):\n        x = self.backbone(img)\n        if self.with_neck:\n            x = self.neck(x)\n        return x\n\n    def forward_dummy(self, img):\n        x = self.extract_feat(img)\n        rpn_outs = self.rpn_head(x)\n        return rpn_outs\n\n    def forward_train(self,\n                      img,\n                      img_metas,\n                      gt_bboxes=None,\n                      gt_bboxes_ignore=None):\n        """"""\n        Args:\n            img (Tensor): Input images of shape (N, C, H, W).\n                Typically these should be mean centered and std scaled.\n            img_metas (list[dict]): A List of image info dict where each dict\n                has: \'img_shape\', \'scale_factor\', \'flip\', and may also contain\n                \'filename\', \'ori_shape\', \'pad_shape\', and \'img_norm_cfg\'.\n                For details on the values of these keys see\n                :class:`mmdet.datasets.pipelines.Collect`.\n            gt_bboxes (list[Tensor]): Each item are the truth boxes for each\n                image in [tl_x, tl_y, br_x, br_y] format.\n            gt_bboxes_ignore (None | list[Tensor]): Specify which bounding\n                boxes can be ignored when computing the loss.\n\n        Returns:\n            dict[str, Tensor]: A dictionary of loss components.\n        """"""\n        if self.train_cfg.rpn.get(\'debug\', False):\n            self.rpn_head.debug_imgs = tensor2imgs(img)\n\n        x = self.extract_feat(img)\n        rpn_outs = self.rpn_head(x)\n\n        rpn_loss_inputs = rpn_outs + (gt_bboxes, img_metas)\n        losses = self.rpn_head.loss(\n            *rpn_loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)\n        return losses\n\n    def simple_test(self, img, img_metas, rescale=False):\n        x = self.extract_feat(img)\n        proposal_list = self.simple_test_rpn(x, img_metas)\n        if rescale:\n            for proposals, meta in zip(proposal_list, img_metas):\n                proposals[:, :4] /= proposals.new_tensor(meta[\'scale_factor\'])\n\n        # TODO: remove this restriction\n        return proposal_list[0].cpu().numpy()\n\n    def aug_test(self, imgs, img_metas, rescale=False):\n        proposal_list = self.aug_test_rpn(\n            self.extract_feats(imgs), img_metas, self.test_cfg.rpn)\n        if not rescale:\n            for proposals, img_meta in zip(proposal_list, img_metas[0]):\n                img_shape = img_meta[\'img_shape\']\n                scale_factor = img_meta[\'scale_factor\']\n                flip = img_meta[\'flip\']\n                flip_direction = img_meta[\'flip_direction\']\n                proposals[:, :4] = bbox_mapping(proposals[:, :4], img_shape,\n                                                scale_factor, flip,\n                                                flip_direction)\n        # TODO: remove this restriction\n        return proposal_list[0].cpu().numpy()\n\n    def show_result(self, data, result, dataset=None, top_k=20):\n        """"""Show RPN proposals on the image.\n\n        Although we assume batch size is 1, this method supports arbitrary\n        batch size.\n        """"""\n        img_tensor = data[\'img\'][0]\n        img_metas = data[\'img_metas\'][0].data[0]\n        imgs = tensor2imgs(img_tensor, **img_metas[0][\'img_norm_cfg\'])\n        assert len(imgs) == len(img_metas)\n        for img, img_meta in zip(imgs, img_metas):\n            h, w, _ = img_meta[\'img_shape\']\n            img_show = img[:h, :w, :]\n            mmcv.imshow_bboxes(img_show, result, top_k=top_k)\n'"
mmdet/models/detectors/single_stage.py,1,"b'import torch.nn as nn\n\nfrom mmdet.core import bbox2result\nfrom ..builder import DETECTORS, build_backbone, build_head, build_neck\nfrom .base import BaseDetector\n\n\n@DETECTORS.register_module()\nclass SingleStageDetector(BaseDetector):\n    """"""Base class for single-stage detectors.\n\n    Single-stage detectors directly and densely predict bounding boxes on the\n    output features of the backbone+neck.\n    """"""\n\n    def __init__(self,\n                 backbone,\n                 neck=None,\n                 bbox_head=None,\n                 train_cfg=None,\n                 test_cfg=None,\n                 pretrained=None):\n        super(SingleStageDetector, self).__init__()\n        self.backbone = build_backbone(backbone)\n        if neck is not None:\n            self.neck = build_neck(neck)\n        bbox_head.update(train_cfg=train_cfg)\n        bbox_head.update(test_cfg=test_cfg)\n        self.bbox_head = build_head(bbox_head)\n        self.train_cfg = train_cfg\n        self.test_cfg = test_cfg\n        self.init_weights(pretrained=pretrained)\n\n    def init_weights(self, pretrained=None):\n        super(SingleStageDetector, self).init_weights(pretrained)\n        self.backbone.init_weights(pretrained=pretrained)\n        if self.with_neck:\n            if isinstance(self.neck, nn.Sequential):\n                for m in self.neck:\n                    m.init_weights()\n            else:\n                self.neck.init_weights()\n        self.bbox_head.init_weights()\n\n    def extract_feat(self, img):\n        """"""Directly extract features from the backbone+neck\n        """"""\n        x = self.backbone(img)\n        if self.with_neck:\n            x = self.neck(x)\n        return x\n\n    def forward_dummy(self, img):\n        """"""Used for computing network flops.\n\n        See `mmdetection/tools/get_flops.py`\n        """"""\n        x = self.extract_feat(img)\n        outs = self.bbox_head(x)\n        return outs\n\n    def forward_train(self,\n                      img,\n                      img_metas,\n                      gt_bboxes,\n                      gt_labels,\n                      gt_bboxes_ignore=None):\n        """"""\n        Args:\n            img (Tensor): Input images of shape (N, C, H, W).\n                Typically these should be mean centered and std scaled.\n            img_metas (list[dict]): A List of image info dict where each dict\n                has: \'img_shape\', \'scale_factor\', \'flip\', and may also contain\n                \'filename\', \'ori_shape\', \'pad_shape\', and \'img_norm_cfg\'.\n                For details on the values of these keys see\n                :class:`mmdet.datasets.pipelines.Collect`.\n            gt_bboxes (list[Tensor]): Each item are the truth boxes for each\n                image in [tl_x, tl_y, br_x, br_y] format.\n            gt_labels (list[Tensor]): Class indices corresponding to each box\n            gt_bboxes_ignore (None | list[Tensor]): Specify which bounding\n                boxes can be ignored when computing the loss.\n\n        Returns:\n            dict[str, Tensor]: A dictionary of loss components.\n        """"""\n        x = self.extract_feat(img)\n        outs = self.bbox_head(x)\n        loss_inputs = outs + (gt_bboxes, gt_labels, img_metas)\n        losses = self.bbox_head.loss(\n            *loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)\n        return losses\n\n    def simple_test(self, img, img_metas, rescale=False):\n        x = self.extract_feat(img)\n        outs = self.bbox_head(x)\n        bbox_list = self.bbox_head.get_bboxes(\n            *outs, img_metas, rescale=rescale)\n        bbox_results = [\n            bbox2result(det_bboxes, det_labels, self.bbox_head.num_classes)\n            for det_bboxes, det_labels in bbox_list\n        ]\n        return bbox_results[0]\n\n    def aug_test(self, imgs, img_metas, rescale=False):\n        raise NotImplementedError\n'"
mmdet/models/detectors/test_mixins.py,0,"b""import logging\nimport sys\n\nfrom mmdet.core import merge_aug_proposals\n\nlogger = logging.getLogger(__name__)\n\nif sys.version_info >= (3, 7):\n    from mmdet.utils.contextmanagers import completed\n\n\nclass RPNTestMixin(object):\n\n    if sys.version_info >= (3, 7):\n\n        async def async_test_rpn(self, x, img_metas):\n            sleep_interval = self.rpn_head.test_cfg.pop(\n                'async_sleep_interval', 0.025)\n            async with completed(\n                    __name__, 'rpn_head_forward',\n                    sleep_interval=sleep_interval):\n                rpn_outs = self.rpn_head(x)\n\n            proposal_list = self.rpn_head.get_bboxes(*rpn_outs, img_metas)\n            return proposal_list\n\n    def simple_test_rpn(self, x, img_metas):\n        rpn_outs = self.rpn_head(x)\n        proposal_list = self.rpn_head.get_bboxes(*rpn_outs, img_metas)\n        return proposal_list\n\n    def aug_test_rpn(self, feats, img_metas):\n        samples_per_gpu = len(img_metas[0])\n        aug_proposals = [[] for _ in range(samples_per_gpu)]\n        for x, img_meta in zip(feats, img_metas):\n            proposal_list = self.simple_test_rpn(x, img_meta)\n            for i, proposals in enumerate(proposal_list):\n                aug_proposals[i].append(proposals)\n        # reorganize the order of 'img_metas' to match the dimensions\n        # of 'aug_proposals'\n        aug_img_metas = []\n        for i in range(samples_per_gpu):\n            aug_img_meta = []\n            for j in range(len(img_metas)):\n                aug_img_meta.append(img_metas[j][i])\n            aug_img_metas.append(aug_img_meta)\n        # after merging, proposals will be rescaled to the original image size\n        merged_proposals = [\n            merge_aug_proposals(proposals, aug_img_meta,\n                                self.rpn_head.test_cfg)\n            for proposals, aug_img_meta in zip(aug_proposals, aug_img_metas)\n        ]\n        return merged_proposals\n"""
mmdet/models/detectors/two_stage.py,2,"b'import torch\nimport torch.nn as nn\n\n# from mmdet.core import bbox2result, bbox2roi, build_assigner, build_sampler\nfrom ..builder import DETECTORS, build_backbone, build_head, build_neck\nfrom .base import BaseDetector\nfrom .test_mixins import RPNTestMixin\n\n\n@DETECTORS.register_module()\nclass TwoStageDetector(BaseDetector, RPNTestMixin):\n    """"""Base class for two-stage detectors.\n\n    Two-stage detectors typically consisting of a region proposal network and a\n    task-specific regression head.\n    """"""\n\n    def __init__(self,\n                 backbone,\n                 neck=None,\n                 rpn_head=None,\n                 roi_head=None,\n                 train_cfg=None,\n                 test_cfg=None,\n                 pretrained=None):\n        super(TwoStageDetector, self).__init__()\n        self.backbone = build_backbone(backbone)\n\n        if neck is not None:\n            self.neck = build_neck(neck)\n\n        if rpn_head is not None:\n            rpn_train_cfg = train_cfg.rpn if train_cfg is not None else None\n            rpn_head_ = rpn_head.copy()\n            rpn_head_.update(train_cfg=rpn_train_cfg, test_cfg=test_cfg.rpn)\n            self.rpn_head = build_head(rpn_head_)\n\n        if roi_head is not None:\n            # update train and test cfg here for now\n            # TODO: refactor assigner & sampler\n            rcnn_train_cfg = train_cfg.rcnn if train_cfg is not None else None\n            roi_head.update(train_cfg=rcnn_train_cfg)\n            roi_head.update(test_cfg=test_cfg.rcnn)\n            self.roi_head = build_head(roi_head)\n\n        self.train_cfg = train_cfg\n        self.test_cfg = test_cfg\n\n        self.init_weights(pretrained=pretrained)\n\n    @property\n    def with_rpn(self):\n        return hasattr(self, \'rpn_head\') and self.rpn_head is not None\n\n    @property\n    def with_roi_head(self):\n        return hasattr(self, \'roi_head\') and self.roi_head is not None\n\n    def init_weights(self, pretrained=None):\n        super(TwoStageDetector, self).init_weights(pretrained)\n        self.backbone.init_weights(pretrained=pretrained)\n        if self.with_neck:\n            if isinstance(self.neck, nn.Sequential):\n                for m in self.neck:\n                    m.init_weights()\n            else:\n                self.neck.init_weights()\n        if self.with_rpn:\n            self.rpn_head.init_weights()\n        if self.with_roi_head:\n            self.roi_head.init_weights(pretrained)\n\n    def extract_feat(self, img):\n        """"""Directly extract features from the backbone+neck\n        """"""\n        x = self.backbone(img)\n        if self.with_neck:\n            x = self.neck(x)\n        return x\n\n    def forward_dummy(self, img):\n        """"""Used for computing network flops.\n\n        See `mmdetection/tools/get_flops.py`\n        """"""\n        outs = ()\n        # backbone\n        x = self.extract_feat(img)\n        # rpn\n        if self.with_rpn:\n            rpn_outs = self.rpn_head(x)\n            outs = outs + (rpn_outs, )\n        proposals = torch.randn(1000, 4).to(img.device)\n        # roi_head\n        roi_outs = self.roi_head.forward_dummy(x, proposals)\n        outs = outs + (roi_outs, )\n        return outs\n\n    def forward_train(self,\n                      img,\n                      img_metas,\n                      gt_bboxes,\n                      gt_labels,\n                      gt_bboxes_ignore=None,\n                      gt_masks=None,\n                      proposals=None,\n                      **kwargs):\n        """"""\n        Args:\n            img (Tensor): of shape (N, C, H, W) encoding input images.\n                Typically these should be mean centered and std scaled.\n\n            img_metas (list[dict]): list of image info dict where each dict\n                has: \'img_shape\', \'scale_factor\', \'flip\', and may also contain\n                \'filename\', \'ori_shape\', \'pad_shape\', and \'img_norm_cfg\'.\n                For details on the values of these keys see\n                `mmdet/datasets/pipelines/formatting.py:Collect`.\n\n            gt_bboxes (list[Tensor]): each item are the truth boxes for each\n                image in [tl_x, tl_y, br_x, br_y] format.\n\n            gt_labels (list[Tensor]): class indices corresponding to each box\n\n            gt_bboxes_ignore (None | list[Tensor]): specify which bounding\n                boxes can be ignored when computing the loss.\n\n            gt_masks (None | Tensor) : true segmentation masks for each box\n                used if the architecture supports a segmentation task.\n\n            proposals : override rpn proposals with custom proposals. Use when\n                `with_rpn` is False.\n\n        Returns:\n            dict[str, Tensor]: a dictionary of loss components\n        """"""\n        x = self.extract_feat(img)\n\n        losses = dict()\n\n        # RPN forward and loss\n        if self.with_rpn:\n            rpn_outs = self.rpn_head(x)\n            rpn_loss_inputs = rpn_outs + (gt_bboxes, img_metas)\n            rpn_losses = self.rpn_head.loss(\n                *rpn_loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)\n            losses.update(rpn_losses)\n\n            proposal_cfg = self.train_cfg.get(\'rpn_proposal\',\n                                              self.test_cfg.rpn)\n            proposal_list = self.rpn_head.get_bboxes(\n                *rpn_outs, img_metas, cfg=proposal_cfg)\n        else:\n            proposal_list = proposals\n\n        roi_losses = self.roi_head.forward_train(x, img_metas, proposal_list,\n                                                 gt_bboxes, gt_labels,\n                                                 gt_bboxes_ignore, gt_masks,\n                                                 **kwargs)\n        losses.update(roi_losses)\n\n        return losses\n\n    async def async_simple_test(self,\n                                img,\n                                img_meta,\n                                proposals=None,\n                                rescale=False):\n        """"""Async test without augmentation.""""""\n        assert self.with_bbox, \'Bbox head must be implemented.\'\n        x = self.extract_feat(img)\n\n        if proposals is None:\n            proposal_list = await self.async_test_rpn(x, img_meta)\n        else:\n            proposal_list = proposals\n\n        return await self.roi_head.async_simple_test(\n            x, proposal_list, img_meta, rescale=rescale)\n\n    def simple_test(self, img, img_metas, proposals=None, rescale=False):\n        """"""Test without augmentation.""""""\n        assert self.with_bbox, \'Bbox head must be implemented.\'\n\n        x = self.extract_feat(img)\n\n        if proposals is None:\n            proposal_list = self.simple_test_rpn(x, img_metas)\n        else:\n            proposal_list = proposals\n\n        return self.roi_head.simple_test(\n            x, proposal_list, img_metas, rescale=rescale)\n\n    def aug_test(self, imgs, img_metas, rescale=False):\n        """"""Test with augmentations.\n\n        If rescale is False, then returned bboxes and masks will fit the scale\n        of imgs[0].\n        """"""\n        # recompute feats to save memory\n        x = self.extract_feats(imgs)\n        proposal_list = self.aug_test_rpn(x, img_metas)\n        return self.roi_head.aug_test(\n            x, proposal_list, img_metas, rescale=rescale)\n'"
mmdet/models/losses/__init__.py,0,"b""from .accuracy import Accuracy, accuracy\nfrom .balanced_l1_loss import BalancedL1Loss, balanced_l1_loss\nfrom .cross_entropy_loss import (CrossEntropyLoss, binary_cross_entropy,\n                                 cross_entropy, mask_cross_entropy)\nfrom .focal_loss import FocalLoss, sigmoid_focal_loss\nfrom .ghm_loss import GHMC, GHMR\nfrom .iou_loss import (BoundedIoULoss, GIoULoss, IoULoss, bounded_iou_loss,\n                       iou_loss)\nfrom .mse_loss import MSELoss, mse_loss\nfrom .pisa_loss import carl_loss, isr_p\nfrom .smooth_l1_loss import L1Loss, SmoothL1Loss, l1_loss, smooth_l1_loss\nfrom .utils import reduce_loss, weight_reduce_loss, weighted_loss\n\n__all__ = [\n    'accuracy', 'Accuracy', 'cross_entropy', 'binary_cross_entropy',\n    'mask_cross_entropy', 'CrossEntropyLoss', 'sigmoid_focal_loss',\n    'FocalLoss', 'smooth_l1_loss', 'SmoothL1Loss', 'balanced_l1_loss',\n    'BalancedL1Loss', 'mse_loss', 'MSELoss', 'iou_loss', 'bounded_iou_loss',\n    'IoULoss', 'BoundedIoULoss', 'GIoULoss', 'GHMC', 'GHMR', 'reduce_loss',\n    'weight_reduce_loss', 'weighted_loss', 'L1Loss', 'l1_loss', 'isr_p',\n    'carl_loss'\n]\n"""
mmdet/models/losses/accuracy.py,1,"b'import torch.nn as nn\n\n\ndef accuracy(pred, target, topk=1):\n    assert isinstance(topk, (int, tuple))\n    if isinstance(topk, int):\n        topk = (topk, )\n        return_single = True\n    else:\n        return_single = False\n\n    maxk = max(topk)\n    _, pred_label = pred.topk(maxk, dim=1)\n    pred_label = pred_label.t()\n    correct = pred_label.eq(target.view(1, -1).expand_as(pred_label))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n        res.append(correct_k.mul_(100.0 / pred.size(0)))\n    return res[0] if return_single else res\n\n\nclass Accuracy(nn.Module):\n\n    def __init__(self, topk=(1, )):\n        super().__init__()\n        self.topk = topk\n\n    def forward(self, pred, target):\n        return accuracy(pred, target, self.topk)\n'"
mmdet/models/losses/balanced_l1_loss.py,4,"b'import numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom ..builder import LOSSES\nfrom .utils import weighted_loss\n\n\n@weighted_loss\ndef balanced_l1_loss(pred,\n                     target,\n                     beta=1.0,\n                     alpha=0.5,\n                     gamma=1.5,\n                     reduction=\'mean\'):\n    assert beta > 0\n    assert pred.size() == target.size() and target.numel() > 0\n\n    diff = torch.abs(pred - target)\n    b = np.e**(gamma / alpha) - 1\n    loss = torch.where(\n        diff < beta, alpha / b *\n        (b * diff + 1) * torch.log(b * diff / beta + 1) - alpha * diff,\n        gamma * diff + gamma / b - alpha * beta)\n\n    return loss\n\n\n@LOSSES.register_module()\nclass BalancedL1Loss(nn.Module):\n    """"""Balanced L1 Loss\n\n    arXiv: https://arxiv.org/pdf/1904.02701.pdf (CVPR 2019)\n    """"""\n\n    def __init__(self,\n                 alpha=0.5,\n                 gamma=1.5,\n                 beta=1.0,\n                 reduction=\'mean\',\n                 loss_weight=1.0):\n        super(BalancedL1Loss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.beta = beta\n        self.reduction = reduction\n        self.loss_weight = loss_weight\n\n    def forward(self,\n                pred,\n                target,\n                weight=None,\n                avg_factor=None,\n                reduction_override=None,\n                **kwargs):\n        assert reduction_override in (None, \'none\', \'mean\', \'sum\')\n        reduction = (\n            reduction_override if reduction_override else self.reduction)\n        loss_bbox = self.loss_weight * balanced_l1_loss(\n            pred,\n            target,\n            weight,\n            alpha=self.alpha,\n            gamma=self.gamma,\n            beta=self.beta,\n            reduction=reduction,\n            avg_factor=avg_factor,\n            **kwargs)\n        return loss_bbox\n'"
mmdet/models/losses/cross_entropy_loss.py,4,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..builder import LOSSES\nfrom .utils import weight_reduce_loss\n\n\ndef cross_entropy(pred,\n                  label,\n                  weight=None,\n                  reduction='mean',\n                  avg_factor=None,\n                  class_weight=None):\n    # element-wise losses\n    loss = F.cross_entropy(pred, label, weight=class_weight, reduction='none')\n\n    # apply weights and do the reduction\n    if weight is not None:\n        weight = weight.float()\n    loss = weight_reduce_loss(\n        loss, weight=weight, reduction=reduction, avg_factor=avg_factor)\n\n    return loss\n\n\ndef _expand_binary_labels(labels, label_weights, label_channels):\n    # Caution: this function should only be used in RPN\n    # in other files such as in ghm_loss, the _expand_binary_labels\n    # is used for multi-class classification.\n    bin_labels = labels.new_full((labels.size(0), label_channels), 0)\n    inds = torch.nonzero(labels >= 1, as_tuple=False).squeeze()\n    if inds.numel() > 0:\n        bin_labels[inds, labels[inds] - 1] = 1\n    if label_weights is None:\n        bin_label_weights = None\n    else:\n        bin_label_weights = label_weights.view(-1, 1).expand(\n            label_weights.size(0), label_channels)\n    return bin_labels, bin_label_weights\n\n\ndef binary_cross_entropy(pred,\n                         label,\n                         weight=None,\n                         reduction='mean',\n                         avg_factor=None,\n                         class_weight=None):\n    if pred.dim() != label.dim():\n        label, weight = _expand_binary_labels(label, weight, pred.size(-1))\n\n    # weighted element-wise losses\n    if weight is not None:\n        weight = weight.float()\n    loss = F.binary_cross_entropy_with_logits(\n        pred, label.float(), weight=class_weight, reduction='none')\n    # do the reduction for the weighted loss\n    loss = weight_reduce_loss(\n        loss, weight, reduction=reduction, avg_factor=avg_factor)\n\n    return loss\n\n\ndef mask_cross_entropy(pred,\n                       target,\n                       label,\n                       reduction='mean',\n                       avg_factor=None,\n                       class_weight=None):\n    # TODO: handle these two reserved arguments\n    assert reduction == 'mean' and avg_factor is None\n    num_rois = pred.size()[0]\n    inds = torch.arange(0, num_rois, dtype=torch.long, device=pred.device)\n    pred_slice = pred[inds, label].squeeze(1)\n    return F.binary_cross_entropy_with_logits(\n        pred_slice, target, weight=class_weight, reduction='mean')[None]\n\n\n@LOSSES.register_module()\nclass CrossEntropyLoss(nn.Module):\n\n    def __init__(self,\n                 use_sigmoid=False,\n                 use_mask=False,\n                 reduction='mean',\n                 class_weight=None,\n                 loss_weight=1.0):\n        super(CrossEntropyLoss, self).__init__()\n        assert (use_sigmoid is False) or (use_mask is False)\n        self.use_sigmoid = use_sigmoid\n        self.use_mask = use_mask\n        self.reduction = reduction\n        self.loss_weight = loss_weight\n        self.class_weight = class_weight\n\n        if self.use_sigmoid:\n            self.cls_criterion = binary_cross_entropy\n        elif self.use_mask:\n            self.cls_criterion = mask_cross_entropy\n        else:\n            self.cls_criterion = cross_entropy\n\n    def forward(self,\n                cls_score,\n                label,\n                weight=None,\n                avg_factor=None,\n                reduction_override=None,\n                **kwargs):\n        assert reduction_override in (None, 'none', 'mean', 'sum')\n        reduction = (\n            reduction_override if reduction_override else self.reduction)\n        if self.class_weight is not None:\n            class_weight = cls_score.new_tensor(self.class_weight)\n        else:\n            class_weight = None\n        loss_cls = self.loss_weight * self.cls_criterion(\n            cls_score,\n            label,\n            weight,\n            class_weight=class_weight,\n            reduction=reduction,\n            avg_factor=avg_factor,\n            **kwargs)\n        return loss_cls\n"""
mmdet/models/losses/focal_loss.py,2,"b'import torch.nn as nn\nimport torch.nn.functional as F\n\nfrom mmdet.ops import sigmoid_focal_loss as _sigmoid_focal_loss\nfrom ..builder import LOSSES\nfrom .utils import weight_reduce_loss\n\n\n# This method is only for debugging\ndef py_sigmoid_focal_loss(pred,\n                          target,\n                          weight=None,\n                          gamma=2.0,\n                          alpha=0.25,\n                          reduction=\'mean\',\n                          avg_factor=None):\n    pred_sigmoid = pred.sigmoid()\n    target = target.type_as(pred)\n    pt = (1 - pred_sigmoid) * target + pred_sigmoid * (1 - target)\n    focal_weight = (alpha * target + (1 - alpha) *\n                    (1 - target)) * pt.pow(gamma)\n    loss = F.binary_cross_entropy_with_logits(\n        pred, target, reduction=\'none\') * focal_weight\n    loss = weight_reduce_loss(loss, weight, reduction, avg_factor)\n    return loss\n\n\ndef sigmoid_focal_loss(pred,\n                       target,\n                       weight=None,\n                       gamma=2.0,\n                       alpha=0.25,\n                       reduction=\'mean\',\n                       avg_factor=None):\n    # Function.apply does not accept keyword arguments, so the decorator\n    # ""weighted_loss"" is not applicable\n    loss = _sigmoid_focal_loss(pred, target, gamma, alpha)\n    if weight is not None:\n        if weight.shape != loss.shape:\n            if weight.size(0) == loss.size(0):\n                # For most cases, weight is of shape (num_priors, ),\n                #  which means it does not have the second axis num_class\n                weight = weight.view(-1, 1)\n            else:\n                # Sometimes, weight per anchor per class is also needed. e.g.\n                #  in FSAF. But it may be flattened of shape\n                #  (num_priors x num_class, ), while loss is still of shape\n                #  (num_priors, num_class).\n                assert weight.numel() == loss.numel()\n                weight = weight.view(loss.size(0), -1)\n        assert weight.ndim == loss.ndim\n    loss = weight_reduce_loss(loss, weight, reduction, avg_factor)\n    return loss\n\n\n@LOSSES.register_module()\nclass FocalLoss(nn.Module):\n\n    def __init__(self,\n                 use_sigmoid=True,\n                 gamma=2.0,\n                 alpha=0.25,\n                 reduction=\'mean\',\n                 loss_weight=1.0):\n        super(FocalLoss, self).__init__()\n        assert use_sigmoid is True, \'Only sigmoid focal loss supported now.\'\n        self.use_sigmoid = use_sigmoid\n        self.gamma = gamma\n        self.alpha = alpha\n        self.reduction = reduction\n        self.loss_weight = loss_weight\n\n    def forward(self,\n                pred,\n                target,\n                weight=None,\n                avg_factor=None,\n                reduction_override=None):\n        assert reduction_override in (None, \'none\', \'mean\', \'sum\')\n        reduction = (\n            reduction_override if reduction_override else self.reduction)\n        if self.use_sigmoid:\n            loss_cls = self.loss_weight * sigmoid_focal_loss(\n                pred,\n                target,\n                weight,\n                gamma=self.gamma,\n                alpha=self.alpha,\n                reduction=reduction,\n                avg_factor=avg_factor)\n        else:\n            raise NotImplementedError\n        return loss_cls\n'"
mmdet/models/losses/ghm_loss.py,12,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..builder import LOSSES\n\n\ndef _expand_onehot_labels(labels, label_weights, label_channels):\n    bin_labels = labels.new_full((labels.size(0), label_channels), 0)\n    inds = torch.nonzero(\n        (labels >= 0) & (labels < label_channels), as_tuple=False).squeeze()\n    if inds.numel() > 0:\n        bin_labels[inds, labels[inds]] = 1\n    bin_label_weights = label_weights.view(-1, 1).expand(\n        label_weights.size(0), label_channels)\n    return bin_labels, bin_label_weights\n\n\n# TODO: code refactoring to make it consistent with other losses\n@LOSSES.register_module()\nclass GHMC(nn.Module):\n    """"""GHM Classification Loss.\n\n    Details of the theorem can be viewed in the paper\n    ""Gradient Harmonized Single-stage Detector"".\n    https://arxiv.org/abs/1811.05181\n\n    Args:\n        bins (int): Number of the unit regions for distribution calculation.\n        momentum (float): The parameter for moving average.\n        use_sigmoid (bool): Can only be true for BCE based loss now.\n        loss_weight (float): The weight of the total GHM-C loss.\n    """"""\n\n    def __init__(self, bins=10, momentum=0, use_sigmoid=True, loss_weight=1.0):\n        super(GHMC, self).__init__()\n        self.bins = bins\n        self.momentum = momentum\n        edges = torch.arange(bins + 1).float() / bins\n        self.register_buffer(\'edges\', edges)\n        self.edges[-1] += 1e-6\n        if momentum > 0:\n            acc_sum = torch.zeros(bins)\n            self.register_buffer(\'acc_sum\', acc_sum)\n        self.use_sigmoid = use_sigmoid\n        if not self.use_sigmoid:\n            raise NotImplementedError\n        self.loss_weight = loss_weight\n\n    def forward(self, pred, target, label_weight, *args, **kwargs):\n        """"""Calculate the GHM-C loss.\n\n        Args:\n            pred (float tensor of size [batch_num, class_num]):\n                The direct prediction of classification fc layer.\n            target (float tensor of size [batch_num, class_num]):\n                Binary class target for each sample.\n            label_weight (float tensor of size [batch_num, class_num]):\n                the value is 1 if the sample is valid and 0 if ignored.\n        Returns:\n            The gradient harmonized loss.\n        """"""\n        # the target should be binary class label\n        if pred.dim() != target.dim():\n            target, label_weight = _expand_onehot_labels(\n                target, label_weight, pred.size(-1))\n        target, label_weight = target.float(), label_weight.float()\n        edges = self.edges\n        mmt = self.momentum\n        weights = torch.zeros_like(pred)\n\n        # gradient length\n        g = torch.abs(pred.sigmoid().detach() - target)\n\n        valid = label_weight > 0\n        tot = max(valid.float().sum().item(), 1.0)\n        n = 0  # n valid bins\n        for i in range(self.bins):\n            inds = (g >= edges[i]) & (g < edges[i + 1]) & valid\n            num_in_bin = inds.sum().item()\n            if num_in_bin > 0:\n                if mmt > 0:\n                    self.acc_sum[i] = mmt * self.acc_sum[i] \\\n                        + (1 - mmt) * num_in_bin\n                    weights[inds] = tot / self.acc_sum[i]\n                else:\n                    weights[inds] = tot / num_in_bin\n                n += 1\n        if n > 0:\n            weights = weights / n\n\n        loss = F.binary_cross_entropy_with_logits(\n            pred, target, weights, reduction=\'sum\') / tot\n        return loss * self.loss_weight\n\n\n# TODO: code refactoring to make it consistent with other losses\n@LOSSES.register_module()\nclass GHMR(nn.Module):\n    """"""GHM Regression Loss.\n\n    Details of the theorem can be viewed in the paper\n    ""Gradient Harmonized Single-stage Detector""\n    https://arxiv.org/abs/1811.05181\n\n    Args:\n        mu (float): The parameter for the Authentic Smooth L1 loss.\n        bins (int): Number of the unit regions for distribution calculation.\n        momentum (float): The parameter for moving average.\n        loss_weight (float): The weight of the total GHM-R loss.\n    """"""\n\n    def __init__(self, mu=0.02, bins=10, momentum=0, loss_weight=1.0):\n        super(GHMR, self).__init__()\n        self.mu = mu\n        self.bins = bins\n        edges = torch.arange(bins + 1).float() / bins\n        self.register_buffer(\'edges\', edges)\n        self.edges[-1] = 1e3\n        self.momentum = momentum\n        if momentum > 0:\n            acc_sum = torch.zeros(bins)\n            self.register_buffer(\'acc_sum\', acc_sum)\n        self.loss_weight = loss_weight\n\n    # TODO: support reduction parameter\n    def forward(self, pred, target, label_weight, avg_factor=None):\n        """"""Calculate the GHM-R loss.\n\n        Args:\n            pred (float tensor of size [batch_num, 4 (* class_num)]):\n                The prediction of box regression layer. Channel number can be 4\n                or 4 * class_num depending on whether it is class-agnostic.\n            target (float tensor of size [batch_num, 4 (* class_num)]):\n                The target regression values with the same size of pred.\n            label_weight (float tensor of size [batch_num, 4 (* class_num)]):\n                The weight of each sample, 0 if ignored.\n        Returns:\n            The gradient harmonized loss.\n        """"""\n        mu = self.mu\n        edges = self.edges\n        mmt = self.momentum\n\n        # ASL1 loss\n        diff = pred - target\n        loss = torch.sqrt(diff * diff + mu * mu) - mu\n\n        # gradient length\n        g = torch.abs(diff / torch.sqrt(mu * mu + diff * diff)).detach()\n        weights = torch.zeros_like(g)\n\n        valid = label_weight > 0\n        tot = max(label_weight.float().sum().item(), 1.0)\n        n = 0  # n: valid bins\n        for i in range(self.bins):\n            inds = (g >= edges[i]) & (g < edges[i + 1]) & valid\n            num_in_bin = inds.sum().item()\n            if num_in_bin > 0:\n                n += 1\n                if mmt > 0:\n                    self.acc_sum[i] = mmt * self.acc_sum[i] \\\n                        + (1 - mmt) * num_in_bin\n                    weights[inds] = tot / self.acc_sum[i]\n                else:\n                    weights[inds] = tot / num_in_bin\n        if n > 0:\n            weights /= n\n\n        loss = loss * weights\n        loss = loss.sum() / tot\n        return loss * self.loss_weight\n'"
mmdet/models/losses/iou_loss.py,17,"b'import torch\nimport torch.nn as nn\n\nfrom mmdet.core import bbox_overlaps\nfrom ..builder import LOSSES\nfrom .utils import weighted_loss\n\n\n@weighted_loss\ndef iou_loss(pred, target, eps=1e-6):\n    """"""IoU loss.\n\n    Computing the IoU loss between a set of predicted bboxes and target bboxes.\n    The loss is calculated as negative log of IoU.\n\n    Args:\n        pred (Tensor): Predicted bboxes of format (x1, y1, x2, y2),\n            shape (n, 4).\n        target (Tensor): Corresponding gt bboxes, shape (n, 4).\n        eps (float): Eps to avoid log(0).\n\n    Return:\n        Tensor: Loss tensor.\n    """"""\n    ious = bbox_overlaps(pred, target, is_aligned=True).clamp(min=eps)\n    loss = -ious.log()\n    return loss\n\n\n@weighted_loss\ndef bounded_iou_loss(pred, target, beta=0.2, eps=1e-3):\n    """"""Improving Object Localization with Fitness NMS and Bounded IoU Loss,\n    https://arxiv.org/abs/1711.00164.\n\n    Args:\n        pred (tensor): Predicted bboxes.\n        target (tensor): Target bboxes.\n        beta (float): beta parameter in smoothl1.\n        eps (float): eps to avoid NaN.\n    """"""\n    pred_ctrx = (pred[:, 0] + pred[:, 2]) * 0.5\n    pred_ctry = (pred[:, 1] + pred[:, 3]) * 0.5\n    pred_w = pred[:, 2] - pred[:, 0]\n    pred_h = pred[:, 3] - pred[:, 1]\n    with torch.no_grad():\n        target_ctrx = (target[:, 0] + target[:, 2]) * 0.5\n        target_ctry = (target[:, 1] + target[:, 3]) * 0.5\n        target_w = target[:, 2] - target[:, 0]\n        target_h = target[:, 3] - target[:, 1]\n\n    dx = target_ctrx - pred_ctrx\n    dy = target_ctry - pred_ctry\n\n    loss_dx = 1 - torch.max(\n        (target_w - 2 * dx.abs()) /\n        (target_w + 2 * dx.abs() + eps), torch.zeros_like(dx))\n    loss_dy = 1 - torch.max(\n        (target_h - 2 * dy.abs()) /\n        (target_h + 2 * dy.abs() + eps), torch.zeros_like(dy))\n    loss_dw = 1 - torch.min(target_w / (pred_w + eps), pred_w /\n                            (target_w + eps))\n    loss_dh = 1 - torch.min(target_h / (pred_h + eps), pred_h /\n                            (target_h + eps))\n    loss_comb = torch.stack([loss_dx, loss_dy, loss_dw, loss_dh],\n                            dim=-1).view(loss_dx.size(0), -1)\n\n    loss = torch.where(loss_comb < beta, 0.5 * loss_comb * loss_comb / beta,\n                       loss_comb - 0.5 * beta)\n    return loss\n\n\n@weighted_loss\ndef giou_loss(pred, target, eps=1e-7):\n    """"""\n    Generalized Intersection over Union: A Metric and A Loss for\n    Bounding Box Regression\n    https://arxiv.org/abs/1902.09630\n\n    code refer to:\n    https://github.com/sfzhang15/ATSS/blob/master/atss_core/modeling/rpn/atss/loss.py#L36\n\n    Args:\n        pred (Tensor): Predicted bboxes of format (x1, y1, x2, y2),\n            shape (n, 4).\n        target (Tensor): Corresponding gt bboxes, shape (n, 4).\n        eps (float): Eps to avoid log(0).\n\n    Return:\n        Tensor: Loss tensor.\n    """"""\n    # overlap\n    lt = torch.max(pred[:, :2], target[:, :2])\n    rb = torch.min(pred[:, 2:], target[:, 2:])\n    wh = (rb - lt).clamp(min=0)\n    overlap = wh[:, 0] * wh[:, 1]\n\n    # union\n    ap = (pred[:, 2] - pred[:, 0]) * (pred[:, 3] - pred[:, 1])\n    ag = (target[:, 2] - target[:, 0]) * (target[:, 3] - target[:, 1])\n    union = ap + ag - overlap + eps\n\n    # IoU\n    ious = overlap / union\n\n    # enclose area\n    enclose_x1y1 = torch.min(pred[:, :2], target[:, :2])\n    enclose_x2y2 = torch.max(pred[:, 2:], target[:, 2:])\n    enclose_wh = (enclose_x2y2 - enclose_x1y1).clamp(min=0)\n    enclose_area = enclose_wh[:, 0] * enclose_wh[:, 1] + eps\n\n    # GIoU\n    gious = ious - (enclose_area - union) / enclose_area\n    loss = 1 - gious\n    return loss\n\n\n@LOSSES.register_module()\nclass IoULoss(nn.Module):\n\n    def __init__(self, eps=1e-6, reduction=\'mean\', loss_weight=1.0):\n        super(IoULoss, self).__init__()\n        self.eps = eps\n        self.reduction = reduction\n        self.loss_weight = loss_weight\n\n    def forward(self,\n                pred,\n                target,\n                weight=None,\n                avg_factor=None,\n                reduction_override=None,\n                **kwargs):\n        if weight is not None and not torch.any(weight > 0):\n            return (pred * weight).sum()  # 0\n        assert reduction_override in (None, \'none\', \'mean\', \'sum\')\n        reduction = (\n            reduction_override if reduction_override else self.reduction)\n        if weight is not None and weight.dim() > 1:\n            # TODO: remove this in the future\n            # reduce the weight of shape (n, 4) to (n,) to match the\n            # iou_loss of shape (n,)\n            assert weight.shape == pred.shape\n            weight = weight.mean(-1)\n        loss = self.loss_weight * iou_loss(\n            pred,\n            target,\n            weight,\n            eps=self.eps,\n            reduction=reduction,\n            avg_factor=avg_factor,\n            **kwargs)\n        return loss\n\n\n@LOSSES.register_module()\nclass BoundedIoULoss(nn.Module):\n\n    def __init__(self, beta=0.2, eps=1e-3, reduction=\'mean\', loss_weight=1.0):\n        super(BoundedIoULoss, self).__init__()\n        self.beta = beta\n        self.eps = eps\n        self.reduction = reduction\n        self.loss_weight = loss_weight\n\n    def forward(self,\n                pred,\n                target,\n                weight=None,\n                avg_factor=None,\n                reduction_override=None,\n                **kwargs):\n        if weight is not None and not torch.any(weight > 0):\n            return (pred * weight).sum()  # 0\n        assert reduction_override in (None, \'none\', \'mean\', \'sum\')\n        reduction = (\n            reduction_override if reduction_override else self.reduction)\n        loss = self.loss_weight * bounded_iou_loss(\n            pred,\n            target,\n            weight,\n            beta=self.beta,\n            eps=self.eps,\n            reduction=reduction,\n            avg_factor=avg_factor,\n            **kwargs)\n        return loss\n\n\n@LOSSES.register_module()\nclass GIoULoss(nn.Module):\n\n    def __init__(self, eps=1e-6, reduction=\'mean\', loss_weight=1.0):\n        super(GIoULoss, self).__init__()\n        self.eps = eps\n        self.reduction = reduction\n        self.loss_weight = loss_weight\n\n    def forward(self,\n                pred,\n                target,\n                weight=None,\n                avg_factor=None,\n                reduction_override=None,\n                **kwargs):\n        if weight is not None and not torch.any(weight > 0):\n            return (pred * weight).sum()  # 0\n        assert reduction_override in (None, \'none\', \'mean\', \'sum\')\n        reduction = (\n            reduction_override if reduction_override else self.reduction)\n        if weight is not None and weight.dim() > 1:\n            # TODO: remove this in the future\n            # reduce the weight of shape (n, 4) to (n,) to match the\n            # giou_loss of shape (n,)\n            assert weight.shape == pred.shape\n            weight = weight.mean(-1)\n        loss = self.loss_weight * giou_loss(\n            pred,\n            target,\n            weight,\n            eps=self.eps,\n            reduction=reduction,\n            avg_factor=avg_factor,\n            **kwargs)\n        return loss\n'"
mmdet/models/losses/mse_loss.py,2,"b""import torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..builder import LOSSES\nfrom .utils import weighted_loss\n\n\n@weighted_loss\ndef mse_loss(pred, target):\n    return F.mse_loss(pred, target, reduction='none')\n\n\n@LOSSES.register_module()\nclass MSELoss(nn.Module):\n\n    def __init__(self, reduction='mean', loss_weight=1.0):\n        super().__init__()\n        self.reduction = reduction\n        self.loss_weight = loss_weight\n\n    def forward(self, pred, target, weight=None, avg_factor=None):\n        loss = self.loss_weight * mse_loss(\n            pred,\n            target,\n            weight,\n            reduction=self.reduction,\n            avg_factor=avg_factor)\n        return loss\n"""
mmdet/models/losses/pisa_loss.py,1,"b'import torch\n\nfrom mmdet.core import bbox_overlaps\n\n\ndef isr_p(cls_score,\n          bbox_pred,\n          bbox_targets,\n          rois,\n          sampling_results,\n          loss_cls,\n          bbox_coder,\n          k=2,\n          bias=0,\n          num_class=80):\n    """"""Importance-based Sample Reweighting (ISR_P), positive part.\n\n    Args:\n        cls_score (Tensor): Predicted classification scores.\n        bbox_pred (Tensor): Predicted bbox deltas.\n        bbox_targets (tuple[Tensor]): A tuple of bbox targets, the are\n            labels, label_weights, bbox_targets, bbox_weights, respectively.\n        rois (Tensor): Anchors (single_stage) in shape (n, 4) or RoIs\n            (two_stage) in shape (n, 5).\n        sampling_results (obj): Sampling results.\n        loss_cls (func): Classification loss func of the head.\n        bbox_coder (obj): BBox coder of the head.\n        k (float): Power of the non-linear mapping.\n        bias (float): Shift of the non-linear mapping.\n        num_class (int): Number of classes, default: 80.\n\n    Return:\n        tuple([Tensor]): labels, imp_based_label_weights, bbox_targets,\n            bbox_target_weights\n    """"""\n\n    labels, label_weights, bbox_targets, bbox_weights = bbox_targets\n    pos_label_inds = ((labels >= 0) &\n                      (labels < num_class)).nonzero().reshape(-1)\n    pos_labels = labels[pos_label_inds]\n\n    # if no positive samples, return the original targets\n    num_pos = float(pos_label_inds.size(0))\n    if num_pos == 0:\n        return labels, label_weights, bbox_targets, bbox_weights\n\n    # merge pos_assigned_gt_inds of per image to a single tensor\n    gts = list()\n    last_max_gt = 0\n    for i in range(len(sampling_results)):\n        gt_i = sampling_results[i].pos_assigned_gt_inds\n        gts.append(gt_i + last_max_gt)\n        last_max_gt = gt_i.max() + 1\n    gts = torch.cat(gts)\n    assert len(gts) == num_pos\n\n    cls_score = cls_score.detach()\n    bbox_pred = bbox_pred.detach()\n\n    # For single stage detectors, rois here indicate anchors, in shape (N, 4)\n    # For two stage detectors, rois are in shape (N, 5)\n    if rois.size(-1) == 5:\n        pos_rois = rois[pos_label_inds][:, 1:]\n    else:\n        pos_rois = rois[pos_label_inds]\n\n    if bbox_pred.size(-1) > 4:\n        bbox_pred = bbox_pred.view(bbox_pred.size(0), -1, 4)\n        pos_delta_pred = bbox_pred[pos_label_inds, pos_labels].view(-1, 4)\n    else:\n        pos_delta_pred = bbox_pred[pos_label_inds].view(-1, 4)\n\n    # compute iou of the predicted bbox and the corresponding GT\n    pos_delta_target = bbox_targets[pos_label_inds].view(-1, 4)\n    pos_bbox_pred = bbox_coder.decode(pos_rois, pos_delta_pred)\n    target_bbox_pred = bbox_coder.decode(pos_rois, pos_delta_target)\n    ious = bbox_overlaps(pos_bbox_pred, target_bbox_pred, is_aligned=True)\n\n    pos_imp_weights = label_weights[pos_label_inds]\n    # Two steps to compute IoU-HLR. Samples are first sorted by IoU locally,\n    # then sorted again within the same-rank group\n    max_l_num = pos_labels.bincount().max()\n    for label in pos_labels.unique():\n        l_inds = (pos_labels == label).nonzero().view(-1)\n        l_gts = gts[l_inds]\n        for t in l_gts.unique():\n            t_inds = l_inds[l_gts == t]\n            t_ious = ious[t_inds]\n            _, t_iou_rank_idx = t_ious.sort(descending=True)\n            _, t_iou_rank = t_iou_rank_idx.sort()\n            ious[t_inds] += max_l_num - t_iou_rank.float()\n        l_ious = ious[l_inds]\n        _, l_iou_rank_idx = l_ious.sort(descending=True)\n        _, l_iou_rank = l_iou_rank_idx.sort()  # IoU-HLR\n        # linearly map HLR to label weights\n        pos_imp_weights[l_inds] *= (max_l_num - l_iou_rank.float()) / max_l_num\n\n    pos_imp_weights = (bias + pos_imp_weights * (1 - bias)).pow(k)\n\n    # normalize to make the new weighted loss value equal to the original loss\n    pos_loss_cls = loss_cls(\n        cls_score[pos_label_inds], pos_labels, reduction_override=\'none\')\n    if pos_loss_cls.dim() > 1:\n        ori_pos_loss_cls = pos_loss_cls * label_weights[pos_label_inds][:,\n                                                                        None]\n        new_pos_loss_cls = pos_loss_cls * pos_imp_weights[:, None]\n    else:\n        ori_pos_loss_cls = pos_loss_cls * label_weights[pos_label_inds]\n        new_pos_loss_cls = pos_loss_cls * pos_imp_weights\n    pos_loss_cls_ratio = ori_pos_loss_cls.sum() / new_pos_loss_cls.sum()\n    pos_imp_weights = pos_imp_weights * pos_loss_cls_ratio\n    label_weights[pos_label_inds] = pos_imp_weights\n\n    bbox_targets = labels, label_weights, bbox_targets, bbox_weights\n    return bbox_targets\n\n\ndef carl_loss(cls_score,\n              labels,\n              bbox_pred,\n              bbox_targets,\n              loss_bbox,\n              k=1,\n              bias=0.2,\n              avg_factor=None,\n              sigmoid=False,\n              num_class=80):\n    """"""Classification-Aware Regression Loss (CARL).\n\n    Args:\n        cls_score (Tensor): Predicted classification scores.\n        labels (Tensor): Targets of classification.\n        bbox_pred (Tensor): Predicted bbox deltas.\n        bbox_targets (Tensor): Target of bbox regression.\n        loss_bbox (func): Regression loss func of the head.\n        bbox_coder (obj): BBox coder of the head.\n        k (float): Power of the non-linear mapping.\n        bias (float): Shift of the non-linear mapping.\n        avg_factor (int): Average factor used in regression loss.\n        sigmoid (bool): Activation of the classification score.\n        num_class (int): Number of classes, default: 80.\n\n    Return:\n        dict: CARL loss dict.\n    """"""\n    pos_label_inds = ((labels >= 0) &\n                      (labels < num_class)).nonzero().reshape(-1)\n    if pos_label_inds.numel() == 0:\n        return dict(loss_carl=cls_score.sum()[None] * 0.)\n    pos_labels = labels[pos_label_inds]\n\n    # multiply pos_cls_score with the corresponding bbox weight\n    # and remain gradient\n    if sigmoid:\n        pos_cls_score = cls_score.sigmoid()[pos_label_inds, pos_labels]\n    else:\n        pos_cls_score = cls_score.softmax(-1)[pos_label_inds, pos_labels]\n    carl_loss_weights = (bias + (1 - bias) * pos_cls_score).pow(k)\n\n    # normalize carl_loss_weight to make its sum equal to num positive\n    num_pos = float(pos_cls_score.size(0))\n    weight_ratio = num_pos / carl_loss_weights.sum()\n    carl_loss_weights *= weight_ratio\n\n    if avg_factor is None:\n        avg_factor = bbox_targets.size(0)\n    # if is class agnostic, bbox pred is in shape (N, 4)\n    # otherwise, bbox pred is in shape (N, #classes, 4)\n    if bbox_pred.size(-1) > 4:\n        bbox_pred = bbox_pred.view(bbox_pred.size(0), -1, 4)\n        pos_bbox_preds = bbox_pred[pos_label_inds, pos_labels]\n    else:\n        pos_bbox_preds = bbox_pred[pos_label_inds]\n    ori_loss_reg = loss_bbox(\n        pos_bbox_preds,\n        bbox_targets[pos_label_inds],\n        reduction_override=\'none\') / avg_factor\n    loss_carl = (ori_loss_reg * carl_loss_weights[:, None]).sum()\n    return dict(loss_carl=loss_carl[None])\n'"
mmdet/models/losses/smooth_l1_loss.py,4,"b""import torch\nimport torch.nn as nn\n\nfrom ..builder import LOSSES\nfrom .utils import weighted_loss\n\n\n@weighted_loss\ndef smooth_l1_loss(pred, target, beta=1.0):\n    assert beta > 0\n    assert pred.size() == target.size() and target.numel() > 0\n    diff = torch.abs(pred - target)\n    loss = torch.where(diff < beta, 0.5 * diff * diff / beta,\n                       diff - 0.5 * beta)\n    return loss\n\n\n@weighted_loss\ndef l1_loss(pred, target):\n    assert pred.size() == target.size() and target.numel() > 0\n    loss = torch.abs(pred - target)\n    return loss\n\n\n@LOSSES.register_module()\nclass SmoothL1Loss(nn.Module):\n\n    def __init__(self, beta=1.0, reduction='mean', loss_weight=1.0):\n        super(SmoothL1Loss, self).__init__()\n        self.beta = beta\n        self.reduction = reduction\n        self.loss_weight = loss_weight\n\n    def forward(self,\n                pred,\n                target,\n                weight=None,\n                avg_factor=None,\n                reduction_override=None,\n                **kwargs):\n        assert reduction_override in (None, 'none', 'mean', 'sum')\n        reduction = (\n            reduction_override if reduction_override else self.reduction)\n        loss_bbox = self.loss_weight * smooth_l1_loss(\n            pred,\n            target,\n            weight,\n            beta=self.beta,\n            reduction=reduction,\n            avg_factor=avg_factor,\n            **kwargs)\n        return loss_bbox\n\n\n@LOSSES.register_module()\nclass L1Loss(nn.Module):\n\n    def __init__(self, reduction='mean', loss_weight=1.0):\n        super(L1Loss, self).__init__()\n        self.reduction = reduction\n        self.loss_weight = loss_weight\n\n    def forward(self,\n                pred,\n                target,\n                weight=None,\n                avg_factor=None,\n                reduction_override=None):\n        assert reduction_override in (None, 'none', 'mean', 'sum')\n        reduction = (\n            reduction_override if reduction_override else self.reduction)\n        loss_bbox = self.loss_weight * l1_loss(\n            pred, target, weight, reduction=reduction, avg_factor=avg_factor)\n        return loss_bbox\n"""
mmdet/models/losses/utils.py,4,"b'import functools\n\nimport torch.nn.functional as F\n\n\ndef reduce_loss(loss, reduction):\n    """"""Reduce loss as specified.\n\n    Args:\n        loss (Tensor): Elementwise loss tensor.\n        reduction (str): Options are ""none"", ""mean"" and ""sum"".\n\n    Return:\n        Tensor: Reduced loss tensor.\n    """"""\n    reduction_enum = F._Reduction.get_enum(reduction)\n    # none: 0, elementwise_mean:1, sum: 2\n    if reduction_enum == 0:\n        return loss\n    elif reduction_enum == 1:\n        return loss.mean()\n    elif reduction_enum == 2:\n        return loss.sum()\n\n\ndef weight_reduce_loss(loss, weight=None, reduction=\'mean\', avg_factor=None):\n    """"""Apply element-wise weight and reduce loss.\n\n    Args:\n        loss (Tensor): Element-wise loss.\n        weight (Tensor): Element-wise weights.\n        reduction (str): Same as built-in losses of PyTorch.\n        avg_factor (float): Avarage factor when computing the mean of losses.\n\n    Returns:\n        Tensor: Processed loss values.\n    """"""\n    # if weight is specified, apply element-wise weight\n    if weight is not None:\n        loss = loss * weight\n\n    # if avg_factor is not specified, just reduce the loss\n    if avg_factor is None:\n        loss = reduce_loss(loss, reduction)\n    else:\n        # if reduction is mean, then average the loss by avg_factor\n        if reduction == \'mean\':\n            loss = loss.sum() / avg_factor\n        # if reduction is \'none\', then do nothing, otherwise raise an error\n        elif reduction != \'none\':\n            raise ValueError(\'avg_factor can not be used with reduction=""sum""\')\n    return loss\n\n\ndef weighted_loss(loss_func):\n    """"""Create a weighted version of a given loss function.\n\n    To use this decorator, the loss function must have the signature like\n    `loss_func(pred, target, **kwargs)`. The function only needs to compute\n    element-wise loss without any reduction. This decorator will add weight\n    and reduction arguments to the function. The decorated function will have\n    the signature like `loss_func(pred, target, weight=None, reduction=\'mean\',\n    avg_factor=None, **kwargs)`.\n\n    :Example:\n\n    >>> import torch\n    >>> @weighted_loss\n    >>> def l1_loss(pred, target):\n    >>>     return (pred - target).abs()\n\n    >>> pred = torch.Tensor([0, 2, 3])\n    >>> target = torch.Tensor([1, 1, 1])\n    >>> weight = torch.Tensor([1, 0, 1])\n\n    >>> l1_loss(pred, target)\n    tensor(1.3333)\n    >>> l1_loss(pred, target, weight)\n    tensor(1.)\n    >>> l1_loss(pred, target, reduction=\'none\')\n    tensor([1., 1., 2.])\n    >>> l1_loss(pred, target, weight, avg_factor=2)\n    tensor(1.5000)\n    """"""\n\n    @functools.wraps(loss_func)\n    def wrapper(pred,\n                target,\n                weight=None,\n                reduction=\'mean\',\n                avg_factor=None,\n                **kwargs):\n        # get element-wise loss\n        loss = loss_func(pred, target, **kwargs)\n        loss = weight_reduce_loss(loss, weight, reduction, avg_factor)\n        return loss\n\n    return wrapper\n'"
mmdet/models/necks/__init__.py,0,"b""from .bfp import BFP\nfrom .fpn import FPN\nfrom .fpn_carafe import FPN_CARAFE\nfrom .hrfpn import HRFPN\nfrom .nas_fpn import NASFPN\nfrom .nasfcos_fpn import NASFCOS_FPN\nfrom .pafpn import PAFPN\n\n__all__ = [\n    'FPN', 'BFP', 'HRFPN', 'NASFPN', 'FPN_CARAFE', 'PAFPN', 'NASFCOS_FPN'\n]\n"""
mmdet/models/necks/bfp.py,2,"b'import torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import ConvModule, xavier_init\n\nfrom mmdet.ops import NonLocal2D\nfrom ..builder import NECKS\n\n\n@NECKS.register_module()\nclass BFP(nn.Module):\n    """"""BFP (Balanced Feature Pyrmamids)\n\n    BFP takes multi-level features as inputs and gather them into a single one,\n    then refine the gathered feature and scatter the refined results to\n    multi-level features. This module is used in Libra R-CNN (CVPR 2019), see\n    https://arxiv.org/pdf/1904.02701.pdf for details.\n\n    Args:\n        in_channels (int): Number of input channels (feature maps of all levels\n            should have the same channels).\n        num_levels (int): Number of input feature levels.\n        conv_cfg (dict): The config dict for convolution layers.\n        norm_cfg (dict): The config dict for normalization layers.\n        refine_level (int): Index of integration and refine level of BSF in\n            multi-level features from bottom to top.\n        refine_type (str): Type of the refine op, currently support\n            [None, \'conv\', \'non_local\'].\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 num_levels,\n                 refine_level=2,\n                 refine_type=None,\n                 conv_cfg=None,\n                 norm_cfg=None):\n        super(BFP, self).__init__()\n        assert refine_type in [None, \'conv\', \'non_local\']\n\n        self.in_channels = in_channels\n        self.num_levels = num_levels\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n\n        self.refine_level = refine_level\n        self.refine_type = refine_type\n        assert 0 <= self.refine_level < self.num_levels\n\n        if self.refine_type == \'conv\':\n            self.refine = ConvModule(\n                self.in_channels,\n                self.in_channels,\n                3,\n                padding=1,\n                conv_cfg=self.conv_cfg,\n                norm_cfg=self.norm_cfg)\n        elif self.refine_type == \'non_local\':\n            self.refine = NonLocal2D(\n                self.in_channels,\n                reduction=1,\n                use_scale=False,\n                conv_cfg=self.conv_cfg,\n                norm_cfg=self.norm_cfg)\n\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                xavier_init(m, distribution=\'uniform\')\n\n    def forward(self, inputs):\n        assert len(inputs) == self.num_levels\n\n        # step 1: gather multi-level features by resize and average\n        feats = []\n        gather_size = inputs[self.refine_level].size()[2:]\n        for i in range(self.num_levels):\n            if i < self.refine_level:\n                gathered = F.adaptive_max_pool2d(\n                    inputs[i], output_size=gather_size)\n            else:\n                gathered = F.interpolate(\n                    inputs[i], size=gather_size, mode=\'nearest\')\n            feats.append(gathered)\n\n        bsf = sum(feats) / len(feats)\n\n        # step 2: refine gathered features\n        if self.refine_type is not None:\n            bsf = self.refine(bsf)\n\n        # step 3: scatter refined features to multi-levels by a residual path\n        outs = []\n        for i in range(self.num_levels):\n            out_size = inputs[i].size()[2:]\n            if i < self.refine_level:\n                residual = F.interpolate(bsf, size=out_size, mode=\'nearest\')\n            else:\n                residual = F.adaptive_max_pool2d(bsf, output_size=out_size)\n            outs.append(residual + inputs[i])\n\n        return tuple(outs)\n'"
mmdet/models/necks/fpn.py,7,"b'import torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import ConvModule, xavier_init\n\nfrom mmdet.core import auto_fp16\nfrom ..builder import NECKS\n\n\n@NECKS.register_module()\nclass FPN(nn.Module):\n    """"""\n    Feature Pyramid Network.\n\n    This is an implementation of - Feature Pyramid Networks for Object\n    Detection (https://arxiv.org/abs/1612.03144)\n\n    Args:\n        in_channels (List[int]): Number of input channels per scale.\n        out_channels (int): Number of output channels (used at each scale)\n        num_outs (int): Number of output scales.\n        start_level (int): Index of the start input backbone level used to\n            build the feature pyramid. Default: 0.\n        end_level (int): Index of the end input backbone level (exclusive) to\n            build the feature pyramid. Default: -1, which means the last level.\n        add_extra_convs (bool | str): If bool, it decides whether to add conv\n            layers on top of the original feature maps. Default to False.\n            If True, its actual mode is specified by `extra_convs_on_inputs`.\n            If str, it specifies the source feature map of the extra convs.\n            Only the following options are allowed\n\n            - \'on_input\': Last feat map of neck inputs (i.e. backbone feature).\n            - \'on_lateral\':  Last feature map after lateral convs.\n            - \'on_output\': The last output feature map after fpn convs.\n        extra_convs_on_inputs (bool, deprecated): Whether to apply extra convs\n            on the original feature from the backbone. If True,\n            it is equivalent to `add_extra_convs=\'on_input\'`. If False, it is\n            equivalent to set `add_extra_convs=\'on_output\'`. Default to True.\n        relu_before_extra_convs (bool): Whether to apply relu before the extra\n            conv. Default: False.\n        no_norm_on_lateral (bool): Whether to apply norm on lateral.\n            Default: False.\n        conv_cfg (dict): Config dict for convolution layer. Default: None.\n        norm_cfg (dict): Config dict for normalization layer. Default: None.\n        act_cfg (str): Config dict for activation layer in ConvModule.\n            Default: None.\n        upsample_cfg (dict): Config dict for interpolate layer.\n            Default: `dict(mode=\'nearest\')`\n\n    Example:\n        >>> import torch\n        >>> in_channels = [2, 3, 5, 7]\n        >>> scales = [340, 170, 84, 43]\n        >>> inputs = [torch.rand(1, c, s, s)\n        ...           for c, s in zip(in_channels, scales)]\n        >>> self = FPN(in_channels, 11, len(in_channels)).eval()\n        >>> outputs = self.forward(inputs)\n        >>> for i in range(len(outputs)):\n        ...     print(f\'outputs[{i}].shape = {outputs[i].shape}\')\n        outputs[0].shape = torch.Size([1, 11, 340, 340])\n        outputs[1].shape = torch.Size([1, 11, 170, 170])\n        outputs[2].shape = torch.Size([1, 11, 84, 84])\n        outputs[3].shape = torch.Size([1, 11, 43, 43])\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 num_outs,\n                 start_level=0,\n                 end_level=-1,\n                 add_extra_convs=False,\n                 extra_convs_on_inputs=True,\n                 relu_before_extra_convs=False,\n                 no_norm_on_lateral=False,\n                 conv_cfg=None,\n                 norm_cfg=None,\n                 act_cfg=None,\n                 upsample_cfg=dict(mode=\'nearest\')):\n        super(FPN, self).__init__()\n        assert isinstance(in_channels, list)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.num_ins = len(in_channels)\n        self.num_outs = num_outs\n        self.relu_before_extra_convs = relu_before_extra_convs\n        self.no_norm_on_lateral = no_norm_on_lateral\n        self.fp16_enabled = False\n        self.upsample_cfg = upsample_cfg.copy()\n\n        if end_level == -1:\n            self.backbone_end_level = self.num_ins\n            assert num_outs >= self.num_ins - start_level\n        else:\n            # if end_level < inputs, no extra level is allowed\n            self.backbone_end_level = end_level\n            assert end_level <= len(in_channels)\n            assert num_outs == end_level - start_level\n        self.start_level = start_level\n        self.end_level = end_level\n        self.add_extra_convs = add_extra_convs\n        assert isinstance(add_extra_convs, (str, bool))\n        if isinstance(add_extra_convs, str):\n            # Extra_convs_source choices: \'on_input\', \'on_lateral\', \'on_output\'\n            assert add_extra_convs in (\'on_input\', \'on_lateral\', \'on_output\')\n        elif add_extra_convs:  # True\n            if extra_convs_on_inputs:\n                # For compatibility with previous release\n                # TODO: deprecate `extra_convs_on_inputs`\n                self.add_extra_convs = \'on_input\'\n            else:\n                self.add_extra_convs = \'on_output\'\n\n        self.lateral_convs = nn.ModuleList()\n        self.fpn_convs = nn.ModuleList()\n\n        for i in range(self.start_level, self.backbone_end_level):\n            l_conv = ConvModule(\n                in_channels[i],\n                out_channels,\n                1,\n                conv_cfg=conv_cfg,\n                norm_cfg=norm_cfg if not self.no_norm_on_lateral else None,\n                act_cfg=act_cfg,\n                inplace=False)\n            fpn_conv = ConvModule(\n                out_channels,\n                out_channels,\n                3,\n                padding=1,\n                conv_cfg=conv_cfg,\n                norm_cfg=norm_cfg,\n                act_cfg=act_cfg,\n                inplace=False)\n\n            self.lateral_convs.append(l_conv)\n            self.fpn_convs.append(fpn_conv)\n\n        # add extra conv layers (e.g., RetinaNet)\n        extra_levels = num_outs - self.backbone_end_level + self.start_level\n        if self.add_extra_convs and extra_levels >= 1:\n            for i in range(extra_levels):\n                if i == 0 and self.add_extra_convs == \'on_input\':\n                    in_channels = self.in_channels[self.backbone_end_level - 1]\n                else:\n                    in_channels = out_channels\n                extra_fpn_conv = ConvModule(\n                    in_channels,\n                    out_channels,\n                    3,\n                    stride=2,\n                    padding=1,\n                    conv_cfg=conv_cfg,\n                    norm_cfg=norm_cfg,\n                    act_cfg=act_cfg,\n                    inplace=False)\n                self.fpn_convs.append(extra_fpn_conv)\n\n    # default init_weights for conv(msra) and norm in ConvModule\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                xavier_init(m, distribution=\'uniform\')\n\n    @auto_fp16()\n    def forward(self, inputs):\n        assert len(inputs) == len(self.in_channels)\n\n        # build laterals\n        laterals = [\n            lateral_conv(inputs[i + self.start_level])\n            for i, lateral_conv in enumerate(self.lateral_convs)\n        ]\n\n        # build top-down path\n        used_backbone_levels = len(laterals)\n        for i in range(used_backbone_levels - 1, 0, -1):\n            # In some cases, fixing `scale factor` (e.g. 2) is preferred, but\n            #  it cannot co-exist with `size` in `F.interpolate`.\n            if \'scale_factor\' in self.upsample_cfg:\n                laterals[i - 1] += F.interpolate(laterals[i],\n                                                 **self.upsample_cfg)\n            else:\n                prev_shape = laterals[i - 1].shape[2:]\n                laterals[i - 1] += F.interpolate(\n                    laterals[i], size=prev_shape, **self.upsample_cfg)\n\n        # build outputs\n        # part 1: from original levels\n        outs = [\n            self.fpn_convs[i](laterals[i]) for i in range(used_backbone_levels)\n        ]\n        # part 2: add extra levels\n        if self.num_outs > len(outs):\n            # use max pool to get more levels on top of outputs\n            # (e.g., Faster R-CNN, Mask R-CNN)\n            if not self.add_extra_convs:\n                for i in range(self.num_outs - used_backbone_levels):\n                    outs.append(F.max_pool2d(outs[-1], 1, stride=2))\n            # add conv layers on top of original feature maps (RetinaNet)\n            else:\n                if self.add_extra_convs == \'on_input\':\n                    extra_source = inputs[self.backbone_end_level - 1]\n                elif self.add_extra_convs == \'on_lateral\':\n                    extra_source = laterals[-1]\n                elif self.add_extra_convs == \'on_output\':\n                    extra_source = outs[-1]\n                else:\n                    raise NotImplementedError\n                outs.append(self.fpn_convs[used_backbone_levels](extra_source))\n                for i in range(used_backbone_levels + 1, self.num_outs):\n                    if self.relu_before_extra_convs:\n                        outs.append(self.fpn_convs[i](F.relu(outs[-1])))\n                    else:\n                        outs.append(self.fpn_convs[i](outs[-1]))\n        return tuple(outs)\n'"
mmdet/models/necks/fpn_carafe.py,1,"b'import torch.nn as nn\nfrom mmcv.cnn import ConvModule, build_upsample_layer, xavier_init\n\nfrom mmdet.ops.carafe import CARAFEPack\nfrom ..builder import NECKS\n\n\n@NECKS.register_module()\nclass FPN_CARAFE(nn.Module):\n    """"""FPN_CARAFE is a more flexible implementation of FPN.\n    It allows more choice for upsample methods during the top-down pathway.\n\n    It can reproduce the preformance of ICCV 2019 paper\n    CARAFE: Content-Aware ReAssembly of FEatures\n    Please refer to https://arxiv.org/abs/1905.02188 for more details.\n\n    Args:\n        in_channels (list[int]): Number of channels for each input feature map.\n        out_channels (int): Output channels of feature pyramids.\n        num_outs (int): Number of output stages.\n        start_level (int): Start level of feature pyramids.\n            (Default: 0)\n        end_level (int): End level of feature pyramids.\n            (Default: -1 indicates the last level).\n        norm_cfg (dict): Dictionary to construct and config norm layer.\n        activate (str): Type of activation function in ConvModule\n            (Default: None indicates w/o activation).\n        order (dict): Order of components in ConvModule.\n        upsample (str): Type of upsample layer.\n        upsample_cfg (dict): Dictionary to construct and config upsample layer.\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 num_outs,\n                 start_level=0,\n                 end_level=-1,\n                 norm_cfg=None,\n                 act_cfg=None,\n                 order=(\'conv\', \'norm\', \'act\'),\n                 upsample_cfg=dict(\n                     type=\'carafe\',\n                     up_kernel=5,\n                     up_group=1,\n                     encoder_kernel=3,\n                     encoder_dilation=1)):\n        super(FPN_CARAFE, self).__init__()\n        assert isinstance(in_channels, list)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.num_ins = len(in_channels)\n        self.num_outs = num_outs\n        self.norm_cfg = norm_cfg\n        self.act_cfg = act_cfg\n        self.with_bias = norm_cfg is None\n        self.upsample_cfg = upsample_cfg.copy()\n        self.upsample = self.upsample_cfg.get(\'type\')\n        self.relu = nn.ReLU(inplace=False)\n\n        self.order = order\n        assert order in [(\'conv\', \'norm\', \'act\'), (\'act\', \'conv\', \'norm\')]\n\n        assert self.upsample in [\n            \'nearest\', \'bilinear\', \'deconv\', \'pixel_shuffle\', \'carafe\', None\n        ]\n        if self.upsample in [\'deconv\', \'pixel_shuffle\']:\n            assert hasattr(\n                self.upsample_cfg,\n                \'upsample_kernel\') and self.upsample_cfg.upsample_kernel > 0\n            self.upsample_kernel = self.upsample_cfg.pop(\'upsample_kernel\')\n\n        if end_level == -1:\n            self.backbone_end_level = self.num_ins\n            assert num_outs >= self.num_ins - start_level\n        else:\n            # if end_level < inputs, no extra level is allowed\n            self.backbone_end_level = end_level\n            assert end_level <= len(in_channels)\n            assert num_outs == end_level - start_level\n        self.start_level = start_level\n        self.end_level = end_level\n\n        self.lateral_convs = nn.ModuleList()\n        self.fpn_convs = nn.ModuleList()\n        self.upsample_modules = nn.ModuleList()\n\n        for i in range(self.start_level, self.backbone_end_level):\n            l_conv = ConvModule(\n                in_channels[i],\n                out_channels,\n                1,\n                norm_cfg=norm_cfg,\n                bias=self.with_bias,\n                act_cfg=act_cfg,\n                inplace=False,\n                order=self.order)\n            fpn_conv = ConvModule(\n                out_channels,\n                out_channels,\n                3,\n                padding=1,\n                norm_cfg=self.norm_cfg,\n                bias=self.with_bias,\n                act_cfg=act_cfg,\n                inplace=False,\n                order=self.order)\n            if i != self.backbone_end_level - 1:\n                upsample_cfg_ = self.upsample_cfg.copy()\n                if self.upsample == \'deconv\':\n                    upsample_cfg_.update(\n                        in_channels=out_channels,\n                        out_channels=out_channels,\n                        kernel_size=self.upsample_kernel,\n                        stride=2,\n                        padding=(self.upsample_kernel - 1) // 2,\n                        output_padding=(self.upsample_kernel - 1) // 2)\n                elif self.upsample == \'pixel_shuffle\':\n                    upsample_cfg_.update(\n                        in_channels=out_channels,\n                        out_channels=out_channels,\n                        scale_factor=2,\n                        upsample_kernel=self.upsample_kernel)\n                elif self.upsample == \'carafe\':\n                    upsample_cfg_.update(channels=out_channels, scale_factor=2)\n                else:\n                    # suppress warnings\n                    align_corners = (None\n                                     if self.upsample == \'nearest\' else False)\n                    upsample_cfg_.update(\n                        scale_factor=2,\n                        mode=self.upsample,\n                        align_corners=align_corners)\n                upsample_module = build_upsample_layer(upsample_cfg_)\n                self.upsample_modules.append(upsample_module)\n            self.lateral_convs.append(l_conv)\n            self.fpn_convs.append(fpn_conv)\n\n        # add extra conv layers (e.g., RetinaNet)\n        extra_out_levels = (\n            num_outs - self.backbone_end_level + self.start_level)\n        if extra_out_levels >= 1:\n            for i in range(extra_out_levels):\n                in_channels = (\n                    self.in_channels[self.backbone_end_level -\n                                     1] if i == 0 else out_channels)\n                extra_l_conv = ConvModule(\n                    in_channels,\n                    out_channels,\n                    3,\n                    stride=2,\n                    padding=1,\n                    norm_cfg=norm_cfg,\n                    bias=self.with_bias,\n                    act_cfg=act_cfg,\n                    inplace=False,\n                    order=self.order)\n                if self.upsample == \'deconv\':\n                    upsampler_cfg_ = dict(\n                        in_channels=out_channels,\n                        out_channels=out_channels,\n                        kernel_size=self.upsample_kernel,\n                        stride=2,\n                        padding=(self.upsample_kernel - 1) // 2,\n                        output_padding=(self.upsample_kernel - 1) // 2)\n                elif self.upsample == \'pixel_shuffle\':\n                    upsampler_cfg_ = dict(\n                        in_channels=out_channels,\n                        out_channels=out_channels,\n                        scale_factor=2,\n                        upsample_kernel=self.upsample_kernel)\n                elif self.upsample == \'carafe\':\n                    upsampler_cfg_ = dict(\n                        channels=out_channels,\n                        scale_factor=2,\n                        **self.upsample_cfg)\n                else:\n                    # suppress warnings\n                    align_corners = (None\n                                     if self.upsample == \'nearest\' else False)\n                    upsampler_cfg_ = dict(\n                        scale_factor=2,\n                        mode=self.upsample,\n                        align_corners=align_corners)\n                upsampler_cfg_[\'type\'] = self.upsample\n                upsample_module = build_upsample_layer(upsampler_cfg_)\n                extra_fpn_conv = ConvModule(\n                    out_channels,\n                    out_channels,\n                    3,\n                    padding=1,\n                    norm_cfg=self.norm_cfg,\n                    bias=self.with_bias,\n                    act_cfg=act_cfg,\n                    inplace=False,\n                    order=self.order)\n                self.upsample_modules.append(upsample_module)\n                self.fpn_convs.append(extra_fpn_conv)\n                self.lateral_convs.append(extra_l_conv)\n\n    # default init_weights for conv(msra) and norm in ConvModule\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n                xavier_init(m, distribution=\'uniform\')\n        for m in self.modules():\n            if isinstance(m, CARAFEPack):\n                m.init_weights()\n\n    def slice_as(self, src, dst):\n        # slice src as dst\n        # src should have the same or larger size than dst\n        assert (src.size(2) >= dst.size(2)) and (src.size(3) >= dst.size(3))\n        if src.size(2) == dst.size(2) and src.size(3) == dst.size(3):\n            return src\n        else:\n            return src[:, :, :dst.size(2), :dst.size(3)]\n\n    def tensor_add(self, a, b):\n        if a.size() == b.size():\n            c = a + b\n        else:\n            c = a + self.slice_as(b, a)\n        return c\n\n    def forward(self, inputs):\n        assert len(inputs) == len(self.in_channels)\n\n        # build laterals\n        laterals = []\n        for i, lateral_conv in enumerate(self.lateral_convs):\n            if i <= self.backbone_end_level - self.start_level:\n                input = inputs[min(i + self.start_level, len(inputs) - 1)]\n            else:\n                input = laterals[-1]\n            lateral = lateral_conv(input)\n            laterals.append(lateral)\n\n        # build top-down path\n        for i in range(len(laterals) - 1, 0, -1):\n            if self.upsample is not None:\n                upsample_feat = self.upsample_modules[i - 1](laterals[i])\n            else:\n                upsample_feat = laterals[i]\n            laterals[i - 1] = self.tensor_add(laterals[i - 1], upsample_feat)\n\n        # build outputs\n        num_conv_outs = len(self.fpn_convs)\n        outs = []\n        for i in range(num_conv_outs):\n            out = self.fpn_convs[i](laterals[i])\n            outs.append(out)\n        return tuple(outs)\n'"
mmdet/models/necks/hrfpn.py,4,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import ConvModule, caffe2_xavier_init\nfrom torch.utils.checkpoint import checkpoint\n\nfrom ..builder import NECKS\n\n\n@NECKS.register_module()\nclass HRFPN(nn.Module):\n    """"""HRFPN (High Resolution Feature Pyrmamids)\n\n    arXiv: https://arxiv.org/abs/1904.04514\n\n    Args:\n        in_channels (list): number of channels for each branch.\n        out_channels (int): output channels of feature pyramids.\n        num_outs (int): number of output stages.\n        pooling_type (str): pooling for generating feature pyramids\n            from {MAX, AVG}.\n        conv_cfg (dict): dictionary to construct and config conv layer.\n        norm_cfg (dict): dictionary to construct and config norm layer.\n        with_cp  (bool): Use checkpoint or not. Using checkpoint will save some\n            memory while slowing down the training speed.\n        stride (int): stride of 3x3 convolutional layers\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 num_outs=5,\n                 pooling_type=\'AVG\',\n                 conv_cfg=None,\n                 norm_cfg=None,\n                 with_cp=False,\n                 stride=1):\n        super(HRFPN, self).__init__()\n        assert isinstance(in_channels, list)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.num_ins = len(in_channels)\n        self.num_outs = num_outs\n        self.with_cp = with_cp\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n\n        self.reduction_conv = ConvModule(\n            sum(in_channels),\n            out_channels,\n            kernel_size=1,\n            conv_cfg=self.conv_cfg,\n            act_cfg=None)\n\n        self.fpn_convs = nn.ModuleList()\n        for i in range(self.num_outs):\n            self.fpn_convs.append(\n                ConvModule(\n                    out_channels,\n                    out_channels,\n                    kernel_size=3,\n                    padding=1,\n                    stride=stride,\n                    conv_cfg=self.conv_cfg,\n                    act_cfg=None))\n\n        if pooling_type == \'MAX\':\n            self.pooling = F.max_pool2d\n        else:\n            self.pooling = F.avg_pool2d\n\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                caffe2_xavier_init(m)\n\n    def forward(self, inputs):\n        assert len(inputs) == self.num_ins\n        outs = [inputs[0]]\n        for i in range(1, self.num_ins):\n            outs.append(\n                F.interpolate(inputs[i], scale_factor=2**i, mode=\'bilinear\'))\n        out = torch.cat(outs, dim=1)\n        if out.requires_grad and self.with_cp:\n            out = checkpoint(self.reduction_conv, out)\n        else:\n            out = self.reduction_conv(out)\n        outs = [out]\n        for i in range(1, self.num_outs):\n            outs.append(self.pooling(out, kernel_size=2**i, stride=2**i))\n        outputs = []\n\n        for i in range(self.num_outs):\n            if outs[i].requires_grad and self.with_cp:\n                tmp_out = checkpoint(self.fpn_convs[i], outs[i])\n            else:\n                tmp_out = self.fpn_convs[i](outs[i])\n            outputs.append(tmp_out)\n        return tuple(outputs)\n'"
mmdet/models/necks/nas_fpn.py,1,"b'import torch.nn as nn\nfrom mmcv.cnn import ConvModule, caffe2_xavier_init\n\nfrom mmdet.ops.merge_cells import GlobalPoolingCell, SumCell\nfrom ..builder import NECKS\n\n\n@NECKS.register_module()\nclass NASFPN(nn.Module):\n    """"""NAS-FPN.\n\n    NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object\n    Detection. (https://arxiv.org/abs/1904.07392)\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 num_outs,\n                 stack_times,\n                 start_level=0,\n                 end_level=-1,\n                 add_extra_convs=False,\n                 norm_cfg=None):\n        super(NASFPN, self).__init__()\n        assert isinstance(in_channels, list)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.num_ins = len(in_channels)  # num of input feature levels\n        self.num_outs = num_outs  # num of output feature levels\n        self.stack_times = stack_times\n        self.norm_cfg = norm_cfg\n\n        if end_level == -1:\n            self.backbone_end_level = self.num_ins\n            assert num_outs >= self.num_ins - start_level\n        else:\n            # if end_level < inputs, no extra level is allowed\n            self.backbone_end_level = end_level\n            assert end_level <= len(in_channels)\n            assert num_outs == end_level - start_level\n        self.start_level = start_level\n        self.end_level = end_level\n        self.add_extra_convs = add_extra_convs\n\n        # add lateral connections\n        self.lateral_convs = nn.ModuleList()\n        for i in range(self.start_level, self.backbone_end_level):\n            l_conv = ConvModule(\n                in_channels[i],\n                out_channels,\n                1,\n                norm_cfg=norm_cfg,\n                act_cfg=None)\n            self.lateral_convs.append(l_conv)\n\n        # add extra downsample layers (stride-2 pooling or conv)\n        extra_levels = num_outs - self.backbone_end_level + self.start_level\n        self.extra_downsamples = nn.ModuleList()\n        for i in range(extra_levels):\n            extra_conv = ConvModule(\n                out_channels, out_channels, 1, norm_cfg=norm_cfg, act_cfg=None)\n            self.extra_downsamples.append(\n                nn.Sequential(extra_conv, nn.MaxPool2d(2, 2)))\n\n        # add NAS FPN connections\n        self.fpn_stages = nn.ModuleList()\n        for _ in range(self.stack_times):\n            stage = nn.ModuleDict()\n            # gp(p6, p4) -> p4_1\n            stage[\'gp_64_4\'] = GlobalPoolingCell(\n                in_channels=out_channels,\n                out_channels=out_channels,\n                out_norm_cfg=norm_cfg)\n            # sum(p4_1, p4) -> p4_2\n            stage[\'sum_44_4\'] = SumCell(\n                in_channels=out_channels,\n                out_channels=out_channels,\n                out_norm_cfg=norm_cfg)\n            # sum(p4_2, p3) -> p3_out\n            stage[\'sum_43_3\'] = SumCell(\n                in_channels=out_channels,\n                out_channels=out_channels,\n                out_norm_cfg=norm_cfg)\n            # sum(p3_out, p4_2) -> p4_out\n            stage[\'sum_34_4\'] = SumCell(\n                in_channels=out_channels,\n                out_channels=out_channels,\n                out_norm_cfg=norm_cfg)\n            # sum(p5, gp(p4_out, p3_out)) -> p5_out\n            stage[\'gp_43_5\'] = GlobalPoolingCell(with_out_conv=False)\n            stage[\'sum_55_5\'] = SumCell(\n                in_channels=out_channels,\n                out_channels=out_channels,\n                out_norm_cfg=norm_cfg)\n            # sum(p7, gp(p5_out, p4_2)) -> p7_out\n            stage[\'gp_54_7\'] = GlobalPoolingCell(with_out_conv=False)\n            stage[\'sum_77_7\'] = SumCell(\n                in_channels=out_channels,\n                out_channels=out_channels,\n                out_norm_cfg=norm_cfg)\n            # gp(p7_out, p5_out) -> p6_out\n            stage[\'gp_75_6\'] = GlobalPoolingCell(\n                in_channels=out_channels,\n                out_channels=out_channels,\n                out_norm_cfg=norm_cfg)\n            self.fpn_stages.append(stage)\n\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                caffe2_xavier_init(m)\n\n    def forward(self, inputs):\n        # build P3-P5\n        feats = [\n            lateral_conv(inputs[i + self.start_level])\n            for i, lateral_conv in enumerate(self.lateral_convs)\n        ]\n        # build P6-P7 on top of P5\n        for downsample in self.extra_downsamples:\n            feats.append(downsample(feats[-1]))\n\n        p3, p4, p5, p6, p7 = feats\n\n        for stage in self.fpn_stages:\n            # gp(p6, p4) -> p4_1\n            p4_1 = stage[\'gp_64_4\'](p6, p4, out_size=p4.shape[-2:])\n            # sum(p4_1, p4) -> p4_2\n            p4_2 = stage[\'sum_44_4\'](p4_1, p4, out_size=p4.shape[-2:])\n            # sum(p4_2, p3) -> p3_out\n            p3 = stage[\'sum_43_3\'](p4_2, p3, out_size=p3.shape[-2:])\n            # sum(p3_out, p4_2) -> p4_out\n            p4 = stage[\'sum_34_4\'](p3, p4_2, out_size=p4.shape[-2:])\n            # sum(p5, gp(p4_out, p3_out)) -> p5_out\n            p5_tmp = stage[\'gp_43_5\'](p4, p3, out_size=p5.shape[-2:])\n            p5 = stage[\'sum_55_5\'](p5, p5_tmp, out_size=p5.shape[-2:])\n            # sum(p7, gp(p5_out, p4_2)) -> p7_out\n            p7_tmp = stage[\'gp_54_7\'](p5, p4_2, out_size=p7.shape[-2:])\n            p7 = stage[\'sum_77_7\'](p7, p7_tmp, out_size=p7.shape[-2:])\n            # gp(p7_out, p5_out) -> p6_out\n            p6 = stage[\'gp_75_6\'](p7, p5, out_size=p6.shape[-2:])\n\n        return p3, p4, p5, p6, p7\n'"
mmdet/models/necks/nasfcos_fpn.py,2,"b'import torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import ConvModule, caffe2_xavier_init\n\nfrom mmdet.ops.merge_cells import ConcatCell\nfrom ..builder import NECKS\n\n\n@NECKS.register_module\nclass NASFCOS_FPN(nn.Module):\n    """"""FPN structure in NASFPN\n\n    NAS-FCOS: Fast Neural Architecture Search for Object Detection\n    <https://arxiv.org/abs/1906.04423>\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 num_outs,\n                 start_level=1,\n                 end_level=-1,\n                 add_extra_convs=False,\n                 conv_cfg=None,\n                 norm_cfg=None):\n        super(NASFCOS_FPN, self).__init__()\n        assert isinstance(in_channels, list)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.num_ins = len(in_channels)\n        self.num_outs = num_outs\n        self.norm_cfg = norm_cfg\n        self.conv_cfg = conv_cfg\n\n        if end_level == -1:\n            self.backbone_end_level = self.num_ins\n            assert num_outs >= self.num_ins - start_level\n        else:\n            self.backbone_end_level = end_level\n            assert end_level <= len(in_channels)\n            assert num_outs == end_level - start_level\n        self.start_level = start_level\n        self.end_level = end_level\n        self.add_extra_convs = add_extra_convs\n\n        self.adapt_convs = nn.ModuleList()\n        for i in range(self.start_level, self.backbone_end_level):\n            adapt_conv = ConvModule(\n                in_channels[i],\n                out_channels,\n                1,\n                stride=1,\n                padding=0,\n                bias=False,\n                norm_cfg=dict(type=\'BN\'),\n                act_cfg=dict(type=\'ReLU\', inplace=False))\n            self.adapt_convs.append(adapt_conv)\n\n        # C2 is omitted according to the paper\n        extra_levels = num_outs - self.backbone_end_level + self.start_level\n\n        def build_concat_cell(with_input1_conv, with_input2_conv):\n            cell_conv_cfg = dict(\n                kernel_size=1, padding=0, bias=False, groups=out_channels)\n            return ConcatCell(\n                in_channels=out_channels,\n                out_channels=out_channels,\n                with_out_conv=True,\n                out_conv_cfg=cell_conv_cfg,\n                out_norm_cfg=dict(type=\'BN\'),\n                out_conv_order=(\'norm\', \'act\', \'conv\'),\n                with_input1_conv=with_input1_conv,\n                with_input2_conv=with_input2_conv,\n                input_conv_cfg=conv_cfg,\n                input_norm_cfg=norm_cfg,\n                upsample_mode=\'nearest\')\n\n        # Denote c3=f0, c4=f1, c5=f2 for convince\n        self.fpn = nn.ModuleDict()\n        self.fpn[\'c22_1\'] = build_concat_cell(True, True)\n        self.fpn[\'c22_2\'] = build_concat_cell(True, True)\n        self.fpn[\'c32\'] = build_concat_cell(True, False)\n        self.fpn[\'c02\'] = build_concat_cell(True, False)\n        self.fpn[\'c42\'] = build_concat_cell(True, True)\n        self.fpn[\'c36\'] = build_concat_cell(True, True)\n        self.fpn[\'c61\'] = build_concat_cell(True, True)  # f9\n        self.extra_downsamples = nn.ModuleList()\n        for i in range(extra_levels):\n            extra_act_cfg = None if i == 0 \\\n                else dict(type=\'ReLU\', inplace=False)\n            self.extra_downsamples.append(\n                ConvModule(\n                    out_channels,\n                    out_channels,\n                    3,\n                    stride=2,\n                    padding=1,\n                    act_cfg=extra_act_cfg,\n                    order=(\'act\', \'norm\', \'conv\')))\n\n    def forward(self, inputs):\n        feats = [\n            adapt_conv(inputs[i + self.start_level])\n            for i, adapt_conv in enumerate(self.adapt_convs)\n        ]\n\n        for (i, module_name) in enumerate(self.fpn):\n            idx_1, idx_2 = int(module_name[1]), int(module_name[2])\n            res = self.fpn[module_name](feats[idx_1], feats[idx_2])\n            feats.append(res)\n\n        ret = []\n        for (idx, input_idx) in zip([9, 8, 7], [1, 2, 3]):  # add P3, P4, P5\n            feats1, feats2 = feats[idx], feats[5]\n            feats2_resize = F.interpolate(\n                feats2,\n                size=feats1.size()[2:],\n                mode=\'bilinear\',\n                align_corners=False)\n\n            feats_sum = feats1 + feats2_resize\n            ret.append(\n                F.interpolate(\n                    feats_sum,\n                    size=inputs[input_idx].size()[2:],\n                    mode=\'bilinear\',\n                    align_corners=False))\n\n        for submodule in self.extra_downsamples:\n            ret.append(submodule(ret[-1]))\n\n        return tuple(ret)\n\n    def init_weights(self):\n        for module in self.fpn.values():\n            if hasattr(module, \'conv_out\'):\n                caffe2_xavier_init(module.out_conv.conv)\n\n        for modules in [\n                self.adapt_convs.modules(),\n                self.extra_downsamples.modules()\n        ]:\n            for module in modules:\n                if isinstance(module, nn.Conv2d):\n                    caffe2_xavier_init(module)\n'"
mmdet/models/necks/pafpn.py,2,"b'import torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import ConvModule\n\nfrom mmdet.core import auto_fp16\nfrom ..builder import NECKS\nfrom .fpn import FPN\n\n\n@NECKS.register_module()\nclass PAFPN(FPN):\n    """"""Path Aggregation Network for Instance Segmentation.\n\n    This is an implementation of the PAFPN in Path Aggregation Network\n    (https://arxiv.org/abs/1803.01534).\n\n    Args:\n        in_channels (List[int]): Number of input channels per scale.\n        out_channels (int): Number of output channels (used at each scale)\n        num_outs (int): Number of output scales.\n        start_level (int): Index of the start input backbone level used to\n            build the feature pyramid. Default: 0.\n        end_level (int): Index of the end input backbone level (exclusive) to\n            build the feature pyramid. Default: -1, which means the last level.\n        add_extra_convs (bool): Whether to add conv layers on top of the\n            original feature maps. Default: False.\n        extra_convs_on_inputs (bool): Whether to apply extra conv on\n            the original feature from the backbone. Default: False.\n        relu_before_extra_convs (bool): Whether to apply relu before the extra\n            conv. Default: False.\n        no_norm_on_lateral (bool): Whether to apply norm on lateral.\n            Default: False.\n        conv_cfg (dict): Config dict for convolution layer. Default: None.\n        norm_cfg (dict): Config dict for normalization layer. Default: None.\n        act_cfg (str): Config dict for activation layer in ConvModule.\n            Default: None.\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 num_outs,\n                 start_level=0,\n                 end_level=-1,\n                 add_extra_convs=False,\n                 extra_convs_on_inputs=True,\n                 relu_before_extra_convs=False,\n                 no_norm_on_lateral=False,\n                 conv_cfg=None,\n                 norm_cfg=None,\n                 act_cfg=None):\n        super(PAFPN,\n              self).__init__(in_channels, out_channels, num_outs, start_level,\n                             end_level, add_extra_convs, extra_convs_on_inputs,\n                             relu_before_extra_convs, no_norm_on_lateral,\n                             conv_cfg, norm_cfg, act_cfg)\n        # add extra bottom up pathway\n        self.downsample_convs = nn.ModuleList()\n        self.pafpn_convs = nn.ModuleList()\n        for i in range(self.start_level + 1, self.backbone_end_level):\n            d_conv = ConvModule(\n                out_channels,\n                out_channels,\n                3,\n                stride=2,\n                padding=1,\n                conv_cfg=conv_cfg,\n                norm_cfg=norm_cfg,\n                act_cfg=act_cfg,\n                inplace=False)\n            pafpn_conv = ConvModule(\n                out_channels,\n                out_channels,\n                3,\n                padding=1,\n                conv_cfg=conv_cfg,\n                norm_cfg=norm_cfg,\n                act_cfg=act_cfg,\n                inplace=False)\n            self.downsample_convs.append(d_conv)\n            self.pafpn_convs.append(pafpn_conv)\n\n    @auto_fp16()\n    def forward(self, inputs):\n        assert len(inputs) == len(self.in_channels)\n\n        # build laterals\n        laterals = [\n            lateral_conv(inputs[i + self.start_level])\n            for i, lateral_conv in enumerate(self.lateral_convs)\n        ]\n\n        # build top-down path\n        used_backbone_levels = len(laterals)\n        for i in range(used_backbone_levels - 1, 0, -1):\n            prev_shape = laterals[i - 1].shape[2:]\n            laterals[i - 1] += F.interpolate(\n                laterals[i], size=prev_shape, mode=\'nearest\')\n\n        # build outputs\n        # part 1: from original levels\n        inter_outs = [\n            self.fpn_convs[i](laterals[i]) for i in range(used_backbone_levels)\n        ]\n\n        # part 2: add bottom-up path\n        for i in range(0, used_backbone_levels - 1):\n            inter_outs[i + 1] += self.downsample_convs[i](inter_outs[i])\n\n        outs = []\n        outs.append(inter_outs[0])\n        outs.extend([\n            self.pafpn_convs[i - 1](inter_outs[i])\n            for i in range(1, used_backbone_levels)\n        ])\n\n        # part 3: add extra levels\n        if self.num_outs > len(outs):\n            # use max pool to get more levels on top of outputs\n            # (e.g., Faster R-CNN, Mask R-CNN)\n            if not self.add_extra_convs:\n                for i in range(self.num_outs - used_backbone_levels):\n                    outs.append(F.max_pool2d(outs[-1], 1, stride=2))\n            # add conv layers on top of original feature maps (RetinaNet)\n            else:\n                if self.extra_convs_on_inputs:\n                    orig = inputs[self.backbone_end_level - 1]\n                    outs.append(self.fpn_convs[used_backbone_levels](orig))\n                else:\n                    outs.append(self.fpn_convs[used_backbone_levels](outs[-1]))\n                for i in range(used_backbone_levels + 1, self.num_outs):\n                    if self.relu_before_extra_convs:\n                        outs.append(self.fpn_convs[i](F.relu(outs[-1])))\n                    else:\n                        outs.append(self.fpn_convs[i](outs[-1]))\n        return tuple(outs)\n'"
mmdet/models/roi_heads/__init__.py,0,"b""from .base_roi_head import BaseRoIHead\nfrom .bbox_heads import (BBoxHead, ConvFCBBoxHead, DoubleConvFCBBoxHead,\n                         Shared2FCBBoxHead, Shared4Conv1FCBBoxHead)\nfrom .cascade_roi_head import CascadeRoIHead\nfrom .double_roi_head import DoubleHeadRoIHead\nfrom .grid_roi_head import GridRoIHead\nfrom .htc_roi_head import HybridTaskCascadeRoIHead\nfrom .mask_heads import (FCNMaskHead, FusedSemanticHead, GridHead, HTCMaskHead,\n                         MaskIoUHead)\nfrom .mask_scoring_roi_head import MaskScoringRoIHead\nfrom .pisa_roi_head import PISARoIHead\nfrom .roi_extractors import SingleRoIExtractor\nfrom .shared_heads import ResLayer\n\n__all__ = [\n    'BaseRoIHead', 'CascadeRoIHead', 'DoubleHeadRoIHead', 'MaskScoringRoIHead',\n    'HybridTaskCascadeRoIHead', 'GridRoIHead', 'ResLayer', 'BBoxHead',\n    'ConvFCBBoxHead', 'Shared2FCBBoxHead', 'Shared4Conv1FCBBoxHead',\n    'DoubleConvFCBBoxHead', 'FCNMaskHead', 'HTCMaskHead', 'FusedSemanticHead',\n    'GridHead', 'MaskIoUHead', 'SingleRoIExtractor', 'PISARoIHead'\n]\n"""
mmdet/models/roi_heads/base_roi_head.py,1,"b'from abc import ABCMeta, abstractmethod\n\nimport torch.nn as nn\n\nfrom ..builder import build_shared_head\n\n\nclass BaseRoIHead(nn.Module, metaclass=ABCMeta):\n    """"""Base class for RoIHeads""""""\n\n    def __init__(self,\n                 bbox_roi_extractor=None,\n                 bbox_head=None,\n                 mask_roi_extractor=None,\n                 mask_head=None,\n                 shared_head=None,\n                 train_cfg=None,\n                 test_cfg=None):\n        super(BaseRoIHead, self).__init__()\n        self.train_cfg = train_cfg\n        self.test_cfg = test_cfg\n        if shared_head is not None:\n            self.shared_head = build_shared_head(shared_head)\n\n        if bbox_head is not None:\n            self.init_bbox_head(bbox_roi_extractor, bbox_head)\n\n        if mask_head is not None:\n            self.init_mask_head(mask_roi_extractor, mask_head)\n\n        self.init_assigner_sampler()\n\n    @property\n    def with_bbox(self):\n        return hasattr(self, \'bbox_head\') and self.bbox_head is not None\n\n    @property\n    def with_mask(self):\n        return hasattr(self, \'mask_head\') and self.mask_head is not None\n\n    @property\n    def with_shared_head(self):\n        return hasattr(self, \'shared_head\') and self.shared_head is not None\n\n    @abstractmethod\n    def init_weights(self, pretrained):\n        pass\n\n    @abstractmethod\n    def init_bbox_head(self):\n        pass\n\n    @abstractmethod\n    def init_mask_head(self):\n        pass\n\n    @abstractmethod\n    def init_assigner_sampler(self):\n        pass\n\n    @abstractmethod\n    def forward_train(self,\n                      x,\n                      img_meta,\n                      proposal_list,\n                      gt_bboxes,\n                      gt_labels,\n                      gt_bboxes_ignore=None,\n                      gt_masks=None,\n                      **kwargs):\n        """"""Forward function during training""""""\n        pass\n\n    async def async_simple_test(self, x, img_meta, **kwargs):\n        raise NotImplementedError\n\n    def simple_test(self,\n                    x,\n                    proposal_list,\n                    img_meta,\n                    proposals=None,\n                    rescale=False,\n                    **kwargs):\n        """"""Test without augmentation.""""""\n        pass\n\n    def aug_test(self, x, proposal_list, img_metas, rescale=False, **kwargs):\n        """"""Test with augmentations.\n\n        If rescale is False, then returned bboxes and masks will fit the scale\n        of imgs[0].\n        """"""\n        pass\n'"
mmdet/models/roi_heads/cascade_roi_head.py,3,"b'import torch\nimport torch.nn as nn\n\nfrom mmdet.core import (bbox2result, bbox2roi, bbox_mapping, build_assigner,\n                        build_sampler, merge_aug_bboxes, merge_aug_masks,\n                        multiclass_nms)\nfrom ..builder import HEADS, build_head, build_roi_extractor\nfrom .base_roi_head import BaseRoIHead\nfrom .test_mixins import BBoxTestMixin, MaskTestMixin\n\n\n@HEADS.register_module()\nclass CascadeRoIHead(BaseRoIHead, BBoxTestMixin, MaskTestMixin):\n    """"""Cascade roi head including one bbox head and one mask head.\n\n    https://arxiv.org/abs/1712.00726\n    """"""\n\n    def __init__(self,\n                 num_stages,\n                 stage_loss_weights,\n                 bbox_roi_extractor=None,\n                 bbox_head=None,\n                 mask_roi_extractor=None,\n                 mask_head=None,\n                 shared_head=None,\n                 train_cfg=None,\n                 test_cfg=None):\n        assert bbox_roi_extractor is not None\n        assert bbox_head is not None\n        assert shared_head is None, \\\n            \'Shared head is not supported in Cascade RCNN anymore\'\n        self.num_stages = num_stages\n        self.stage_loss_weights = stage_loss_weights\n        super(CascadeRoIHead, self).__init__(\n            bbox_roi_extractor=bbox_roi_extractor,\n            bbox_head=bbox_head,\n            mask_roi_extractor=mask_roi_extractor,\n            mask_head=mask_head,\n            shared_head=shared_head,\n            train_cfg=train_cfg,\n            test_cfg=test_cfg)\n\n    def init_bbox_head(self, bbox_roi_extractor, bbox_head):\n        self.bbox_roi_extractor = nn.ModuleList()\n        self.bbox_head = nn.ModuleList()\n        if not isinstance(bbox_roi_extractor, list):\n            bbox_roi_extractor = [\n                bbox_roi_extractor for _ in range(self.num_stages)\n            ]\n        if not isinstance(bbox_head, list):\n            bbox_head = [bbox_head for _ in range(self.num_stages)]\n        assert len(bbox_roi_extractor) == len(bbox_head) == self.num_stages\n        for roi_extractor, head in zip(bbox_roi_extractor, bbox_head):\n            self.bbox_roi_extractor.append(build_roi_extractor(roi_extractor))\n            self.bbox_head.append(build_head(head))\n\n    def init_mask_head(self, mask_roi_extractor, mask_head):\n        self.mask_head = nn.ModuleList()\n        if not isinstance(mask_head, list):\n            mask_head = [mask_head for _ in range(self.num_stages)]\n        assert len(mask_head) == self.num_stages\n        for head in mask_head:\n            self.mask_head.append(build_head(head))\n        if mask_roi_extractor is not None:\n            self.share_roi_extractor = False\n            self.mask_roi_extractor = nn.ModuleList()\n            if not isinstance(mask_roi_extractor, list):\n                mask_roi_extractor = [\n                    mask_roi_extractor for _ in range(self.num_stages)\n                ]\n            assert len(mask_roi_extractor) == self.num_stages\n            for roi_extractor in mask_roi_extractor:\n                self.mask_roi_extractor.append(\n                    build_roi_extractor(roi_extractor))\n        else:\n            self.share_roi_extractor = True\n            self.mask_roi_extractor = self.bbox_roi_extractor\n\n    def init_assigner_sampler(self):\n        # build assigner and smapler for each stage\n        self.bbox_assigner = []\n        self.bbox_sampler = []\n        if self.train_cfg is not None:\n            for rcnn_train_cfg in self.train_cfg:\n                self.bbox_assigner.append(\n                    build_assigner(rcnn_train_cfg.assigner))\n                self.bbox_sampler.append(build_sampler(rcnn_train_cfg.sampler))\n\n    def init_weights(self, pretrained):\n        if self.with_shared_head:\n            self.shared_head.init_weights(pretrained=pretrained)\n        for i in range(self.num_stages):\n            if self.with_bbox:\n                self.bbox_roi_extractor[i].init_weights()\n                self.bbox_head[i].init_weights()\n            if self.with_mask:\n                if not self.share_roi_extractor:\n                    self.mask_roi_extractor[i].init_weights()\n                self.mask_head[i].init_weights()\n\n    def forward_dummy(self, x, proposals):\n        # bbox head\n        outs = ()\n        rois = bbox2roi([proposals])\n        if self.with_bbox:\n            for i in range(self.num_stages):\n                bbox_results = self._bbox_forward(i, x, rois)\n                outs = outs + (bbox_results[\'cls_score\'],\n                               bbox_results[\'bbox_pred\'])\n        # mask heads\n        if self.with_mask:\n            mask_rois = rois[:100]\n            for i in range(self.num_stages):\n                mask_results = self._mask_forward(i, x, mask_rois)\n                outs = outs + (mask_results[\'mask_pred\'], )\n        return outs\n\n    def _bbox_forward(self, stage, x, rois):\n        bbox_roi_extractor = self.bbox_roi_extractor[stage]\n        bbox_head = self.bbox_head[stage]\n        bbox_feats = bbox_roi_extractor(x[:bbox_roi_extractor.num_inputs],\n                                        rois)\n        # do not support caffe_c4 model anymore\n        cls_score, bbox_pred = bbox_head(bbox_feats)\n\n        bbox_results = dict(\n            cls_score=cls_score, bbox_pred=bbox_pred, bbox_feats=bbox_feats)\n        return bbox_results\n\n    def _bbox_forward_train(self, stage, x, sampling_results, gt_bboxes,\n                            gt_labels, rcnn_train_cfg):\n        rois = bbox2roi([res.bboxes for res in sampling_results])\n        bbox_results = self._bbox_forward(stage, x, rois)\n        bbox_targets = self.bbox_head[stage].get_targets(\n            sampling_results, gt_bboxes, gt_labels, rcnn_train_cfg)\n        loss_bbox = self.bbox_head[stage].loss(bbox_results[\'cls_score\'],\n                                               bbox_results[\'bbox_pred\'], rois,\n                                               *bbox_targets)\n\n        bbox_results.update(\n            loss_bbox=loss_bbox, rois=rois, bbox_targets=bbox_targets)\n        return bbox_results\n\n    def _mask_forward(self, stage, x, rois):\n        mask_roi_extractor = self.mask_roi_extractor[stage]\n        mask_head = self.mask_head[stage]\n        mask_feats = mask_roi_extractor(x[:mask_roi_extractor.num_inputs],\n                                        rois)\n        # do not support caffe_c4 model anymore\n        mask_pred = mask_head(mask_feats)\n\n        mask_results = dict(mask_pred=mask_pred)\n        return mask_results\n\n    def _mask_forward_train(self,\n                            stage,\n                            x,\n                            sampling_results,\n                            gt_masks,\n                            rcnn_train_cfg,\n                            bbox_feats=None):\n        pos_rois = bbox2roi([res.pos_bboxes for res in sampling_results])\n        if len(pos_rois) == 0:\n            # If there are no predicted and/or truth boxes, then we cannot\n            # compute head / mask losses\n            return dict(loss_mask=None)\n        mask_results = self._mask_forward(stage, x, pos_rois)\n\n        mask_targets = self.mask_head[stage].get_targets(\n            sampling_results, gt_masks, rcnn_train_cfg)\n        pos_labels = torch.cat([res.pos_gt_labels for res in sampling_results])\n        loss_mask = self.mask_head[stage].loss(mask_results[\'mask_pred\'],\n                                               mask_targets, pos_labels)\n\n        mask_results.update(loss_mask=loss_mask)\n        return mask_results\n\n    def forward_train(self,\n                      x,\n                      img_metas,\n                      proposal_list,\n                      gt_bboxes,\n                      gt_labels,\n                      gt_bboxes_ignore=None,\n                      gt_masks=None):\n        """"""\n        Args:\n            x (list[Tensor]): list of multi-level img features.\n\n            img_metas (list[dict]): list of image info dict where each dict\n                has: \'img_shape\', \'scale_factor\', \'flip\', and may also contain\n                \'filename\', \'ori_shape\', \'pad_shape\', and \'img_norm_cfg\'.\n                For details on the values of these keys see\n                `mmdet/datasets/pipelines/formatting.py:Collect`.\n\n            proposals (list[Tensors]): list of region proposals.\n\n            gt_bboxes (list[Tensor]): each item are the truth boxes for each\n                image in [tl_x, tl_y, br_x, br_y] format.\n\n            gt_labels (list[Tensor]): class indices corresponding to each box\n\n            gt_bboxes_ignore (None | list[Tensor]): specify which bounding\n                boxes can be ignored when computing the loss.\n\n            gt_masks (None | Tensor) : true segmentation masks for each box\n                used if the architecture supports a segmentation task.\n\n        Returns:\n            dict[str, Tensor]: a dictionary of loss components\n        """"""\n        losses = dict()\n        for i in range(self.num_stages):\n            self.current_stage = i\n            rcnn_train_cfg = self.train_cfg[i]\n            lw = self.stage_loss_weights[i]\n\n            # assign gts and sample proposals\n            sampling_results = []\n            if self.with_bbox or self.with_mask:\n                bbox_assigner = self.bbox_assigner[i]\n                bbox_sampler = self.bbox_sampler[i]\n                num_imgs = len(img_metas)\n                if gt_bboxes_ignore is None:\n                    gt_bboxes_ignore = [None for _ in range(num_imgs)]\n\n                for j in range(num_imgs):\n                    assign_result = bbox_assigner.assign(\n                        proposal_list[j], gt_bboxes[j], gt_bboxes_ignore[j],\n                        gt_labels[j])\n                    sampling_result = bbox_sampler.sample(\n                        assign_result,\n                        proposal_list[j],\n                        gt_bboxes[j],\n                        gt_labels[j],\n                        feats=[lvl_feat[j][None] for lvl_feat in x])\n                    sampling_results.append(sampling_result)\n\n            # bbox head forward and loss\n            bbox_results = self._bbox_forward_train(i, x, sampling_results,\n                                                    gt_bboxes, gt_labels,\n                                                    rcnn_train_cfg)\n\n            for name, value in bbox_results[\'loss_bbox\'].items():\n                losses[f\'s{i}.{name}\'] = (\n                    value * lw if \'loss\' in name else value)\n\n            # mask head forward and loss\n            if self.with_mask:\n                mask_results = self._mask_forward_train(\n                    i, x, sampling_results, gt_masks, rcnn_train_cfg,\n                    bbox_results[\'bbox_feats\'])\n                # TODO: Support empty tensor input. #2280\n                if mask_results[\'loss_mask\'] is not None:\n                    for name, value in mask_results[\'loss_mask\'].items():\n                        losses[f\'s{i}.{name}\'] = (\n                            value * lw if \'loss\' in name else value)\n\n            # refine bboxes\n            if i < self.num_stages - 1:\n                pos_is_gts = [res.pos_is_gt for res in sampling_results]\n                # bbox_targets is a tuple\n                roi_labels = bbox_results[\'bbox_targets\'][0]\n                with torch.no_grad():\n                    proposal_list = self.bbox_head[i].refine_bboxes(\n                        bbox_results[\'rois\'], roi_labels,\n                        bbox_results[\'bbox_pred\'], pos_is_gts, img_metas)\n\n        return losses\n\n    def simple_test(self, x, proposal_list, img_metas, rescale=False):\n        """"""Test without augmentation.""""""\n        assert self.with_bbox, \'Bbox head must be implemented.\'\n        img_shape = img_metas[0][\'img_shape\']\n        ori_shape = img_metas[0][\'ori_shape\']\n        scale_factor = img_metas[0][\'scale_factor\']\n\n        # ""ms"" in variable names means multi-stage\n        ms_bbox_result = {}\n        ms_segm_result = {}\n        ms_scores = []\n        rcnn_test_cfg = self.test_cfg\n\n        rois = bbox2roi(proposal_list)\n        for i in range(self.num_stages):\n            bbox_results = self._bbox_forward(i, x, rois)\n            ms_scores.append(bbox_results[\'cls_score\'])\n\n            if i < self.num_stages - 1:\n                bbox_label = bbox_results[\'cls_score\'].argmax(dim=1)\n                rois = self.bbox_head[i].regress_by_class(\n                    rois, bbox_label, bbox_results[\'bbox_pred\'], img_metas[0])\n\n        cls_score = sum(ms_scores) / self.num_stages\n        det_bboxes, det_labels = self.bbox_head[-1].get_bboxes(\n            rois,\n            cls_score,\n            bbox_results[\'bbox_pred\'],\n            img_shape,\n            scale_factor,\n            rescale=rescale,\n            cfg=rcnn_test_cfg)\n        bbox_result = bbox2result(det_bboxes, det_labels,\n                                  self.bbox_head[-1].num_classes)\n        ms_bbox_result[\'ensemble\'] = bbox_result\n\n        if self.with_mask:\n            if det_bboxes.shape[0] == 0:\n                mask_classes = self.mask_head[-1].num_classes\n                segm_result = [[] for _ in range(mask_classes)]\n            else:\n                _bboxes = (\n                    det_bboxes[:, :4] * det_bboxes.new_tensor(scale_factor)\n                    if rescale else det_bboxes)\n\n                mask_rois = bbox2roi([_bboxes])\n                aug_masks = []\n                for i in range(self.num_stages):\n                    mask_results = self._mask_forward(i, x, mask_rois)\n                    aug_masks.append(\n                        mask_results[\'mask_pred\'].sigmoid().cpu().numpy())\n                merged_masks = merge_aug_masks(aug_masks,\n                                               [img_metas] * self.num_stages,\n                                               self.test_cfg)\n                segm_result = self.mask_head[-1].get_seg_masks(\n                    merged_masks, _bboxes, det_labels, rcnn_test_cfg,\n                    ori_shape, scale_factor, rescale)\n            ms_segm_result[\'ensemble\'] = segm_result\n\n        if self.with_mask:\n            results = (ms_bbox_result[\'ensemble\'], ms_segm_result[\'ensemble\'])\n        else:\n            results = ms_bbox_result[\'ensemble\']\n\n        return results\n\n    def aug_test(self, features, proposal_list, img_metas, rescale=False):\n        """"""Test with augmentations.\n\n        If rescale is False, then returned bboxes and masks will fit the scale\n        of imgs[0].\n        """"""\n        rcnn_test_cfg = self.test_cfg\n        aug_bboxes = []\n        aug_scores = []\n        for x, img_meta in zip(features, img_metas):\n            # only one image in the batch\n            img_shape = img_meta[0][\'img_shape\']\n            scale_factor = img_meta[0][\'scale_factor\']\n            flip = img_meta[0][\'flip\']\n            flip_direction = img_meta[0][\'flip_direction\']\n\n            proposals = bbox_mapping(proposal_list[0][:, :4], img_shape,\n                                     scale_factor, flip, flip_direction)\n            # ""ms"" in variable names means multi-stage\n            ms_scores = []\n\n            rois = bbox2roi([proposals])\n            for i in range(self.num_stages):\n                bbox_results = self._bbox_forward(i, x, rois)\n                ms_scores.append(bbox_results[\'cls_score\'])\n\n                if i < self.num_stages - 1:\n                    bbox_label = bbox_results[\'cls_score\'].argmax(dim=1)\n                    rois = self.bbox_head[i].regress_by_class(\n                        rois, bbox_label, bbox_results[\'bbox_pred\'],\n                        img_meta[0])\n\n            cls_score = sum(ms_scores) / float(len(ms_scores))\n            bboxes, scores = self.bbox_head[-1].get_bboxes(\n                rois,\n                cls_score,\n                bbox_results[\'bbox_pred\'],\n                img_shape,\n                scale_factor,\n                rescale=False,\n                cfg=None)\n            aug_bboxes.append(bboxes)\n            aug_scores.append(scores)\n\n        # after merging, bboxes will be rescaled to the original image size\n        merged_bboxes, merged_scores = merge_aug_bboxes(\n            aug_bboxes, aug_scores, img_metas, rcnn_test_cfg)\n        det_bboxes, det_labels = multiclass_nms(merged_bboxes, merged_scores,\n                                                rcnn_test_cfg.score_thr,\n                                                rcnn_test_cfg.nms,\n                                                rcnn_test_cfg.max_per_img)\n\n        bbox_result = bbox2result(det_bboxes, det_labels,\n                                  self.bbox_head[-1].num_classes)\n\n        if self.with_mask:\n            if det_bboxes.shape[0] == 0:\n                segm_result = [[]\n                               for _ in range(self.mask_head[-1].num_classes)]\n            else:\n                aug_masks = []\n                aug_img_metas = []\n                for x, img_meta in zip(features, img_metas):\n                    img_shape = img_meta[0][\'img_shape\']\n                    scale_factor = img_meta[0][\'scale_factor\']\n                    flip = img_meta[0][\'flip\']\n                    flip_direction = img_meta[0][\'flip_direction\']\n                    _bboxes = bbox_mapping(det_bboxes[:, :4], img_shape,\n                                           scale_factor, flip, flip_direction)\n                    mask_rois = bbox2roi([_bboxes])\n                    for i in range(self.num_stages):\n                        mask_results = self._mask_forward(i, x, mask_rois)\n                        aug_masks.append(\n                            mask_results[\'mask_pred\'].sigmoid().cpu().numpy())\n                        aug_img_metas.append(img_meta)\n                merged_masks = merge_aug_masks(aug_masks, aug_img_metas,\n                                               self.test_cfg)\n\n                ori_shape = img_metas[0][0][\'ori_shape\']\n                segm_result = self.mask_head[-1].get_seg_masks(\n                    merged_masks,\n                    det_bboxes,\n                    det_labels,\n                    rcnn_test_cfg,\n                    ori_shape,\n                    scale_factor=1.0,\n                    rescale=False)\n            return bbox_result, segm_result\n        else:\n            return bbox_result\n'"
mmdet/models/roi_heads/double_roi_head.py,0,"b'from ..builder import HEADS\nfrom .standard_roi_head import StandardRoIHead\n\n\n@HEADS.register_module()\nclass DoubleHeadRoIHead(StandardRoIHead):\n    """"""RoI head for Double Head RCNN\n\n    https://arxiv.org/abs/1904.06493\n    """"""\n\n    def __init__(self, reg_roi_scale_factor, **kwargs):\n        super(DoubleHeadRoIHead, self).__init__(**kwargs)\n        self.reg_roi_scale_factor = reg_roi_scale_factor\n\n    def _bbox_forward(self, x, rois):\n        bbox_cls_feats = self.bbox_roi_extractor(\n            x[:self.bbox_roi_extractor.num_inputs], rois)\n        bbox_reg_feats = self.bbox_roi_extractor(\n            x[:self.bbox_roi_extractor.num_inputs],\n            rois,\n            roi_scale_factor=self.reg_roi_scale_factor)\n        if self.with_shared_head:\n            bbox_cls_feats = self.shared_head(bbox_cls_feats)\n            bbox_reg_feats = self.shared_head(bbox_reg_feats)\n        cls_score, bbox_pred = self.bbox_head(bbox_cls_feats, bbox_reg_feats)\n\n        bbox_results = dict(\n            cls_score=cls_score,\n            bbox_pred=bbox_pred,\n            bbox_feats=bbox_cls_feats)\n        return bbox_results\n'"
mmdet/models/roi_heads/grid_roi_head.py,4,"b'import torch\n\nfrom mmdet.core import bbox2result, bbox2roi\nfrom ..builder import HEADS, build_head, build_roi_extractor\nfrom .standard_roi_head import StandardRoIHead\n\n\n@HEADS.register_module()\nclass GridRoIHead(StandardRoIHead):\n    """"""Grid roi head for Grid R-CNN.\n\n    https://arxiv.org/abs/1811.12030\n    """"""\n\n    def __init__(self, grid_roi_extractor, grid_head, **kwargs):\n        assert grid_head is not None\n        super(GridRoIHead, self).__init__(**kwargs)\n        if grid_roi_extractor is not None:\n            self.grid_roi_extractor = build_roi_extractor(grid_roi_extractor)\n            self.share_roi_extractor = False\n        else:\n            self.share_roi_extractor = True\n            self.grid_roi_extractor = self.bbox_roi_extractor\n        self.grid_head = build_head(grid_head)\n\n    def init_weights(self, pretrained):\n        super(GridRoIHead, self).init_weights(pretrained)\n        self.grid_head.init_weights()\n        if not self.share_roi_extractor:\n            self.grid_roi_extractor.init_weights()\n\n    def _random_jitter(self, sampling_results, img_metas, amplitude=0.15):\n        """"""Ramdom jitter positive proposals for training.""""""\n        for sampling_result, img_meta in zip(sampling_results, img_metas):\n            bboxes = sampling_result.pos_bboxes\n            random_offsets = bboxes.new_empty(bboxes.shape[0], 4).uniform_(\n                -amplitude, amplitude)\n            # before jittering\n            cxcy = (bboxes[:, 2:4] + bboxes[:, :2]) / 2\n            wh = (bboxes[:, 2:4] - bboxes[:, :2]).abs()\n            # after jittering\n            new_cxcy = cxcy + wh * random_offsets[:, :2]\n            new_wh = wh * (1 + random_offsets[:, 2:])\n            # xywh to xyxy\n            new_x1y1 = (new_cxcy - new_wh / 2)\n            new_x2y2 = (new_cxcy + new_wh / 2)\n            new_bboxes = torch.cat([new_x1y1, new_x2y2], dim=1)\n            # clip bboxes\n            max_shape = img_meta[\'img_shape\']\n            if max_shape is not None:\n                new_bboxes[:, 0::2].clamp_(min=0, max=max_shape[1] - 1)\n                new_bboxes[:, 1::2].clamp_(min=0, max=max_shape[0] - 1)\n\n            sampling_result.pos_bboxes = new_bboxes\n        return sampling_results\n\n    def forward_dummy(self, x, proposals):\n        # bbox head\n        outs = ()\n        rois = bbox2roi([proposals])\n        if self.with_bbox:\n            bbox_results = self._bbox_forward(x, rois)\n            outs = outs + (bbox_results[\'cls_score\'],\n                           bbox_results[\'bbox_pred\'])\n\n        # grid head\n        grid_rois = rois[:100]\n        grid_feats = self.grid_roi_extractor(\n            x[:self.grid_roi_extractor.num_inputs], grid_rois)\n        if self.with_shared_head:\n            grid_feats = self.shared_head(grid_feats)\n        grid_pred = self.grid_head(grid_feats)\n        outs = outs + (grid_pred, )\n\n        # mask head\n        if self.with_mask:\n            mask_rois = rois[:100]\n            mask_results = self._mask_forward(x, mask_rois)\n            outs = outs + (mask_results[\'mask_pred\'], )\n        return outs\n\n    def _bbox_forward_train(self, x, sampling_results, gt_bboxes, gt_labels,\n                            img_metas):\n        bbox_results = super(GridRoIHead,\n                             self)._bbox_forward_train(x, sampling_results,\n                                                       gt_bboxes, gt_labels,\n                                                       img_metas)\n\n        # Grid head forward and loss\n        sampling_results = self._random_jitter(sampling_results, img_metas)\n        pos_rois = bbox2roi([res.pos_bboxes for res in sampling_results])\n\n        # GN in head does not support zero shape input\n        if pos_rois.shape[0] == 0:\n            return bbox_results\n\n        grid_feats = self.grid_roi_extractor(\n            x[:self.grid_roi_extractor.num_inputs], pos_rois)\n        if self.with_shared_head:\n            grid_feats = self.shared_head(grid_feats)\n        # Accelerate training\n        max_sample_num_grid = self.train_cfg.get(\'max_num_grid\', 192)\n        sample_idx = torch.randperm(\n            grid_feats.shape[0])[:min(grid_feats.shape[0], max_sample_num_grid\n                                      )]\n        grid_feats = grid_feats[sample_idx]\n\n        grid_pred = self.grid_head(grid_feats)\n\n        grid_targets = self.grid_head.get_targets(sampling_results,\n                                                  self.train_cfg)\n        grid_targets = grid_targets[sample_idx]\n\n        loss_grid = self.grid_head.loss(grid_pred, grid_targets)\n\n        bbox_results[\'loss_bbox\'].update(loss_grid)\n        return bbox_results\n\n    def simple_test(self,\n                    x,\n                    proposal_list,\n                    img_metas,\n                    proposals=None,\n                    rescale=False):\n        """"""Test without augmentation.""""""\n        assert self.with_bbox, \'Bbox head must be implemented.\'\n\n        det_bboxes, det_labels = self.simple_test_bboxes(\n            x, img_metas, proposal_list, self.test_cfg, rescale=False)\n        # pack rois into bboxes\n        grid_rois = bbox2roi([det_bboxes[:, :4]])\n        grid_feats = self.grid_roi_extractor(\n            x[:len(self.grid_roi_extractor.featmap_strides)], grid_rois)\n        if grid_rois.shape[0] != 0:\n            self.grid_head.test_mode = True\n            grid_pred = self.grid_head(grid_feats)\n            det_bboxes = self.grid_head.get_bboxes(det_bboxes,\n                                                   grid_pred[\'fused\'],\n                                                   img_metas)\n            if rescale:\n                scale_factor = img_metas[0][\'scale_factor\']\n                if not isinstance(scale_factor, (float, torch.Tensor)):\n                    scale_factor = det_bboxes.new_tensor(scale_factor)\n                det_bboxes[:, :4] /= scale_factor\n        else:\n            det_bboxes = torch.Tensor([])\n\n        bbox_results = bbox2result(det_bboxes, det_labels,\n                                   self.bbox_head.num_classes)\n\n        if not self.with_mask:\n            return bbox_results\n        else:\n            segm_results = self.simple_test_mask(\n                x, img_metas, det_bboxes, det_labels, rescale=rescale)\n            return bbox_results, segm_results\n'"
mmdet/models/roi_heads/htc_roi_head.py,4,"b'import torch\nimport torch.nn.functional as F\n\nfrom mmdet.core import (bbox2result, bbox2roi, bbox_mapping, merge_aug_bboxes,\n                        merge_aug_masks, multiclass_nms)\nfrom ..builder import HEADS, build_head, build_roi_extractor\nfrom .cascade_roi_head import CascadeRoIHead\n\n\n@HEADS.register_module()\nclass HybridTaskCascadeRoIHead(CascadeRoIHead):\n    """"""Hybrid task cascade roi head including one bbox head and one mask head.\n\n    https://arxiv.org/abs/1901.07518\n    """"""\n\n    def __init__(self,\n                 num_stages,\n                 stage_loss_weights,\n                 semantic_roi_extractor=None,\n                 semantic_head=None,\n                 semantic_fusion=(\'bbox\', \'mask\'),\n                 interleaved=True,\n                 mask_info_flow=True,\n                 **kwargs):\n        super(HybridTaskCascadeRoIHead,\n              self).__init__(num_stages, stage_loss_weights, **kwargs)\n        assert self.with_bbox and self.with_mask\n        assert not self.with_shared_head  # shared head is not supported\n\n        if semantic_head is not None:\n            self.semantic_roi_extractor = build_roi_extractor(\n                semantic_roi_extractor)\n            self.semantic_head = build_head(semantic_head)\n\n        self.semantic_fusion = semantic_fusion\n        self.interleaved = interleaved\n        self.mask_info_flow = mask_info_flow\n\n    def init_weights(self, pretrained):\n        super(HybridTaskCascadeRoIHead, self).init_weights(pretrained)\n        if self.with_semantic:\n            self.semantic_head.init_weights()\n\n    @property\n    def with_semantic(self):\n        if hasattr(self, \'semantic_head\') and self.semantic_head is not None:\n            return True\n        else:\n            return False\n\n    def forward_dummy(self, x, proposals):\n        outs = ()\n        # semantic head\n        if self.with_semantic:\n            _, semantic_feat = self.semantic_head(x)\n        else:\n            semantic_feat = None\n        # bbox heads\n        rois = bbox2roi([proposals])\n        for i in range(self.num_stages):\n            bbox_results = self._bbox_forward(\n                i, x, rois, semantic_feat=semantic_feat)\n            outs = outs + (bbox_results[\'cls_score\'],\n                           bbox_results[\'bbox_pred\'])\n        # mask heads\n        if self.with_mask:\n            mask_rois = rois[:100]\n            mask_roi_extractor = self.mask_roi_extractor[-1]\n            mask_feats = mask_roi_extractor(\n                x[:len(mask_roi_extractor.featmap_strides)], mask_rois)\n            if self.with_semantic and \'mask\' in self.semantic_fusion:\n                mask_semantic_feat = self.semantic_roi_extractor(\n                    [semantic_feat], mask_rois)\n                mask_feats += mask_semantic_feat\n            last_feat = None\n            for i in range(self.num_stages):\n                mask_head = self.mask_head[i]\n                if self.mask_info_flow:\n                    mask_pred, last_feat = mask_head(mask_feats, last_feat)\n                else:\n                    mask_pred = mask_head(mask_feats)\n                outs = outs + (mask_pred, )\n        return outs\n\n    def _bbox_forward_train(self,\n                            stage,\n                            x,\n                            sampling_results,\n                            gt_bboxes,\n                            gt_labels,\n                            rcnn_train_cfg,\n                            semantic_feat=None):\n        bbox_head = self.bbox_head[stage]\n        rois = bbox2roi([res.bboxes for res in sampling_results])\n        bbox_results = self._bbox_forward(\n            stage, x, rois, semantic_feat=semantic_feat)\n\n        bbox_targets = bbox_head.get_targets(sampling_results, gt_bboxes,\n                                             gt_labels, rcnn_train_cfg)\n        loss_bbox = bbox_head.loss(bbox_results[\'cls_score\'],\n                                   bbox_results[\'bbox_pred\'], rois,\n                                   *bbox_targets)\n\n        bbox_results.update(\n            loss_bbox=loss_bbox,\n            rois=rois,\n            bbox_targets=bbox_targets,\n        )\n        return bbox_results\n\n    def _mask_forward_train(self,\n                            stage,\n                            x,\n                            sampling_results,\n                            gt_masks,\n                            rcnn_train_cfg,\n                            semantic_feat=None):\n        mask_roi_extractor = self.mask_roi_extractor[stage]\n        mask_head = self.mask_head[stage]\n        pos_rois = bbox2roi([res.pos_bboxes for res in sampling_results])\n        mask_feats = mask_roi_extractor(x[:mask_roi_extractor.num_inputs],\n                                        pos_rois)\n\n        # semantic feature fusion\n        # element-wise sum for original features and pooled semantic features\n        if self.with_semantic and \'mask\' in self.semantic_fusion:\n            mask_semantic_feat = self.semantic_roi_extractor([semantic_feat],\n                                                             pos_rois)\n            if mask_semantic_feat.shape[-2:] != mask_feats.shape[-2:]:\n                mask_semantic_feat = F.adaptive_avg_pool2d(\n                    mask_semantic_feat, mask_feats.shape[-2:])\n            mask_feats += mask_semantic_feat\n\n        # mask information flow\n        # forward all previous mask heads to obtain last_feat, and fuse it\n        # with the normal mask feature\n        if self.mask_info_flow:\n            last_feat = None\n            for i in range(stage):\n                last_feat = self.mask_head[i](\n                    mask_feats, last_feat, return_logits=False)\n            mask_pred = mask_head(mask_feats, last_feat, return_feat=False)\n        else:\n            mask_pred = mask_head(mask_feats, return_feat=False)\n\n        mask_targets = mask_head.get_targets(sampling_results, gt_masks,\n                                             rcnn_train_cfg)\n        pos_labels = torch.cat([res.pos_gt_labels for res in sampling_results])\n        loss_mask = mask_head.loss(mask_pred, mask_targets, pos_labels)\n\n        mask_results = dict(loss_mask=loss_mask)\n        return mask_results\n\n    def _bbox_forward(self, stage, x, rois, semantic_feat=None):\n        bbox_roi_extractor = self.bbox_roi_extractor[stage]\n        bbox_head = self.bbox_head[stage]\n        bbox_feats = bbox_roi_extractor(\n            x[:len(bbox_roi_extractor.featmap_strides)], rois)\n        if self.with_semantic and \'bbox\' in self.semantic_fusion:\n            bbox_semantic_feat = self.semantic_roi_extractor([semantic_feat],\n                                                             rois)\n            if bbox_semantic_feat.shape[-2:] != bbox_feats.shape[-2:]:\n                bbox_semantic_feat = F.adaptive_avg_pool2d(\n                    bbox_semantic_feat, bbox_feats.shape[-2:])\n            bbox_feats += bbox_semantic_feat\n        cls_score, bbox_pred = bbox_head(bbox_feats)\n\n        bbox_results = dict(cls_score=cls_score, bbox_pred=bbox_pred)\n        return bbox_results\n\n    def _mask_forward_test(self, stage, x, bboxes, semantic_feat=None):\n        mask_roi_extractor = self.mask_roi_extractor[stage]\n        mask_head = self.mask_head[stage]\n        mask_rois = bbox2roi([bboxes])\n        mask_feats = mask_roi_extractor(\n            x[:len(mask_roi_extractor.featmap_strides)], mask_rois)\n        if self.with_semantic and \'mask\' in self.semantic_fusion:\n            mask_semantic_feat = self.semantic_roi_extractor([semantic_feat],\n                                                             mask_rois)\n            if mask_semantic_feat.shape[-2:] != mask_feats.shape[-2:]:\n                mask_semantic_feat = F.adaptive_avg_pool2d(\n                    mask_semantic_feat, mask_feats.shape[-2:])\n            mask_feats += mask_semantic_feat\n        if self.mask_info_flow:\n            last_feat = None\n            last_pred = None\n            for i in range(stage):\n                mask_pred, last_feat = self.mask_head[i](mask_feats, last_feat)\n                if last_pred is not None:\n                    mask_pred = mask_pred + last_pred\n                last_pred = mask_pred\n            mask_pred = mask_head(mask_feats, last_feat, return_feat=False)\n            if last_pred is not None:\n                mask_pred = mask_pred + last_pred\n        else:\n            mask_pred = mask_head(mask_feats)\n        return mask_pred\n\n    def forward_train(self,\n                      x,\n                      img_metas,\n                      proposal_list,\n                      gt_bboxes,\n                      gt_labels,\n                      gt_bboxes_ignore=None,\n                      gt_masks=None,\n                      gt_semantic_seg=None):\n        # semantic segmentation part\n        # 2 outputs: segmentation prediction and embedded features\n        losses = dict()\n        if self.with_semantic:\n            semantic_pred, semantic_feat = self.semantic_head(x)\n            loss_seg = self.semantic_head.loss(semantic_pred, gt_semantic_seg)\n            losses[\'loss_semantic_seg\'] = loss_seg\n        else:\n            semantic_feat = None\n\n        for i in range(self.num_stages):\n            self.current_stage = i\n            rcnn_train_cfg = self.train_cfg[i]\n            lw = self.stage_loss_weights[i]\n\n            # assign gts and sample proposals\n            sampling_results = []\n            bbox_assigner = self.bbox_assigner[i]\n            bbox_sampler = self.bbox_sampler[i]\n            num_imgs = len(img_metas)\n            if gt_bboxes_ignore is None:\n                gt_bboxes_ignore = [None for _ in range(num_imgs)]\n\n            for j in range(num_imgs):\n                assign_result = bbox_assigner.assign(proposal_list[j],\n                                                     gt_bboxes[j],\n                                                     gt_bboxes_ignore[j],\n                                                     gt_labels[j])\n                sampling_result = bbox_sampler.sample(\n                    assign_result,\n                    proposal_list[j],\n                    gt_bboxes[j],\n                    gt_labels[j],\n                    feats=[lvl_feat[j][None] for lvl_feat in x])\n                sampling_results.append(sampling_result)\n\n            # bbox head forward and loss\n            bbox_results = \\\n                self._bbox_forward_train(\n                    i, x, sampling_results, gt_bboxes, gt_labels,\n                    rcnn_train_cfg, semantic_feat)\n            roi_labels = bbox_results[\'bbox_targets\'][0]\n\n            for name, value in bbox_results[\'loss_bbox\'].items():\n                losses[f\'s{i}.{name}\'] = (\n                    value * lw if \'loss\' in name else value)\n\n            # mask head forward and loss\n            if self.with_mask:\n                # interleaved execution: use regressed bboxes by the box branch\n                # to train the mask branch\n                if self.interleaved:\n                    pos_is_gts = [res.pos_is_gt for res in sampling_results]\n                    with torch.no_grad():\n                        proposal_list = self.bbox_head[i].refine_bboxes(\n                            bbox_results[\'rois\'], roi_labels,\n                            bbox_results[\'bbox_pred\'], pos_is_gts, img_metas)\n                        # re-assign and sample 512 RoIs from 512 RoIs\n                        sampling_results = []\n                        for j in range(num_imgs):\n                            assign_result = bbox_assigner.assign(\n                                proposal_list[j], gt_bboxes[j],\n                                gt_bboxes_ignore[j], gt_labels[j])\n                            sampling_result = bbox_sampler.sample(\n                                assign_result,\n                                proposal_list[j],\n                                gt_bboxes[j],\n                                gt_labels[j],\n                                feats=[lvl_feat[j][None] for lvl_feat in x])\n                            sampling_results.append(sampling_result)\n                mask_results = self._mask_forward_train(\n                    i, x, sampling_results, gt_masks, rcnn_train_cfg,\n                    semantic_feat)\n                for name, value in mask_results[\'loss_mask\'].items():\n                    losses[f\'s{i}.{name}\'] = (\n                        value * lw if \'loss\' in name else value)\n\n            # refine bboxes (same as Cascade R-CNN)\n            if i < self.num_stages - 1 and not self.interleaved:\n                pos_is_gts = [res.pos_is_gt for res in sampling_results]\n                with torch.no_grad():\n                    proposal_list = self.bbox_head[i].refine_bboxes(\n                        bbox_results[\'rois\'], roi_labels,\n                        bbox_results[\'bbox_pred\'], pos_is_gts, img_metas)\n\n        return losses\n\n    def simple_test(self, x, proposal_list, img_metas, rescale=False):\n        if self.with_semantic:\n            _, semantic_feat = self.semantic_head(x)\n        else:\n            semantic_feat = None\n\n        img_shape = img_metas[0][\'img_shape\']\n        ori_shape = img_metas[0][\'ori_shape\']\n        scale_factor = img_metas[0][\'scale_factor\']\n\n        # ""ms"" in variable names means multi-stage\n        ms_bbox_result = {}\n        ms_segm_result = {}\n        ms_scores = []\n        rcnn_test_cfg = self.test_cfg\n\n        rois = bbox2roi(proposal_list)\n        for i in range(self.num_stages):\n            bbox_head = self.bbox_head[i]\n            bbox_results = self._bbox_forward(\n                i, x, rois, semantic_feat=semantic_feat)\n            ms_scores.append(bbox_results[\'cls_score\'])\n\n            if i < self.num_stages - 1:\n                bbox_label = bbox_results[\'cls_score\'].argmax(dim=1)\n                rois = bbox_head.regress_by_class(rois, bbox_label,\n                                                  bbox_results[\'bbox_pred\'],\n                                                  img_metas[0])\n\n        cls_score = sum(ms_scores) / float(len(ms_scores))\n        det_bboxes, det_labels = self.bbox_head[-1].get_bboxes(\n            rois,\n            cls_score,\n            bbox_results[\'bbox_pred\'],\n            img_shape,\n            scale_factor,\n            rescale=rescale,\n            cfg=rcnn_test_cfg)\n        bbox_result = bbox2result(det_bboxes, det_labels,\n                                  self.bbox_head[-1].num_classes)\n        ms_bbox_result[\'ensemble\'] = bbox_result\n\n        if self.with_mask:\n            if det_bboxes.shape[0] == 0:\n                mask_classes = self.mask_head[-1].num_classes\n                segm_result = [[] for _ in range(mask_classes)]\n            else:\n                _bboxes = (\n                    det_bboxes[:, :4] * det_bboxes.new_tensor(scale_factor)\n                    if rescale else det_bboxes)\n\n                mask_rois = bbox2roi([_bboxes])\n                aug_masks = []\n                mask_roi_extractor = self.mask_roi_extractor[-1]\n                mask_feats = mask_roi_extractor(\n                    x[:len(mask_roi_extractor.featmap_strides)], mask_rois)\n                if self.with_semantic and \'mask\' in self.semantic_fusion:\n                    mask_semantic_feat = self.semantic_roi_extractor(\n                        [semantic_feat], mask_rois)\n                    mask_feats += mask_semantic_feat\n                last_feat = None\n                for i in range(self.num_stages):\n                    mask_head = self.mask_head[i]\n                    if self.mask_info_flow:\n                        mask_pred, last_feat = mask_head(mask_feats, last_feat)\n                    else:\n                        mask_pred = mask_head(mask_feats)\n                    aug_masks.append(mask_pred.sigmoid().cpu().numpy())\n                merged_masks = merge_aug_masks(aug_masks,\n                                               [img_metas] * self.num_stages,\n                                               self.test_cfg)\n                segm_result = self.mask_head[-1].get_seg_masks(\n                    merged_masks, _bboxes, det_labels, rcnn_test_cfg,\n                    ori_shape, scale_factor, rescale)\n            ms_segm_result[\'ensemble\'] = segm_result\n\n        if self.with_mask:\n            results = (ms_bbox_result[\'ensemble\'], ms_segm_result[\'ensemble\'])\n        else:\n            results = ms_bbox_result[\'ensemble\']\n\n        return results\n\n    def aug_test(self, img_feats, proposal_list, img_metas, rescale=False):\n        """"""Test with augmentations.\n\n        If rescale is False, then returned bboxes and masks will fit the scale\n        of imgs[0].\n        """"""\n        if self.with_semantic:\n            semantic_feats = [\n                self.semantic_head(feat)[1] for feat in img_feats\n            ]\n        else:\n            semantic_feats = [None] * len(img_metas)\n\n        rcnn_test_cfg = self.test_cfg\n        aug_bboxes = []\n        aug_scores = []\n        for x, img_meta, semantic in zip(img_feats, img_metas, semantic_feats):\n            # only one image in the batch\n            img_shape = img_meta[0][\'img_shape\']\n            scale_factor = img_meta[0][\'scale_factor\']\n            flip = img_meta[0][\'flip\']\n            flip_direction = img_meta[0][\'flip_direction\']\n\n            proposals = bbox_mapping(proposal_list[0][:, :4], img_shape,\n                                     scale_factor, flip, flip_direction)\n            # ""ms"" in variable names means multi-stage\n            ms_scores = []\n\n            rois = bbox2roi([proposals])\n            for i in range(self.num_stages):\n                bbox_head = self.bbox_head[i]\n                bbox_results = self._bbox_forward(\n                    i, x, rois, semantic_feat=semantic)\n                ms_scores.append(bbox_results[\'cls_score\'])\n\n                if i < self.num_stages - 1:\n                    bbox_label = bbox_results[\'cls_score\'].argmax(dim=1)\n                    rois = bbox_head.regress_by_class(\n                        rois, bbox_label, bbox_results[\'bbox_pred\'],\n                        img_meta[0])\n\n            cls_score = sum(ms_scores) / float(len(ms_scores))\n            bboxes, scores = self.bbox_head[-1].get_bboxes(\n                rois,\n                cls_score,\n                bbox_results[\'bbox_pred\'],\n                img_shape,\n                scale_factor,\n                rescale=False,\n                cfg=None)\n            aug_bboxes.append(bboxes)\n            aug_scores.append(scores)\n\n        # after merging, bboxes will be rescaled to the original image size\n        merged_bboxes, merged_scores = merge_aug_bboxes(\n            aug_bboxes, aug_scores, img_metas, rcnn_test_cfg)\n        det_bboxes, det_labels = multiclass_nms(merged_bboxes, merged_scores,\n                                                rcnn_test_cfg.score_thr,\n                                                rcnn_test_cfg.nms,\n                                                rcnn_test_cfg.max_per_img)\n\n        bbox_result = bbox2result(det_bboxes, det_labels,\n                                  self.bbox_head[-1].num_classes)\n\n        if self.with_mask:\n            if det_bboxes.shape[0] == 0:\n                segm_result = [[]\n                               for _ in range(self.mask_head[-1].num_classes -\n                                              1)]\n            else:\n                aug_masks = []\n                aug_img_metas = []\n                for x, img_meta, semantic in zip(img_feats, img_metas,\n                                                 semantic_feats):\n                    img_shape = img_meta[0][\'img_shape\']\n                    scale_factor = img_meta[0][\'scale_factor\']\n                    flip = img_meta[0][\'flip\']\n                    flip_direction = img_meta[0][\'flip_direction\']\n                    _bboxes = bbox_mapping(det_bboxes[:, :4], img_shape,\n                                           scale_factor, flip, flip_direction)\n                    mask_rois = bbox2roi([_bboxes])\n                    mask_feats = self.mask_roi_extractor[-1](\n                        x[:len(self.mask_roi_extractor[-1].featmap_strides)],\n                        mask_rois)\n                    if self.with_semantic:\n                        semantic_feat = semantic\n                        mask_semantic_feat = self.semantic_roi_extractor(\n                            [semantic_feat], mask_rois)\n                        if mask_semantic_feat.shape[-2:] != mask_feats.shape[\n                                -2:]:\n                            mask_semantic_feat = F.adaptive_avg_pool2d(\n                                mask_semantic_feat, mask_feats.shape[-2:])\n                        mask_feats += mask_semantic_feat\n                    last_feat = None\n                    for i in range(self.num_stages):\n                        mask_head = self.mask_head[i]\n                        if self.mask_info_flow:\n                            mask_pred, last_feat = mask_head(\n                                mask_feats, last_feat)\n                        else:\n                            mask_pred = mask_head(mask_feats)\n                        aug_masks.append(mask_pred.sigmoid().cpu().numpy())\n                        aug_img_metas.append(img_meta)\n                merged_masks = merge_aug_masks(aug_masks, aug_img_metas,\n                                               self.test_cfg)\n\n                ori_shape = img_metas[0][0][\'ori_shape\']\n                segm_result = self.mask_head[-1].get_seg_masks(\n                    merged_masks,\n                    det_bboxes,\n                    det_labels,\n                    rcnn_test_cfg,\n                    ori_shape,\n                    scale_factor=1.0,\n                    rescale=False)\n            return bbox_result, segm_result\n        else:\n            return bbox_result\n'"
mmdet/models/roi_heads/mask_scoring_roi_head.py,1,"b'import torch\n\nfrom mmdet.core import bbox2roi\nfrom ..builder import HEADS, build_head\nfrom .standard_roi_head import StandardRoIHead\n\n\n@HEADS.register_module()\nclass MaskScoringRoIHead(StandardRoIHead):\n    """"""Mask Scoring RoIHead for Mask Scoring RCNN.\n\n    https://arxiv.org/abs/1903.00241\n    """"""\n\n    def __init__(self, mask_iou_head, **kwargs):\n        assert mask_iou_head is not None\n        super(MaskScoringRoIHead, self).__init__(**kwargs)\n        self.mask_iou_head = build_head(mask_iou_head)\n\n    def init_weights(self, pretrained):\n        super(MaskScoringRoIHead, self).init_weights(pretrained)\n        self.mask_iou_head.init_weights()\n\n    def _mask_forward_train(self, x, sampling_results, bbox_feats, gt_masks,\n                            img_metas):\n        pos_labels = torch.cat([res.pos_gt_labels for res in sampling_results])\n        mask_results = super(MaskScoringRoIHead,\n                             self)._mask_forward_train(x, sampling_results,\n                                                       bbox_feats, gt_masks,\n                                                       img_metas)\n        if mask_results[\'loss_mask\'] is None:\n            return mask_results\n\n        # mask iou head forward and loss\n        pos_mask_pred = mask_results[\'mask_pred\'][\n            range(mask_results[\'mask_pred\'].size(0)), pos_labels]\n        mask_iou_pred = self.mask_iou_head(mask_results[\'mask_feats\'],\n                                           pos_mask_pred)\n        pos_mask_iou_pred = mask_iou_pred[range(mask_iou_pred.size(0)),\n                                          pos_labels]\n\n        mask_iou_targets = self.mask_iou_head.get_targets(\n            sampling_results, gt_masks, pos_mask_pred,\n            mask_results[\'mask_targets\'], self.train_cfg)\n        loss_mask_iou = self.mask_iou_head.loss(pos_mask_iou_pred,\n                                                mask_iou_targets)\n        mask_results[\'loss_mask\'].update(loss_mask_iou)\n        return mask_results\n\n    def simple_test_mask(self,\n                         x,\n                         img_metas,\n                         det_bboxes,\n                         det_labels,\n                         rescale=False):\n        # image shape of the first image in the batch (only one)\n        ori_shape = img_metas[0][\'ori_shape\']\n        scale_factor = img_metas[0][\'scale_factor\']\n\n        if det_bboxes.shape[0] == 0:\n            segm_result = [[] for _ in range(self.mask_head.num_classes)]\n            mask_scores = [[] for _ in range(self.mask_head.num_classes)]\n        else:\n            # if det_bboxes is rescaled to the original image size, we need to\n            # rescale it back to the testing scale to obtain RoIs.\n            _bboxes = (\n                det_bboxes[:, :4] *\n                det_bboxes.new_tensor(scale_factor) if rescale else det_bboxes)\n            mask_rois = bbox2roi([_bboxes])\n            mask_results = self._mask_forward(x, mask_rois)\n            segm_result = self.mask_head.get_seg_masks(\n                mask_results[\'mask_pred\'], _bboxes, det_labels, self.test_cfg,\n                ori_shape, scale_factor, rescale)\n            # get mask scores with mask iou head\n            mask_iou_pred = self.mask_iou_head(\n                mask_results[\'mask_feats\'],\n                mask_results[\'mask_pred\'][range(det_labels.size(0)),\n                                          det_labels])\n            mask_scores = self.mask_iou_head.get_mask_scores(\n                mask_iou_pred, det_bboxes, det_labels)\n        return segm_result, mask_scores\n'"
mmdet/models/roi_heads/pisa_roi_head.py,0,"b'from mmdet.core import bbox2roi\nfrom ..builder import HEADS\nfrom ..losses.pisa_loss import carl_loss, isr_p\nfrom .standard_roi_head import StandardRoIHead\n\n\n@HEADS.register_module()\nclass PISARoIHead(StandardRoIHead):\n\n    def forward_train(self,\n                      x,\n                      img_metas,\n                      proposal_list,\n                      gt_bboxes,\n                      gt_labels,\n                      gt_bboxes_ignore=None,\n                      gt_masks=None):\n        """""" StandardRoIHead with PrIme Sample Attention (PISA),\n        described in `PISA <https://arxiv.org/abs/1904.04821>`_.\n\n        Args:\n            x (list[Tensor]): List of multi-level img features.\n            img_metas (list[dict]): List of image info dict where each dict\n                has: \'img_shape\', \'scale_factor\', \'flip\', and may also contain\n                \'filename\', \'ori_shape\', \'pad_shape\', and \'img_norm_cfg\'.\n                For details on the values of these keys see\n                `mmdet/datasets/pipelines/formatting.py:Collect`.\n            proposals (list[Tensors]): List of region proposals.\n            gt_bboxes (list[Tensor]): Each item are the truth boxes for each\n                image in [tl_x, tl_y, br_x, br_y] format.\n            gt_labels (list[Tensor]): Class indices corresponding to each box\n            gt_bboxes_ignore (list[Tensor], optional): Specify which bounding\n                boxes can be ignored when computing the loss.\n            gt_masks (None | Tensor) : True segmentation masks for each box\n                used if the architecture supports a segmentation task.\n\n        Returns:\n            dict[str, Tensor]: a dictionary of loss components\n        """"""\n        # assign gts and sample proposals\n        if self.with_bbox or self.with_mask:\n            num_imgs = len(img_metas)\n            if gt_bboxes_ignore is None:\n                gt_bboxes_ignore = [None for _ in range(num_imgs)]\n            sampling_results = []\n            neg_label_weights = []\n            for i in range(num_imgs):\n                assign_result = self.bbox_assigner.assign(\n                    proposal_list[i], gt_bboxes[i], gt_bboxes_ignore[i],\n                    gt_labels[i])\n                sampling_result = self.bbox_sampler.sample(\n                    assign_result,\n                    proposal_list[i],\n                    gt_bboxes[i],\n                    gt_labels[i],\n                    feats=[lvl_feat[i][None] for lvl_feat in x])\n                # neg label weight is obtained by sampling when using ISR-N\n                neg_label_weight = None\n                if isinstance(sampling_result, tuple):\n                    sampling_result, neg_label_weight = sampling_result\n                sampling_results.append(sampling_result)\n                neg_label_weights.append(neg_label_weight)\n\n        losses = dict()\n        # bbox head forward and loss\n        if self.with_bbox:\n            bbox_results = self._bbox_forward_train(\n                x,\n                sampling_results,\n                gt_bboxes,\n                gt_labels,\n                img_metas,\n                neg_label_weights=neg_label_weights)\n            losses.update(bbox_results[\'loss_bbox\'])\n\n        # mask head forward and loss\n        if self.with_mask:\n            mask_results = self._mask_forward_train(x, sampling_results,\n                                                    bbox_results[\'bbox_feats\'],\n                                                    gt_masks, img_metas)\n            # TODO: Support empty tensor input. #2280\n            if mask_results[\'loss_mask\'] is not None:\n                losses.update(mask_results[\'loss_mask\'])\n\n        return losses\n\n    def _bbox_forward(self, x, rois):\n        # TODO: a more flexible way to decide which feature maps to use\n        bbox_feats = self.bbox_roi_extractor(\n            x[:self.bbox_roi_extractor.num_inputs], rois)\n        if self.with_shared_head:\n            bbox_feats = self.shared_head(bbox_feats)\n        cls_score, bbox_pred = self.bbox_head(bbox_feats)\n\n        bbox_results = dict(\n            cls_score=cls_score, bbox_pred=bbox_pred, bbox_feats=bbox_feats)\n        return bbox_results\n\n    def _bbox_forward_train(self,\n                            x,\n                            sampling_results,\n                            gt_bboxes,\n                            gt_labels,\n                            img_metas,\n                            neg_label_weights=None):\n        rois = bbox2roi([res.bboxes for res in sampling_results])\n\n        bbox_results = self._bbox_forward(x, rois)\n\n        bbox_targets = self.bbox_head.get_targets(sampling_results, gt_bboxes,\n                                                  gt_labels, self.train_cfg)\n\n        # neg_label_weights obtained by sampler is image-wise, mapping back to\n        # the corresponding location in label weights\n        if neg_label_weights[0] is not None:\n            label_weights = bbox_targets[1]\n            cur_num_rois = 0\n            for i in range(len(sampling_results)):\n                num_pos = sampling_results[i].pos_inds.size(0)\n                num_neg = sampling_results[i].neg_inds.size(0)\n                label_weights[cur_num_rois + num_pos:cur_num_rois + num_pos +\n                              num_neg] = neg_label_weights[i]\n                cur_num_rois += num_pos + num_neg\n\n        cls_score = bbox_results[\'cls_score\']\n        bbox_pred = bbox_results[\'bbox_pred\']\n\n        # Apply ISR-P\n        isr_cfg = self.train_cfg.get(\'isr\', None)\n        if isr_cfg is not None:\n            bbox_targets = isr_p(\n                cls_score,\n                bbox_pred,\n                bbox_targets,\n                rois,\n                sampling_results,\n                self.bbox_head.loss_cls,\n                self.bbox_head.bbox_coder,\n                **isr_cfg,\n                num_class=self.bbox_head.num_classes)\n        loss_bbox = self.bbox_head.loss(cls_score, bbox_pred, rois,\n                                        *bbox_targets)\n\n        # Add CARL Loss\n        carl_cfg = self.train_cfg.get(\'carl\', None)\n        if carl_cfg is not None:\n            loss_carl = carl_loss(\n                cls_score,\n                bbox_targets[0],\n                bbox_pred,\n                bbox_targets[2],\n                self.bbox_head.loss_bbox,\n                **carl_cfg,\n                num_class=self.bbox_head.num_classes)\n            loss_bbox.update(loss_carl)\n\n        bbox_results.update(loss_bbox=loss_bbox)\n        return bbox_results\n'"
mmdet/models/roi_heads/standard_roi_head.py,6,"b'import torch\n\nfrom mmdet.core import bbox2result, bbox2roi, build_assigner, build_sampler\nfrom ..builder import HEADS, build_head, build_roi_extractor\nfrom .base_roi_head import BaseRoIHead\nfrom .test_mixins import BBoxTestMixin, MaskTestMixin\n\n\n@HEADS.register_module()\nclass StandardRoIHead(BaseRoIHead, BBoxTestMixin, MaskTestMixin):\n    """"""Simplest base roi head including one bbox head and one mask head.\n    """"""\n\n    def init_assigner_sampler(self):\n        self.bbox_assigner = None\n        self.bbox_sampler = None\n        if self.train_cfg:\n            self.bbox_assigner = build_assigner(self.train_cfg.assigner)\n            self.bbox_sampler = build_sampler(\n                self.train_cfg.sampler, context=self)\n\n    def init_bbox_head(self, bbox_roi_extractor, bbox_head):\n        self.bbox_roi_extractor = build_roi_extractor(bbox_roi_extractor)\n        self.bbox_head = build_head(bbox_head)\n\n    def init_mask_head(self, mask_roi_extractor, mask_head):\n        if mask_roi_extractor is not None:\n            self.mask_roi_extractor = build_roi_extractor(mask_roi_extractor)\n            self.share_roi_extractor = False\n        else:\n            self.share_roi_extractor = True\n            self.mask_roi_extractor = self.bbox_roi_extractor\n        self.mask_head = build_head(mask_head)\n\n    def init_weights(self, pretrained):\n        if self.with_shared_head:\n            self.shared_head.init_weights(pretrained=pretrained)\n        if self.with_bbox:\n            self.bbox_roi_extractor.init_weights()\n            self.bbox_head.init_weights()\n        if self.with_mask:\n            self.mask_head.init_weights()\n            if not self.share_roi_extractor:\n                self.mask_roi_extractor.init_weights()\n\n    def forward_dummy(self, x, proposals):\n        # bbox head\n        outs = ()\n        rois = bbox2roi([proposals])\n        if self.with_bbox:\n            bbox_results = self._bbox_forward(x, rois)\n            outs = outs + (bbox_results[\'cls_score\'],\n                           bbox_results[\'bbox_pred\'])\n        # mask head\n        if self.with_mask:\n            mask_rois = rois[:100]\n            mask_results = self._mask_forward(x, mask_rois)\n            outs = outs + (mask_results[\'mask_pred\'], )\n        return outs\n\n    def forward_train(self,\n                      x,\n                      img_metas,\n                      proposal_list,\n                      gt_bboxes,\n                      gt_labels,\n                      gt_bboxes_ignore=None,\n                      gt_masks=None):\n        """"""\n        Args:\n            x (list[Tensor]): list of multi-level img features.\n\n            img_metas (list[dict]): list of image info dict where each dict\n                has: \'img_shape\', \'scale_factor\', \'flip\', and may also contain\n                \'filename\', \'ori_shape\', \'pad_shape\', and \'img_norm_cfg\'.\n                For details on the values of these keys see\n                `mmdet/datasets/pipelines/formatting.py:Collect`.\n\n            proposals (list[Tensors]): list of region proposals.\n\n            gt_bboxes (list[Tensor]): each item are the truth boxes for each\n                image in [tl_x, tl_y, br_x, br_y] format.\n\n            gt_labels (list[Tensor]): class indices corresponding to each box\n\n            gt_bboxes_ignore (None | list[Tensor]): specify which bounding\n                boxes can be ignored when computing the loss.\n\n            gt_masks (None | Tensor) : true segmentation masks for each box\n                used if the architecture supports a segmentation task.\n\n        Returns:\n            dict[str, Tensor]: a dictionary of loss components\n        """"""\n        # assign gts and sample proposals\n        if self.with_bbox or self.with_mask:\n            num_imgs = len(img_metas)\n            if gt_bboxes_ignore is None:\n                gt_bboxes_ignore = [None for _ in range(num_imgs)]\n            sampling_results = []\n            for i in range(num_imgs):\n                assign_result = self.bbox_assigner.assign(\n                    proposal_list[i], gt_bboxes[i], gt_bboxes_ignore[i],\n                    gt_labels[i])\n                sampling_result = self.bbox_sampler.sample(\n                    assign_result,\n                    proposal_list[i],\n                    gt_bboxes[i],\n                    gt_labels[i],\n                    feats=[lvl_feat[i][None] for lvl_feat in x])\n                sampling_results.append(sampling_result)\n\n        losses = dict()\n        # bbox head forward and loss\n        if self.with_bbox:\n            bbox_results = self._bbox_forward_train(x, sampling_results,\n                                                    gt_bboxes, gt_labels,\n                                                    img_metas)\n            losses.update(bbox_results[\'loss_bbox\'])\n\n        # mask head forward and loss\n        if self.with_mask:\n            mask_results = self._mask_forward_train(x, sampling_results,\n                                                    bbox_results[\'bbox_feats\'],\n                                                    gt_masks, img_metas)\n            # TODO: Support empty tensor input. #2280\n            if mask_results[\'loss_mask\'] is not None:\n                losses.update(mask_results[\'loss_mask\'])\n\n        return losses\n\n    def _bbox_forward(self, x, rois):\n        # TODO: a more flexible way to decide which feature maps to use\n        bbox_feats = self.bbox_roi_extractor(\n            x[:self.bbox_roi_extractor.num_inputs], rois)\n        if self.with_shared_head:\n            bbox_feats = self.shared_head(bbox_feats)\n        cls_score, bbox_pred = self.bbox_head(bbox_feats)\n\n        bbox_results = dict(\n            cls_score=cls_score, bbox_pred=bbox_pred, bbox_feats=bbox_feats)\n        return bbox_results\n\n    def _bbox_forward_train(self, x, sampling_results, gt_bboxes, gt_labels,\n                            img_metas):\n        rois = bbox2roi([res.bboxes for res in sampling_results])\n        bbox_results = self._bbox_forward(x, rois)\n\n        bbox_targets = self.bbox_head.get_targets(sampling_results, gt_bboxes,\n                                                  gt_labels, self.train_cfg)\n        loss_bbox = self.bbox_head.loss(bbox_results[\'cls_score\'],\n                                        bbox_results[\'bbox_pred\'], rois,\n                                        *bbox_targets)\n\n        bbox_results.update(loss_bbox=loss_bbox)\n        return bbox_results\n\n    def _mask_forward_train(self, x, sampling_results, bbox_feats, gt_masks,\n                            img_metas):\n        if not self.share_roi_extractor:\n            pos_rois = bbox2roi([res.pos_bboxes for res in sampling_results])\n            if pos_rois.shape[0] == 0:\n                return dict(loss_mask=None)\n            mask_results = self._mask_forward(x, pos_rois)\n        else:\n            pos_inds = []\n            device = bbox_feats.device\n            for res in sampling_results:\n                pos_inds.append(\n                    torch.ones(\n                        res.pos_bboxes.shape[0],\n                        device=device,\n                        dtype=torch.uint8))\n                pos_inds.append(\n                    torch.zeros(\n                        res.neg_bboxes.shape[0],\n                        device=device,\n                        dtype=torch.uint8))\n            pos_inds = torch.cat(pos_inds)\n            if pos_inds.shape[0] == 0:\n                return dict(loss_mask=None)\n            mask_results = self._mask_forward(\n                x, pos_inds=pos_inds, bbox_feats=bbox_feats)\n\n        mask_targets = self.mask_head.get_targets(sampling_results, gt_masks,\n                                                  self.train_cfg)\n        pos_labels = torch.cat([res.pos_gt_labels for res in sampling_results])\n        loss_mask = self.mask_head.loss(mask_results[\'mask_pred\'],\n                                        mask_targets, pos_labels)\n\n        mask_results.update(loss_mask=loss_mask, mask_targets=mask_targets)\n        return mask_results\n\n    def _mask_forward(self, x, rois=None, pos_inds=None, bbox_feats=None):\n        assert ((rois is not None) ^\n                (pos_inds is not None and bbox_feats is not None))\n        if rois is not None:\n            mask_feats = self.mask_roi_extractor(\n                x[:self.mask_roi_extractor.num_inputs], rois)\n            if self.with_shared_head:\n                mask_feats = self.shared_head(mask_feats)\n        else:\n            assert bbox_feats is not None\n            mask_feats = bbox_feats[pos_inds]\n\n        mask_pred = self.mask_head(mask_feats)\n        mask_results = dict(mask_pred=mask_pred, mask_feats=mask_feats)\n        return mask_results\n\n    async def async_simple_test(self,\n                                x,\n                                proposal_list,\n                                img_metas,\n                                proposals=None,\n                                rescale=False):\n        """"""Async test without augmentation.""""""\n        assert self.with_bbox, \'Bbox head must be implemented.\'\n\n        det_bboxes, det_labels = await self.async_test_bboxes(\n            x, img_metas, proposal_list, self.test_cfg, rescale=rescale)\n        bbox_results = bbox2result(det_bboxes, det_labels,\n                                   self.bbox_head.num_classes)\n        if not self.with_mask:\n            return bbox_results\n        else:\n            segm_results = await self.async_test_mask(\n                x,\n                img_metas,\n                det_bboxes,\n                det_labels,\n                rescale=rescale,\n                mask_test_cfg=self.test_cfg.get(\'mask\'))\n            return bbox_results, segm_results\n\n    def simple_test(self,\n                    x,\n                    proposal_list,\n                    img_metas,\n                    proposals=None,\n                    rescale=False):\n        """"""Test without augmentation.""""""\n        assert self.with_bbox, \'Bbox head must be implemented.\'\n\n        det_bboxes, det_labels = self.simple_test_bboxes(\n            x, img_metas, proposal_list, self.test_cfg, rescale=rescale)\n        bbox_results = bbox2result(det_bboxes, det_labels,\n                                   self.bbox_head.num_classes)\n\n        if not self.with_mask:\n            return bbox_results\n        else:\n            segm_results = self.simple_test_mask(\n                x, img_metas, det_bboxes, det_labels, rescale=rescale)\n            return bbox_results, segm_results\n\n    def aug_test(self, x, proposal_list, img_metas, rescale=False):\n        """"""Test with augmentations.\n\n        If rescale is False, then returned bboxes and masks will fit the scale\n        of imgs[0].\n        """"""\n        # recompute feats to save memory\n        det_bboxes, det_labels = self.aug_test_bboxes(x, img_metas,\n                                                      proposal_list,\n                                                      self.test_cfg)\n\n        if rescale:\n            _det_bboxes = det_bboxes\n        else:\n            _det_bboxes = det_bboxes.clone()\n            _det_bboxes[:, :4] *= det_bboxes.new_tensor(\n                img_metas[0][0][\'scale_factor\'])\n        bbox_results = bbox2result(_det_bboxes, det_labels,\n                                   self.bbox_head.num_classes)\n\n        # det_bboxes always keep the original scale\n        if self.with_mask:\n            segm_results = self.aug_test_mask(x, img_metas, det_bboxes,\n                                              det_labels)\n            return bbox_results, segm_results\n        else:\n            return bbox_results\n'"
mmdet/models/roi_heads/test_mixins.py,1,"b'import logging\nimport sys\n\nimport torch\n\nfrom mmdet.core import (bbox2roi, bbox_mapping, merge_aug_bboxes,\n                        merge_aug_masks, multiclass_nms)\n\nlogger = logging.getLogger(__name__)\n\nif sys.version_info >= (3, 7):\n    from mmdet.utils.contextmanagers import completed\n\n\nclass BBoxTestMixin(object):\n\n    if sys.version_info >= (3, 7):\n\n        async def async_test_bboxes(self,\n                                    x,\n                                    img_metas,\n                                    proposals,\n                                    rcnn_test_cfg,\n                                    rescale=False,\n                                    bbox_semaphore=None,\n                                    global_lock=None):\n            """"""Async test only det bboxes without augmentation.""""""\n            rois = bbox2roi(proposals)\n            roi_feats = self.bbox_roi_extractor(\n                x[:len(self.bbox_roi_extractor.featmap_strides)], rois)\n            if self.with_shared_head:\n                roi_feats = self.shared_head(roi_feats)\n            sleep_interval = rcnn_test_cfg.get(\'async_sleep_interval\', 0.017)\n\n            async with completed(\n                    __name__, \'bbox_head_forward\',\n                    sleep_interval=sleep_interval):\n                cls_score, bbox_pred = self.bbox_head(roi_feats)\n\n            img_shape = img_metas[0][\'img_shape\']\n            scale_factor = img_metas[0][\'scale_factor\']\n            det_bboxes, det_labels = self.bbox_head.get_bboxes(\n                rois,\n                cls_score,\n                bbox_pred,\n                img_shape,\n                scale_factor,\n                rescale=rescale,\n                cfg=rcnn_test_cfg)\n            return det_bboxes, det_labels\n\n    def simple_test_bboxes(self,\n                           x,\n                           img_metas,\n                           proposals,\n                           rcnn_test_cfg,\n                           rescale=False):\n        """"""Test only det bboxes without augmentation.""""""\n        rois = bbox2roi(proposals)\n        bbox_results = self._bbox_forward(x, rois)\n        img_shape = img_metas[0][\'img_shape\']\n        scale_factor = img_metas[0][\'scale_factor\']\n        det_bboxes, det_labels = self.bbox_head.get_bboxes(\n            rois,\n            bbox_results[\'cls_score\'],\n            bbox_results[\'bbox_pred\'],\n            img_shape,\n            scale_factor,\n            rescale=rescale,\n            cfg=rcnn_test_cfg)\n        return det_bboxes, det_labels\n\n    def aug_test_bboxes(self, feats, img_metas, proposal_list, rcnn_test_cfg):\n        aug_bboxes = []\n        aug_scores = []\n        for x, img_meta in zip(feats, img_metas):\n            # only one image in the batch\n            img_shape = img_meta[0][\'img_shape\']\n            scale_factor = img_meta[0][\'scale_factor\']\n            flip = img_meta[0][\'flip\']\n            flip_direction = img_meta[0][\'flip_direction\']\n            # TODO more flexible\n            proposals = bbox_mapping(proposal_list[0][:, :4], img_shape,\n                                     scale_factor, flip, flip_direction)\n            rois = bbox2roi([proposals])\n            # recompute feature maps to save GPU memory\n            bbox_results = self._bbox_forward(x, rois)\n            bboxes, scores = self.bbox_head.get_bboxes(\n                rois,\n                bbox_results[\'cls_score\'],\n                bbox_results[\'bbox_pred\'],\n                img_shape,\n                scale_factor,\n                rescale=False,\n                cfg=None)\n            aug_bboxes.append(bboxes)\n            aug_scores.append(scores)\n        # after merging, bboxes will be rescaled to the original image size\n        merged_bboxes, merged_scores = merge_aug_bboxes(\n            aug_bboxes, aug_scores, img_metas, rcnn_test_cfg)\n        det_bboxes, det_labels = multiclass_nms(merged_bboxes, merged_scores,\n                                                rcnn_test_cfg.score_thr,\n                                                rcnn_test_cfg.nms,\n                                                rcnn_test_cfg.max_per_img)\n        return det_bboxes, det_labels\n\n\nclass MaskTestMixin(object):\n\n    if sys.version_info >= (3, 7):\n\n        async def async_test_mask(self,\n                                  x,\n                                  img_metas,\n                                  det_bboxes,\n                                  det_labels,\n                                  rescale=False,\n                                  mask_test_cfg=None):\n            # image shape of the first image in the batch (only one)\n            ori_shape = img_metas[0][\'ori_shape\']\n            scale_factor = img_metas[0][\'scale_factor\']\n            if det_bboxes.shape[0] == 0:\n                segm_result = [[] for _ in range(self.mask_head.num_classes)]\n            else:\n                _bboxes = (\n                    det_bboxes[:, :4] *\n                    scale_factor if rescale else det_bboxes)\n                mask_rois = bbox2roi([_bboxes])\n                mask_feats = self.mask_roi_extractor(\n                    x[:len(self.mask_roi_extractor.featmap_strides)],\n                    mask_rois)\n\n                if self.with_shared_head:\n                    mask_feats = self.shared_head(mask_feats)\n                if mask_test_cfg and mask_test_cfg.get(\'async_sleep_interval\'):\n                    sleep_interval = mask_test_cfg[\'async_sleep_interval\']\n                else:\n                    sleep_interval = 0.035\n                async with completed(\n                        __name__,\n                        \'mask_head_forward\',\n                        sleep_interval=sleep_interval):\n                    mask_pred = self.mask_head(mask_feats)\n                segm_result = self.mask_head.get_seg_masks(\n                    mask_pred, _bboxes, det_labels, self.test_cfg, ori_shape,\n                    scale_factor, rescale)\n            return segm_result\n\n    def simple_test_mask(self,\n                         x,\n                         img_metas,\n                         det_bboxes,\n                         det_labels,\n                         rescale=False):\n        # image shape of the first image in the batch (only one)\n        ori_shape = img_metas[0][\'ori_shape\']\n        scale_factor = img_metas[0][\'scale_factor\']\n        if det_bboxes.shape[0] == 0:\n            segm_result = [[] for _ in range(self.mask_head.num_classes)]\n        else:\n            # if det_bboxes is rescaled to the original image size, we need to\n            # rescale it back to the testing scale to obtain RoIs.\n            if rescale and not isinstance(scale_factor, float):\n                scale_factor = torch.from_numpy(scale_factor).to(\n                    det_bboxes.device)\n            _bboxes = (\n                det_bboxes[:, :4] * scale_factor if rescale else det_bboxes)\n            mask_rois = bbox2roi([_bboxes])\n            mask_results = self._mask_forward(x, mask_rois)\n            segm_result = self.mask_head.get_seg_masks(\n                mask_results[\'mask_pred\'], _bboxes, det_labels, self.test_cfg,\n                ori_shape, scale_factor, rescale)\n        return segm_result\n\n    def aug_test_mask(self, feats, img_metas, det_bboxes, det_labels):\n        if det_bboxes.shape[0] == 0:\n            segm_result = [[] for _ in range(self.mask_head.num_classes)]\n        else:\n            aug_masks = []\n            for x, img_meta in zip(feats, img_metas):\n                img_shape = img_meta[0][\'img_shape\']\n                scale_factor = img_meta[0][\'scale_factor\']\n                flip = img_meta[0][\'flip\']\n                flip_direction = img_meta[0][\'flip_direction\']\n                _bboxes = bbox_mapping(det_bboxes[:, :4], img_shape,\n                                       scale_factor, flip, flip_direction)\n                mask_rois = bbox2roi([_bboxes])\n                mask_results = self._mask_forward(x, mask_rois)\n                # convert to numpy array to save memory\n                aug_masks.append(\n                    mask_results[\'mask_pred\'].sigmoid().cpu().numpy())\n            merged_masks = merge_aug_masks(aug_masks, img_metas, self.test_cfg)\n\n            ori_shape = img_metas[0][0][\'ori_shape\']\n            segm_result = self.mask_head.get_seg_masks(\n                merged_masks,\n                det_bboxes,\n                det_labels,\n                self.test_cfg,\n                ori_shape,\n                scale_factor=1.0,\n                rescale=False)\n        return segm_result\n'"
mmdet/models/utils/__init__.py,0,"b""from .res_layer import ResLayer\n\n__all__ = ['ResLayer']\n"""
mmdet/models/utils/res_layer.py,0,"b'from mmcv.cnn import build_conv_layer, build_norm_layer\nfrom torch import nn as nn\n\n\nclass ResLayer(nn.Sequential):\n    """"""ResLayer to build ResNet style backbone.\n\n    Args:\n        block (nn.Module): block used to build ResLayer.\n        inplanes (int): inplanes of block.\n        planes (int): planes of block.\n        num_blocks (int): number of blocks.\n        stride (int): stride of the first block. Default: 1\n        avg_down (bool): Use AvgPool instead of stride conv when\n            downsampling in the bottleneck. Default: False\n        conv_cfg (dict): dictionary to construct and config conv layer.\n            Default: None\n        norm_cfg (dict): dictionary to construct and config norm layer.\n            Default: dict(type=\'BN\')\n    """"""\n\n    def __init__(self,\n                 block,\n                 inplanes,\n                 planes,\n                 num_blocks,\n                 stride=1,\n                 avg_down=False,\n                 conv_cfg=None,\n                 norm_cfg=dict(type=\'BN\'),\n                 **kwargs):\n        self.block = block\n\n        downsample = None\n        if stride != 1 or inplanes != planes * block.expansion:\n            downsample = []\n            conv_stride = stride\n            if avg_down and stride != 1:\n                conv_stride = 1\n                downsample.append(\n                    nn.AvgPool2d(\n                        kernel_size=stride,\n                        stride=stride,\n                        ceil_mode=True,\n                        count_include_pad=False))\n            downsample.extend([\n                build_conv_layer(\n                    conv_cfg,\n                    inplanes,\n                    planes * block.expansion,\n                    kernel_size=1,\n                    stride=conv_stride,\n                    bias=False),\n                build_norm_layer(norm_cfg, planes * block.expansion)[1]\n            ])\n            downsample = nn.Sequential(*downsample)\n\n        layers = []\n        layers.append(\n            block(\n                inplanes=inplanes,\n                planes=planes,\n                stride=stride,\n                downsample=downsample,\n                conv_cfg=conv_cfg,\n                norm_cfg=norm_cfg,\n                **kwargs))\n        inplanes = planes * block.expansion\n        for i in range(1, num_blocks):\n            layers.append(\n                block(\n                    inplanes=inplanes,\n                    planes=planes,\n                    stride=1,\n                    conv_cfg=conv_cfg,\n                    norm_cfg=norm_cfg,\n                    **kwargs))\n        super(ResLayer, self).__init__(*layers)\n'"
mmdet/ops/carafe/__init__.py,0,"b""from .carafe import CARAFE, CARAFENaive, CARAFEPack, carafe, carafe_naive\n\n__all__ = ['carafe', 'carafe_naive', 'CARAFE', 'CARAFENaive', 'CARAFEPack']\n"""
mmdet/ops/carafe/carafe.py,12,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import UPSAMPLE_LAYERS, normal_init, xavier_init\nfrom torch.autograd import Function\nfrom torch.nn.modules.module import Module\n\nfrom . import carafe_ext, carafe_naive_ext\n\n\nclass CARAFENaiveFunction(Function):\n\n    @staticmethod\n    def forward(ctx, features, masks, kernel_size, group_size, scale_factor):\n        assert scale_factor >= 1\n        assert masks.size(1) == kernel_size * kernel_size * group_size\n        assert masks.size(-1) == features.size(-1) * scale_factor\n        assert masks.size(-2) == features.size(-2) * scale_factor\n        assert features.size(1) % group_size == 0\n        assert (kernel_size - 1) % 2 == 0 and kernel_size >= 1\n        ctx.kernel_size = kernel_size\n        ctx.group_size = group_size\n        ctx.scale_factor = scale_factor\n        ctx.feature_size = features.size()\n        ctx.mask_size = masks.size()\n\n        n, c, h, w = features.size()\n        output = features.new_zeros((n, c, h * scale_factor, w * scale_factor))\n        if features.is_cuda:\n            carafe_naive_ext.forward(features, masks, kernel_size, group_size,\n                                     scale_factor, output)\n        else:\n            raise NotImplementedError\n\n        if features.requires_grad or masks.requires_grad:\n            ctx.save_for_backward(features, masks)\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        assert grad_output.is_cuda\n\n        features, masks = ctx.saved_tensors\n        kernel_size = ctx.kernel_size\n        group_size = ctx.group_size\n        scale_factor = ctx.scale_factor\n\n        grad_input = torch.zeros_like(features)\n        grad_masks = torch.zeros_like(masks)\n        carafe_naive_ext.backward(grad_output.contiguous(), features, masks,\n                                  kernel_size, group_size, scale_factor,\n                                  grad_input, grad_masks)\n\n        return grad_input, grad_masks, None, None, None\n\n\ncarafe_naive = CARAFENaiveFunction.apply\n\n\nclass CARAFENaive(Module):\n\n    def __init__(self, kernel_size, group_size, scale_factor):\n        super(CARAFENaive, self).__init__()\n\n        assert isinstance(kernel_size, int) and isinstance(\n            group_size, int) and isinstance(scale_factor, int)\n        self.kernel_size = kernel_size\n        self.group_size = group_size\n        self.scale_factor = scale_factor\n\n    def forward(self, features, masks):\n        return CARAFENaiveFunction.apply(features, masks, self.kernel_size,\n                                         self.group_size, self.scale_factor)\n\n\nclass CARAFEFunction(Function):\n\n    @staticmethod\n    def forward(ctx, features, masks, kernel_size, group_size, scale_factor):\n        assert scale_factor >= 1\n        assert masks.size(1) == kernel_size * kernel_size * group_size\n        assert masks.size(-1) == features.size(-1) * scale_factor\n        assert masks.size(-2) == features.size(-2) * scale_factor\n        assert features.size(1) % group_size == 0\n        assert (kernel_size - 1) % 2 == 0 and kernel_size >= 1\n        ctx.kernel_size = kernel_size\n        ctx.group_size = group_size\n        ctx.scale_factor = scale_factor\n        ctx.feature_size = features.size()\n        ctx.mask_size = masks.size()\n\n        n, c, h, w = features.size()\n        output = features.new_zeros((n, c, h * scale_factor, w * scale_factor))\n        routput = features.new_zeros(output.size(), requires_grad=False)\n        rfeatures = features.new_zeros(features.size(), requires_grad=False)\n        rmasks = masks.new_zeros(masks.size(), requires_grad=False)\n        if features.is_cuda:\n            carafe_ext.forward(features, rfeatures, masks, rmasks, kernel_size,\n                               group_size, scale_factor, routput, output)\n        else:\n            raise NotImplementedError\n\n        if features.requires_grad or masks.requires_grad:\n            ctx.save_for_backward(features, masks, rfeatures)\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        assert grad_output.is_cuda\n\n        features, masks, rfeatures = ctx.saved_tensors\n        kernel_size = ctx.kernel_size\n        group_size = ctx.group_size\n        scale_factor = ctx.scale_factor\n\n        rgrad_output = torch.zeros_like(grad_output, requires_grad=False)\n        rgrad_input_hs = torch.zeros_like(grad_output, requires_grad=False)\n        rgrad_input = torch.zeros_like(features, requires_grad=False)\n        rgrad_masks = torch.zeros_like(masks, requires_grad=False)\n        grad_input = torch.zeros_like(features, requires_grad=False)\n        grad_masks = torch.zeros_like(masks, requires_grad=False)\n        carafe_ext.backward(grad_output.contiguous(), rfeatures, masks,\n                            kernel_size, group_size, scale_factor,\n                            rgrad_output, rgrad_input_hs, rgrad_input,\n                            rgrad_masks, grad_input, grad_masks)\n        return grad_input, grad_masks, None, None, None, None\n\n\ncarafe = CARAFEFunction.apply\n\n\nclass CARAFE(Module):\n    """""" CARAFE: Content-Aware ReAssembly of FEatures\n\n    Please refer to https://arxiv.org/abs/1905.02188 for more details.\n\n    Args:\n        kernel_size (int): reassemble kernel size\n        group_size (int): reassemble group size\n        scale_factor (int): upsample ratio\n\n    Returns:\n        upsampled feature map\n    """"""\n\n    def __init__(self, kernel_size, group_size, scale_factor):\n        super(CARAFE, self).__init__()\n\n        assert isinstance(kernel_size, int) and isinstance(\n            group_size, int) and isinstance(scale_factor, int)\n        self.kernel_size = kernel_size\n        self.group_size = group_size\n        self.scale_factor = scale_factor\n\n    def forward(self, features, masks):\n        return CARAFEFunction.apply(features, masks, self.kernel_size,\n                                    self.group_size, self.scale_factor)\n\n\n@UPSAMPLE_LAYERS.register_module(name=\'carafe\')\nclass CARAFEPack(nn.Module):\n    """""" A unified package of CARAFE upsampler that contains:\n    1) channel compressor 2) content encoder 3) CARAFE op\n\n    Official implementation of ICCV 2019 paper\n    CARAFE: Content-Aware ReAssembly of FEatures\n    Please refer to https://arxiv.org/abs/1905.02188 for more details.\n\n    Args:\n        channels (int): input feature channels\n        scale_factor (int): upsample ratio\n        up_kernel (int): kernel size of CARAFE op\n        up_group (int): group size of CARAFE op\n        encoder_kernel (int): kernel size of content encoder\n        encoder_dilation (int): dilation of content encoder\n        compressed_channels (int): output channels of channels compressor\n\n    Returns:\n        upsampled feature map\n    """"""\n\n    def __init__(self,\n                 channels,\n                 scale_factor,\n                 up_kernel=5,\n                 up_group=1,\n                 encoder_kernel=3,\n                 encoder_dilation=1,\n                 compressed_channels=64):\n        super(CARAFEPack, self).__init__()\n        self.channels = channels\n        self.scale_factor = scale_factor\n        self.up_kernel = up_kernel\n        self.up_group = up_group\n        self.encoder_kernel = encoder_kernel\n        self.encoder_dilation = encoder_dilation\n        self.compressed_channels = compressed_channels\n        self.channel_compressor = nn.Conv2d(channels, self.compressed_channels,\n                                            1)\n        self.content_encoder = nn.Conv2d(\n            self.compressed_channels,\n            self.up_kernel * self.up_kernel * self.up_group *\n            self.scale_factor * self.scale_factor,\n            self.encoder_kernel,\n            padding=int((self.encoder_kernel - 1) * self.encoder_dilation / 2),\n            dilation=self.encoder_dilation,\n            groups=1)\n        self.init_weights()\n\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                xavier_init(m, distribution=\'uniform\')\n        normal_init(self.content_encoder, std=0.001)\n\n    def kernel_normalizer(self, mask):\n        mask = F.pixel_shuffle(mask, self.scale_factor)\n        n, mask_c, h, w = mask.size()\n        mask_channel = int(mask_c / (self.up_kernel * self.up_kernel))\n        mask = mask.view(n, mask_channel, -1, h, w)\n\n        mask = F.softmax(mask, dim=2)\n        mask = mask.view(n, mask_c, h, w).contiguous()\n\n        return mask\n\n    def feature_reassemble(self, x, mask):\n        x = carafe(x, mask, self.up_kernel, self.up_group, self.scale_factor)\n        return x\n\n    def forward(self, x):\n        compressed_x = self.channel_compressor(x)\n        mask = self.content_encoder(compressed_x)\n        mask = self.kernel_normalizer(mask)\n\n        x = self.feature_reassemble(x, mask)\n        return x\n'"
mmdet/ops/carafe/grad_check.py,9,"b""import os.path as osp\nimport sys\n\nimport mmcv\nimport torch\nfrom torch.autograd import gradcheck\n\nsys.path.append(osp.abspath(osp.join(__file__, '../../')))\nfrom mmdet.ops.carafe import CARAFE, CARAFENaive  # noqa: E402, isort:skip\nfrom mmdet.ops.carafe import carafe, carafe_naive  # noqa: E402, isort:skip\n\nfeat = torch.randn(2, 64, 3, 3, requires_grad=True, device='cuda:0').double()\nmask = torch.randn(\n    2, 100, 6, 6, requires_grad=True, device='cuda:0').sigmoid().double()\n\nprint('Gradcheck for carafe...')\ntest = gradcheck(CARAFE(5, 4, 2), (feat, mask), atol=1e-4, eps=1e-4)\nprint(test)\n\nprint('Gradcheck for carafe naive...')\ntest = gradcheck(CARAFENaive(5, 4, 2), (feat, mask), atol=1e-4, eps=1e-4)\nprint(test)\n\nfeat = torch.randn(\n    2, 1024, 100, 100, requires_grad=True, device='cuda:0').float()\nmask = torch.randn(\n    2, 25, 200, 200, requires_grad=True, device='cuda:0').sigmoid().float()\nloop_num = 500\n\ntime_forward = 0\ntime_backward = 0\nbar = mmcv.ProgressBar(loop_num)\ntimer = mmcv.Timer()\nfor i in range(loop_num):\n    x = carafe(feat.clone(), mask.clone(), 5, 1, 2)\n    torch.cuda.synchronize()\n    time_forward += timer.since_last_check()\n    x.sum().backward(retain_graph=True)\n    torch.cuda.synchronize()\n    time_backward += timer.since_last_check()\n    bar.update()\nforward_speed = (time_forward + 1e-3) * 1e3 / loop_num\nbackward_speed = (time_backward + 1e-3) * 1e3 / loop_num\nprint(f'\\nCARAFE time forward: {forward_speed} '\n      f'ms/iter | time backward: {backward_speed} ms/iter')\n\ntime_naive_forward = 0\ntime_naive_backward = 0\nbar = mmcv.ProgressBar(loop_num)\ntimer = mmcv.Timer()\nfor i in range(loop_num):\n    x = carafe_naive(feat.clone(), mask.clone(), 5, 1, 2)\n    torch.cuda.synchronize()\n    time_naive_forward += timer.since_last_check()\n    x.sum().backward(retain_graph=True)\n    torch.cuda.synchronize()\n    time_naive_backward += timer.since_last_check()\n    bar.update()\nforward_speed = (time_naive_forward + 1e-3) * 1e3 / loop_num\nbackward_speed = (time_naive_backward + 1e-3) * 1e3 / loop_num\nprint('\\nCARAFE naive time forward: '\n      f'{forward_speed} ms/iter | time backward: {backward_speed} ms/iter')\n"""
mmdet/ops/carafe/setup.py,1,"b""from setuptools import setup\n\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\nNVCC_ARGS = [\n    '-D__CUDA_NO_HALF_OPERATORS__',\n    '-D__CUDA_NO_HALF_CONVERSIONS__',\n    '-D__CUDA_NO_HALF2_OPERATORS__',\n]\n\nsetup(\n    name='carafe',\n    ext_modules=[\n        CUDAExtension(\n            'carafe_ext', [\n                'src/cuda/carafe_cuda.cpp', 'src/cuda/carafe_cuda_kernel.cu',\n                'src/carafe_ext.cpp'\n            ],\n            define_macros=[('WITH_CUDA', None)],\n            extra_compile_args={\n                'cxx': [],\n                'nvcc': NVCC_ARGS\n            }),\n        CUDAExtension(\n            'carafe_naive_ext', [\n                'src/cuda/carafe_naive_cuda.cpp',\n                'src/cuda/carafe_naive_cuda_kernel.cu',\n                'src/carafe_naive_ext.cpp'\n            ],\n            define_macros=[('WITH_CUDA', None)],\n            extra_compile_args={\n                'cxx': [],\n                'nvcc': NVCC_ARGS\n            })\n    ],\n    cmdclass={'build_ext': BuildExtension})\n"""
mmdet/ops/dcn/__init__.py,0,"b""from .deform_conv import (DeformConv, DeformConvPack, ModulatedDeformConv,\n                          ModulatedDeformConvPack, deform_conv,\n                          modulated_deform_conv)\nfrom .deform_pool import (DeformRoIPooling, DeformRoIPoolingPack,\n                          ModulatedDeformRoIPoolingPack, deform_roi_pooling)\n\n__all__ = [\n    'DeformConv', 'DeformConvPack', 'ModulatedDeformConv',\n    'ModulatedDeformConvPack', 'DeformRoIPooling', 'DeformRoIPoolingPack',\n    'ModulatedDeformRoIPoolingPack', 'deform_conv', 'modulated_deform_conv',\n    'deform_roi_pooling'\n]\n"""
mmdet/ops/dcn/deform_conv.py,19,"b'import math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import CONV_LAYERS\nfrom mmcv.utils import print_log\nfrom torch.autograd import Function\nfrom torch.autograd.function import once_differentiable\nfrom torch.nn.modules.utils import _pair, _single\n\nfrom . import deform_conv_ext\n\n\nclass DeformConvFunction(Function):\n\n    @staticmethod\n    def forward(ctx,\n                input,\n                offset,\n                weight,\n                stride=1,\n                padding=0,\n                dilation=1,\n                groups=1,\n                deformable_groups=1,\n                im2col_step=64):\n        if input is not None and input.dim() != 4:\n            raise ValueError(f\'Expected 4D tensor as input, got {input.dim()}\'\n                             \'D tensor instead.\')\n        ctx.stride = _pair(stride)\n        ctx.padding = _pair(padding)\n        ctx.dilation = _pair(dilation)\n        ctx.groups = groups\n        ctx.deformable_groups = deformable_groups\n        ctx.im2col_step = im2col_step\n\n        ctx.save_for_backward(input, offset, weight)\n\n        output = input.new_empty(\n            DeformConvFunction._output_size(input, weight, ctx.padding,\n                                            ctx.dilation, ctx.stride))\n\n        ctx.bufs_ = [input.new_empty(0), input.new_empty(0)]  # columns, ones\n\n        if not input.is_cuda:\n            raise NotImplementedError\n        else:\n            cur_im2col_step = min(ctx.im2col_step, input.shape[0])\n            assert (input.shape[0] %\n                    cur_im2col_step) == 0, \'im2col step must divide batchsize\'\n            deform_conv_ext.deform_conv_forward(\n                input, weight, offset, output, ctx.bufs_[0], ctx.bufs_[1],\n                weight.size(3), weight.size(2), ctx.stride[1], ctx.stride[0],\n                ctx.padding[1], ctx.padding[0], ctx.dilation[1],\n                ctx.dilation[0], ctx.groups, ctx.deformable_groups,\n                cur_im2col_step)\n        return output\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_output):\n        input, offset, weight = ctx.saved_tensors\n\n        grad_input = grad_offset = grad_weight = None\n\n        if not grad_output.is_cuda:\n            raise NotImplementedError\n        else:\n            cur_im2col_step = min(ctx.im2col_step, input.shape[0])\n            assert (input.shape[0] %\n                    cur_im2col_step) == 0, \'im2col step must divide batchsize\'\n\n            if ctx.needs_input_grad[0] or ctx.needs_input_grad[1]:\n                grad_input = torch.zeros_like(input)\n                grad_offset = torch.zeros_like(offset)\n                deform_conv_ext.deform_conv_backward_input(\n                    input, offset, grad_output, grad_input,\n                    grad_offset, weight, ctx.bufs_[0], weight.size(3),\n                    weight.size(2), ctx.stride[1], ctx.stride[0],\n                    ctx.padding[1], ctx.padding[0], ctx.dilation[1],\n                    ctx.dilation[0], ctx.groups, ctx.deformable_groups,\n                    cur_im2col_step)\n\n            if ctx.needs_input_grad[2]:\n                grad_weight = torch.zeros_like(weight)\n                deform_conv_ext.deform_conv_backward_parameters(\n                    input, offset, grad_output,\n                    grad_weight, ctx.bufs_[0], ctx.bufs_[1], weight.size(3),\n                    weight.size(2), ctx.stride[1], ctx.stride[0],\n                    ctx.padding[1], ctx.padding[0], ctx.dilation[1],\n                    ctx.dilation[0], ctx.groups, ctx.deformable_groups, 1,\n                    cur_im2col_step)\n\n        return (grad_input, grad_offset, grad_weight, None, None, None, None,\n                None)\n\n    @staticmethod\n    def _output_size(input, weight, padding, dilation, stride):\n        channels = weight.size(0)\n        output_size = (input.size(0), channels)\n        for d in range(input.dim() - 2):\n            in_size = input.size(d + 2)\n            pad = padding[d]\n            kernel = dilation[d] * (weight.size(d + 2) - 1) + 1\n            stride_ = stride[d]\n            output_size += ((in_size + (2 * pad) - kernel) // stride_ + 1, )\n        if not all(map(lambda s: s > 0, output_size)):\n            raise ValueError(\'convolution input is too small (output would be \'\n                             f\'{""x"".join(map(str, output_size))})\')\n        return output_size\n\n\nclass ModulatedDeformConvFunction(Function):\n\n    @staticmethod\n    def forward(ctx,\n                input,\n                offset,\n                mask,\n                weight,\n                bias=None,\n                stride=1,\n                padding=0,\n                dilation=1,\n                groups=1,\n                deformable_groups=1):\n        ctx.stride = stride\n        ctx.padding = padding\n        ctx.dilation = dilation\n        ctx.groups = groups\n        ctx.deformable_groups = deformable_groups\n        ctx.with_bias = bias is not None\n        if not ctx.with_bias:\n            bias = input.new_empty(1)  # fake tensor\n        if not input.is_cuda:\n            raise NotImplementedError\n        if weight.requires_grad or mask.requires_grad or offset.requires_grad \\\n                or input.requires_grad:\n            ctx.save_for_backward(input, offset, mask, weight, bias)\n        output = input.new_empty(\n            ModulatedDeformConvFunction._infer_shape(ctx, input, weight))\n        ctx._bufs = [input.new_empty(0), input.new_empty(0)]\n        deform_conv_ext.modulated_deform_conv_forward(\n            input, weight, bias, ctx._bufs[0], offset, mask, output,\n            ctx._bufs[1], weight.shape[2], weight.shape[3], ctx.stride,\n            ctx.stride, ctx.padding, ctx.padding, ctx.dilation, ctx.dilation,\n            ctx.groups, ctx.deformable_groups, ctx.with_bias)\n        return output\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_output):\n        if not grad_output.is_cuda:\n            raise NotImplementedError\n        input, offset, mask, weight, bias = ctx.saved_tensors\n        grad_input = torch.zeros_like(input)\n        grad_offset = torch.zeros_like(offset)\n        grad_mask = torch.zeros_like(mask)\n        grad_weight = torch.zeros_like(weight)\n        grad_bias = torch.zeros_like(bias)\n        deform_conv_ext.modulated_deform_conv_backward(\n            input, weight, bias, ctx._bufs[0], offset, mask, ctx._bufs[1],\n            grad_input, grad_weight, grad_bias, grad_offset, grad_mask,\n            grad_output, weight.shape[2], weight.shape[3], ctx.stride,\n            ctx.stride, ctx.padding, ctx.padding, ctx.dilation, ctx.dilation,\n            ctx.groups, ctx.deformable_groups, ctx.with_bias)\n        if not ctx.with_bias:\n            grad_bias = None\n\n        return (grad_input, grad_offset, grad_mask, grad_weight, grad_bias,\n                None, None, None, None, None)\n\n    @staticmethod\n    def _infer_shape(ctx, input, weight):\n        n = input.size(0)\n        channels_out = weight.size(0)\n        height, width = input.shape[2:4]\n        kernel_h, kernel_w = weight.shape[2:4]\n        height_out = (height + 2 * ctx.padding -\n                      (ctx.dilation * (kernel_h - 1) + 1)) // ctx.stride + 1\n        width_out = (width + 2 * ctx.padding -\n                     (ctx.dilation * (kernel_w - 1) + 1)) // ctx.stride + 1\n        return n, channels_out, height_out, width_out\n\n\ndeform_conv = DeformConvFunction.apply\nmodulated_deform_conv = ModulatedDeformConvFunction.apply\n\n\nclass DeformConv(nn.Module):\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 groups=1,\n                 deformable_groups=1,\n                 bias=False):\n        super(DeformConv, self).__init__()\n\n        assert not bias\n        assert in_channels % groups == 0, \\\n            f\'in_channels {in_channels} is not divisible by groups {groups}\'\n        assert out_channels % groups == 0, \\\n            f\'out_channels {out_channels} is not divisible \' \\\n            f\'by groups {groups}\'\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = _pair(kernel_size)\n        self.stride = _pair(stride)\n        self.padding = _pair(padding)\n        self.dilation = _pair(dilation)\n        self.groups = groups\n        self.deformable_groups = deformable_groups\n        # enable compatibility with nn.Conv2d\n        self.transposed = False\n        self.output_padding = _single(0)\n\n        self.weight = nn.Parameter(\n            torch.Tensor(out_channels, in_channels // self.groups,\n                         *self.kernel_size))\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        n = self.in_channels\n        for k in self.kernel_size:\n            n *= k\n        stdv = 1. / math.sqrt(n)\n        self.weight.data.uniform_(-stdv, stdv)\n\n    def forward(self, x, offset):\n        # To fix an assert error in deform_conv_cuda.cpp:128\n        # input image is smaller than kernel\n        input_pad = (\n            x.size(2) < self.kernel_size[0] or x.size(3) < self.kernel_size[1])\n        if input_pad:\n            pad_h = max(self.kernel_size[0] - x.size(2), 0)\n            pad_w = max(self.kernel_size[1] - x.size(3), 0)\n            x = F.pad(x, (0, pad_w, 0, pad_h), \'constant\', 0).contiguous()\n            offset = F.pad(offset, (0, pad_w, 0, pad_h), \'constant\',\n                           0).contiguous()\n        out = deform_conv(x, offset, self.weight, self.stride, self.padding,\n                          self.dilation, self.groups, self.deformable_groups)\n        if input_pad:\n            out = out[:, :, :out.size(2) - pad_h, :out.size(3) -\n                      pad_w].contiguous()\n        return out\n\n\n@CONV_LAYERS.register_module(\'DCN\')\nclass DeformConvPack(DeformConv):\n    """"""A Deformable Conv Encapsulation that acts as normal Conv layers.\n\n    The offset tensor is like `[y0, x0, y1, x1, y2, x2, ..., y8, x8]`.\n    The spatial arrangement is like:\n    ```\n    (x0, y0) (x1, y1) (x2, y2)\n    (x3, y3) (x4, y4) (x5, y5)\n    (x6, y6) (x7, y7) (x8, y8)\n    ```\n\n    Args:\n        in_channels (int): Same as nn.Conv2d.\n        out_channels (int): Same as nn.Conv2d.\n        kernel_size (int or tuple[int]): Same as nn.Conv2d.\n        stride (int or tuple[int]): Same as nn.Conv2d.\n        padding (int or tuple[int]): Same as nn.Conv2d.\n        dilation (int or tuple[int]): Same as nn.Conv2d.\n        groups (int): Same as nn.Conv2d.\n        bias (bool or str): If specified as `auto`, it will be decided by the\n            norm_cfg. Bias will be set as True if norm_cfg is None, otherwise\n            False.\n    """"""\n\n    _version = 2\n\n    def __init__(self, *args, **kwargs):\n        super(DeformConvPack, self).__init__(*args, **kwargs)\n\n        self.conv_offset = nn.Conv2d(\n            self.in_channels,\n            self.deformable_groups * 2 * self.kernel_size[0] *\n            self.kernel_size[1],\n            kernel_size=self.kernel_size,\n            stride=_pair(self.stride),\n            padding=_pair(self.padding),\n            dilation=_pair(self.dilation),\n            bias=True)\n        self.init_offset()\n\n    def init_offset(self):\n        self.conv_offset.weight.data.zero_()\n        self.conv_offset.bias.data.zero_()\n\n    def forward(self, x):\n        offset = self.conv_offset(x)\n        return deform_conv(x, offset, self.weight, self.stride, self.padding,\n                           self.dilation, self.groups, self.deformable_groups)\n\n    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n                              missing_keys, unexpected_keys, error_msgs):\n        version = local_metadata.get(\'version\', None)\n\n        if version is None or version < 2:\n            # the key is different in early versions\n            # In version < 2, DeformConvPack loads previous benchmark models.\n            if (prefix + \'conv_offset.weight\' not in state_dict\n                    and prefix[:-1] + \'_offset.weight\' in state_dict):\n                state_dict[prefix + \'conv_offset.weight\'] = state_dict.pop(\n                    prefix[:-1] + \'_offset.weight\')\n            if (prefix + \'conv_offset.bias\' not in state_dict\n                    and prefix[:-1] + \'_offset.bias\' in state_dict):\n                state_dict[prefix +\n                           \'conv_offset.bias\'] = state_dict.pop(prefix[:-1] +\n                                                                \'_offset.bias\')\n\n        if version is not None and version > 1:\n            print_log(\n                f\'DeformConvPack {prefix.rstrip(""."")} is upgraded to \'\n                \'version 2.\',\n                logger=\'root\')\n\n        super()._load_from_state_dict(state_dict, prefix, local_metadata,\n                                      strict, missing_keys, unexpected_keys,\n                                      error_msgs)\n\n\nclass ModulatedDeformConv(nn.Module):\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 groups=1,\n                 deformable_groups=1,\n                 bias=True):\n        super(ModulatedDeformConv, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = _pair(kernel_size)\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n        self.groups = groups\n        self.deformable_groups = deformable_groups\n        self.with_bias = bias\n        # enable compatibility with nn.Conv2d\n        self.transposed = False\n        self.output_padding = _single(0)\n\n        self.weight = nn.Parameter(\n            torch.Tensor(out_channels, in_channels // groups,\n                         *self.kernel_size))\n        if bias:\n            self.bias = nn.Parameter(torch.Tensor(out_channels))\n        else:\n            self.register_parameter(\'bias\', None)\n        self.init_weights()\n\n    def init_weights(self):\n        n = self.in_channels\n        for k in self.kernel_size:\n            n *= k\n        stdv = 1. / math.sqrt(n)\n        self.weight.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.zero_()\n\n    def forward(self, x, offset, mask):\n        return modulated_deform_conv(x, offset, mask, self.weight, self.bias,\n                                     self.stride, self.padding, self.dilation,\n                                     self.groups, self.deformable_groups)\n\n\n@CONV_LAYERS.register_module(\'DCNv2\')\nclass ModulatedDeformConvPack(ModulatedDeformConv):\n    """"""A ModulatedDeformable Conv Encapsulation that acts as normal Conv layers.\n\n    Args:\n        in_channels (int): Same as nn.Conv2d.\n        out_channels (int): Same as nn.Conv2d.\n        kernel_size (int or tuple[int]): Same as nn.Conv2d.\n        stride (int or tuple[int]): Same as nn.Conv2d.\n        padding (int or tuple[int]): Same as nn.Conv2d.\n        dilation (int or tuple[int]): Same as nn.Conv2d.\n        groups (int): Same as nn.Conv2d.\n        bias (bool or str): If specified as `auto`, it will be decided by the\n            norm_cfg. Bias will be set as True if norm_cfg is None, otherwise\n            False.\n    """"""\n\n    _version = 2\n\n    def __init__(self, *args, **kwargs):\n        super(ModulatedDeformConvPack, self).__init__(*args, **kwargs)\n\n        self.conv_offset = nn.Conv2d(\n            self.in_channels,\n            self.deformable_groups * 3 * self.kernel_size[0] *\n            self.kernel_size[1],\n            kernel_size=self.kernel_size,\n            stride=_pair(self.stride),\n            padding=_pair(self.padding),\n            dilation=_pair(self.dilation),\n            bias=True)\n        self.init_weights()\n\n    def init_weights(self):\n        super(ModulatedDeformConvPack, self).init_weights()\n        if hasattr(self, \'conv_offset\'):\n            self.conv_offset.weight.data.zero_()\n            self.conv_offset.bias.data.zero_()\n\n    def forward(self, x):\n        out = self.conv_offset(x)\n        o1, o2, mask = torch.chunk(out, 3, dim=1)\n        offset = torch.cat((o1, o2), dim=1)\n        mask = torch.sigmoid(mask)\n        return modulated_deform_conv(x, offset, mask, self.weight, self.bias,\n                                     self.stride, self.padding, self.dilation,\n                                     self.groups, self.deformable_groups)\n\n    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n                              missing_keys, unexpected_keys, error_msgs):\n        version = local_metadata.get(\'version\', None)\n\n        if version is None or version < 2:\n            # the key is different in early versions\n            # In version < 2, ModulatedDeformConvPack\n            # loads previous benchmark models.\n            if (prefix + \'conv_offset.weight\' not in state_dict\n                    and prefix[:-1] + \'_offset.weight\' in state_dict):\n                state_dict[prefix + \'conv_offset.weight\'] = state_dict.pop(\n                    prefix[:-1] + \'_offset.weight\')\n            if (prefix + \'conv_offset.bias\' not in state_dict\n                    and prefix[:-1] + \'_offset.bias\' in state_dict):\n                state_dict[prefix +\n                           \'conv_offset.bias\'] = state_dict.pop(prefix[:-1] +\n                                                                \'_offset.bias\')\n\n        if version is not None and version > 1:\n            print_log(\n                f\'ModulatedDeformConvPack {prefix.rstrip(""."")} is upgraded to \'\n                \'version 2.\',\n                logger=\'root\')\n\n        super()._load_from_state_dict(state_dict, prefix, local_metadata,\n                                      strict, missing_keys, unexpected_keys,\n                                      error_msgs)\n'"
mmdet/ops/dcn/deform_pool.py,6,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd import Function\nfrom torch.autograd.function import once_differentiable\nfrom torch.nn.modules.utils import _pair\n\nfrom . import deform_pool_ext\n\n\nclass DeformRoIPoolingFunction(Function):\n\n    @staticmethod\n    def forward(ctx,\n                data,\n                rois,\n                offset,\n                spatial_scale,\n                out_size,\n                out_channels,\n                no_trans,\n                group_size=1,\n                part_size=None,\n                sample_per_part=4,\n                trans_std=.0):\n        # TODO: support unsquare RoIs\n        out_h, out_w = _pair(out_size)\n        assert isinstance(out_h, int) and isinstance(out_w, int)\n        assert out_h == out_w\n        out_size = out_h  # out_h and out_w must be equal\n\n        ctx.spatial_scale = spatial_scale\n        ctx.out_size = out_size\n        ctx.out_channels = out_channels\n        ctx.no_trans = no_trans\n        ctx.group_size = group_size\n        ctx.part_size = out_size if part_size is None else part_size\n        ctx.sample_per_part = sample_per_part\n        ctx.trans_std = trans_std\n\n        assert 0.0 <= ctx.trans_std <= 1.0\n        if not data.is_cuda:\n            raise NotImplementedError\n\n        n = rois.shape[0]\n        output = data.new_empty(n, out_channels, out_size, out_size)\n        output_count = data.new_empty(n, out_channels, out_size, out_size)\n        deform_pool_ext.deform_psroi_pooling_forward(\n            data, rois, offset, output, output_count, ctx.no_trans,\n            ctx.spatial_scale, ctx.out_channels, ctx.group_size, ctx.out_size,\n            ctx.part_size, ctx.sample_per_part, ctx.trans_std)\n\n        if data.requires_grad or rois.requires_grad or offset.requires_grad:\n            ctx.save_for_backward(data, rois, offset)\n        ctx.output_count = output_count\n\n        return output\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_output):\n        if not grad_output.is_cuda:\n            raise NotImplementedError\n\n        data, rois, offset = ctx.saved_tensors\n        output_count = ctx.output_count\n        grad_input = torch.zeros_like(data)\n        grad_rois = None\n        grad_offset = torch.zeros_like(offset)\n\n        deform_pool_ext.deform_psroi_pooling_backward(\n            grad_output, data, rois, offset, output_count, grad_input,\n            grad_offset, ctx.no_trans, ctx.spatial_scale, ctx.out_channels,\n            ctx.group_size, ctx.out_size, ctx.part_size, ctx.sample_per_part,\n            ctx.trans_std)\n        return (grad_input, grad_rois, grad_offset, None, None, None, None,\n                None, None, None, None)\n\n\ndeform_roi_pooling = DeformRoIPoolingFunction.apply\n\n\nclass DeformRoIPooling(nn.Module):\n\n    def __init__(self,\n                 spatial_scale,\n                 out_size,\n                 out_channels,\n                 no_trans,\n                 group_size=1,\n                 part_size=None,\n                 sample_per_part=4,\n                 trans_std=.0):\n        super(DeformRoIPooling, self).__init__()\n        self.spatial_scale = spatial_scale\n        self.out_size = _pair(out_size)\n        self.out_channels = out_channels\n        self.no_trans = no_trans\n        self.group_size = group_size\n        self.part_size = out_size if part_size is None else part_size\n        self.sample_per_part = sample_per_part\n        self.trans_std = trans_std\n\n    def forward(self, data, rois, offset):\n        if self.no_trans:\n            offset = data.new_empty(0)\n        return deform_roi_pooling(data, rois, offset, self.spatial_scale,\n                                  self.out_size, self.out_channels,\n                                  self.no_trans, self.group_size,\n                                  self.part_size, self.sample_per_part,\n                                  self.trans_std)\n\n\nclass DeformRoIPoolingPack(DeformRoIPooling):\n\n    def __init__(self,\n                 spatial_scale,\n                 out_size,\n                 out_channels,\n                 no_trans,\n                 group_size=1,\n                 part_size=None,\n                 sample_per_part=4,\n                 trans_std=.0,\n                 num_offset_fcs=3,\n                 deform_fc_channels=1024):\n        super(DeformRoIPoolingPack,\n              self).__init__(spatial_scale, out_size, out_channels, no_trans,\n                             group_size, part_size, sample_per_part, trans_std)\n\n        self.num_offset_fcs = num_offset_fcs\n        self.deform_fc_channels = deform_fc_channels\n\n        if not no_trans:\n            seq = []\n            ic = self.out_size[0] * self.out_size[1] * self.out_channels\n            for i in range(self.num_offset_fcs):\n                if i < self.num_offset_fcs - 1:\n                    oc = self.deform_fc_channels\n                else:\n                    oc = self.out_size[0] * self.out_size[1] * 2\n                seq.append(nn.Linear(ic, oc))\n                ic = oc\n                if i < self.num_offset_fcs - 1:\n                    seq.append(nn.ReLU(inplace=True))\n            self.offset_fc = nn.Sequential(*seq)\n            self.offset_fc[-1].weight.data.zero_()\n            self.offset_fc[-1].bias.data.zero_()\n\n    def forward(self, data, rois):\n        assert data.size(1) == self.out_channels\n        n = rois.shape[0]\n        if n == 0:\n            return data.new_empty(n, self.out_channels, self.out_size[0],\n                                  self.out_size[1])\n        if self.no_trans:\n            offset = data.new_empty(0)\n            return deform_roi_pooling(data, rois, offset, self.spatial_scale,\n                                      self.out_size, self.out_channels,\n                                      self.no_trans, self.group_size,\n                                      self.part_size, self.sample_per_part,\n                                      self.trans_std)\n        else:\n            offset = data.new_empty(0)\n            x = deform_roi_pooling(data, rois, offset, self.spatial_scale,\n                                   self.out_size, self.out_channels, True,\n                                   self.group_size, self.part_size,\n                                   self.sample_per_part, self.trans_std)\n            offset = self.offset_fc(x.view(n, -1))\n            offset = offset.view(n, 2, self.out_size[0], self.out_size[1])\n            return deform_roi_pooling(data, rois, offset, self.spatial_scale,\n                                      self.out_size, self.out_channels,\n                                      self.no_trans, self.group_size,\n                                      self.part_size, self.sample_per_part,\n                                      self.trans_std)\n\n\nclass ModulatedDeformRoIPoolingPack(DeformRoIPooling):\n\n    def __init__(self,\n                 spatial_scale,\n                 out_size,\n                 out_channels,\n                 no_trans,\n                 group_size=1,\n                 part_size=None,\n                 sample_per_part=4,\n                 trans_std=.0,\n                 num_offset_fcs=3,\n                 num_mask_fcs=2,\n                 deform_fc_channels=1024):\n        super(ModulatedDeformRoIPoolingPack,\n              self).__init__(spatial_scale, out_size, out_channels, no_trans,\n                             group_size, part_size, sample_per_part, trans_std)\n\n        self.num_offset_fcs = num_offset_fcs\n        self.num_mask_fcs = num_mask_fcs\n        self.deform_fc_channels = deform_fc_channels\n\n        if not no_trans:\n            offset_fc_seq = []\n            ic = self.out_size[0] * self.out_size[1] * self.out_channels\n            for i in range(self.num_offset_fcs):\n                if i < self.num_offset_fcs - 1:\n                    oc = self.deform_fc_channels\n                else:\n                    oc = self.out_size[0] * self.out_size[1] * 2\n                offset_fc_seq.append(nn.Linear(ic, oc))\n                ic = oc\n                if i < self.num_offset_fcs - 1:\n                    offset_fc_seq.append(nn.ReLU(inplace=True))\n            self.offset_fc = nn.Sequential(*offset_fc_seq)\n            self.offset_fc[-1].weight.data.zero_()\n            self.offset_fc[-1].bias.data.zero_()\n\n            mask_fc_seq = []\n            ic = self.out_size[0] * self.out_size[1] * self.out_channels\n            for i in range(self.num_mask_fcs):\n                if i < self.num_mask_fcs - 1:\n                    oc = self.deform_fc_channels\n                else:\n                    oc = self.out_size[0] * self.out_size[1]\n                mask_fc_seq.append(nn.Linear(ic, oc))\n                ic = oc\n                if i < self.num_mask_fcs - 1:\n                    mask_fc_seq.append(nn.ReLU(inplace=True))\n                else:\n                    mask_fc_seq.append(nn.Sigmoid())\n            self.mask_fc = nn.Sequential(*mask_fc_seq)\n            self.mask_fc[-2].weight.data.zero_()\n            self.mask_fc[-2].bias.data.zero_()\n\n    def forward(self, data, rois):\n        assert data.size(1) == self.out_channels\n        n = rois.shape[0]\n        if n == 0:\n            return data.new_empty(n, self.out_channels, self.out_size[0],\n                                  self.out_size[1])\n        if self.no_trans:\n            offset = data.new_empty(0)\n            return deform_roi_pooling(data, rois, offset, self.spatial_scale,\n                                      self.out_size, self.out_channels,\n                                      self.no_trans, self.group_size,\n                                      self.part_size, self.sample_per_part,\n                                      self.trans_std)\n        else:\n            offset = data.new_empty(0)\n            x = deform_roi_pooling(data, rois, offset, self.spatial_scale,\n                                   self.out_size, self.out_channels, True,\n                                   self.group_size, self.part_size,\n                                   self.sample_per_part, self.trans_std)\n            offset = self.offset_fc(x.view(n, -1))\n            offset = offset.view(n, 2, self.out_size[0], self.out_size[1])\n            mask = self.mask_fc(x.view(n, -1))\n            mask = mask.view(n, 1, self.out_size[0], self.out_size[1])\n            return deform_roi_pooling(\n                data, rois, offset, self.spatial_scale, self.out_size,\n                self.out_channels, self.no_trans, self.group_size,\n                self.part_size, self.sample_per_part, self.trans_std) * mask\n'"
mmdet/ops/masked_conv/__init__.py,0,"b""from .masked_conv import MaskedConv2d, masked_conv2d\n\n__all__ = ['masked_conv2d', 'MaskedConv2d']\n"""
mmdet/ops/masked_conv/masked_conv.py,6,"b'import math\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Function\nfrom torch.autograd.function import once_differentiable\nfrom torch.nn.modules.utils import _pair\n\nfrom . import masked_conv2d_ext\n\n\nclass MaskedConv2dFunction(Function):\n\n    @staticmethod\n    def forward(ctx, features, mask, weight, bias, padding=0, stride=1):\n        assert mask.dim() == 3 and mask.size(0) == 1\n        assert features.dim() == 4 and features.size(0) == 1\n        assert features.size()[2:] == mask.size()[1:]\n        pad_h, pad_w = _pair(padding)\n        stride_h, stride_w = _pair(stride)\n        if stride_h != 1 or stride_w != 1:\n            raise ValueError(\n                \'Stride could not only be 1 in masked_conv2d currently.\')\n        if not features.is_cuda:\n            raise NotImplementedError\n\n        out_channel, in_channel, kernel_h, kernel_w = weight.size()\n\n        batch_size = features.size(0)\n        out_h = int(\n            math.floor((features.size(2) + 2 * pad_h -\n                        (kernel_h - 1) - 1) / stride_h + 1))\n        out_w = int(\n            math.floor((features.size(3) + 2 * pad_w -\n                        (kernel_h - 1) - 1) / stride_w + 1))\n        mask_inds = torch.nonzero(mask[0] > 0, as_tuple=False)\n        output = features.new_zeros(batch_size, out_channel, out_h, out_w)\n        if mask_inds.numel() > 0:\n            mask_h_idx = mask_inds[:, 0].contiguous()\n            mask_w_idx = mask_inds[:, 1].contiguous()\n            data_col = features.new_zeros(in_channel * kernel_h * kernel_w,\n                                          mask_inds.size(0))\n            masked_conv2d_ext.masked_im2col_forward(features, mask_h_idx,\n                                                    mask_w_idx, kernel_h,\n                                                    kernel_w, pad_h, pad_w,\n                                                    data_col)\n\n            masked_output = torch.addmm(1, bias[:, None], 1,\n                                        weight.view(out_channel, -1), data_col)\n            masked_conv2d_ext.masked_col2im_forward(masked_output, mask_h_idx,\n                                                    mask_w_idx, out_h, out_w,\n                                                    out_channel, output)\n        return output\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_output):\n        return (None, ) * 5\n\n\nmasked_conv2d = MaskedConv2dFunction.apply\n\n\nclass MaskedConv2d(nn.Conv2d):\n    """"""A MaskedConv2d which inherits the official Conv2d.\n\n    The masked forward doesn\'t implement the backward function and only\n    supports the stride parameter to be 1 currently.\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 groups=1,\n                 bias=True):\n        super(MaskedConv2d,\n              self).__init__(in_channels, out_channels, kernel_size, stride,\n                             padding, dilation, groups, bias)\n\n    def forward(self, input, mask=None):\n        if mask is None:  # fallback to the normal Conv2d\n            return super(MaskedConv2d, self).forward(input)\n        else:\n            return masked_conv2d(input, mask, self.weight, self.bias,\n                                 self.padding)\n'"
mmdet/ops/nms/__init__.py,0,"b""from .nms_wrapper import batched_nms, nms, nms_match, soft_nms\n\n__all__ = ['nms', 'soft_nms', 'batched_nms', 'nms_match']\n"""
mmdet/ops/nms/nms_wrapper.py,18,"b'import numpy as np\nimport torch\n\nfrom . import nms_ext\n\n\ndef nms(dets, iou_thr, device_id=None):\n    """"""Dispatch to either CPU or GPU NMS implementations.\n\n    The input can be either a torch tensor or numpy array. GPU NMS will be used\n    if the input is a gpu tensor or device_id is specified, otherwise CPU NMS\n    will be used. The returned type will always be the same as inputs.\n\n    Arguments:\n        dets (torch.Tensor or np.ndarray): bboxes with scores.\n        iou_thr (float): IoU threshold for NMS.\n        device_id (int, optional): when `dets` is a numpy array, if `device_id`\n            is None, then cpu nms is used, otherwise gpu_nms will be used.\n\n    Returns:\n        tuple: kept bboxes and indice, which is always the same data type as\n            the input.\n\n    Example:\n        >>> dets = np.array([[49.1, 32.4, 51.0, 35.9, 0.9],\n        >>>                  [49.3, 32.9, 51.0, 35.3, 0.9],\n        >>>                  [49.2, 31.8, 51.0, 35.4, 0.5],\n        >>>                  [35.1, 11.5, 39.1, 15.7, 0.5],\n        >>>                  [35.6, 11.8, 39.3, 14.2, 0.5],\n        >>>                  [35.3, 11.5, 39.9, 14.5, 0.4],\n        >>>                  [35.2, 11.7, 39.7, 15.7, 0.3]], dtype=np.float32)\n        >>> iou_thr = 0.6\n        >>> suppressed, inds = nms(dets, iou_thr)\n        >>> assert len(inds) == len(suppressed) == 3\n    """"""\n    # convert dets (tensor or numpy array) to tensor\n    if isinstance(dets, torch.Tensor):\n        is_numpy = False\n        dets_th = dets\n    elif isinstance(dets, np.ndarray):\n        is_numpy = True\n        device = \'cpu\' if device_id is None else f\'cuda:{device_id}\'\n        dets_th = torch.from_numpy(dets).to(device)\n    else:\n        raise TypeError(\'dets must be either a Tensor or numpy array, \'\n                        f\'but got {type(dets)}\')\n\n    # execute cpu or cuda nms\n    if dets_th.shape[0] == 0:\n        inds = dets_th.new_zeros(0, dtype=torch.long)\n    else:\n        if dets_th.is_cuda:\n            inds = nms_ext.nms(dets_th, iou_thr)\n        else:\n            inds = nms_ext.nms(dets_th, iou_thr)\n\n    if is_numpy:\n        inds = inds.cpu().numpy()\n    return dets[inds, :], inds\n\n\ndef soft_nms(dets, iou_thr, method=\'linear\', sigma=0.5, min_score=1e-3):\n    """"""Dispatch to only CPU Soft NMS implementations.\n\n    The input can be either a torch tensor or numpy array.\n    The returned type will always be the same as inputs.\n\n    Arguments:\n        dets (torch.Tensor or np.ndarray): bboxes with scores.\n        iou_thr (float): IoU threshold for Soft NMS.\n        method (str): either \'linear\' or \'gaussian\'\n        sigma (float): hyperparameter for gaussian method\n        min_score (float): score filter threshold\n\n    Returns:\n        tuple: new det bboxes and indice, which is always the same\n        data type as the input.\n\n    Example:\n        >>> dets = np.array([[4., 3., 5., 3., 0.9],\n        >>>                  [4., 3., 5., 4., 0.9],\n        >>>                  [3., 1., 3., 1., 0.5],\n        >>>                  [3., 1., 3., 1., 0.5],\n        >>>                  [3., 1., 3., 1., 0.4],\n        >>>                  [3., 1., 3., 1., 0.0]], dtype=np.float32)\n        >>> iou_thr = 0.6\n        >>> new_dets, inds = soft_nms(dets, iou_thr, sigma=0.5)\n        >>> assert len(inds) == len(new_dets) == 5\n    """"""\n    # convert dets (tensor or numpy array) to tensor\n    if isinstance(dets, torch.Tensor):\n        is_tensor = True\n        dets_t = dets.detach().cpu()\n    elif isinstance(dets, np.ndarray):\n        is_tensor = False\n        dets_t = torch.from_numpy(dets)\n    else:\n        raise TypeError(\'dets must be either a Tensor or numpy array, \'\n                        f\'but got {type(dets)}\')\n\n    method_codes = {\'linear\': 1, \'gaussian\': 2}\n    if method not in method_codes:\n        raise ValueError(f\'Invalid method for SoftNMS: {method}\')\n    results = nms_ext.soft_nms(dets_t, iou_thr, method_codes[method], sigma,\n                               min_score)\n\n    new_dets = results[:, :5]\n    inds = results[:, 5]\n\n    if is_tensor:\n        return new_dets.to(\n            device=dets.device, dtype=dets.dtype), inds.to(\n                device=dets.device, dtype=torch.long)\n    else:\n        return new_dets.numpy().astype(dets.dtype), inds.numpy().astype(\n            np.int64)\n\n\ndef batched_nms(bboxes, scores, inds, nms_cfg, class_agnostic=False):\n    """"""Performs non-maximum suppression in a batched fashion.\n\n    Modified from https://github.com/pytorch/vision/blob\n    /505cd6957711af790211896d32b40291bea1bc21/torchvision/ops/boxes.py#L39.\n    In order to perform NMS independently per class, we add an offset to all\n    the boxes. The offset is dependent only on the class idx, and is large\n    enough so that boxes from different classes do not overlap.\n\n    Arguments:\n        bboxes (torch.Tensor): bboxes in shape (N, 4).\n        scores (torch.Tensor): scores in shape (N, ).\n        inds (torch.Tensor): each index value correspond to a bbox cluster,\n            and NMS will not be applied between elements of different inds,\n            shape (N, ).\n        nms_cfg (dict): specify nms type and class_agnostic as well as other\n            parameters like iou_thr.\n        class_agnostic (bool): if true, nms is class agnostic,\n            i.e. IoU thresholding happens over all bboxes,\n            regardless of the predicted class\n\n    Returns:\n        tuple: kept bboxes and indice.\n    """"""\n    nms_cfg_ = nms_cfg.copy()\n    class_agnostic = nms_cfg_.pop(\'class_agnostic\', class_agnostic)\n    if class_agnostic:\n        bboxes_for_nms = bboxes\n    else:\n        max_coordinate = bboxes.max()\n        offsets = inds.to(bboxes) * (max_coordinate + 1)\n        bboxes_for_nms = bboxes + offsets[:, None]\n    nms_type = nms_cfg_.pop(\'type\', \'nms\')\n    nms_op = eval(nms_type)\n    dets, keep = nms_op(\n        torch.cat([bboxes_for_nms, scores[:, None]], -1), **nms_cfg_)\n    bboxes = bboxes[keep]\n    scores = dets[:, -1]\n    return torch.cat([bboxes, scores[:, None]], -1), keep\n\n\ndef nms_match(dets, thresh):\n    """"""Matched dets into different groups by NMS.\n\n    NMS match is Similar to NMS but when a bbox is suppressed, nms match will\n    record the indice of supporessed bbox and form a group with the indice of\n    kept bbox. In each group, indice is sorted as score order.\n\n    Arguments:\n        dets (torch.Tensor | np.ndarray): Det bboxes with scores, shape (N, 5).\n        iou_thr (float): IoU thresh for NMS.\n\n    Returns:\n        List[Tensor | ndarray]: The outer list corresponds different matched\n            group, the inner Tensor corresponds the indices for a group in\n            score order.\n    """"""\n    if dets.shape[0] == 0:\n        matched = []\n    else:\n        assert dets.shape[-1] == 5, \'inputs dets.shape should be (N, 5), \' \\\n                                    f\'but get {dets.shape}\'\n        if isinstance(dets, torch.Tensor):\n            dets_t = dets.detach().cpu()\n        else:\n            dets_t = torch.from_numpy(dets)\n        matched = nms_ext.nms_match(dets_t, thresh)\n\n    if isinstance(dets, torch.Tensor):\n        return [dets.new_tensor(m, dtype=torch.long) for m in matched]\n    else:\n        return [np.array(m, dtype=np.int) for m in matched]\n'"
mmdet/ops/roi_align/__init__.py,0,"b""from .roi_align import RoIAlign, roi_align\n\n__all__ = ['roi_align', 'RoIAlign']\n"""
mmdet/ops/roi_align/gradcheck.py,3,"b""import os.path as osp\nimport sys\n\nimport numpy as np\nimport torch\nfrom torch.autograd import gradcheck\n\nsys.path.append(osp.abspath(osp.join(__file__, '../../')))\nfrom roi_align import RoIAlign  # noqa: E402, isort:skip\n\nfeat_size = 15\nspatial_scale = 1.0 / 8\nimg_size = feat_size / spatial_scale\nnum_imgs = 2\nnum_rois = 20\n\nbatch_ind = np.random.randint(num_imgs, size=(num_rois, 1))\nrois = np.random.rand(num_rois, 4) * img_size * 0.5\nrois[:, 2:] += img_size * 0.5\nrois = np.hstack((batch_ind, rois))\n\nfeat = torch.randn(\n    num_imgs, 16, feat_size, feat_size, requires_grad=True, device='cuda:0')\nrois = torch.from_numpy(rois).float().cuda()\ninputs = (feat, rois)\nprint('Gradcheck for roi align...')\ntest = gradcheck(RoIAlign(3, spatial_scale), inputs, atol=1e-3, eps=1e-3)\nprint(test)\ntest = gradcheck(RoIAlign(3, spatial_scale, 2), inputs, atol=1e-3, eps=1e-3)\nprint(test)\n"""
mmdet/ops/roi_align/roi_align.py,3,"b'from torch import nn\nfrom torch.autograd import Function\nfrom torch.autograd.function import once_differentiable\nfrom torch.nn.modules.utils import _pair\n\nfrom . import roi_align_ext\n\n\nclass RoIAlignFunction(Function):\n\n    @staticmethod\n    def forward(ctx,\n                features,\n                rois,\n                out_size,\n                spatial_scale,\n                sample_num=0,\n                aligned=True):\n        out_h, out_w = _pair(out_size)\n        assert isinstance(out_h, int) and isinstance(out_w, int)\n        ctx.spatial_scale = spatial_scale\n        ctx.sample_num = sample_num\n        ctx.save_for_backward(rois)\n        ctx.feature_size = features.size()\n        ctx.aligned = aligned\n\n        if aligned:\n            output = roi_align_ext.forward_v2(features, rois, spatial_scale,\n                                              out_h, out_w, sample_num,\n                                              aligned)\n        elif features.is_cuda:\n            (batch_size, num_channels, data_height,\n             data_width) = features.size()\n            num_rois = rois.size(0)\n\n            output = features.new_zeros(num_rois, num_channels, out_h, out_w)\n            roi_align_ext.forward_v1(features, rois, out_h, out_w,\n                                     spatial_scale, sample_num, output)\n        else:\n            raise NotImplementedError\n\n        return output\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_output):\n        feature_size = ctx.feature_size\n        spatial_scale = ctx.spatial_scale\n        sample_num = ctx.sample_num\n        rois = ctx.saved_tensors[0]\n        aligned = ctx.aligned\n        assert feature_size is not None\n\n        batch_size, num_channels, data_height, data_width = feature_size\n        out_w = grad_output.size(3)\n        out_h = grad_output.size(2)\n\n        grad_input = grad_rois = None\n        if not aligned:\n            if ctx.needs_input_grad[0]:\n                grad_input = rois.new_zeros(batch_size, num_channels,\n                                            data_height, data_width)\n                roi_align_ext.backward_v1(grad_output.contiguous(), rois,\n                                          out_h, out_w, spatial_scale,\n                                          sample_num, grad_input)\n        else:\n            grad_input = roi_align_ext.backward_v2(grad_output, rois,\n                                                   spatial_scale, out_h, out_w,\n                                                   batch_size, num_channels,\n                                                   data_height, data_width,\n                                                   sample_num, aligned)\n\n        return grad_input, grad_rois, None, None, None, None\n\n\nroi_align = RoIAlignFunction.apply\n\n\nclass RoIAlign(nn.Module):\n\n    def __init__(self,\n                 out_size,\n                 spatial_scale,\n                 sample_num=0,\n                 use_torchvision=False,\n                 aligned=True):\n        """"""\n        Args:\n            out_size (tuple): h, w\n            spatial_scale (float): scale the input boxes by this number\n            sample_num (int): number of inputs samples to take for each\n                output sample. 2 to take samples densely for current models.\n            use_torchvision (bool): whether to use roi_align from torchvision\n            aligned (bool): if False, use the legacy implementation in\n                MMDetection. If True, align the results more perfectly.\n\n        Note:\n            The implementation of RoIAlign when aligned=True is modified from\n            https://github.com/facebookresearch/detectron2/\n\n            The meaning of aligned=True:\n\n            Given a continuous coordinate c, its two neighboring pixel\n            indices (in our pixel model) are computed by floor(c - 0.5) and\n            ceil(c - 0.5). For example, c=1.3 has pixel neighbors with discrete\n            indices [0] and [1] (which are sampled from the underlying signal\n            at continuous coordinates 0.5 and 1.5). But the original roi_align\n            (aligned=False) does not subtract the 0.5 when computing\n            neighboring pixel indices and therefore it uses pixels with a\n            slightly incorrect alignment (relative to our pixel model) when\n            performing bilinear interpolation.\n\n            With `aligned=True`,\n            we first appropriately scale the ROI and then shift it by -0.5\n            prior to calling roi_align. This produces the correct neighbors;\n\n            The difference does not make a difference to the model\'s\n            performance if ROIAlign is used together with conv layers.\n        """"""\n        super(RoIAlign, self).__init__()\n        self.out_size = _pair(out_size)\n        self.spatial_scale = float(spatial_scale)\n        self.aligned = aligned\n        self.sample_num = int(sample_num)\n        self.use_torchvision = use_torchvision\n        assert not (use_torchvision and\n                    aligned), \'Torchvision does not support aligned RoIAlgin\'\n\n    def forward(self, features, rois):\n        """"""\n        Args:\n            features: NCHW images\n            rois: Bx5 boxes. First column is the index into N. The other 4\n            columns are xyxy.\n        """"""\n        assert rois.dim() == 2 and rois.size(1) == 5\n\n        if self.use_torchvision:\n            from torchvision.ops import roi_align as tv_roi_align\n            return tv_roi_align(features, rois, self.out_size,\n                                self.spatial_scale, self.sample_num)\n        else:\n            return roi_align(features, rois, self.out_size, self.spatial_scale,\n                             self.sample_num, self.aligned)\n\n    def __repr__(self):\n        indent_str = \'\\n    \'\n        format_str = self.__class__.__name__\n        format_str += f\'({indent_str}out_size={self.out_size},\'\n        format_str += f\'{indent_str}spatial_scale={self.spatial_scale},\'\n        format_str += f\'{indent_str}sample_num={self.sample_num},\'\n        format_str += f\'{indent_str}use_torchvision={self.use_torchvision},\'\n        format_str += f\'{indent_str}aligned={self.aligned})\'\n        return format_str\n'"
mmdet/ops/roi_pool/__init__.py,0,"b""from .roi_pool import RoIPool, roi_pool\n\n__all__ = ['roi_pool', 'RoIPool']\n"""
mmdet/ops/roi_pool/gradcheck.py,3,"b""import os.path as osp\nimport sys\n\nimport torch\nfrom torch.autograd import gradcheck\n\nsys.path.append(osp.abspath(osp.join(__file__, '../../')))\nfrom roi_pool import RoIPool  # noqa: E402, isort:skip\n\nfeat = torch.randn(4, 16, 15, 15, requires_grad=True).cuda()\nrois = torch.Tensor([[0, 0, 0, 50, 50], [0, 10, 30, 43, 55],\n                     [1, 67, 40, 110, 120]]).cuda()\ninputs = (feat, rois)\nprint('Gradcheck for roi pooling...')\ntest = gradcheck(RoIPool(4, 1.0 / 8), inputs, eps=1e-5, atol=1e-3)\nprint(test)\n"""
mmdet/ops/roi_pool/roi_pool.py,5,"b""import torch\nimport torch.nn as nn\nfrom torch.autograd import Function\nfrom torch.autograd.function import once_differentiable\nfrom torch.nn.modules.utils import _pair\n\nfrom . import roi_pool_ext\n\n\nclass RoIPoolFunction(Function):\n\n    @staticmethod\n    def forward(ctx, features, rois, out_size, spatial_scale):\n        assert features.is_cuda\n        out_h, out_w = _pair(out_size)\n        assert isinstance(out_h, int) and isinstance(out_w, int)\n        ctx.save_for_backward(rois)\n        num_channels = features.size(1)\n        num_rois = rois.size(0)\n        out_size = (num_rois, num_channels, out_h, out_w)\n        output = features.new_zeros(out_size)\n        argmax = features.new_zeros(out_size, dtype=torch.int)\n        roi_pool_ext.forward(features, rois, out_h, out_w, spatial_scale,\n                             output, argmax)\n        ctx.spatial_scale = spatial_scale\n        ctx.feature_size = features.size()\n        ctx.argmax = argmax\n\n        return output\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_output):\n        assert grad_output.is_cuda\n        spatial_scale = ctx.spatial_scale\n        feature_size = ctx.feature_size\n        argmax = ctx.argmax\n        rois = ctx.saved_tensors[0]\n        assert feature_size is not None\n\n        grad_input = grad_rois = None\n        if ctx.needs_input_grad[0]:\n            grad_input = grad_output.new_zeros(feature_size)\n            roi_pool_ext.backward(grad_output.contiguous(), rois, argmax,\n                                  spatial_scale, grad_input)\n\n        return grad_input, grad_rois, None, None\n\n\nroi_pool = RoIPoolFunction.apply\n\n\nclass RoIPool(nn.Module):\n\n    def __init__(self, out_size, spatial_scale, use_torchvision=False):\n        super(RoIPool, self).__init__()\n\n        self.out_size = _pair(out_size)\n        self.spatial_scale = float(spatial_scale)\n        self.use_torchvision = use_torchvision\n\n    def forward(self, features, rois):\n        if self.use_torchvision:\n            from torchvision.ops import roi_pool as tv_roi_pool\n            return tv_roi_pool(features, rois, self.out_size,\n                               self.spatial_scale)\n        else:\n            return roi_pool(features, rois, self.out_size, self.spatial_scale)\n\n    def __repr__(self):\n        format_str = self.__class__.__name__\n        format_str += f'(out_size={self.out_size}, '\n        format_str += f'spatial_scale={self.spatial_scale}, '\n        format_str += f'use_torchvision={self.use_torchvision})'\n        return format_str\n"""
mmdet/ops/sigmoid_focal_loss/__init__.py,0,"b""from .sigmoid_focal_loss import SigmoidFocalLoss, sigmoid_focal_loss\r\n\r\n__all__ = ['SigmoidFocalLoss', 'sigmoid_focal_loss']\r\n"""
mmdet/ops/sigmoid_focal_loss/sigmoid_focal_loss.py,3,"b""import torch.nn as nn\nfrom torch.autograd import Function\nfrom torch.autograd.function import once_differentiable\n\nfrom . import sigmoid_focal_loss_ext\n\n\nclass SigmoidFocalLossFunction(Function):\n\n    @staticmethod\n    def forward(ctx, input, target, gamma=2.0, alpha=0.25):\n        ctx.save_for_backward(input, target)\n        num_classes = input.shape[1]\n        ctx.num_classes = num_classes\n        ctx.gamma = gamma\n        ctx.alpha = alpha\n\n        loss = sigmoid_focal_loss_ext.forward(input, target, num_classes,\n                                              gamma, alpha)\n        return loss\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, d_loss):\n        input, target = ctx.saved_tensors\n        num_classes = ctx.num_classes\n        gamma = ctx.gamma\n        alpha = ctx.alpha\n        d_loss = d_loss.contiguous()\n        d_input = sigmoid_focal_loss_ext.backward(input, target, d_loss,\n                                                  num_classes, gamma, alpha)\n        return d_input, None, None, None, None\n\n\nsigmoid_focal_loss = SigmoidFocalLossFunction.apply\n\n\n# TODO: remove this module\nclass SigmoidFocalLoss(nn.Module):\n\n    def __init__(self, gamma, alpha):\n        super(SigmoidFocalLoss, self).__init__()\n        self.gamma = gamma\n        self.alpha = alpha\n\n    def forward(self, logits, targets):\n        assert logits.is_cuda\n        loss = sigmoid_focal_loss(logits, targets, self.gamma, self.alpha)\n        return loss.sum()\n\n    def __repr__(self):\n        tmpstr = self.__class__.__name__\n        tmpstr += f'(gamma={self.gamma}, alpha={self.alpha})'\n        return tmpstr\n"""
mmdet/ops/utils/__init__.py,0,"b""# from . import compiling_info\nfrom .compiling_info import get_compiler_version, get_compiling_cuda_version\n\n# get_compiler_version = compiling_info.get_compiler_version\n# get_compiling_cuda_version = compiling_info.get_compiling_cuda_version\n\n__all__ = ['get_compiler_version', 'get_compiling_cuda_version']\n"""
mmdet/core/bbox/assigners/__init__.py,0,"b""from .approx_max_iou_assigner import ApproxMaxIoUAssigner\nfrom .assign_result import AssignResult\nfrom .atss_assigner import ATSSAssigner\nfrom .base_assigner import BaseAssigner\nfrom .center_region_assigner import CenterRegionAssigner\nfrom .max_iou_assigner import MaxIoUAssigner\nfrom .point_assigner import PointAssigner\n\n__all__ = [\n    'BaseAssigner', 'MaxIoUAssigner', 'ApproxMaxIoUAssigner', 'AssignResult',\n    'PointAssigner', 'ATSSAssigner', 'CenterRegionAssigner'\n]\n"""
mmdet/core/bbox/assigners/approx_max_iou_assigner.py,2,"b'import torch\n\nfrom ..builder import BBOX_ASSIGNERS\nfrom ..iou_calculators import build_iou_calculator\nfrom .max_iou_assigner import MaxIoUAssigner\n\n\n@BBOX_ASSIGNERS.register_module()\nclass ApproxMaxIoUAssigner(MaxIoUAssigner):\n    """"""Assign a corresponding gt bbox or background to each bbox.\n\n    Each proposals will be assigned with an integer indicating the ground truth\n     index. (semi-positive index: gt label (0-based), -1: background)\n\n    - -1: negative sample, no assigned gt\n    - semi-positive integer: positive sample, index (0-based) of assigned gt\n\n    Args:\n        pos_iou_thr (float): IoU threshold for positive bboxes.\n        neg_iou_thr (float or tuple): IoU threshold for negative bboxes.\n        min_pos_iou (float): Minimum iou for a bbox to be considered as a\n            positive bbox. Positive samples can have smaller IoU than\n            pos_iou_thr due to the 4th step (assign max IoU sample to each gt).\n        gt_max_assign_all (bool): Whether to assign all bboxes with the same\n            highest overlap with some gt to that gt.\n        ignore_iof_thr (float): IoF threshold for ignoring bboxes (if\n            `gt_bboxes_ignore` is specified). Negative values mean not\n            ignoring any bboxes.\n        ignore_wrt_candidates (bool): Whether to compute the iof between\n            `bboxes` and `gt_bboxes_ignore`, or the contrary.\n        match_low_quality (bool): Whether to allow quality matches. This is\n            usually allowed for RPN and single stage detectors, but not allowed\n            in the second stage.\n        gpu_assign_thr (int): The upper bound of the number of GT for GPU\n            assign. When the number of gt is above this threshold, will assign\n            on CPU device. Negative values mean not assign on CPU.\n    """"""\n\n    def __init__(self,\n                 pos_iou_thr,\n                 neg_iou_thr,\n                 min_pos_iou=.0,\n                 gt_max_assign_all=True,\n                 ignore_iof_thr=-1,\n                 ignore_wrt_candidates=True,\n                 match_low_quality=True,\n                 gpu_assign_thr=-1,\n                 iou_calculator=dict(type=\'BboxOverlaps2D\')):\n        self.pos_iou_thr = pos_iou_thr\n        self.neg_iou_thr = neg_iou_thr\n        self.min_pos_iou = min_pos_iou\n        self.gt_max_assign_all = gt_max_assign_all\n        self.ignore_iof_thr = ignore_iof_thr\n        self.ignore_wrt_candidates = ignore_wrt_candidates\n        self.gpu_assign_thr = gpu_assign_thr\n        self.match_low_quality = match_low_quality\n        self.iou_calculator = build_iou_calculator(iou_calculator)\n\n    def assign(self,\n               approxs,\n               squares,\n               approxs_per_octave,\n               gt_bboxes,\n               gt_bboxes_ignore=None,\n               gt_labels=None):\n        """"""Assign gt to approxs.\n\n        This method assign a gt bbox to each group of approxs (bboxes),\n        each group of approxs is represent by a base approx (bbox) and\n        will be assigned with -1, or a semi-positive number.\n        background_label (-1) means negative sample,\n        semi-positive number is the index (0-based) of assigned gt.\n        The assignment is done in following steps, the order matters.\n\n        1. assign every bbox to background_label (-1)\n        2. use the max IoU of each group of approxs to assign\n        2. assign proposals whose iou with all gts < neg_iou_thr to background\n        3. for each bbox, if the iou with its nearest gt >= pos_iou_thr,\n           assign it to that bbox\n        4. for each gt bbox, assign its nearest proposals (may be more than\n           one) to itself\n\n        Args:\n            approxs (Tensor): Bounding boxes to be assigned,\n                shape(approxs_per_octave*n, 4).\n            squares (Tensor): Base Bounding boxes to be assigned,\n                shape(n, 4).\n            approxs_per_octave (int): number of approxs per octave\n            gt_bboxes (Tensor): Groundtruth boxes, shape (k, 4).\n            gt_bboxes_ignore (Tensor, optional): Ground truth bboxes that are\n                labelled as `ignored`, e.g., crowd boxes in COCO.\n            gt_labels (Tensor, optional): Label of gt_bboxes, shape (k, ).\n\n        Returns:\n            :obj:`AssignResult`: The assign result.\n        """"""\n        num_squares = squares.size(0)\n        num_gts = gt_bboxes.size(0)\n\n        if num_squares == 0 or num_gts == 0:\n            # No predictions and/or truth, return empty assignment\n            overlaps = approxs.new(num_gts, num_squares)\n            assign_result = self.assign_wrt_overlaps(overlaps, gt_labels)\n            return assign_result\n\n        # re-organize anchors by approxs_per_octave x num_squares\n        approxs = torch.transpose(\n            approxs.view(num_squares, approxs_per_octave, 4), 0,\n            1).contiguous().view(-1, 4)\n        assign_on_cpu = True if (self.gpu_assign_thr > 0) and (\n            num_gts > self.gpu_assign_thr) else False\n        # compute overlap and assign gt on CPU when number of GT is large\n        if assign_on_cpu:\n            device = approxs.device\n            approxs = approxs.cpu()\n            gt_bboxes = gt_bboxes.cpu()\n            if gt_bboxes_ignore is not None:\n                gt_bboxes_ignore = gt_bboxes_ignore.cpu()\n            if gt_labels is not None:\n                gt_labels = gt_labels.cpu()\n        all_overlaps = self.iou_calculator(approxs, gt_bboxes)\n\n        overlaps, _ = all_overlaps.view(approxs_per_octave, num_squares,\n                                        num_gts).max(dim=0)\n        overlaps = torch.transpose(overlaps, 0, 1)\n\n        if (self.ignore_iof_thr > 0 and gt_bboxes_ignore is not None\n                and gt_bboxes_ignore.numel() > 0 and squares.numel() > 0):\n            if self.ignore_wrt_candidates:\n                ignore_overlaps = self.iou_calculator(\n                    squares, gt_bboxes_ignore, mode=\'iof\')\n                ignore_max_overlaps, _ = ignore_overlaps.max(dim=1)\n            else:\n                ignore_overlaps = self.iou_calculator(\n                    gt_bboxes_ignore, squares, mode=\'iof\')\n                ignore_max_overlaps, _ = ignore_overlaps.max(dim=0)\n            overlaps[:, ignore_max_overlaps > self.ignore_iof_thr] = -1\n\n        assign_result = self.assign_wrt_overlaps(overlaps, gt_labels)\n        if assign_on_cpu:\n            assign_result.gt_inds = assign_result.gt_inds.to(device)\n            assign_result.max_overlaps = assign_result.max_overlaps.to(device)\n            if assign_result.labels is not None:\n                assign_result.labels = assign_result.labels.to(device)\n        return assign_result\n'"
mmdet/core/bbox/assigners/assign_result.py,20,"b'import torch\n\nfrom mmdet.utils import util_mixins\n\n\nclass AssignResult(util_mixins.NiceRepr):\n    """"""\n    Stores assignments between predicted and truth boxes.\n\n    Attributes:\n        num_gts (int): the number of truth boxes considered when computing this\n            assignment\n\n        gt_inds (LongTensor): for each predicted box indicates the 1-based\n            index of the assigned truth box. 0 means unassigned and -1 means\n            ignore.\n\n        max_overlaps (FloatTensor): the iou between the predicted box and its\n            assigned truth box.\n\n        labels (None | LongTensor): If specified, for each predicted box\n            indicates the category label of the assigned truth box.\n\n    Example:\n        >>> # An assign result between 4 predicted boxes and 9 true boxes\n        >>> # where only two boxes were assigned.\n        >>> num_gts = 9\n        >>> max_overlaps = torch.LongTensor([0, .5, .9, 0])\n        >>> gt_inds = torch.LongTensor([-1, 1, 2, 0])\n        >>> labels = torch.LongTensor([0, 3, 4, 0])\n        >>> self = AssignResult(num_gts, gt_inds, max_overlaps, labels)\n        >>> print(str(self))  # xdoctest: +IGNORE_WANT\n        <AssignResult(num_gts=9, gt_inds.shape=(4,), max_overlaps.shape=(4,),\n                      labels.shape=(4,))>\n        >>> # Force addition of gt labels (when adding gt as proposals)\n        >>> new_labels = torch.LongTensor([3, 4, 5])\n        >>> self.add_gt_(new_labels)\n        >>> print(str(self))  # xdoctest: +IGNORE_WANT\n        <AssignResult(num_gts=9, gt_inds.shape=(7,), max_overlaps.shape=(7,),\n                      labels.shape=(7,))>\n    """"""\n\n    def __init__(self, num_gts, gt_inds, max_overlaps, labels=None):\n        self.num_gts = num_gts\n        self.gt_inds = gt_inds\n        self.max_overlaps = max_overlaps\n        self.labels = labels\n        # Interface for possible user-defined properties\n        self._extra_properties = {}\n\n    @property\n    def num_preds(self):\n        """"""\n        Return the number of predictions in this assignment\n        """"""\n        return len(self.gt_inds)\n\n    def set_extra_property(self, key, value):\n        """"""Set user-defined new property""""""\n        assert key not in self.info\n        self._extra_properties[key] = value\n\n    def get_extra_property(self, key):\n        """"""Get user-defined property""""""\n        return self._extra_properties.get(key, None)\n\n    @property\n    def info(self):\n        """"""\n        Returns a dictionary of info about the object\n        """"""\n        basic_info = {\n            \'num_gts\': self.num_gts,\n            \'num_preds\': self.num_preds,\n            \'gt_inds\': self.gt_inds,\n            \'max_overlaps\': self.max_overlaps,\n            \'labels\': self.labels,\n        }\n        basic_info.update(self._extra_properties)\n        return basic_info\n\n    def __nice__(self):\n        """"""\n        Create a ""nice"" summary string describing this assign result\n        """"""\n        parts = []\n        parts.append(f\'num_gts={self.num_gts!r}\')\n        if self.gt_inds is None:\n            parts.append(f\'gt_inds={self.gt_inds!r}\')\n        else:\n            parts.append(f\'gt_inds.shape={tuple(self.gt_inds.shape)!r}\')\n        if self.max_overlaps is None:\n            parts.append(f\'max_overlaps={self.max_overlaps!r}\')\n        else:\n            parts.append(\'max_overlaps.shape=\'\n                         f\'{tuple(self.max_overlaps.shape)!r}\')\n        if self.labels is None:\n            parts.append(f\'labels={self.labels!r}\')\n        else:\n            parts.append(f\'labels.shape={tuple(self.labels.shape)!r}\')\n        return \', \'.join(parts)\n\n    @classmethod\n    def random(cls, **kwargs):\n        """"""Create random AssignResult for tests or debugging.\n\n        Args:\n            num_preds: number of predicted boxes\n            num_gts: number of true boxes\n            p_ignore (float): probability of a predicted box assinged to an\n                ignored truth\n            p_assigned (float): probability of a predicted box not being\n                assigned\n            p_use_label (float | bool): with labels or not\n            rng (None | int | numpy.random.RandomState): seed or state\n\n        Returns:\n            :obj:`AssignResult`: Randomly generated assign results.\n\n        Example:\n            >>> from mmdet.core.bbox.assigners.assign_result import *  # NOQA\n            >>> self = AssignResult.random()\n            >>> print(self.info)\n        """"""\n        from mmdet.core.bbox import demodata\n        rng = demodata.ensure_rng(kwargs.get(\'rng\', None))\n\n        num_gts = kwargs.get(\'num_gts\', None)\n        num_preds = kwargs.get(\'num_preds\', None)\n        p_ignore = kwargs.get(\'p_ignore\', 0.3)\n        p_assigned = kwargs.get(\'p_assigned\', 0.7)\n        p_use_label = kwargs.get(\'p_use_label\', 0.5)\n        num_classes = kwargs.get(\'p_use_label\', 3)\n\n        if num_gts is None:\n            num_gts = rng.randint(0, 8)\n        if num_preds is None:\n            num_preds = rng.randint(0, 16)\n\n        if num_gts == 0:\n            max_overlaps = torch.zeros(num_preds, dtype=torch.float32)\n            gt_inds = torch.zeros(num_preds, dtype=torch.int64)\n            if p_use_label is True or p_use_label < rng.rand():\n                labels = torch.zeros(num_preds, dtype=torch.int64)\n            else:\n                labels = None\n        else:\n            import numpy as np\n            # Create an overlap for each predicted box\n            max_overlaps = torch.from_numpy(rng.rand(num_preds))\n\n            # Construct gt_inds for each predicted box\n            is_assigned = torch.from_numpy(rng.rand(num_preds) < p_assigned)\n            # maximum number of assignments constraints\n            n_assigned = min(num_preds, min(num_gts, is_assigned.sum()))\n\n            assigned_idxs = np.where(is_assigned)[0]\n            rng.shuffle(assigned_idxs)\n            assigned_idxs = assigned_idxs[0:n_assigned]\n            assigned_idxs.sort()\n\n            is_assigned[:] = 0\n            is_assigned[assigned_idxs] = True\n\n            is_ignore = torch.from_numpy(\n                rng.rand(num_preds) < p_ignore) & is_assigned\n\n            gt_inds = torch.zeros(num_preds, dtype=torch.int64)\n\n            true_idxs = np.arange(num_gts)\n            rng.shuffle(true_idxs)\n            true_idxs = torch.from_numpy(true_idxs)\n            gt_inds[is_assigned] = true_idxs[:n_assigned]\n\n            gt_inds = torch.from_numpy(\n                rng.randint(1, num_gts + 1, size=num_preds))\n            gt_inds[is_ignore] = -1\n            gt_inds[~is_assigned] = 0\n            max_overlaps[~is_assigned] = 0\n\n            if p_use_label is True or p_use_label < rng.rand():\n                if num_classes == 0:\n                    labels = torch.zeros(num_preds, dtype=torch.int64)\n                else:\n                    labels = torch.from_numpy(\n                        # remind that we set FG labels to [0, num_class-1]\n                        # since mmdet v2.0\n                        # BG cat_id: num_class\n                        rng.randint(0, num_classes, size=num_preds))\n                    labels[~is_assigned] = 0\n            else:\n                labels = None\n\n        self = cls(num_gts, gt_inds, max_overlaps, labels)\n        return self\n\n    def add_gt_(self, gt_labels):\n        self_inds = torch.arange(\n            1, len(gt_labels) + 1, dtype=torch.long, device=gt_labels.device)\n        self.gt_inds = torch.cat([self_inds, self.gt_inds])\n\n        self.max_overlaps = torch.cat(\n            [self.max_overlaps.new_ones(len(gt_labels)), self.max_overlaps])\n\n        if self.labels is not None:\n            self.labels = torch.cat([gt_labels, self.labels])\n'"
mmdet/core/bbox/assigners/atss_assigner.py,9,"b'import torch\n\nfrom ..builder import BBOX_ASSIGNERS\nfrom ..iou_calculators import build_iou_calculator\nfrom .assign_result import AssignResult\nfrom .base_assigner import BaseAssigner\n\n\n@BBOX_ASSIGNERS.register_module()\nclass ATSSAssigner(BaseAssigner):\n    """"""Assign a corresponding gt bbox or background to each bbox.\n\n    Each proposals will be assigned with `0` or a positive integer\n    indicating the ground truth index.\n\n    - 0: negative sample, no assigned gt\n    - positive integer: positive sample, index (1-based) of assigned gt\n\n    Args:\n        topk (float): number of bbox selected in each level\n    """"""\n\n    def __init__(self, topk, iou_calculator=dict(type=\'BboxOverlaps2D\')):\n        self.topk = topk\n        self.iou_calculator = build_iou_calculator(iou_calculator)\n\n    # https://github.com/sfzhang15/ATSS/blob/master/atss_core/modeling/rpn/atss/loss.py\n\n    def assign(self,\n               bboxes,\n               num_level_bboxes,\n               gt_bboxes,\n               gt_bboxes_ignore=None,\n               gt_labels=None):\n        """"""Assign gt to bboxes.\n\n        The assignment is done in following steps\n\n        1. compute iou between all bbox (bbox of all pyramid levels) and gt\n        2. compute center distance between all bbox and gt\n        3. on each pyramid level, for each gt, select k bbox whose center\n           are closest to the gt center, so we total select k*l bbox as\n           candidates for each gt\n        4. get corresponding iou for the these candidates, and compute the\n           mean and std, set mean + std as the iou threshold\n        5. select these candidates whose iou are greater than or equal to\n           the threshold as postive\n        6. limit the positive sample\'s center in gt\n\n\n        Args:\n            bboxes (Tensor): Bounding boxes to be assigned, shape(n, 4).\n            num_level_bboxes (List): num of bboxes in each level\n            gt_bboxes (Tensor): Groundtruth boxes, shape (k, 4).\n            gt_bboxes_ignore (Tensor, optional): Ground truth bboxes that are\n                labelled as `ignored`, e.g., crowd boxes in COCO.\n            gt_labels (Tensor, optional): Label of gt_bboxes, shape (k, ).\n\n        Returns:\n            :obj:`AssignResult`: The assign result.\n        """"""\n        INF = 100000000\n        bboxes = bboxes[:, :4]\n        num_gt, num_bboxes = gt_bboxes.size(0), bboxes.size(0)\n\n        # compute iou between all bbox and gt\n        overlaps = self.iou_calculator(bboxes, gt_bboxes)\n\n        # assign 0 by default\n        assigned_gt_inds = overlaps.new_full((num_bboxes, ),\n                                             0,\n                                             dtype=torch.long)\n\n        if num_gt == 0 or num_bboxes == 0:\n            # No ground truth or boxes, return empty assignment\n            max_overlaps = overlaps.new_zeros((num_bboxes, ))\n            if num_gt == 0:\n                # No truth, assign everything to background\n                assigned_gt_inds[:] = 0\n            if gt_labels is None:\n                assigned_labels = None\n            else:\n                assigned_labels = overlaps.new_full((num_bboxes, ),\n                                                    -1,\n                                                    dtype=torch.long)\n            return AssignResult(\n                num_gt, assigned_gt_inds, max_overlaps, labels=assigned_labels)\n\n        # compute center distance between all bbox and gt\n        gt_cx = (gt_bboxes[:, 0] + gt_bboxes[:, 2]) / 2.0\n        gt_cy = (gt_bboxes[:, 1] + gt_bboxes[:, 3]) / 2.0\n        gt_points = torch.stack((gt_cx, gt_cy), dim=1)\n\n        bboxes_cx = (bboxes[:, 0] + bboxes[:, 2]) / 2.0\n        bboxes_cy = (bboxes[:, 1] + bboxes[:, 3]) / 2.0\n        bboxes_points = torch.stack((bboxes_cx, bboxes_cy), dim=1)\n\n        distances = (bboxes_points[:, None, :] -\n                     gt_points[None, :, :]).pow(2).sum(-1).sqrt()\n\n        # Selecting candidates based on the center distance\n        candidate_idxs = []\n        start_idx = 0\n        for level, bboxes_per_level in enumerate(num_level_bboxes):\n            # on each pyramid level, for each gt,\n            # select k bbox whose center are closest to the gt center\n            end_idx = start_idx + bboxes_per_level\n            distances_per_level = distances[start_idx:end_idx, :]\n            _, topk_idxs_per_level = distances_per_level.topk(\n                self.topk, dim=0, largest=False)\n            candidate_idxs.append(topk_idxs_per_level + start_idx)\n            start_idx = end_idx\n        candidate_idxs = torch.cat(candidate_idxs, dim=0)\n\n        # get corresponding iou for the these candidates, and compute the\n        # mean and std, set mean + std as the iou threshold\n        candidate_overlaps = overlaps[candidate_idxs, torch.arange(num_gt)]\n        overlaps_mean_per_gt = candidate_overlaps.mean(0)\n        overlaps_std_per_gt = candidate_overlaps.std(0)\n        overlaps_thr_per_gt = overlaps_mean_per_gt + overlaps_std_per_gt\n\n        is_pos = candidate_overlaps >= overlaps_thr_per_gt[None, :]\n\n        # limit the positive sample\'s center in gt\n        for gt_idx in range(num_gt):\n            candidate_idxs[:, gt_idx] += gt_idx * num_bboxes\n        ep_bboxes_cx = bboxes_cx.view(1, -1).expand(\n            num_gt, num_bboxes).contiguous().view(-1)\n        ep_bboxes_cy = bboxes_cy.view(1, -1).expand(\n            num_gt, num_bboxes).contiguous().view(-1)\n        candidate_idxs = candidate_idxs.view(-1)\n\n        # calculate the left, top, right, bottom distance between positive\n        # bbox center and gt side\n        l_ = ep_bboxes_cx[candidate_idxs].view(-1, num_gt) - gt_bboxes[:, 0]\n        t_ = ep_bboxes_cy[candidate_idxs].view(-1, num_gt) - gt_bboxes[:, 1]\n        r_ = gt_bboxes[:, 2] - ep_bboxes_cx[candidate_idxs].view(-1, num_gt)\n        b_ = gt_bboxes[:, 3] - ep_bboxes_cy[candidate_idxs].view(-1, num_gt)\n        is_in_gts = torch.stack([l_, t_, r_, b_], dim=1).min(dim=1)[0] > 0.01\n        is_pos = is_pos & is_in_gts\n\n        # if an anchor box is assigned to multiple gts,\n        # the one with the highest IoU will be selected.\n        overlaps_inf = torch.full_like(overlaps,\n                                       -INF).t().contiguous().view(-1)\n        index = candidate_idxs.view(-1)[is_pos.view(-1)]\n        overlaps_inf[index] = overlaps.t().contiguous().view(-1)[index]\n        overlaps_inf = overlaps_inf.view(num_gt, -1).t()\n\n        max_overlaps, argmax_overlaps = overlaps_inf.max(dim=1)\n        assigned_gt_inds[\n            max_overlaps != -INF] = argmax_overlaps[max_overlaps != -INF] + 1\n\n        if gt_labels is not None:\n            assigned_labels = assigned_gt_inds.new_full((num_bboxes, ), -1)\n            pos_inds = torch.nonzero(\n                assigned_gt_inds > 0, as_tuple=False).squeeze()\n            if pos_inds.numel() > 0:\n                assigned_labels[pos_inds] = gt_labels[\n                    assigned_gt_inds[pos_inds] - 1]\n        else:\n            assigned_labels = None\n        return AssignResult(\n            num_gt, assigned_gt_inds, max_overlaps, labels=assigned_labels)\n'"
mmdet/core/bbox/assigners/base_assigner.py,0,"b'from abc import ABCMeta, abstractmethod\n\n\nclass BaseAssigner(metaclass=ABCMeta):\n\n    @abstractmethod\n    def assign(self, bboxes, gt_bboxes, gt_bboxes_ignore=None, gt_labels=None):\n        pass\n'"
mmdet/core/bbox/assigners/center_region_assigner.py,15,"b'import torch\n\nfrom ..builder import BBOX_ASSIGNERS\nfrom ..iou_calculators import build_iou_calculator\nfrom .assign_result import AssignResult\nfrom .base_assigner import BaseAssigner\n\n\ndef scale_boxes(bboxes, scale):\n    """"""Expand an array of boxes by a given scale.\n\n    Args:\n        bboxes (Tensor): Shape (m, 4)\n        scale (float): The scale factor of bboxes\n\n    Returns:\n        (Tensor): Shape (m, 4). Scaled bboxes\n    """"""\n    assert bboxes.size(1) == 4\n    w_half = (bboxes[:, 2] - bboxes[:, 0]) * .5\n    h_half = (bboxes[:, 3] - bboxes[:, 1]) * .5\n    x_c = (bboxes[:, 2] + bboxes[:, 0]) * .5\n    y_c = (bboxes[:, 3] + bboxes[:, 1]) * .5\n\n    w_half *= scale\n    h_half *= scale\n\n    boxes_scaled = torch.zeros_like(bboxes)\n    boxes_scaled[:, 0] = x_c - w_half\n    boxes_scaled[:, 2] = x_c + w_half\n    boxes_scaled[:, 1] = y_c - h_half\n    boxes_scaled[:, 3] = y_c + h_half\n    return boxes_scaled\n\n\ndef is_located_in(points, bboxes):\n    """"""Are points located in bboxes\n\n    Args:\n      points (Tensor): Points, shape: (m, 2).\n      bboxes (Tensor): Bounding boxes, shape: (n, 4).\n\n    Return:\n      Tensor: Flags indicating if points are located in bboxes, shape: (m, n).\n    """"""\n    assert points.size(1) == 2\n    assert bboxes.size(1) == 4\n    return (points[:, 0].unsqueeze(1) > bboxes[:, 0].unsqueeze(0)) & \\\n           (points[:, 0].unsqueeze(1) < bboxes[:, 2].unsqueeze(0)) & \\\n           (points[:, 1].unsqueeze(1) > bboxes[:, 1].unsqueeze(0)) & \\\n           (points[:, 1].unsqueeze(1) < bboxes[:, 3].unsqueeze(0))\n\n\ndef bboxes_area(bboxes):\n    """"""Compute the area of an array of bboxes.\n\n    Args:\n        bboxes (Tensor): The coordinates ox bboxes. Shape: (m, 4)\n\n    Returns:\n        Tensor: Area of the bboxes. Shape: (m, )\n\n    """"""\n    assert bboxes.size(1) == 4\n    w = (bboxes[:, 2] - bboxes[:, 0])\n    h = (bboxes[:, 3] - bboxes[:, 1])\n    areas = w * h\n    return areas\n\n\n@BBOX_ASSIGNERS.register_module()\nclass CenterRegionAssigner(BaseAssigner):\n    """"""Assign pixels at the center region of a bbox as positive.\n\n    Each proposals will be assigned with `-1`, `0`, or a positive integer\n    indicating the ground truth index.\n    - -1: negative samples\n    - semi-positive numbers: positive sample, index (0-based) of assigned gt\n\n    Args:\n        pos_scale (float): Threshold within which pixels are\n          labelled as positive.\n        neg_scale (float): Threshold above which pixels are\n          labelled as positive.\n        min_pos_iof (float): Minimum iof of a pixel with a gt to be\n          labelled as positive. Default: 1e-2\n        ignore_gt_scale (float): Threshold within which the pixels\n          are ignored when the gt is labelled as shadowed. Default: 0.5\n    """"""\n\n    def __init__(self,\n                 pos_scale,\n                 neg_scale,\n                 min_pos_iof=1e-2,\n                 ignore_gt_scale=0.5,\n                 iou_calculator=dict(type=\'BboxOverlaps2D\')):\n        self.pos_scale = pos_scale\n        self.neg_scale = neg_scale\n        self.min_pos_iof = min_pos_iof\n        self.ignore_gt_scale = ignore_gt_scale\n        self.iou_calculator = build_iou_calculator(iou_calculator)\n\n    def get_gt_priorities(self, gt_bboxes):\n        """"""Get gt priorities according to their areas.\n\n        Smaller gt has higher priority.\n\n        Args:\n            gt_bboxes (Tensor): Ground truth boxes, shape (k, 4).\n\n        Returns:\n            Tensor: The priority of gts so that gts with larger priority is\n              more likely to be assigned. Shape (k, )\n\n        """"""\n        gt_areas = bboxes_area(gt_bboxes)\n        # Rank all gt bbox areas. Smaller objects has larger priority\n        _, sort_idx = gt_areas.sort(descending=True)\n        return sort_idx\n\n    def assign(self, bboxes, gt_bboxes, gt_bboxes_ignore=None, gt_labels=None):\n        """"""Assign gt to bboxes.\n\n        This method assigns gts to every bbox (proposal/anchor), each bbox will\n         be assigned with -1, or a semi-positive number. -1 means negative\n         sample, semi-positive number is the index (0-based) of assigned gt.\n\n        Args:\n            bboxes (Tensor): Bounding boxes to be assigned, shape(n, 4).\n            gt_bboxes (Tensor): Groundtruth boxes, shape (k, 4).\n            gt_bboxes_ignore (tensor, optional): Ground truth bboxes that are\n              labelled as `ignored`, e.g., crowd boxes in COCO.\n            gt_labels (tensor, optional): Label of gt_bboxes, shape (num_gts,).\n\n        Returns:\n            :obj:`AssignResult`: The assigned result. Note that shadowed_labels\n              of shape (N, 2) is also added as an `assign_result` attribute.\n              `shadowed_labels` is a tensor composed of N pairs of\n              [anchor_ind, class_label], where N is the number of anchors that\n              lie in the outer region of a gt, anchor_ind is the shadowed\n              anchor index and class_label is the shadowed class label.\n\n        Example:\n            >>> self = CenterRegionAssigner(0.2, 0.2)\n            >>> bboxes = torch.Tensor([[0, 0, 10, 10], [10, 10, 20, 20]])\n            >>> gt_bboxes = torch.Tensor([[0, 0, 10, 10]])\n            >>> assign_result = self.assign(bboxes, gt_bboxes)\n            >>> expected_gt_inds = torch.LongTensor([1, 0])\n            >>> assert torch.all(assign_result.gt_inds == expected_gt_inds)\n\n        """"""\n        # There are in total 5 steps in the pixel assignment\n        # 1. Find core (the center region, say inner 0.2)\n        #     and shadow (the relatively ourter part, say inner 0.2-0.5)\n        #     regions of every gt.\n        # 2. Find all prior bboxes that lie in gt_core and gt_shadow regions\n        # 3. Assign prior bboxes in gt_core with a one-hot id of the gt in\n        #      the image.\n        #    3.1. For overlapping objects, the prior bboxes in gt_core is\n        #           assigned with the object with smallest area\n        # 4. Assign prior bboxes with class label according to its gt id.\n        #    4.1. Assign -1 to prior bboxes lying in shadowed gts\n        #    4.2. Assign positive prior boxes with the corresponding label\n        # 5. Find pixels lying in the shadow of an object and assign them with\n        #      background label, but set the loss weight of its corresponding\n        #      gt to zero.\n        assert bboxes.size(1) == 4, \'bboxes must have size of 4\'\n        # 1. Find core positive and shadow region of every gt\n        gt_core = scale_boxes(gt_bboxes, self.pos_scale)\n        gt_shadow = scale_boxes(gt_bboxes, self.neg_scale)\n\n        # 2. Find prior bboxes that lie in gt_core and gt_shadow regions\n        bbox_centers = (bboxes[:, 2:4] + bboxes[:, 0:2]) / 2\n        # The center points lie within the gt boxes\n        is_bbox_in_gt = is_located_in(bbox_centers, gt_bboxes)\n        # Only calculate bbox and gt_core IoF. This enables small prior bboxes\n        #   to match large gts\n        bbox_and_gt_core_overlaps = self.iou_calculator(\n            bboxes, gt_core, mode=\'iof\')\n        # The center point of effective priors should be within the gt box\n        is_bbox_in_gt_core = is_bbox_in_gt & (\n            bbox_and_gt_core_overlaps > self.min_pos_iof)  # shape (n, k)\n\n        is_bbox_in_gt_shadow = (\n            self.iou_calculator(bboxes, gt_shadow, mode=\'iof\') >\n            self.min_pos_iof)\n        # Rule out center effective positive pixels\n        is_bbox_in_gt_shadow &= (~is_bbox_in_gt_core)\n\n        num_gts, num_bboxes = gt_bboxes.size(0), bboxes.size(0)\n        if num_gts == 0 or num_bboxes == 0:\n            # If no gts exist, assign all pixels to negative\n            assigned_gt_ids = \\\n                is_bbox_in_gt_core.new_zeros((num_bboxes,),\n                                             dtype=torch.long)\n            pixels_in_gt_shadow = assigned_gt_ids.new_empty((0, 2))\n        else:\n            # Step 3: assign a one-hot gt id to each pixel, and smaller objects\n            #    have high priority to assign the pixel.\n            sort_idx = self.get_gt_priorities(gt_bboxes)\n            assigned_gt_ids, pixels_in_gt_shadow = \\\n                self.assign_one_hot_gt_indices(is_bbox_in_gt_core,\n                                               is_bbox_in_gt_shadow,\n                                               gt_priority=sort_idx)\n\n        if gt_bboxes_ignore is not None and gt_bboxes_ignore.numel() > 0:\n            # No ground truth or boxes, return empty assignment\n            gt_bboxes_ignore = scale_boxes(\n                gt_bboxes_ignore, scale=self.ignore_gt_scale)\n            is_bbox_in_ignored_gts = is_located_in(bbox_centers,\n                                                   gt_bboxes_ignore)\n            is_bbox_in_ignored_gts = is_bbox_in_ignored_gts.any(dim=1)\n            assigned_gt_ids[is_bbox_in_ignored_gts] = -1\n\n        # 4. Assign prior bboxes with class label according to its gt id.\n        assigned_labels = None\n        shadowed_pixel_labels = None\n        if gt_labels is not None:\n            # Default assigned label is the background (-1)\n            assigned_labels = assigned_gt_ids.new_full((num_bboxes, ), -1)\n            pos_inds = torch.nonzero(\n                assigned_gt_ids > 0, as_tuple=False).squeeze()\n            if pos_inds.numel() > 0:\n                assigned_labels[pos_inds] = gt_labels[assigned_gt_ids[pos_inds]\n                                                      - 1]\n            # 5. Find pixels lying in the shadow of an object\n            shadowed_pixel_labels = pixels_in_gt_shadow.clone()\n            if pixels_in_gt_shadow.numel() > 0:\n                pixel_idx, gt_idx =\\\n                    pixels_in_gt_shadow[:, 0], pixels_in_gt_shadow[:, 1]\n                assert (assigned_gt_ids[pixel_idx] != gt_idx).all(), \\\n                    \'Some pixels are dually assigned to ignore and gt!\'\n                shadowed_pixel_labels[:, 1] = gt_labels[gt_idx - 1]\n                # When a pixel is both positive and shadowed, set it as shadow.\n                override = (\n                    assigned_labels[pixel_idx] == shadowed_pixel_labels[:, 1])\n                assigned_labels[pixel_idx[override]] = -1\n                assigned_gt_ids[pixel_idx[override]] = 0\n\n        assign_result = AssignResult(\n            num_gts, assigned_gt_ids, None, labels=assigned_labels)\n        # Add shadowed_labels as assign_result property. Shape: (num_shadow, 2)\n        assign_result.set_extra_property(\'shadowed_labels\',\n                                         shadowed_pixel_labels)\n        return assign_result\n\n    def assign_one_hot_gt_indices(self,\n                                  is_bbox_in_gt_core,\n                                  is_bbox_in_gt_shadow,\n                                  gt_priority=None):\n        """"""Assign only one gt index to each prior box\n\n        Gts with large gt_priority are more likely to be assigned.\n\n        Args:\n            is_bbox_in_gt_core (Tensor): Bool tensor indicating the bbox center\n              is in the core area of a gt (e.g. 0-0.2).\n              Shape: (num_prior, num_gt).\n            is_bbox_in_gt_shadow (Tensor): Bool tensor indicating the bbox\n              center is in the shadowed area of a gt (e.g. 0.2-0.5).\n              Shape: (num_prior, num_gt).\n            gt_priority (Tensor): Priorities of gts. The gt with a higher\n              priority is more likely to be assigned to the bbox when the bbox\n              match with multiple gts. Shape: (num_gt, ).\n\n        Returns:\n            assigned_gt_inds: The assigned gt index of each prior bbox\n              (i.e. index from 1 to num_gts). Shape: (num_prior, ).\n            shadowed_gt_inds: shadowed gt indices. It is a tensor of shape\n              (num_ignore, 2) with first column being the shadowed prior bbox\n              indices and the second column the shadowed gt indices (1-based)\n        """"""\n        num_bboxes, num_gts = is_bbox_in_gt_core.shape\n\n        if gt_priority is None:\n            gt_priority = torch.arange(\n                num_gts, device=is_bbox_in_gt_core.device)\n        assert gt_priority.size(0) == num_gts\n        # The bigger gt_priority, the more preferable to be assigned\n        # The assigned inds are by default 0 (background)\n        assigned_gt_inds = is_bbox_in_gt_core.new_zeros((num_bboxes, ),\n                                                        dtype=torch.long)\n        # Shadowed bboxes are assigned to be background. But the corresponding\n        #   label is ignored during loss calculation, which is done through\n        #   shadowed_gt_inds\n        shadowed_gt_inds = torch.nonzero(is_bbox_in_gt_shadow, as_tuple=False)\n        if is_bbox_in_gt_core.sum() == 0:  # No gt match\n            shadowed_gt_inds[:, 1] += 1  # 1-based. For consistency issue\n            return assigned_gt_inds, shadowed_gt_inds\n\n        # The priority of each prior box and gt pair. If one prior box is\n        #  matched bo multiple gts. Only the pair with the highest priority\n        #  is saved\n        pair_priority = is_bbox_in_gt_core.new_full((num_bboxes, num_gts),\n                                                    -1,\n                                                    dtype=torch.long)\n\n        # Each bbox could match with multiple gts.\n        # The following codes deal with this situation\n        # Matched  bboxes (to any gt). Shape: (num_pos_anchor, )\n        inds_of_match = torch.any(is_bbox_in_gt_core, dim=1)\n        # The matched gt index of each positive bbox. Length >= num_pos_anchor\n        #   , since one bbox could match multiple gts\n        matched_bbox_gt_inds = torch.nonzero(\n            is_bbox_in_gt_core, as_tuple=False)[:, 1]\n        # Assign priority to each bbox-gt pair.\n        pair_priority[is_bbox_in_gt_core] = gt_priority[matched_bbox_gt_inds]\n        _, argmax_priority = pair_priority[inds_of_match].max(dim=1)\n        assigned_gt_inds[inds_of_match] = argmax_priority + 1  # 1-based\n        # Zero-out the assigned anchor box to filter the shadowed gt indices\n        is_bbox_in_gt_core[inds_of_match, argmax_priority] = 0\n        # Concat the shadowed indices due to overlapping with that out side of\n        #   effective scale. shape: (total_num_ignore, 2)\n        shadowed_gt_inds = torch.cat(\n            (shadowed_gt_inds, torch.nonzero(\n                is_bbox_in_gt_core, as_tuple=False)),\n            dim=0)\n        # `is_bbox_in_gt_core` should be changed back to keep arguments intact.\n        is_bbox_in_gt_core[inds_of_match, argmax_priority] = 1\n        # 1-based shadowed gt indices, to be consistent with `assigned_gt_inds`\n        shadowed_gt_inds[:, 1] += 1\n        return assigned_gt_inds, shadowed_gt_inds\n'"
mmdet/core/bbox/assigners/max_iou_assigner.py,7,"b'import torch\n\nfrom ..builder import BBOX_ASSIGNERS\nfrom ..iou_calculators import build_iou_calculator\nfrom .assign_result import AssignResult\nfrom .base_assigner import BaseAssigner\n\n\n@BBOX_ASSIGNERS.register_module()\nclass MaxIoUAssigner(BaseAssigner):\n    """"""Assign a corresponding gt bbox or background to each bbox.\n\n    Each proposals will be assigned with `-1`, or a semi-positive integer\n    indicating the ground truth index.\n\n    - -1: negative sample, no assigned gt\n    - semi-positive integer: positive sample, index (0-based) of assigned gt\n\n    Args:\n        pos_iou_thr (float): IoU threshold for positive bboxes.\n        neg_iou_thr (float or tuple): IoU threshold for negative bboxes.\n        min_pos_iou (float): Minimum iou for a bbox to be considered as a\n            positive bbox. Positive samples can have smaller IoU than\n            pos_iou_thr due to the 4th step (assign max IoU sample to each gt).\n        gt_max_assign_all (bool): Whether to assign all bboxes with the same\n            highest overlap with some gt to that gt.\n        ignore_iof_thr (float): IoF threshold for ignoring bboxes (if\n            `gt_bboxes_ignore` is specified). Negative values mean not\n            ignoring any bboxes.\n        ignore_wrt_candidates (bool): Whether to compute the iof between\n            `bboxes` and `gt_bboxes_ignore`, or the contrary.\n        match_low_quality (bool): Whether to allow low quality matches. This is\n            usually allowed for RPN and single stage detectors, but not allowed\n            in the second stage. Details are demonetrated in Step 4.\n        gpu_assign_thr (int): The upper bound of the number of GT for GPU\n            assign. When the number of gt is above this threshold, will assign\n            on CPU device. Negative values mean not assign on CPU.\n    """"""\n\n    def __init__(self,\n                 pos_iou_thr,\n                 neg_iou_thr,\n                 min_pos_iou=.0,\n                 gt_max_assign_all=True,\n                 ignore_iof_thr=-1,\n                 ignore_wrt_candidates=True,\n                 match_low_quality=True,\n                 gpu_assign_thr=-1,\n                 iou_calculator=dict(type=\'BboxOverlaps2D\')):\n        self.pos_iou_thr = pos_iou_thr\n        self.neg_iou_thr = neg_iou_thr\n        self.min_pos_iou = min_pos_iou\n        self.gt_max_assign_all = gt_max_assign_all\n        self.ignore_iof_thr = ignore_iof_thr\n        self.ignore_wrt_candidates = ignore_wrt_candidates\n        self.gpu_assign_thr = gpu_assign_thr\n        self.match_low_quality = match_low_quality\n        self.iou_calculator = build_iou_calculator(iou_calculator)\n\n    def assign(self, bboxes, gt_bboxes, gt_bboxes_ignore=None, gt_labels=None):\n        """"""Assign gt to bboxes.\n\n        This method assign a gt bbox to every bbox (proposal/anchor), each bbox\n        will be assigned with -1, or a semi-positive number. -1 means negative\n        sample, semi-positive number is the index (0-based) of assigned gt.\n        The assignment is done in following steps, the order matters.\n\n        1. assign every bbox to the background\n        2. assign proposals whose iou with all gts < neg_iou_thr to 0\n        3. for each bbox, if the iou with its nearest gt >= pos_iou_thr,\n           assign it to that bbox\n        4. for each gt bbox, assign its nearest proposals (may be more than\n           one) to itself\n\n        Args:\n            bboxes (Tensor): Bounding boxes to be assigned, shape(n, 4).\n            gt_bboxes (Tensor): Groundtruth boxes, shape (k, 4).\n            gt_bboxes_ignore (Tensor, optional): Ground truth bboxes that are\n                labelled as `ignored`, e.g., crowd boxes in COCO.\n            gt_labels (Tensor, optional): Label of gt_bboxes, shape (k, ).\n\n        Returns:\n            :obj:`AssignResult`: The assign result.\n\n        Example:\n            >>> self = MaxIoUAssigner(0.5, 0.5)\n            >>> bboxes = torch.Tensor([[0, 0, 10, 10], [10, 10, 20, 20]])\n            >>> gt_bboxes = torch.Tensor([[0, 0, 10, 9]])\n            >>> assign_result = self.assign(bboxes, gt_bboxes)\n            >>> expected_gt_inds = torch.LongTensor([1, 0])\n            >>> assert torch.all(assign_result.gt_inds == expected_gt_inds)\n        """"""\n        assign_on_cpu = True if (self.gpu_assign_thr > 0) and (\n            gt_bboxes.shape[0] > self.gpu_assign_thr) else False\n        # compute overlap and assign gt on CPU when number of GT is large\n        if assign_on_cpu:\n            device = bboxes.device\n            bboxes = bboxes.cpu()\n            gt_bboxes = gt_bboxes.cpu()\n            if gt_bboxes_ignore is not None:\n                gt_bboxes_ignore = gt_bboxes_ignore.cpu()\n            if gt_labels is not None:\n                gt_labels = gt_labels.cpu()\n\n        overlaps = self.iou_calculator(gt_bboxes, bboxes)\n\n        if (self.ignore_iof_thr > 0 and gt_bboxes_ignore is not None\n                and gt_bboxes_ignore.numel() > 0 and bboxes.numel() > 0):\n            if self.ignore_wrt_candidates:\n                ignore_overlaps = self.iou_calculator(\n                    bboxes, gt_bboxes_ignore, mode=\'iof\')\n                ignore_max_overlaps, _ = ignore_overlaps.max(dim=1)\n            else:\n                ignore_overlaps = self.iou_calculator(\n                    gt_bboxes_ignore, bboxes, mode=\'iof\')\n                ignore_max_overlaps, _ = ignore_overlaps.max(dim=0)\n            overlaps[:, ignore_max_overlaps > self.ignore_iof_thr] = -1\n\n        assign_result = self.assign_wrt_overlaps(overlaps, gt_labels)\n        if assign_on_cpu:\n            assign_result.gt_inds = assign_result.gt_inds.to(device)\n            assign_result.max_overlaps = assign_result.max_overlaps.to(device)\n            if assign_result.labels is not None:\n                assign_result.labels = assign_result.labels.to(device)\n        return assign_result\n\n    def assign_wrt_overlaps(self, overlaps, gt_labels=None):\n        """"""Assign w.r.t. the overlaps of bboxes with gts.\n\n        Args:\n            overlaps (Tensor): Overlaps between k gt_bboxes and n bboxes,\n                shape(k, n).\n            gt_labels (Tensor, optional): Labels of k gt_bboxes, shape (k, ).\n\n        Returns:\n            :obj:`AssignResult`: The assign result.\n        """"""\n        num_gts, num_bboxes = overlaps.size(0), overlaps.size(1)\n\n        # 1. assign -1 by default\n        assigned_gt_inds = overlaps.new_full((num_bboxes, ),\n                                             -1,\n                                             dtype=torch.long)\n\n        if num_gts == 0 or num_bboxes == 0:\n            # No ground truth or boxes, return empty assignment\n            max_overlaps = overlaps.new_zeros((num_bboxes, ))\n            if num_gts == 0:\n                # No truth, assign everything to background\n                assigned_gt_inds[:] = 0\n            if gt_labels is None:\n                assigned_labels = None\n            else:\n                assigned_labels = overlaps.new_full((num_bboxes, ),\n                                                    -1,\n                                                    dtype=torch.long)\n            return AssignResult(\n                num_gts,\n                assigned_gt_inds,\n                max_overlaps,\n                labels=assigned_labels)\n\n        # for each anchor, which gt best overlaps with it\n        # for each anchor, the max iou of all gts\n        max_overlaps, argmax_overlaps = overlaps.max(dim=0)\n        # for each gt, which anchor best overlaps with it\n        # for each gt, the max iou of all proposals\n        gt_max_overlaps, gt_argmax_overlaps = overlaps.max(dim=1)\n\n        # 2. assign negative: below\n        # the negative inds are set to be 0\n        if isinstance(self.neg_iou_thr, float):\n            assigned_gt_inds[(max_overlaps >= 0)\n                             & (max_overlaps < self.neg_iou_thr)] = 0\n        elif isinstance(self.neg_iou_thr, tuple):\n            assert len(self.neg_iou_thr) == 2\n            assigned_gt_inds[(max_overlaps >= self.neg_iou_thr[0])\n                             & (max_overlaps < self.neg_iou_thr[1])] = 0\n\n        # 3. assign positive: above positive IoU threshold\n        pos_inds = max_overlaps >= self.pos_iou_thr\n        assigned_gt_inds[pos_inds] = argmax_overlaps[pos_inds] + 1\n\n        if self.match_low_quality:\n            # Low-quality matching will overwirte the assigned_gt_inds assigned\n            # in Step 3. Thus, the assigned gt might not be the best one for\n            # prediction.\n            # For example, if bbox A has 0.9 and 0.8 iou with GT bbox 1 & 2,\n            # bbox 1 will be assigned as the best target for bbox A in step 3.\n            # However, if GT bbox 2\'s gt_argmax_overlaps = A, bbox A\'s\n            # assigned_gt_inds will be overwritten to be bbox B.\n            # This might be the reason that it is not used in ROI Heads.\n            for i in range(num_gts):\n                if gt_max_overlaps[i] >= self.min_pos_iou:\n                    if self.gt_max_assign_all:\n                        max_iou_inds = overlaps[i, :] == gt_max_overlaps[i]\n                        assigned_gt_inds[max_iou_inds] = i + 1\n                    else:\n                        assigned_gt_inds[gt_argmax_overlaps[i]] = i + 1\n\n        if gt_labels is not None:\n            assigned_labels = assigned_gt_inds.new_full((num_bboxes, ), -1)\n            pos_inds = torch.nonzero(\n                assigned_gt_inds > 0, as_tuple=False).squeeze()\n            if pos_inds.numel() > 0:\n                assigned_labels[pos_inds] = gt_labels[\n                    assigned_gt_inds[pos_inds] - 1]\n        else:\n            assigned_labels = None\n\n        return AssignResult(\n            num_gts, assigned_gt_inds, max_overlaps, labels=assigned_labels)\n'"
mmdet/core/bbox/assigners/point_assigner.py,10,"b'import torch\n\nfrom ..builder import BBOX_ASSIGNERS\nfrom .assign_result import AssignResult\nfrom .base_assigner import BaseAssigner\n\n\n@BBOX_ASSIGNERS.register_module()\nclass PointAssigner(BaseAssigner):\n    """"""Assign a corresponding gt bbox or background to each point.\n\n    Each proposals will be assigned with `0`, or a positive integer\n    indicating the ground truth index.\n\n    - 0: negative sample, no assigned gt\n    - positive integer: positive sample, index (1-based) of assigned gt\n\n    """"""\n\n    def __init__(self, scale=4, pos_num=3):\n        self.scale = scale\n        self.pos_num = pos_num\n\n    def assign(self, points, gt_bboxes, gt_bboxes_ignore=None, gt_labels=None):\n        """"""Assign gt to points.\n\n        This method assign a gt bbox to every points set, each points set\n        will be assigned with  the background_label (-1), or a label number.\n        -1 is background, and semi-positive number is the index (0-based) of\n        assigned gt.\n        The assignment is done in following steps, the order matters.\n\n        1. assign every points to the background_label (-1)\n        2. A point is assigned to some gt bbox if\n            (i) the point is within the k closest points to the gt bbox\n            (ii) the distance between this point and the gt is smaller than\n                other gt bboxes\n\n        Args:\n            points (Tensor): points to be assigned, shape(n, 3) while last\n                dimension stands for (x, y, stride).\n            gt_bboxes (Tensor): Groundtruth boxes, shape (k, 4).\n            gt_bboxes_ignore (Tensor, optional): Ground truth bboxes that are\n                labelled as `ignored`, e.g., crowd boxes in COCO.\n                NOTE: currently unused.\n            gt_labels (Tensor, optional): Label of gt_bboxes, shape (k, ).\n\n        Returns:\n            :obj:`AssignResult`: The assign result.\n        """"""\n        num_points = points.shape[0]\n        num_gts = gt_bboxes.shape[0]\n\n        if num_gts == 0 or num_points == 0:\n            # If no truth assign everything to the background\n            assigned_gt_inds = points.new_full((num_points, ),\n                                               0,\n                                               dtype=torch.long)\n            if gt_labels is None:\n                assigned_labels = None\n            else:\n                assigned_labels = points.new_full((num_points, ),\n                                                  -1,\n                                                  dtype=torch.long)\n            return AssignResult(\n                num_gts, assigned_gt_inds, None, labels=assigned_labels)\n\n        points_xy = points[:, :2]\n        points_stride = points[:, 2]\n        points_lvl = torch.log2(\n            points_stride).int()  # [3...,4...,5...,6...,7...]\n        lvl_min, lvl_max = points_lvl.min(), points_lvl.max()\n\n        # assign gt box\n        gt_bboxes_xy = (gt_bboxes[:, :2] + gt_bboxes[:, 2:]) / 2\n        gt_bboxes_wh = (gt_bboxes[:, 2:] - gt_bboxes[:, :2]).clamp(min=1e-6)\n        scale = self.scale\n        gt_bboxes_lvl = ((torch.log2(gt_bboxes_wh[:, 0] / scale) +\n                          torch.log2(gt_bboxes_wh[:, 1] / scale)) / 2).int()\n        gt_bboxes_lvl = torch.clamp(gt_bboxes_lvl, min=lvl_min, max=lvl_max)\n\n        # stores the assigned gt index of each point\n        assigned_gt_inds = points.new_zeros((num_points, ), dtype=torch.long)\n        # stores the assigned gt dist (to this point) of each point\n        assigned_gt_dist = points.new_full((num_points, ), float(\'inf\'))\n        points_range = torch.arange(points.shape[0])\n\n        for idx in range(num_gts):\n            gt_lvl = gt_bboxes_lvl[idx]\n            # get the index of points in this level\n            lvl_idx = gt_lvl == points_lvl\n            points_index = points_range[lvl_idx]\n            # get the points in this level\n            lvl_points = points_xy[lvl_idx, :]\n            # get the center point of gt\n            gt_point = gt_bboxes_xy[[idx], :]\n            # get width and height of gt\n            gt_wh = gt_bboxes_wh[[idx], :]\n            # compute the distance between gt center and\n            #   all points in this level\n            points_gt_dist = ((lvl_points - gt_point) / gt_wh).norm(dim=1)\n            # find the nearest k points to gt center in this level\n            min_dist, min_dist_index = torch.topk(\n                points_gt_dist, self.pos_num, largest=False)\n            # the index of nearest k points to gt center in this level\n            min_dist_points_index = points_index[min_dist_index]\n            # The less_than_recorded_index stores the index\n            #   of min_dist that is less then the assigned_gt_dist. Where\n            #   assigned_gt_dist stores the dist from previous assigned gt\n            #   (if exist) to each point.\n            less_than_recorded_index = min_dist < assigned_gt_dist[\n                min_dist_points_index]\n            # The min_dist_points_index stores the index of points satisfy:\n            #   (1) it is k nearest to current gt center in this level.\n            #   (2) it is closer to current gt center than other gt center.\n            min_dist_points_index = min_dist_points_index[\n                less_than_recorded_index]\n            # assign the result\n            assigned_gt_inds[min_dist_points_index] = idx + 1\n            assigned_gt_dist[min_dist_points_index] = min_dist[\n                less_than_recorded_index]\n\n        if gt_labels is not None:\n            assigned_labels = assigned_gt_inds.new_full((num_points, ), -1)\n            pos_inds = torch.nonzero(\n                assigned_gt_inds > 0, as_tuple=False).squeeze()\n            if pos_inds.numel() > 0:\n                assigned_labels[pos_inds] = gt_labels[\n                    assigned_gt_inds[pos_inds] - 1]\n        else:\n            assigned_labels = None\n\n        return AssignResult(\n            num_gts, assigned_gt_inds, None, labels=assigned_labels)\n'"
mmdet/core/bbox/coder/__init__.py,0,"b""from .base_bbox_coder import BaseBBoxCoder\nfrom .delta_xywh_bbox_coder import DeltaXYWHBBoxCoder\nfrom .legacy_delta_xywh_bbox_coder import LegacyDeltaXYWHBBoxCoder\nfrom .pseudo_bbox_coder import PseudoBBoxCoder\nfrom .tblr_bbox_coder import TBLRBBoxCoder\n\n__all__ = [\n    'BaseBBoxCoder', 'PseudoBBoxCoder', 'DeltaXYWHBBoxCoder',\n    'LegacyDeltaXYWHBBoxCoder', 'TBLRBBoxCoder'\n]\n"""
mmdet/core/bbox/coder/base_bbox_coder.py,0,"b'from abc import ABCMeta, abstractmethod\n\n\nclass BaseBBoxCoder(metaclass=ABCMeta):\n\n    def __init__(self, **kwargs):\n        pass\n\n    @abstractmethod\n    def encode(self, bboxes, gt_bboxes):\n        pass\n\n    @abstractmethod\n    def decode(self, bboxes, bboxes_pred):\n        pass\n'"
mmdet/core/bbox/coder/delta_xywh_bbox_coder.py,6,"b'import numpy as np\nimport torch\n\nfrom ..builder import BBOX_CODERS\nfrom .base_bbox_coder import BaseBBoxCoder\n\n\n@BBOX_CODERS.register_module()\nclass DeltaXYWHBBoxCoder(BaseBBoxCoder):\n    """"""Delta XYWH BBox coder\n\n    Following the practice in `R-CNN <https://arxiv.org/abs/1311.2524>`_,\n    this coder encodes bbox (x1, y1, x2, y2) into delta (dx, dy, dw, dh) and\n    decodes delta (dx, dy, dw, dh) back to original bbox (x1, y1, x2, y2).\n\n    Args:\n        target_means (Sequence[float]): denormalizing means of target for\n            delta coordinates\n        target_stds (Sequence[float]): denormalizing standard deviation of\n            target for delta coordinates\n    """"""\n\n    def __init__(self,\n                 target_means=(0., 0., 0., 0.),\n                 target_stds=(1., 1., 1., 1.)):\n        super(BaseBBoxCoder, self).__init__()\n        self.means = target_means\n        self.stds = target_stds\n\n    def encode(self, bboxes, gt_bboxes):\n        assert bboxes.size(0) == gt_bboxes.size(0)\n        assert bboxes.size(-1) == gt_bboxes.size(-1) == 4\n        encoded_bboxes = bbox2delta(bboxes, gt_bboxes, self.means, self.stds)\n        return encoded_bboxes\n\n    def decode(self,\n               bboxes,\n               pred_bboxes,\n               max_shape=None,\n               wh_ratio_clip=16 / 1000):\n        assert pred_bboxes.size(0) == bboxes.size(0)\n        decoded_bboxes = delta2bbox(bboxes, pred_bboxes, self.means, self.stds,\n                                    max_shape, wh_ratio_clip)\n\n        return decoded_bboxes\n\n\ndef bbox2delta(proposals, gt, means=(0., 0., 0., 0.), stds=(1., 1., 1., 1.)):\n    """"""Compute deltas of proposals w.r.t. gt.\n\n    We usually compute the deltas of x, y, w, h of proposals w.r.t ground\n    truth bboxes to get regression target.\n    This is the inverse function of `delta2bbox()`\n\n    Args:\n        proposals (Tensor): Boxes to be transformed, shape (N, ..., 4)\n        gt (Tensor): Gt bboxes to be used as base, shape (N, ..., 4)\n        means (Sequence[float]): Denormalizing means for delta coordinates\n        stds (Sequence[float]): Denormalizing standard deviation for delta\n            coordinates\n\n    Returns:\n        Tensor: deltas with shape (N, 4), where columns represent dx, dy,\n            dw, dh.\n\n    """"""\n    assert proposals.size() == gt.size()\n\n    proposals = proposals.float()\n    gt = gt.float()\n    px = (proposals[..., 0] + proposals[..., 2]) * 0.5\n    py = (proposals[..., 1] + proposals[..., 3]) * 0.5\n    pw = proposals[..., 2] - proposals[..., 0]\n    ph = proposals[..., 3] - proposals[..., 1]\n\n    gx = (gt[..., 0] + gt[..., 2]) * 0.5\n    gy = (gt[..., 1] + gt[..., 3]) * 0.5\n    gw = gt[..., 2] - gt[..., 0]\n    gh = gt[..., 3] - gt[..., 1]\n\n    dx = (gx - px) / pw\n    dy = (gy - py) / ph\n    dw = torch.log(gw / pw)\n    dh = torch.log(gh / ph)\n    deltas = torch.stack([dx, dy, dw, dh], dim=-1)\n\n    means = deltas.new_tensor(means).unsqueeze(0)\n    stds = deltas.new_tensor(stds).unsqueeze(0)\n    deltas = deltas.sub_(means).div_(stds)\n\n    return deltas\n\n\ndef delta2bbox(rois,\n               deltas,\n               means=(0., 0., 0., 0.),\n               stds=(1., 1., 1., 1.),\n               max_shape=None,\n               wh_ratio_clip=16 / 1000):\n    """"""Apply deltas to shift/scale base boxes.\n\n    Typically the rois are anchor or proposed bounding boxes and the deltas are\n    network outputs used to shift/scale those boxes.\n    This is the inverse function of `bbox2delta()`\n\n    Args:\n        rois (Tensor): Boxes to be transformed. Has shape (N, 4)\n        deltas (Tensor): Encoded offsets with respect to each roi.\n            Has shape (N, 4 * num_classes). Note N = num_anchors * W * H when\n            rois is a grid of anchors. Offset encoding follows [1]_.\n        means (Sequence[float]): Denormalizing means for delta coordinates\n        stds (Sequence[float]): Denormalizing standard deviation for delta\n            coordinates\n        max_shape (tuple[int, int]): Maximum bounds for boxes. specifies (H, W)\n        wh_ratio_clip (float): Maximum aspect ratio for boxes.\n\n    Returns:\n        Tensor: Boxes with shape (N, 4), where columns represent\n            tl_x, tl_y, br_x, br_y.\n\n    References:\n        .. [1] https://arxiv.org/abs/1311.2524\n\n    Example:\n        >>> rois = torch.Tensor([[ 0.,  0.,  1.,  1.],\n        >>>                      [ 0.,  0.,  1.,  1.],\n        >>>                      [ 0.,  0.,  1.,  1.],\n        >>>                      [ 5.,  5.,  5.,  5.]])\n        >>> deltas = torch.Tensor([[  0.,   0.,   0.,   0.],\n        >>>                        [  1.,   1.,   1.,   1.],\n        >>>                        [  0.,   0.,   2.,  -1.],\n        >>>                        [ 0.7, -1.9, -0.5,  0.3]])\n        >>> delta2bbox(rois, deltas, max_shape=(32, 32))\n        tensor([[0.0000, 0.0000, 1.0000, 1.0000],\n                [0.1409, 0.1409, 2.8591, 2.8591],\n                [0.0000, 0.3161, 4.1945, 0.6839],\n                [5.0000, 5.0000, 5.0000, 5.0000]])\n    """"""\n    means = deltas.new_tensor(means).repeat(1, deltas.size(1) // 4)\n    stds = deltas.new_tensor(stds).repeat(1, deltas.size(1) // 4)\n    denorm_deltas = deltas * stds + means\n    dx = denorm_deltas[:, 0::4]\n    dy = denorm_deltas[:, 1::4]\n    dw = denorm_deltas[:, 2::4]\n    dh = denorm_deltas[:, 3::4]\n    max_ratio = np.abs(np.log(wh_ratio_clip))\n    dw = dw.clamp(min=-max_ratio, max=max_ratio)\n    dh = dh.clamp(min=-max_ratio, max=max_ratio)\n    # Compute center of each roi\n    px = ((rois[:, 0] + rois[:, 2]) * 0.5).unsqueeze(1).expand_as(dx)\n    py = ((rois[:, 1] + rois[:, 3]) * 0.5).unsqueeze(1).expand_as(dy)\n    # Compute width/height of each roi\n    pw = (rois[:, 2] - rois[:, 0]).unsqueeze(1).expand_as(dw)\n    ph = (rois[:, 3] - rois[:, 1]).unsqueeze(1).expand_as(dh)\n    # Use exp(network energy) to enlarge/shrink each roi\n    gw = pw * dw.exp()\n    gh = ph * dh.exp()\n    # Use network energy to shift the center of each roi\n    gx = px + pw * dx\n    gy = py + ph * dy\n    # Convert center-xy/width/height to top-left, bottom-right\n    x1 = gx - gw * 0.5\n    y1 = gy - gh * 0.5\n    x2 = gx + gw * 0.5\n    y2 = gy + gh * 0.5\n    if max_shape is not None:\n        x1 = x1.clamp(min=0, max=max_shape[1])\n        y1 = y1.clamp(min=0, max=max_shape[0])\n        x2 = x2.clamp(min=0, max=max_shape[1])\n        y2 = y2.clamp(min=0, max=max_shape[0])\n    bboxes = torch.stack([x1, y1, x2, y2], dim=-1).view_as(deltas)\n    return bboxes\n'"
mmdet/core/bbox/coder/legacy_delta_xywh_bbox_coder.py,6,"b'import numpy as np\nimport torch\n\nfrom ..builder import BBOX_CODERS\nfrom .base_bbox_coder import BaseBBoxCoder\n\n\n@BBOX_CODERS.register_module()\nclass LegacyDeltaXYWHBBoxCoder(BaseBBoxCoder):\n    """"""Legacy Delta XYWH BBox coder used in MMDet V1.x\n\n    Following the practice in R-CNN [1]_, this coder encodes bbox (x1, y1, x2,\n    y2) into delta (dx, dy, dw, dh) and decodes delta (dx, dy, dw, dh)\n    back to original bbox (x1, y1, x2, y2).\n\n    Note:\n        The main difference between `LegacyDeltaXYWHBBoxCoder` and\n        `DeltaXYWHBBoxCoder` is whether ``+ 1`` is used during width and height\n        calculation. We suggest to only use this coder when testing with\n        MMDet V1.x models.\n\n    References:\n        .. [1] https://arxiv.org/abs/1311.2524\n\n    Args:\n        target_means (Sequence[float]): denormalizing means of target for\n            delta coordinates\n        target_stds (Sequence[float]): denormalizing standard deviation of\n            target for delta coordinates\n    """"""\n\n    def __init__(self,\n                 target_means=(0., 0., 0., 0.),\n                 target_stds=(1., 1., 1., 1.)):\n        super(BaseBBoxCoder, self).__init__()\n        self.means = target_means\n        self.stds = target_stds\n\n    def encode(self, bboxes, gt_bboxes):\n        assert bboxes.size(0) == gt_bboxes.size(0)\n        assert bboxes.size(-1) == gt_bboxes.size(-1) == 4\n        encoded_bboxes = legacy_bbox2delta(bboxes, gt_bboxes, self.means,\n                                           self.stds)\n        return encoded_bboxes\n\n    def decode(self,\n               bboxes,\n               pred_bboxes,\n               max_shape=None,\n               wh_ratio_clip=16 / 1000):\n        assert pred_bboxes.size(0) == bboxes.size(0)\n        decoded_bboxes = legacy_delta2bbox(bboxes, pred_bboxes, self.means,\n                                           self.stds, max_shape, wh_ratio_clip)\n\n        return decoded_bboxes\n\n\ndef legacy_bbox2delta(proposals,\n                      gt,\n                      means=(0., 0., 0., 0.),\n                      stds=(1., 1., 1., 1.)):\n    """"""Compute deltas of proposals w.r.t. gt in the MMDet V1.x manner.\n\n    We usually compute the deltas of x, y, w, h of proposals w.r.t ground\n    truth bboxes to get regression target.\n    This is the inverse function of `delta2bbox()`\n\n    Args:\n        proposals (Tensor): Boxes to be transformed, shape (N, ..., 4)\n        gt (Tensor): Gt bboxes to be used as base, shape (N, ..., 4)\n        means (Sequence[float]): Denormalizing means for delta coordinates\n        stds (Sequence[float]): Denormalizing standard deviation for delta\n            coordinates\n\n    Returns:\n        Tensor: deltas with shape (N, 4), where columns represent dx, dy,\n            dw, dh.\n\n    """"""\n    assert proposals.size() == gt.size()\n\n    proposals = proposals.float()\n    gt = gt.float()\n    px = (proposals[..., 0] + proposals[..., 2]) * 0.5\n    py = (proposals[..., 1] + proposals[..., 3]) * 0.5\n    pw = proposals[..., 2] - proposals[..., 0] + 1.0\n    ph = proposals[..., 3] - proposals[..., 1] + 1.0\n\n    gx = (gt[..., 0] + gt[..., 2]) * 0.5\n    gy = (gt[..., 1] + gt[..., 3]) * 0.5\n    gw = gt[..., 2] - gt[..., 0] + 1.0\n    gh = gt[..., 3] - gt[..., 1] + 1.0\n\n    dx = (gx - px) / pw\n    dy = (gy - py) / ph\n    dw = torch.log(gw / pw)\n    dh = torch.log(gh / ph)\n    deltas = torch.stack([dx, dy, dw, dh], dim=-1)\n\n    means = deltas.new_tensor(means).unsqueeze(0)\n    stds = deltas.new_tensor(stds).unsqueeze(0)\n    deltas = deltas.sub_(means).div_(stds)\n\n    return deltas\n\n\ndef legacy_delta2bbox(rois,\n                      deltas,\n                      means=(0., 0., 0., 0.),\n                      stds=(1., 1., 1., 1.),\n                      max_shape=None,\n                      wh_ratio_clip=16 / 1000):\n    """"""Apply deltas to shift/scale base boxes in the MMDet V1.x manner.\n\n    Typically the rois are anchor or proposed bounding boxes and the deltas are\n    network outputs used to shift/scale those boxes.\n    This is the inverse function of `bbox2delta()`\n\n    Args:\n        rois (Tensor): Boxes to be transformed. Has shape (N, 4)\n        deltas (Tensor): Encoded offsets with respect to each roi.\n            Has shape (N, 4 * num_classes). Note N = num_anchors * W * H when\n            rois is a grid of anchors. Offset encoding follows [1]_.\n        means (Sequence[float]): Denormalizing means for delta coordinates\n        stds (Sequence[float]): Denormalizing standard deviation for delta\n            coordinates\n        max_shape (tuple[int, int]): Maximum bounds for boxes. specifies (H, W)\n        wh_ratio_clip (float): Maximum aspect ratio for boxes.\n\n    Returns:\n        Tensor: Boxes with shape (N, 4), where columns represent\n            tl_x, tl_y, br_x, br_y.\n\n    References:\n        .. [1] https://arxiv.org/abs/1311.2524\n\n    Example:\n        >>> rois = torch.Tensor([[ 0.,  0.,  1.,  1.],\n        >>>                      [ 0.,  0.,  1.,  1.],\n        >>>                      [ 0.,  0.,  1.,  1.],\n        >>>                      [ 5.,  5.,  5.,  5.]])\n        >>> deltas = torch.Tensor([[  0.,   0.,   0.,   0.],\n        >>>                        [  1.,   1.,   1.,   1.],\n        >>>                        [  0.,   0.,   2.,  -1.],\n        >>>                        [ 0.7, -1.9, -0.5,  0.3]])\n        >>> legacy_delta2bbox(rois, deltas, max_shape=(32, 32))\n        tensor([[0.0000, 0.0000, 1.5000, 1.5000],\n                [0.0000, 0.0000, 5.2183, 5.2183],\n                [0.0000, 0.1321, 7.8891, 0.8679],\n                [5.3967, 2.4251, 6.0033, 3.7749]])\n    """"""\n    means = deltas.new_tensor(means).repeat(1, deltas.size(1) // 4)\n    stds = deltas.new_tensor(stds).repeat(1, deltas.size(1) // 4)\n    denorm_deltas = deltas * stds + means\n    dx = denorm_deltas[:, 0::4]\n    dy = denorm_deltas[:, 1::4]\n    dw = denorm_deltas[:, 2::4]\n    dh = denorm_deltas[:, 3::4]\n    max_ratio = np.abs(np.log(wh_ratio_clip))\n    dw = dw.clamp(min=-max_ratio, max=max_ratio)\n    dh = dh.clamp(min=-max_ratio, max=max_ratio)\n    # Compute center of each roi\n    px = ((rois[:, 0] + rois[:, 2]) * 0.5).unsqueeze(1).expand_as(dx)\n    py = ((rois[:, 1] + rois[:, 3]) * 0.5).unsqueeze(1).expand_as(dy)\n    # Compute width/height of each roi\n    pw = (rois[:, 2] - rois[:, 0] + 1.0).unsqueeze(1).expand_as(dw)\n    ph = (rois[:, 3] - rois[:, 1] + 1.0).unsqueeze(1).expand_as(dh)\n    # Use exp(network energy) to enlarge/shrink each roi\n    gw = pw * dw.exp()\n    gh = ph * dh.exp()\n    # Use network energy to shift the center of each roi\n    gx = px + pw * dx\n    gy = py + ph * dy\n    # Convert center-xy/width/height to top-left, bottom-right\n\n    # The true legacy box coder should +- 0.5 here.\n    # However, current implementation improves the performance when testing\n    # the models trained in MMDetection 1.X (~0.5 bbox AP, 0.2 mask AP)\n    x1 = gx - gw * 0.5\n    y1 = gy - gh * 0.5\n    x2 = gx + gw * 0.5\n    y2 = gy + gh * 0.5\n    if max_shape is not None:\n        x1 = x1.clamp(min=0, max=max_shape[1] - 1)\n        y1 = y1.clamp(min=0, max=max_shape[0] - 1)\n        x2 = x2.clamp(min=0, max=max_shape[1] - 1)\n        y2 = y2.clamp(min=0, max=max_shape[0] - 1)\n    bboxes = torch.stack([x1, y1, x2, y2], dim=-1).view_as(deltas)\n    return bboxes\n'"
mmdet/core/bbox/coder/pseudo_bbox_coder.py,0,"b'from ..builder import BBOX_CODERS\nfrom .base_bbox_coder import BaseBBoxCoder\n\n\n@BBOX_CODERS.register_module()\nclass PseudoBBoxCoder(BaseBBoxCoder):\n\n    def __init__(self, **kwargs):\n        super(BaseBBoxCoder, self).__init__(**kwargs)\n\n    def encode(self, bboxes, gt_bboxes):\n        return gt_bboxes\n\n    def decode(self, bboxes, pred_bboxes):\n        return pred_bboxes\n'"
mmdet/core/bbox/coder/tblr_bbox_coder.py,6,"b'import torch\n\nfrom ..builder import BBOX_CODERS\nfrom .base_bbox_coder import BaseBBoxCoder\n\n\n@BBOX_CODERS.register_module()\nclass TBLRBBoxCoder(BaseBBoxCoder):\n    """"""TBLR BBox coder\n\n    Following the practice in `FSAF <https://arxiv.org/abs/1903.00621>`_,\n    this coder encodes gt bboxes (x1, y1, x2, y2) into (top, bottom, left,\n    right) and decode it back to the original.\n\n    Args:\n        normalizer (list | float): Normalization factor to be\n          divided with when coding the coordinates. If it is a list, it should\n          have length of 4 indicating normalization factor in tblr dims.\n          Otherwise it is a unified float factor for all dims. Default: 4.0\n    """"""\n\n    def __init__(self, normalizer=4.0):\n        super(BaseBBoxCoder, self).__init__()\n        self.normalizer = normalizer\n\n    def encode(self, bboxes, gt_bboxes):\n        assert bboxes.size(0) == gt_bboxes.size(0)\n        assert bboxes.size(-1) == gt_bboxes.size(-1) == 4\n        encoded_bboxes = bboxes2tblr(\n            bboxes, gt_bboxes, normalizer=self.normalizer)\n        return encoded_bboxes\n\n    def decode(self, bboxes, pred_bboxes, max_shape=None):\n        assert pred_bboxes.size(0) == bboxes.size(0)\n        decoded_bboxes = tblr2bboxes(\n            bboxes,\n            pred_bboxes,\n            normalizer=self.normalizer,\n            max_shape=max_shape)\n\n        return decoded_bboxes\n\n\ndef bboxes2tblr(priors, gts, normalizer=4.0, normalize_by_wh=True):\n    """"""Encode ground truth boxes to tblr coordinate\n\n    It first convert the gt coordinate to tblr format,\n     (top, bottom, left, right), relative to prior box centers.\n     The tblr coordinate may be normalized by the side length of prior bboxes\n     if `normalize_by_wh` is specified as True, and it is then normalized by\n     the `normalizer` factor.\n\n    Args:\n        priors (Tensor): Prior boxes in point form\n            Shape: (num_proposals,4).\n        gts (Tensor): Coords of ground truth for each prior in point-form\n            Shape: (num_proposals, 4).\n        normalizer (Sequence[float] | float): normalization parameter of\n            encoded boxes. If it is a list, it has to have length = 4.\n            Default: 4.0\n        normalize_by_wh (bool): Whether to normalize tblr coordinate by the\n            side length (wh) of prior bboxes.\n\n    Return:\n        encoded boxes (Tensor), Shape: (num_proposals, 4)\n    """"""\n\n    # dist b/t match center and prior\'s center\n    if not isinstance(normalizer, float):\n        normalizer = torch.tensor(normalizer, device=priors.device)\n        assert len(normalizer) == 4, \'Normalizer must have length = 4\'\n    assert priors.size(0) == gts.size(0)\n    prior_centers = (priors[:, 0:2] + priors[:, 2:4]) / 2\n    xmin, ymin, xmax, ymax = gts.split(1, dim=1)\n    top = prior_centers[:, 1].unsqueeze(1) - ymin\n    bottom = ymax - prior_centers[:, 1].unsqueeze(1)\n    left = prior_centers[:, 0].unsqueeze(1) - xmin\n    right = xmax - prior_centers[:, 0].unsqueeze(1)\n    loc = torch.cat((top, bottom, left, right), dim=1)\n    if normalize_by_wh:\n        # Normalize tblr by anchor width and height\n        wh = priors[:, 2:4] - priors[:, 0:2]\n        w, h = torch.split(wh, 1, dim=1)\n        loc[:, :2] /= h  # tb is normalized by h\n        loc[:, 2:] /= w  # lr is normalized by w\n    # Normalize tblr by the given normalization factor\n    return loc / normalizer\n\n\ndef tblr2bboxes(priors,\n                tblr,\n                normalizer=4.0,\n                normalize_by_wh=True,\n                max_shape=None):\n    """"""Decode tblr outputs to prediction boxes\n\n    The process includes 3 steps: 1) De-normalize tblr coordinates by\n    multiplying it with `normalizer`; 2) De-normalize tblr coordinates by the\n    prior bbox width and height if `normalize_by_wh` is `True`; 3) Convert\n    tblr (top, bottom, left, right) pair relative to the center of priors back\n    to (xmin, ymin, xmax, ymax) coordinate.\n\n    Args:\n        priors (Tensor): Prior boxes in point form (x0, y0, x1, y1)\n          Shape: (n,4).\n        tblr (Tensor): Coords of network output in tblr form\n          Shape: (n, 4).\n        normalizer (Sequence[float] | float): Normalization parameter of\n          encoded boxes. By list, it represents the normalization factors at\n          tblr dims. By float, it is the unified normalization factor at all\n          dims. Default: 4.0\n        normalize_by_wh (bool): Whether the tblr coordinates have been\n          normalized by the side length (wh) of prior bboxes.\n        max_shape (tuple, optional): Shape of the image. Decoded bboxes\n          exceeding which will be clamped.\n\n    Return:\n        encoded boxes (Tensor), Shape: (n, 4)\n    """"""\n    if not isinstance(normalizer, float):\n        normalizer = torch.tensor(normalizer, device=priors.device)\n        assert len(normalizer) == 4, \'Normalizer must have length = 4\'\n    assert priors.size(0) == tblr.size(0)\n    loc_decode = tblr * normalizer\n    prior_centers = (priors[:, 0:2] + priors[:, 2:4]) / 2\n    if normalize_by_wh:\n        wh = priors[:, 2:4] - priors[:, 0:2]\n        w, h = torch.split(wh, 1, dim=1)\n        loc_decode[:, :2] *= h  # tb\n        loc_decode[:, 2:] *= w  # lr\n    top, bottom, left, right = loc_decode.split(1, dim=1)\n    xmin = prior_centers[:, 0].unsqueeze(1) - left\n    xmax = prior_centers[:, 0].unsqueeze(1) + right\n    ymin = prior_centers[:, 1].unsqueeze(1) - top\n    ymax = prior_centers[:, 1].unsqueeze(1) + bottom\n    boxes = torch.cat((xmin, ymin, xmax, ymax), dim=1)\n    if max_shape is not None:\n        boxes[:, 0].clamp_(min=0, max=max_shape[1])\n        boxes[:, 1].clamp_(min=0, max=max_shape[0])\n        boxes[:, 2].clamp_(min=0, max=max_shape[1])\n        boxes[:, 3].clamp_(min=0, max=max_shape[0])\n    return boxes\n'"
mmdet/core/bbox/iou_calculators/__init__.py,0,"b""from .builder import build_iou_calculator\nfrom .iou2d_calculator import BboxOverlaps2D, bbox_overlaps\n\n__all__ = ['build_iou_calculator', 'BboxOverlaps2D', 'bbox_overlaps']\n"""
mmdet/core/bbox/iou_calculators/builder.py,0,"b""from mmcv.utils import Registry, build_from_cfg\n\nIOU_CALCULATORS = Registry('IoU calculator')\n\n\ndef build_iou_calculator(cfg, default_args=None):\n    return build_from_cfg(cfg, IOU_CALCULATORS, default_args)\n"""
mmdet/core/bbox/iou_calculators/iou2d_calculator.py,8,"b'import torch\n\nfrom .builder import IOU_CALCULATORS\n\n\n@IOU_CALCULATORS.register_module()\nclass BboxOverlaps2D(object):\n    """"""2D IoU Calculator""""""\n\n    def __call__(self, bboxes1, bboxes2, mode=\'iou\', is_aligned=False):\n        """"""Calculate IoU between 2D bboxes\n\n        Args:\n            bboxes1 (Tensor): bboxes have shape (m, 4) in <x1, y1, x2, y2>\n                format, or shape (m, 5) in <x1, y1, x2, y2, score> format.\n            bboxes2 (Tensor): bboxes have shape (m, 4) in <x1, y1, x2, y2>\n                format, shape (m, 5) in <x1, y1, x2, y2, score> format, or be\n                empty. If is_aligned is ``True``, then m and n must be equal.\n            mode (str): ""iou"" (intersection over union) or iof (intersection\n                over foreground).\n\n        Returns:\n            ious(Tensor): shape (m, n) if is_aligned == False else shape (m, 1)\n        """"""\n        assert bboxes1.size(-1) in [0, 4, 5]\n        assert bboxes2.size(-1) in [0, 4, 5]\n        if bboxes2.size(-1) == 5:\n            bboxes2 = bboxes2[..., :4]\n        if bboxes1.size(-1) == 5:\n            bboxes1 = bboxes1[..., :4]\n        return bbox_overlaps(bboxes1, bboxes2, mode, is_aligned)\n\n    def __repr__(self):\n        repr_str = self.__class__.__name__ + \'()\'\n        return repr_str\n\n\ndef bbox_overlaps(bboxes1, bboxes2, mode=\'iou\', is_aligned=False):\n    """"""Calculate overlap between two set of bboxes.\n\n    If ``is_aligned`` is ``False``, then calculate the ious between each bbox\n    of bboxes1 and bboxes2, otherwise the ious between each aligned pair of\n    bboxes1 and bboxes2.\n\n    Args:\n        bboxes1 (Tensor): shape (m, 4) in <x1, y1, x2, y2> format or empty.\n        bboxes2 (Tensor): shape (n, 4) in <x1, y1, x2, y2> format or empty.\n            If is_aligned is ``True``, then m and n must be equal.\n        mode (str): ""iou"" (intersection over union) or iof (intersection over\n            foreground).\n\n    Returns:\n        ious(Tensor): shape (m, n) if is_aligned == False else shape (m, 1)\n\n    Example:\n        >>> bboxes1 = torch.FloatTensor([\n        >>>     [0, 0, 10, 10],\n        >>>     [10, 10, 20, 20],\n        >>>     [32, 32, 38, 42],\n        >>> ])\n        >>> bboxes2 = torch.FloatTensor([\n        >>>     [0, 0, 10, 20],\n        >>>     [0, 10, 10, 19],\n        >>>     [10, 10, 20, 20],\n        >>> ])\n        >>> bbox_overlaps(bboxes1, bboxes2)\n        tensor([[0.5000, 0.0000, 0.0000],\n                [0.0000, 0.0000, 1.0000],\n                [0.0000, 0.0000, 0.0000]])\n\n    Example:\n        >>> empty = torch.FloatTensor([])\n        >>> nonempty = torch.FloatTensor([\n        >>>     [0, 0, 10, 9],\n        >>> ])\n        >>> assert tuple(bbox_overlaps(empty, nonempty).shape) == (0, 1)\n        >>> assert tuple(bbox_overlaps(nonempty, empty).shape) == (1, 0)\n        >>> assert tuple(bbox_overlaps(empty, empty).shape) == (0, 0)\n    """"""\n\n    assert mode in [\'iou\', \'iof\']\n    # Either the boxes are empty or the length of boxes\'s last dimenstion is 4\n    assert (bboxes1.size(-1) == 4 or bboxes1.size(0) == 0)\n    assert (bboxes2.size(-1) == 4 or bboxes2.size(0) == 0)\n\n    rows = bboxes1.size(0)\n    cols = bboxes2.size(0)\n    if is_aligned:\n        assert rows == cols\n\n    if rows * cols == 0:\n        return bboxes1.new(rows, 1) if is_aligned else bboxes1.new(rows, cols)\n\n    if is_aligned:\n        lt = torch.max(bboxes1[:, :2], bboxes2[:, :2])  # [rows, 2]\n        rb = torch.min(bboxes1[:, 2:], bboxes2[:, 2:])  # [rows, 2]\n\n        wh = (rb - lt).clamp(min=0)  # [rows, 2]\n        overlap = wh[:, 0] * wh[:, 1]\n        area1 = (bboxes1[:, 2] - bboxes1[:, 0]) * (\n            bboxes1[:, 3] - bboxes1[:, 1])\n\n        if mode == \'iou\':\n            area2 = (bboxes2[:, 2] - bboxes2[:, 0]) * (\n                bboxes2[:, 3] - bboxes2[:, 1])\n            ious = overlap / (area1 + area2 - overlap)\n        else:\n            ious = overlap / area1\n    else:\n        lt = torch.max(bboxes1[:, None, :2], bboxes2[:, :2])  # [rows, cols, 2]\n        rb = torch.min(bboxes1[:, None, 2:], bboxes2[:, 2:])  # [rows, cols, 2]\n\n        wh = (rb - lt).clamp(min=0)  # [rows, cols, 2]\n        overlap = wh[:, :, 0] * wh[:, :, 1]\n        area1 = (bboxes1[:, 2] - bboxes1[:, 0]) * (\n            bboxes1[:, 3] - bboxes1[:, 1])\n\n        if mode == \'iou\':\n            area2 = (bboxes2[:, 2] - bboxes2[:, 0]) * (\n                bboxes2[:, 3] - bboxes2[:, 1])\n            ious = overlap / (area1[:, None] + area2 - overlap)\n        else:\n            ious = overlap / (area1[:, None])\n\n    return ious\n'"
mmdet/core/bbox/samplers/__init__.py,0,"b""from .base_sampler import BaseSampler\nfrom .combined_sampler import CombinedSampler\nfrom .instance_balanced_pos_sampler import InstanceBalancedPosSampler\nfrom .iou_balanced_neg_sampler import IoUBalancedNegSampler\nfrom .ohem_sampler import OHEMSampler\nfrom .pseudo_sampler import PseudoSampler\nfrom .random_sampler import RandomSampler\nfrom .sampling_result import SamplingResult\nfrom .score_hlr_sampler import ScoreHLRSampler\n\n__all__ = [\n    'BaseSampler', 'PseudoSampler', 'RandomSampler',\n    'InstanceBalancedPosSampler', 'IoUBalancedNegSampler', 'CombinedSampler',\n    'OHEMSampler', 'SamplingResult', 'ScoreHLRSampler'\n]\n"""
mmdet/core/bbox/samplers/base_sampler.py,4,"b'from abc import ABCMeta, abstractmethod\n\nimport torch\n\nfrom .sampling_result import SamplingResult\n\n\nclass BaseSampler(metaclass=ABCMeta):\n\n    def __init__(self,\n                 num,\n                 pos_fraction,\n                 neg_pos_ub=-1,\n                 add_gt_as_proposals=True,\n                 **kwargs):\n        self.num = num\n        self.pos_fraction = pos_fraction\n        self.neg_pos_ub = neg_pos_ub\n        self.add_gt_as_proposals = add_gt_as_proposals\n        self.pos_sampler = self\n        self.neg_sampler = self\n\n    @abstractmethod\n    def _sample_pos(self, assign_result, num_expected, **kwargs):\n        pass\n\n    @abstractmethod\n    def _sample_neg(self, assign_result, num_expected, **kwargs):\n        pass\n\n    def sample(self,\n               assign_result,\n               bboxes,\n               gt_bboxes,\n               gt_labels=None,\n               **kwargs):\n        """"""Sample positive and negative bboxes.\n\n        This is a simple implementation of bbox sampling given candidates,\n        assigning results and ground truth bboxes.\n\n        Args:\n            assign_result (:obj:`AssignResult`): Bbox assigning results.\n            bboxes (Tensor): Boxes to be sampled from.\n            gt_bboxes (Tensor): Ground truth bboxes.\n            gt_labels (Tensor, optional): Class labels of ground truth bboxes.\n\n        Returns:\n            :obj:`SamplingResult`: Sampling result.\n\n        Example:\n            >>> from mmdet.core.bbox import RandomSampler\n            >>> from mmdet.core.bbox import AssignResult\n            >>> from mmdet.core.bbox.demodata import ensure_rng, random_boxes\n            >>> rng = ensure_rng(None)\n            >>> assign_result = AssignResult.random(rng=rng)\n            >>> bboxes = random_boxes(assign_result.num_preds, rng=rng)\n            >>> gt_bboxes = random_boxes(assign_result.num_gts, rng=rng)\n            >>> gt_labels = None\n            >>> self = RandomSampler(num=32, pos_fraction=0.5, neg_pos_ub=-1,\n            >>>                      add_gt_as_proposals=False)\n            >>> self = self.sample(assign_result, bboxes, gt_bboxes, gt_labels)\n        """"""\n        if len(bboxes.shape) < 2:\n            bboxes = bboxes[None, :]\n\n        bboxes = bboxes[:, :4]\n\n        gt_flags = bboxes.new_zeros((bboxes.shape[0], ), dtype=torch.uint8)\n        if self.add_gt_as_proposals and len(gt_bboxes) > 0:\n            if gt_labels is None:\n                raise ValueError(\n                    \'gt_labels must be given when add_gt_as_proposals is True\')\n            bboxes = torch.cat([gt_bboxes, bboxes], dim=0)\n            assign_result.add_gt_(gt_labels)\n            gt_ones = bboxes.new_ones(gt_bboxes.shape[0], dtype=torch.uint8)\n            gt_flags = torch.cat([gt_ones, gt_flags])\n\n        num_expected_pos = int(self.num * self.pos_fraction)\n        pos_inds = self.pos_sampler._sample_pos(\n            assign_result, num_expected_pos, bboxes=bboxes, **kwargs)\n        # We found that sampled indices have duplicated items occasionally.\n        # (may be a bug of PyTorch)\n        pos_inds = pos_inds.unique()\n        num_sampled_pos = pos_inds.numel()\n        num_expected_neg = self.num - num_sampled_pos\n        if self.neg_pos_ub >= 0:\n            _pos = max(1, num_sampled_pos)\n            neg_upper_bound = int(self.neg_pos_ub * _pos)\n            if num_expected_neg > neg_upper_bound:\n                num_expected_neg = neg_upper_bound\n        neg_inds = self.neg_sampler._sample_neg(\n            assign_result, num_expected_neg, bboxes=bboxes, **kwargs)\n        neg_inds = neg_inds.unique()\n\n        sampling_result = SamplingResult(pos_inds, neg_inds, bboxes, gt_bboxes,\n                                         assign_result, gt_flags)\n        return sampling_result\n'"
mmdet/core/bbox/samplers/combined_sampler.py,0,"b'from ..builder import BBOX_SAMPLERS, build_sampler\nfrom .base_sampler import BaseSampler\n\n\n@BBOX_SAMPLERS.register_module()\nclass CombinedSampler(BaseSampler):\n\n    def __init__(self, pos_sampler, neg_sampler, **kwargs):\n        super(CombinedSampler, self).__init__(**kwargs)\n        self.pos_sampler = build_sampler(pos_sampler, **kwargs)\n        self.neg_sampler = build_sampler(neg_sampler, **kwargs)\n\n    def _sample_pos(self, **kwargs):\n        raise NotImplementedError\n\n    def _sample_neg(self, **kwargs):\n        raise NotImplementedError\n'"
mmdet/core/bbox/samplers/instance_balanced_pos_sampler.py,5,"b'import numpy as np\nimport torch\n\nfrom ..builder import BBOX_SAMPLERS\nfrom .random_sampler import RandomSampler\n\n\n@BBOX_SAMPLERS.register_module()\nclass InstanceBalancedPosSampler(RandomSampler):\n\n    def _sample_pos(self, assign_result, num_expected, **kwargs):\n        pos_inds = torch.nonzero(assign_result.gt_inds > 0, as_tuple=False)\n        if pos_inds.numel() != 0:\n            pos_inds = pos_inds.squeeze(1)\n        if pos_inds.numel() <= num_expected:\n            return pos_inds\n        else:\n            unique_gt_inds = assign_result.gt_inds[pos_inds].unique()\n            num_gts = len(unique_gt_inds)\n            num_per_gt = int(round(num_expected / float(num_gts)) + 1)\n            sampled_inds = []\n            for i in unique_gt_inds:\n                inds = torch.nonzero(\n                    assign_result.gt_inds == i.item(), as_tuple=False)\n                if inds.numel() != 0:\n                    inds = inds.squeeze(1)\n                else:\n                    continue\n                if len(inds) > num_per_gt:\n                    inds = self.random_choice(inds, num_per_gt)\n                sampled_inds.append(inds)\n            sampled_inds = torch.cat(sampled_inds)\n            if len(sampled_inds) < num_expected:\n                num_extra = num_expected - len(sampled_inds)\n                extra_inds = np.array(\n                    list(set(pos_inds.cpu()) - set(sampled_inds.cpu())))\n                if len(extra_inds) > num_extra:\n                    extra_inds = self.random_choice(extra_inds, num_extra)\n                extra_inds = torch.from_numpy(extra_inds).to(\n                    assign_result.gt_inds.device).long()\n                sampled_inds = torch.cat([sampled_inds, extra_inds])\n            elif len(sampled_inds) > num_expected:\n                sampled_inds = self.random_choice(sampled_inds, num_expected)\n            return sampled_inds\n'"
mmdet/core/bbox/samplers/iou_balanced_neg_sampler.py,2,"b'import numpy as np\nimport torch\n\nfrom ..builder import BBOX_SAMPLERS\nfrom .random_sampler import RandomSampler\n\n\n@BBOX_SAMPLERS.register_module()\nclass IoUBalancedNegSampler(RandomSampler):\n    """"""IoU Balanced Sampling\n\n    arXiv: https://arxiv.org/pdf/1904.02701.pdf (CVPR 2019)\n\n    Sampling proposals according to their IoU. `floor_fraction` of needed RoIs\n    are sampled from proposals whose IoU are lower than `floor_thr` randomly.\n    The others are sampled from proposals whose IoU are higher than\n    `floor_thr`. These proposals are sampled from some bins evenly, which are\n    split by `num_bins` via IoU evenly.\n\n    Args:\n        num (int): number of proposals.\n        pos_fraction (float): fraction of positive proposals.\n        floor_thr (float): threshold (minimum) IoU for IoU balanced sampling,\n            set to -1 if all using IoU balanced sampling.\n        floor_fraction (float): sampling fraction of proposals under floor_thr.\n        num_bins (int): number of bins in IoU balanced sampling.\n    """"""\n\n    def __init__(self,\n                 num,\n                 pos_fraction,\n                 floor_thr=-1,\n                 floor_fraction=0,\n                 num_bins=3,\n                 **kwargs):\n        super(IoUBalancedNegSampler, self).__init__(num, pos_fraction,\n                                                    **kwargs)\n        assert floor_thr >= 0 or floor_thr == -1\n        assert 0 <= floor_fraction <= 1\n        assert num_bins >= 1\n\n        self.floor_thr = floor_thr\n        self.floor_fraction = floor_fraction\n        self.num_bins = num_bins\n\n    def sample_via_interval(self, max_overlaps, full_set, num_expected):\n        max_iou = max_overlaps.max()\n        iou_interval = (max_iou - self.floor_thr) / self.num_bins\n        per_num_expected = int(num_expected / self.num_bins)\n\n        sampled_inds = []\n        for i in range(self.num_bins):\n            start_iou = self.floor_thr + i * iou_interval\n            end_iou = self.floor_thr + (i + 1) * iou_interval\n            tmp_set = set(\n                np.where(\n                    np.logical_and(max_overlaps >= start_iou,\n                                   max_overlaps < end_iou))[0])\n            tmp_inds = list(tmp_set & full_set)\n            if len(tmp_inds) > per_num_expected:\n                tmp_sampled_set = self.random_choice(tmp_inds,\n                                                     per_num_expected)\n            else:\n                tmp_sampled_set = np.array(tmp_inds, dtype=np.int)\n            sampled_inds.append(tmp_sampled_set)\n\n        sampled_inds = np.concatenate(sampled_inds)\n        if len(sampled_inds) < num_expected:\n            num_extra = num_expected - len(sampled_inds)\n            extra_inds = np.array(list(full_set - set(sampled_inds)))\n            if len(extra_inds) > num_extra:\n                extra_inds = self.random_choice(extra_inds, num_extra)\n            sampled_inds = np.concatenate([sampled_inds, extra_inds])\n\n        return sampled_inds\n\n    def _sample_neg(self, assign_result, num_expected, **kwargs):\n        neg_inds = torch.nonzero(assign_result.gt_inds == 0, as_tuple=False)\n        if neg_inds.numel() != 0:\n            neg_inds = neg_inds.squeeze(1)\n        if len(neg_inds) <= num_expected:\n            return neg_inds\n        else:\n            max_overlaps = assign_result.max_overlaps.cpu().numpy()\n            # balance sampling for negative samples\n            neg_set = set(neg_inds.cpu().numpy())\n\n            if self.floor_thr > 0:\n                floor_set = set(\n                    np.where(\n                        np.logical_and(max_overlaps >= 0,\n                                       max_overlaps < self.floor_thr))[0])\n                iou_sampling_set = set(\n                    np.where(max_overlaps >= self.floor_thr)[0])\n            elif self.floor_thr == 0:\n                floor_set = set(np.where(max_overlaps == 0)[0])\n                iou_sampling_set = set(\n                    np.where(max_overlaps > self.floor_thr)[0])\n            else:\n                floor_set = set()\n                iou_sampling_set = set(\n                    np.where(max_overlaps > self.floor_thr)[0])\n                # for sampling interval calculation\n                self.floor_thr = 0\n\n            floor_neg_inds = list(floor_set & neg_set)\n            iou_sampling_neg_inds = list(iou_sampling_set & neg_set)\n            num_expected_iou_sampling = int(num_expected *\n                                            (1 - self.floor_fraction))\n            if len(iou_sampling_neg_inds) > num_expected_iou_sampling:\n                if self.num_bins >= 2:\n                    iou_sampled_inds = self.sample_via_interval(\n                        max_overlaps, set(iou_sampling_neg_inds),\n                        num_expected_iou_sampling)\n                else:\n                    iou_sampled_inds = self.random_choice(\n                        iou_sampling_neg_inds, num_expected_iou_sampling)\n            else:\n                iou_sampled_inds = np.array(\n                    iou_sampling_neg_inds, dtype=np.int)\n            num_expected_floor = num_expected - len(iou_sampled_inds)\n            if len(floor_neg_inds) > num_expected_floor:\n                sampled_floor_inds = self.random_choice(\n                    floor_neg_inds, num_expected_floor)\n            else:\n                sampled_floor_inds = np.array(floor_neg_inds, dtype=np.int)\n            sampled_inds = np.concatenate(\n                (sampled_floor_inds, iou_sampled_inds))\n            if len(sampled_inds) < num_expected:\n                num_extra = num_expected - len(sampled_inds)\n                extra_inds = np.array(list(neg_set - set(sampled_inds)))\n                if len(extra_inds) > num_extra:\n                    extra_inds = self.random_choice(extra_inds, num_extra)\n                sampled_inds = np.concatenate((sampled_inds, extra_inds))\n            sampled_inds = torch.from_numpy(sampled_inds).long().to(\n                assign_result.gt_inds.device)\n            return sampled_inds\n'"
mmdet/core/bbox/samplers/ohem_sampler.py,3,"b'import torch\n\nfrom ..builder import BBOX_SAMPLERS\nfrom ..transforms import bbox2roi\nfrom .base_sampler import BaseSampler\n\n\n@BBOX_SAMPLERS.register_module()\nclass OHEMSampler(BaseSampler):\n    """"""\n    Online Hard Example Mining Sampler described in [1]_.\n\n    References:\n        .. [1] https://arxiv.org/pdf/1604.03540.pdf\n    """"""\n\n    def __init__(self,\n                 num,\n                 pos_fraction,\n                 context,\n                 neg_pos_ub=-1,\n                 add_gt_as_proposals=True,\n                 **kwargs):\n        super(OHEMSampler, self).__init__(num, pos_fraction, neg_pos_ub,\n                                          add_gt_as_proposals)\n        if not hasattr(context, \'num_stages\'):\n            self.bbox_roi_extractor = context.bbox_roi_extractor\n            self.bbox_head = context.bbox_head\n        else:\n            self.bbox_roi_extractor = context.bbox_roi_extractor[\n                context.current_stage]\n            self.bbox_head = context.bbox_head[context.current_stage]\n\n    def hard_mining(self, inds, num_expected, bboxes, labels, feats):\n        with torch.no_grad():\n            rois = bbox2roi([bboxes])\n            bbox_feats = self.bbox_roi_extractor(\n                feats[:self.bbox_roi_extractor.num_inputs], rois)\n            cls_score, _ = self.bbox_head(bbox_feats)\n            loss = self.bbox_head.loss(\n                cls_score=cls_score,\n                bbox_pred=None,\n                rois=rois,\n                labels=labels,\n                label_weights=cls_score.new_ones(cls_score.size(0)),\n                bbox_targets=None,\n                bbox_weights=None,\n                reduction_override=\'none\')[\'loss_cls\']\n            _, topk_loss_inds = loss.topk(num_expected)\n        return inds[topk_loss_inds]\n\n    def _sample_pos(self,\n                    assign_result,\n                    num_expected,\n                    bboxes=None,\n                    feats=None,\n                    **kwargs):\n        # Sample some hard positive samples\n        pos_inds = torch.nonzero(assign_result.gt_inds > 0, as_tuple=False)\n        if pos_inds.numel() != 0:\n            pos_inds = pos_inds.squeeze(1)\n        if pos_inds.numel() <= num_expected:\n            return pos_inds\n        else:\n            return self.hard_mining(pos_inds, num_expected, bboxes[pos_inds],\n                                    assign_result.labels[pos_inds], feats)\n\n    def _sample_neg(self,\n                    assign_result,\n                    num_expected,\n                    bboxes=None,\n                    feats=None,\n                    **kwargs):\n        # Sample some hard negative samples\n        neg_inds = torch.nonzero(assign_result.gt_inds == 0, as_tuple=False)\n        if neg_inds.numel() != 0:\n            neg_inds = neg_inds.squeeze(1)\n        if len(neg_inds) <= num_expected:\n            return neg_inds\n        else:\n            neg_labels = assign_result.labels.new_empty(\n                neg_inds.size(0)).fill_(self.bbox_head.num_classes)\n            return self.hard_mining(neg_inds, num_expected, bboxes[neg_inds],\n                                    neg_labels, feats)\n'"
mmdet/core/bbox/samplers/pseudo_sampler.py,3,"b'import torch\n\nfrom ..builder import BBOX_SAMPLERS\nfrom .base_sampler import BaseSampler\nfrom .sampling_result import SamplingResult\n\n\n@BBOX_SAMPLERS.register_module()\nclass PseudoSampler(BaseSampler):\n\n    def __init__(self, **kwargs):\n        pass\n\n    def _sample_pos(self, **kwargs):\n        raise NotImplementedError\n\n    def _sample_neg(self, **kwargs):\n        raise NotImplementedError\n\n    def sample(self, assign_result, bboxes, gt_bboxes, **kwargs):\n        pos_inds = torch.nonzero(\n            assign_result.gt_inds > 0, as_tuple=False).squeeze(-1).unique()\n        neg_inds = torch.nonzero(\n            assign_result.gt_inds == 0, as_tuple=False).squeeze(-1).unique()\n        gt_flags = bboxes.new_zeros(bboxes.shape[0], dtype=torch.uint8)\n        sampling_result = SamplingResult(pos_inds, neg_inds, bboxes, gt_bboxes,\n                                         assign_result, gt_flags)\n        return sampling_result\n'"
mmdet/core/bbox/samplers/random_sampler.py,6,"b'import torch\n\nfrom ..builder import BBOX_SAMPLERS\nfrom .base_sampler import BaseSampler\n\n\n@BBOX_SAMPLERS.register_module()\nclass RandomSampler(BaseSampler):\n\n    def __init__(self,\n                 num,\n                 pos_fraction,\n                 neg_pos_ub=-1,\n                 add_gt_as_proposals=True,\n                 **kwargs):\n        from mmdet.core.bbox import demodata\n        super(RandomSampler, self).__init__(num, pos_fraction, neg_pos_ub,\n                                            add_gt_as_proposals)\n        self.rng = demodata.ensure_rng(kwargs.get(\'rng\', None))\n\n    def random_choice(self, gallery, num):\n        """"""Random select some elements from the gallery.\n\n        If `gallery` is a Tensor, the returned indices will be a Tensor;\n        If `gallery` is a ndarray or list, the returned indices will be a\n        ndarray.\n\n        Args:\n            gallery (Tensor | ndarray | list): indices pool.\n            num (int): expected sample num.\n\n        Returns:\n            Tensor or ndarray: sampled indices.\n        """"""\n        assert len(gallery) >= num\n\n        is_tensor = isinstance(gallery, torch.Tensor)\n        if not is_tensor:\n            gallery = torch.tensor(\n                gallery, dtype=torch.long, device=torch.cuda.current_device())\n        perm = torch.randperm(gallery.numel(), device=gallery.device)[:num]\n        rand_inds = gallery[perm]\n        if not is_tensor:\n            rand_inds = rand_inds.cpu().numpy()\n        return rand_inds\n\n    def _sample_pos(self, assign_result, num_expected, **kwargs):\n        """"""Randomly sample some positive samples.""""""\n        pos_inds = torch.nonzero(assign_result.gt_inds > 0, as_tuple=False)\n        if pos_inds.numel() != 0:\n            pos_inds = pos_inds.squeeze(1)\n        if pos_inds.numel() <= num_expected:\n            return pos_inds\n        else:\n            return self.random_choice(pos_inds, num_expected)\n\n    def _sample_neg(self, assign_result, num_expected, **kwargs):\n        """"""Randomly sample some negative samples.""""""\n        neg_inds = torch.nonzero(assign_result.gt_inds == 0, as_tuple=False)\n        if neg_inds.numel() != 0:\n            neg_inds = neg_inds.squeeze(1)\n        if len(neg_inds) <= num_expected:\n            return neg_inds\n        else:\n            return self.random_choice(neg_inds, num_expected)\n'"
mmdet/core/bbox/samplers/sampling_result.py,8,"b'import torch\n\nfrom mmdet.utils import util_mixins\n\n\nclass SamplingResult(util_mixins.NiceRepr):\n    """"""Bbox sampling result.\n\n    Example:\n        >>> # xdoctest: +IGNORE_WANT\n        >>> from mmdet.core.bbox.samplers.sampling_result import *  # NOQA\n        >>> self = SamplingResult.random(rng=10)\n        >>> print(f\'self = {self}\')\n        self = <SamplingResult({\n            \'neg_bboxes\': torch.Size([12, 4]),\n            \'neg_inds\': tensor([ 0,  1,  2,  4,  5,  6,  7,  8,  9, 10, 11, 12]),\n            \'num_gts\': 4,\n            \'pos_assigned_gt_inds\': tensor([], dtype=torch.int64),\n            \'pos_bboxes\': torch.Size([0, 4]),\n            \'pos_inds\': tensor([], dtype=torch.int64),\n            \'pos_is_gt\': tensor([], dtype=torch.uint8)\n        })>\n    """"""\n\n    def __init__(self, pos_inds, neg_inds, bboxes, gt_bboxes, assign_result,\n                 gt_flags):\n        self.pos_inds = pos_inds\n        self.neg_inds = neg_inds\n        self.pos_bboxes = bboxes[pos_inds]\n        self.neg_bboxes = bboxes[neg_inds]\n        self.pos_is_gt = gt_flags[pos_inds]\n\n        self.num_gts = gt_bboxes.shape[0]\n        self.pos_assigned_gt_inds = assign_result.gt_inds[pos_inds] - 1\n\n        if gt_bboxes.numel() == 0:\n            # hack for index error case\n            assert self.pos_assigned_gt_inds.numel() == 0\n            self.pos_gt_bboxes = torch.empty_like(gt_bboxes).view(-1, 4)\n        else:\n            if len(gt_bboxes.shape) < 2:\n                gt_bboxes = gt_bboxes.view(-1, 4)\n\n            self.pos_gt_bboxes = gt_bboxes[self.pos_assigned_gt_inds, :]\n\n        if assign_result.labels is not None:\n            self.pos_gt_labels = assign_result.labels[pos_inds]\n        else:\n            self.pos_gt_labels = None\n\n    @property\n    def bboxes(self):\n        return torch.cat([self.pos_bboxes, self.neg_bboxes])\n\n    def to(self, device):\n        """"""Change the device of the data inplace.\n\n        Example:\n            >>> self = SamplingResult.random()\n            >>> print(f\'self = {self.to(None)}\')\n            >>> # xdoctest: +REQUIRES(--gpu)\n            >>> print(f\'self = {self.to(0)}\')\n        """"""\n        _dict = self.__dict__\n        for key, value in _dict.items():\n            if isinstance(value, torch.Tensor):\n                _dict[key] = value.to(device)\n        return self\n\n    def __nice__(self):\n        data = self.info.copy()\n        data[\'pos_bboxes\'] = data.pop(\'pos_bboxes\').shape\n        data[\'neg_bboxes\'] = data.pop(\'neg_bboxes\').shape\n        parts = [f""\'{k}\': {v!r}"" for k, v in sorted(data.items())]\n        body = \'    \' + \',\\n    \'.join(parts)\n        return \'{\\n\' + body + \'\\n}\'\n\n    @property\n    def info(self):\n        """"""Returns a dictionary of info about the object.""""""\n        return {\n            \'pos_inds\': self.pos_inds,\n            \'neg_inds\': self.neg_inds,\n            \'pos_bboxes\': self.pos_bboxes,\n            \'neg_bboxes\': self.neg_bboxes,\n            \'pos_is_gt\': self.pos_is_gt,\n            \'num_gts\': self.num_gts,\n            \'pos_assigned_gt_inds\': self.pos_assigned_gt_inds,\n        }\n\n    @classmethod\n    def random(cls, rng=None, **kwargs):\n        """"""\n        Args:\n            rng (None | int | numpy.random.RandomState): seed or state.\n            kwargs (keyword arguments):\n                - num_preds: number of predicted boxes\n                - num_gts: number of true boxes\n                - p_ignore (float): probability of a predicted box assinged to\n                    an ignored truth.\n                - p_assigned (float): probability of a predicted box not being\n                    assigned.\n                - p_use_label (float | bool): with labels or not.\n\n        Returns:\n            :obj:`SamplingResult`: Randomly generated sampling result.\n\n        Example:\n            >>> from mmdet.core.bbox.samplers.sampling_result import *  # NOQA\n            >>> self = SamplingResult.random()\n            >>> print(self.__dict__)\n        """"""\n        from mmdet.core.bbox.samplers.random_sampler import RandomSampler\n        from mmdet.core.bbox.assigners.assign_result import AssignResult\n        from mmdet.core.bbox import demodata\n        rng = demodata.ensure_rng(rng)\n\n        # make probabalistic?\n        num = 32\n        pos_fraction = 0.5\n        neg_pos_ub = -1\n\n        assign_result = AssignResult.random(rng=rng, **kwargs)\n\n        # Note we could just compute an assignment\n        bboxes = demodata.random_boxes(assign_result.num_preds, rng=rng)\n        gt_bboxes = demodata.random_boxes(assign_result.num_gts, rng=rng)\n\n        if rng.rand() > 0.2:\n            # sometimes algorithms squeeze their data, be robust to that\n            gt_bboxes = gt_bboxes.squeeze()\n            bboxes = bboxes.squeeze()\n\n        if assign_result.labels is None:\n            gt_labels = None\n        else:\n            gt_labels = None  # todo\n\n        if gt_labels is None:\n            add_gt_as_proposals = False\n        else:\n            add_gt_as_proposals = True  # make probabalistic?\n\n        sampler = RandomSampler(\n            num,\n            pos_fraction,\n            neg_pos_ubo=neg_pos_ub,\n            add_gt_as_proposals=add_gt_as_proposals,\n            rng=rng)\n        self = sampler.sample(assign_result, bboxes, gt_bboxes, gt_labels)\n        return self\n'"
mmdet/core/bbox/samplers/score_hlr_sampler.py,15,"b'import torch\n\nfrom mmdet.ops import nms_match\nfrom ..builder import BBOX_SAMPLERS\nfrom ..transforms import bbox2roi\nfrom .base_sampler import BaseSampler\nfrom .sampling_result import SamplingResult\n\n\n@BBOX_SAMPLERS.register_module()\nclass ScoreHLRSampler(BaseSampler):\n    """"""Importance-based Sample Reweighting (ISR_N), negative part,\n       described in `PISA <https://arxiv.org/abs/1904.04821>`_.\n\n    References:\n        .. [1] https://arxiv.org/pdf/1604.03540.pdf\n\n    Score hierarchical local rank (HLR) differentiates with RandomSampler in\n    negative part. It firstly computes Score-HLR in a two-step way,\n    then linearly maps score hlr to the loss weights.\n\n    Args:\n        num (int): Total number of sampled RoIs.\n        pos_fraction (float): Fraction of positive samples.\n        context (:obj:`BaseRoIHead`): RoI head that the sampler belongs to.\n        neg_pos_ub (int): Upper bound of the ratio of num negative to num\n            positive, -1 means no upper bound.\n        add_gt_as_proposals (bool): Whether to add ground truth as proposals.\n        k (float): Power of the non-linear mapping.\n        bias (float): Shift of the non-linear mapping.\n        score_thr (float): Minimum score that a negative sample is to be\n            considered as valid bbox.\n    """"""\n\n    def __init__(self,\n                 num,\n                 pos_fraction,\n                 context,\n                 neg_pos_ub=-1,\n                 add_gt_as_proposals=True,\n                 k=0.5,\n                 bias=0,\n                 score_thr=0.05,\n                 iou_thr=0.5,\n                 **kwargs):\n        super().__init__(num, pos_fraction, neg_pos_ub, add_gt_as_proposals)\n        self.k = k\n        self.bias = bias\n        self.score_thr = score_thr\n        self.iou_thr = iou_thr\n        self.context = context\n        # context of cascade detectors is a list, so distinguish them here.\n        if not hasattr(context, \'num_stages\'):\n            self.bbox_roi_extractor = context.bbox_roi_extractor\n            self.bbox_head = context.bbox_head\n            self.with_shared_head = context.with_shared_head\n            if self.with_shared_head:\n                self.shared_head = context.shared_head\n        else:\n            self.bbox_roi_extractor = context.bbox_roi_extractor[\n                context.current_stage]\n            self.bbox_head = context.bbox_head[context.current_stage]\n\n    @staticmethod\n    def random_choice(gallery, num):\n        """"""Randomly select some elements from the gallery.\n\n        If `gallery` is a Tensor, the returned indices will be a Tensor;\n        If `gallery` is a ndarray or list, the returned indices will be a\n        ndarray.\n\n        Args:\n            gallery (Tensor | ndarray | list): indices pool.\n            num (int): expected sample num.\n\n        Returns:\n            Tensor or ndarray: sampled indices.\n        """"""\n        assert len(gallery) >= num\n\n        is_tensor = isinstance(gallery, torch.Tensor)\n        if not is_tensor:\n            gallery = torch.tensor(\n                gallery, dtype=torch.long, device=torch.cuda.current_device())\n        perm = torch.randperm(gallery.numel(), device=gallery.device)[:num]\n        rand_inds = gallery[perm]\n        if not is_tensor:\n            rand_inds = rand_inds.cpu().numpy()\n        return rand_inds\n\n    def _sample_pos(self, assign_result, num_expected, **kwargs):\n        """"""Randomly sample some positive samples.""""""\n        pos_inds = torch.nonzero(assign_result.gt_inds > 0).flatten()\n        if pos_inds.numel() <= num_expected:\n            return pos_inds\n        else:\n            return self.random_choice(pos_inds, num_expected)\n\n    def _sample_neg(self,\n                    assign_result,\n                    num_expected,\n                    bboxes,\n                    feats=None,\n                    img_meta=None,\n                    **kwargs):\n        """"""Sample negative samples.\n\n        Score-HLR sampler is done in the following steps:\n        1. Take the maximum positive score prediction of each negative samples\n            as s_i.\n        2. Filter out negative samples whose s_i <= score_thr, the left samples\n            are called valid samples.\n        3. Use NMS-Match to divide valid samples into different groups,\n            samples in the same group will greatly overlap with each other\n        4. Rank the matched samples in two-steps to get Score-HLR.\n            (1) In the same group, rank samples with their scores.\n            (2) In the same score rank across different groups,\n                rank samples with their scores again.\n        5. Linearly map Score-HLR to the final label weights.\n\n        Args:\n            assign_result (:obj:`AssignResult`): result of assigner.\n            num_expected (int): Expected number of samples.\n            bboxes (Tensor): bbox to be sampled.\n            feats (Tensor): Features come from FPN.\n            img_meta (dict): Meta information dictionary.\n        """"""\n        neg_inds = torch.nonzero(assign_result.gt_inds == 0).flatten()\n        num_neg = neg_inds.size(0)\n        if num_neg == 0:\n            return neg_inds, None\n        with torch.no_grad():\n            neg_bboxes = bboxes[neg_inds]\n            neg_rois = bbox2roi([neg_bboxes])\n            bbox_result = self.context._bbox_forward(feats, neg_rois)\n            cls_score, bbox_pred = bbox_result[\'cls_score\'], bbox_result[\n                \'bbox_pred\']\n\n            ori_loss = self.bbox_head.loss(\n                cls_score=cls_score,\n                bbox_pred=None,\n                rois=None,\n                labels=neg_inds.new_full((num_neg, ),\n                                         self.bbox_head.num_classes),\n                label_weights=cls_score.new_ones(num_neg),\n                bbox_targets=None,\n                bbox_weights=None,\n                reduction_override=\'none\')[\'loss_cls\']\n\n            # filter out samples with the max score lower than score_thr\n            max_score, argmax_score = cls_score.softmax(-1)[:, :-1].max(-1)\n            valid_inds = (max_score > self.score_thr).nonzero().view(-1)\n            invalid_inds = (max_score <= self.score_thr).nonzero().view(-1)\n            num_valid = valid_inds.size(0)\n            num_invalid = invalid_inds.size(0)\n\n            num_expected = min(num_neg, num_expected)\n            num_hlr = min(num_valid, num_expected)\n            num_rand = num_expected - num_hlr\n            if num_valid > 0:\n                valid_rois = neg_rois[valid_inds]\n                valid_max_score = max_score[valid_inds]\n                valid_argmax_score = argmax_score[valid_inds]\n                valid_bbox_pred = bbox_pred[valid_inds]\n\n                # valid_bbox_pred shape: [num_valid, #num_classes, 4]\n                valid_bbox_pred = valid_bbox_pred.view(\n                    valid_bbox_pred.size(0), -1, 4)\n                selected_bbox_pred = valid_bbox_pred[range(num_valid),\n                                                     valid_argmax_score]\n                pred_bboxes = self.bbox_head.bbox_coder.decode(\n                    valid_rois[:, 1:], selected_bbox_pred)\n                pred_bboxes_with_score = torch.cat(\n                    [pred_bboxes, valid_max_score[:, None]], -1)\n                group = nms_match(pred_bboxes_with_score, self.iou_thr)\n\n                # imp: importance\n                imp = cls_score.new_zeros(num_valid)\n                for g in group:\n                    g_score = valid_max_score[g]\n                    # g_score has already sorted\n                    rank = g_score.new_tensor(range(g_score.size(0)))\n                    imp[g] = num_valid - rank + g_score\n                _, imp_rank_inds = imp.sort(descending=True)\n                _, imp_rank = imp_rank_inds.sort()\n                hlr_inds = imp_rank_inds[:num_expected]\n\n                if num_rand > 0:\n                    rand_inds = torch.randperm(num_invalid)[:num_rand]\n                    select_inds = torch.cat(\n                        [valid_inds[hlr_inds], invalid_inds[rand_inds]])\n                else:\n                    select_inds = valid_inds[hlr_inds]\n\n                neg_label_weights = cls_score.new_ones(num_expected)\n\n                up_bound = max(num_expected, num_valid)\n                imp_weights = (up_bound -\n                               imp_rank[hlr_inds].float()) / up_bound\n                neg_label_weights[:num_hlr] = imp_weights\n                neg_label_weights[num_hlr:] = imp_weights.min()\n                neg_label_weights = (self.bias +\n                                     (1 - self.bias) * neg_label_weights).pow(\n                                         self.k)\n                ori_selected_loss = ori_loss[select_inds]\n                new_loss = ori_selected_loss * neg_label_weights\n                norm_ratio = ori_selected_loss.sum() / new_loss.sum()\n                neg_label_weights *= norm_ratio\n            else:\n                neg_label_weights = cls_score.new_ones(num_expected)\n                select_inds = torch.randperm(num_neg)[:num_expected]\n\n            return neg_inds[select_inds], neg_label_weights\n\n    def sample(self,\n               assign_result,\n               bboxes,\n               gt_bboxes,\n               gt_labels=None,\n               img_meta=None,\n               **kwargs):\n        """"""Sample positive and negative bboxes.\n\n        This is a simple implementation of bbox sampling given candidates,\n        assigning results and ground truth bboxes.\n\n        Args:\n            assign_result (:obj:`AssignResult`): Bbox assigning results.\n            bboxes (Tensor): Boxes to be sampled from.\n            gt_bboxes (Tensor): Ground truth bboxes.\n            gt_labels (Tensor, optional): Class labels of ground truth bboxes.\n\n        Returns:\n            tuple[:obj:`SamplingResult`, Tensor]: Sampling result and negetive\n                label weights.\n        """"""\n        bboxes = bboxes[:, :4]\n\n        gt_flags = bboxes.new_zeros((bboxes.shape[0], ), dtype=torch.uint8)\n        if self.add_gt_as_proposals:\n            bboxes = torch.cat([gt_bboxes, bboxes], dim=0)\n            assign_result.add_gt_(gt_labels)\n            gt_ones = bboxes.new_ones(gt_bboxes.shape[0], dtype=torch.uint8)\n            gt_flags = torch.cat([gt_ones, gt_flags])\n\n        num_expected_pos = int(self.num * self.pos_fraction)\n        pos_inds = self.pos_sampler._sample_pos(\n            assign_result, num_expected_pos, bboxes=bboxes, **kwargs)\n        num_sampled_pos = pos_inds.numel()\n        num_expected_neg = self.num - num_sampled_pos\n        if self.neg_pos_ub >= 0:\n            _pos = max(1, num_sampled_pos)\n            neg_upper_bound = int(self.neg_pos_ub * _pos)\n            if num_expected_neg > neg_upper_bound:\n                num_expected_neg = neg_upper_bound\n        neg_inds, neg_label_weights = self.neg_sampler._sample_neg(\n            assign_result,\n            num_expected_neg,\n            bboxes,\n            img_meta=img_meta,\n            **kwargs)\n\n        return SamplingResult(pos_inds, neg_inds, bboxes, gt_bboxes,\n                              assign_result, gt_flags), neg_label_weights\n'"
mmdet/models/roi_heads/bbox_heads/__init__.py,0,"b""from .bbox_head import BBoxHead\nfrom .convfc_bbox_head import (ConvFCBBoxHead, Shared2FCBBoxHead,\n                               Shared4Conv1FCBBoxHead)\nfrom .double_bbox_head import DoubleConvFCBBoxHead\n\n__all__ = [\n    'BBoxHead', 'ConvFCBBoxHead', 'Shared2FCBBoxHead',\n    'Shared4Conv1FCBBoxHead', 'DoubleConvFCBBoxHead'\n]\n"""
mmdet/models/roi_heads/bbox_heads/bbox_head.py,23,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.modules.utils import _pair\n\nfrom mmdet.core import (auto_fp16, build_bbox_coder, force_fp32, multi_apply,\n                        multiclass_nms)\nfrom mmdet.models.builder import HEADS, build_loss\nfrom mmdet.models.losses import accuracy\n\n\n@HEADS.register_module()\nclass BBoxHead(nn.Module):\n    """"""Simplest RoI head, with only two fc layers for classification and\n    regression respectively""""""\n\n    def __init__(self,\n                 with_avg_pool=False,\n                 with_cls=True,\n                 with_reg=True,\n                 roi_feat_size=7,\n                 in_channels=256,\n                 num_classes=80,\n                 bbox_coder=dict(\n                     type=\'DeltaXYWHBBoxCoder\',\n                     target_means=[0., 0., 0., 0.],\n                     target_stds=[0.1, 0.1, 0.2, 0.2]),\n                 reg_class_agnostic=False,\n                 reg_decoded_bbox=False,\n                 loss_cls=dict(\n                     type=\'CrossEntropyLoss\',\n                     use_sigmoid=False,\n                     loss_weight=1.0),\n                 loss_bbox=dict(\n                     type=\'SmoothL1Loss\', beta=1.0, loss_weight=1.0)):\n        super(BBoxHead, self).__init__()\n        assert with_cls or with_reg\n        self.with_avg_pool = with_avg_pool\n        self.with_cls = with_cls\n        self.with_reg = with_reg\n        self.roi_feat_size = _pair(roi_feat_size)\n        self.roi_feat_area = self.roi_feat_size[0] * self.roi_feat_size[1]\n        self.in_channels = in_channels\n        self.num_classes = num_classes\n        self.reg_class_agnostic = reg_class_agnostic\n        self.reg_decoded_bbox = reg_decoded_bbox\n        self.fp16_enabled = False\n\n        self.bbox_coder = build_bbox_coder(bbox_coder)\n        self.loss_cls = build_loss(loss_cls)\n        self.loss_bbox = build_loss(loss_bbox)\n\n        in_channels = self.in_channels\n        if self.with_avg_pool:\n            self.avg_pool = nn.AvgPool2d(self.roi_feat_size)\n        else:\n            in_channels *= self.roi_feat_area\n        if self.with_cls:\n            # need to add background class\n            self.fc_cls = nn.Linear(in_channels, num_classes + 1)\n        if self.with_reg:\n            out_dim_reg = 4 if reg_class_agnostic else 4 * num_classes\n            self.fc_reg = nn.Linear(in_channels, out_dim_reg)\n        self.debug_imgs = None\n\n    def init_weights(self):\n        # conv layers are already initialized by ConvModule\n        if self.with_cls:\n            nn.init.normal_(self.fc_cls.weight, 0, 0.01)\n            nn.init.constant_(self.fc_cls.bias, 0)\n        if self.with_reg:\n            nn.init.normal_(self.fc_reg.weight, 0, 0.001)\n            nn.init.constant_(self.fc_reg.bias, 0)\n\n    @auto_fp16()\n    def forward(self, x):\n        if self.with_avg_pool:\n            x = self.avg_pool(x)\n        x = x.view(x.size(0), -1)\n        cls_score = self.fc_cls(x) if self.with_cls else None\n        bbox_pred = self.fc_reg(x) if self.with_reg else None\n        return cls_score, bbox_pred\n\n    def _get_target_single(self, pos_bboxes, neg_bboxes, pos_gt_bboxes,\n                           pos_gt_labels, cfg):\n        num_pos = pos_bboxes.size(0)\n        num_neg = neg_bboxes.size(0)\n        num_samples = num_pos + num_neg\n\n        # original implementation uses new_zeros since BG are set to be 0\n        # now use empty & fill because BG cat_id = num_classes,\n        # FG cat_id = [0, num_classes-1]\n        labels = pos_bboxes.new_full((num_samples, ),\n                                     self.num_classes,\n                                     dtype=torch.long)\n        label_weights = pos_bboxes.new_zeros(num_samples)\n        bbox_targets = pos_bboxes.new_zeros(num_samples, 4)\n        bbox_weights = pos_bboxes.new_zeros(num_samples, 4)\n        if num_pos > 0:\n            labels[:num_pos] = pos_gt_labels\n            pos_weight = 1.0 if cfg.pos_weight <= 0 else cfg.pos_weight\n            label_weights[:num_pos] = pos_weight\n            if not self.reg_decoded_bbox:\n                pos_bbox_targets = self.bbox_coder.encode(\n                    pos_bboxes, pos_gt_bboxes)\n            else:\n                pos_bbox_targets = pos_gt_bboxes\n            bbox_targets[:num_pos, :] = pos_bbox_targets\n            bbox_weights[:num_pos, :] = 1\n        if num_neg > 0:\n            label_weights[-num_neg:] = 1.0\n\n        return labels, label_weights, bbox_targets, bbox_weights\n\n    def get_targets(self,\n                    sampling_results,\n                    gt_bboxes,\n                    gt_labels,\n                    rcnn_train_cfg,\n                    concat=True):\n        pos_bboxes_list = [res.pos_bboxes for res in sampling_results]\n        neg_bboxes_list = [res.neg_bboxes for res in sampling_results]\n        pos_gt_bboxes_list = [res.pos_gt_bboxes for res in sampling_results]\n        pos_gt_labels_list = [res.pos_gt_labels for res in sampling_results]\n        labels, label_weights, bbox_targets, bbox_weights = multi_apply(\n            self._get_target_single,\n            pos_bboxes_list,\n            neg_bboxes_list,\n            pos_gt_bboxes_list,\n            pos_gt_labels_list,\n            cfg=rcnn_train_cfg)\n\n        if concat:\n            labels = torch.cat(labels, 0)\n            label_weights = torch.cat(label_weights, 0)\n            bbox_targets = torch.cat(bbox_targets, 0)\n            bbox_weights = torch.cat(bbox_weights, 0)\n        return labels, label_weights, bbox_targets, bbox_weights\n\n    @force_fp32(apply_to=(\'cls_score\', \'bbox_pred\'))\n    def loss(self,\n             cls_score,\n             bbox_pred,\n             rois,\n             labels,\n             label_weights,\n             bbox_targets,\n             bbox_weights,\n             reduction_override=None):\n        losses = dict()\n        if cls_score is not None:\n            avg_factor = max(torch.sum(label_weights > 0).float().item(), 1.)\n            if cls_score.numel() > 0:\n                losses[\'loss_cls\'] = self.loss_cls(\n                    cls_score,\n                    labels,\n                    label_weights,\n                    avg_factor=avg_factor,\n                    reduction_override=reduction_override)\n                losses[\'acc\'] = accuracy(cls_score, labels)\n        if bbox_pred is not None:\n            bg_class_ind = self.num_classes\n            # 0~self.num_classes-1 are FG, self.num_classes is BG\n            pos_inds = (labels >= 0) & (labels < bg_class_ind)\n            # do not perform bounding box regression for BG anymore.\n            if pos_inds.any():\n                if self.reg_decoded_bbox:\n                    bbox_pred = self.bbox_coder.decode(rois[:, 1:], bbox_pred)\n                if self.reg_class_agnostic:\n                    pos_bbox_pred = bbox_pred.view(\n                        bbox_pred.size(0), 4)[pos_inds.type(torch.bool)]\n                else:\n                    pos_bbox_pred = bbox_pred.view(\n                        bbox_pred.size(0), -1,\n                        4)[pos_inds.type(torch.bool),\n                           labels[pos_inds.type(torch.bool)]]\n                losses[\'loss_bbox\'] = self.loss_bbox(\n                    pos_bbox_pred,\n                    bbox_targets[pos_inds.type(torch.bool)],\n                    bbox_weights[pos_inds.type(torch.bool)],\n                    avg_factor=bbox_targets.size(0),\n                    reduction_override=reduction_override)\n            else:\n                losses[\'loss_bbox\'] = bbox_pred.sum() * 0\n        return losses\n\n    @force_fp32(apply_to=(\'cls_score\', \'bbox_pred\'))\n    def get_bboxes(self,\n                   rois,\n                   cls_score,\n                   bbox_pred,\n                   img_shape,\n                   scale_factor,\n                   rescale=False,\n                   cfg=None):\n        if isinstance(cls_score, list):\n            cls_score = sum(cls_score) / float(len(cls_score))\n        scores = F.softmax(cls_score, dim=1) if cls_score is not None else None\n\n        if bbox_pred is not None:\n            bboxes = self.bbox_coder.decode(\n                rois[:, 1:], bbox_pred, max_shape=img_shape)\n        else:\n            bboxes = rois[:, 1:].clone()\n            if img_shape is not None:\n                bboxes[:, [0, 2]].clamp_(min=0, max=img_shape[1])\n                bboxes[:, [1, 3]].clamp_(min=0, max=img_shape[0])\n\n        if rescale:\n            if isinstance(scale_factor, float):\n                bboxes /= scale_factor\n            else:\n                scale_factor = bboxes.new_tensor(scale_factor)\n                bboxes = (bboxes.view(bboxes.size(0), -1, 4) /\n                          scale_factor).view(bboxes.size()[0], -1)\n\n        if cfg is None:\n            return bboxes, scores\n        else:\n            det_bboxes, det_labels = multiclass_nms(bboxes, scores,\n                                                    cfg.score_thr, cfg.nms,\n                                                    cfg.max_per_img)\n\n            return det_bboxes, det_labels\n\n    @force_fp32(apply_to=(\'bbox_preds\', ))\n    def refine_bboxes(self, rois, labels, bbox_preds, pos_is_gts, img_metas):\n        """"""Refine bboxes during training.\n\n        Args:\n            rois (Tensor): Shape (n*bs, 5), where n is image number per GPU,\n                and bs is the sampled RoIs per image. The first column is\n                the image id and the next 4 columns are x1, y1, x2, y2.\n            labels (Tensor): Shape (n*bs, ).\n            bbox_preds (Tensor): Shape (n*bs, 4) or (n*bs, 4*#class).\n            pos_is_gts (list[Tensor]): Flags indicating if each positive bbox\n                is a gt bbox.\n            img_metas (list[dict]): Meta info of each image.\n\n        Returns:\n            list[Tensor]: Refined bboxes of each image in a mini-batch.\n\n        Example:\n            >>> # xdoctest: +REQUIRES(module:kwarray)\n            >>> import kwarray\n            >>> import numpy as np\n            >>> from mmdet.core.bbox.demodata import random_boxes\n            >>> self = BBoxHead(reg_class_agnostic=True)\n            >>> n_roi = 2\n            >>> n_img = 4\n            >>> scale = 512\n            >>> rng = np.random.RandomState(0)\n            >>> img_metas = [{\'img_shape\': (scale, scale)}\n            ...              for _ in range(n_img)]\n            >>> # Create rois in the expected format\n            >>> roi_boxes = random_boxes(n_roi, scale=scale, rng=rng)\n            >>> img_ids = torch.randint(0, n_img, (n_roi,))\n            >>> img_ids = img_ids.float()\n            >>> rois = torch.cat([img_ids[:, None], roi_boxes], dim=1)\n            >>> # Create other args\n            >>> labels = torch.randint(0, 2, (n_roi,)).long()\n            >>> bbox_preds = random_boxes(n_roi, scale=scale, rng=rng)\n            >>> # For each image, pretend random positive boxes are gts\n            >>> is_label_pos = (labels.numpy() > 0).astype(np.int)\n            >>> lbl_per_img = kwarray.group_items(is_label_pos,\n            ...                                   img_ids.numpy())\n            >>> pos_per_img = [sum(lbl_per_img.get(gid, []))\n            ...                for gid in range(n_img)]\n            >>> pos_is_gts = [\n            >>>     torch.randint(0, 2, (npos,)).byte().sort(\n            >>>         descending=True)[0]\n            >>>     for npos in pos_per_img\n            >>> ]\n            >>> bboxes_list = self.refine_bboxes(rois, labels, bbox_preds,\n            >>>                    pos_is_gts, img_metas)\n            >>> print(bboxes_list)\n        """"""\n        img_ids = rois[:, 0].long().unique(sorted=True)\n        assert img_ids.numel() <= len(img_metas)\n\n        bboxes_list = []\n        for i in range(len(img_metas)):\n            inds = torch.nonzero(\n                rois[:, 0] == i, as_tuple=False).squeeze(dim=1)\n            num_rois = inds.numel()\n\n            bboxes_ = rois[inds, 1:]\n            label_ = labels[inds]\n            bbox_pred_ = bbox_preds[inds]\n            img_meta_ = img_metas[i]\n            pos_is_gts_ = pos_is_gts[i]\n\n            bboxes = self.regress_by_class(bboxes_, label_, bbox_pred_,\n                                           img_meta_)\n\n            # filter gt bboxes\n            pos_keep = 1 - pos_is_gts_\n            keep_inds = pos_is_gts_.new_ones(num_rois)\n            keep_inds[:len(pos_is_gts_)] = pos_keep\n\n            bboxes_list.append(bboxes[keep_inds.type(torch.bool)])\n\n        return bboxes_list\n\n    @force_fp32(apply_to=(\'bbox_pred\', ))\n    def regress_by_class(self, rois, label, bbox_pred, img_meta):\n        """"""Regress the bbox for the predicted class. Used in Cascade R-CNN.\n\n        Args:\n            rois (Tensor): shape (n, 4) or (n, 5)\n            label (Tensor): shape (n, )\n            bbox_pred (Tensor): shape (n, 4*(#class)) or (n, 4)\n            img_meta (dict): Image meta info.\n\n        Returns:\n            Tensor: Regressed bboxes, the same shape as input rois.\n        """"""\n        assert rois.size(1) == 4 or rois.size(1) == 5, repr(rois.shape)\n\n        if not self.reg_class_agnostic:\n            label = label * 4\n            inds = torch.stack((label, label + 1, label + 2, label + 3), 1)\n            bbox_pred = torch.gather(bbox_pred, 1, inds)\n        assert bbox_pred.size(1) == 4\n\n        if rois.size(1) == 4:\n            new_rois = self.bbox_coder.decode(\n                rois, bbox_pred, max_shape=img_meta[\'img_shape\'])\n        else:\n            bboxes = self.bbox_coder.decode(\n                rois[:, 1:], bbox_pred, max_shape=img_meta[\'img_shape\'])\n            new_rois = torch.cat((rois[:, [0]], bboxes), dim=1)\n\n        return new_rois\n'"
mmdet/models/roi_heads/bbox_heads/convfc_bbox_head.py,1,"b'import torch.nn as nn\nfrom mmcv.cnn import ConvModule\n\nfrom mmdet.models.builder import HEADS\nfrom .bbox_head import BBoxHead\n\n\n@HEADS.register_module()\nclass ConvFCBBoxHead(BBoxHead):\n    r""""""More general bbox head, with shared conv and fc layers and two optional\n    separated branches.\n\n    .. code-block:: none\n\n                                    /-> cls convs -> cls fcs -> cls\n        shared convs -> shared fcs\n                                    \\-> reg convs -> reg fcs -> reg\n    """"""  # noqa: W605\n\n    def __init__(self,\n                 num_shared_convs=0,\n                 num_shared_fcs=0,\n                 num_cls_convs=0,\n                 num_cls_fcs=0,\n                 num_reg_convs=0,\n                 num_reg_fcs=0,\n                 conv_out_channels=256,\n                 fc_out_channels=1024,\n                 conv_cfg=None,\n                 norm_cfg=None,\n                 *args,\n                 **kwargs):\n        super(ConvFCBBoxHead, self).__init__(*args, **kwargs)\n        assert (num_shared_convs + num_shared_fcs + num_cls_convs +\n                num_cls_fcs + num_reg_convs + num_reg_fcs > 0)\n        if num_cls_convs > 0 or num_reg_convs > 0:\n            assert num_shared_fcs == 0\n        if not self.with_cls:\n            assert num_cls_convs == 0 and num_cls_fcs == 0\n        if not self.with_reg:\n            assert num_reg_convs == 0 and num_reg_fcs == 0\n        self.num_shared_convs = num_shared_convs\n        self.num_shared_fcs = num_shared_fcs\n        self.num_cls_convs = num_cls_convs\n        self.num_cls_fcs = num_cls_fcs\n        self.num_reg_convs = num_reg_convs\n        self.num_reg_fcs = num_reg_fcs\n        self.conv_out_channels = conv_out_channels\n        self.fc_out_channels = fc_out_channels\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n\n        # add shared convs and fcs\n        self.shared_convs, self.shared_fcs, last_layer_dim = \\\n            self._add_conv_fc_branch(\n                self.num_shared_convs, self.num_shared_fcs, self.in_channels,\n                True)\n        self.shared_out_channels = last_layer_dim\n\n        # add cls specific branch\n        self.cls_convs, self.cls_fcs, self.cls_last_dim = \\\n            self._add_conv_fc_branch(\n                self.num_cls_convs, self.num_cls_fcs, self.shared_out_channels)\n\n        # add reg specific branch\n        self.reg_convs, self.reg_fcs, self.reg_last_dim = \\\n            self._add_conv_fc_branch(\n                self.num_reg_convs, self.num_reg_fcs, self.shared_out_channels)\n\n        if self.num_shared_fcs == 0 and not self.with_avg_pool:\n            if self.num_cls_fcs == 0:\n                self.cls_last_dim *= self.roi_feat_area\n            if self.num_reg_fcs == 0:\n                self.reg_last_dim *= self.roi_feat_area\n\n        self.relu = nn.ReLU(inplace=True)\n        # reconstruct fc_cls and fc_reg since input channels are changed\n        if self.with_cls:\n            self.fc_cls = nn.Linear(self.cls_last_dim, self.num_classes + 1)\n        if self.with_reg:\n            out_dim_reg = (4 if self.reg_class_agnostic else 4 *\n                           self.num_classes)\n            self.fc_reg = nn.Linear(self.reg_last_dim, out_dim_reg)\n\n    def _add_conv_fc_branch(self,\n                            num_branch_convs,\n                            num_branch_fcs,\n                            in_channels,\n                            is_shared=False):\n        """"""Add shared or separable branch\n\n        convs -> avg pool (optional) -> fcs\n        """"""\n        last_layer_dim = in_channels\n        # add branch specific conv layers\n        branch_convs = nn.ModuleList()\n        if num_branch_convs > 0:\n            for i in range(num_branch_convs):\n                conv_in_channels = (\n                    last_layer_dim if i == 0 else self.conv_out_channels)\n                branch_convs.append(\n                    ConvModule(\n                        conv_in_channels,\n                        self.conv_out_channels,\n                        3,\n                        padding=1,\n                        conv_cfg=self.conv_cfg,\n                        norm_cfg=self.norm_cfg))\n            last_layer_dim = self.conv_out_channels\n        # add branch specific fc layers\n        branch_fcs = nn.ModuleList()\n        if num_branch_fcs > 0:\n            # for shared branch, only consider self.with_avg_pool\n            # for separated branches, also consider self.num_shared_fcs\n            if (is_shared\n                    or self.num_shared_fcs == 0) and not self.with_avg_pool:\n                last_layer_dim *= self.roi_feat_area\n            for i in range(num_branch_fcs):\n                fc_in_channels = (\n                    last_layer_dim if i == 0 else self.fc_out_channels)\n                branch_fcs.append(\n                    nn.Linear(fc_in_channels, self.fc_out_channels))\n            last_layer_dim = self.fc_out_channels\n        return branch_convs, branch_fcs, last_layer_dim\n\n    def init_weights(self):\n        super(ConvFCBBoxHead, self).init_weights()\n        # conv layers are already initialized by ConvModule\n        for module_list in [self.shared_fcs, self.cls_fcs, self.reg_fcs]:\n            for m in module_list.modules():\n                if isinstance(m, nn.Linear):\n                    nn.init.xavier_uniform_(m.weight)\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        # shared part\n        if self.num_shared_convs > 0:\n            for conv in self.shared_convs:\n                x = conv(x)\n\n        if self.num_shared_fcs > 0:\n            if self.with_avg_pool:\n                x = self.avg_pool(x)\n\n            x = x.flatten(1)\n\n            for fc in self.shared_fcs:\n                x = self.relu(fc(x))\n        # separate branches\n        x_cls = x\n        x_reg = x\n\n        for conv in self.cls_convs:\n            x_cls = conv(x_cls)\n        if x_cls.dim() > 2:\n            if self.with_avg_pool:\n                x_cls = self.avg_pool(x_cls)\n            x_cls = x_cls.flatten(1)\n        for fc in self.cls_fcs:\n            x_cls = self.relu(fc(x_cls))\n\n        for conv in self.reg_convs:\n            x_reg = conv(x_reg)\n        if x_reg.dim() > 2:\n            if self.with_avg_pool:\n                x_reg = self.avg_pool(x_reg)\n            x_reg = x_reg.flatten(1)\n        for fc in self.reg_fcs:\n            x_reg = self.relu(fc(x_reg))\n\n        cls_score = self.fc_cls(x_cls) if self.with_cls else None\n        bbox_pred = self.fc_reg(x_reg) if self.with_reg else None\n        return cls_score, bbox_pred\n\n\n@HEADS.register_module()\nclass Shared2FCBBoxHead(ConvFCBBoxHead):\n\n    def __init__(self, fc_out_channels=1024, *args, **kwargs):\n        super(Shared2FCBBoxHead, self).__init__(\n            num_shared_convs=0,\n            num_shared_fcs=2,\n            num_cls_convs=0,\n            num_cls_fcs=0,\n            num_reg_convs=0,\n            num_reg_fcs=0,\n            fc_out_channels=fc_out_channels,\n            *args,\n            **kwargs)\n\n\n@HEADS.register_module()\nclass Shared4Conv1FCBBoxHead(ConvFCBBoxHead):\n\n    def __init__(self, fc_out_channels=1024, *args, **kwargs):\n        super(Shared4Conv1FCBBoxHead, self).__init__(\n            num_shared_convs=4,\n            num_shared_fcs=1,\n            num_cls_convs=0,\n            num_cls_fcs=0,\n            num_reg_convs=0,\n            num_reg_fcs=0,\n            fc_out_channels=fc_out_channels,\n            *args,\n            **kwargs)\n'"
mmdet/models/roi_heads/bbox_heads/double_bbox_head.py,1,"b'import torch.nn as nn\nfrom mmcv.cnn import ConvModule, normal_init, xavier_init\n\nfrom mmdet.models.backbones.resnet import Bottleneck\nfrom mmdet.models.builder import HEADS\nfrom .bbox_head import BBoxHead\n\n\nclass BasicResBlock(nn.Module):\n    """"""Basic residual block.\n\n    This block is a little different from the block in the ResNet backbone.\n    The kernel size of conv1 is 1 in this block while 3 in ResNet BasicBlock.\n\n    Args:\n        in_channels (int): Channels of the input feature map.\n        out_channels (int): Channels of the output feature map.\n        conv_cfg (dict): The config dict for convolution layers.\n        norm_cfg (dict): The config dict for normalization layers.\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 conv_cfg=None,\n                 norm_cfg=dict(type=\'BN\')):\n        super(BasicResBlock, self).__init__()\n\n        # main path\n        self.conv1 = ConvModule(\n            in_channels,\n            in_channels,\n            kernel_size=3,\n            padding=1,\n            bias=False,\n            conv_cfg=conv_cfg,\n            norm_cfg=norm_cfg)\n        self.conv2 = ConvModule(\n            in_channels,\n            out_channels,\n            kernel_size=1,\n            bias=False,\n            conv_cfg=conv_cfg,\n            norm_cfg=norm_cfg,\n            act_cfg=None)\n\n        # identity path\n        self.conv_identity = ConvModule(\n            in_channels,\n            out_channels,\n            kernel_size=1,\n            conv_cfg=conv_cfg,\n            norm_cfg=norm_cfg,\n            act_cfg=None)\n\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        identity = x\n\n        x = self.conv1(x)\n        x = self.conv2(x)\n\n        identity = self.conv_identity(identity)\n        out = x + identity\n\n        out = self.relu(out)\n        return out\n\n\n@HEADS.register_module()\nclass DoubleConvFCBBoxHead(BBoxHead):\n    r""""""Bbox head used in Double-Head R-CNN\n\n    .. code-block:: none\n\n                                          /-> cls\n                      /-> shared convs ->\n                                          \\-> reg\n        roi features\n                                          /-> cls\n                      \\-> shared fc    ->\n                                          \\-> reg\n    """"""  # noqa: W605\n\n    def __init__(self,\n                 num_convs=0,\n                 num_fcs=0,\n                 conv_out_channels=1024,\n                 fc_out_channels=1024,\n                 conv_cfg=None,\n                 norm_cfg=dict(type=\'BN\'),\n                 **kwargs):\n        kwargs.setdefault(\'with_avg_pool\', True)\n        super(DoubleConvFCBBoxHead, self).__init__(**kwargs)\n        assert self.with_avg_pool\n        assert num_convs > 0\n        assert num_fcs > 0\n        self.num_convs = num_convs\n        self.num_fcs = num_fcs\n        self.conv_out_channels = conv_out_channels\n        self.fc_out_channels = fc_out_channels\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n\n        # increase the channel of input features\n        self.res_block = BasicResBlock(self.in_channels,\n                                       self.conv_out_channels)\n\n        # add conv heads\n        self.conv_branch = self._add_conv_branch()\n        # add fc heads\n        self.fc_branch = self._add_fc_branch()\n\n        out_dim_reg = 4 if self.reg_class_agnostic else 4 * self.num_classes\n        self.fc_reg = nn.Linear(self.conv_out_channels, out_dim_reg)\n\n        self.fc_cls = nn.Linear(self.fc_out_channels, self.num_classes + 1)\n        self.relu = nn.ReLU(inplace=True)\n\n    def _add_conv_branch(self):\n        """"""Add the fc branch which consists of a sequential of conv layers""""""\n        branch_convs = nn.ModuleList()\n        for i in range(self.num_convs):\n            branch_convs.append(\n                Bottleneck(\n                    inplanes=self.conv_out_channels,\n                    planes=self.conv_out_channels // 4,\n                    conv_cfg=self.conv_cfg,\n                    norm_cfg=self.norm_cfg))\n        return branch_convs\n\n    def _add_fc_branch(self):\n        """"""Add the fc branch which consists of a sequential of fc layers""""""\n        branch_fcs = nn.ModuleList()\n        for i in range(self.num_fcs):\n            fc_in_channels = (\n                self.in_channels *\n                self.roi_feat_area if i == 0 else self.fc_out_channels)\n            branch_fcs.append(nn.Linear(fc_in_channels, self.fc_out_channels))\n        return branch_fcs\n\n    def init_weights(self):\n        # conv layers are already initialized by ConvModule\n        normal_init(self.fc_cls, std=0.01)\n        normal_init(self.fc_reg, std=0.001)\n\n        for m in self.fc_branch.modules():\n            if isinstance(m, nn.Linear):\n                xavier_init(m, distribution=\'uniform\')\n\n    def forward(self, x_cls, x_reg):\n        # conv head\n        x_conv = self.res_block(x_reg)\n\n        for conv in self.conv_branch:\n            x_conv = conv(x_conv)\n\n        if self.with_avg_pool:\n            x_conv = self.avg_pool(x_conv)\n\n        x_conv = x_conv.view(x_conv.size(0), -1)\n        bbox_pred = self.fc_reg(x_conv)\n\n        # fc head\n        x_fc = x_cls.view(x_cls.size(0), -1)\n        for fc in self.fc_branch:\n            x_fc = self.relu(fc(x_fc))\n\n        cls_score = self.fc_cls(x_fc)\n\n        return cls_score, bbox_pred\n'"
mmdet/models/roi_heads/mask_heads/__init__.py,0,"b""from .fcn_mask_head import FCNMaskHead\nfrom .fused_semantic_head import FusedSemanticHead\nfrom .grid_head import GridHead\nfrom .htc_mask_head import HTCMaskHead\nfrom .maskiou_head import MaskIoUHead\n\n__all__ = [\n    'FCNMaskHead', 'HTCMaskHead', 'FusedSemanticHead', 'GridHead',\n    'MaskIoUHead'\n]\n"""
mmdet/models/roi_heads/mask_heads/fcn_mask_head.py,28,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import ConvModule, build_upsample_layer\nfrom torch.nn.modules.utils import _pair\n\nfrom mmdet.core import auto_fp16, force_fp32, mask_target\nfrom mmdet.models.builder import HEADS, build_loss\nfrom mmdet.ops import Conv2d\nfrom mmdet.ops.carafe import CARAFEPack\n\nBYTES_PER_FLOAT = 4\n# TODO: This memory limit may be too much or too little. It would be better to\n# determine it based on available resources.\nGPU_MEM_LIMIT = 1024**3  # 1 GB memory limit\n\n\n@HEADS.register_module()\nclass FCNMaskHead(nn.Module):\n\n    def __init__(self,\n                 num_convs=4,\n                 roi_feat_size=14,\n                 in_channels=256,\n                 conv_kernel_size=3,\n                 conv_out_channels=256,\n                 num_classes=80,\n                 class_agnostic=False,\n                 upsample_cfg=dict(type=\'deconv\', scale_factor=2),\n                 conv_cfg=None,\n                 norm_cfg=None,\n                 loss_mask=dict(\n                     type=\'CrossEntropyLoss\', use_mask=True, loss_weight=1.0)):\n        super(FCNMaskHead, self).__init__()\n        self.upsample_cfg = upsample_cfg.copy()\n        if self.upsample_cfg[\'type\'] not in [\n                None, \'deconv\', \'nearest\', \'bilinear\', \'carafe\'\n        ]:\n            raise ValueError(\n                f\'Invalid upsample method {self.upsample_cfg[""type""]}, \'\n                \'accepted methods are ""deconv"", ""nearest"", ""bilinear"", \'\n                \'""carafe""\')\n        self.num_convs = num_convs\n        # WARN: roi_feat_size is reserved and not used\n        self.roi_feat_size = _pair(roi_feat_size)\n        self.in_channels = in_channels\n        self.conv_kernel_size = conv_kernel_size\n        self.conv_out_channels = conv_out_channels\n        self.upsample_method = self.upsample_cfg.get(\'type\')\n        self.scale_factor = self.upsample_cfg.pop(\'scale_factor\')\n        self.num_classes = num_classes\n        self.class_agnostic = class_agnostic\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        self.fp16_enabled = False\n        self.loss_mask = build_loss(loss_mask)\n\n        self.convs = nn.ModuleList()\n        for i in range(self.num_convs):\n            in_channels = (\n                self.in_channels if i == 0 else self.conv_out_channels)\n            padding = (self.conv_kernel_size - 1) // 2\n            self.convs.append(\n                ConvModule(\n                    in_channels,\n                    self.conv_out_channels,\n                    self.conv_kernel_size,\n                    padding=padding,\n                    conv_cfg=conv_cfg,\n                    norm_cfg=norm_cfg))\n        upsample_in_channels = (\n            self.conv_out_channels if self.num_convs > 0 else in_channels)\n        upsample_cfg_ = self.upsample_cfg.copy()\n        if self.upsample_method is None:\n            self.upsample = None\n        elif self.upsample_method == \'deconv\':\n            upsample_cfg_.update(\n                in_channels=upsample_in_channels,\n                out_channels=self.conv_out_channels,\n                kernel_size=self.scale_factor,\n                stride=self.scale_factor)\n        elif self.upsample_method == \'carafe\':\n            upsample_cfg_.update(\n                channels=upsample_in_channels, scale_factor=self.scale_factor)\n        else:\n            # suppress warnings\n            align_corners = (None\n                             if self.upsample_method == \'nearest\' else False)\n            upsample_cfg_.update(\n                scale_factor=self.scale_factor,\n                mode=self.upsample_method,\n                align_corners=align_corners)\n        self.upsample = build_upsample_layer(upsample_cfg_)\n\n        out_channels = 1 if self.class_agnostic else self.num_classes\n        logits_in_channel = (\n            self.conv_out_channels\n            if self.upsample_method == \'deconv\' else upsample_in_channels)\n        self.conv_logits = Conv2d(logits_in_channel, out_channels, 1)\n        self.relu = nn.ReLU(inplace=True)\n        self.debug_imgs = None\n\n    def init_weights(self):\n        for m in [self.upsample, self.conv_logits]:\n            if m is None:\n                continue\n            elif isinstance(m, CARAFEPack):\n                m.init_weights()\n            else:\n                nn.init.kaiming_normal_(\n                    m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n                nn.init.constant_(m.bias, 0)\n\n    @auto_fp16()\n    def forward(self, x):\n        for conv in self.convs:\n            x = conv(x)\n        if self.upsample is not None:\n            x = self.upsample(x)\n            if self.upsample_method == \'deconv\':\n                x = self.relu(x)\n        mask_pred = self.conv_logits(x)\n        return mask_pred\n\n    def get_targets(self, sampling_results, gt_masks, rcnn_train_cfg):\n        pos_proposals = [res.pos_bboxes for res in sampling_results]\n        pos_assigned_gt_inds = [\n            res.pos_assigned_gt_inds for res in sampling_results\n        ]\n        mask_targets = mask_target(pos_proposals, pos_assigned_gt_inds,\n                                   gt_masks, rcnn_train_cfg)\n        return mask_targets\n\n    @force_fp32(apply_to=(\'mask_pred\', ))\n    def loss(self, mask_pred, mask_targets, labels):\n        loss = dict()\n        if mask_pred.size(0) == 0:\n            loss_mask = mask_pred.sum() * 0\n        else:\n            if self.class_agnostic:\n                loss_mask = self.loss_mask(mask_pred, mask_targets,\n                                           torch.zeros_like(labels))\n            else:\n                loss_mask = self.loss_mask(mask_pred, mask_targets, labels)\n        loss[\'loss_mask\'] = loss_mask\n        return loss\n\n    def get_seg_masks(self, mask_pred, det_bboxes, det_labels, rcnn_test_cfg,\n                      ori_shape, scale_factor, rescale):\n        """"""Get segmentation masks from mask_pred and bboxes.\n\n        Args:\n            mask_pred (Tensor or ndarray): shape (n, #class, h, w).\n                For single-scale testing, mask_pred is the direct output of\n                model, whose type is Tensor, while for multi-scale testing,\n                it will be converted to numpy array outside of this method.\n            det_bboxes (Tensor): shape (n, 4/5)\n            det_labels (Tensor): shape (n, )\n            img_shape (Tensor): shape (3, )\n            rcnn_test_cfg (dict): rcnn testing config\n            ori_shape: original image size\n\n        Returns:\n            list[list]: encoded masks\n        """"""\n        if isinstance(mask_pred, torch.Tensor):\n            mask_pred = mask_pred.sigmoid()\n        else:\n            mask_pred = det_bboxes.new_tensor(mask_pred)\n\n        device = mask_pred.device\n        cls_segms = [[] for _ in range(self.num_classes)\n                     ]  # BG is not included in num_classes\n        bboxes = det_bboxes[:, :4]\n        labels = det_labels\n\n        if rescale:\n            img_h, img_w = ori_shape[:2]\n        else:\n            img_h = np.round(ori_shape[0] * scale_factor).astype(np.int32)\n            img_w = np.round(ori_shape[1] * scale_factor).astype(np.int32)\n            scale_factor = 1.0\n\n        if not isinstance(scale_factor, (float, torch.Tensor)):\n            scale_factor = bboxes.new_tensor(scale_factor)\n        bboxes = bboxes / scale_factor\n\n        N = len(mask_pred)\n        # The actual implementation split the input into chunks,\n        # and paste them chunk by chunk.\n        if device.type == \'cpu\':\n            # CPU is most efficient when they are pasted one by one with\n            # skip_empty=True, so that it performs minimal number of\n            # operations.\n            num_chunks = N\n        else:\n            # GPU benefits from parallelism for larger chunks,\n            # but may have memory issue\n            num_chunks = int(\n                np.ceil(N * img_h * img_w * BYTES_PER_FLOAT / GPU_MEM_LIMIT))\n            assert (num_chunks <=\n                    N), \'Default GPU_MEM_LIMIT is too small; try increasing it\'\n        chunks = torch.chunk(torch.arange(N, device=device), num_chunks)\n\n        threshold = rcnn_test_cfg.mask_thr_binary\n        im_mask = torch.zeros(\n            N,\n            img_h,\n            img_w,\n            device=device,\n            dtype=torch.bool if threshold >= 0 else torch.uint8)\n\n        if not self.class_agnostic:\n            mask_pred = mask_pred[range(N), labels][:, None]\n\n        for inds in chunks:\n            masks_chunk, spatial_inds = _do_paste_mask(\n                mask_pred[inds],\n                bboxes[inds],\n                img_h,\n                img_w,\n                skip_empty=device.type == \'cpu\')\n\n            if threshold >= 0:\n                masks_chunk = (masks_chunk >= threshold).to(dtype=torch.bool)\n            else:\n                # for visualization and debugging\n                masks_chunk = (masks_chunk * 255).to(dtype=torch.uint8)\n\n            im_mask[(inds, ) + spatial_inds] = masks_chunk\n\n        for i in range(N):\n            cls_segms[labels[i]].append(im_mask[i].cpu().numpy())\n        return cls_segms\n\n\ndef _do_paste_mask(masks, boxes, img_h, img_w, skip_empty=True):\n    """"""Paste instance masks acoording to boxes.\n\n    This implementation is modified from\n    https://github.com/facebookresearch/detectron2/\n\n    Args:\n        masks (Tensor): N, 1, H, W\n        boxes (Tensor): N, 4\n        img_h (int): Height of the image to be pasted.\n        img_w (int): Width of the image to be pasted.\n        skip_empty (bool): Only paste masks within the region that\n            tightly bound all boxes, and returns the results this region only.\n            An important optimization for CPU.\n\n    Returns:\n        tuple: (Tensor, tuple). The first item is mask tensor, the second one\n            is the slice object.\n        If skip_empty == False, the whole image will be pasted. It will\n            return a mask of shape (N, img_h, img_w) and an empty tuple.\n        If skip_empty == True, only area around the mask will be pasted.\n            A mask of shape (N, h\', w\') and its start and end coordinates\n            in the original image will be returned.\n    """"""\n    # On GPU, paste all masks together (up to chunk size)\n    # by using the entire image to sample the masks\n    # Compared to pasting them one by one,\n    # this has more operations but is faster on COCO-scale dataset.\n    device = masks.device\n    if skip_empty:\n        x0_int, y0_int = torch.clamp(\n            boxes.min(dim=0).values.floor()[:2] - 1,\n            min=0).to(dtype=torch.int32)\n        x1_int = torch.clamp(\n            boxes[:, 2].max().ceil() + 1, max=img_w).to(dtype=torch.int32)\n        y1_int = torch.clamp(\n            boxes[:, 3].max().ceil() + 1, max=img_h).to(dtype=torch.int32)\n    else:\n        x0_int, y0_int = 0, 0\n        x1_int, y1_int = img_w, img_h\n    x0, y0, x1, y1 = torch.split(boxes, 1, dim=1)  # each is Nx1\n\n    N = masks.shape[0]\n\n    img_y = torch.arange(\n        y0_int, y1_int, device=device, dtype=torch.float32) + 0.5\n    img_x = torch.arange(\n        x0_int, x1_int, device=device, dtype=torch.float32) + 0.5\n    img_y = (img_y - y0) / (y1 - y0) * 2 - 1\n    img_x = (img_x - x0) / (x1 - x0) * 2 - 1\n    # img_x, img_y have shapes (N, w), (N, h)\n    if torch.isinf(img_x).any():\n        inds = torch.where(torch.isinf(img_x))\n        img_x[inds] = 0\n    if torch.isinf(img_y).any():\n        inds = torch.where(torch.isinf(img_y))\n        img_y[inds] = 0\n\n    gx = img_x[:, None, :].expand(N, img_y.size(1), img_x.size(1))\n    gy = img_y[:, :, None].expand(N, img_y.size(1), img_x.size(1))\n    grid = torch.stack([gx, gy], dim=3)\n\n    img_masks = F.grid_sample(\n        masks.to(dtype=torch.float32), grid, align_corners=False)\n\n    if skip_empty:\n        return img_masks[:, 0], (slice(y0_int, y1_int), slice(x0_int, x1_int))\n    else:\n        return img_masks[:, 0], ()\n'"
mmdet/models/roi_heads/mask_heads/fused_semantic_head.py,2,"b'import torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import ConvModule, kaiming_init\n\nfrom mmdet.core import auto_fp16, force_fp32\nfrom mmdet.models.builder import HEADS\n\n\n@HEADS.register_module()\nclass FusedSemanticHead(nn.Module):\n    r""""""Multi-level fused semantic segmentation head.\n\n    .. code-block:: none\n\n        in_1 -> 1x1 conv ---\n                            |\n        in_2 -> 1x1 conv -- |\n                           ||\n        in_3 -> 1x1 conv - ||\n                          |||                  /-> 1x1 conv (mask prediction)\n        in_4 -> 1x1 conv -----> 3x3 convs (*4)\n                            |                  \\-> 1x1 conv (feature)\n        in_5 -> 1x1 conv ---\n    """"""  # noqa: W605\n\n    def __init__(self,\n                 num_ins,\n                 fusion_level,\n                 num_convs=4,\n                 in_channels=256,\n                 conv_out_channels=256,\n                 num_classes=183,\n                 ignore_label=255,\n                 loss_weight=0.2,\n                 conv_cfg=None,\n                 norm_cfg=None):\n        super(FusedSemanticHead, self).__init__()\n        self.num_ins = num_ins\n        self.fusion_level = fusion_level\n        self.num_convs = num_convs\n        self.in_channels = in_channels\n        self.conv_out_channels = conv_out_channels\n        self.num_classes = num_classes\n        self.ignore_label = ignore_label\n        self.loss_weight = loss_weight\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        self.fp16_enabled = False\n\n        self.lateral_convs = nn.ModuleList()\n        for i in range(self.num_ins):\n            self.lateral_convs.append(\n                ConvModule(\n                    self.in_channels,\n                    self.in_channels,\n                    1,\n                    conv_cfg=self.conv_cfg,\n                    norm_cfg=self.norm_cfg,\n                    inplace=False))\n\n        self.convs = nn.ModuleList()\n        for i in range(self.num_convs):\n            in_channels = self.in_channels if i == 0 else conv_out_channels\n            self.convs.append(\n                ConvModule(\n                    in_channels,\n                    conv_out_channels,\n                    3,\n                    padding=1,\n                    conv_cfg=self.conv_cfg,\n                    norm_cfg=self.norm_cfg))\n        self.conv_embedding = ConvModule(\n            conv_out_channels,\n            conv_out_channels,\n            1,\n            conv_cfg=self.conv_cfg,\n            norm_cfg=self.norm_cfg)\n        self.conv_logits = nn.Conv2d(conv_out_channels, self.num_classes, 1)\n\n        self.criterion = nn.CrossEntropyLoss(ignore_index=ignore_label)\n\n    def init_weights(self):\n        kaiming_init(self.conv_logits)\n\n    @auto_fp16()\n    def forward(self, feats):\n        x = self.lateral_convs[self.fusion_level](feats[self.fusion_level])\n        fused_size = tuple(x.shape[-2:])\n        for i, feat in enumerate(feats):\n            if i != self.fusion_level:\n                feat = F.interpolate(\n                    feat, size=fused_size, mode=\'bilinear\', align_corners=True)\n                x += self.lateral_convs[i](feat)\n\n        for i in range(self.num_convs):\n            x = self.convs[i](x)\n\n        mask_pred = self.conv_logits(x)\n        x = self.conv_embedding(x)\n        return mask_pred, x\n\n    @force_fp32(apply_to=(\'mask_pred\', ))\n    def loss(self, mask_pred, labels):\n        labels = labels.squeeze(1).long()\n        loss_semantic_seg = self.criterion(mask_pred, labels)\n        loss_semantic_seg *= self.loss_weight\n        return loss_semantic_seg\n'"
mmdet/models/roi_heads/mask_heads/grid_head.py,10,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import ConvModule, kaiming_init, normal_init\n\nfrom mmdet.models.builder import HEADS, build_loss\n\n\n@HEADS.register_module()\nclass GridHead(nn.Module):\n\n    def __init__(self,\n                 grid_points=9,\n                 num_convs=8,\n                 roi_feat_size=14,\n                 in_channels=256,\n                 conv_kernel_size=3,\n                 point_feat_channels=64,\n                 deconv_kernel_size=4,\n                 class_agnostic=False,\n                 loss_grid=dict(\n                     type=\'CrossEntropyLoss\', use_sigmoid=True,\n                     loss_weight=15),\n                 conv_cfg=None,\n                 norm_cfg=dict(type=\'GN\', num_groups=36)):\n        super(GridHead, self).__init__()\n        self.grid_points = grid_points\n        self.num_convs = num_convs\n        self.roi_feat_size = roi_feat_size\n        self.in_channels = in_channels\n        self.conv_kernel_size = conv_kernel_size\n        self.point_feat_channels = point_feat_channels\n        self.conv_out_channels = self.point_feat_channels * self.grid_points\n        self.class_agnostic = class_agnostic\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        if isinstance(norm_cfg, dict) and norm_cfg[\'type\'] == \'GN\':\n            assert self.conv_out_channels % norm_cfg[\'num_groups\'] == 0\n\n        assert self.grid_points >= 4\n        self.grid_size = int(np.sqrt(self.grid_points))\n        if self.grid_size * self.grid_size != self.grid_points:\n            raise ValueError(\'grid_points must be a square number\')\n\n        # the predicted heatmap is half of whole_map_size\n        if not isinstance(self.roi_feat_size, int):\n            raise ValueError(\'Only square RoIs are supporeted in Grid R-CNN\')\n        self.whole_map_size = self.roi_feat_size * 4\n\n        # compute point-wise sub-regions\n        self.sub_regions = self.calc_sub_regions()\n\n        self.convs = []\n        for i in range(self.num_convs):\n            in_channels = (\n                self.in_channels if i == 0 else self.conv_out_channels)\n            stride = 2 if i == 0 else 1\n            padding = (self.conv_kernel_size - 1) // 2\n            self.convs.append(\n                ConvModule(\n                    in_channels,\n                    self.conv_out_channels,\n                    self.conv_kernel_size,\n                    stride=stride,\n                    padding=padding,\n                    conv_cfg=self.conv_cfg,\n                    norm_cfg=self.norm_cfg,\n                    bias=True))\n        self.convs = nn.Sequential(*self.convs)\n\n        self.deconv1 = nn.ConvTranspose2d(\n            self.conv_out_channels,\n            self.conv_out_channels,\n            kernel_size=deconv_kernel_size,\n            stride=2,\n            padding=(deconv_kernel_size - 2) // 2,\n            groups=grid_points)\n        self.norm1 = nn.GroupNorm(grid_points, self.conv_out_channels)\n        self.deconv2 = nn.ConvTranspose2d(\n            self.conv_out_channels,\n            grid_points,\n            kernel_size=deconv_kernel_size,\n            stride=2,\n            padding=(deconv_kernel_size - 2) // 2,\n            groups=grid_points)\n\n        # find the 4-neighbor of each grid point\n        self.neighbor_points = []\n        grid_size = self.grid_size\n        for i in range(grid_size):  # i-th column\n            for j in range(grid_size):  # j-th row\n                neighbors = []\n                if i > 0:  # left: (i - 1, j)\n                    neighbors.append((i - 1) * grid_size + j)\n                if j > 0:  # up: (i, j - 1)\n                    neighbors.append(i * grid_size + j - 1)\n                if j < grid_size - 1:  # down: (i, j + 1)\n                    neighbors.append(i * grid_size + j + 1)\n                if i < grid_size - 1:  # right: (i + 1, j)\n                    neighbors.append((i + 1) * grid_size + j)\n                self.neighbor_points.append(tuple(neighbors))\n        # total edges in the grid\n        self.num_edges = sum([len(p) for p in self.neighbor_points])\n\n        self.forder_trans = nn.ModuleList()  # first-order feature transition\n        self.sorder_trans = nn.ModuleList()  # second-order feature transition\n        for neighbors in self.neighbor_points:\n            fo_trans = nn.ModuleList()\n            so_trans = nn.ModuleList()\n            for _ in range(len(neighbors)):\n                # each transition module consists of a 5x5 depth-wise conv and\n                # 1x1 conv.\n                fo_trans.append(\n                    nn.Sequential(\n                        nn.Conv2d(\n                            self.point_feat_channels,\n                            self.point_feat_channels,\n                            5,\n                            stride=1,\n                            padding=2,\n                            groups=self.point_feat_channels),\n                        nn.Conv2d(self.point_feat_channels,\n                                  self.point_feat_channels, 1)))\n                so_trans.append(\n                    nn.Sequential(\n                        nn.Conv2d(\n                            self.point_feat_channels,\n                            self.point_feat_channels,\n                            5,\n                            1,\n                            2,\n                            groups=self.point_feat_channels),\n                        nn.Conv2d(self.point_feat_channels,\n                                  self.point_feat_channels, 1)))\n            self.forder_trans.append(fo_trans)\n            self.sorder_trans.append(so_trans)\n\n        self.loss_grid = build_loss(loss_grid)\n\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n                # TODO: compare mode = ""fan_in"" or ""fan_out""\n                kaiming_init(m)\n        for m in self.modules():\n            if isinstance(m, nn.ConvTranspose2d):\n                normal_init(m, std=0.001)\n        nn.init.constant_(self.deconv2.bias, -np.log(0.99 / 0.01))\n\n    def forward(self, x):\n        assert x.shape[-1] == x.shape[-2] == self.roi_feat_size\n        # RoI feature transformation, downsample 2x\n        x = self.convs(x)\n\n        c = self.point_feat_channels\n        # first-order fusion\n        x_fo = [None for _ in range(self.grid_points)]\n        for i, points in enumerate(self.neighbor_points):\n            x_fo[i] = x[:, i * c:(i + 1) * c]\n            for j, point_idx in enumerate(points):\n                x_fo[i] = x_fo[i] + self.forder_trans[i][j](\n                    x[:, point_idx * c:(point_idx + 1) * c])\n\n        # second-order fusion\n        x_so = [None for _ in range(self.grid_points)]\n        for i, points in enumerate(self.neighbor_points):\n            x_so[i] = x[:, i * c:(i + 1) * c]\n            for j, point_idx in enumerate(points):\n                x_so[i] = x_so[i] + self.sorder_trans[i][j](x_fo[point_idx])\n\n        # predicted heatmap with fused features\n        x2 = torch.cat(x_so, dim=1)\n        x2 = self.deconv1(x2)\n        x2 = F.relu(self.norm1(x2), inplace=True)\n        heatmap = self.deconv2(x2)\n\n        # predicted heatmap with original features (applicable during training)\n        if self.training:\n            x1 = x\n            x1 = self.deconv1(x1)\n            x1 = F.relu(self.norm1(x1), inplace=True)\n            heatmap_unfused = self.deconv2(x1)\n        else:\n            heatmap_unfused = heatmap\n\n        return dict(fused=heatmap, unfused=heatmap_unfused)\n\n    def calc_sub_regions(self):\n        """"""Compute point specific representation regions.\n\n        See Grid R-CNN Plus (https://arxiv.org/abs/1906.05688) for details.\n        """"""\n        # to make it consistent with the original implementation, half_size\n        # is computed as 2 * quarter_size, which is smaller\n        half_size = self.whole_map_size // 4 * 2\n        sub_regions = []\n        for i in range(self.grid_points):\n            x_idx = i // self.grid_size\n            y_idx = i % self.grid_size\n            if x_idx == 0:\n                sub_x1 = 0\n            elif x_idx == self.grid_size - 1:\n                sub_x1 = half_size\n            else:\n                ratio = x_idx / (self.grid_size - 1) - 0.25\n                sub_x1 = max(int(ratio * self.whole_map_size), 0)\n\n            if y_idx == 0:\n                sub_y1 = 0\n            elif y_idx == self.grid_size - 1:\n                sub_y1 = half_size\n            else:\n                ratio = y_idx / (self.grid_size - 1) - 0.25\n                sub_y1 = max(int(ratio * self.whole_map_size), 0)\n            sub_regions.append(\n                (sub_x1, sub_y1, sub_x1 + half_size, sub_y1 + half_size))\n        return sub_regions\n\n    def get_targets(self, sampling_results, rcnn_train_cfg):\n        # mix all samples (across images) together.\n        pos_bboxes = torch.cat([res.pos_bboxes for res in sampling_results],\n                               dim=0).cpu()\n        pos_gt_bboxes = torch.cat(\n            [res.pos_gt_bboxes for res in sampling_results], dim=0).cpu()\n        assert pos_bboxes.shape == pos_gt_bboxes.shape\n\n        # expand pos_bboxes to 2x of original size\n        x1 = pos_bboxes[:, 0] - (pos_bboxes[:, 2] - pos_bboxes[:, 0]) / 2\n        y1 = pos_bboxes[:, 1] - (pos_bboxes[:, 3] - pos_bboxes[:, 1]) / 2\n        x2 = pos_bboxes[:, 2] + (pos_bboxes[:, 2] - pos_bboxes[:, 0]) / 2\n        y2 = pos_bboxes[:, 3] + (pos_bboxes[:, 3] - pos_bboxes[:, 1]) / 2\n        pos_bboxes = torch.stack([x1, y1, x2, y2], dim=-1)\n        pos_bbox_ws = (pos_bboxes[:, 2] - pos_bboxes[:, 0]).unsqueeze(-1)\n        pos_bbox_hs = (pos_bboxes[:, 3] - pos_bboxes[:, 1]).unsqueeze(-1)\n\n        num_rois = pos_bboxes.shape[0]\n        map_size = self.whole_map_size\n        # this is not the final target shape\n        targets = torch.zeros((num_rois, self.grid_points, map_size, map_size),\n                              dtype=torch.float)\n\n        # pre-compute interpolation factors for all grid points.\n        # the first item is the factor of x-dim, and the second is y-dim.\n        # for a 9-point grid, factors are like (1, 0), (0.5, 0.5), (0, 1)\n        factors = []\n        for j in range(self.grid_points):\n            x_idx = j // self.grid_size\n            y_idx = j % self.grid_size\n            factors.append((1 - x_idx / (self.grid_size - 1),\n                            1 - y_idx / (self.grid_size - 1)))\n\n        radius = rcnn_train_cfg.pos_radius\n        radius2 = radius**2\n        for i in range(num_rois):\n            # ignore small bboxes\n            if (pos_bbox_ws[i] <= self.grid_size\n                    or pos_bbox_hs[i] <= self.grid_size):\n                continue\n            # for each grid point, mark a small circle as positive\n            for j in range(self.grid_points):\n                factor_x, factor_y = factors[j]\n                gridpoint_x = factor_x * pos_gt_bboxes[i, 0] + (\n                    1 - factor_x) * pos_gt_bboxes[i, 2]\n                gridpoint_y = factor_y * pos_gt_bboxes[i, 1] + (\n                    1 - factor_y) * pos_gt_bboxes[i, 3]\n\n                cx = int((gridpoint_x - pos_bboxes[i, 0]) / pos_bbox_ws[i] *\n                         map_size)\n                cy = int((gridpoint_y - pos_bboxes[i, 1]) / pos_bbox_hs[i] *\n                         map_size)\n\n                for x in range(cx - radius, cx + radius + 1):\n                    for y in range(cy - radius, cy + radius + 1):\n                        if x >= 0 and x < map_size and y >= 0 and y < map_size:\n                            if (x - cx)**2 + (y - cy)**2 <= radius2:\n                                targets[i, j, y, x] = 1\n        # reduce the target heatmap size by a half\n        # proposed in Grid R-CNN Plus (https://arxiv.org/abs/1906.05688).\n        sub_targets = []\n        for i in range(self.grid_points):\n            sub_x1, sub_y1, sub_x2, sub_y2 = self.sub_regions[i]\n            sub_targets.append(targets[:, [i], sub_y1:sub_y2, sub_x1:sub_x2])\n        sub_targets = torch.cat(sub_targets, dim=1)\n        sub_targets = sub_targets.to(sampling_results[0].pos_bboxes.device)\n        return sub_targets\n\n    def loss(self, grid_pred, grid_targets):\n        loss_fused = self.loss_grid(grid_pred[\'fused\'], grid_targets)\n        loss_unfused = self.loss_grid(grid_pred[\'unfused\'], grid_targets)\n        loss_grid = loss_fused + loss_unfused\n        return dict(loss_grid=loss_grid)\n\n    def get_bboxes(self, det_bboxes, grid_pred, img_metas):\n        # TODO: refactoring\n        assert det_bboxes.shape[0] == grid_pred.shape[0]\n        det_bboxes = det_bboxes.cpu()\n        cls_scores = det_bboxes[:, [4]]\n        det_bboxes = det_bboxes[:, :4]\n        grid_pred = grid_pred.sigmoid().cpu()\n\n        R, c, h, w = grid_pred.shape\n        half_size = self.whole_map_size // 4 * 2\n        assert h == w == half_size\n        assert c == self.grid_points\n\n        # find the point with max scores in the half-sized heatmap\n        grid_pred = grid_pred.view(R * c, h * w)\n        pred_scores, pred_position = grid_pred.max(dim=1)\n        xs = pred_position % w\n        ys = pred_position // w\n\n        # get the position in the whole heatmap instead of half-sized heatmap\n        for i in range(self.grid_points):\n            xs[i::self.grid_points] += self.sub_regions[i][0]\n            ys[i::self.grid_points] += self.sub_regions[i][1]\n\n        # reshape to (num_rois, grid_points)\n        pred_scores, xs, ys = tuple(\n            map(lambda x: x.view(R, c), [pred_scores, xs, ys]))\n\n        # get expanded pos_bboxes\n        widths = (det_bboxes[:, 2] - det_bboxes[:, 0]).unsqueeze(-1)\n        heights = (det_bboxes[:, 3] - det_bboxes[:, 1]).unsqueeze(-1)\n        x1 = (det_bboxes[:, 0, None] - widths / 2)\n        y1 = (det_bboxes[:, 1, None] - heights / 2)\n        # map the grid point to the absolute coordinates\n        abs_xs = (xs.float() + 0.5) / w * widths + x1\n        abs_ys = (ys.float() + 0.5) / h * heights + y1\n\n        # get the grid points indices that fall on the bbox boundaries\n        x1_inds = [i for i in range(self.grid_size)]\n        y1_inds = [i * self.grid_size for i in range(self.grid_size)]\n        x2_inds = [\n            self.grid_points - self.grid_size + i\n            for i in range(self.grid_size)\n        ]\n        y2_inds = [(i + 1) * self.grid_size - 1 for i in range(self.grid_size)]\n\n        # voting of all grid points on some boundary\n        bboxes_x1 = (abs_xs[:, x1_inds] * pred_scores[:, x1_inds]).sum(\n            dim=1, keepdim=True) / (\n                pred_scores[:, x1_inds].sum(dim=1, keepdim=True))\n        bboxes_y1 = (abs_ys[:, y1_inds] * pred_scores[:, y1_inds]).sum(\n            dim=1, keepdim=True) / (\n                pred_scores[:, y1_inds].sum(dim=1, keepdim=True))\n        bboxes_x2 = (abs_xs[:, x2_inds] * pred_scores[:, x2_inds]).sum(\n            dim=1, keepdim=True) / (\n                pred_scores[:, x2_inds].sum(dim=1, keepdim=True))\n        bboxes_y2 = (abs_ys[:, y2_inds] * pred_scores[:, y2_inds]).sum(\n            dim=1, keepdim=True) / (\n                pred_scores[:, y2_inds].sum(dim=1, keepdim=True))\n\n        bbox_res = torch.cat(\n            [bboxes_x1, bboxes_y1, bboxes_x2, bboxes_y2, cls_scores], dim=1)\n        bbox_res[:, [0, 2]].clamp_(min=0, max=img_metas[0][\'img_shape\'][1])\n        bbox_res[:, [1, 3]].clamp_(min=0, max=img_metas[0][\'img_shape\'][0])\n\n        return bbox_res\n'"
mmdet/models/roi_heads/mask_heads/htc_mask_head.py,0,"b""from mmcv.cnn import ConvModule\n\nfrom mmdet.models.builder import HEADS\nfrom .fcn_mask_head import FCNMaskHead\n\n\n@HEADS.register_module()\nclass HTCMaskHead(FCNMaskHead):\n\n    def __init__(self, with_conv_res=True, *args, **kwargs):\n        super(HTCMaskHead, self).__init__(*args, **kwargs)\n        self.with_conv_res = with_conv_res\n        if self.with_conv_res:\n            self.conv_res = ConvModule(\n                self.conv_out_channels,\n                self.conv_out_channels,\n                1,\n                conv_cfg=self.conv_cfg,\n                norm_cfg=self.norm_cfg)\n\n    def init_weights(self):\n        super(HTCMaskHead, self).init_weights()\n        if self.with_conv_res:\n            self.conv_res.init_weights()\n\n    def forward(self, x, res_feat=None, return_logits=True, return_feat=True):\n        if res_feat is not None:\n            assert self.with_conv_res\n            res_feat = self.conv_res(res_feat)\n            x = x + res_feat\n        for conv in self.convs:\n            x = conv(x)\n        res_feat = x\n        outs = []\n        if return_logits:\n            x = self.upsample(x)\n            if self.upsample_method == 'deconv':\n                x = self.relu(x)\n            mask_pred = self.conv_logits(x)\n            outs.append(mask_pred)\n        if return_feat:\n            outs.append(res_feat)\n        return outs if len(outs) > 1 else outs[0]\n"""
mmdet/models/roi_heads/mask_heads/maskiou_head.py,5,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nfrom mmcv.cnn import kaiming_init, normal_init\nfrom torch.nn.modules.utils import _pair\n\nfrom mmdet.core import force_fp32\nfrom mmdet.models.builder import HEADS, build_loss\nfrom mmdet.ops import Conv2d, Linear, MaxPool2d\n\n\n@HEADS.register_module()\nclass MaskIoUHead(nn.Module):\n    """"""Mask IoU Head.\n\n    This head predicts the IoU of predicted masks and corresponding gt masks.\n    """"""\n\n    def __init__(self,\n                 num_convs=4,\n                 num_fcs=2,\n                 roi_feat_size=14,\n                 in_channels=256,\n                 conv_out_channels=256,\n                 fc_out_channels=1024,\n                 num_classes=80,\n                 loss_iou=dict(type=\'MSELoss\', loss_weight=0.5)):\n        super(MaskIoUHead, self).__init__()\n        self.in_channels = in_channels\n        self.conv_out_channels = conv_out_channels\n        self.fc_out_channels = fc_out_channels\n        self.num_classes = num_classes\n        self.fp16_enabled = False\n\n        self.convs = nn.ModuleList()\n        for i in range(num_convs):\n            if i == 0:\n                # concatenation of mask feature and mask prediction\n                in_channels = self.in_channels + 1\n            else:\n                in_channels = self.conv_out_channels\n            stride = 2 if i == num_convs - 1 else 1\n            self.convs.append(\n                Conv2d(\n                    in_channels,\n                    self.conv_out_channels,\n                    3,\n                    stride=stride,\n                    padding=1))\n\n        roi_feat_size = _pair(roi_feat_size)\n        pooled_area = (roi_feat_size[0] // 2) * (roi_feat_size[1] // 2)\n        self.fcs = nn.ModuleList()\n        for i in range(num_fcs):\n            in_channels = (\n                self.conv_out_channels *\n                pooled_area if i == 0 else self.fc_out_channels)\n            self.fcs.append(Linear(in_channels, self.fc_out_channels))\n\n        self.fc_mask_iou = Linear(self.fc_out_channels, self.num_classes)\n        self.relu = nn.ReLU()\n        self.max_pool = MaxPool2d(2, 2)\n        self.loss_iou = build_loss(loss_iou)\n\n    def init_weights(self):\n        for conv in self.convs:\n            kaiming_init(conv)\n        for fc in self.fcs:\n            kaiming_init(\n                fc,\n                a=1,\n                mode=\'fan_in\',\n                nonlinearity=\'leaky_relu\',\n                distribution=\'uniform\')\n        normal_init(self.fc_mask_iou, std=0.01)\n\n    def forward(self, mask_feat, mask_pred):\n        mask_pred = mask_pred.sigmoid()\n        mask_pred_pooled = self.max_pool(mask_pred.unsqueeze(1))\n\n        x = torch.cat((mask_feat, mask_pred_pooled), 1)\n\n        for conv in self.convs:\n            x = self.relu(conv(x))\n        x = x.flatten(1)\n        for fc in self.fcs:\n            x = self.relu(fc(x))\n        mask_iou = self.fc_mask_iou(x)\n        return mask_iou\n\n    @force_fp32(apply_to=(\'mask_iou_pred\', ))\n    def loss(self, mask_iou_pred, mask_iou_targets):\n        pos_inds = mask_iou_targets > 0\n        if pos_inds.sum() > 0:\n            loss_mask_iou = self.loss_iou(mask_iou_pred[pos_inds],\n                                          mask_iou_targets[pos_inds])\n        else:\n            loss_mask_iou = mask_iou_pred.sum() * 0\n        return dict(loss_mask_iou=loss_mask_iou)\n\n    @force_fp32(apply_to=(\'mask_pred\', ))\n    def get_targets(self, sampling_results, gt_masks, mask_pred, mask_targets,\n                    rcnn_train_cfg):\n        """"""Compute target of mask IoU.\n\n        Mask IoU target is the IoU of the predicted mask (inside a bbox) and\n        the gt mask of corresponding gt mask (the whole instance).\n        The intersection area is computed inside the bbox, and the gt mask area\n        is computed with two steps, firstly we compute the gt area inside the\n        bbox, then divide it by the area ratio of gt area inside the bbox and\n        the gt area of the whole instance.\n\n        Args:\n            sampling_results (list[:obj:`SamplingResult`]): sampling results.\n            gt_masks (BitmapMask | PolygonMask): Gt masks (the whole instance)\n                of each image, with the same shape of the input image.\n            mask_pred (Tensor): Predicted masks of each positive proposal,\n                shape (num_pos, h, w).\n            mask_targets (Tensor): Gt mask of each positive proposal,\n                binary map of the shape (num_pos, h, w).\n            rcnn_train_cfg (dict): Training config for R-CNN part.\n\n        Returns:\n            Tensor: mask iou target (length == num positive).\n        """"""\n        pos_proposals = [res.pos_bboxes for res in sampling_results]\n        pos_assigned_gt_inds = [\n            res.pos_assigned_gt_inds for res in sampling_results\n        ]\n\n        # compute the area ratio of gt areas inside the proposals and\n        # the whole instance\n        area_ratios = map(self._get_area_ratio, pos_proposals,\n                          pos_assigned_gt_inds, gt_masks)\n        area_ratios = torch.cat(list(area_ratios))\n        assert mask_targets.size(0) == area_ratios.size(0)\n\n        mask_pred = (mask_pred > rcnn_train_cfg.mask_thr_binary).float()\n        mask_pred_areas = mask_pred.sum((-1, -2))\n\n        # mask_pred and mask_targets are binary maps\n        overlap_areas = (mask_pred * mask_targets).sum((-1, -2))\n\n        # compute the mask area of the whole instance\n        gt_full_areas = mask_targets.sum((-1, -2)) / (area_ratios + 1e-7)\n\n        mask_iou_targets = overlap_areas / (\n            mask_pred_areas + gt_full_areas - overlap_areas)\n        return mask_iou_targets\n\n    def _get_area_ratio(self, pos_proposals, pos_assigned_gt_inds, gt_masks):\n        """"""Compute area ratio of the gt mask inside the proposal and the gt\n        mask of the corresponding instance""""""\n        num_pos = pos_proposals.size(0)\n        if num_pos > 0:\n            area_ratios = []\n            proposals_np = pos_proposals.cpu().numpy()\n            pos_assigned_gt_inds = pos_assigned_gt_inds.cpu().numpy()\n            # compute mask areas of gt instances (batch processing for speedup)\n            gt_instance_mask_area = gt_masks.areas\n            for i in range(num_pos):\n                gt_mask = gt_masks[pos_assigned_gt_inds[i]]\n\n                # crop the gt mask inside the proposal\n                bbox = proposals_np[i, :].astype(np.int32)\n                gt_mask_in_proposal = gt_mask.crop(bbox)\n\n                ratio = gt_mask_in_proposal.areas[0] / (\n                    gt_instance_mask_area[pos_assigned_gt_inds[i]] + 1e-7)\n                area_ratios.append(ratio)\n            area_ratios = torch.from_numpy(np.stack(area_ratios)).float().to(\n                pos_proposals.device)\n        else:\n            area_ratios = pos_proposals.new_zeros((0, ))\n        return area_ratios\n\n    @force_fp32(apply_to=(\'mask_iou_pred\', ))\n    def get_mask_scores(self, mask_iou_pred, det_bboxes, det_labels):\n        """"""Get the mask scores.\n\n        mask_score = bbox_score * mask_iou\n        """"""\n        inds = range(det_labels.size(0))\n        mask_scores = mask_iou_pred[inds, det_labels] * det_bboxes[inds, -1]\n        mask_scores = mask_scores.cpu().numpy()\n        det_labels = det_labels.cpu().numpy()\n        return [mask_scores[det_labels == i] for i in range(self.num_classes)]\n'"
mmdet/models/roi_heads/roi_extractors/__init__.py,0,"b""from .groie import SumGenericRoiExtractor\nfrom .single_level import SingleRoIExtractor\n\n__all__ = [\n    'SingleRoIExtractor',\n    'SumGenericRoiExtractor',\n]\n"""
mmdet/models/roi_heads/roi_extractors/groie.py,0,"b'""""""Generic RoI Extractor.\n\nA novel Region of Interest Extraction Layer for Instance Segmentation.\n""""""\n\nfrom torch import nn\n\nfrom mmdet.core import force_fp32\nfrom mmdet.models.builder import ROI_EXTRACTORS\nfrom mmdet.ops.plugin import build_plugin_layer\nfrom .single_level import SingleRoIExtractor\n\n\n@ROI_EXTRACTORS.register_module\nclass SumGenericRoiExtractor(SingleRoIExtractor):\n    """"""Extract RoI features from all summed feature maps levels.\n\n    https://arxiv.org/abs/2004.13665\n\n    Args:\n        pre_cfg (dict): Specify pre-processing modules.\n        post_cfg (dict): Specify post-processing modules.\n        kwargs (keyword arguments): Arguments that are the same\n            as :class:`SingleRoIExtractor`.\n    """"""\n\n    def __init__(self, pre_cfg, post_cfg, **kwargs):\n        super(SumGenericRoiExtractor, self).__init__(**kwargs)\n\n        # build pre/post processing modules\n        self.post_module = build_plugin_layer(post_cfg, \'_post_module\')[1]\n        self.pre_module = build_plugin_layer(pre_cfg, \'_pre_module\')[1]\n        self.relu = nn.ReLU(inplace=False)\n\n    @force_fp32(apply_to=(\'feats\', ), out_fp16=True)\n    def forward(self, feats, rois, roi_scale_factor=None):\n        if len(feats) == 1:\n            return self.roi_layers[0](feats[0], rois)\n\n        out_size = self.roi_layers[0].out_size\n        num_levels = len(feats)\n        roi_feats = feats[0].new_zeros(\n            rois.size(0), self.out_channels, *out_size)\n\n        # some times rois is an empty tensor\n        if roi_feats.shape[0] == 0:\n            return roi_feats\n\n        if roi_scale_factor is not None:\n            rois = self.roi_rescale(rois, roi_scale_factor)\n\n        for i in range(num_levels):\n            # apply pre-processing to a RoI extracted from each layer\n            roi_feats_t = self.roi_layers[i](feats[i], rois)\n            roi_feats_t = self.pre_module(roi_feats_t)\n            roi_feats_t = self.relu(roi_feats_t)\n            # and sum them all\n            roi_feats += roi_feats_t\n\n        # apply post-processing before return the result\n        x = self.post_module(roi_feats)\n        return x\n'"
mmdet/models/roi_heads/roi_extractors/single_level.py,4,"b'import torch\nimport torch.nn as nn\n\nfrom mmdet import ops\nfrom mmdet.core import force_fp32\nfrom mmdet.models.builder import ROI_EXTRACTORS\n\n\n@ROI_EXTRACTORS.register_module()\nclass SingleRoIExtractor(nn.Module):\n    """"""Extract RoI features from a single level feature map.\n\n    If there are mulitple input feature levels, each RoI is mapped to a level\n    according to its scale.\n\n    Args:\n        roi_layer (dict): Specify RoI layer type and arguments.\n        out_channels (int): Output channels of RoI layers.\n        featmap_strides (int): Strides of input feature maps.\n        finest_scale (int): Scale threshold of mapping to level 0.\n    """"""\n\n    def __init__(self,\n                 roi_layer,\n                 out_channels,\n                 featmap_strides,\n                 finest_scale=56):\n        super(SingleRoIExtractor, self).__init__()\n        self.roi_layers = self.build_roi_layers(roi_layer, featmap_strides)\n        self.out_channels = out_channels\n        self.featmap_strides = featmap_strides\n        self.finest_scale = finest_scale\n        self.fp16_enabled = False\n\n    @property\n    def num_inputs(self):\n        """"""int: Input feature map levels.""""""\n        return len(self.featmap_strides)\n\n    def init_weights(self):\n        pass\n\n    def build_roi_layers(self, layer_cfg, featmap_strides):\n        cfg = layer_cfg.copy()\n        layer_type = cfg.pop(\'type\')\n        assert hasattr(ops, layer_type)\n        layer_cls = getattr(ops, layer_type)\n        roi_layers = nn.ModuleList(\n            [layer_cls(spatial_scale=1 / s, **cfg) for s in featmap_strides])\n        return roi_layers\n\n    def map_roi_levels(self, rois, num_levels):\n        """"""Map rois to corresponding feature levels by scales.\n\n        - scale < finest_scale * 2: level 0\n        - finest_scale * 2 <= scale < finest_scale * 4: level 1\n        - finest_scale * 4 <= scale < finest_scale * 8: level 2\n        - scale >= finest_scale * 8: level 3\n\n        Args:\n            rois (Tensor): Input RoIs, shape (k, 5).\n            num_levels (int): Total level number.\n\n        Returns:\n            Tensor: Level index (0-based) of each RoI, shape (k, )\n        """"""\n        scale = torch.sqrt(\n            (rois[:, 3] - rois[:, 1]) * (rois[:, 4] - rois[:, 2]))\n        target_lvls = torch.floor(torch.log2(scale / self.finest_scale + 1e-6))\n        target_lvls = target_lvls.clamp(min=0, max=num_levels - 1).long()\n        return target_lvls\n\n    def roi_rescale(self, rois, scale_factor):\n        cx = (rois[:, 1] + rois[:, 3]) * 0.5\n        cy = (rois[:, 2] + rois[:, 4]) * 0.5\n        w = rois[:, 3] - rois[:, 1]\n        h = rois[:, 4] - rois[:, 2]\n        new_w = w * scale_factor\n        new_h = h * scale_factor\n        x1 = cx - new_w * 0.5\n        x2 = cx + new_w * 0.5\n        y1 = cy - new_h * 0.5\n        y2 = cy + new_h * 0.5\n        new_rois = torch.stack((rois[:, 0], x1, y1, x2, y2), dim=-1)\n        return new_rois\n\n    @force_fp32(apply_to=(\'feats\', ), out_fp16=True)\n    def forward(self, feats, rois, roi_scale_factor=None):\n        out_size = self.roi_layers[0].out_size\n        num_levels = len(feats)\n        roi_feats = feats[0].new_zeros(\n            rois.size(0), self.out_channels, *out_size)\n\n        if num_levels == 1:\n            if len(rois) == 0:\n                return roi_feats\n            return self.roi_layers[0](feats[0], rois)\n\n        target_lvls = self.map_roi_levels(rois, num_levels)\n        if roi_scale_factor is not None:\n            rois = self.roi_rescale(rois, roi_scale_factor)\n        for i in range(num_levels):\n            inds = target_lvls == i\n            if inds.any():\n                rois_ = rois[inds, :]\n                roi_feats_t = self.roi_layers[i](feats[i], rois_)\n                roi_feats[inds] = roi_feats_t\n            else:\n                roi_feats += sum(x.view(-1)[0] for x in self.parameters()) * 0.\n        return roi_feats\n'"
mmdet/models/roi_heads/shared_heads/__init__.py,0,"b""from .res_layer import ResLayer\n\n__all__ = ['ResLayer']\n"""
mmdet/models/roi_heads/shared_heads/res_layer.py,1,"b""import torch.nn as nn\nfrom mmcv.cnn import constant_init, kaiming_init\nfrom mmcv.runner import load_checkpoint\n\nfrom mmdet.core import auto_fp16\nfrom mmdet.models.backbones import ResNet\nfrom mmdet.models.builder import SHARED_HEADS\nfrom mmdet.models.utils import ResLayer as _ResLayer\nfrom mmdet.utils import get_root_logger\n\n\n@SHARED_HEADS.register_module()\nclass ResLayer(nn.Module):\n\n    def __init__(self,\n                 depth,\n                 stage=3,\n                 stride=2,\n                 dilation=1,\n                 style='pytorch',\n                 norm_cfg=dict(type='BN', requires_grad=True),\n                 norm_eval=True,\n                 with_cp=False,\n                 dcn=None):\n        super(ResLayer, self).__init__()\n        self.norm_eval = norm_eval\n        self.norm_cfg = norm_cfg\n        self.stage = stage\n        self.fp16_enabled = False\n        block, stage_blocks = ResNet.arch_settings[depth]\n        stage_block = stage_blocks[stage]\n        planes = 64 * 2**stage\n        inplanes = 64 * 2**(stage - 1) * block.expansion\n\n        res_layer = _ResLayer(\n            block,\n            inplanes,\n            planes,\n            stage_block,\n            stride=stride,\n            dilation=dilation,\n            style=style,\n            with_cp=with_cp,\n            norm_cfg=self.norm_cfg,\n            dcn=dcn)\n        self.add_module(f'layer{stage + 1}', res_layer)\n\n    def init_weights(self, pretrained=None):\n        if isinstance(pretrained, str):\n            logger = get_root_logger()\n            load_checkpoint(self, pretrained, strict=False, logger=logger)\n        elif pretrained is None:\n            for m in self.modules():\n                if isinstance(m, nn.Conv2d):\n                    kaiming_init(m)\n                elif isinstance(m, nn.BatchNorm2d):\n                    constant_init(m, 1)\n        else:\n            raise TypeError('pretrained must be a str or None')\n\n    @auto_fp16()\n    def forward(self, x):\n        res_layer = getattr(self, f'layer{self.stage + 1}')\n        out = res_layer(x)\n        return out\n\n    def train(self, mode=True):\n        super(ResLayer, self).train(mode)\n        if self.norm_eval:\n            for m in self.modules():\n                if isinstance(m, nn.BatchNorm2d):\n                    m.eval()\n"""
