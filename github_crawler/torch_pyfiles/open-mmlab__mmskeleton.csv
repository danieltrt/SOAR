file_path,api_count,code
mmskl.py,0,"b'import argparse\nimport os\nimport sys\nimport logging\n\nimport torch\nimport mmskeleton\nfrom mmcv import Config\nfrom mmskeleton.utils import call_obj, set_attr, get_attr\n"""""" Configuration Structure\n\nargparse_cfg:\n  <shortcut_name 1>:\n    bind_to: <full variable path>\n    help: <help message>\n  <shortcut_name 2>:\n    ...\n\nprocessor_cfg:\n  name: <full processor path>\n  ...\n\n""""""\n\nconfig_shortcut = dict(\n    pose_demo_HD=\'./configs/pose_estimation/pose_demo_HD.yaml\',\n    pose_demo=\'./configs/pose_estimation/pose_demo.yaml\')\n\n\ndef parse_cfg():\n\n    parser = argparse.ArgumentParser(description=\'Run a processor.\')\n    parser.add_argument(\'config\', help=\'configuration file path\')\n\n    if len(sys.argv) <= 1:\n        args = parser.parse_args()\n        return\n    if sys.argv[1] == \'-h\' or sys.argv[1] == \'--help\':\n        args = parser.parse_args()\n        return\n\n    # pop positional args\n    config_args = []\n    tmp = []\n    for i, arg in enumerate(sys.argv):\n        if i > 1:\n            if arg[0] != \'-\':\n                tmp.append(i)\n            else:\n                break\n\n    for i in tmp[::-1]:\n        config_args.append(sys.argv.pop(i))\n    branch = config_args\n\n    # load argument setting from configuration file\n    if sys.argv[1] in config_shortcut:\n        sys.argv[1] = config_shortcut[sys.argv[1]]\n\n    print(\'Load configuration information from {}\'.format(sys.argv[1]))\n    cfg = Config.fromfile(sys.argv[1])\n    for b in branch:\n        if b in cfg:\n            cfg = get_attr(cfg, b)\n        else:\n            print(\'The branch ""{}"" can not be found in {}.\'.format(\n                \'-\'.join(branch), sys.argv[1]))\n            return dict()\n    if \'description\' in cfg:\n        parser.description = cfg.description\n    if \'argparse_cfg\' not in cfg:\n        cfg.argparse_cfg = dict()\n    for key, info in cfg.argparse_cfg.items():\n        if \'bind_to\' not in info:\n            continue\n        default = get_attr(cfg, info[\'bind_to\'])\n        if \'type\' not in info:\n            if default is not None:\n                info[\'type\'] = type(default)\n        else:\n            info[\'type\'] = eval(info[\'type\'])\n        kwargs = dict(default=default)\n        kwargs.update({k: v for k, v in info.items() if k != \'bind_to\'})\n        parser.add_argument(\'--\' + key, **kwargs)\n    args = parser.parse_args()\n\n    # update config from command line\n    for key, info in cfg.argparse_cfg.items():\n        if \'bind_to\' not in info:\n            continue\n        value = getattr(args, key)\n        set_attr(cfg, info[\'bind_to\'], value)\n\n    # replace pre_defined arguments in configuration files\n    def replace(cfg, **format_args):\n        if isinstance(cfg, str):\n            return cfg.format(**format_args)\n        if isinstance(cfg, dict):\n            for k, v in cfg.items():\n                set_attr(cfg, k, replace(v, **format_args))\n        elif isinstance(cfg, list):\n            for k in range(len(cfg)):\n                cfg[k] = replace(cfg[k], **format_args)\n        return cfg\n\n    format_args = dict()\n    format_args[\'config_path\'] = args.config\n    format_args[\'config_name\'] = os.path.basename(format_args[\'config_path\'])\n    format_args[\'config_prefix\'] = format_args[\'config_name\'].split(\'.\')[0]\n    cfg = replace(cfg, **format_args)\n    return cfg\n\n\ndef main():\n    cfg = parse_cfg()\n    if \'processor_cfg\' in cfg:\n        call_obj(**cfg.processor_cfg)\n    else:\n        print(\'No processor specified.\')\n\n\nif __name__ == ""__main__"":\n    main()\n'"
setup.py,2,"b'import os\nimport sys\nimport platform\nimport subprocess\nimport time\n\nfrom setuptools import find_packages, setup, Extension, dist\nfrom setuptools.command.install import install\ndist.Distribution().fetch_build_eggs([\'Cython\', \'numpy>=1.11.1\', \'torch\'])\n\nimport numpy as np\nfrom Cython.Build import cythonize  # noqa: E402\nimport torch\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\n\ndef readme():\n    with open(\'README.md\', encoding=\'utf-8\') as f:\n        content = f.read()\n    return content\n\n\nMAJOR = 0\nMINOR = 7\nPATCH = \'\'\nSUFFIX = \'rc1\'\nSHORT_VERSION = \'{}.{}.{}{}\'.format(MAJOR, MINOR, PATCH, SUFFIX)\n\nversion_file = \'mmskeleton/version.py\'\n\n\ndef get_git_hash():\n    def _minimal_ext_cmd(cmd):\n        # construct minimal environment\n        env = {}\n        for k in [\'SYSTEMROOT\', \'PATH\', \'HOME\']:\n            v = os.environ.get(k)\n            if v is not None:\n                env[k] = v\n        # LANGUAGE is used on win32\n        env[\'LANGUAGE\'] = \'C\'\n        env[\'LANG\'] = \'C\'\n        env[\'LC_ALL\'] = \'C\'\n        out = subprocess.Popen(cmd, stdout=subprocess.PIPE,\n                               env=env).communicate()[0]\n        return out\n\n    try:\n        out = _minimal_ext_cmd([\'git\', \'rev-parse\', \'HEAD\'])\n        sha = out.strip().decode(\'ascii\')\n    except OSError:\n        sha = \'unknown\'\n\n    return sha\n\n\ndef get_hash():\n    if os.path.exists(\'.git\'):\n        sha = get_git_hash()[:7]\n    elif os.path.exists(version_file):\n        try:\n            from mmskeleton.version import __version__\n            sha = __version__.split(\'+\')[-1]\n        except ImportError:\n            raise ImportError(\'Unable to get git version\')\n    else:\n        sha = \'unknown\'\n\n    return sha\n\n\ndef write_version_py():\n    content = """"""# GENERATED VERSION FILE\n# TIME: {}\n__version__ = \'{}\'\nshort_version = \'{}\'\nmmskl_home = r\'{}\'\n""""""\n    sha = get_hash()\n    VERSION = SHORT_VERSION + \'+\' + sha\n    MMSKELETON_HOME = os.path.dirname(os.path.realpath(__file__))\n\n    with open(version_file, \'w\') as f:\n        f.write(\n            content.format(time.asctime(), VERSION, SHORT_VERSION,\n                           MMSKELETON_HOME))\n\n\ndef get_version():\n    with open(version_file, \'r\') as f:\n        exec(compile(f.read(), version_file, \'exec\'))\n    return locals()[\'__version__\']\n\n\ndef get_requirements(filename=\'requirements.txt\'):\n    here = os.path.dirname(os.path.realpath(__file__))\n    with open(os.path.join(here, filename), \'r\') as f:\n        requires = [line.replace(\'\\n\', \'\') for line in f.readlines()]\n    return requires\n\n\ndef make_cython_ext(name, module, sources):\n    extra_compile_args = None\n    if platform.system() != \'Windows\':\n        extra_compile_args = {\n            \'cxx\': [\'-Wno-unused-function\', \'-Wno-write-strings\']\n        }\n    extension = Extension(\'{}.{}\'.format(\n        module, name), [os.path.join(*module.split(\'.\'), p) for p in sources],\n                          include_dirs=[np.get_include()],\n                          language=\'c++\',\n                          extra_compile_args=extra_compile_args)\n    extension, = cythonize(extension)\n    return extension\n\n\ndef make_cuda_ext(name, module, sources, include_dirs=[]):\n\n    define_macros = []\n\n    if torch.cuda.is_available() or os.getenv(\'FORCE_CUDA\', \'0\') == \'1\':\n        define_macros += [(""WITH_CUDA"", None)]\n    else:\n        raise EnvironmentError(\'CUDA is required to compile MMSkeleton!\')\n\n    return CUDAExtension(\n        name=\'{}.{}\'.format(module, name),\n        sources=[os.path.join(*module.split(\'.\'), p) for p in sources],\n        define_macros=define_macros,\n        include_dirs=include_dirs,\n        extra_compile_args={\n            \'cxx\': [],\n            \'nvcc\': [\n                \'-D__CUDA_NO_HALF_OPERATORS__\',\n                \'-D__CUDA_NO_HALF_CONVERSIONS__\',\n                \'-D__CUDA_NO_HALF2_OPERATORS__\',\n            ]\n        })\n\n\nif __name__ == \'__main__\':\n\n    install_requires = get_requirements()\n    if ""--mmdet"" in sys.argv:\n        sys.argv.remove(""--mmdet"")\n        install_requires += [\'mmdet\']\n\n        import subprocess\n        subprocess.check_call([sys.executable, ""-m"", ""pip"", ""install"", \'https://github.com/open-mmlab/mmdetection/archive/v1.0rc1.zip\', \'-v\'])\n\n    write_version_py()\n    setup(\n        name=\'mmskeleton\',\n        version=get_version(),\n        scripts=[\'./tools/mmskl\'],\n        description=\'Open MMLab Skeleton-based Human Understanding Toolbox\',\n        long_description=readme(),\n        keywords=\'computer vision, human understanding, action recognition\',\n        url=\'https://github.com/open-mmlab/mmskeleton\',\n        packages=find_packages(exclude=(\'configs\', \'tools\', \'demo\')),\n        package_data={\'mmskeleton.ops\': [\'*/*.so\']},\n        classifiers=[\n            \'Development Status :: 4 - Beta\',\n            \'License :: OSI Approved :: Apache Software License\',\n            \'Operating System :: OS Independent\',\n            \'Programming Language :: Python :: 2\',\n            \'Programming Language :: Python :: 2.7\',\n            \'Programming Language :: Python :: 3\',\n            \'Programming Language :: Python :: 3.4\',\n            \'Programming Language :: Python :: 3.5\',\n            \'Programming Language :: Python :: 3.6\',\n        ],\n        license=\'Apache License 2.0\',\n        setup_requires=[\'pytest-runner\'],\n        tests_require=[\'pytest\'],\n        dependency_links=[\n            \'https://github.com/open-mmlab/mmdetection/tarball/v1.0rc1/#egg=mmdet-v1.0rc1\'\n        ],\n        install_requires=install_requires,\n        # ext_modules=[\n        #     make_cython_ext(name=\'cpu_nms\',\n        #                     module=\'mmskeleton.ops.nms\',\n        #                     sources=[\'cpu_nms.pyx\']),\n        #     make_cuda_ext(name=\'gpu_nms\',\n        #                   module=\'mmskeleton.ops.nms\',\n        #                   sources=[\'nms_kernel.cu\', \'gpu_nms.pyx\'],\n        #                   include_dirs=[np.get_include()]),\n        # ],\n        cmdclass={\n            \'build_ext\': BuildExtension,\n        },\n        zip_safe=False)\n'"
mmskeleton/__init__.py,0,"b'from . import utils\nfrom . import datasets, processor, models, ops, apis\nfrom .datasets.skeleton import skeleton_process'"
tools/publish_model.py,2,"b""import argparse\nimport subprocess\n\nimport torch\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description='Process a checkpoint to be published')\n    parser.add_argument('in_file', help='input checkpoint filename')\n    parser.add_argument('out_file', help='output checkpoint filename')\n    args = parser.parse_args()\n    return args\n\n\ndef process_checkpoint(in_file, out_file):\n    checkpoint = torch.load(in_file, map_location='cpu')\n    # remove optimizer for smaller file size\n    if 'optimizer' in checkpoint:\n        del checkpoint['optimizer']\n    # if it is necessary to remove some sensitive data in checkpoint['meta'],\n    # add the code here.\n    torch.save(checkpoint, out_file)\n    sha = subprocess.check_output(['sha256sum', out_file]).decode()\n    final_file = out_file.rstrip('.pth') + '-{}.pth'.format(sha[:8])\n    subprocess.Popen(['mv', out_file, final_file])\n\n\ndef main():\n    args = parse_args()\n    process_checkpoint(args.in_file, args.out_file)\n\n\nif __name__ == '__main__':\n    main()\n"""
configs/mmdet/cascade_rcnn_r50_fpn_1x.py,0,"b""# model settings\nmodel = dict(type='CascadeRCNN',\n             num_stages=3,\n             pretrained='torchvision://resnet50',\n             backbone=dict(type='ResNet',\n                           depth=50,\n                           num_stages=4,\n                           out_indices=(0, 1, 2, 3),\n                           frozen_stages=1,\n                           style='pytorch'),\n             neck=dict(type='FPN',\n                       in_channels=[256, 512, 1024, 2048],\n                       out_channels=256,\n                       num_outs=5),\n             rpn_head=dict(type='RPNHead',\n                           in_channels=256,\n                           feat_channels=256,\n                           anchor_scales=[8],\n                           anchor_ratios=[0.5, 1.0, 2.0],\n                           anchor_strides=[4, 8, 16, 32, 64],\n                           target_means=[.0, .0, .0, .0],\n                           target_stds=[1.0, 1.0, 1.0, 1.0],\n                           loss_cls=dict(type='CrossEntropyLoss',\n                                         use_sigmoid=True,\n                                         loss_weight=1.0),\n                           loss_bbox=dict(type='SmoothL1Loss',\n                                          beta=1.0 / 9.0,\n                                          loss_weight=1.0)),\n             bbox_roi_extractor=dict(type='SingleRoIExtractor',\n                                     roi_layer=dict(type='RoIAlign',\n                                                    out_size=7,\n                                                    sample_num=2),\n                                     out_channels=256,\n                                     featmap_strides=[4, 8, 16, 32]),\n             bbox_head=[\n                 dict(type='SharedFCBBoxHead',\n                      num_fcs=2,\n                      in_channels=256,\n                      fc_out_channels=1024,\n                      roi_feat_size=7,\n                      num_classes=81,\n                      target_means=[0., 0., 0., 0.],\n                      target_stds=[0.1, 0.1, 0.2, 0.2],\n                      reg_class_agnostic=True,\n                      loss_cls=dict(type='CrossEntropyLoss',\n                                    use_sigmoid=False,\n                                    loss_weight=1.0),\n                      loss_bbox=dict(type='SmoothL1Loss',\n                                     beta=1.0,\n                                     loss_weight=1.0)),\n                 dict(type='SharedFCBBoxHead',\n                      num_fcs=2,\n                      in_channels=256,\n                      fc_out_channels=1024,\n                      roi_feat_size=7,\n                      num_classes=81,\n                      target_means=[0., 0., 0., 0.],\n                      target_stds=[0.05, 0.05, 0.1, 0.1],\n                      reg_class_agnostic=True,\n                      loss_cls=dict(type='CrossEntropyLoss',\n                                    use_sigmoid=False,\n                                    loss_weight=1.0),\n                      loss_bbox=dict(type='SmoothL1Loss',\n                                     beta=1.0,\n                                     loss_weight=1.0)),\n                 dict(type='SharedFCBBoxHead',\n                      num_fcs=2,\n                      in_channels=256,\n                      fc_out_channels=1024,\n                      roi_feat_size=7,\n                      num_classes=81,\n                      target_means=[0., 0., 0., 0.],\n                      target_stds=[0.033, 0.033, 0.067, 0.067],\n                      reg_class_agnostic=True,\n                      loss_cls=dict(type='CrossEntropyLoss',\n                                    use_sigmoid=False,\n                                    loss_weight=1.0),\n                      loss_bbox=dict(type='SmoothL1Loss',\n                                     beta=1.0,\n                                     loss_weight=1.0))\n             ])\n# model training and testing settings\ntrain_cfg = dict(rpn=dict(assigner=dict(type='MaxIoUAssigner',\n                                        pos_iou_thr=0.7,\n                                        neg_iou_thr=0.3,\n                                        min_pos_iou=0.3,\n                                        ignore_iof_thr=-1),\n                          sampler=dict(type='RandomSampler',\n                                       num=256,\n                                       pos_fraction=0.5,\n                                       neg_pos_ub=-1,\n                                       add_gt_as_proposals=False),\n                          allowed_border=0,\n                          pos_weight=-1,\n                          debug=False),\n                 rpn_proposal=dict(nms_across_levels=False,\n                                   nms_pre=2000,\n                                   nms_post=2000,\n                                   max_num=2000,\n                                   nms_thr=0.7,\n                                   min_bbox_size=0),\n                 rcnn=[\n                     dict(assigner=dict(type='MaxIoUAssigner',\n                                        pos_iou_thr=0.5,\n                                        neg_iou_thr=0.5,\n                                        min_pos_iou=0.5,\n                                        ignore_iof_thr=-1),\n                          sampler=dict(type='RandomSampler',\n                                       num=512,\n                                       pos_fraction=0.25,\n                                       neg_pos_ub=-1,\n                                       add_gt_as_proposals=True),\n                          pos_weight=-1,\n                          debug=False),\n                     dict(assigner=dict(type='MaxIoUAssigner',\n                                        pos_iou_thr=0.6,\n                                        neg_iou_thr=0.6,\n                                        min_pos_iou=0.6,\n                                        ignore_iof_thr=-1),\n                          sampler=dict(type='RandomSampler',\n                                       num=512,\n                                       pos_fraction=0.25,\n                                       neg_pos_ub=-1,\n                                       add_gt_as_proposals=True),\n                          pos_weight=-1,\n                          debug=False),\n                     dict(assigner=dict(type='MaxIoUAssigner',\n                                        pos_iou_thr=0.7,\n                                        neg_iou_thr=0.7,\n                                        min_pos_iou=0.7,\n                                        ignore_iof_thr=-1),\n                          sampler=dict(type='RandomSampler',\n                                       num=512,\n                                       pos_fraction=0.25,\n                                       neg_pos_ub=-1,\n                                       add_gt_as_proposals=True),\n                          pos_weight=-1,\n                          debug=False)\n                 ],\n                 stage_loss_weights=[1, 0.5, 0.25])\ntest_cfg = dict(rpn=dict(nms_across_levels=False,\n                         nms_pre=1000,\n                         nms_post=1000,\n                         max_num=1000,\n                         nms_thr=0.7,\n                         min_bbox_size=0),\n                rcnn=dict(score_thr=0.05,\n                          nms=dict(type='nms', iou_thr=0.5),\n                          max_per_img=100),\n                keep_all_stages=False)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(mean=[123.675, 116.28, 103.53],\n                    std=[58.395, 57.12, 57.375],\n                    to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='MultiScaleFlipAug',\n         img_scale=(1333, 800),\n         flip=False,\n         transforms=[\n             dict(type='Resize', keep_ratio=True),\n             dict(type='RandomFlip'),\n             dict(type='Normalize', **img_norm_cfg),\n             dict(type='Pad', size_divisor=32),\n             dict(type='ImageToTensor', keys=['img']),\n             dict(type='Collect', keys=['img']),\n         ])\n]\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(type=dataset_type,\n               ann_file=data_root + 'annotations/instances_train2017.json',\n               img_prefix=data_root + 'train2017/',\n               pipeline=train_pipeline),\n    val=dict(type=dataset_type,\n             ann_file=data_root + 'annotations/instances_val2017.json',\n             img_prefix=data_root + 'val2017/',\n             pipeline=test_pipeline),\n    test=dict(type=dataset_type,\n              ann_file=data_root + 'annotations/instances_val2017.json',\n              img_prefix=data_root + 'val2017/',\n              pipeline=test_pipeline))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(policy='step',\n                 warmup='linear',\n                 warmup_iters=500,\n                 warmup_ratio=1.0 / 3,\n                 step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/cascade_rcnn_r50_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/mmdet/htc_dconv_c3-c5_mstrain_400_1400_x101_64x4d_fpn_20e.py,0,"b""# model settings\nmodel = dict(type='HybridTaskCascade',\n             num_stages=3,\n             pretrained='open-mmlab://resnext101_64x4d',\n             interleaved=True,\n             mask_info_flow=True,\n             backbone=dict(type='ResNeXt',\n                           depth=101,\n                           groups=64,\n                           base_width=4,\n                           num_stages=4,\n                           out_indices=(0, 1, 2, 3),\n                           frozen_stages=1,\n                           style='pytorch',\n                           dcn=dict(modulated=False,\n                                    groups=64,\n                                    deformable_groups=1,\n                                    fallback_on_stride=False),\n                           stage_with_dcn=(False, True, True, True)),\n             neck=dict(type='FPN',\n                       in_channels=[256, 512, 1024, 2048],\n                       out_channels=256,\n                       num_outs=5),\n             rpn_head=dict(type='RPNHead',\n                           in_channels=256,\n                           feat_channels=256,\n                           anchor_scales=[8],\n                           anchor_ratios=[0.5, 1.0, 2.0],\n                           anchor_strides=[4, 8, 16, 32, 64],\n                           target_means=[.0, .0, .0, .0],\n                           target_stds=[1.0, 1.0, 1.0, 1.0],\n                           loss_cls=dict(type='CrossEntropyLoss',\n                                         use_sigmoid=True,\n                                         loss_weight=1.0),\n                           loss_bbox=dict(type='SmoothL1Loss',\n                                          beta=1.0 / 9.0,\n                                          loss_weight=1.0)),\n             bbox_roi_extractor=dict(type='SingleRoIExtractor',\n                                     roi_layer=dict(type='RoIAlign',\n                                                    out_size=7,\n                                                    sample_num=2),\n                                     out_channels=256,\n                                     featmap_strides=[4, 8, 16, 32]),\n             bbox_head=[\n                 dict(type='SharedFCBBoxHead',\n                      num_fcs=2,\n                      in_channels=256,\n                      fc_out_channels=1024,\n                      roi_feat_size=7,\n                      num_classes=81,\n                      target_means=[0., 0., 0., 0.],\n                      target_stds=[0.1, 0.1, 0.2, 0.2],\n                      reg_class_agnostic=True,\n                      loss_cls=dict(type='CrossEntropyLoss',\n                                    use_sigmoid=False,\n                                    loss_weight=1.0),\n                      loss_bbox=dict(type='SmoothL1Loss',\n                                     beta=1.0,\n                                     loss_weight=1.0)),\n                 dict(type='SharedFCBBoxHead',\n                      num_fcs=2,\n                      in_channels=256,\n                      fc_out_channels=1024,\n                      roi_feat_size=7,\n                      num_classes=81,\n                      target_means=[0., 0., 0., 0.],\n                      target_stds=[0.05, 0.05, 0.1, 0.1],\n                      reg_class_agnostic=True,\n                      loss_cls=dict(type='CrossEntropyLoss',\n                                    use_sigmoid=False,\n                                    loss_weight=1.0),\n                      loss_bbox=dict(type='SmoothL1Loss',\n                                     beta=1.0,\n                                     loss_weight=1.0)),\n                 dict(type='SharedFCBBoxHead',\n                      num_fcs=2,\n                      in_channels=256,\n                      fc_out_channels=1024,\n                      roi_feat_size=7,\n                      num_classes=81,\n                      target_means=[0., 0., 0., 0.],\n                      target_stds=[0.033, 0.033, 0.067, 0.067],\n                      reg_class_agnostic=True,\n                      loss_cls=dict(type='CrossEntropyLoss',\n                                    use_sigmoid=False,\n                                    loss_weight=1.0),\n                      loss_bbox=dict(type='SmoothL1Loss',\n                                     beta=1.0,\n                                     loss_weight=1.0))\n             ],\n             mask_roi_extractor=dict(type='SingleRoIExtractor',\n                                     roi_layer=dict(type='RoIAlign',\n                                                    out_size=14,\n                                                    sample_num=2),\n                                     out_channels=256,\n                                     featmap_strides=[4, 8, 16, 32]),\n             mask_head=dict(type='HTCMaskHead',\n                            num_convs=4,\n                            in_channels=256,\n                            conv_out_channels=256,\n                            num_classes=81,\n                            loss_mask=dict(type='CrossEntropyLoss',\n                                           use_mask=True,\n                                           loss_weight=1.0)),\n             semantic_roi_extractor=dict(type='SingleRoIExtractor',\n                                         roi_layer=dict(type='RoIAlign',\n                                                        out_size=14,\n                                                        sample_num=2),\n                                         out_channels=256,\n                                         featmap_strides=[8]),\n             semantic_head=dict(type='FusedSemanticHead',\n                                num_ins=5,\n                                fusion_level=1,\n                                num_convs=4,\n                                in_channels=256,\n                                conv_out_channels=256,\n                                num_classes=183,\n                                ignore_label=255,\n                                loss_weight=0.2))\n# model training and testing settings\ntrain_cfg = dict(rpn=dict(assigner=dict(type='MaxIoUAssigner',\n                                        pos_iou_thr=0.7,\n                                        neg_iou_thr=0.3,\n                                        min_pos_iou=0.3,\n                                        ignore_iof_thr=-1),\n                          sampler=dict(type='RandomSampler',\n                                       num=256,\n                                       pos_fraction=0.5,\n                                       neg_pos_ub=-1,\n                                       add_gt_as_proposals=False),\n                          allowed_border=0,\n                          pos_weight=-1,\n                          debug=False),\n                 rpn_proposal=dict(nms_across_levels=False,\n                                   nms_pre=2000,\n                                   nms_post=2000,\n                                   max_num=2000,\n                                   nms_thr=0.7,\n                                   min_bbox_size=0),\n                 rcnn=[\n                     dict(assigner=dict(type='MaxIoUAssigner',\n                                        pos_iou_thr=0.5,\n                                        neg_iou_thr=0.5,\n                                        min_pos_iou=0.5,\n                                        ignore_iof_thr=-1),\n                          sampler=dict(type='RandomSampler',\n                                       num=512,\n                                       pos_fraction=0.25,\n                                       neg_pos_ub=-1,\n                                       add_gt_as_proposals=True),\n                          mask_size=28,\n                          pos_weight=-1,\n                          debug=False),\n                     dict(assigner=dict(type='MaxIoUAssigner',\n                                        pos_iou_thr=0.6,\n                                        neg_iou_thr=0.6,\n                                        min_pos_iou=0.6,\n                                        ignore_iof_thr=-1),\n                          sampler=dict(type='RandomSampler',\n                                       num=512,\n                                       pos_fraction=0.25,\n                                       neg_pos_ub=-1,\n                                       add_gt_as_proposals=True),\n                          mask_size=28,\n                          pos_weight=-1,\n                          debug=False),\n                     dict(assigner=dict(type='MaxIoUAssigner',\n                                        pos_iou_thr=0.7,\n                                        neg_iou_thr=0.7,\n                                        min_pos_iou=0.7,\n                                        ignore_iof_thr=-1),\n                          sampler=dict(type='RandomSampler',\n                                       num=512,\n                                       pos_fraction=0.25,\n                                       neg_pos_ub=-1,\n                                       add_gt_as_proposals=True),\n                          mask_size=28,\n                          pos_weight=-1,\n                          debug=False)\n                 ],\n                 stage_loss_weights=[1, 0.5, 0.25])\ntest_cfg = dict(rpn=dict(nms_across_levels=False,\n                         nms_pre=1000,\n                         nms_post=1000,\n                         max_num=1000,\n                         nms_thr=0.7,\n                         min_bbox_size=0),\n                rcnn=dict(score_thr=0.001,\n                          nms=dict(type='nms', iou_thr=0.5),\n                          max_per_img=100,\n                          mask_thr_binary=0.5),\n                keep_all_stages=False)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(mean=[123.675, 116.28, 103.53],\n                    std=[58.395, 57.12, 57.375],\n                    to_rgb=True)\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True, with_mask=True,\n         with_seg=True),\n    dict(type='Resize',\n         img_scale=[(1600, 400), (1600, 1400)],\n         multiscale_mode='range',\n         keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='SegResizeFlipPadRescale', scale_factor=1 / 8),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect',\n         keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks',\n               'gt_semantic_seg']),\n]\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='MultiScaleFlipAug',\n         img_scale=(1333, 800),\n         flip=False,\n         transforms=[\n             dict(type='Resize', keep_ratio=True),\n             dict(type='RandomFlip', flip_ratio=0.5),\n             dict(type='Normalize', **img_norm_cfg),\n             dict(type='Pad', size_divisor=32),\n             dict(type='ImageToTensor', keys=['img']),\n             dict(type='Collect', keys=['img']),\n         ])\n]\ndata = dict(\n    imgs_per_gpu=1,\n    workers_per_gpu=1,\n    train=dict(type=dataset_type,\n               ann_file=data_root + 'annotations/instances_train2017.json',\n               img_prefix=data_root + 'train2017/',\n               seg_prefix=data_root + 'stuffthingmaps/train2017/',\n               pipeline=train_pipeline),\n    val=dict(type=dataset_type,\n             ann_file=data_root + 'annotations/instances_val2017.json',\n             img_prefix=data_root + 'val2017/',\n             pipeline=test_pipeline),\n    test=dict(type=dataset_type,\n              ann_file=data_root + 'annotations/instances_val2017.json',\n              img_prefix=data_root + 'val2017/',\n              pipeline=test_pipeline))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(policy='step',\n                 warmup='linear',\n                 warmup_iters=500,\n                 warmup_ratio=1.0 / 3,\n                 step=[16, 19])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 20\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/htc_dconv_c3-c5_mstrain_400_1400_x101_64x4d_fpn_20e'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
deprecated/origin_stgcn_repo/main.py,0,"b""#!/usr/bin/env python\nimport argparse\nimport sys\n\n# torchlight\nimport torchlight\nfrom torchlight import import_class\n\nif __name__ == '__main__':\n\n    parser = argparse.ArgumentParser(description='Processor collection')\n\n    # region register processor yapf: disable\n    processors = dict()\n    processors['recognition'] = import_class('processor.recognition.REC_Processor')\n    processors['demo_old'] = import_class('processor.demo_old.Demo')\n    processors['demo'] = import_class('processor.demo_realtime.DemoRealtime')\n    processors['demo_offline'] = import_class('processor.demo_offline.DemoOffline')\n    #endregion yapf: enable\n\n    # add sub-parser\n    subparsers = parser.add_subparsers(dest='processor')\n    for k, p in processors.items():\n        subparsers.add_parser(k, parents=[p.get_parser()])\n\n    # read arguments\n    arg = parser.parse_args()\n\n    # start\n    Processor = processors[arg.processor]\n    p = Processor(sys.argv[2:])\n\n    p.start()\n"""
mmskeleton/apis/__init__.py,0,"b'from .estimation import init_pose_estimator, inference_pose_estimator'"
mmskeleton/apis/estimation.py,0,"b'import os\nimport numpy as np\nimport torch\nimport mmcv\n\nfrom mmskeleton.datasets.utils.video_demo import VideoDemo\nfrom mmskeleton.utils import get_mmskeleton_url\nfrom mmskeleton.processor.apis import init_twodimestimator, inference_twodimestimator\n\nimport mmdet.apis\n\n\ndef init_pose_estimator(detection_cfg, estimation_cfg, device=None):\n\n    detection_model_file = detection_cfg.model_cfg\n    detection_checkpoint_file = get_mmskeleton_url(\n        detection_cfg.checkpoint_file)\n    detection_model = mmdet.apis.init_detector(detection_model_file,\n                                               detection_checkpoint_file,\n                                               device=\'cpu\')\n\n    skeleton_model_file = estimation_cfg.model_cfg\n    skeletion_checkpoint_file = estimation_cfg.checkpoint_file\n    skeleton_model = init_twodimestimator(skeleton_model_file,\n                                          skeletion_checkpoint_file,\n                                          device=\'cpu\')\n\n    if device is not None:\n        os.environ[""CUDA_VISIBLE_DEVICES""] = str(device)\n        detection_model = detection_model.cuda()\n        skeleton_model = skeleton_model.cuda()\n\n    pose_estimator = (detection_model, skeleton_model, detection_cfg,\n                      estimation_cfg)\n    return pose_estimator\n\n\ndef inference_pose_estimator(pose_estimator, image):\n    detection_model, skeleton_model, detection_cfg, estimation_cfg = pose_estimator\n    bbox_result = mmdet.apis.inference_detector(detection_model, image)\n    person_bbox, labels = VideoDemo.bbox_filter(bbox_result,\n                                                detection_cfg.bbox_thre)\n    if len(person_bbox) > 0:\n        has_return = True\n        person, meta = VideoDemo.skeleton_preprocess(image[:, :, ::-1],\n                                                     person_bbox,\n                                                     estimation_cfg.data_cfg)\n        preds, maxvals = inference_twodimestimator(skeleton_model,\n                                                   person.cuda(), meta, True)\n\n    else:\n        has_return = False\n        preds, maxvals, meta = None, None, None\n\n    result = dict(joint_preds=preds,\n                  joint_scores=maxvals,\n                  meta=meta,\n                  has_return=has_return,\n                  person_bbox=person_bbox)\n\n    return result\n'"
mmskeleton/datasets/__init__.py,0,b'from .coco import COCODataset\r\nfrom .data_pipeline import DataPipeline\r\nfrom .skeleton import SkeletonLoader'
mmskeleton/datasets/coco.py,0,"b'# ------------------------------------------------------------------------------\r\n# Copyright (c) Microsoft\r\n# Licensed under the MIT License.\r\n# Written by Bin Xiao (Bin.Xiao@microsoft.com)\r\n# ------------------------------------------------------------------------------\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nfrom collections import defaultdict\r\nfrom collections import OrderedDict\r\nimport logging\r\nimport os\r\nimport json\r\nimport numpy as np\r\n\r\nfrom .estimation import EstiamtionDataset\r\nfrom ..ops.nms.nms import oks_nms\r\nfrom ..ops.nms.nms import soft_oks_nms\r\nfrom pycocotools import COCO, COCOeval\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\n\r\nclass COCODataset(EstiamtionDataset):\r\n    \'\'\'\r\n    ""keypoints"": {\r\n        0: ""nose"",\r\n        1: ""left_eye"",\r\n        2: ""right_eye"",\r\n        3: ""left_ear"",\r\n        4: ""right_ear"",\r\n        5: ""left_shoulder"",\r\n        6: ""right_shoulder"",\r\n        7: ""left_elbow"",\r\n        8: ""right_elbow"",\r\n        9: ""left_wrist"",\r\n        10: ""right_wrist"",\r\n        11: ""left_hip"",\r\n        12: ""right_hip"",\r\n        13: ""left_knee"",\r\n        14: ""right_knee"",\r\n        15: ""left_ankle"",\r\n        16: ""right_ankle""\r\n    },\r\n\t""skeleton"": [\r\n        [16,14],[14,12],[17,15],[15,13],[12,13],[6,12],[7,13], [6,7],[6,8],\r\n        [7,9],[8,10],[9,11],[2,3],[1,2],[1,3],[2,4],[3,5],[4,6],[5,7]]\r\n    \'\'\'\r\n    def __init__(self, cfg, root, image_set, is_train, transform=None):\r\n        super().__init__(cfg, root, image_set, is_train, transform)\r\n        self.nms_thre = cfg.test.nms_thre\r\n        self.image_thre = cfg.test.image_thre\r\n        self.soft_nms = cfg.test.soft_nms\r\n        self.oks_thre = cfg.test.oks_thre\r\n        self.in_vis_thre = cfg.test.in_vis_thre\r\n        self.bbox_file = cfg.test.bbox_file\r\n        self.use_gt_bbox = cfg.test.use_gt_bbox\r\n        self.image_width = cfg.test.image_size[0]\r\n        self.image_height = cfg.test.image_size[1]\r\n        self.aspect_ratio = self.image_width * 1.0 / self.image_height\r\n        self.pixel_std = 200\r\n\r\n        self.coco = COCO(self._get_ann_file_keypoint())\r\n\r\n        # deal with class names\r\n        cats = [\r\n            cat[\'name\'] for cat in self.coco.loadCats(self.coco.getCatIds())\r\n        ]\r\n        self.classes = [\'__background__\'] + cats\r\n        logger.info(\'=> classes: {}\'.format(self.classes))\r\n        self.num_classes = len(self.classes)\r\n        self._class_to_ind = dict(zip(self.classes, range(self.num_classes)))\r\n        self._class_to_coco_ind = dict(zip(cats, self.coco.getCatIds()))\r\n        self._coco_ind_to_class_ind = dict([(self._class_to_coco_ind[cls],\r\n                                             self._class_to_ind[cls])\r\n                                            for cls in self.classes[1:]])\r\n\r\n        # load image file names\r\n        self.image_set_index = self._load_image_set_index()\r\n        self.num_images = len(self.image_set_index)\r\n        logger.info(\'=> num_images: {}\'.format(self.num_images))\r\n\r\n        self.num_joints = 17\r\n        self.flip_pairs = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12],\r\n                           [13, 14], [15, 16]]\r\n        self.parent_ids = None\r\n        self.upper_body_ids = (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\r\n        self.lower_body_ids = (11, 12, 13, 14, 15, 16)\r\n\r\n        self.joints_weight = np.array([\r\n            1., 1., 1., 1., 1., 1., 1., 1.2, 1.2, 1.5, 1.5, 1., 1., 1.2, 1.2,\r\n            1.5, 1.5\r\n        ],\r\n                                      dtype=np.float32).reshape(\r\n                                          (self.num_joints, 1))\r\n\r\n        self.db = self._get_db()\r\n\r\n        if is_train and cfg.train.select_data:\r\n            self.db = self.select_data(self.db)\r\n\r\n        logger.info(\'=> load {} samples\'.format(len(self.db)))\r\n\r\n    def _get_ann_file_keypoint(self):\r\n        """""" self.root / annotations / person_keypoints_train2017.json """"""\r\n        prefix = \'person_keypoints\' \\\r\n            if \'test\' not in self.image_set else \'image_info\'\r\n        return os.path.join(self.root, \'annotations\',\r\n                            prefix + \'_\' + self.image_set + \'.json\')\r\n\r\n    def _load_image_set_index(self):\r\n        """""" image id: int """"""\r\n        image_ids = self.coco.getImgIds()\r\n        return image_ids\r\n\r\n    def _get_db(self):\r\n        if self.is_train or self.use_gt_bbox:\r\n            # use ground truth bbox\r\n            gt_db = self._load_coco_keypoint_annotations()\r\n        else:\r\n            # use bbox from detection\r\n            gt_db = self._load_coco_person_detection_results()\r\n        return gt_db\r\n\r\n    def _load_coco_keypoint_annotations(self):\r\n        """""" ground truth bbox and keypoints """"""\r\n        gt_db = []\r\n        for index in self.image_set_index:\r\n            gt_db.extend(self._load_coco_keypoint_annotation_kernal(index))\r\n        return gt_db\r\n\r\n    def _load_coco_keypoint_annotation_kernal(self, index):\r\n        """"""\r\n        coco ann: [u\'segmentation\', u\'area\', u\'iscrowd\', u\'image_id\', u\'bbox\', u\'category_id\', u\'id\']\r\n        iscrowd:\r\n            crowd instances are handled by marking their overlaps with all categories to -1\r\n            and later excluded in training\r\n        bbox:\r\n            [x1, y1, w, h]\r\n        :param index: coco image id\r\n        :return: db entry\r\n        """"""\r\n        im_ann = self.coco.loadImgs(index)[0]\r\n        width = im_ann[\'width\']\r\n        height = im_ann[\'height\']\r\n\r\n        annIds = self.coco.getAnnIds(imgIds=index, iscrowd=False)\r\n        objs = self.coco.loadAnns(annIds)\r\n\r\n        # sanitize bboxes\r\n        valid_objs = []\r\n        for obj in objs:\r\n            x, y, w, h = obj[\'bbox\']\r\n            x1 = np.max((0, x))\r\n            y1 = np.max((0, y))\r\n            x2 = np.min((width - 1, x1 + np.max((0, w - 1))))\r\n            y2 = np.min((height - 1, y1 + np.max((0, h - 1))))\r\n            if obj[\'area\'] > 0 and x2 >= x1 and y2 >= y1:\r\n                obj[\'clean_bbox\'] = [x1, y1, x2 - x1, y2 - y1]\r\n                valid_objs.append(obj)\r\n        objs = valid_objs\r\n\r\n        rec = []\r\n        for obj in objs:\r\n            cls = self._coco_ind_to_class_ind[obj[\'category_id\']]\r\n            if cls != 1:\r\n                continue\r\n\r\n            # ignore objs without keypoints annotation\r\n            if max(obj[\'keypoints\']) == 0:\r\n                continue\r\n\r\n            joints_3d = np.zeros((self.num_joints, 3), dtype=np.float)\r\n            joints_3d_vis = np.zeros((self.num_joints, 3), dtype=np.float)\r\n            for ipt in range(self.num_joints):\r\n                joints_3d[ipt, 0] = obj[\'keypoints\'][ipt * 3 + 0]\r\n                joints_3d[ipt, 1] = obj[\'keypoints\'][ipt * 3 + 1]\r\n                joints_3d[ipt, 2] = 0\r\n                t_vis = obj[\'keypoints\'][ipt * 3 + 2]\r\n                if t_vis > 1:\r\n                    t_vis = 1\r\n                joints_3d_vis[ipt, 0] = t_vis\r\n                joints_3d_vis[ipt, 1] = t_vis\r\n                joints_3d_vis[ipt, 2] = 0\r\n\r\n            center, scale = self._box2cs(obj[\'clean_bbox\'][:4])\r\n            rec.append({\r\n                \'image\': self.image_path_from_index(index),\r\n                \'center\': center,\r\n                \'scale\': scale,\r\n                \'joints_3d\': joints_3d,\r\n                \'joints_3d_vis\': joints_3d_vis,\r\n                \'filename\': \'\',\r\n                \'imgnum\': 0,\r\n            })\r\n\r\n        return rec\r\n\r\n    def _box2cs(self, box):\r\n        x, y, w, h = box[:4]\r\n        return self._xywh2cs(x, y, w, h)\r\n\r\n    def _xywh2cs(self, x, y, w, h):\r\n        center = np.zeros((2), dtype=np.float32)\r\n        center[0] = x + w * 0.5\r\n        center[1] = y + h * 0.5\r\n\r\n        if w > self.aspect_ratio * h:\r\n            h = w * 1.0 / self.aspect_ratio\r\n        elif w < self.aspect_ratio * h:\r\n            w = h * self.aspect_ratio\r\n        scale = np.array([w * 1.0 / self.pixel_std, h * 1.0 / self.pixel_std],\r\n                         dtype=np.float32)\r\n        if center[0] != -1:\r\n            scale = scale * 1.25\r\n\r\n        return center, scale\r\n\r\n    def image_path_from_index(self, index):\r\n        """""" example: images / train2017 / 000000119993.jpg """"""\r\n        file_name = \'%012d.jpg\' % index\r\n        if \'2014\' in self.image_set:\r\n            file_name = \'COCO_%s_\' % self.image_set + file_name\r\n\r\n        prefix = \'test2017\' if \'test\' in self.image_set else self.image_set\r\n\r\n        data_name = prefix + \'.zip@\' if self.data_format == \'zip\' else prefix\r\n\r\n        image_path = os.path.join(self.root, data_name, file_name)\r\n\r\n        return image_path\r\n\r\n    def _load_coco_person_detection_results(self):\r\n        all_boxes = None\r\n        with open(self.bbox_file, \'r\') as f:\r\n            all_boxes = json.load(f)\r\n\r\n        if not all_boxes:\r\n            logger.error(\'=> Load %s fail!\' % self.bbox_file)\r\n            return None\r\n\r\n        logger.info(\'=> Total boxes: {}\'.format(len(all_boxes)))\r\n\r\n        kpt_db = []\r\n        num_boxes = 0\r\n        for n_img in range(0, len(all_boxes)):\r\n            det_res = all_boxes[n_img]\r\n            if det_res[\'category_id\'] != 1:\r\n                continue\r\n            img_name = self.image_path_from_index(det_res[\'image_id\'])\r\n            box = det_res[\'bbox\']\r\n            score = det_res[\'score\']\r\n\r\n            if score < self.image_thre:\r\n                continue\r\n\r\n            num_boxes = num_boxes + 1\r\n\r\n            center, scale = self._box2cs(box)\r\n            joints_3d = np.zeros((self.num_joints, 3), dtype=np.float)\r\n            joints_3d_vis = np.ones((self.num_joints, 3), dtype=np.float)\r\n            kpt_db.append({\r\n                \'image\': img_name,\r\n                \'center\': center,\r\n                \'scale\': scale,\r\n                \'score\': score,\r\n                \'joints_3d\': joints_3d,\r\n                \'joints_3d_vis\': joints_3d_vis,\r\n            })\r\n\r\n        logger.info(\'=> Total boxes after fliter low score@{}: {}\'.format(\r\n            self.image_thre, num_boxes))\r\n        return kpt_db\r\n\r\n    def evaluate(self, cfg, preds, output_dir, all_boxes, img_path, *args,\r\n                 **kwargs):\r\n        rank = cfg.rank\r\n\r\n        res_folder = os.path.join(output_dir, \'results\')\r\n        if not os.path.exists(res_folder):\r\n            try:\r\n                os.makedirs(res_folder)\r\n            except Exception:\r\n                logger.error(\'Fail to make {}\'.format(res_folder))\r\n\r\n        res_file = os.path.join(\r\n            res_folder,\r\n            \'keypoints_{}_results_{}.json\'.format(self.image_set, rank))\r\n        # person x (keypoints)\r\n        _kpts = []\r\n        for idx, kpt in enumerate(preds):\r\n            _kpts.append({\r\n                \'keypoints\': kpt,\r\n                \'center\': all_boxes[idx][0:2],\r\n                \'scale\': all_boxes[idx][2:4],\r\n                \'area\': all_boxes[idx][4],\r\n                \'score\': all_boxes[idx][5],\r\n                \'image\': int(img_path[idx][-16:-4])\r\n            })\r\n        # image x person x (keypoints)\r\n        kpts = defaultdict(list)\r\n        for kpt in _kpts:\r\n            kpts[kpt[\'image\']].append(kpt)\r\n\r\n        # rescoring and oks nms\r\n        num_joints = self.num_joints\r\n        in_vis_thre = self.in_vis_thre\r\n        oks_thre = self.oks_thre\r\n        oks_nmsed_kpts = []\r\n        for img in kpts.keys():\r\n            img_kpts = kpts[img]\r\n            for n_p in img_kpts:\r\n                box_score = n_p[\'score\']\r\n                kpt_score = 0\r\n                valid_num = 0\r\n                for n_jt in range(0, num_joints):\r\n                    t_s = n_p[\'keypoints\'][n_jt][2]\r\n                    if t_s > in_vis_thre:\r\n                        kpt_score = kpt_score + t_s\r\n                        valid_num = valid_num + 1\r\n                if valid_num != 0:\r\n                    kpt_score = kpt_score / valid_num\r\n                # rescoring\r\n                n_p[\'score\'] = kpt_score * box_score\r\n\r\n            if self.soft_nms:\r\n                keep = soft_oks_nms(\r\n                    [img_kpts[i] for i in range(len(img_kpts))], oks_thre)\r\n            else:\r\n                keep = oks_nms([img_kpts[i] for i in range(len(img_kpts))],\r\n                               oks_thre)\r\n\r\n            if len(keep) == 0:\r\n                oks_nmsed_kpts.append(img_kpts)\r\n            else:\r\n                oks_nmsed_kpts.append([img_kpts[_keep] for _keep in keep])\r\n\r\n        self._write_coco_keypoint_results(oks_nmsed_kpts, res_file)\r\n        if \'test\' not in self.image_set:\r\n            info_str = self._do_python_keypoint_eval(res_file, res_folder)\r\n            name_value = OrderedDict(info_str)\r\n            return name_value, name_value[\'AP\']\r\n        else:\r\n            return {\'Null\': 0}, 0\r\n\r\n    def _write_coco_keypoint_results(self, keypoints, res_file):\r\n        data_pack = [{\r\n            \'cat_id\': self._class_to_coco_ind[cls],\r\n            \'cls_ind\': cls_ind,\r\n            \'cls\': cls,\r\n            \'ann_type\': \'keypoints\',\r\n            \'keypoints\': keypoints\r\n        } for cls_ind, cls in enumerate(self.classes)\r\n                     if not cls == \'__background__\']\r\n\r\n        results = self._coco_keypoint_results_one_category_kernel(data_pack[0])\r\n        logger.info(\'=> writing results json to %s\' % res_file)\r\n        with open(res_file, \'w\') as f:\r\n            json.dump(results, f, sort_keys=True, indent=4)\r\n        try:\r\n            json.load(open(res_file))\r\n        except Exception:\r\n            content = []\r\n            with open(res_file, \'r\') as f:\r\n                for line in f:\r\n                    content.append(line)\r\n            content[-1] = \']\'\r\n            with open(res_file, \'w\') as f:\r\n                for c in content:\r\n                    f.write(c)\r\n\r\n    def _coco_keypoint_results_one_category_kernel(self, data_pack):\r\n        cat_id = data_pack[\'cat_id\']\r\n        keypoints = data_pack[\'keypoints\']\r\n        cat_results = []\r\n\r\n        for img_kpts in keypoints:\r\n            if len(img_kpts) == 0:\r\n                continue\r\n\r\n            _key_points = np.array(\r\n                [img_kpts[k][\'keypoints\'] for k in range(len(img_kpts))])\r\n            key_points = np.zeros((_key_points.shape[0], self.num_joints * 3),\r\n                                  dtype=np.float)\r\n\r\n            for ipt in range(self.num_joints):\r\n                key_points[:, ipt * 3 + 0] = _key_points[:, ipt, 0]\r\n                key_points[:, ipt * 3 + 1] = _key_points[:, ipt, 1]\r\n                key_points[:, ipt * 3 + 2] = _key_points[:, ipt,\r\n                                                         2]  # keypoints score.\r\n\r\n            result = [{\r\n                \'image_id\': img_kpts[k][\'image\'],\r\n                \'category_id\': cat_id,\r\n                \'keypoints\': list(key_points[k]),\r\n                \'score\': img_kpts[k][\'score\'],\r\n                \'center\': list(img_kpts[k][\'center\']),\r\n                \'scale\': list(img_kpts[k][\'scale\'])\r\n            } for k in range(len(img_kpts))]\r\n            cat_results.extend(result)\r\n\r\n        return cat_results\r\n\r\n    def _do_python_keypoint_eval(self, res_file, res_folder):\r\n        coco_dt = self.coco.loadRes(res_file)\r\n        coco_eval = COCOeval(self.coco, coco_dt, \'keypoints\')\r\n        coco_eval.params.useSegm = None\r\n        coco_eval.evaluate()\r\n        coco_eval.accumulate()\r\n        coco_eval.summarize()\r\n\r\n        stats_names = [\r\n            \'AP\', \'Ap .5\', \'AP .75\', \'AP (M)\', \'AP (L)\', \'AR\', \'AR .5\',\r\n            \'AR .75\', \'AR (M)\', \'AR (L)\'\r\n        ]\r\n\r\n        info_str = []\r\n        for ind, name in enumerate(stats_names):\r\n            info_str.append((name, coco_eval.stats[ind]))\r\n\r\n        return info_str'"
mmskeleton/datasets/data_pipeline.py,1,"b'import os\nimport numpy as np\nimport json\nimport torch\nfrom mmskeleton.utils import call_obj\nfrom .utils import skeleton\n\n\nclass DataPipeline(torch.utils.data.Dataset):\n    def __init__(self, data_source, pipeline=[], num_sample=-1):\n\n        self.data_source = call_obj(**data_source)\n        self.pipeline = pipeline\n        self.num_sample = num_sample if num_sample > 0 else len(\n            self.data_source)\n\n    def __len__(self):\n        return self.num_sample\n\n    def __getitem__(self, index):\n        data = index\n        data = self.data_source[index]\n        for stage_args in self.pipeline:\n            data = call_obj(data=data, **stage_args)\n        return data\n'"
mmskeleton/datasets/estimation.py,3,"b""# ------------------------------------------------------------------------------\r\n# Copyright (c) Microsoft\r\n# Licensed under the MIT License.\r\n# Written by Bin Xiao (Bin.Xiao@microsoft.com)\r\n# ------------------------------------------------------------------------------\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport copy\r\nimport logging\r\nimport random\r\n\r\nimport cv2\r\nimport numpy as np\r\nimport torch\r\nfrom torch.utils.data import Dataset\r\n\r\nfrom .utils import coco_transform\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\n\r\nclass EstiamtionDataset(Dataset):\r\n    def __init__(self, cfg, root, image_set, is_train, transform=None):\r\n        self.num_joints = 0\r\n        self.pixel_std = 200\r\n        self.flip_pairs = []\r\n        self.parent_ids = []\r\n\r\n        self.is_train = is_train\r\n        self.root = root\r\n        self.image_set = image_set\r\n\r\n        self.output_path = cfg.train.out_dir\r\n        self.data_format = cfg.train.data_format\r\n\r\n        self.scale_factor = cfg.train.scale_factor\r\n        self.rotation_factor = cfg.train.rot_factor\r\n        self.flip = cfg.train.flip\r\n        self.num_joints_half_body = cfg.train.num_joints_half_body\r\n        self.prob_half_body = cfg.train.prob_half_body\r\n        self.color_rgb = cfg.train.color_rgb\r\n\r\n        self.target_type = cfg.train.target_type\r\n        self.image_size = np.array(cfg.train.image_size)\r\n        self.heatmap_size = np.array(cfg.train.heatmap_size)\r\n        self.sigma = cfg.train.sigma\r\n        self.use_different_joints_weight = cfg.train.loss_use_different_joints_weights\r\n        self.joints_weight = 1\r\n\r\n        self.transform = transform\r\n        self.db = []\r\n\r\n    def _get_db(self):\r\n        raise NotImplementedError\r\n\r\n    def evaluate(self, cfg, preds, output_dir, *args, **kwargs):\r\n        raise NotImplementedError\r\n\r\n    def half_body_transform(self, joints, joints_vis):\r\n        upper_joints = []\r\n        lower_joints = []\r\n        for joint_id in range(self.num_joints):\r\n            if joints_vis[joint_id][0] > 0:\r\n                if joint_id in self.upper_body_ids:\r\n                    upper_joints.append(joints[joint_id])\r\n                else:\r\n                    lower_joints.append(joints[joint_id])\r\n\r\n        if np.random.randn() < 0.5 and len(upper_joints) > 2:\r\n            selected_joints = upper_joints\r\n        else:\r\n            selected_joints = lower_joints \\\r\n                if len(lower_joints) > 2 else upper_joints\r\n\r\n        if len(selected_joints) < 2:\r\n            return None, None\r\n\r\n        selected_joints = np.array(selected_joints, dtype=np.float32)\r\n        center = selected_joints.mean(axis=0)[:2]\r\n\r\n        left_top = np.amin(selected_joints, axis=0)\r\n        right_bottom = np.amax(selected_joints, axis=0)\r\n\r\n        w = right_bottom[0] - left_top[0]\r\n        h = right_bottom[1] - left_top[1]\r\n\r\n        if w > self.aspect_ratio * h:\r\n            h = w * 1.0 / self.aspect_ratio\r\n        elif w < self.aspect_ratio * h:\r\n            w = h * self.aspect_ratio\r\n\r\n        scale = np.array(\r\n            [\r\n                w * 1.0 / self.pixel_std,\r\n                h * 1.0 / self.pixel_std\r\n            ],\r\n            dtype=np.float32\r\n        )\r\n\r\n        scale = scale * 1.5\r\n\r\n        return center, scale\r\n\r\n    def __len__(self,):\r\n        return len(self.db)\r\n\r\n    def __getitem__(self, idx):\r\n        db_rec = copy.deepcopy(self.db[idx])\r\n\r\n        image_file = db_rec['image']\r\n        filename = db_rec['filename'] if 'filename' in db_rec else ''\r\n        imgnum = db_rec['imgnum'] if 'imgnum' in db_rec else ''\r\n\r\n        if self.data_format == 'zip':\r\n            from .utils import zipreader\r\n            data_numpy = zipreader.imread(\r\n                image_file, cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION\r\n            )\r\n        else:\r\n            data_numpy = cv2.imread(\r\n                image_file, cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION\r\n            )\r\n\r\n        if self.color_rgb:\r\n            data_numpy = cv2.cvtColor(data_numpy, cv2.COLOR_BGR2RGB)\r\n\r\n        if data_numpy is None:\r\n            logger.error('=> fail to read {}'.format(image_file))\r\n            raise ValueError('Fail to read {}'.format(image_file))\r\n\r\n        joints = db_rec['joints_3d']\r\n        joints_vis = db_rec['joints_3d_vis']\r\n\r\n        c = db_rec['center']\r\n        s = db_rec['scale']\r\n        score = db_rec['score'] if 'score' in db_rec else 1\r\n        r = 0\r\n\r\n        if self.is_train:\r\n            if (np.sum(joints_vis[:, 0]) > self.num_joints_half_body\r\n                and np.random.rand() < self.prob_half_body):\r\n                c_half_body, s_half_body = self.half_body_transform(\r\n                    joints, joints_vis\r\n                )\r\n\r\n                if c_half_body is not None and s_half_body is not None:\r\n                    c, s = c_half_body, s_half_body\r\n\r\n            sf = self.scale_factor\r\n            rf = self.rotation_factor\r\n            s = s * np.clip(np.random.randn()*sf + 1, 1 - sf, 1 + sf)\r\n            r = np.clip(np.random.randn()*rf, -rf*2, rf*2) \\\r\n                if random.random() <= 0.6 else 0\r\n\r\n            if self.flip and random.random() <= 0.5:\r\n                data_numpy = data_numpy[:, ::-1, :]\r\n                joints, joints_vis = coco_transform.fliplr_joints(\r\n                    joints, joints_vis, data_numpy.shape[1], self.flip_pairs)\r\n                c[0] = data_numpy.shape[1] - c[0] - 1\r\n\r\n        trans = coco_transform.get_affine_transform(c, s, r, self.image_size)\r\n        input = cv2.warpAffine(\r\n            data_numpy,\r\n            trans,\r\n            (int(self.image_size[0]), int(self.image_size[1])),\r\n            flags=cv2.INTER_LINEAR)\r\n\r\n        if self.transform:\r\n            input = self.transform(input)\r\n\r\n        for i in range(self.num_joints):\r\n            if joints_vis[i, 0] > 0.0:\r\n                joints[i, 0:2] = coco_transform.affine_transform(joints[i, 0:2], trans)\r\n\r\n        target, target_weight = self.generate_target(joints, joints_vis)\r\n\r\n        target = torch.from_numpy(target)\r\n        target_weight = torch.from_numpy(target_weight)\r\n\r\n        meta = {\r\n            'image': image_file,\r\n            'filename': filename,\r\n            'imgnum': imgnum,\r\n            'joints': joints,\r\n            'joints_vis': joints_vis,\r\n            'center': c,\r\n            'scale': s,\r\n            'rotation': r,\r\n            'score': score\r\n        }\r\n\r\n        return input, meta , target, target_weight\r\n\r\n    def select_data(self, db):\r\n        db_selected = []\r\n        for rec in db:\r\n            num_vis = 0\r\n            joints_x = 0.0\r\n            joints_y = 0.0\r\n            for joint, joint_vis in zip(\r\n                    rec['joints_3d'], rec['joints_3d_vis']):\r\n                if joint_vis[0] <= 0:\r\n                    continue\r\n                num_vis += 1\r\n\r\n                joints_x += joint[0]\r\n                joints_y += joint[1]\r\n            if num_vis == 0:\r\n                continue\r\n\r\n            joints_x, joints_y = joints_x / num_vis, joints_y / num_vis\r\n\r\n            area = rec['scale'][0] * rec['scale'][1] * (self.pixel_std**2)\r\n            joints_center = np.array([joints_x, joints_y])\r\n            bbox_center = np.array(rec['center'])\r\n            diff_norm2 = np.linalg.norm((joints_center-bbox_center), 2)\r\n            ks = np.exp(-1.0*(diff_norm2**2) / ((0.2)**2*2.0*area))\r\n\r\n            metric = (0.2 / 16) * num_vis + 0.45 - 0.2 / 16\r\n            if ks > metric:\r\n                db_selected.append(rec)\r\n\r\n        logger.info('=> num db: {}'.format(len(db)))\r\n        logger.info('=> num selected db: {}'.format(len(db_selected)))\r\n        return db_selected\r\n\r\n    def generate_target(self, joints, joints_vis):\r\n        '''\r\n        :param joints:  [num_joints, 3]\r\n        :param joints_vis: [num_joints, 3]\r\n        :return: target, target_weight(1: visible, 0: invisible)\r\n        '''\r\n        target_weight = np.ones((self.num_joints, 1), dtype=np.float32)\r\n        target_weight[:, 0] = joints_vis[:, 0]\r\n\r\n        assert self.target_type == 'gaussian', \\\r\n            'Only support gaussian map now!'\r\n\r\n        if self.target_type == 'gaussian':\r\n            target = np.zeros((self.num_joints,\r\n                               self.heatmap_size[1],\r\n                               self.heatmap_size[0]),\r\n                              dtype=np.float32)\r\n\r\n            tmp_size = self.sigma * 3\r\n\r\n            for joint_id in range(self.num_joints):\r\n                feat_stride = self.image_size / self.heatmap_size\r\n                mu_x = int(joints[joint_id][0] / feat_stride[0] + 0.5)\r\n                mu_y = int(joints[joint_id][1] / feat_stride[1] + 0.5)\r\n                # Check that any part of the gaussian is in-bounds\r\n                ul = [int(mu_x - tmp_size), int(mu_y - tmp_size)]\r\n                br = [int(mu_x + tmp_size + 1), int(mu_y + tmp_size + 1)]\r\n                if ul[0] >= self.heatmap_size[0] or ul[1] >= self.heatmap_size[1] \\\r\n                        or br[0] < 0 or br[1] < 0:\r\n                    # If not, just return the image as is\r\n                    target_weight[joint_id] = 0\r\n                    continue\r\n\r\n                # # Generate gaussian\r\n                size = 2 * tmp_size + 1\r\n                x = np.arange(0, size, 1, np.float32)\r\n                y = x[:, np.newaxis]\r\n                x0 = y0 = size // 2\r\n                # The gaussian is not normalized, we want the center value to equal 1\r\n                g = np.exp(- ((x - x0) ** 2 + (y - y0) ** 2) / (2 * self.sigma ** 2))\r\n\r\n                # Usable gaussian range\r\n                g_x = max(0, -ul[0]), min(br[0], self.heatmap_size[0]) - ul[0]\r\n                g_y = max(0, -ul[1]), min(br[1], self.heatmap_size[1]) - ul[1]\r\n                # Image range\r\n                img_x = max(0, ul[0]), min(br[0], self.heatmap_size[0])\r\n                img_y = max(0, ul[1]), min(br[1], self.heatmap_size[1])\r\n\r\n                v = target_weight[joint_id]\r\n                if v > 0.5:\r\n                    target[joint_id][img_y[0]:img_y[1], img_x[0]:img_x[1]] = \\\r\n                        g[g_y[0]:g_y[1], g_x[0]:g_x[1]]\r\n\r\n        if self.use_different_joints_weight:\r\n            target_weight = np.multiply(target_weight, self.joints_weight)\r\n\r\n        return target, target_weight\r\n"""
mmskeleton/models/__init__.py,0,b''
mmskeleton/ops/__init__.py,0,b'from . import st_gcn'
mmskeleton/processor/__init__.py,0,"b'from . import recognition, pose_demo'"
mmskeleton/processor/apis.py,4,"b'import os\r\nimport cv2\r\nimport torch\r\nimport torchvision\r\nimport math\r\nimport numpy as np\r\n\r\nfrom mmskeleton.utils import call_obj\r\nfrom mmskeleton.utils import load_checkpoint\r\nfrom .utils.infernce_utils import get_final_preds\r\nfrom mmcv.utils import Config\r\nfrom collections import OrderedDict\r\nfrom mmskeleton.datasets.utils.coco_transform import flip_back\r\nfrom mmskeleton.datasets.utils.video_demo import VideoDemo\r\nfrom mmskeleton.utils import get_mmskeleton_url\r\n\r\nflip_pairs = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13, 14],\r\n              [15, 16]]\r\n\r\n\r\ndef init_twodimestimator(config, checkpoint=None, device=\'cpu\'):\r\n    if isinstance(config, str):\r\n        config = Config.fromfile(config)\r\n        config = config.processor_cfg\r\n    elif isinstance(config, OrderedDict):\r\n        config = config\r\n    else:\r\n        raise ValueError(\r\n            \'Input config type is: {}, expect ""str"" or ""Orderdict""\'.format(\r\n                type(config)))\r\n    model_cfg = config.model_cfg\r\n\r\n    if isinstance(model_cfg, list):\r\n        model = [call_obj(**c) for c in model_cfg]\r\n        model = torch.nn.Sequential(*model)\r\n    else:\r\n        model = call_obj(**model_cfg)\r\n    load_checkpoint(model, checkpoint, map_location=device)\r\n    model.to(device)\r\n    model = model.eval()\r\n\r\n    return model\r\n\r\n\r\ndef inference_twodimestimator(model, input, meta, flip=False):\r\n    with torch.no_grad():\r\n        outputs = model.forward(input, return_loss=False)\r\n        if isinstance(outputs, list):\r\n            output = outputs[-1]\r\n        else:\r\n            output = outputs\r\n        if flip:\r\n            input_flipped = np.flip(input.cpu().numpy(), 3).copy()\r\n            input_flipped = torch.from_numpy(input_flipped).cuda()\r\n            outputs_flipped = model(input_flipped, return_loss=False)\r\n            if isinstance(outputs_flipped, list):\r\n                output_flipped = outputs_flipped[-1]\r\n            else:\r\n                output_flipped = outputs_flipped\r\n            output_flipped = flip_back(output_flipped.detach().cpu().numpy(),\r\n                                       flip_pairs)\r\n            output_flipped = torch.from_numpy(output_flipped.copy()).cuda()\r\n            # feature is not aligned, shift flipped heatmap for higher accuracy\r\n\r\n            output_flipped[:, :, :, 1:] = \\\r\n                    output_flipped.clone()[:, :, :, 0:-1]\r\n            output = (output + output_flipped) * 0.5\r\n        c = meta[\'center\'].numpy()\r\n        s = meta[\'scale\'].numpy()\r\n        preds, maxvals = get_final_preds(True,\r\n                                         output.detach().cpu().numpy(), c, s)\r\n\r\n    return preds, maxvals\r\n\r\n\r\ndef save_batch_image_with_joints(batch_image,\r\n                                 batch_joints,\r\n                                 batch_joints_vis,\r\n                                 nrow=8,\r\n                                 padding=2):\r\n\r\n    grid = torchvision.utils.make_grid(batch_image, nrow, padding, True)\r\n    ndarr = grid.mul(255).clamp(0, 255).byte().permute(1, 2, 0).cpu().numpy()\r\n    ndarr = ndarr.copy() * 0\r\n\r\n    nmaps = batch_image.size(0)\r\n    xmaps = min(nrow, nmaps)\r\n    ymaps = int(math.ceil(float(nmaps) / xmaps))\r\n    height = int(batch_image.size(2) + padding)\r\n    width = int(batch_image.size(3) + padding)\r\n    k = 0\r\n    for y in range(ymaps):\r\n        for x in range(xmaps):\r\n            if k >= nmaps:\r\n                break\r\n            joints = batch_joints[k]\r\n            joints_vis = batch_joints_vis[k]\r\n\r\n            for joint, joint_vis in zip(joints, joints_vis):\r\n                joint[0] = x * width + padding + joint[0]\r\n                joint[1] = y * height + padding + joint[1]\r\n                if joint_vis[0]:\r\n                    cv2.circle(ndarr, (int(joint[0]), int(joint[1])), 2,\r\n                               [255, 0, 0], 2)\r\n            k = k + 1\r\n    return ndarr'"
mmskeleton/processor/image2skeleton.py,4,"b'import torch\r\nimport mmcv\r\nimport logging\r\nimport torch.multiprocessing as mp\r\nimport numpy as np\r\nimport cv2\r\nfrom time import time\r\nfrom mmskeleton.utils import cache_checkpoint, get_mmskeleton_url\r\nfrom mmskeleton.datasets.utils.video_demo import VideoDemo\r\nfrom mmdet.apis import init_detector, inference_detector, show_result_pyplot\r\nfrom mmskeleton.processor.apis import init_twodimestimator, inference_twodimestimator, save_batch_image_with_joints\r\nfrom mmcv.utils import ProgressBar\r\nimport os\r\nlogger = logging.getLogger()\r\nimport sys\r\nsys.setrecursionlimit(1000000)\r\n\r\n\r\ndef save(image, det_image, pred, name):\r\n    batch_size = pred.shape[0]\r\n    num_joints = pred.shape[1]\r\n    cimage = np.expand_dims(image, axis=0)\r\n    cimage = torch.from_numpy(cimage)\r\n    pred = torch.from_numpy(pred)\r\n    cimage = cimage.permute(0, 3, 1, 2)\r\n    pred_vis = torch.ones((batch_size, num_joints, 1))\r\n    ndrr = save_batch_image_with_joints(cimage, pred, pred_vis)\r\n    mask = ndrr[:, :, 0] == 255\r\n    mask = np.expand_dims(mask, axis=2)\r\n    out = ndrr * mask + det_image * (1 - mask)\r\n    mmcv.imwrite(out, name)\r\n\r\n\r\ndef worker(video_file, index, detection_cfg, skeleton_cfg, skeleon_data_cfg,\r\n           device, result_queue):\r\n    os.environ[""CUDA_VISIBLE_DEVICES""] = str(device)\r\n    video_frames = mmcv.VideoReader(video_file)\r\n\r\n    # load model\r\n    detection_model_file = detection_cfg.model_cfg\r\n    detection_checkpoint_file = get_mmskeleton_url(\r\n        detection_cfg.checkpoint_file)\r\n    detection_model = init_detector(detection_model_file,\r\n                                    detection_checkpoint_file,\r\n                                    device=\'cpu\')\r\n    skeleton_model_file = skeleton_cfg.model_cfg\r\n    skeletion_checkpoint_file = skeleton_cfg.checkpoint_file\r\n    skeleton_model = init_twodimestimator(skeleton_model_file,\r\n                                          skeletion_checkpoint_file,\r\n                                          device=\'cpu\')\r\n\r\n    detection_model = detection_model.cuda()\r\n    skeleton_model = skeleton_model.cuda()\r\n\r\n    for idx in index:\r\n        skeleton_result = dict()\r\n        image = video_frames[idx]\r\n        draw_image = image.copy()\r\n        bbox_result = inference_detector(detection_model, image)\r\n\r\n        person_bbox, labels = VideoDemo.bbox_filter(bbox_result,\r\n                                                    detection_cfg.bbox_thre)\r\n\r\n        if len(person_bbox) > 0:\r\n            person, meta = VideoDemo.skeleton_preprocess(\r\n                image[:, :, ::-1], person_bbox, skeleon_data_cfg)\r\n            preds, maxvals = inference_twodimestimator(skeleton_model,\r\n                                                       person.cuda(), meta,\r\n                                                       True)\r\n            results = VideoDemo.skeleton_postprocess(preds, maxvals, meta)\r\n            if skeleon_data_cfg.save_video:\r\n                file = os.path.join(skeleon_data_cfg.img_dir,\r\n                                    \'{}.png\'.format(idx))\r\n                mmcv.imshow_det_bboxes(draw_image,\r\n                                       person_bbox,\r\n                                       labels,\r\n                                       detection_model.CLASSES,\r\n                                       score_thr=detection_cfg.bbox_thre,\r\n                                       show=False,\r\n                                       wait_time=0)\r\n                save(image, draw_image, results, file)\r\n\r\n        else:\r\n            preds, maxvals = None, None\r\n            if skeleon_data_cfg.save_video:\r\n                file = os.path.join(skeleon_data_cfg.img_dir,\r\n                                    \'{}.png\'.format(idx))\r\n                mmcv.imwrite(image, file)\r\n        skeleton_result[\'frame_index\'] = idx\r\n        skeleton_result[\'position_preds\'] = preds\r\n        skeleton_result[\'position_maxvals\'] = maxvals\r\n        result_queue.put(skeleton_result)\r\n\r\n\r\ndef inference(\r\n        detection_cfg,\r\n        skeleton_cfg,\r\n        dataset_cfg,\r\n        gpus=1,\r\n        worker_per_gpu=1,\r\n):\r\n    # get frame num\r\n    video_file = dataset_cfg.video_file\r\n    video_name = video_file.strip(\'/n\').split(\'/\')[-1]\r\n    video_frames = mmcv.VideoReader(video_file)\r\n    num_frames = len(video_frames)\r\n    del video_frames\r\n\r\n    data_cfg = skeleton_cfg.data_cfg\r\n    if data_cfg.save_video:\r\n        data_cfg.img_dir = os.path.join(data_cfg.save_dir,\r\n                                        \'{}.img\'.format(video_name))\r\n\r\n        if os.path.exists(data_cfg.img_dir):\r\n            import shutil\r\n            shutil.rmtree(data_cfg.img_dir)\r\n\r\n        os.makedirs(data_cfg.img_dir)\r\n\r\n    # cache model checkpoints\r\n    cache_checkpoint(detection_cfg.checkpoint_file)\r\n    cache_checkpoint(skeleton_cfg.checkpoint_file)\r\n\r\n    # multiprocess settings\r\n    context = mp.get_context(\'spawn\')\r\n    result_queue = context.Queue(num_frames)\r\n    procs = []\r\n    for w in range(gpus * worker_per_gpu):\r\n        shred_list = list(range(w, num_frames, gpus * worker_per_gpu))\r\n        p = context.Process(target=worker,\r\n                            args=(video_file, shred_list, detection_cfg,\r\n                                  skeleton_cfg, data_cfg, w % gpus,\r\n                                  result_queue))\r\n        p.start()\r\n        procs.append(p)\r\n    all_result = []\r\n    print(\'\\nPose estimation start:\')\r\n    prog_bar = ProgressBar(num_frames)\r\n    for i in range(num_frames):\r\n        t = result_queue.get()\r\n        all_result.append(t)\r\n        prog_bar.update()\r\n    for p in procs:\r\n        p.join()\r\n    if len(all_result) == num_frames and data_cfg.save_video:\r\n        print(\'\\n\\nGenerate video:\')\r\n        video_path = os.path.join(data_cfg.save_dir, video_name)\r\n        mmcv.frames2video(data_cfg.img_dir,\r\n                          video_path,\r\n                          filename_tmpl=\'{:01d}.png\')\r\n        print(\'Video was saved to {}\'.format(video_path))\r\n'"
mmskeleton/processor/pose_demo.py,0,"b""import torch\r\nimport mmcv\r\nimport numpy as np\r\nimport cv2\r\nimport os\r\nfrom mmskeleton.apis.estimation import init_pose_estimator, inference_pose_estimator\r\nfrom multiprocessing import current_process, Process, Manager\r\nfrom mmskeleton.utils import cache_checkpoint, third_party\r\nfrom mmcv.utils import ProgressBar\r\n\r\n\r\ndef render(image, pred, person_bbox, bbox_thre=0):\r\n    if pred is None:\r\n        return image\r\n\r\n    mmcv.imshow_det_bboxes(image,\r\n                           person_bbox,\r\n                           np.zeros(len(person_bbox)).astype(int),\r\n                           class_names=['person'],\r\n                           score_thr=bbox_thre,\r\n                           show=False,\r\n                           wait_time=0)\r\n\r\n    for person_pred in pred:\r\n        for joint_pred in person_pred:\r\n            cv2.circle(image, (int(joint_pred[0]), int(joint_pred[1])), 2,\r\n                       [255, 0, 0], 2)\r\n\r\n    return np.uint8(image)\r\n\r\n\r\npose_estimators = dict()\r\n\r\n\r\ndef worker(inputs, results, gpu, detection_cfg, estimation_cfg, render_image):\r\n    worker_id = current_process()._identity[0] - 1\r\n    global pose_estimators\r\n    if worker_id not in pose_estimators:\r\n        pose_estimators[worker_id] = init_pose_estimator(detection_cfg,\r\n                                                         estimation_cfg,\r\n                                                         device=gpu)\r\n    while not inputs.empty():\r\n        try:\r\n            idx, image = inputs.get_nowait()\r\n        except:\r\n            return\r\n\r\n        res = inference_pose_estimator(pose_estimators[worker_id], image)\r\n        res['frame_index'] = idx\r\n\r\n        if render_image:\r\n            res['render_image'] = render(image, res['joint_preds'],\r\n                                         res['person_bbox'],\r\n                                         detection_cfg.bbox_thre)\r\n\r\n        results.put(res)\r\n\r\n\r\ndef inference(detection_cfg,\r\n              estimation_cfg,\r\n              video_file,\r\n              gpus=1,\r\n              worker_per_gpu=1,\r\n              save_dir=None):\r\n\r\n    if not third_party.is_exist('mmdet'):\r\n        print(\r\n            '\\nERROR: This demo requires mmdet for detecting bounding boxes of person. Please install mmdet first.'\r\n        )\r\n        exit(1)\r\n\r\n    video_frames = mmcv.VideoReader(video_file)\r\n    all_result = []\r\n    print('\\nPose estimation:')\r\n\r\n    # case for single process\r\n    if gpus == 1 and worker_per_gpu == 1:\r\n        model = init_pose_estimator(detection_cfg, estimation_cfg, device=0)\r\n        prog_bar = ProgressBar(len(video_frames))\r\n        for i, image in enumerate(video_frames):\r\n            res = inference_pose_estimator(model, image)\r\n            res['frame_index'] = i\r\n            if save_dir is not None:\r\n                res['render_image'] = render(image, res['joint_preds'],\r\n                                             res['person_bbox'],\r\n                                             detection_cfg.bbox_thre)\r\n            all_result.append(res)\r\n            prog_bar.update()\r\n\r\n    # case for multi-process\r\n    else:\r\n        cache_checkpoint(detection_cfg.checkpoint_file)\r\n        cache_checkpoint(estimation_cfg.checkpoint_file)\r\n        num_worker = gpus * worker_per_gpu\r\n        procs = []\r\n        inputs = Manager().Queue(len(video_frames))\r\n        results = Manager().Queue(len(video_frames))\r\n\r\n        for i, image in enumerate(video_frames):\r\n            inputs.put((i, image))\r\n\r\n        for i in range(num_worker):\r\n            p = Process(target=worker,\r\n                        args=(inputs, results, i % gpus, detection_cfg,\r\n                              estimation_cfg, save_dir is not None))\r\n            procs.append(p)\r\n            p.start()\r\n        for i in range(len(video_frames)):\r\n            t = results.get()\r\n            all_result.append(t)\r\n            if 'prog_bar' not in locals():\r\n                prog_bar = ProgressBar(len(video_frames))\r\n            prog_bar.update()\r\n        for p in procs:\r\n            p.join()\r\n\r\n    # sort results\r\n    all_result = sorted(all_result, key=lambda x: x['frame_index'])\r\n\r\n    # generate video\r\n    if (len(all_result) == len(video_frames)) and (save_dir is not None):\r\n        print('\\n\\nGenerate video:')\r\n        video_name = video_file.strip('/n').split('/')[-1]\r\n        if not os.path.isdir(save_dir):\r\n            os.makedirs(save_dir)\r\n        video_path = os.path.join(save_dir, video_name)\r\n        vwriter = cv2.VideoWriter(video_path,\r\n                                  mmcv.video.io.VideoWriter_fourcc(*('mp4v')),\r\n                                  video_frames.fps, video_frames.resolution)\r\n        prog_bar = ProgressBar(len(video_frames))\r\n        for r in all_result:\r\n            vwriter.write(r['render_image'])\r\n            prog_bar.update()\r\n        vwriter.release()\r\n        print('\\nVideo was saved to {}'.format(video_path))\r\n\r\n    return all_result"""
mmskeleton/processor/recognition.py,7,"b""from collections import OrderedDict\nimport torch\nimport logging\nimport numpy as np\nfrom mmskeleton.utils import call_obj, import_obj, load_checkpoint\nfrom mmcv.runner import Runner\nfrom mmcv import Config, ProgressBar\nfrom mmcv.parallel import MMDataParallel\n\n\ndef test(model_cfg,\n         dataset_cfg,\n         checkpoint,\n         batch_size=None,\n         gpu_batch_size=None,\n         gpus=-1,\n         workers=4):\n\n    # calculate batch size\n    if gpus < 0:\n        gpus = torch.cuda.device_count()\n    if (batch_size is None) and (gpu_batch_size is not None):\n        batch_size = gpu_batch_size * gpus\n    assert batch_size is not None, 'Please appoint batch_size or gpu_batch_size.'\n\n    dataset = call_obj(**dataset_cfg)\n    data_loader = torch.utils.data.DataLoader(dataset=dataset,\n                                              batch_size=batch_size,\n                                              shuffle=False,\n                                              num_workers=workers)\n\n    # put model on gpus\n    if isinstance(model_cfg, list):\n        model = [call_obj(**c) for c in model_cfg]\n        model = torch.nn.Sequential(*model)\n    else:\n        model = call_obj(**model_cfg)\n    load_checkpoint(model, checkpoint, map_location='cpu')\n    model = MMDataParallel(model, device_ids=range(gpus)).cuda()\n    model.eval()\n\n    results = []\n    labels = []\n    prog_bar = ProgressBar(len(dataset))\n    for data, label in data_loader:\n        with torch.no_grad():\n            output = model(data)\n            output = model(data).data.cpu().numpy()\n\n        results.append(output)\n        labels.append(label)\n        for i in range(len(data)):\n            prog_bar.update()\n    results = np.concatenate(results)\n    labels = np.concatenate(labels)\n\n    print('Top 1: {:.2f}%'.format(100 * topk_accuracy(results, labels, 1)))\n    print('Top 5: {:.2f}%'.format(100 * topk_accuracy(results, labels, 5)))\n\n\ndef train(\n        work_dir,\n        model_cfg,\n        loss_cfg,\n        dataset_cfg,\n        optimizer_cfg,\n        total_epochs,\n        training_hooks,\n        batch_size=None,\n        gpu_batch_size=None,\n        workflow=[('train', 1)],\n        gpus=-1,\n        log_level=0,\n        workers=4,\n        resume_from=None,\n        load_from=None,\n):\n\n    # calculate batch size\n    if gpus < 0:\n        gpus = torch.cuda.device_count()\n    if (batch_size is None) and (gpu_batch_size is not None):\n        batch_size = gpu_batch_size * gpus\n    assert batch_size is not None, 'Please appoint batch_size or gpu_batch_size.'\n\n    # prepare data loaders\n    if isinstance(dataset_cfg, dict):\n        dataset_cfg = [dataset_cfg]\n    data_loaders = [\n        torch.utils.data.DataLoader(dataset=call_obj(**d),\n                                    batch_size=batch_size,\n                                    shuffle=True,\n                                    num_workers=workers,\n                                    drop_last=True) for d in dataset_cfg\n    ]\n\n    # put model on gpus\n    if isinstance(model_cfg, list):\n        model = [call_obj(**c) for c in model_cfg]\n        model = torch.nn.Sequential(*model)\n    else:\n        model = call_obj(**model_cfg)\n    model.apply(weights_init)\n\n    model = MMDataParallel(model, device_ids=range(gpus)).cuda()\n    loss = call_obj(**loss_cfg)\n\n    # build runner\n    optimizer = call_obj(params=model.parameters(), **optimizer_cfg)\n    runner = Runner(model, batch_processor, optimizer, work_dir, log_level)\n    runner.register_training_hooks(**training_hooks)\n\n    if resume_from:\n        runner.resume(resume_from)\n    elif load_from:\n        runner.load_checkpoint(load_from)\n\n    # run\n    workflow = [tuple(w) for w in workflow]\n    runner.run(data_loaders, workflow, total_epochs, loss=loss)\n\n\n# process a batch of data\ndef batch_processor(model, datas, train_mode, loss):\n\n    data, label = datas\n    data = data.cuda()\n    label = label.cuda()\n\n    # forward\n    output = model(data)\n    losses = loss(output, label)\n\n    # output\n    log_vars = dict(loss=losses.item())\n    if not train_mode:\n        log_vars['top1'] = topk_accuracy(output, label)\n        log_vars['top5'] = topk_accuracy(output, label, 5)\n\n    outputs = dict(loss=losses, log_vars=log_vars, num_samples=len(data.data))\n    return outputs\n\n\ndef topk_accuracy(score, label, k=1):\n    rank = score.argsort()\n    hit_top_k = [l in rank[i, -k:] for i, l in enumerate(label)]\n    accuracy = sum(hit_top_k) * 1.0 / len(hit_top_k)\n    return accuracy\n\n\ndef weights_init(model):\n    classname = model.__class__.__name__\n    if classname.find('Conv1d') != -1:\n        model.weight.data.normal_(0.0, 0.02)\n        if model.bias is not None:\n            model.bias.data.fill_(0)\n    elif classname.find('Conv2d') != -1:\n        model.weight.data.normal_(0.0, 0.02)\n        if model.bias is not None:\n            model.bias.data.fill_(0)\n    elif classname.find('BatchNorm') != -1:\n        model.weight.data.normal_(1.0, 0.02)\n        model.bias.data.fill_(0)\n"""
mmskeleton/processor/recognition_demo.py,1,"b""import torch\r\nimport mmcv\r\nimport logging\r\nimport torch.multiprocessing as mp\r\nimport numpy as np\r\nimport cv2\r\nfrom time import time\r\nfrom mmcv.utils import ProgressBar\r\nfrom .pose_demo import inference as pose_inference\r\nfrom mmskeleton.utils import call_obj, load_checkpoint\r\n\r\n\r\ndef init_recognizer(recognition_cfg, device):\r\n    model = call_obj(**(recognition_cfg.model_cfg))\r\n    load_checkpoint(model,\r\n                    recognition_cfg.checkpoint_file,\r\n                    map_location=device)\r\n    return model\r\n\r\n\r\ndef inference(detection_cfg,\r\n              estimation_cfg,\r\n              recognition_cfg,\r\n              video_file,\r\n              gpus=1,\r\n              worker_per_gpu=1,\r\n              save_dir=None):\r\n\r\n    recognizer = init_recognizer(recognition_cfg, 0)\r\n    # import IPython\r\n    # IPython.embed()\r\n    resolution = mmcv.VideoReader(video_file).resolution\r\n    results = pose_inference(detection_cfg, estimation_cfg, video_file, gpus,\r\n                             worker_per_gpu)\r\n\r\n    seq = np.zeros((1, 3, len(results), 17, 1))\r\n    for i, r in enumerate(results):\r\n        if r['joint_preds'] is not None:\r\n            seq[0, 0, i, :, 0] = r['joint_preds'][0, :, 0] / resolution[0]\r\n            seq[0, 1, i, :, 0] = r['joint_preds'][0, :, 1] / resolution[1]\r\n            seq[0, 2, i, :, 0] = r['joint_scores'][0, :, 0]\r\n\r\n    import IPython\r\n    IPython.embed()\r\n\r\n    return results"""
mmskeleton/processor/skeleton_dataset.py,3,"b""import os\r\nimport json\r\nimport mmcv\r\nimport numpy as np\r\nimport ntpath\r\nfrom mmskeleton.apis.estimation import init_pose_estimator, inference_pose_estimator\r\nfrom mmskeleton.utils import call_obj\r\nfrom mmskeleton.datasets import skeleton\r\nfrom multiprocessing import current_process, Process, Manager\r\nfrom mmskeleton.utils import cache_checkpoint\r\nfrom mmcv.utils import ProgressBar\r\n\r\npose_estimators = dict()\r\n\r\n\r\ndef worker(inputs, results, gpu, detection_cfg, estimation_cfg):\r\n    worker_id = current_process()._identity[0] - 1\r\n    global pose_estimators\r\n    if worker_id not in pose_estimators:\r\n        pose_estimators[worker_id] = init_pose_estimator(\r\n            detection_cfg, estimation_cfg, device=gpu)\r\n    while True:\r\n        idx, image = inputs.get()\r\n\r\n        # end signal\r\n        if image is None:\r\n            return\r\n\r\n        res = inference_pose_estimator(pose_estimators[worker_id], image)\r\n        res['frame_index'] = idx\r\n        results.put(res)\r\n\r\n\r\ndef build(detection_cfg,\r\n          estimation_cfg,\r\n          tracker_cfg,\r\n          video_dir,\r\n          out_dir,\r\n          gpus=1,\r\n          worker_per_gpu=1,\r\n          video_max_length=10000,\r\n          category_annotation=None):\r\n\r\n    cache_checkpoint(detection_cfg.checkpoint_file)\r\n    cache_checkpoint(estimation_cfg.checkpoint_file)\r\n    if tracker_cfg is not None:\r\n        raise NotImplementedError\r\n\r\n    if not os.path.isdir(out_dir):\r\n        os.makedirs(out_dir)\r\n\r\n    if category_annotation is None:\r\n        video_categories = dict()\r\n    else:\r\n        with open(category_annotation) as f:\r\n            video_categories = json.load(f)['annotations']\r\n\r\n    inputs = Manager().Queue(video_max_length)\r\n    results = Manager().Queue(video_max_length)\r\n\r\n    num_worker = gpus * worker_per_gpu\r\n    procs = []\r\n    for i in range(num_worker):\r\n        p = Process(\r\n            target=worker,\r\n            args=(inputs, results, i % gpus, detection_cfg, estimation_cfg))\r\n        procs.append(p)\r\n        p.start()\r\n\r\n    video_file_list = os.listdir(video_dir)\r\n    prog_bar = ProgressBar(len(video_file_list))\r\n    for video_file in video_file_list:\r\n\r\n        reader = mmcv.VideoReader(os.path.join(video_dir, video_file))\r\n        video_frames = reader[:video_max_length]\r\n        annotations = []\r\n        num_keypoints = -1\r\n\r\n        for i, image in enumerate(video_frames):\r\n            inputs.put((i, image))\r\n\r\n        for i in range(len(video_frames)):\r\n            t = results.get()\r\n            if not t['has_return']:\r\n                continue\r\n\r\n            num_person = len(t['joint_preds'])\r\n            assert len(t['person_bbox']) == num_person\r\n\r\n            for j in range(num_person):\r\n                keypoints = [[p[0], p[1], round(s[0], 2)] for p, s in zip(\r\n                    t['joint_preds'][j].round().astype(int).tolist(), t[\r\n                        'joint_scores'][j].tolist())]\r\n                num_keypoints = len(keypoints)\r\n                person_info = dict(\r\n                    person_bbox=t['person_bbox'][j].round().astype(int)\r\n                    .tolist(),\r\n                    frame_index=t['frame_index'],\r\n                    id=j,\r\n                    person_id=None,\r\n                    keypoints=keypoints)\r\n                annotations.append(person_info)\r\n\r\n        # output results\r\n        annotations = sorted(annotations, key=lambda x: x['frame_index'])\r\n        category_id = video_categories[video_file][\r\n            'category_id'] if video_file in video_categories else -1\r\n        info = dict(\r\n            video_name=video_file,\r\n            resolution=reader.resolution,\r\n            num_frame=len(video_frames),\r\n            num_keypoints=num_keypoints,\r\n            keypoint_channels=['x', 'y', 'score'],\r\n            version='1.0')\r\n        video_info = dict(\r\n            info=info, category_id=category_id, annotations=annotations)\r\n        with open(os.path.join(out_dir, video_file + '.json'), 'w') as f:\r\n            json.dump(video_info, f)\r\n\r\n        prog_bar.update()\r\n\r\n    # send end signals\r\n    for p in procs:\r\n        inputs.put((-1, None))\r\n    # wait to finish\r\n    for p in procs:\r\n        p.join()\r\n\r\n    print('\\nBuild skeleton dataset to {}.'.format(out_dir))\r\n    return video_info\r\n\r\n\r\nimport torch\r\n\r\n\r\ndef f(data):\r\n    fmap = data['data'] * mask\r\n    for _ in range(fmap.ndim - 1):\r\n        fmap = fmap.sum(1)\r\n    fmap = fmap / np.sum(mask)\r\n    return fmap\r\n\r\n\r\ndef dataset_analysis(dataset_cfg, mask_channel=2, workers=16, batch_size=16):\r\n    dataset = call_obj(**dataset_cfg)\r\n    data_loader = torch.utils.data.DataLoader(\r\n        dataset=dataset,\r\n        batch_size=batch_size,\r\n        shuffle=False,\r\n        num_workers=workers)\r\n\r\n    prog_bar = ProgressBar(len(dataset))\r\n    for k, (data, mask) in enumerate(data_loader):\r\n        assert mask.size(1) == 1\r\n        n = data.size(0)\r\n        c = data.size(1)\r\n        if k == 0:\r\n            means = [[] for i in range(c)]\r\n            stds = [[] for i in range(c)]\r\n        mask = mask.expand(data.size()).type_as(data)\r\n        data = data * mask\r\n        sum = data.reshape(n * c, -1).sum(1)\r\n        num = mask.reshape(n * c, -1).sum(1)\r\n        mean = sum / num\r\n        diff = (data.reshape(n * c, -1) - mean.view(n * c, 1)) * mask.view(\r\n            n * c, -1)\r\n        std = ((diff**2).sum(1) / num)**0.5\r\n        mean = mean.view(n, c)\r\n        std = std.view(n, c)\r\n        for i in range(c):\r\n            m = mean[:, i]\r\n            m = m[~torch.isnan(m)]\r\n            if len(m) > 0:\r\n                means[i].append(m.mean())\r\n            s = std[:, i]\r\n            s = s[~torch.isnan(s)]\r\n            if len(s) > 0:\r\n                stds[i].append(s.mean())\r\n        for i in range(n):\r\n            prog_bar.update()\r\n    means = [np.mean(m) for m in means]\r\n    stds = [np.mean(s) for s in stds]\r\n    print('\\n\\nDataset analysis result:')\r\n    print('\\tmean of channels : {}'.format(means))\r\n    print('\\tstd of channels  : {}'.format(stds))"""
mmskeleton/processor/twodimestimation.py,9,"b""import torch\r\nimport numpy as np\r\nfrom collections import OrderedDict\r\nfrom mmskeleton.utils import call_obj, import_obj, load_checkpoint, get_mmskeleton_url\r\nfrom mmcv import ProgressBar\r\nfrom mmcv.parallel import MMDataParallel\r\nfrom mmskeleton.datasets.utils.coco_transform import flip_back\r\nfrom .utils.infernce_utils import get_final_preds\r\nimport torchvision.transforms as transforms\r\nfrom mmdet.apis import init_detector, inference_detector, show_result_pyplot\r\nfrom mmskeleton.processor.apis import init_twodimestimator, inference_twodimestimator\r\nfrom mmskeleton.datasets.utils.coco_transform import xywh2cs, get_affine_transform\r\nimport cv2\r\n\r\n\r\n# parse loss\r\ndef parse_losses(losses):\r\n    log_vars = OrderedDict()\r\n    for loss_name, loss_value in losses.items():\r\n        if isinstance(loss_value, torch.Tensor):\r\n            log_vars[loss_name] = loss_value.mean()\r\n        elif isinstance(loss_value, list):\r\n            log_vars[loss_name] = sum(_loss.mean() for _loss in loss_value)\r\n        else:\r\n            raise TypeError(\r\n                '{} is not a tensor or list of tensors'.format(loss_name))\r\n\r\n    loss = sum(_value for _key, _value in log_vars.items() if 'loss' in _key)\r\n\r\n    log_vars['loss'] = loss\r\n    # print(log_vars)\r\n    for name in log_vars:\r\n        log_vars[name] = log_vars[name].item()\r\n\r\n    return loss, log_vars\r\n\r\n\r\n# process a batch of data\r\ndef batch_processor(models, datas, train_mode):\r\n\r\n    losses = models.forward(*datas, return_loss=True)\r\n    loss, log_vars = parse_losses(losses)\r\n    outputs = dict(loss=loss, log_vars=log_vars, num_samples=datas[0].size(0))\r\n\r\n    return outputs\r\n\r\n\r\n# train\r\ndef train(\r\n        work_dir,\r\n        model_cfg,\r\n        dataset_cfg,\r\n        batch_size,\r\n        optimizer_cfg,\r\n        total_epochs,\r\n        training_hooks,\r\n        workflow=[('train', 1)],\r\n        gpus=1,\r\n        log_level=0,\r\n        workers=4,\r\n        resume_from=None,\r\n        load_from=None,\r\n):\r\n    # prepare data loaders\r\n    if isinstance(dataset_cfg, dict):\r\n        dataset_cfg = [dataset_cfg]\r\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\r\n                                     std=[0.229, 0.224, 0.225])\r\n    data_loaders = [\r\n        torch.utils.data.DataLoader(\r\n            dataset=call_obj(**d,\r\n                             transform=transforms.Compose([\r\n                                 transforms.ToTensor(),\r\n                                 normalize,\r\n                             ])),\r\n            batch_size=batch_size * gpus,\r\n            shuffle=True,\r\n            num_workers=workers,\r\n            drop_last=True) for d in dataset_cfg\r\n    ]\r\n\r\n    # put model on gpus\r\n    if isinstance(model_cfg, list):\r\n        model = [call_obj(**c) for c in model_cfg]\r\n        model = torch.nn.Sequential(*model)\r\n    else:\r\n        model = call_obj(**model_cfg)\r\n    model = MMDataParallel(model, device_ids=range(gpus)).cuda()\r\n    # build runner\r\n    optimizer = call_obj(params=model.parameters(), **optimizer_cfg)\r\n    runner = Runner(model, batch_processor, optimizer, work_dir, log_level)\r\n    runner.register_training_hooks(**training_hooks)\r\n\r\n    if resume_from:\r\n        runner.resume(resume_from)\r\n    elif load_from:\r\n        runner.load_checkpoint(load_from)\r\n    # run\r\n    workflow = [tuple(w) for w in workflow]\r\n    runner.run(data_loaders, workflow, total_epochs)\r\n\r\n\r\n# test\r\ndef test(test_cfg,\r\n         model_cfg,\r\n         dataset_cfg,\r\n         checkpoint,\r\n         batch_size,\r\n         work_dir,\r\n         gpus=1,\r\n         workers=4):\r\n\r\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\r\n                                     std=[0.229, 0.224, 0.225])\r\n\r\n    dataset = call_obj(**dataset_cfg,\r\n                       transform=transforms.Compose([\r\n                           transforms.ToTensor(),\r\n                           normalize,\r\n                       ]))\r\n\r\n    data_loader = torch.utils.data.DataLoader(dataset=dataset,\r\n                                              batch_size=batch_size * gpus,\r\n                                              shuffle=False,\r\n                                              num_workers=workers * gpus)\r\n\r\n    # put model on gpus\r\n    if isinstance(model_cfg, list):\r\n        model = [call_obj(**c) for c in model_cfg]\r\n        model = torch.nn.Sequential(*model)\r\n    else:\r\n        model = call_obj(**model_cfg)\r\n\r\n    load_checkpoint(model, checkpoint, map_location='cpu')\r\n    model = MMDataParallel(model, device_ids=range(gpus)).cuda()\r\n    model.eval()\r\n    # prepare for evaluation\r\n    num_samples = len(dataset)\r\n    prog_bar = ProgressBar(num_samples // (batch_size * gpus) + 1)\r\n    all_preds = np.zeros((num_samples, model_cfg.skeleton_head.num_joints, 3),\r\n                         dtype=np.float32)\r\n\r\n    all_boxes = np.zeros((num_samples, 6))\r\n    filenames = []\r\n    imgnums = []\r\n    image_path = []\r\n    idx = 0\r\n\r\n    # copy from hrnet\r\n    with torch.no_grad():\r\n        for i, (input, meta, target, target_weight) in enumerate(data_loader):\r\n            # get prediction\r\n            outputs = model.forward(input, return_loss=False)\r\n            if isinstance(outputs, list):\r\n                output = outputs[-1]\r\n            else:\r\n                output = outputs\r\n            # filp test\r\n            if test_cfg.flip:\r\n                input_flipped = np.flip(input.cpu().numpy(), 3).copy()\r\n                input_flipped = torch.from_numpy(input_flipped).cuda()\r\n                outputs_flipped = model(input_flipped, return_loss=False)\r\n                if isinstance(outputs_flipped, list):\r\n                    output_flipped = outputs_flipped[-1]\r\n                else:\r\n                    output_flipped = outputs_flipped\r\n                output_flipped = flip_back(output_flipped.cpu().numpy(),\r\n                                           dataset.flip_pairs)\r\n                output_flipped = torch.from_numpy(output_flipped.copy()).cuda()\r\n                # feature is not aligned, shift flipped heatmap for higher accuracy\r\n                if test_cfg.shift_heatmap:\r\n                    output_flipped[:, :, :, 1:] = \\\r\n                        output_flipped.clone()[:, :, :, 0:-1]\r\n                output = (output + output_flipped) * 0.5\r\n\r\n            c = meta['center'].numpy()\r\n            s = meta['scale'].numpy()\r\n            score = meta['score'].numpy()\r\n\r\n            num_images = input.size(0)\r\n            preds, maxvals = get_final_preds(test_cfg.post_process,\r\n                                             output.detach().cpu().numpy(), c,\r\n                                             s)\r\n\r\n            all_preds[idx:idx + num_images, :, 0:2] = preds[:, :, 0:2]\r\n            all_preds[idx:idx + num_images, :, 2:3] = maxvals\r\n            # double check this all_boxes parts\r\n            all_boxes[idx:idx + num_images, 0:2] = c[:, 0:2]\r\n            all_boxes[idx:idx + num_images, 2:4] = s[:, 0:2]\r\n            all_boxes[idx:idx + num_images, 4] = np.prod(s * 200, 1)\r\n            all_boxes[idx:idx + num_images, 5] = score\r\n            image_path.extend(meta['image'])\r\n\r\n            idx += num_images\r\n            prog_bar.update()\r\n\r\n        name_values, perf_indicator = dataset.evaluate(test_cfg, all_preds,\r\n                                                       work_dir, all_boxes,\r\n                                                       image_path, filenames,\r\n                                                       imgnums)\r\n    return perf_indicator\r\n\r\n\r\ndef inference_model(\r\n        images,\r\n        detection_model,\r\n        skeleton_model,\r\n):\r\n    batch_size = images.size()[0]\r\n    skeleton_results = dict()\r\n    # process each batch image by image\r\n    for idx, b in enumerate(batch_size):\r\n        # get person bboxes\r\n        image = images[b, :, :, :]\r\n        bbox_result = inference_detector(detection_model, image)\r\n        from IPython import embed\r\n        embed()\r\n    #     person_bboxes = bbox_result_filter(bbox_result)\r\n    #     # get skeleton estimation\r\n    #     if person_bboxes.shape[0] > 0:\r\n    #         image, meta = preprocess_skeleton_inputs(image, person_bboxes)\r\n    #         skeleton_result, maxval= inference_twodimestimator(skeleton_model, image)\r\n    #         skeleton_results[str(idx)] = skeleton_result\r\n    # return skeleton_results\r\n\r\n\r\ndef inference(detection_cfg,\r\n              skeleton_cfg,\r\n              dataset_cfg,\r\n              batch_size,\r\n              gpus=1,\r\n              workers=4):\r\n\r\n    dataset = call_obj(**dataset_cfg)\r\n    data_loader = torch.utils.data.DataLoader(dataset=dataset,\r\n                                              batch_size=batch_size * gpus,\r\n                                              shuffle=False,\r\n                                              num_workers=workers * gpus)\r\n\r\n    # build detection model\r\n    detection_model_file = detection_cfg.model_cfg\r\n    detection_checkpoint_file = detection_cfg.checkpoint_file\r\n\r\n    detection_model = init_detector(detection_model_file,\r\n                                    detection_checkpoint_file,\r\n                                    device='cuda:0')\r\n    from IPython import embed\r\n    embed()\r\n    detection_model = MMDataParallel(detection_model,\r\n                                     device_ids=range(gpus)).cuda()\r\n\r\n    # skeleton_model_file = skeleton_cfg.model_file\r\n    # skeleton_checkpint_file = skeleton_cfg.checkpoint_file\r\n    # skeleton_model = init_twodimestimator(skeleton_model_file,\r\n    #                                       skeleton_checkpint_file,\r\n    #                                       device='cpu')\r\n    # skeleton_model = MMDataParallel(skeleton_model, device_ids=range(gpus)).cuda()\r\n\r\n    for idx, image in enumerate(data_loader):\r\n        skeleton_resluts = inference_model(image, detection_model,\r\n                                           skeleton_model)\r\n    return skeleton_resluts\r\n"""
mmskeleton/utils/__init__.py,0,"b""from . import third_party\nfrom .importer import import_obj, call_obj, set_attr, get_attr\nfrom .checkpoint import load_checkpoint, get_mmskeleton_url, cache_checkpoint\nfrom .config import Config\n\n__all__ = [\n    'import_obj',\n    'call_obj',\n    'set_attr',\n    'get_attr',\n    'load_checkpoint',\n    'get_mmskeleton_url',\n    'cache_checkpoint',\n    'Config',\n    'third_party',\n]\n"""
mmskeleton/utils/checkpoint.py,0,"b'from mmcv.runner import load_checkpoint as mmcv_load_checkpoint\nfrom mmcv.runner.checkpoint import load_url_dist\nimport urllib\n\n\nmmskeleton_model_urls = {\n    \'st_gcn/kinetics-skeleton\': ""https://open-mmlab.s3.ap-northeast-2.amazonaws.com/mmskeleton/models/st-gcn/st_gcn.kinetics-6fa43f73.pth"",\n    \'st_gcn/ntu-xsub\': ""https://open-mmlab.s3.ap-northeast-2.amazonaws.com/mmskeleton/models/st-gcn/st_gcn.ntu-xsub-300b57d4.pth"",\n    \'st_gcn/ntu-xview\': ""https://open-mmlab.s3.ap-northeast-2.amazonaws.com/mmskeleton/models/st-gcn/st_gcn.ntu-xview-9ba67746.pth"",\n    \'mmdet/htc_dconv_c3-c5_mstrain_400_1400_x101_64x4d_fpn_20e\': \'https://open-mmlab.s3.ap-northeast-2.amazonaws.com/mmskeleton/models/mmdet/htc_dconv_c3-c5_mstrain_400_1400_x101_64x4d_fpn_20e_20190408-0e50669c.pth\',\n    \'pose_estimation/pose_hrnet_w32_256x192\': \'https://open-mmlab.s3.ap-northeast-2.amazonaws.com/mmskeleton/models/pose_estimation/pose_hrnet_w32_256x192-76ea353b.pth\',\n    \'mmdet/cascade_rcnn_r50_fpn_20e\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/mmdetection/models/cascade_rcnn_r50_fpn_20e_20181123-db483a09.pth\',\n}  # yapf: disable\n\n\ndef load_checkpoint(model, filename, *args, **kwargs):\n    try:\n        filename = get_mmskeleton_url(filename)\n        return mmcv_load_checkpoint(model, filename, *args, **kwargs)\n    except (urllib.error.HTTPError, urllib.error.URLError) as e:\n        raise Exception(url_error_message.format(filename)) from e\n\n\ndef get_mmskeleton_url(filename):\n    if filename.startswith(\'mmskeleton://\'):\n        model_name = filename[13:]\n        model_url = (mmskeleton_model_urls[model_name])\n        return model_url\n    return filename\n\n\ndef cache_checkpoint(filename):\n    try:\n        filename = get_mmskeleton_url(filename)\n        load_url_dist(get_mmskeleton_url(filename))\n    except (urllib.error.HTTPError, urllib.error.URLError) as e:\n        raise Exception(url_error_message.format(filename)) from e\n\n\nurl_error_message = """"""\n\n==================================================\nMMSkeleton fail to load checkpoint from url: \n    {}\nPlease check your network connection. Or manually download checkpoints according to the instructor:\n    https://github.com/open-mmlab/mmskeleton/blob/master/doc/MODEL_ZOO.md\n""""""'"
mmskeleton/utils/config.py,0,"b'import os\nfrom mmcv import Config as BaseConfig\nfrom mmskeleton.version import mmskl_home\n\n\nclass Config(BaseConfig):\n    @staticmethod\n    def fromfile(filename):\n        try:\n            return BaseConfig.fromfile(filename)\n        except:\n            return BaseConfig.fromfile(os.path.join(mmskl_home, filename))\n'"
mmskeleton/utils/importer.py,0,"b""import sys\n\n\ndef import_obj(type):\n    if not isinstance(type, str):\n        raise ImportError('Object type should be string.')\n\n    # if type[0] == '.':\n    #     type = 'mmskeleton' + type\n\n    mod_str, _sep, class_str = type.rpartition('.')\n    try:\n        __import__(mod_str)\n        return getattr(sys.modules[mod_str], class_str)\n    except ModuleNotFoundError:\n        if type[0:11] != 'mmskeleton.':\n            return import_obj('mmskeleton.' + type)\n        raise ModuleNotFoundError('Object {} cannot be found in {}.'.format(\n            class_str, mod_str))\n\n\ndef call_obj(type, **kwargs):\n    if isinstance(type, str):\n        return import_obj(type)(**kwargs)\n    elif callable(type):\n        return type(**kwargs)\n    else:\n        raise ValueError('type should be string all callable.')\n\n\ndef set_attr(obj, type, value):\n    if not isinstance(type, str):\n        raise ImportError('Attribute type should be string.')\n\n    attr, _sep, others = type.partition('.')\n    if others == '':\n        attr = int(attr) if attr.isdigit() else attr\n        obj[attr] = value\n        # setattr(obj, attr, value)\n    else:\n        attr = int(attr) if attr.isdigit() else attr\n        set_attr(obj[attr], others, value)\n\n\ndef get_attr(obj, type):\n    if not isinstance(type, str):\n        raise ImportError('Attribute type should be string.')\n\n    attr, _sep, others = type.partition('.')\n\n    if attr == '':\n        return obj\n    else:\n        attr = int(attr) if attr.isdigit() else attr\n        return get_attr(obj[attr], others)\n"""
mmskeleton/utils/third_party.py,0,"b'import lazy_import\n\npycocotools = lazy_import.lazy_module(""pycocotools"")\nCOCO = lazy_import.lazy_module(""pycocotools.COCO"")\nCOCOeval = lazy_import.lazy_module(""pycocotools.COCOeval"")\nmmdet = lazy_import.lazy_module(""mmdet"")\nlazy_import.lazy_module(""mmdet.apis"")\n\n\ndef is_exist(module_name):\n    module = __import__(module_name)\n    try:\n        lazy_import._load_module(module)\n        return True\n    except ImportError:\n        return False'"
deprecated/origin_stgcn_repo/feeder/__init__.py,0,b'from . import tools'
deprecated/origin_stgcn_repo/feeder/feeder.py,4,"b'# sys\nimport os\nimport sys\nimport numpy as np\nimport random\nimport pickle\n\n# torch\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\n\n# visualization\nimport time\n\n# operation\nfrom . import tools\n\n\nclass Feeder(torch.utils.data.Dataset):\n    """""" Feeder for skeleton-based action recognition\n    Arguments:\n        data_path: the path to \'.npy\' data, the shape of data should be (N, C, T, V, M)\n        label_path: the path to label\n        random_choose: If true, randomly choose a portion of the input sequence\n        random_shift: If true, randomly pad zeros at the begining or end of sequence\n        window_size: The length of the output sequence\n        normalization: If true, normalize input sequence\n        debug: If true, only use the first 100 samples\n    """"""\n    def __init__(self,\n                 data_path,\n                 label_path,\n                 random_choose=False,\n                 random_move=False,\n                 window_size=-1,\n                 debug=False,\n                 mmap=True):\n        self.debug = debug\n        self.data_path = data_path\n        self.label_path = label_path\n        self.random_choose = random_choose\n        self.random_move = random_move\n        self.window_size = window_size\n\n        self.load_data(mmap)\n\n    def load_data(self, mmap):\n        # data: N C V T M\n\n        # load label\n        with open(self.label_path, \'rb\') as f:\n            self.sample_name, self.label = pickle.load(f)\n\n        # load data\n        if mmap:\n            self.data = np.load(self.data_path, mmap_mode=\'r\')\n        else:\n            self.data = np.load(self.data_path)\n\n        if self.debug:\n            self.label = self.label[0:100]\n            self.data = self.data[0:100]\n            self.sample_name = self.sample_name[0:100]\n\n        self.N, self.C, self.T, self.V, self.M = self.data.shape\n\n    def __len__(self):\n        return len(self.label)\n\n    def __getitem__(self, index):\n        # get data\n        data_numpy = np.array(self.data[index])\n        label = self.label[index]\n\n        # processing\n        if self.random_choose:\n            data_numpy = tools.random_choose(data_numpy, self.window_size)\n        elif self.window_size > 0:\n            data_numpy = tools.auto_pading(data_numpy, self.window_size)\n        if self.random_move:\n            data_numpy = tools.random_move(data_numpy)\n\n        return data_numpy, label'"
deprecated/origin_stgcn_repo/feeder/feeder_kinetics.py,2,"b'# sys\nimport os\nimport sys\nimport numpy as np\nimport random\nimport pickle\nimport json\n# torch\nimport torch\nimport torch.nn as nn\nfrom torchvision import datasets, transforms\n\n# operation\nfrom . import tools\n\n\nclass Feeder_kinetics(torch.utils.data.Dataset):\n    """""" Feeder for skeleton-based action recognition in kinetics-skeleton dataset\n    Arguments:\n        data_path: the path to \'.npy\' data, the shape of data should be (N, C, T, V, M)\n        label_path: the path to label\n        random_choose: If true, randomly choose a portion of the input sequence\n        random_shift: If true, randomly pad zeros at the begining or end of sequence\n        random_move: If true, perform randomly but continuously changed transformation to input sequence\n        window_size: The length of the output sequence\n        pose_matching: If ture, match the pose between two frames\n        num_person_in: The number of people the feeder can observe in the input sequence\n        num_person_out: The number of people the feeder in the output sequence\n        debug: If true, only use the first 100 samples\n    """"""\n    def __init__(self,\n                 data_path,\n                 label_path,\n                 ignore_empty_sample=True,\n                 random_choose=False,\n                 random_shift=False,\n                 random_move=False,\n                 window_size=-1,\n                 pose_matching=False,\n                 num_person_in=5,\n                 num_person_out=2,\n                 debug=False):\n        self.debug = debug\n        self.data_path = data_path\n        self.label_path = label_path\n        self.random_choose = random_choose\n        self.random_shift = random_shift\n        self.random_move = random_move\n        self.window_size = window_size\n        self.num_person_in = num_person_in\n        self.num_person_out = num_person_out\n        self.pose_matching = pose_matching\n        self.ignore_empty_sample = ignore_empty_sample\n\n        self.load_data()\n\n    def load_data(self):\n        # load file list\n        self.sample_name = os.listdir(self.data_path)\n\n        if self.debug:\n            self.sample_name = self.sample_name[0:2]\n\n        # load label\n        label_path = self.label_path\n        with open(label_path) as f:\n            label_info = json.load(f)\n\n        sample_id = [name.split(\'.\')[0] for name in self.sample_name]\n        self.label = np.array(\n            [label_info[id][\'label_index\'] for id in sample_id])\n        has_skeleton = np.array(\n            [label_info[id][\'has_skeleton\'] for id in sample_id])\n\n        # ignore the samples which does not has skeleton sequence\n        if self.ignore_empty_sample:\n            self.sample_name = [\n                s for h, s in zip(has_skeleton, self.sample_name) if h\n            ]\n            self.label = self.label[has_skeleton]\n\n        # output data shape (N, C, T, V, M)\n        self.N = len(self.sample_name)  #sample\n        self.C = 3  #channel\n        self.T = 300  #frame\n        self.V = 18  #joint\n        self.M = self.num_person_out  #person\n\n    def __len__(self):\n        return len(self.sample_name)\n\n    def __iter__(self):\n        return self\n\n    def __getitem__(self, index):\n\n        # output shape (C, T, V, M)\n        # get data\n        sample_name = self.sample_name[index]\n        sample_path = os.path.join(self.data_path, sample_name)\n        with open(sample_path, \'r\') as f:\n            video_info = json.load(f)\n\n        # fill data_numpy\n        data_numpy = np.zeros((self.C, self.T, self.V, self.num_person_in))\n        for frame_info in video_info[\'data\']:\n            frame_index = frame_info[\'frame_index\']\n            for m, skeleton_info in enumerate(frame_info[""skeleton""]):\n                if m >= self.num_person_in:\n                    break\n                pose = skeleton_info[\'pose\']\n                score = skeleton_info[\'score\']\n                data_numpy[0, frame_index, :, m] = pose[0::2]\n                data_numpy[1, frame_index, :, m] = pose[1::2]\n                data_numpy[2, frame_index, :, m] = score\n\n        # centralization\n        data_numpy[0:2] = data_numpy[0:2] - 0.5\n        data_numpy[0][data_numpy[2] == 0] = 0\n        data_numpy[1][data_numpy[2] == 0] = 0\n\n        # get & check label index\n        label = video_info[\'label_index\']\n        assert (self.label[index] == label)\n\n        # data augmentation\n        if self.random_shift:\n            data_numpy = tools.random_shift(data_numpy)\n        if self.random_choose:\n            data_numpy = tools.random_choose(data_numpy, self.window_size)\n        elif self.window_size > 0:\n            data_numpy = tools.auto_pading(data_numpy, self.window_size)\n        if self.random_move:\n            data_numpy = tools.random_move(data_numpy)\n\n        # sort by score\n        sort_index = (-data_numpy[2, :, :, :].sum(axis=1)).argsort(axis=1)\n        for t, s in enumerate(sort_index):\n            data_numpy[:, t, :, :] = data_numpy[:, t, :, s].transpose(\n                (1, 2, 0))\n        data_numpy = data_numpy[:, :, :, 0:self.num_person_out]\n\n        # match poses between 2 frames\n        if self.pose_matching:\n            data_numpy = tools.openpose_match(data_numpy)\n\n        return data_numpy, label\n\n    def top_k(self, score, top_k):\n        assert (all(self.label >= 0))\n\n        rank = score.argsort()\n        hit_top_k = [l in rank[i, -top_k:] for i, l in enumerate(self.label)]\n        return sum(hit_top_k) * 1.0 / len(hit_top_k)\n\n    def top_k_by_category(self, score, top_k):\n        assert (all(self.label >= 0))\n        return tools.top_k_by_category(self.label, score, top_k)\n\n    def calculate_recall_precision(self, score):\n        assert (all(self.label >= 0))\n        return tools.calculate_recall_precision(self.label, score)\n'"
deprecated/origin_stgcn_repo/feeder/tools.py,0,"b'import numpy as np\nimport random\n\n\ndef downsample(data_numpy, step, random_sample=True):\n    # input: C,T,V,M\n    begin = np.random.randint(step) if random_sample else 0\n    return data_numpy[:, begin::step, :, :]\n\n\ndef temporal_slice(data_numpy, step):\n    # input: C,T,V,M\n    C, T, V, M = data_numpy.shape\n    return data_numpy.reshape(C, T / step, step, V, M).transpose(\n        (0, 1, 3, 2, 4)).reshape(C, T / step, V, step * M)\n\n\ndef mean_subtractor(data_numpy, mean):\n    # input: C,T,V,M\n    # naive version\n    if mean == 0:\n        return\n    C, T, V, M = data_numpy.shape\n    valid_frame = (data_numpy != 0).sum(axis=3).sum(axis=2).sum(axis=0) > 0\n    begin = valid_frame.argmax()\n    end = len(valid_frame) - valid_frame[::-1].argmax()\n    data_numpy[:, :end, :, :] = data_numpy[:, :end, :, :] - mean\n    return data_numpy\n\n\ndef auto_pading(data_numpy, size, random_pad=False):\n    C, T, V, M = data_numpy.shape\n    if T < size:\n        begin = random.randint(0, size - T) if random_pad else 0\n        data_numpy_paded = np.zeros((C, size, V, M))\n        data_numpy_paded[:, begin:begin + T, :, :] = data_numpy\n        return data_numpy_paded\n    else:\n        return data_numpy\n\n\ndef random_choose(data_numpy, size, auto_pad=True):\n    # input: C,T,V,M\n    C, T, V, M = data_numpy.shape\n    if T == size:\n        return data_numpy\n    elif T < size:\n        if auto_pad:\n            return auto_pading(data_numpy, size, random_pad=True)\n        else:\n            return data_numpy\n    else:\n        begin = random.randint(0, T - size)\n        return data_numpy[:, begin:begin + size, :, :]\n\n\ndef random_move(data_numpy,\n                angle_candidate=[-10., -5., 0., 5., 10.],\n                scale_candidate=[0.9, 1.0, 1.1],\n                transform_candidate=[-0.2, -0.1, 0.0, 0.1, 0.2],\n                move_time_candidate=[1]):\n    # input: C,T,V,M\n    C, T, V, M = data_numpy.shape\n    move_time = random.choice(move_time_candidate)\n    node = np.arange(0, T, T * 1.0 / move_time).round().astype(int)\n    node = np.append(node, T)\n    num_node = len(node)\n\n    A = np.random.choice(angle_candidate, num_node)\n    S = np.random.choice(scale_candidate, num_node)\n    T_x = np.random.choice(transform_candidate, num_node)\n    T_y = np.random.choice(transform_candidate, num_node)\n\n    a = np.zeros(T)\n    s = np.zeros(T)\n    t_x = np.zeros(T)\n    t_y = np.zeros(T)\n\n    # linspace\n    for i in range(num_node - 1):\n        a[node[i]:node[i + 1]] = np.linspace(\n            A[i], A[i + 1], node[i + 1] - node[i]) * np.pi / 180\n        s[node[i]:node[i + 1]] = np.linspace(S[i], S[i + 1],\n                                             node[i + 1] - node[i])\n        t_x[node[i]:node[i + 1]] = np.linspace(T_x[i], T_x[i + 1],\n                                               node[i + 1] - node[i])\n        t_y[node[i]:node[i + 1]] = np.linspace(T_y[i], T_y[i + 1],\n                                               node[i + 1] - node[i])\n\n    theta = np.array([[np.cos(a) * s, -np.sin(a) * s],\n                      [np.sin(a) * s, np.cos(a) * s]])\n\n    # perform transformation\n    for i_frame in range(T):\n        xy = data_numpy[0:2, i_frame, :, :]\n        new_xy = np.dot(theta[:, :, i_frame], xy.reshape(2, -1))\n        new_xy[0] += t_x[i_frame]\n        new_xy[1] += t_y[i_frame]\n        data_numpy[0:2, i_frame, :, :] = new_xy.reshape(2, V, M)\n\n    return data_numpy\n\n\ndef random_shift(data_numpy):\n    # input: C,T,V,M\n    C, T, V, M = data_numpy.shape\n    data_shift = np.zeros(data_numpy.shape)\n    valid_frame = (data_numpy != 0).sum(axis=3).sum(axis=2).sum(axis=0) > 0\n    begin = valid_frame.argmax()\n    end = len(valid_frame) - valid_frame[::-1].argmax()\n\n    size = end - begin\n    bias = random.randint(0, T - size)\n    data_shift[:, bias:bias + size, :, :] = data_numpy[:, begin:end, :, :]\n\n    return data_shift\n\n\ndef openpose_match(data_numpy):\n    C, T, V, M = data_numpy.shape\n    assert (C == 3)\n    score = data_numpy[2, :, :, :].sum(axis=1)\n    # the rank of body confidence in each frame (shape: T-1, M)\n    rank = (-score[0:T - 1]).argsort(axis=1).reshape(T - 1, M)\n\n    # data of frame 1\n    xy1 = data_numpy[0:2, 0:T - 1, :, :].reshape(2, T - 1, V, M, 1)\n    # data of frame 2\n    xy2 = data_numpy[0:2, 1:T, :, :].reshape(2, T - 1, V, 1, M)\n    # square of distance between frame 1&2 (shape: T-1, M, M)\n    distance = ((xy2 - xy1)**2).sum(axis=2).sum(axis=0)\n\n    # match pose\n    forward_map = np.zeros((T, M), dtype=int) - 1\n    forward_map[0] = range(M)\n    for m in range(M):\n        choose = (rank == m)\n        forward = distance[choose].argmin(axis=1)\n        for t in range(T - 1):\n            distance[t, :, forward[t]] = np.inf\n        forward_map[1:][choose] = forward\n    assert (np.all(forward_map >= 0))\n\n    # string data\n    for t in range(T - 1):\n        forward_map[t + 1] = forward_map[t + 1][forward_map[t]]\n\n    # generate data\n    new_data_numpy = np.zeros(data_numpy.shape)\n    for t in range(T):\n        new_data_numpy[:, t, :, :] = data_numpy[:, t, :,\n                                                forward_map[t]].transpose(\n                                                    1, 2, 0)\n    data_numpy = new_data_numpy\n\n    # score sort\n    trace_score = data_numpy[2, :, :, :].sum(axis=1).sum(axis=0)\n    rank = (-trace_score).argsort()\n    data_numpy = data_numpy[:, :, :, rank]\n\n    return data_numpy\n\n\ndef top_k_by_category(label, score, top_k):\n    instance_num, class_num = score.shape\n    rank = score.argsort()\n    hit_top_k = [[] for i in range(class_num)]\n    for i in range(instance_num):\n        l = label[i]\n        hit_top_k[l].append(l in rank[i, -top_k:])\n\n    accuracy_list = []\n    for hit_per_category in hit_top_k:\n        if hit_per_category:\n            accuracy_list.append(\n                sum(hit_per_category) * 1.0 / len(hit_per_category))\n        else:\n            accuracy_list.append(0.0)\n    return accuracy_list\n\n\ndef calculate_recall_precision(label, score):\n    instance_num, class_num = score.shape\n    rank = score.argsort()\n    confusion_matrix = np.zeros([class_num, class_num])\n\n    for i in range(instance_num):\n        true_l = label[i]\n        pred_l = rank[i, -1]\n        confusion_matrix[true_l][pred_l] += 1\n\n    precision = []\n    recall = []\n\n    for i in range(class_num):\n        true_p = confusion_matrix[i][i]\n        false_n = sum(confusion_matrix[i, :]) - true_p\n        false_p = sum(confusion_matrix[:, i]) - true_p\n        precision.append(true_p * 1.0 / (true_p + false_p))\n        recall.append(true_p * 1.0 / (true_p + false_n))\n\n    return precision, recall'"
deprecated/origin_stgcn_repo/net/__init__.py,0,b'from . import utils'
deprecated/origin_stgcn_repo/net/st_gcn.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nfrom net.utils.tgcn import ConvTemporalGraphical\nfrom net.utils.graph import Graph\n\n\nclass Model(nn.Module):\n    r""""""Spatial temporal graph convolutional networks.\n\n    Args:\n        in_channels (int): Number of channels in the input data\n        num_class (int): Number of classes for the classification task\n        graph_args (dict): The arguments for building the graph\n        edge_importance_weighting (bool): If ``True``, adds a learnable\n            importance weighting to the edges of the graph\n        **kwargs (optional): Other parameters for graph convolution units\n\n    Shape:\n        - Input: :math:`(N, in_channels, T_{in}, V_{in}, M_{in})`\n        - Output: :math:`(N, num_class)` where\n            :math:`N` is a batch size,\n            :math:`T_{in}` is a length of input sequence,\n            :math:`V_{in}` is the number of graph nodes,\n            :math:`M_{in}` is the number of instance in a frame.\n    """"""\n    def __init__(self, in_channels, num_class, graph_args,\n                 edge_importance_weighting, **kwargs):\n        super().__init__()\n\n        # load graph\n        self.graph = Graph(**graph_args)\n        A = torch.tensor(self.graph.A,\n                         dtype=torch.float32,\n                         requires_grad=False)\n        self.register_buffer(\'A\', A)\n\n        # build networks\n        spatial_kernel_size = A.size(0)\n        temporal_kernel_size = 9\n        kernel_size = (temporal_kernel_size, spatial_kernel_size)\n        self.data_bn = nn.BatchNorm1d(in_channels * A.size(1))\n        kwargs0 = {k: v for k, v in kwargs.items() if k != \'dropout\'}\n        self.st_gcn_networks = nn.ModuleList((\n            st_gcn(in_channels, 64, kernel_size, 1, residual=False, **kwargs0),\n            st_gcn(64, 64, kernel_size, 1, **kwargs),\n            st_gcn(64, 64, kernel_size, 1, **kwargs),\n            st_gcn(64, 64, kernel_size, 1, **kwargs),\n            st_gcn(64, 128, kernel_size, 2, **kwargs),\n            st_gcn(128, 128, kernel_size, 1, **kwargs),\n            st_gcn(128, 128, kernel_size, 1, **kwargs),\n            st_gcn(128, 256, kernel_size, 2, **kwargs),\n            st_gcn(256, 256, kernel_size, 1, **kwargs),\n            st_gcn(256, 256, kernel_size, 1, **kwargs),\n        ))\n\n        # initialize parameters for edge importance weighting\n        if edge_importance_weighting:\n            self.edge_importance = nn.ParameterList([\n                nn.Parameter(torch.ones(self.A.size()))\n                for i in self.st_gcn_networks\n            ])\n        else:\n            self.edge_importance = [1] * len(self.st_gcn_networks)\n\n        # fcn for prediction\n        self.fcn = nn.Conv2d(256, num_class, kernel_size=1)\n\n    def forward(self, x):\n\n        # data normalization\n        N, C, T, V, M = x.size()\n        x = x.permute(0, 4, 3, 1, 2).contiguous()\n        x = x.view(N * M, V * C, T)\n        x = self.data_bn(x)\n        x = x.view(N, M, V, C, T)\n        x = x.permute(0, 1, 3, 4, 2).contiguous()\n        x = x.view(N * M, C, T, V)\n\n        # forwad\n        for gcn, importance in zip(self.st_gcn_networks, self.edge_importance):\n            x, _ = gcn(x, self.A * importance)\n\n        # global pooling\n        x = F.avg_pool2d(x, x.size()[2:])\n        x = x.view(N, M, -1, 1, 1).mean(dim=1)\n\n        # prediction\n        x = self.fcn(x)\n        x = x.view(x.size(0), -1)\n\n        return x\n\n    def extract_feature(self, x):\n\n        # data normalization\n        N, C, T, V, M = x.size()\n        x = x.permute(0, 4, 3, 1, 2).contiguous()\n        x = x.view(N * M, V * C, T)\n        x = self.data_bn(x)\n        x = x.view(N, M, V, C, T)\n        x = x.permute(0, 1, 3, 4, 2).contiguous()\n        x = x.view(N * M, C, T, V)\n\n        # forwad\n        for gcn, importance in zip(self.st_gcn_networks, self.edge_importance):\n            x, _ = gcn(x, self.A * importance)\n\n        _, c, t, v = x.size()\n        feature = x.view(N, M, c, t, v).permute(0, 2, 3, 4, 1)\n\n        # prediction\n        x = self.fcn(x)\n        output = x.view(N, M, -1, t, v).permute(0, 2, 3, 4, 1)\n\n        return output, feature\n\n\nclass st_gcn(nn.Module):\n    r""""""Applies a spatial temporal graph convolution over an input graph sequence.\n\n    Args:\n        in_channels (int): Number of channels in the input sequence data\n        out_channels (int): Number of channels produced by the convolution\n        kernel_size (tuple): Size of the temporal convolving kernel and graph convolving kernel\n        stride (int, optional): Stride of the temporal convolution. Default: 1\n        dropout (int, optional): Dropout rate of the final output. Default: 0\n        residual (bool, optional): If ``True``, applies a residual mechanism. Default: ``True``\n\n    Shape:\n        - Input[0]: Input graph sequence in :math:`(N, in_channels, T_{in}, V)` format\n        - Input[1]: Input graph adjacency matrix in :math:`(K, V, V)` format\n        - Output[0]: Outpu graph sequence in :math:`(N, out_channels, T_{out}, V)` format\n        - Output[1]: Graph adjacency matrix for output data in :math:`(K, V, V)` format\n\n        where\n            :math:`N` is a batch size,\n            :math:`K` is the spatial kernel size, as :math:`K == kernel_size[1]`,\n            :math:`T_{in}/T_{out}` is a length of input/output sequence,\n            :math:`V` is the number of graph nodes.\n\n    """"""\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 dropout=0,\n                 residual=True):\n        super().__init__()\n\n        assert len(kernel_size) == 2\n        assert kernel_size[0] % 2 == 1\n        padding = ((kernel_size[0] - 1) // 2, 0)\n\n        self.gcn = ConvTemporalGraphical(in_channels, out_channels,\n                                         kernel_size[1])\n\n        self.tcn = nn.Sequential(\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(\n                out_channels,\n                out_channels,\n                (kernel_size[0], 1),\n                (stride, 1),\n                padding,\n            ),\n            nn.BatchNorm2d(out_channels),\n            nn.Dropout(dropout, inplace=True),\n        )\n\n        if not residual:\n            self.residual = lambda x: 0\n\n        elif (in_channels == out_channels) and (stride == 1):\n            self.residual = lambda x: x\n\n        else:\n            self.residual = nn.Sequential(\n                nn.Conv2d(in_channels,\n                          out_channels,\n                          kernel_size=1,\n                          stride=(stride, 1)),\n                nn.BatchNorm2d(out_channels),\n            )\n\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x, A):\n\n        res = self.residual(x)\n        x, A = self.gcn(x, A)\n        x = self.tcn(x) + res\n\n        return self.relu(x), A'"
deprecated/origin_stgcn_repo/net/st_gcn_twostream.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nfrom net.utils.tgcn import ConvTemporalGraphical\nfrom net.utils.graph import Graph\n\nfrom .st_gcn import Model as ST_GCN\n\n\nclass Model(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n\n        self.origin_stream = ST_GCN(*args, **kwargs)\n        self.motion_stream = ST_GCN(*args, **kwargs)\n\n    def forward(self, x):\n        N, C, T, V, M = x.size()\n        m = torch.cat((torch.cuda.FloatTensor(N, C, 1, V, M).zero_(),\n                       x[:, :, 1:-1] - 0.5 * x[:, :, 2:] - 0.5 * x[:, :, :-2],\n                       torch.cuda.FloatTensor(N, C, 1, V, M).zero_()), 2)\n\n        res = self.origin_stream(x) + self.motion_stream(m)\n        return res'"
deprecated/origin_stgcn_repo/processor/__init__.py,0,b''
deprecated/origin_stgcn_repo/processor/demo_offline.py,1,"b'#!/usr/bin/env python\nimport os\nimport sys\nimport argparse\nimport json\nimport shutil\nimport time\n\nimport numpy as np\nimport torch\nimport skvideo.io\n\nfrom .io import IO\nimport tools\nimport tools.utils as utils\n\nimport cv2\n\nclass DemoOffline(IO):\n\n    def start(self):\n        \n        # initiate\n        label_name_path = \'./resource/kinetics_skeleton/label_name.txt\'\n        with open(label_name_path) as f:\n            label_name = f.readlines()\n            label_name = [line.rstrip() for line in label_name]\n            self.label_name = label_name\n\n        # pose estimation\n        video, data_numpy = self.pose_estimation()\n\n        # action recognition\n        data = torch.from_numpy(data_numpy)\n        data = data.unsqueeze(0)\n        data = data.float().to(self.dev).detach()  # (1, channel, frame, joint, person)\n\n        # model predict\n        voting_label_name, video_label_name, output, intensity = self.predict(data)\n\n        # render the video\n        images = self.render_video(data_numpy, voting_label_name,\n                            video_label_name, intensity, video)\n\n        # visualize\n        for image in images:\n            image = image.astype(np.uint8)\n            cv2.imshow(""ST-GCN"", image)\n            if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n                break\n\n    def predict(self, data):\n        # forward\n        output, feature = self.model.extract_feature(data)\n        output = output[0]\n        feature = feature[0]\n        intensity = (feature*feature).sum(dim=0)**0.5\n        intensity = intensity.cpu().detach().numpy()\n\n        # get result\n        # classification result of the full sequence\n        voting_label = output.sum(dim=3).sum(\n            dim=2).sum(dim=1).argmax(dim=0)\n        voting_label_name = self.label_name[voting_label]\n        # classification result for each person of the latest frame\n        num_person = data.size(4)\n        latest_frame_label = [output[:, :, :, m].sum(\n            dim=2)[:, -1].argmax(dim=0) for m in range(num_person)]\n        latest_frame_label_name = [self.label_name[l]\n                                   for l in latest_frame_label]\n\n        num_person = output.size(3)\n        num_frame = output.size(1)\n        video_label_name = list()\n        for t in range(num_frame):\n            frame_label_name = list()\n            for m in range(num_person):\n                person_label = output[:, t, :, m].sum(dim=1).argmax(dim=0)\n                person_label_name = self.label_name[person_label]\n                frame_label_name.append(person_label_name)\n            video_label_name.append(frame_label_name)\n        return voting_label_name, video_label_name, output, intensity\n\n    def render_video(self, data_numpy, voting_label_name, video_label_name, intensity, video):\n        images = utils.visualization.stgcn_visualize(\n            data_numpy,\n            self.model.graph.edge,\n            intensity, video,\n            voting_label_name,\n            video_label_name,\n            self.arg.height)\n        return images\n\n    def pose_estimation(self):\n        # load openpose python api\n        if self.arg.openpose is not None:\n            sys.path.append(\'{}/python\'.format(self.arg.openpose))\n            sys.path.append(\'{}/build/python\'.format(self.arg.openpose))\n        try:\n            from openpose import pyopenpose as op\n        except:\n            print(\'Can not find Openpose Python API.\')\n            return\n\n\n        video_name = self.arg.video.split(\'/\')[-1].split(\'.\')[0]\n\n        # initiate\n        opWrapper = op.WrapperPython()\n        params = dict(model_folder=\'./models\', model_pose=\'COCO\')\n        opWrapper.configure(params)\n        opWrapper.start()\n        self.model.eval()\n        video_capture = cv2.VideoCapture(self.arg.video)\n        video_length = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n        pose_tracker = naive_pose_tracker(data_frame=video_length)\n\n        # pose estimation\n        start_time = time.time()\n        frame_index = 0\n        video = list()\n        while(True):\n\n            # get image\n            ret, orig_image = video_capture.read()\n            if orig_image is None:\n                break\n            source_H, source_W, _ = orig_image.shape\n            orig_image = cv2.resize(\n                orig_image, (256 * source_W // source_H, 256))\n            H, W, _ = orig_image.shape\n            video.append(orig_image)\n\n            # pose estimation\n            datum = op.Datum()\n            datum.cvInputData = orig_image\n            opWrapper.emplaceAndPop([datum])\n            multi_pose = datum.poseKeypoints  # (num_person, num_joint, 3)\n            if len(multi_pose.shape) != 3:\n                continue\n\n            # normalization\n            multi_pose[:, :, 0] = multi_pose[:, :, 0]/W\n            multi_pose[:, :, 1] = multi_pose[:, :, 1]/H\n            multi_pose[:, :, 0:2] = multi_pose[:, :, 0:2] - 0.5\n            multi_pose[:, :, 0][multi_pose[:, :, 2] == 0] = 0\n            multi_pose[:, :, 1][multi_pose[:, :, 2] == 0] = 0\n\n            # pose tracking\n            pose_tracker.update(multi_pose, frame_index)\n            frame_index += 1\n\n            print(\'Pose estimation ({}/{}).\'.format(frame_index, video_length))\n\n        data_numpy = pose_tracker.get_skeleton_sequence()\n        return video, data_numpy\n\n    @staticmethod\n    def get_parser(add_help=False):\n\n        # parameter priority: command line > config > default\n        parent_parser = IO.get_parser(add_help=False)\n        parser = argparse.ArgumentParser(\n            add_help=add_help,\n            parents=[parent_parser],\n            description=\'Demo for Spatial Temporal Graph Convolution Network\')\n\n        # region arguments yapf: disable\n        parser.add_argument(\'--video\',\n                            default=\'./resource/media/skateboarding.mp4\',\n                            help=\'Path to video\')\n        parser.add_argument(\'--openpose\',\n                            default=None,\n                            help=\'Path to openpose\')\n        parser.add_argument(\'--model_input_frame\',\n                            default=128,\n                            type=int)\n        parser.add_argument(\'--model_fps\',\n                            default=30,\n                            type=int)\n        parser.add_argument(\'--height\',\n                            default=1080,\n                            type=int,\n                            help=\'height of frame in the output video.\')\n        parser.set_defaults(\n            config=\'./config/st_gcn/kinetics-skeleton/demo_offline.yaml\')\n        parser.set_defaults(print_log=False)\n        # endregion yapf: enable\n\n        return parser\n\nclass naive_pose_tracker():\n    """""" A simple tracker for recording person poses and generating skeleton sequences.\n    For actual occasion, I recommend you to implement a robuster tracker.\n    Pull-requests are welcomed.\n    """"""\n\n    def __init__(self, data_frame=128, num_joint=18, max_frame_dis=np.inf):\n        self.data_frame = data_frame\n        self.num_joint = num_joint\n        self.max_frame_dis = max_frame_dis\n        self.latest_frame = 0\n        self.trace_info = list()\n\n    def update(self, multi_pose, current_frame):\n        # multi_pose.shape: (num_person, num_joint, 3)\n\n        if current_frame <= self.latest_frame:\n            return\n\n        if len(multi_pose.shape) != 3:\n            return\n\n        score_order = (-multi_pose[:, :, 2].sum(axis=1)).argsort(axis=0)\n        for p in multi_pose[score_order]:\n\n            # match existing traces\n            matching_trace = None\n            matching_dis = None\n            for trace_index, (trace, latest_frame) in enumerate(self.trace_info):\n                # trace.shape: (num_frame, num_joint, 3)\n                if current_frame <= latest_frame:\n                    continue\n                mean_dis, is_close = self.get_dis(trace, p)\n                if is_close:\n                    if matching_trace is None:\n                        matching_trace = trace_index\n                        matching_dis = mean_dis\n                    elif matching_dis > mean_dis:\n                        matching_trace = trace_index\n                        matching_dis = mean_dis\n\n            # update trace information\n            if matching_trace is not None:\n                trace, latest_frame = self.trace_info[matching_trace]\n\n                # padding zero if the trace is fractured\n                pad_mode = \'interp\' if latest_frame == self.latest_frame else \'zero\'\n                pad = current_frame-latest_frame-1\n                new_trace = self.cat_pose(trace, p, pad, pad_mode)\n                self.trace_info[matching_trace] = (new_trace, current_frame)\n\n            else:\n                new_trace = np.array([p])\n                self.trace_info.append((new_trace, current_frame))\n\n        self.latest_frame = current_frame\n\n    def get_skeleton_sequence(self):\n\n        # remove old traces\n        valid_trace_index = []\n        for trace_index, (trace, latest_frame) in enumerate(self.trace_info):\n            if self.latest_frame - latest_frame < self.data_frame:\n                valid_trace_index.append(trace_index)\n        self.trace_info = [self.trace_info[v] for v in valid_trace_index]\n\n        num_trace = len(self.trace_info)\n        if num_trace == 0:\n            return None\n\n        data = np.zeros((3, self.data_frame, self.num_joint, num_trace))\n        for trace_index, (trace, latest_frame) in enumerate(self.trace_info):\n            end = self.data_frame - (self.latest_frame - latest_frame)\n            d = trace[-end:]\n            beg = end - len(d)\n            data[:, beg:end, :, trace_index] = d.transpose((2, 0, 1))\n\n        return data\n\n    # concatenate pose to a trace\n    def cat_pose(self, trace, pose, pad, pad_mode):\n        # trace.shape: (num_frame, num_joint, 3)\n        num_joint = pose.shape[0]\n        num_channel = pose.shape[1]\n        if pad != 0:\n            if pad_mode == \'zero\':\n                trace = np.concatenate(\n                    (trace, np.zeros((pad, num_joint, 3))), 0)\n            elif pad_mode == \'interp\':\n                last_pose = trace[-1]\n                coeff = [(p+1)/(pad+1) for p in range(pad)]\n                interp_pose = [(1-c)*last_pose + c*pose for c in coeff]\n                trace = np.concatenate((trace, interp_pose), 0)\n        new_trace = np.concatenate((trace, [pose]), 0)\n        return new_trace\n\n    # calculate the distance between a existing trace and the input pose\n\n    def get_dis(self, trace, pose):\n        last_pose_xy = trace[-1, :, 0:2]\n        curr_pose_xy = pose[:, 0:2]\n\n        mean_dis = ((((last_pose_xy - curr_pose_xy)**2).sum(1))**0.5).mean()\n        wh = last_pose_xy.max(0) - last_pose_xy.min(0)\n        scale = (wh[0] * wh[1]) ** 0.5 + 0.0001\n        is_close = mean_dis < scale * self.max_frame_dis\n        return mean_dis, is_close\n'"
deprecated/origin_stgcn_repo/processor/demo_old.py,1,"b'#!/usr/bin/env python\nimport os\nimport argparse\nimport json\nimport shutil\n\nimport numpy as np\nimport torch\nimport skvideo.io\n\nfrom .io import IO\nimport tools\nimport tools.utils as utils\n\nclass Demo(IO):\n    """"""\n        Demo for Skeleton-based Action Recgnition\n    """"""\n    def start(self):\n\n        openpose = \'{}/examples/openpose/openpose.bin\'.format(self.arg.openpose)\n        video_name = self.arg.video.split(\'/\')[-1].split(\'.\')[0]\n        output_snippets_dir = \'./data/openpose_estimation/snippets/{}\'.format(video_name)\n        output_sequence_dir = \'./data/openpose_estimation/data\'\n        output_sequence_path = \'{}/{}.json\'.format(output_sequence_dir, video_name)\n        output_result_dir = self.arg.output_dir\n        output_result_path = \'{}/{}.mp4\'.format(output_result_dir, video_name)\n        label_name_path = \'./resource/kinetics_skeleton/label_name.txt\'\n        with open(label_name_path) as f:\n            label_name = f.readlines()\n            label_name = [line.rstrip() for line in label_name]\n    \n        # pose estimation\n        openpose_args = dict(\n            video=self.arg.video,\n            write_json=output_snippets_dir,\n            display=0,\n            render_pose=0, \n            model_pose=\'COCO\')\n        command_line = openpose + \' \'\n        command_line += \' \'.join([\'--{} {}\'.format(k, v) for k, v in openpose_args.items()])\n        shutil.rmtree(output_snippets_dir, ignore_errors=True)\n        os.makedirs(output_snippets_dir)\n        os.system(command_line)\n\n        # pack openpose ouputs\n        video = utils.video.get_video_frames(self.arg.video)\n        height, width, _ = video[0].shape\n        video_info = utils.openpose.json_pack(\n            output_snippets_dir, video_name, width, height)\n        if not os.path.exists(output_sequence_dir):\n            os.makedirs(output_sequence_dir)\n        with open(output_sequence_path, \'w\') as outfile:\n            json.dump(video_info, outfile)\n        if len(video_info[\'data\']) == 0:\n            print(\'Can not find pose estimation results.\')\n            return\n        else:\n            print(\'Pose estimation complete.\')\n\n        # parse skeleton data\n        pose, _ = utils.video.video_info_parsing(video_info)\n        data = torch.from_numpy(pose)\n        data = data.unsqueeze(0)\n        data = data.float().to(self.dev).detach()\n\n        # extract feature\n        print(\'\\nNetwork forwad...\')\n        self.model.eval()\n        output, feature = self.model.extract_feature(data)\n        output = output[0]\n        feature = feature[0]\n        intensity = (feature*feature).sum(dim=0)**0.5\n        intensity = intensity.cpu().detach().numpy()\n        label = output.sum(dim=3).sum(dim=2).sum(dim=1).argmax(dim=0)\n        print(\'Prediction result: {}\'.format(label_name[label]))\n        print(\'Done.\')\n\n        # visualization\n        print(\'\\nVisualization...\')\n        label_sequence = output.sum(dim=2).argmax(dim=0)\n        label_name_sequence = [[label_name[p] for p in l ]for l in label_sequence]\n        edge = self.model.graph.edge\n        images = utils.visualization.stgcn_visualize(\n            pose, edge, intensity, video,label_name[label] , label_name_sequence, self.arg.height)\n        print(\'Done.\')\n\n        # save video\n        print(\'\\nSaving...\')\n        if not os.path.exists(output_result_dir):\n            os.makedirs(output_result_dir)\n        writer = skvideo.io.FFmpegWriter(output_result_path,\n                                        outputdict={\'-b\': \'300000000\'})\n        for img in images:\n            writer.writeFrame(img)\n        writer.close()\n        print(\'The Demo result has been saved in {}.\'.format(output_result_path))\n\n    @staticmethod\n    def get_parser(add_help=False):\n\n        # parameter priority: command line > config > default\n        parent_parser = IO.get_parser(add_help=False)\n        parser = argparse.ArgumentParser(\n            add_help=add_help,\n            parents=[parent_parser],\n            description=\'Demo for Spatial Temporal Graph Convolution Network\')\n\n        # region arguments yapf: disable\n        parser.add_argument(\'--video\',\n            default=\'./resource/media/skateboarding.mp4\',\n            help=\'Path to video\')\n        parser.add_argument(\'--openpose\',\n            default=\'3dparty/openpose/build\',\n            help=\'Path to openpose\')\n        parser.add_argument(\'--output_dir\',\n            default=\'./data/demo_result\',\n            help=\'Path to save results\')\n        parser.add_argument(\'--height\',\n            default=1080,\n            type=int)\n        parser.set_defaults(config=\'./config/st_gcn/kinetics-skeleton/demo_old.yaml\')\n        parser.set_defaults(print_log=False)\n        # endregion yapf: enable\n\n        return parser\n'"
deprecated/origin_stgcn_repo/processor/demo_realtime.py,1,"b'#!/usr/bin/env python\nimport os\nimport sys\nimport argparse\nimport json\nimport shutil\nimport time\n\nimport numpy as np\nimport torch\nimport skvideo.io\n\nfrom .io import IO\nimport tools\nimport tools.utils as utils\n\nimport cv2\n\nclass DemoRealtime(IO):\n    """""" A demo for utilizing st-gcn in the realtime action recognition.\n    The Openpose python-api is required for this demo.\n\n    Since the pre-trained model is trained on videos with 30fps,\n    and Openpose is hard to achieve this high speed in the single GPU,\n    if you want to predict actions by **camera** in realtime,\n    either data interpolation or new pre-trained model\n    is required.\n\n    Pull requests are always welcome.\n    """"""\n\n    def start(self):\n        # load openpose python api\n        if self.arg.openpose is not None:\n            sys.path.append(\'{}/python\'.format(self.arg.openpose))\n            sys.path.append(\'{}/build/python\'.format(self.arg.openpose))\n        try:\n            from openpose import pyopenpose as op\n        except:\n            print(\'Can not find Openpose Python API.\')\n            return\n\n        video_name = self.arg.video.split(\'/\')[-1].split(\'.\')[0]\n        label_name_path = \'./resource/kinetics_skeleton/label_name.txt\'\n        with open(label_name_path) as f:\n            label_name = f.readlines()\n            label_name = [line.rstrip() for line in label_name]\n            self.label_name = label_name\n\n        # initiate\n        opWrapper = op.WrapperPython()\n        params = dict(model_folder=\'./models\', model_pose=\'COCO\')\n        opWrapper.configure(params)\n        opWrapper.start()\n        self.model.eval()\n        pose_tracker = naive_pose_tracker()\n\n        if self.arg.video == \'camera_source\':\n            video_capture = cv2.VideoCapture(0)\n        else:\n            video_capture = cv2.VideoCapture(self.arg.video)\n\n        # start recognition\n        start_time = time.time()\n        frame_index = 0\n        while(True):\n\n            tic = time.time()\n\n            # get image\n            ret, orig_image = video_capture.read()\n            if orig_image is None:\n                break\n            source_H, source_W, _ = orig_image.shape\n            orig_image = cv2.resize(\n                orig_image, (256 * source_W // source_H, 256))\n            H, W, _ = orig_image.shape\n            \n            # pose estimation\n            datum = op.Datum()\n            datum.cvInputData = orig_image\n            opWrapper.emplaceAndPop([datum])\n            multi_pose = datum.poseKeypoints  # (num_person, num_joint, 3)\n            if len(multi_pose.shape) != 3:\n                continue\n\n            # normalization\n            multi_pose[:, :, 0] = multi_pose[:, :, 0]/W\n            multi_pose[:, :, 1] = multi_pose[:, :, 1]/H\n            multi_pose[:, :, 0:2] = multi_pose[:, :, 0:2] - 0.5\n            multi_pose[:, :, 0][multi_pose[:, :, 2] == 0] = 0\n            multi_pose[:, :, 1][multi_pose[:, :, 2] == 0] = 0\n\n            # pose tracking\n            if self.arg.video == \'camera_source\':\n                frame_index = int((time.time() - start_time)*self.arg.fps)\n            else:\n                frame_index += 1\n            pose_tracker.update(multi_pose, frame_index)\n            data_numpy = pose_tracker.get_skeleton_sequence()\n            data = torch.from_numpy(data_numpy)\n            data = data.unsqueeze(0)\n            data = data.float().to(self.dev).detach()  # (1, channel, frame, joint, person)\n\n            # model predict\n            voting_label_name, video_label_name, output, intensity = self.predict(\n                data)\n\n            # visualization\n            app_fps = 1 / (time.time() - tic)\n            image = self.render(data_numpy, voting_label_name,\n                                video_label_name, intensity, orig_image, app_fps)\n            cv2.imshow(""ST-GCN"", image)\n            if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n                break\n\n    def predict(self, data):\n        # forward\n        output, feature = self.model.extract_feature(data)\n        output = output[0]\n        feature = feature[0]\n        intensity = (feature*feature).sum(dim=0)**0.5\n        intensity = intensity.cpu().detach().numpy()\n\n        # get result\n        # classification result of the full sequence\n        voting_label = output.sum(dim=3).sum(\n            dim=2).sum(dim=1).argmax(dim=0)\n        voting_label_name = self.label_name[voting_label]\n        # classification result for each person of the latest frame\n        num_person = data.size(4)\n        latest_frame_label = [output[:, :, :, m].sum(\n            dim=2)[:, -1].argmax(dim=0) for m in range(num_person)]\n        latest_frame_label_name = [self.label_name[l]\n                                   for l in latest_frame_label]\n\n        num_person = output.size(3)\n        num_frame = output.size(1)\n        video_label_name = list()\n        for t in range(num_frame):\n            frame_label_name = list()\n            for m in range(num_person):\n                person_label = output[:, t, :, m].sum(dim=1).argmax(dim=0)\n                person_label_name = self.label_name[person_label]\n                frame_label_name.append(person_label_name)\n            video_label_name.append(frame_label_name)\n        return voting_label_name, video_label_name, output, intensity\n\n    def render(self, data_numpy, voting_label_name, video_label_name, intensity, orig_image, fps=0):\n        images = utils.visualization.stgcn_visualize(\n            data_numpy[:, [-1]],\n            self.model.graph.edge,\n            intensity[[-1]], [orig_image],\n            voting_label_name,\n            [video_label_name[-1]],\n            self.arg.height,\n            fps=fps)\n        image = next(images)\n        image = image.astype(np.uint8)\n        return image\n\n    @staticmethod\n    def get_parser(add_help=False):\n\n        # parameter priority: command line > config > default\n        parent_parser = IO.get_parser(add_help=False)\n        parser = argparse.ArgumentParser(\n            add_help=add_help,\n            parents=[parent_parser],\n            description=\'Demo for Spatial Temporal Graph Convolution Network\')\n\n        # region arguments yapf: disable\n        parser.add_argument(\'--video\',\n                            default=\'./resource/media/skateboarding.mp4\',\n                            help=\'Path to video\')\n        parser.add_argument(\'--openpose\',\n                            default=None,\n                            help=\'Path to openpose\')\n        parser.add_argument(\'--model_input_frame\',\n                            default=128,\n                            type=int)\n        parser.add_argument(\'--model_fps\',\n                            default=30,\n                            type=int)\n        parser.add_argument(\'--height\',\n                            default=1080,\n                            type=int,\n                            help=\'height of frame in the output video.\')\n        parser.set_defaults(\n            config=\'./config/st_gcn/kinetics-skeleton/demo_realtime.yaml\')\n        parser.set_defaults(print_log=False)\n        # endregion yapf: enable\n\n        return parser\n\nclass naive_pose_tracker():\n    """""" A simple tracker for recording person poses and generating skeleton sequences.\n    For actual occasion, I recommend you to implement a robuster tracker.\n    Pull-requests are welcomed.\n    """"""\n\n    def __init__(self, data_frame=128, num_joint=18, max_frame_dis=np.inf):\n        self.data_frame = data_frame\n        self.num_joint = num_joint\n        self.max_frame_dis = max_frame_dis\n        self.latest_frame = 0\n        self.trace_info = list()\n\n    def update(self, multi_pose, current_frame):\n        # multi_pose.shape: (num_person, num_joint, 3)\n\n        if current_frame <= self.latest_frame:\n            return\n\n        if len(multi_pose.shape) != 3:\n            return\n\n        score_order = (-multi_pose[:, :, 2].sum(axis=1)).argsort(axis=0)\n        for p in multi_pose[score_order]:\n\n            # match existing traces\n            matching_trace = None\n            matching_dis = None\n            for trace_index, (trace, latest_frame) in enumerate(self.trace_info):\n                # trace.shape: (num_frame, num_joint, 3)\n                if current_frame <= latest_frame:\n                    continue\n                mean_dis, is_close = self.get_dis(trace, p)\n                if is_close:\n                    if matching_trace is None:\n                        matching_trace = trace_index\n                        matching_dis = mean_dis\n                    elif matching_dis > mean_dis:\n                        matching_trace = trace_index\n                        matching_dis = mean_dis\n\n            # update trace information\n            if matching_trace is not None:\n                trace, latest_frame = self.trace_info[matching_trace]\n\n                # padding zero if the trace is fractured\n                pad_mode = \'interp\' if latest_frame == self.latest_frame else \'zero\'\n                pad = current_frame-latest_frame-1\n                new_trace = self.cat_pose(trace, p, pad, pad_mode)\n                self.trace_info[matching_trace] = (new_trace, current_frame)\n\n            else:\n                new_trace = np.array([p])\n                self.trace_info.append((new_trace, current_frame))\n\n        self.latest_frame = current_frame\n\n    def get_skeleton_sequence(self):\n\n        # remove old traces\n        valid_trace_index = []\n        for trace_index, (trace, latest_frame) in enumerate(self.trace_info):\n            if self.latest_frame - latest_frame < self.data_frame:\n                valid_trace_index.append(trace_index)\n        self.trace_info = [self.trace_info[v] for v in valid_trace_index]\n\n        num_trace = len(self.trace_info)\n        if num_trace == 0:\n            return None\n\n        data = np.zeros((3, self.data_frame, self.num_joint, num_trace))\n        for trace_index, (trace, latest_frame) in enumerate(self.trace_info):\n            end = self.data_frame - (self.latest_frame - latest_frame)\n            d = trace[-end:]\n            beg = end - len(d)\n            data[:, beg:end, :, trace_index] = d.transpose((2, 0, 1))\n\n        return data\n\n    # concatenate pose to a trace\n    def cat_pose(self, trace, pose, pad, pad_mode):\n        # trace.shape: (num_frame, num_joint, 3)\n        num_joint = pose.shape[0]\n        num_channel = pose.shape[1]\n        if pad != 0:\n            if pad_mode == \'zero\':\n                trace = np.concatenate(\n                    (trace, np.zeros((pad, num_joint, 3))), 0)\n            elif pad_mode == \'interp\':\n                last_pose = trace[-1]\n                coeff = [(p+1)/(pad+1) for p in range(pad)]\n                interp_pose = [(1-c)*last_pose + c*pose for c in coeff]\n                trace = np.concatenate((trace, interp_pose), 0)\n        new_trace = np.concatenate((trace, [pose]), 0)\n        return new_trace\n\n    # calculate the distance between a existing trace and the input pose\n\n    def get_dis(self, trace, pose):\n        last_pose_xy = trace[-1, :, 0:2]\n        curr_pose_xy = pose[:, 0:2]\n\n        mean_dis = ((((last_pose_xy - curr_pose_xy)**2).sum(1))**0.5).mean()\n        wh = last_pose_xy.max(0) - last_pose_xy.min(0)\n        scale = (wh[0] * wh[1]) ** 0.5 + 0.0001\n        is_close = mean_dis < scale * self.max_frame_dis\n        return mean_dis, is_close\n'"
deprecated/origin_stgcn_repo/processor/io.py,2,"b'#!/usr/bin/env python\n# pylint: disable=W0201\nimport sys\nimport argparse\nimport yaml\nimport numpy as np\n\n# torch\nimport torch\nimport torch.nn as nn\n\n# torchlight\nimport torchlight\nfrom torchlight import str2bool\nfrom torchlight import DictAction\nfrom torchlight import import_class\n\nclass IO():\n    """"""\n        IO Processor\n    """"""\n\n    def __init__(self, argv=None):\n\n        self.load_arg(argv)\n        self.init_environment()\n        self.load_model()\n        self.load_weights()\n        self.gpu()\n\n    def load_arg(self, argv=None):\n        parser = self.get_parser()\n\n        # load arg form config file\n        p = parser.parse_args(argv)\n        if p.config is not None:\n            # load config file\n            with open(p.config, \'r\') as f:\n                default_arg = yaml.load(f, Loader=yaml.FullLoader)\n\n            # update parser from config file\n            key = vars(p).keys()\n            for k in default_arg.keys():\n                if k not in key:\n                    print(\'Unknown Arguments: {}\'.format(k))\n                    assert k in key\n\n            parser.set_defaults(**default_arg)\n\n        self.arg = parser.parse_args(argv)\n\n    def init_environment(self):\n        self.io = torchlight.IO(\n            self.arg.work_dir,\n            save_log=self.arg.save_log,\n            print_log=self.arg.print_log)\n        self.io.save_arg(self.arg)\n\n        # gpu\n        if self.arg.use_gpu:\n            gpus = torchlight.visible_gpu(self.arg.device)\n            torchlight.occupy_gpu(gpus)\n            self.gpus = gpus\n            self.dev = ""cuda:0""\n        else:\n            self.dev = ""cpu""\n\n    def load_model(self):\n        self.model = self.io.load_model(self.arg.model,\n                                        **(self.arg.model_args))\n\n    def load_weights(self):\n        if self.arg.weights:\n            self.model = self.io.load_weights(self.model, self.arg.weights,\n                                              self.arg.ignore_weights)\n\n    def gpu(self):\n        # move modules to gpu\n        self.model = self.model.to(self.dev)\n        for name, value in vars(self).items():\n            cls_name = str(value.__class__)\n            if cls_name.find(\'torch.nn.modules\') != -1:\n                setattr(self, name, value.to(self.dev))\n\n        # model parallel\n        if self.arg.use_gpu and len(self.gpus) > 1:\n            self.model = nn.DataParallel(self.model, device_ids=self.gpus)\n\n    def start(self):\n        self.io.print_log(\'Parameters:\\n{}\\n\'.format(str(vars(self.arg))))\n\n    @staticmethod\n    def get_parser(add_help=False):\n\n        #region arguments yapf: disable\n        # parameter priority: command line > config > default\n        parser = argparse.ArgumentParser( add_help=add_help, description=\'IO Processor\')\n\n        parser.add_argument(\'-w\', \'--work_dir\', default=\'./work_dir/tmp\', help=\'the work folder for storing results\')\n        parser.add_argument(\'-c\', \'--config\', default=None, help=\'path to the configuration file\')\n\n        # processor\n        parser.add_argument(\'--use_gpu\', type=str2bool, default=True, help=\'use GPUs or not\')\n        parser.add_argument(\'--device\', type=int, default=0, nargs=\'+\', help=\'the indexes of GPUs for training or testing\')\n\n        # visulize and debug\n        parser.add_argument(\'--print_log\', type=str2bool, default=True, help=\'print logging or not\')\n        parser.add_argument(\'--save_log\', type=str2bool, default=True, help=\'save logging or not\')\n\n        # model\n        parser.add_argument(\'--model\', default=None, help=\'the model will be used\')\n        parser.add_argument(\'--model_args\', action=DictAction, default=dict(), help=\'the arguments of model\')\n        parser.add_argument(\'--weights\', default=None, help=\'the weights for network initialization\')\n        parser.add_argument(\'--ignore_weights\', type=str, default=[], nargs=\'+\', help=\'the name of weights which will be ignored in the initialization\')\n        #endregion yapf: enable\n\n        return parser\n'"
deprecated/origin_stgcn_repo/processor/processor.py,4,"b'#!/usr/bin/env python\n# pylint: disable=W0201\nimport sys\nimport argparse\nimport yaml\nimport numpy as np\n\n# torch\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# torchlight\nimport torchlight\nfrom torchlight import str2bool\nfrom torchlight import DictAction\nfrom torchlight import import_class\n\nfrom .io import IO\n\nclass Processor(IO):\n    """"""\n        Base Processor\n    """"""\n\n    def __init__(self, argv=None):\n\n        self.load_arg(argv)\n        self.init_environment()\n        self.load_model()\n        self.load_weights()\n        self.gpu()\n        self.load_data()\n        self.load_optimizer()\n\n    def init_environment(self):\n\n        super().init_environment()\n        self.result = dict()\n        self.iter_info = dict()\n        self.epoch_info = dict()\n        self.meta_info = dict(epoch=0, iter=0)\n\n    def load_optimizer(self):\n        pass\n\n    def load_data(self):\n        Feeder = import_class(self.arg.feeder)\n        if \'debug\' not in self.arg.train_feeder_args:\n            self.arg.train_feeder_args[\'debug\'] = self.arg.debug\n        self.data_loader = dict()\n        if self.arg.phase == \'train\':\n            self.data_loader[\'train\'] = torch.utils.data.DataLoader(\n                dataset=Feeder(**self.arg.train_feeder_args),\n                batch_size=self.arg.batch_size,\n                shuffle=True,\n                num_workers=self.arg.num_worker * torchlight.ngpu(\n                    self.arg.device),\n                drop_last=True)\n        if self.arg.test_feeder_args:\n            self.data_loader[\'test\'] = torch.utils.data.DataLoader(\n                dataset=Feeder(**self.arg.test_feeder_args),\n                batch_size=self.arg.test_batch_size,\n                shuffle=False,\n                num_workers=self.arg.num_worker * torchlight.ngpu(\n                    self.arg.device))\n\n    def show_epoch_info(self):\n        for k, v in self.epoch_info.items():\n            self.io.print_log(\'\\t{}: {}\'.format(k, v))\n        if self.arg.pavi_log:\n            self.io.log(\'train\', self.meta_info[\'iter\'], self.epoch_info)\n\n    def show_iter_info(self):\n        if self.meta_info[\'iter\'] % self.arg.log_interval == 0:\n            info =\'\\tIter {} Done.\'.format(self.meta_info[\'iter\'])\n            for k, v in self.iter_info.items():\n                if isinstance(v, float):\n                    info = info + \' | {}: {:.4f}\'.format(k, v)\n                else:\n                    info = info + \' | {}: {}\'.format(k, v)\n\n            self.io.print_log(info)\n\n            if self.arg.pavi_log:\n                self.io.log(\'train\', self.meta_info[\'iter\'], self.iter_info)\n\n    def train(self):\n        for _ in range(100):\n            self.iter_info[\'loss\'] = 0\n            self.show_iter_info()\n            self.meta_info[\'iter\'] += 1\n        self.epoch_info[\'mean loss\'] = 0\n        self.show_epoch_info()\n\n    def test(self):\n        for _ in range(100):\n            self.iter_info[\'loss\'] = 1\n            self.show_iter_info()\n        self.epoch_info[\'mean loss\'] = 1\n        self.show_epoch_info()\n\n    def start(self):\n        self.io.print_log(\'Parameters:\\n{}\\n\'.format(str(vars(self.arg))))\n\n        # training phase\n        if self.arg.phase == \'train\':\n            for epoch in range(self.arg.start_epoch, self.arg.num_epoch):\n                self.meta_info[\'epoch\'] = epoch\n\n                # training\n                self.io.print_log(\'Training epoch: {}\'.format(epoch))\n                self.train()\n                self.io.print_log(\'Done.\')\n\n                # save model\n                if ((epoch + 1) % self.arg.save_interval == 0) or (\n                        epoch + 1 == self.arg.num_epoch):\n                    filename = \'epoch{}_model.pt\'.format(epoch + 1)\n                    self.io.save_model(self.model, filename)\n\n                # evaluation\n                if ((epoch + 1) % self.arg.eval_interval == 0) or (\n                        epoch + 1 == self.arg.num_epoch):\n                    self.io.print_log(\'Eval epoch: {}\'.format(epoch))\n                    self.test()\n                    self.io.print_log(\'Done.\')\n        # test phase\n        elif self.arg.phase == \'test\':\n\n            # the path of weights must be appointed\n            if self.arg.weights is None:\n                raise ValueError(\'Please appoint --weights.\')\n            self.io.print_log(\'Model:   {}.\'.format(self.arg.model))\n            self.io.print_log(\'Weights: {}.\'.format(self.arg.weights))\n\n            # evaluation\n            self.io.print_log(\'Evaluation Start:\')\n            self.test()\n            self.io.print_log(\'Done.\\n\')\n\n            # save the output of model\n            if self.arg.save_result:\n                result_dict = dict(\n                    zip(self.data_loader[\'test\'].dataset.sample_name,\n                        self.result))\n                self.io.save_pkl(result_dict, \'test_result.pkl\')\n\n    @staticmethod\n    def get_parser(add_help=False):\n\n        #region arguments yapf: disable\n        # parameter priority: command line > config > default\n        parser = argparse.ArgumentParser( add_help=add_help, description=\'Base Processor\')\n\n        parser.add_argument(\'-w\', \'--work_dir\', default=\'./work_dir/tmp\', help=\'the work folder for storing results\')\n        parser.add_argument(\'-c\', \'--config\', default=None, help=\'path to the configuration file\')\n\n        # processor\n        parser.add_argument(\'--phase\', default=\'train\', help=\'must be train or test\')\n        parser.add_argument(\'--save_result\', type=str2bool, default=False, help=\'if ture, the output of the model will be stored\')\n        parser.add_argument(\'--start_epoch\', type=int, default=0, help=\'start training from which epoch\')\n        parser.add_argument(\'--num_epoch\', type=int, default=80, help=\'stop training in which epoch\')\n        parser.add_argument(\'--use_gpu\', type=str2bool, default=True, help=\'use GPUs or not\')\n        parser.add_argument(\'--device\', type=int, default=0, nargs=\'+\', help=\'the indexes of GPUs for training or testing\')\n\n        # visulize and debug\n        parser.add_argument(\'--log_interval\', type=int, default=100, help=\'the interval for printing messages (#iteration)\')\n        parser.add_argument(\'--save_interval\', type=int, default=10, help=\'the interval for storing models (#iteration)\')\n        parser.add_argument(\'--eval_interval\', type=int, default=5, help=\'the interval for evaluating models (#iteration)\')\n        parser.add_argument(\'--save_log\', type=str2bool, default=True, help=\'save logging or not\')\n        parser.add_argument(\'--print_log\', type=str2bool, default=True, help=\'print logging or not\')\n        parser.add_argument(\'--pavi_log\', type=str2bool, default=False, help=\'logging on pavi or not\')\n\n        # feeder\n        parser.add_argument(\'--feeder\', default=\'feeder.feeder\', help=\'data loader will be used\')\n        parser.add_argument(\'--num_worker\', type=int, default=4, help=\'the number of worker per gpu for data loader\')\n        parser.add_argument(\'--train_feeder_args\', action=DictAction, default=dict(), help=\'the arguments of data loader for training\')\n        parser.add_argument(\'--test_feeder_args\', action=DictAction, default=dict(), help=\'the arguments of data loader for test\')\n        parser.add_argument(\'--batch_size\', type=int, default=256, help=\'training batch size\')\n        parser.add_argument(\'--test_batch_size\', type=int, default=256, help=\'test batch size\')\n        parser.add_argument(\'--debug\', action=""store_true"", help=\'less data, faster loading\')\n\n        # model\n        parser.add_argument(\'--model\', default=None, help=\'the model will be used\')\n        parser.add_argument(\'--model_args\', action=DictAction, default=dict(), help=\'the arguments of model\')\n        parser.add_argument(\'--weights\', default=None, help=\'the weights for network initialization\')\n        parser.add_argument(\'--ignore_weights\', type=str, default=[], nargs=\'+\', help=\'the name of weights which will be ignored in the initialization\')\n        #endregion yapf: enable\n\n        return parser\n'"
deprecated/origin_stgcn_repo/processor/recognition.py,3,"b'#!/usr/bin/env python\n# pylint: disable=W0201\nimport sys\nimport argparse\nimport yaml\nimport numpy as np\n\n# torch\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# torchlight\nimport torchlight\nfrom torchlight import str2bool\nfrom torchlight import DictAction\nfrom torchlight import import_class\n\nfrom .processor import Processor\n\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find(\'Conv1d\') != -1:\n        m.weight.data.normal_(0.0, 0.02)\n        if m.bias is not None:\n            m.bias.data.fill_(0)\n    elif classname.find(\'Conv2d\') != -1:\n        m.weight.data.normal_(0.0, 0.02)\n        if m.bias is not None:\n            m.bias.data.fill_(0)\n    elif classname.find(\'BatchNorm\') != -1:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)\n\nclass REC_Processor(Processor):\n    """"""\n        Processor for Skeleton-based Action Recgnition\n    """"""\n\n    def load_model(self):\n        self.model = self.io.load_model(self.arg.model,\n                                        **(self.arg.model_args))\n        self.model.apply(weights_init)\n        self.loss = nn.CrossEntropyLoss()\n        \n    def load_optimizer(self):\n        if self.arg.optimizer == \'SGD\':\n            self.optimizer = optim.SGD(\n                self.model.parameters(),\n                lr=self.arg.base_lr,\n                momentum=0.9,\n                nesterov=self.arg.nesterov,\n                weight_decay=self.arg.weight_decay)\n        elif self.arg.optimizer == \'Adam\':\n            self.optimizer = optim.Adam(\n                self.model.parameters(),\n                lr=self.arg.base_lr,\n                weight_decay=self.arg.weight_decay)\n        else:\n            raise ValueError()\n\n    def adjust_lr(self):\n        if self.arg.optimizer == \'SGD\' and self.arg.step:\n            lr = self.arg.base_lr * (\n                0.1**np.sum(self.meta_info[\'epoch\']>= np.array(self.arg.step)))\n            for param_group in self.optimizer.param_groups:\n                param_group[\'lr\'] = lr\n            self.lr = lr\n        else:\n            self.lr = self.arg.base_lr\n\n    def show_topk(self, k):\n        rank = self.result.argsort()\n        hit_top_k = [l in rank[i, -k:] for i, l in enumerate(self.label)]\n        accuracy = sum(hit_top_k) * 1.0 / len(hit_top_k)\n        self.io.print_log(\'\\tTop{}: {:.2f}%\'.format(k, 100 * accuracy))\n\n    def train(self):\n        self.model.train()\n        self.adjust_lr()\n        loader = self.data_loader[\'train\']\n        loss_value = []\n\n        for data, label in loader:\n\n            # get data\n            data = data.float().to(self.dev)\n            label = label.long().to(self.dev)\n\n            # forward\n            output = self.model(data)\n            loss = self.loss(output, label)\n\n            # backward\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n\n            # statistics\n            self.iter_info[\'loss\'] = loss.data.item()\n            self.iter_info[\'lr\'] = \'{:.6f}\'.format(self.lr)\n            loss_value.append(self.iter_info[\'loss\'])\n            self.show_iter_info()\n            self.meta_info[\'iter\'] += 1\n\n        self.epoch_info[\'mean_loss\']= np.mean(loss_value)\n        self.show_epoch_info()\n        self.io.print_timer()\n\n    def test(self, evaluation=True):\n\n        self.model.eval()\n        loader = self.data_loader[\'test\']\n        loss_value = []\n        result_frag = []\n        label_frag = []\n\n        for data, label in loader:\n            \n            # get data\n            data = data.float().to(self.dev)\n            label = label.long().to(self.dev)\n\n            # inference\n            with torch.no_grad():\n                output = self.model(data)\n            result_frag.append(output.data.cpu().numpy())\n\n            # get loss\n            if evaluation:\n                loss = self.loss(output, label)\n                loss_value.append(loss.item())\n                label_frag.append(label.data.cpu().numpy())\n\n        self.result = np.concatenate(result_frag)\n        if evaluation:\n            self.label = np.concatenate(label_frag)\n            self.epoch_info[\'mean_loss\']= np.mean(loss_value)\n            self.show_epoch_info()\n\n            # show top-k accuracy\n            for k in self.arg.show_topk:\n                self.show_topk(k)\n\n    @staticmethod\n    def get_parser(add_help=False):\n\n        # parameter priority: command line > config > default\n        parent_parser = Processor.get_parser(add_help=False)\n        parser = argparse.ArgumentParser(\n            add_help=add_help,\n            parents=[parent_parser],\n            description=\'Spatial Temporal Graph Convolution Network\')\n\n        # region arguments yapf: disable\n        # evaluation\n        parser.add_argument(\'--show_topk\', type=int, default=[1, 5], nargs=\'+\', help=\'which Top K accuracy will be shown\')\n        # optim\n        parser.add_argument(\'--base_lr\', type=float, default=0.01, help=\'initial learning rate\')\n        parser.add_argument(\'--step\', type=int, default=[], nargs=\'+\', help=\'the epoch where optimizer reduce the learning rate\')\n        parser.add_argument(\'--optimizer\', default=\'SGD\', help=\'type of optimizer\')\n        parser.add_argument(\'--nesterov\', type=str2bool, default=True, help=\'use nesterov or not\')\n        parser.add_argument(\'--weight_decay\', type=float, default=0.0001, help=\'weight decay for optimizer\')\n        # endregion yapf: enable\n\n        return parser\n'"
deprecated/origin_stgcn_repo/tools/__init__.py,0,b'from . import utils'
deprecated/origin_stgcn_repo/tools/kinetics_gendata.py,0,"b'import os\nimport sys\nimport pickle\nimport argparse\n\nimport numpy as np\nfrom numpy.lib.format import open_memmap\n\nsys.path.append(\n    os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir)))\nfrom feeder.feeder_kinetics import Feeder_kinetics\n\ntoolbar_width = 30\n\n\ndef print_toolbar(rate, annotation=\'\'):\n    # setup toolbar\n    sys.stdout.write(""{}["".format(annotation))\n    for i in range(toolbar_width):\n        if i * 1.0 / toolbar_width > rate:\n            sys.stdout.write(\' \')\n        else:\n            sys.stdout.write(\'-\')\n        sys.stdout.flush()\n    sys.stdout.write(\']\\r\')\n\n\ndef end_toolbar():\n    sys.stdout.write(""\\n"")\n\n\ndef gendata(\n        data_path,\n        label_path,\n        data_out_path,\n        label_out_path,\n        num_person_in=5,  #observe the first 5 persons \n        num_person_out=2,  #then choose 2 persons with the highest score \n        max_frame=300):\n\n    feeder = Feeder_kinetics(data_path=data_path,\n                             label_path=label_path,\n                             num_person_in=num_person_in,\n                             num_person_out=num_person_out,\n                             window_size=max_frame)\n\n    sample_name = feeder.sample_name\n    sample_label = []\n\n    fp = open_memmap(data_out_path,\n                     dtype=\'float32\',\n                     mode=\'w+\',\n                     shape=(len(sample_name), 3, max_frame, 18,\n                            num_person_out))\n\n    for i, s in enumerate(sample_name):\n        data, label = feeder[i]\n        print_toolbar(\n            i * 1.0 / len(sample_name),\n            \'({:>5}/{:<5}) Processing data: \'.format(i + 1, len(sample_name)))\n        fp[i, :, 0:data.shape[1], :, :] = data\n        sample_label.append(label)\n\n    with open(label_out_path, \'wb\') as f:\n        pickle.dump((sample_name, list(sample_label)), f)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(\n        description=\'Kinetics-skeleton Data Converter.\')\n    parser.add_argument(\'--data_path\',\n                        default=\'data/Kinetics/kinetics-skeleton\')\n    parser.add_argument(\'--out_folder\',\n                        default=\'data/Kinetics/kinetics-skeleton\')\n    arg = parser.parse_args()\n\n    part = [\'train\', \'val\']\n    for p in part:\n        data_path = \'{}/kinetics_{}\'.format(arg.data_path, p)\n        label_path = \'{}/kinetics_{}_label.json\'.format(arg.data_path, p)\n        data_out_path = \'{}/{}_data.npy\'.format(arg.out_folder, p)\n        label_out_path = \'{}/{}_label.pkl\'.format(arg.out_folder, p)\n\n        if not os.path.exists(arg.out_folder):\n            os.makedirs(arg.out_folder)\n        gendata(data_path, label_path, data_out_path, label_out_path)'"
deprecated/origin_stgcn_repo/tools/ntu_gendata.py,0,"b'import os\nimport sys\nimport pickle\n\nimport argparse\nimport numpy as np\nfrom numpy.lib.format import open_memmap\n\nfrom utils.ntu_read_skeleton import read_xyz\n\ntraining_subjects = [\n    1, 2, 4, 5, 8, 9, 13, 14, 15, 16, 17, 18, 19, 25, 27, 28, 31, 34, 35, 38\n]\ntraining_cameras = [2, 3]\nmax_body = 2\nnum_joint = 25\nmax_frame = 300\ntoolbar_width = 30\n\n\ndef print_toolbar(rate, annotation=\'\'):\n    # setup toolbar\n    sys.stdout.write(""{}["".format(annotation))\n    for i in range(toolbar_width):\n        if i * 1.0 / toolbar_width > rate:\n            sys.stdout.write(\' \')\n        else:\n            sys.stdout.write(\'-\')\n        sys.stdout.flush()\n    sys.stdout.write(\']\\r\')\n\n\ndef end_toolbar():\n    sys.stdout.write(""\\n"")\n\n\ndef gendata(data_path,\n            out_path,\n            ignored_sample_path=None,\n            benchmark=\'xview\',\n            part=\'eval\'):\n    if ignored_sample_path != None:\n        with open(ignored_sample_path, \'r\') as f:\n            ignored_samples = [\n                line.strip() + \'.skeleton\' for line in f.readlines()\n            ]\n    else:\n        ignored_samples = []\n    sample_name = []\n    sample_label = []\n    for filename in os.listdir(data_path):\n        if filename in ignored_samples:\n            continue\n        action_class = int(filename[filename.find(\'A\') + 1:filename.find(\'A\') +\n                                    4])\n        subject_id = int(filename[filename.find(\'P\') + 1:filename.find(\'P\') +\n                                  4])\n        camera_id = int(filename[filename.find(\'C\') + 1:filename.find(\'C\') +\n                                 4])\n\n        if benchmark == \'xview\':\n            istraining = (camera_id in training_cameras)\n        elif benchmark == \'xsub\':\n            istraining = (subject_id in training_subjects)\n        else:\n            raise ValueError()\n\n        if part == \'train\':\n            issample = istraining\n        elif part == \'val\':\n            issample = not (istraining)\n        else:\n            raise ValueError()\n\n        if issample:\n            sample_name.append(filename)\n            sample_label.append(action_class - 1)\n\n    with open(\'{}/{}_label.pkl\'.format(out_path, part), \'wb\') as f:\n        pickle.dump((sample_name, list(sample_label)), f)\n    # np.save(\'{}/{}_label.npy\'.format(out_path, part), sample_label)\n\n    fp = open_memmap(\'{}/{}_data.npy\'.format(out_path, part),\n                     dtype=\'float32\',\n                     mode=\'w+\',\n                     shape=(len(sample_label), 3, max_frame, num_joint,\n                            max_body))\n\n    for i, s in enumerate(sample_name):\n        print_toolbar(\n            i * 1.0 / len(sample_label),\n            \'({:>5}/{:<5}) Processing {:>5}-{:<5} data: \'.format(\n                i + 1, len(sample_name), benchmark, part))\n        data = read_xyz(os.path.join(data_path, s),\n                        max_body=max_body,\n                        num_joint=num_joint)\n        fp[i, :, 0:data.shape[1], :, :] = data\n    end_toolbar()\n\n\nif __name__ == \'__main__\':\n\n    parser = argparse.ArgumentParser(description=\'NTU-RGB-D Data Converter.\')\n    parser.add_argument(\'--data_path\',\n                        default=\'data/NTU-RGB-D/nturgb+d_skeletons\')\n    parser.add_argument(\n        \'--ignored_sample_path\',\n        default=\'resource/NTU-RGB-D/samples_with_missing_skeletons.txt\')\n    parser.add_argument(\'--out_folder\', default=\'data/NTU-RGB-D\')\n\n    benchmark = [\'xsub\', \'xview\']\n    part = [\'train\', \'val\']\n    arg = parser.parse_args()\n\n    for b in benchmark:\n        for p in part:\n            out_path = os.path.join(arg.out_folder, b)\n            if not os.path.exists(out_path):\n                os.makedirs(out_path)\n            gendata(arg.data_path,\n                    out_path,\n                    arg.ignored_sample_path,\n                    benchmark=b,\n                    part=p)\n'"
deprecated/origin_stgcn_repo/torchlight/setup.py,0,"b""from setuptools import find_packages, setup\n\nsetup(name='torchlight',\n      version='1.0',\n      description='A mini framework for pytorch',\n      packages=find_packages(),\n      install_requires=[])\n"""
deprecated/tools/data_processing/kinetics_gendata.py,0,"b'import os\nimport sys\nimport pickle\nimport argparse\n\nimport numpy as np\nfrom numpy.lib.format import open_memmap\n\nsys.path.append(\n    os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir)))\nfrom mmskeleton.deprecated.datasets.kinetics_feeder import KineticsFeeder\ntoolbar_width = 30\n\n\ndef print_toolbar(rate, annotation=\'\'):\n    # setup toolbar\n    sys.stdout.write(""{}["".format(annotation))\n    for i in range(toolbar_width):\n        if i * 1.0 / toolbar_width > rate:\n            sys.stdout.write(\' \')\n        else:\n            sys.stdout.write(\'-\')\n        sys.stdout.flush()\n    sys.stdout.write(\']\\r\')\n\n\ndef end_toolbar():\n    sys.stdout.write(""\\n"")\n\n\ndef gendata(\n        data_path,\n        label_path,\n        data_out_path,\n        label_out_path,\n        num_person_in=5,  #observe the first 5 persons \n        num_person_out=2,  #then choose 2 persons with the highest score \n        max_frame=300):\n\n    feeder = KineticsFeeder(data_path=data_path,\n                            label_path=label_path,\n                            num_person_in=num_person_in,\n                            num_person_out=num_person_out,\n                            window_size=max_frame)\n\n    sample_name = feeder.sample_name\n    sample_label = []\n\n    fp = open_memmap(data_out_path,\n                     dtype=\'float32\',\n                     mode=\'w+\',\n                     shape=(len(sample_name), 3, max_frame, 18,\n                            num_person_out))\n\n    for i, s in enumerate(sample_name):\n        data, label = feeder[i]\n        print_toolbar(\n            i * 1.0 / len(sample_name),\n            \'({:>5}/{:<5}) Processing data: \'.format(i + 1, len(sample_name)))\n        fp[i, :, 0:data.shape[1], :, :] = data\n        sample_label.append(label)\n\n    with open(label_out_path, \'wb\') as f:\n        pickle.dump((sample_name, list(sample_label)), f)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(\n        description=\'Kinetics-skeleton Data Converter.\')\n    parser.add_argument(\'--data_path\',\n                        default=\'data/Kinetics/kinetics-skeleton\')\n    parser.add_argument(\'--out_folder\',\n                        default=\'data/Kinetics/kinetics-skeleton\')\n    arg = parser.parse_args()\n\n    part = [\'train\', \'val\']\n    for p in part:\n        data_path = \'{}/kinetics_{}\'.format(arg.data_path, p)\n        label_path = \'{}/kinetics_{}_label.json\'.format(arg.data_path, p)\n        data_out_path = \'{}/{}_data.npy\'.format(arg.out_folder, p)\n        label_out_path = \'{}/{}_label.pkl\'.format(arg.out_folder, p)\n\n        if not os.path.exists(arg.out_folder):\n            os.makedirs(arg.out_folder)\n        gendata(data_path, label_path, data_out_path, label_out_path)'"
deprecated/tools/data_processing/ntu_gendata.py,0,"b'import os\nimport sys\nimport pickle\n\nimport argparse\nimport numpy as np\nfrom numpy.lib.format import open_memmap\n\ntraining_subjects = [\n    1, 2, 4, 5, 8, 9, 13, 14, 15, 16, 17, 18, 19, 25, 27, 28, 31, 34, 35, 38\n]\ntraining_cameras = [2, 3]\nmax_body = 2\nnum_joint = 25\nmax_frame = 300\ntoolbar_width = 30\n\n\ndef read_skeleton(file):\n    with open(file, \'r\') as f:\n        skeleton_sequence = {}\n        skeleton_sequence[\'numFrame\'] = int(f.readline())\n        skeleton_sequence[\'frameInfo\'] = []\n        for t in range(skeleton_sequence[\'numFrame\']):\n            frame_info = {}\n            frame_info[\'numBody\'] = int(f.readline())\n            frame_info[\'bodyInfo\'] = []\n            for m in range(frame_info[\'numBody\']):\n                body_info = {}\n                body_info_key = [\n                    \'bodyID\', \'clipedEdges\', \'handLeftConfidence\',\n                    \'handLeftState\', \'handRightConfidence\', \'handRightState\',\n                    \'isResticted\', \'leanX\', \'leanY\', \'trackingState\'\n                ]\n                body_info = {\n                    k: float(v)\n                    for k, v in zip(body_info_key,\n                                    f.readline().split())\n                }\n                body_info[\'numJoint\'] = int(f.readline())\n                body_info[\'jointInfo\'] = []\n                for v in range(body_info[\'numJoint\']):\n                    joint_info_key = [\n                        \'x\', \'y\', \'z\', \'depthX\', \'depthY\', \'colorX\', \'colorY\',\n                        \'orientationW\', \'orientationX\', \'orientationY\',\n                        \'orientationZ\', \'trackingState\'\n                    ]\n                    joint_info = {\n                        k: float(v)\n                        for k, v in zip(joint_info_key,\n                                        f.readline().split())\n                    }\n                    body_info[\'jointInfo\'].append(joint_info)\n                frame_info[\'bodyInfo\'].append(body_info)\n            skeleton_sequence[\'frameInfo\'].append(frame_info)\n    return skeleton_sequence\n\n\ndef read_xyz(file, max_body=2, num_joint=25):\n    seq_info = read_skeleton(file)\n    data = np.zeros((3, seq_info[\'numFrame\'], num_joint, max_body))\n    for n, f in enumerate(seq_info[\'frameInfo\']):\n        for m, b in enumerate(f[\'bodyInfo\']):\n            for j, v in enumerate(b[\'jointInfo\']):\n                if m < max_body and j < num_joint:\n                    data[:, n, j, m] = [v[\'x\'], v[\'y\'], v[\'z\']]\n                else:\n                    pass\n    return data\n\n\ndef print_toolbar(rate, annotation=\'\'):\n    # setup toolbar\n    sys.stdout.write(""{}["".format(annotation))\n    for i in range(toolbar_width):\n        if i * 1.0 / toolbar_width > rate:\n            sys.stdout.write(\' \')\n        else:\n            sys.stdout.write(\'-\')\n        sys.stdout.flush()\n    sys.stdout.write(\']\\r\')\n\n\ndef end_toolbar():\n    sys.stdout.write(""\\n"")\n\n\ndef gendata(data_path,\n            out_path,\n            ignored_sample_path=None,\n            benchmark=\'xview\',\n            part=\'eval\'):\n    if ignored_sample_path != None:\n        with open(ignored_sample_path, \'r\') as f:\n            ignored_samples = [\n                line.strip() + \'.skeleton\' for line in f.readlines()\n            ]\n    else:\n        ignored_samples = []\n    sample_name = []\n    sample_label = []\n    for filename in os.listdir(data_path):\n        if filename in ignored_samples:\n            continue\n        action_class = int(filename[filename.find(\'A\') + 1:filename.find(\'A\') +\n                                    4])\n        subject_id = int(filename[filename.find(\'P\') + 1:filename.find(\'P\') +\n                                  4])\n        camera_id = int(filename[filename.find(\'C\') + 1:filename.find(\'C\') +\n                                 4])\n\n        if benchmark == \'xview\':\n            istraining = (camera_id in training_cameras)\n        elif benchmark == \'xsub\':\n            istraining = (subject_id in training_subjects)\n        else:\n            raise ValueError()\n\n        if part == \'train\':\n            issample = istraining\n        elif part == \'val\':\n            issample = not (istraining)\n        else:\n            raise ValueError()\n\n        if issample:\n            sample_name.append(filename)\n            sample_label.append(action_class - 1)\n\n    with open(\'{}/{}_label.pkl\'.format(out_path, part), \'wb\') as f:\n        pickle.dump((sample_name, list(sample_label)), f)\n    # np.save(\'{}/{}_label.npy\'.format(out_path, part), sample_label)\n\n    fp = open_memmap(\'{}/{}_data.npy\'.format(out_path, part),\n                     dtype=\'float32\',\n                     mode=\'w+\',\n                     shape=(len(sample_label), 3, max_frame, num_joint,\n                            max_body))\n\n    for i, s in enumerate(sample_name):\n        print_toolbar(\n            i * 1.0 / len(sample_label),\n            \'({:>5}/{:<5}) Processing {:>5}-{:<5} data: \'.format(\n                i + 1, len(sample_name), benchmark, part))\n        data = read_xyz(os.path.join(data_path, s),\n                        max_body=max_body,\n                        num_joint=num_joint)\n        fp[i, :, 0:data.shape[1], :, :] = data\n    end_toolbar()\n\n\nif __name__ == \'__main__\':\n\n    parser = argparse.ArgumentParser(description=\'NTU-RGB-D Data Converter.\')\n    parser.add_argument(\'--data_path\',\n                        default=\'data/NTU-RGB-D/nturgb+d_skeletons\')\n    parser.add_argument(\n        \'--ignored_sample_path\',\n        default=\n        \'deprecated/tools/data_processing/nturgbd_samples_with_missing_skeletons.txt\'\n    )\n    parser.add_argument(\'--out_folder\', default=\'data/NTU-RGB-D\')\n\n    benchmark = [\'xsub\', \'xview\']\n    part = [\'train\', \'val\']\n    arg = parser.parse_args()\n\n    for b in benchmark:\n        for p in part:\n            out_path = os.path.join(arg.out_folder, b)\n            if not os.path.exists(out_path):\n                os.makedirs(out_path)\n            gendata(arg.data_path,\n                    out_path,\n                    arg.ignored_sample_path,\n                    benchmark=b,\n                    part=p)\n'"
mmskeleton/datasets/skeleton/__init__.py,0,b'from .loader import SkeletonLoader\nfrom .skeleton_process import *'
mmskeleton/datasets/skeleton/loader.py,1,"b'import os\nimport numpy as np\nimport json\nimport torch\n\n\nclass SkeletonLoader(torch.utils.data.Dataset):\n    """""" Feeder for skeleton-based action recognition\n    Arguments:\n        data_path: the path to data folder\n        num_track: number of skeleton output\n        pad_value: the values for padding missed joint\n        repeat: times of repeating the dataset\n    """"""\n    def __init__(self, data_dir, num_track=1, repeat=1, num_keypoints=-1):\n\n        self.data_dir = data_dir\n        self.num_track = num_track\n        self.num_keypoints = num_keypoints\n        self.files = [\n            os.path.join(self.data_dir, f) for f in os.listdir(self.data_dir)\n        ] * repeat\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, index):\n\n        with open(self.files[index]) as f:\n            data = json.load(f)\n\n        info = data[\'info\']\n        annotations = data[\'annotations\']\n        num_frame = info[\'num_frame\']\n        num_keypoints = info[\n            \'num_keypoints\'] if self.num_keypoints <= 0 else self.num_keypoints\n        channel = info[\'keypoint_channels\']\n        num_channel = len(channel)\n\n        # get data\n        data[\'data\'] = np.zeros(\n            (num_channel, num_keypoints, num_frame, self.num_track),\n            dtype=np.float32)\n\n        for a in annotations:\n            person_id = a[\'id\'] if a[\'person_id\'] is None else a[\'person_id\']\n            frame_index = a[\'frame_index\']\n            if person_id < self.num_track and frame_index < num_frame:\n                data[\'data\'][:, :, frame_index, person_id] = np.array(\n                    a[\'keypoints\']).transpose()\n\n        return data\n'"
mmskeleton/datasets/skeleton/skeleton_process.py,0,"b'import random\nimport numpy as np\nfrom mmskeleton.deprecated.datasets.utils import skeleton as skeleton_aaai18\n\n# def stgcn_aaai18_dataprocess(data,\n#                              window_size,\n#                              random_choose=False,\n#                              random_move=False):\n#     data = normalize_by_resolution(data)\n#     data = mask_by_visibility(data)\n#     # processing\n#     if random_choose:\n#         data[\'data\'] = skeleton_aaai18.random_choose(data[\'data\'], window_size)\n#     elif window_size > 0:\n#         data[\'data\'] = skeleton_aaai18.auto_pading(data[\'data\'], window_size)\n#     if random_move:\n#         data[\'data\'] = skeleton_aaai18.random_move(data[\'data\'])\n#     data = transpose(data, order=[0, 2, 1, 3])\n#     data = to_tuple(data)\n#     return data\n\n\ndef normalize_by_resolution(data):\n\n    resolution = data[\'info\'][\'resolution\']\n    channel = data[\'info\'][\'keypoint_channels\']\n    np_array = data[\'data\']\n\n    for i, c in enumerate(channel):\n        if c == \'x\':\n            np_array[i] = np_array[i] / resolution[0] - 0.5\n        if c == \'y\':\n            np_array[i] = np_array[i] / resolution[1] - 0.5\n\n    data[\'data\'] = np_array\n    return data\n\n\ndef get_mask(data, mask_channel, mask_threshold=0):\n    data[\'mask\'] = data[\'data\'][[mask_channel]] > mask_threshold\n    return data\n\n\ndef mask(data):\n    data[\'data\'] = data[\'data\'] * data[\'mask\']\n    return data\n\n\ndef normalize(data, mean, std):\n    np_array = data[\'data\']\n    mean = np.array(mean, dtype=np_array.dtype)\n    std = np.array(std, dtype=np_array.dtype)\n    mean = mean.reshape(mean.shape + (1, ) * (np_array.ndim - mean.ndim))\n    std = std.reshape(std.shape + (1, ) * (np_array.ndim - std.ndim))\n    data[\'data\'] = (np_array - mean) / std\n    return data\n\n\ndef normalize_with_mask(data, mean, std, mask_channel, mask_threshold=0):\n    data = get_mask(data, mask_channel, mask_threshold)\n    data = normalize(data, mean, std)\n    data = mask(data)\n    return data\n\n\ndef mask_by_visibility(data):\n\n    channel = data[\'info\'][\'keypoint_channels\']\n    np_array = data[\'data\']\n\n    for i, c in enumerate(channel):\n        if c == \'score\' or c == \'visibility\':\n            mask = (np_array[i] == 0)\n            for j in range(len(channel)):\n                if c != j:\n                    np_array[j][mask] = 0\n\n    data[\'data\'] = np_array\n    return data\n\n\ndef transpose(data, order, key=\'data\'):\n    data[key] = data[key].transpose(order)\n    return data\n\n\ndef to_tuple(data, keys=[\'data\', \'category_id\']):\n    return tuple([data[k] for k in keys])\n\n\ndef temporal_repeat(data, size, random_crop=False):\n    """"""\n    repeat on the time axis.\n    """"""\n\n    np_array = data[\'data\']\n    T = np_array.shape[2]\n\n    if T >= size:\n        if random_crop:\n            np_array = np_array[:, :, random.randint(0, T -\n                                                     size):][:, :, :size]\n        else:\n            np_array = np_array[:, :, :size]\n\n    else:\n        selected_index = np.arange(T)\n        selected_index = np.concatenate(\n            (selected_index, selected_index[1:-1][::-1]))\n        selected_index = np.tile(selected_index,\n                                 size // (2 * T - 2) + 1)[:size]\n\n        np_array = np_array[:, :, selected_index]\n\n    data[\'data\'] = np_array\n    return data\n\n\ndef pad_zero(data, size):\n    np_array = data[\'data\']\n    T = np_array.shape[2]\n    if T < size:\n        pad_shape = list(np_array.shape)\n        pad_shape[2] = size\n        np_array_paded = np.zeros(pad_shape, dtype=np_array.dtype)\n        np_array_paded[:, :, :T, :] = np_array\n        data[\'data\'] = np_array_paded\n    return data\n\n\ndef random_crop(data, size):\n    np_array = data[\'data\']\n    T = np_array.shape[2]\n    if T > size:\n        begin = random.randint(0, T - size)\n        data[\'data\'] = np_array[:, :, begin:begin + size, :]\n    return data\n\n\ndef simulate_camera_moving(data,\n                           angle_candidate=[-10., -5., 0., 5., 10.],\n                           scale_candidate=[0.9, 1.0, 1.1],\n                           transform_candidate=[-0.2, -0.1, 0.0, 0.1, 0.2],\n                           move_time_candidate=[1]):\n\n    channel = data[\'info\'][\'keypoint_channels\']\n    if channel[0] != \'x\' or channel[1] != \'y\':\n        raise NotImplementedError(\n            \'The first two channels of keypoints should be [""x"", ""y""]\')\n\n    np_array = data[\'data\']\n    T = np_array.shape[2]\n\n    move_time = random.choice(move_time_candidate)\n    node = np.arange(0, T, T * 1.0 / move_time).round().astype(int)\n    node = np.append(node, T)\n    num_node = len(node)\n\n    A = np.random.choice(angle_candidate, num_node)\n    S = np.random.choice(scale_candidate, num_node)\n    T_x = np.random.choice(transform_candidate, num_node)\n    T_y = np.random.choice(transform_candidate, num_node)\n\n    a = np.zeros(T)\n    s = np.zeros(T)\n    t_x = np.zeros(T)\n    t_y = np.zeros(T)\n\n    # linspace for parameters of affine transformation\n    for i in range(num_node - 1):\n        a[node[i]:node[i + 1]] = np.linspace(\n            A[i], A[i + 1], node[i + 1] - node[i]) * np.pi / 180\n        s[node[i]:node[i + 1]] = np.linspace(S[i], S[i + 1],\n                                             node[i + 1] - node[i])\n        t_x[node[i]:node[i + 1]] = np.linspace(T_x[i], T_x[i + 1],\n                                               node[i + 1] - node[i])\n        t_y[node[i]:node[i + 1]] = np.linspace(T_y[i], T_y[i + 1],\n                                               node[i + 1] - node[i])\n\n    theta = np.array([[np.cos(a) * s, -np.sin(a) * s],\n                      [np.sin(a) * s, np.cos(a) * s]])\n\n    # perform transformation\n    for i_frame in range(T):\n        xy = np_array[0:2, :, i_frame]\n        new_xy = np.dot(theta[:, :, i_frame], xy.reshape(2, -1))\n        new_xy[0] += t_x[i_frame]\n        new_xy[1] += t_y[i_frame]\n        np_array[0:2, :, i_frame] = new_xy.reshape(*(\n            np_array[0:2, :, i_frame].shape))\n\n    data[\'data\'] = np_array\n    return data\n'"
mmskeleton/datasets/utils/__init__.py,0,"b'from . import skeleton\r\nfrom . import coco_transform\r\nfrom . import zipreader\r\n\r\n__all__ = [skeleton, coco_transform, zipreader]'"
mmskeleton/datasets/utils/coco_transform.py,0,"b'# ------------------------------------------------------------------------------\r\n# Copyright (c) Microsoft\r\n# Licensed under the MIT License.\r\n# Written by Bin Xiao (Bin.Xiao@microsoft.com)\r\n# ------------------------------------------------------------------------------\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport numpy as np\r\nimport cv2\r\n\r\n\r\ndef flip_back(output_flipped, matched_parts):\r\n    \'\'\'\r\n    ouput_flipped: numpy.ndarray(batch_size, num_joints, height, width)\r\n    \'\'\'\r\n    assert output_flipped.ndim == 4,\\\r\n        \'output_flipped should be [batch_size, num_joints, height, width]\'\r\n\r\n    output_flipped = output_flipped[:, :, :, ::-1]\r\n\r\n    for pair in matched_parts:\r\n        tmp = output_flipped[:, pair[0], :, :].copy()\r\n        output_flipped[:, pair[0], :, :] = output_flipped[:, pair[1], :, :]\r\n        output_flipped[:, pair[1], :, :] = tmp\r\n\r\n    return output_flipped\r\n\r\n\r\ndef fliplr_joints(joints, joints_vis, width, matched_parts):\r\n    """"""\r\n    flip coords\r\n    """"""\r\n    # Flip horizontal\r\n    joints[:, 0] = width - joints[:, 0] - 1\r\n\r\n    # Change left-right parts\r\n    for pair in matched_parts:\r\n        joints[pair[0], :], joints[pair[1], :] = \\\r\n            joints[pair[1], :], joints[pair[0], :].copy()\r\n        joints_vis[pair[0], :], joints_vis[pair[1], :] = \\\r\n            joints_vis[pair[1], :], joints_vis[pair[0], :].copy()\r\n\r\n    return joints * joints_vis, joints_vis\r\n\r\n\r\ndef transform_preds(coords, center, scale, output_size):\r\n    target_coords = np.zeros(coords.shape)\r\n    trans = get_affine_transform(center, scale, 0, output_size, inv=1)\r\n    for p in range(coords.shape[0]):\r\n        target_coords[p, 0:2] = affine_transform(coords[p, 0:2], trans)\r\n    return target_coords\r\n\r\n\r\ndef get_affine_transform(center,\r\n                         scale,\r\n                         rot,\r\n                         output_size,\r\n                         shift=np.array([0, 0], dtype=np.float32),\r\n                         inv=0):\r\n    if not isinstance(scale, np.ndarray) and not isinstance(scale, list):\r\n        scale = np.array([scale, scale])\r\n\r\n    scale_tmp = scale * 200.0\r\n    src_w = scale_tmp[0]\r\n    dst_w = output_size[0]\r\n    dst_h = output_size[1]\r\n\r\n    rot_rad = np.pi * rot / 180\r\n    src_dir = get_dir([0, src_w * -0.5], rot_rad)\r\n    dst_dir = np.array([0, dst_w * -0.5], np.float32)\r\n\r\n    src = np.zeros((3, 2), dtype=np.float32)\r\n    dst = np.zeros((3, 2), dtype=np.float32)\r\n    src[0, :] = center + scale_tmp * shift\r\n    src[1, :] = center + src_dir + scale_tmp * shift\r\n    dst[0, :] = [dst_w * 0.5, dst_h * 0.5]\r\n    dst[1, :] = np.array([dst_w * 0.5, dst_h * 0.5]) + dst_dir\r\n\r\n    src[2:, :] = get_3rd_point(src[0, :], src[1, :])\r\n    dst[2:, :] = get_3rd_point(dst[0, :], dst[1, :])\r\n\r\n    if inv:\r\n        trans = cv2.getAffineTransform(np.float32(dst), np.float32(src))\r\n    else:\r\n        trans = cv2.getAffineTransform(np.float32(src), np.float32(dst))\r\n\r\n    return trans\r\n\r\n\r\ndef affine_transform(pt, t):\r\n    new_pt = np.array([pt[0], pt[1], 1.]).T\r\n    new_pt = np.dot(t, new_pt)\r\n    return new_pt[:2]\r\n\r\n\r\ndef get_3rd_point(a, b):\r\n    direct = a - b\r\n    return b + np.array([-direct[1], direct[0]], dtype=np.float32)\r\n\r\n\r\ndef get_dir(src_point, rot_rad):\r\n    sn, cs = np.sin(rot_rad), np.cos(rot_rad)\r\n\r\n    src_result = [0, 0]\r\n    src_result[0] = src_point[0] * cs - src_point[1] * sn\r\n    src_result[1] = src_point[0] * sn + src_point[1] * cs\r\n\r\n    return src_result\r\n\r\n\r\ndef crop(img, center, scale, output_size, rot=0):\r\n    trans = get_affine_transform(center, scale, rot, output_size)\r\n\r\n    dst_img = cv2.warpAffine(img,\r\n                             trans, (int(output_size[0]), int(output_size[1])),\r\n                             flags=cv2.INTER_LINEAR)\r\n\r\n    return dst_img\r\n\r\n\r\ndef xywh2cs(x, y, h, w, aspect_ratio, pixel_std):\r\n    center = np.zeros((2), dtype=np.float32)\r\n    center[0] = x + w * 0.5\r\n    center[1] = y + h * 0.5\r\n    if w > aspect_ratio * h:\r\n        h = w * 1.0 / aspect_ratio\r\n    elif w < aspect_ratio * h:\r\n        w = h * aspect_ratio\r\n    scale = np.array([w * 1.0 / pixel_std, h * 1.0 / pixel_std],\r\n                     dtype=np.float32)\r\n    if center[0] != -1:\r\n        scale = scale * 1.25\r\n    return center, scale\r\n'"
mmskeleton/datasets/utils/skeleton.py,0,"b'import numpy as np\nimport random\n\n\ndef downsample(data_numpy, step, random_sample=True):\n    # input: C,T,V,M\n    begin = np.random.randint(step) if random_sample else 0\n    return data_numpy[:, begin::step, :, :]\n\n\ndef temporal_slice(data_numpy, step):\n    # input: C,T,V,M\n    C, T, V, M = data_numpy.shape\n    return data_numpy.reshape(C, T / step, step, V, M).transpose(\n        (0, 1, 3, 2, 4)).reshape(C, T / step, V, step * M)\n\n\ndef mean_subtractor(data_numpy, mean):\n    # input: C,T,V,M\n    # naive version\n    if mean == 0:\n        return\n    C, T, V, M = data_numpy.shape\n    valid_frame = (data_numpy != 0).sum(axis=3).sum(axis=2).sum(axis=0) > 0\n    begin = valid_frame.argmax()\n    end = len(valid_frame) - valid_frame[::-1].argmax()\n    data_numpy[:, :end, :, :] = data_numpy[:, :end, :, :] - mean\n    return data_numpy\n\n\ndef auto_pading(data_numpy, size, random_pad=False):\n    C, T, V, M = data_numpy.shape\n    if T < size:\n        begin = random.randint(0, size - T) if random_pad else 0\n        data_numpy_paded = np.zeros((C, size, V, M), dtype=data_numpy.dtype)\n        data_numpy_paded[:, begin:begin + T, :, :] = data_numpy\n        return data_numpy_paded\n    else:\n        return data_numpy\n\n\ndef random_choose(data_numpy, size, auto_pad=True):\n    # input: C,T,V,M\n    C, T, V, M = data_numpy.shape\n    if T == size:\n        return data_numpy\n    elif T < size:\n        if auto_pad:\n            return auto_pading(data_numpy, size, random_pad=True)\n        else:\n            return data_numpy\n    else:\n        begin = random.randint(0, T - size)\n        return data_numpy[:, begin:begin + size, :, :]\n\n\ndef random_move(data_numpy,\n                angle_candidate=[-10., -5., 0., 5., 10.],\n                scale_candidate=[0.9, 1.0, 1.1],\n                transform_candidate=[-0.2, -0.1, 0.0, 0.1, 0.2],\n                move_time_candidate=[1]):\n    # input: C,T,V,M\n    C, T, V, M = data_numpy.shape\n    move_time = random.choice(move_time_candidate)\n    node = np.arange(0, T, T * 1.0 / move_time).round().astype(int)\n    node = np.append(node, T)\n    num_node = len(node)\n\n    A = np.random.choice(angle_candidate, num_node)\n    S = np.random.choice(scale_candidate, num_node)\n    T_x = np.random.choice(transform_candidate, num_node)\n    T_y = np.random.choice(transform_candidate, num_node)\n\n    a = np.zeros(T)\n    s = np.zeros(T)\n    t_x = np.zeros(T)\n    t_y = np.zeros(T)\n\n    # linspace\n    for i in range(num_node - 1):\n        a[node[i]:node[i + 1]] = np.linspace(\n            A[i], A[i + 1], node[i + 1] - node[i]) * np.pi / 180\n        s[node[i]:node[i + 1]] = np.linspace(S[i], S[i + 1],\n                                             node[i + 1] - node[i])\n        t_x[node[i]:node[i + 1]] = np.linspace(T_x[i], T_x[i + 1],\n                                               node[i + 1] - node[i])\n        t_y[node[i]:node[i + 1]] = np.linspace(T_y[i], T_y[i + 1],\n                                               node[i + 1] - node[i])\n\n    theta = np.array([[np.cos(a) * s, -np.sin(a) * s],\n                      [np.sin(a) * s, np.cos(a) * s]])\n\n    # perform transformation\n    for i_frame in range(T):\n        xy = data_numpy[0:2, i_frame, :, :]\n        new_xy = np.dot(theta[:, :, i_frame], xy.reshape(2, -1))\n        new_xy[0] += t_x[i_frame]\n        new_xy[1] += t_y[i_frame]\n        data_numpy[0:2, i_frame, :, :] = new_xy.reshape(2, V, M)\n\n    return data_numpy\n\n\ndef random_shift(data_numpy):\n    # input: C,T,V,M\n    C, T, V, M = data_numpy.shape\n    data_shift = np.zeros(data_numpy.shape)\n    valid_frame = (data_numpy != 0).sum(axis=3).sum(axis=2).sum(axis=0) > 0\n    begin = valid_frame.argmax()\n    end = len(valid_frame) - valid_frame[::-1].argmax()\n\n    size = end - begin\n    bias = random.randint(0, T - size)\n    data_shift[:, bias:bias + size, :, :] = data_numpy[:, begin:end, :, :]\n\n    return data_shift\n\n\ndef openpose_match(data_numpy):\n    C, T, V, M = data_numpy.shape\n    assert (C == 3)\n    score = data_numpy[2, :, :, :].sum(axis=1)\n    # the rank of body confidence in each frame (shape: T-1, M)\n    rank = (-score[0:T - 1]).argsort(axis=1).reshape(T - 1, M)\n\n    # data of frame 1\n    xy1 = data_numpy[0:2, 0:T - 1, :, :].reshape(2, T - 1, V, M, 1)\n    # data of frame 2\n    xy2 = data_numpy[0:2, 1:T, :, :].reshape(2, T - 1, V, 1, M)\n    # square of distance between frame 1&2 (shape: T-1, M, M)\n    distance = ((xy2 - xy1)**2).sum(axis=2).sum(axis=0)\n\n    # match pose\n    forward_map = np.zeros((T, M), dtype=int) - 1\n    forward_map[0] = range(M)\n    for m in range(M):\n        choose = (rank == m)\n        forward = distance[choose].argmin(axis=1)\n        for t in range(T - 1):\n            distance[t, :, forward[t]] = np.inf\n        forward_map[1:][choose] = forward\n    assert (np.all(forward_map >= 0))\n\n    # string data\n    for t in range(T - 1):\n        forward_map[t + 1] = forward_map[t + 1][forward_map[t]]\n\n    # generate data\n    new_data_numpy = np.zeros(data_numpy.shape)\n    for t in range(T):\n        new_data_numpy[:, t, :, :] = data_numpy[:, t, :,\n                                                forward_map[t]].transpose(\n                                                    1, 2, 0)\n    data_numpy = new_data_numpy\n\n    # score sort\n    trace_score = data_numpy[2, :, :, :].sum(axis=1).sum(axis=0)\n    rank = (-trace_score).argsort()\n    data_numpy = data_numpy[:, :, :, rank]\n\n    return data_numpy\n\n\ndef top_k_by_category(label, score, top_k):\n    instance_num, class_num = score.shape\n    rank = score.argsort()\n    hit_top_k = [[] for i in range(class_num)]\n    for i in range(instance_num):\n        l = label[i]\n        hit_top_k[l].append(l in rank[i, -top_k:])\n\n    accuracy_list = []\n    for hit_per_category in hit_top_k:\n        if hit_per_category:\n            accuracy_list.append(\n                sum(hit_per_category) * 1.0 / len(hit_per_category))\n        else:\n            accuracy_list.append(0.0)\n    return accuracy_list\n\n\ndef calculate_recall_precision(label, score):\n    instance_num, class_num = score.shape\n    rank = score.argsort()\n    confusion_matrix = np.zeros([class_num, class_num])\n\n    for i in range(instance_num):\n        true_l = label[i]\n        pred_l = rank[i, -1]\n        confusion_matrix[true_l][pred_l] += 1\n\n    precision = []\n    recall = []\n\n    for i in range(class_num):\n        true_p = confusion_matrix[i][i]\n        false_n = sum(confusion_matrix[i, :]) - true_p\n        false_p = sum(confusion_matrix[:, i]) - true_p\n        precision.append(true_p * 1.0 / (true_p + false_p))\n        recall.append(true_p * 1.0 / (true_p + false_n))\n\n    return precision, recall'"
mmskeleton/datasets/utils/video_demo.py,2,"b""import cv2\r\nimport torch\r\nimport mmcv\r\nimport numpy as np\r\nfrom .coco_transform import xywh2cs, get_affine_transform\r\nfrom mmskeleton.ops.nms.nms import oks_nms\r\n\r\n\r\nclass VideoDemo(object):\r\n    def __init__(self, ):\r\n        super(VideoDemo, self).__init__()\r\n\r\n    @staticmethod\r\n    def bbox_filter(bbox_result, bbox_thre=0.0):\r\n        # clone from mmdetection\r\n\r\n        if isinstance(bbox_result, tuple):\r\n            bbox_result, segm_result = bbox_result\r\n        else:\r\n            bbox_result, segm_result = bbox_result, None\r\n\r\n        bboxes = np.vstack(bbox_result)\r\n        bbox_labels = [\r\n            np.full(bbox.shape[0], i, dtype=np.int32)\r\n            for i, bbox in enumerate(bbox_result)\r\n        ]\r\n        labels = np.concatenate(bbox_labels)\r\n        # get bboxes for each person\r\n        person_id = 0\r\n        person_bboxes = bboxes[labels == person_id]\r\n        person_mask = person_bboxes[:, 4] >= bbox_thre\r\n        person_bboxes = person_bboxes[person_mask]\r\n        return person_bboxes, labels[labels == person_id][person_mask]\r\n\r\n    @staticmethod\r\n    def skeleton_preprocess(image, bboxes, skeleton_cfg):\r\n\r\n        # output collector\r\n        result_list = []\r\n        meta = dict()\r\n        meta['scale'] = []\r\n        meta['rotation'] = []\r\n        meta['center'] = []\r\n        meta['score'] = []\r\n\r\n        # preprocess config\r\n        image_size = skeleton_cfg.image_size\r\n        image_width = image_size[0]\r\n        image_height = image_size[1]\r\n        aspect_ratio = image_width * 1.0 / image_height\r\n        pixel_std = skeleton_cfg.pixel_std\r\n        image_mean = skeleton_cfg.image_mean\r\n        image_std = skeleton_cfg.image_std\r\n\r\n        for idx, bbox in enumerate(bboxes):\r\n            x1, y1, x2, y2 = bbox[:4]\r\n            w, h = x2 - x1, y2 - y1\r\n            center, scale = xywh2cs(x1, y1, h, w, aspect_ratio, pixel_std)\r\n            trans = get_affine_transform(center, scale, 0, image_size)\r\n            transformed_image = cv2.warpAffine(\r\n                image,\r\n                trans, (int(image_size[0]), int(image_size[1])),\r\n                flags=cv2.INTER_LINEAR)\r\n            # transfer into Torch.Tensor\r\n            transformed_image = transformed_image / 255.0\r\n            transformed_image = transformed_image - image_mean\r\n            transformed_image = transformed_image / image_std\r\n            transformed_image = transformed_image.transpose(2, 0, 1)\r\n            result_list.append(transformed_image)\r\n            # from IPython import embed; embed()\r\n            meta['scale'].append(scale)\r\n            meta['rotation'].append(0)\r\n            meta['center'].append(center)\r\n            meta['score'].append(bbox[4])\r\n\r\n        result = torch.from_numpy(np.array(result_list)).float()\r\n        for name, data in meta.items():\r\n            meta[name] = torch.from_numpy(np.array(data)).float()\r\n        return result, meta\r\n\r\n    @staticmethod\r\n    def skeleton_postprocess(\r\n            preds,\r\n            max_vals,\r\n            meta,\r\n    ):\r\n\r\n        all_preds = np.concatenate((preds, max_vals), axis=-1)\r\n        _kpts = []\r\n        for idx, kpt in enumerate(all_preds):\r\n            center = meta['center'][idx].numpy()\r\n            scale = meta['scale'][idx].numpy()\r\n            area = np.prod(scale * 200, 0)\r\n            score = meta['score'][idx].numpy()\r\n            _kpts.append({\r\n                'keypoints': kpt,\r\n                'center': center,\r\n                'scale': scale,\r\n                'area': area,\r\n                'score': score,\r\n            })\r\n        num_joints = 17\r\n        in_vis_thre = 0.2\r\n        oks_thre = 0.9\r\n        oks_nmsed_kpts = []\r\n        for n_p in _kpts:\r\n            box_score = n_p['score']\r\n            kpt_score = 0\r\n            valid_num = 0\r\n            for n_jt in range(0, num_joints):\r\n                t_s = n_p['keypoints'][n_jt][2]\r\n                if t_s > in_vis_thre:\r\n                    kpt_score = kpt_score + t_s\r\n                    valid_num = valid_num + 1\r\n            if valid_num != 0:\r\n                kpt_score = kpt_score / valid_num\r\n            # rescoring\r\n            n_p['score'] = kpt_score * box_score\r\n\r\n            keep = oks_nms([_kpts[i] for i in range(len(_kpts))], oks_thre)\r\n\r\n        if len(keep) == 0:\r\n            oks_nmsed_kpts.append(_kpts['keypoints'])\r\n        else:\r\n            oks_nmsed_kpts.append(\r\n                [_kpts[_keep]['keypoints'] for _keep in keep])\r\n\r\n        return np.array(oks_nmsed_kpts[0])\r\n"""
mmskeleton/datasets/utils/zipreader.py,0,"b'# ------------------------------------------------------------------------------\r\n# Copyright (c) Microsoft\r\n# Licensed under the MIT License.\r\n# Written by Bin Xiao (Bin.Xiao@microsoft.com)\r\n# ------------------------------------------------------------------------------\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport os\r\nimport zipfile\r\nimport xml.etree.ElementTree as ET\r\n\r\nimport cv2\r\nimport numpy as np\r\n\r\ntry:\r\n    xrange # Python 2\r\nexcept NameError:\r\n    xrange = range  #\xc2\xa0Python 3\r\n\r\n_im_zfile = []\r\n_xml_path_zip = []\r\n_xml_zfile = []\r\n\r\n\r\ndef imread(filename, flags=cv2.IMREAD_COLOR):\r\n    global _im_zfile\r\n    path = filename\r\n    pos_at = path.index(\'@\')\r\n    if pos_at == -1:\r\n        print(""character \'@\' is not found from the given path \'%s\'""%(path))\r\n        assert 0\r\n    path_zip = path[0: pos_at]\r\n    path_img = path[pos_at + 2:]\r\n    if not os.path.isfile(path_zip):\r\n        print(""zip file \'%s\' is not found""%(path_zip))\r\n        assert 0\r\n    for i in range(len(_im_zfile)):\r\n        if _im_zfile[i][\'path\'] == path_zip:\r\n            data = _im_zfile[i][\'zipfile\'].read(path_img)\r\n            return cv2.imdecode(np.frombuffer(data, np.uint8), flags)\r\n\r\n    _im_zfile.append({\r\n        \'path\': path_zip,\r\n        \'zipfile\': zipfile.ZipFile(path_zip, \'r\')\r\n    })\r\n    data = _im_zfile[-1][\'zipfile\'].read(path_img)\r\n\r\n    return cv2.imdecode(np.frombuffer(data, np.uint8), flags)\r\n\r\n\r\ndef xmlread(filename):\r\n    global _xml_path_zip\r\n    global _xml_zfile\r\n    path = filename\r\n    pos_at = path.index(\'@\')\r\n    if pos_at == -1:\r\n        print(""character \'@\' is not found from the given path \'%s\'""%(path))\r\n        assert 0\r\n    path_zip = path[0: pos_at]\r\n    path_xml = path[pos_at + 2:]\r\n    if not os.path.isfile(path_zip):\r\n        print(""zip file \'%s\' is not found""%(path_zip))\r\n        assert 0\r\n    for i in xrange(len(_xml_path_zip)):\r\n        if _xml_path_zip[i] == path_zip:\r\n            data = _xml_zfile[i].open(path_xml)\r\n            return ET.fromstring(data.read())\r\n    _xml_path_zip.append(path_zip)\r\n    print(""read new xml file \'%s\'""%(path_zip))\r\n    _xml_zfile.append(zipfile.ZipFile(path_zip, \'r\'))\r\n    data = _xml_zfile[-1].open(path_xml)\r\n    return ET.fromstring(data.read())\r\n'"
mmskeleton/deprecated/datasets/kinetics_feeder.py,2,"b'# sys\nimport os\nimport sys\nimport numpy as np\nimport random\nimport pickle\nimport json\n# torch\nimport torch\nimport torch.nn as nn\nfrom torchvision import datasets, transforms\n\n# operation\nfrom mmskeleton.datasets.utils import skeleton\n\n\nclass KineticsFeeder(torch.utils.data.Dataset):\n    """""" Feeder for skeleton-based action recognition in kinetics-skeleton dataset\n    Arguments:\n        data_path: the path to \'.npy\' data, the shape of data should be (N, C, T, V, M)\n        label_path: the path to label\n        random_choose: If true, randomly choose a portion of the input sequence\n        random_shift: If true, randomly pad zeros at the begining or end of sequence\n        random_move: If true, perform randomly but continuously changed transformation to input sequence\n        window_size: The length of the output sequence\n        pose_matching: If ture, match the pose between two frames\n        num_person_in: The number of people the feeder can observe in the input sequence\n        num_person_out: The number of people the feeder in the output sequence\n        debug: If true, only use the first 100 samples\n    """"""\n    def __init__(self,\n                 data_path,\n                 label_path,\n                 ignore_empty_sample=True,\n                 random_choose=False,\n                 random_shift=False,\n                 random_move=False,\n                 window_size=-1,\n                 pose_matching=False,\n                 num_person_in=5,\n                 num_person_out=2,\n                 debug=False):\n        self.debug = debug\n        self.data_path = data_path\n        self.label_path = label_path\n        self.random_choose = random_choose\n        self.random_shift = random_shift\n        self.random_move = random_move\n        self.window_size = window_size\n        self.num_person_in = num_person_in\n        self.num_person_out = num_person_out\n        self.pose_matching = pose_matching\n        self.ignore_empty_sample = ignore_empty_sample\n\n        self.load_data()\n\n    def load_data(self):\n        # load file list\n        self.sample_name = os.listdir(self.data_path)\n\n        if self.debug:\n            self.sample_name = self.sample_name[0:2]\n\n        # load label\n        label_path = self.label_path\n        with open(label_path) as f:\n            label_info = json.load(f)\n\n        sample_id = [name.split(\'.\')[0] for name in self.sample_name]\n        self.label = np.array(\n            [label_info[id][\'label_index\'] for id in sample_id])\n        has_skeleton = np.array(\n            [label_info[id][\'has_skeleton\'] for id in sample_id])\n\n        # ignore the samples which does not has skeleton sequence\n        if self.ignore_empty_sample:\n            self.sample_name = [\n                s for h, s in zip(has_skeleton, self.sample_name) if h\n            ]\n            self.label = self.label[has_skeleton]\n\n        # output data shape (N, C, T, V, M)\n        self.N = len(self.sample_name)  #sample\n        self.C = 3  #channel\n        self.T = 300  #frame\n        self.V = 18  #joint\n        self.M = self.num_person_out  #person\n\n    def __len__(self):\n        return len(self.sample_name)\n\n    def __iter__(self):\n        return self\n\n    def __getitem__(self, index):\n\n        # output shape (C, T, V, M)\n        # get data\n        sample_name = self.sample_name[index]\n        sample_path = os.path.join(self.data_path, sample_name)\n        with open(sample_path, \'r\') as f:\n            video_info = json.load(f)\n\n        # fill data_numpy\n        data_numpy = np.zeros((self.C, self.T, self.V, self.num_person_in))\n        for frame_info in video_info[\'data\']:\n            frame_index = frame_info[\'frame_index\']\n            for m, skeleton_info in enumerate(frame_info[""skeleton""]):\n                if m >= self.num_person_in:\n                    break\n                pose = skeleton_info[\'pose\']\n                score = skeleton_info[\'score\']\n                data_numpy[0, frame_index, :, m] = pose[0::2]\n                data_numpy[1, frame_index, :, m] = pose[1::2]\n                data_numpy[2, frame_index, :, m] = score\n\n        # centralization\n        data_numpy[0:2] = data_numpy[0:2] - 0.5\n        data_numpy[0][data_numpy[2] == 0] = 0\n        data_numpy[1][data_numpy[2] == 0] = 0\n\n        # get & check label index\n        label = video_info[\'label_index\']\n        assert (self.label[index] == label)\n\n        # data augmentation\n        if self.random_shift:\n            data_numpy = skeleton.random_shift(data_numpy)\n        if self.random_choose:\n            data_numpy = skeleton.random_choose(data_numpy, self.window_size)\n        elif self.window_size > 0:\n            data_numpy = skeleton.auto_pading(data_numpy, self.window_size)\n        if self.random_move:\n            data_numpy = skeleton.random_move(data_numpy)\n\n        # sort by score\n        sort_index = (-data_numpy[2, :, :, :].sum(axis=1)).argsort(axis=1)\n        for t, s in enumerate(sort_index):\n            data_numpy[:, t, :, :] = data_numpy[:, t, :, s].transpose(\n                (1, 2, 0))\n        data_numpy = data_numpy[:, :, :, 0:self.num_person_out]\n\n        # match poses between 2 frames\n        if self.pose_matching:\n            data_numpy = skeleton.openpose_match(data_numpy)\n\n        return data_numpy, label\n\n    def top_k(self, score, top_k):\n        assert (all(self.label >= 0))\n\n        rank = score.argsort()\n        hit_top_k = [l in rank[i, -top_k:] for i, l in enumerate(self.label)]\n        return sum(hit_top_k) * 1.0 / len(hit_top_k)\n\n    def top_k_by_category(self, score, top_k):\n        assert (all(self.label >= 0))\n        return skeleton.top_k_by_category(self.label, score, top_k)\n\n    def calculate_recall_precision(self, score):\n        assert (all(self.label >= 0))\n        return skeleton.calculate_recall_precision(self.label, score)\n'"
mmskeleton/deprecated/datasets/recognition.py,1,"b'import os\nimport numpy as np\nimport json\nimport torch\nfrom .utils import skeleton\n\n\nclass SkeletonDataset(torch.utils.data.Dataset):\n    """""" Feeder for skeleton-based action recognition\n    Arguments:\n        data_path: the path to data folder\n        random_choose: If true, randomly choose a portion of the input sequence\n        random_move: If true, randomly perfrom affine transformation\n        window_size: The length of the output sequence\n        repeat: times of repeating the dataset\n        data_subscripts: subscript expression of einsum operation.\n            In the default case, the shape of output data is `(channel, vertex, frames, person)`.\n            To permute the shape to `(channel, frames, vertex, person)`,\n            set `data_subscripts` to \'cvfm->cfvm\'.\n    """"""\n    def __init__(self,\n                 data_dir,\n                 random_choose=False,\n                 random_move=False,\n                 window_size=-1,\n                 num_track=1,\n                 data_subscripts=None,\n                 repeat=1):\n\n        self.data_dir = data_dir\n        self.random_choose = random_choose\n        self.random_move = random_move\n        self.window_size = window_size\n        self.num_track = num_track\n        self.data_subscripts = data_subscripts\n        self.files = [\n            os.path.join(self.data_dir, f) for f in os.listdir(self.data_dir)\n        ] * repeat\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, index):\n\n        with open(self.files[index]) as f:\n            data = json.load(f)\n\n        resolution = data[\'info\'][\'resolution\']\n        category_id = data[\'category_id\']\n        annotations = data[\'annotations\']\n        num_frame = data[\'info\'][\'num_frame\']\n        num_keypoints = data[\'info\'][\'num_keypoints\']\n        channel = data[\'info\'][\'keypoint_channels\']\n        num_channel = len(channel)\n\n        # get data\n        data = np.zeros(\n            (num_channel, num_keypoints, num_frame, self.num_track),\n            dtype=np.float32)\n\n        for a in annotations:\n            person_id = a[\'id\'] if a[\'person_id\'] is None else a[\'person_id\']\n            frame_index = a[\'frame_index\']\n            if person_id < self.num_track and frame_index < num_frame:\n                data[:, :, frame_index, person_id] = np.array(\n                    a[\'keypoints\']).transpose()\n\n        # normalization\n        if self.normalization:\n            for i, c in enumerate(channel):\n                if c == \'x\':\n                    data[i] = data[i] / resolution[0] - 0.5\n                if c == \'y\':\n                    data[i] = data[i] / resolution[1] - 0.5\n                if c == \'score\' or c == \'visibility\':\n                    mask = (data[i] == 0)\n                    for j in range(num_channel):\n                        if c != j:\n                            data[j][mask] = 0\n\n        # permute\n        if self.data_subscripts is not None:\n            data = np.einsum(self.data_subscripts, data)\n\n        # augmentation\n        if self.random_choose:\n            data = skeleton.random_choose(data, self.window_size)\n        elif self.window_size > 0:\n            data = skeleton.auto_pading(data, self.window_size)\n        if self.random_move:\n            data = skeleton.random_move(data)\n\n        return data, category_id'"
mmskeleton/deprecated/datasets/skeleton_feeder.py,1,"b'# sys\nimport numpy as np\nimport pickle\nimport torch\n\n# operation\nfrom .utils import skeleton\n\n\nclass SkeletonFeeder(torch.utils.data.Dataset):\n    """""" Feeder for skeleton-based action recognition\n    Arguments:\n        data_path: the path to \'.npy\' data, the shape of data should be (N, C, T, V, M)\n        label_path: the path to label\n        random_choose: If true, randomly choose a portion of the input sequence\n        random_shift: If true, randomly pad zeros at the begining or end of sequence\n        window_size: The length of the output sequence\n        normalization: If true, normalize input sequence\n        debug: If true, only use the first 100 samples\n    """"""\n    def __init__(self,\n                 data_path,\n                 label_path,\n                 random_choose=False,\n                 random_move=False,\n                 window_size=-1,\n                 debug=False,\n                 mmap=True):\n        self.debug = debug\n        self.data_path = data_path\n        self.label_path = label_path\n        self.random_choose = random_choose\n        self.random_move = random_move\n        self.window_size = window_size\n\n        self.load_data(mmap)\n\n    def load_data(self, mmap):\n        # data: N C V T M\n\n        # load label\n        with open(self.label_path, \'rb\') as f:\n            self.sample_name, self.label = pickle.load(f)\n\n        # load data\n        if mmap:\n            self.data = np.load(self.data_path, mmap_mode=\'r\')\n        else:\n            self.data = np.load(self.data_path)\n\n        if self.debug:\n            self.label = self.label[0:100]\n            self.data = self.data[0:100]\n            self.sample_name = self.sample_name[0:100]\n\n        self.N, self.C, self.T, self.V, self.M = self.data.shape\n\n    def __len__(self):\n        return len(self.label)\n\n    def __getitem__(self, index):\n        # get data\n        data_numpy = np.array(self.data[index])\n        label = self.label[index]\n\n        # processing\n        if self.random_choose:\n            data_numpy = skeleton.random_choose(data_numpy, self.window_size)\n        elif self.window_size > 0:\n            data_numpy = skeleton.auto_pading(data_numpy, self.window_size)\n        if self.random_move:\n            data_numpy = skeleton.random_move(data_numpy)\n\n        return data_numpy, label'"
mmskeleton/deprecated/models/pseudo.py,0,"b""class model():\n    def __init__(self, in_channels, out_channels, weight):\n        print('build a pseudo model:')\n        print('  in_channels: {}'.format(in_channels))\n        print('  out_channels: {}'.format(out_channels))\n        print('  weight: {}'.format(weight))\n        print('')"""
mmskeleton/deprecated/processor/pseudo.py,0,"b""from mmskeleton.utils import call_obj\n\n\ndef train(model_cfg, dataset_cfg, optimizer):\n    model = call_obj(**model_cfg)\n    dataset = call_obj(**dataset_cfg)\n    print('train a pseudo model...')\n    print('done.')\n\n\ndef hello_world(times=10):\n    for i in range(times):\n        print('Hello World!')"""
mmskeleton/models/backbones/__init__.py,0,b'from .st_gcn_aaai18 import ST_GCN_18\r\nfrom .hrnet import HRNet'
mmskeleton/models/backbones/hrnet.py,2,"b'# ------------------------------------------------------------------------------\r\n# Copyright (c) Microsoft\r\n# Licensed under the MIT License.\r\n# Written by Bin Xiao (Bin.Xiao@microsoft.com)\r\n# ------------------------------------------------------------------------------\r\nimport logging\r\n\r\nimport torch.nn as nn\r\nfrom mmcv.cnn import constant_init, kaiming_init\r\nfrom mmcv.runner import load_checkpoint\r\nfrom torch.nn.modules.batchnorm import _BatchNorm\r\n\r\nBN_MOMENTUM = 0.1\r\n\r\ndef conv3x3(in_planes, out_planes, stride=1):\r\n    """"""3x3 convolution with padding""""""\r\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\r\n                     padding=1, bias=False)\r\n\r\n\r\nclass BasicBlock(nn.Module):\r\n    expansion = 1\r\n\r\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\r\n        super(BasicBlock, self).__init__()\r\n        self.conv1 = conv3x3(inplanes, planes, stride)\r\n        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\r\n        self.relu = nn.ReLU(inplace=True)\r\n        self.conv2 = conv3x3(planes, planes)\r\n        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\r\n        self.downsample = downsample\r\n        self.stride = stride\r\n\r\n    def forward(self, x):\r\n        residual = x\r\n\r\n        out = self.conv1(x)\r\n        out = self.bn1(out)\r\n        out = self.relu(out)\r\n\r\n        out = self.conv2(out)\r\n        out = self.bn2(out)\r\n\r\n        if self.downsample is not None:\r\n            residual = self.downsample(x)\r\n\r\n        out += residual\r\n        out = self.relu(out)\r\n\r\n        return out\r\n\r\n\r\nclass Bottleneck(nn.Module):\r\n    expansion = 4\r\n\r\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\r\n        super(Bottleneck, self).__init__()\r\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\r\n        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\r\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\r\n                               padding=1, bias=False)\r\n        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\r\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\r\n                               bias=False)\r\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion,\r\n                                  momentum=BN_MOMENTUM)\r\n        self.relu = nn.ReLU(inplace=True)\r\n        self.downsample = downsample\r\n        self.stride = stride\r\n\r\n    def forward(self, x):\r\n        residual = x\r\n\r\n        out = self.conv1(x)\r\n        out = self.bn1(out)\r\n        out = self.relu(out)\r\n\r\n        out = self.conv2(out)\r\n        out = self.bn2(out)\r\n        out = self.relu(out)\r\n\r\n        out = self.conv3(out)\r\n        out = self.bn3(out)\r\n\r\n        if self.downsample is not None:\r\n            residual = self.downsample(x)\r\n\r\n        out += residual\r\n        out = self.relu(out)\r\n\r\n        return out\r\n\r\nclass HRModule(nn.Module):\r\n    def __init__(self, num_branches, blocks, num_blocks, num_inchannels,\r\n                 num_channels, fuse_method, multi_scale_output=True):\r\n        super(HRModule, self).__init__()\r\n        self._check_branches(\r\n            num_branches,  num_blocks, num_inchannels, num_channels)\r\n\r\n        self.num_inchannels = num_inchannels\r\n        self.fuse_method = fuse_method\r\n        self.num_branches = num_branches\r\n\r\n        self.multi_scale_output = multi_scale_output\r\n\r\n        self.branches = self._make_branches(\r\n            num_branches, blocks, num_blocks, num_channels)\r\n        self.fuse_layers = self._make_fuse_layers()\r\n        self.relu = nn.ReLU(True)\r\n\r\n    def _check_branches(self, num_branches,  num_blocks,\r\n                        num_inchannels, num_channels):\r\n        if num_branches != len(num_blocks):\r\n            error_msg = \'NUM_BRANCHES({}) <> NUM_BLOCKS({})\'.format(\r\n                num_branches, len(num_blocks))\r\n            raise ValueError(error_msg)\r\n\r\n        if num_branches != len(num_channels):\r\n            error_msg = \'NUM_BRANCHES({}) <> NUM_CHANNELS({})\'.format(\r\n                num_branches, len(num_channels))\r\n            raise ValueError(error_msg)\r\n\r\n        if num_branches != len(num_inchannels):\r\n            error_msg = \'NUM_BRANCHES({}) <> NUM_INCHANNELS({})\'.format(\r\n                num_branches, len(num_inchannels))\r\n            raise ValueError(error_msg)\r\n\r\n    def _make_one_branch(self, branch_index, block, num_blocks, num_channels,\r\n                         stride=1):\r\n        downsample = None\r\n        if stride != 1 or \\\r\n           self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:\r\n            downsample = nn.Sequential(\r\n                nn.Conv2d(\r\n                    self.num_inchannels[branch_index],\r\n                    num_channels[branch_index] * block.expansion,\r\n                    kernel_size=1, stride=stride, bias=False\r\n                ),\r\n                nn.BatchNorm2d(\r\n                    num_channels[branch_index] * block.expansion,\r\n                    momentum=BN_MOMENTUM\r\n                ),\r\n            )\r\n\r\n        layers = []\r\n        layers.append(\r\n            block(\r\n                self.num_inchannels[branch_index],\r\n                num_channels[branch_index],\r\n                stride,\r\n                downsample\r\n            )\r\n        )\r\n        self.num_inchannels[branch_index] = \\\r\n            num_channels[branch_index] * block.expansion\r\n        for i in range(1, num_blocks[branch_index]):\r\n            layers.append(\r\n                block(\r\n                    self.num_inchannels[branch_index],\r\n                    num_channels[branch_index]\r\n                )\r\n            )\r\n\r\n        return nn.Sequential(*layers)\r\n\r\n    def _make_branches(self, num_branches, block, num_blocks, num_channels):\r\n        branches = []\r\n\r\n        for i in range(num_branches):\r\n            branches.append(\r\n                self._make_one_branch(i, block, num_blocks, num_channels)\r\n            )\r\n\r\n        return nn.ModuleList(branches)\r\n\r\n    def _make_fuse_layers(self):\r\n        if self.num_branches == 1:\r\n            return None\r\n\r\n        num_branches = self.num_branches\r\n        num_inchannels = self.num_inchannels\r\n        fuse_layers = []\r\n        for i in range(num_branches if self.multi_scale_output else 1):\r\n            fuse_layer = []\r\n            for j in range(num_branches):\r\n                if j > i:\r\n                    fuse_layer.append(\r\n                        nn.Sequential(\r\n                            nn.Conv2d(\r\n                                num_inchannels[j],\r\n                                num_inchannels[i],\r\n                                1, 1, 0, bias=False\r\n                            ),\r\n                            nn.BatchNorm2d(num_inchannels[i]),\r\n                            nn.Upsample(scale_factor=2**(j-i), mode=\'nearest\')\r\n                        )\r\n                    )\r\n                elif j == i:\r\n                    fuse_layer.append(None)\r\n                else:\r\n                    conv3x3s = []\r\n                    for k in range(i-j):\r\n                        if k == i - j - 1:\r\n                            num_outchannels_conv3x3 = num_inchannels[i]\r\n                            conv3x3s.append(\r\n                                nn.Sequential(\r\n                                    nn.Conv2d(\r\n                                        num_inchannels[j],\r\n                                        num_outchannels_conv3x3,\r\n                                        3, 2, 1, bias=False\r\n                                    ),\r\n                                    nn.BatchNorm2d(num_outchannels_conv3x3)\r\n                                )\r\n                            )\r\n                        else:\r\n                            num_outchannels_conv3x3 = num_inchannels[j]\r\n                            conv3x3s.append(\r\n                                nn.Sequential(\r\n                                    nn.Conv2d(\r\n                                        num_inchannels[j],\r\n                                        num_outchannels_conv3x3,\r\n                                        3, 2, 1, bias=False\r\n                                    ),\r\n                                    nn.BatchNorm2d(num_outchannels_conv3x3),\r\n                                    nn.ReLU(True)\r\n                                )\r\n                            )\r\n                    fuse_layer.append(nn.Sequential(*conv3x3s))\r\n            fuse_layers.append(nn.ModuleList(fuse_layer))\r\n\r\n        return nn.ModuleList(fuse_layers)\r\n\r\n    def get_num_inchannels(self):\r\n        return self.num_inchannels\r\n\r\n    def forward(self, x):\r\n        if self.num_branches == 1:\r\n            return [self.branches[0](x[0])]\r\n\r\n        for i in range(self.num_branches):\r\n            x[i] = self.branches[i](x[i])\r\n\r\n        x_fuse = []\r\n\r\n        for i in range(len(self.fuse_layers)):\r\n            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])\r\n            for j in range(1, self.num_branches):\r\n                if i == j:\r\n                    y = y + x[j]\r\n                else:\r\n                    y = y + self.fuse_layers[i][j](x[j])\r\n            x_fuse.append(self.relu(y))\r\n\r\n        return x_fuse\r\n\r\nclass HRNet(nn.Module):\r\n    blocks_dict = {\r\n        \'BASIC\' : BasicBlock,\r\n        \'BOTTLENECK\':Bottleneck\r\n    }\r\n\r\n    def __init__(self, extra, **kwargs):\r\n        self.inplanes = 64\r\n        self.extra = extra\r\n        super(HRNet, self).__init__()\r\n\r\n        # stem net\r\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1,\r\n                               bias=False)\r\n        self.bn1 = nn.BatchNorm2d(64, momentum=BN_MOMENTUM)\r\n        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1,\r\n                               bias=False)\r\n        self.bn2 = nn.BatchNorm2d(64, momentum=BN_MOMENTUM)\r\n        self.relu = nn.ReLU(inplace=True)\r\n        # stage1\r\n        self.stage1_cfg = self.extra[\'stage1\']\r\n        num_channels = self.stage1_cfg[\'num_channels\'][0]\r\n        block_type = self.stage1_cfg[\'block\']\r\n        num_blocks = self.stage1_cfg[\'num_blocks\'][0]\r\n\r\n        block = self.blocks_dict[block_type]\r\n        stage1_out_channels = num_channels * block.expansion\r\n        self.layer1 = self._make_layer(block,\r\n                                       num_channels,\r\n                                       num_blocks\r\n                                       )\r\n\r\n        self.stage2_cfg = self.extra[\'stage2\']\r\n        num_channels = self.stage2_cfg[\'num_channels\']\r\n        block_type = self.stage2_cfg[\'block\']\r\n\r\n        block = self.blocks_dict[block_type]\r\n        num_channels = [channel * block.expansion for channel in num_channels]\r\n        self.transition1 = self._make_transition_layer([stage1_out_channels],\r\n                                                       num_channels)\r\n        self.stage2, pre_stage_channels = self._make_stage(\r\n            self.stage2_cfg, num_channels)\r\n\r\n        # stage 3\r\n\r\n        self.stage3_cfg = self.extra[\'stage3\']\r\n        num_channels = self.stage3_cfg[\'num_channels\']\r\n        block_type = self.stage3_cfg[\'block\']\r\n\r\n        block = self.blocks_dict[block_type]\r\n        num_channels = [channel * block.expansion for channel in num_channels]\r\n        self.transition2 = self._make_transition_layer(pre_stage_channels,\r\n                                                       num_channels)\r\n        self.stage3, pre_stage_channels = self._make_stage(\r\n            self.stage3_cfg, num_channels)\r\n\r\n        # stage 4\r\n        self.stage4_cfg = self.extra[\'stage4\']\r\n        num_channels = self.stage4_cfg[\'num_channels\']\r\n        block_type = self.stage4_cfg[\'block\']\r\n\r\n        block = self.blocks_dict[block_type]\r\n        num_channels = [channel * block.expansion for channel in num_channels]\r\n        self.transition3 = self._make_transition_layer(pre_stage_channels,\r\n                                                       num_channels)\r\n        self.stage4, pre_stage_channels = self._make_stage(\r\n            self.stage4_cfg, num_channels)\r\n        self.init_weights()\r\n\r\n\r\n    def _make_transition_layer(\r\n            self,\r\n            num_channels_pre_layer,\r\n            num_channels_cur_layer):\r\n        num_branches_cur = len(num_channels_cur_layer)\r\n        num_branches_pre = len(num_channels_pre_layer)\r\n\r\n        transition_layers = []\r\n        for i in range(num_branches_cur):\r\n            if i < num_branches_pre:\r\n                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\r\n                    transition_layers.append(\r\n                        nn.Sequential(\r\n                            nn.Conv2d(\r\n                                num_channels_pre_layer[i],\r\n                                num_channels_cur_layer[i],\r\n                                3, 1, 1, bias=False\r\n                            ),\r\n                            nn.BatchNorm2d(num_channels_cur_layer[i]),\r\n                            nn.ReLU(inplace=True)\r\n                        )\r\n                    )\r\n                else:\r\n                    transition_layers.append(None)\r\n            else:\r\n                conv3x3s = []\r\n                for j in range(i+1-num_branches_pre):\r\n                    inchannels = num_channels_pre_layer[-1]\r\n                    outchannels = num_channels_cur_layer[i] \\\r\n                        if j == i-num_branches_pre else inchannels\r\n                    conv3x3s.append(\r\n                        nn.Sequential(\r\n                            nn.Conv2d(\r\n                                inchannels, outchannels, 3, 2, 1, bias=False\r\n                            ),\r\n                            nn.BatchNorm2d(outchannels),\r\n                            nn.ReLU(inplace=True)\r\n                        )\r\n                    )\r\n                transition_layers.append(nn.Sequential(*conv3x3s))\r\n\r\n        return nn.ModuleList(transition_layers)\r\n\r\n    def _make_layer(self,\r\n                    block,\r\n                    planes,\r\n                    blocks,\r\n                    stride=1):\r\n        downsample = None\r\n        if stride != 1 or self.inplanes != planes * block.expansion:\r\n            downsample = nn.Sequential(\r\n                nn.Conv2d(\r\n                    self.inplanes, planes * block.expansion,\r\n                    kernel_size=1, stride=stride, bias=False\r\n                ),\r\n                nn.BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),\r\n            )\r\n\r\n        layers = []\r\n        layers.append(block(self.inplanes, planes, stride, downsample))\r\n        self.inplanes = planes * block.expansion\r\n        for i in range(1, blocks):\r\n            layers.append(block(self.inplanes, planes))\r\n\r\n        return nn.Sequential(*layers)\r\n\r\n    def _make_stage(self, layer_config, num_inchannels,\r\n                    multi_scale_output=True):\r\n        num_modules = layer_config[\'num_modules\']\r\n        num_branches = layer_config[\'num_branches\']\r\n        num_blocks = layer_config[\'num_blocks\']\r\n        num_channels = layer_config[\'num_channels\']\r\n        block = self.blocks_dict[layer_config[\'block\']]\r\n        fuse_method = layer_config[\'fuse_method\']\r\n\r\n        modules = []\r\n        for i in range(num_modules):\r\n            # multi_scale_output is only used last module\r\n            if not multi_scale_output and i == num_modules - 1:\r\n                reset_multi_scale_output = False\r\n            else:\r\n                reset_multi_scale_output = True\r\n\r\n            modules.append(\r\n                HRModule(\r\n                    num_branches,\r\n                    block,\r\n                    num_blocks,\r\n                    num_inchannels,\r\n                    num_channels,\r\n                    fuse_method,\r\n                    reset_multi_scale_output\r\n                )\r\n            )\r\n            num_inchannels = modules[-1].get_num_inchannels()\r\n\r\n        return nn.Sequential(*modules), num_inchannels\r\n\r\n    def forward(self, x):\r\n        x = self.conv1(x)\r\n        x = self.bn1(x)\r\n        x = self.relu(x)\r\n        x = self.conv2(x)\r\n        x = self.bn2(x)\r\n        x = self.relu(x)\r\n        x = self.layer1(x)\r\n\r\n        x_list = []\r\n        for i in range(self.stage2_cfg[\'num_branches\']):\r\n            if self.transition1[i] is not None:\r\n                x_list.append(self.transition1[i](x))\r\n            else:\r\n                x_list.append(x)\r\n        y_list = self.stage2(x_list)\r\n\r\n        x_list = []\r\n        for i in range(self.stage3_cfg[\'num_branches\']):\r\n            if self.transition2[i] is not None:\r\n                x_list.append(self.transition2[i](y_list[-1]))\r\n            else:\r\n                x_list.append(y_list[i])\r\n        y_list = self.stage3(x_list)\r\n\r\n        x_list = []\r\n        for i in range(self.stage4_cfg[\'num_branches\']):\r\n            if self.transition3[i] is not None:\r\n                x_list.append(self.transition3[i](y_list[-1]))\r\n            else:\r\n                x_list.append(y_list[i])\r\n        y_list = self.stage4(x_list)\r\n        return y_list\r\n\r\n    def init_weights(self, pretrained=None):\r\n        if isinstance(pretrained, str):\r\n            logger = logging.getLogger()\r\n            load_checkpoint(self, pretrained, strict=False, logger=logger)\r\n        elif pretrained is None:\r\n            for m in self.modules():\r\n                if isinstance(m, nn.Conv2d):\r\n                    kaiming_init(m)\r\n                elif isinstance(m, (_BatchNorm, nn.GroupNorm)):\r\n                    constant_init(m, 1)'"
mmskeleton/models/backbones/st_gcn_aaai18.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nfrom mmskeleton.ops.st_gcn import ConvTemporalGraphical, Graph\n\n\nclass ST_GCN_18(nn.Module):\n    r""""""Spatial temporal graph convolutional networks.\n\n    Args:\n        in_channels (int): Number of channels in the input data\n        num_class (int): Number of classes for the classification task\n        graph_cfg (dict): The arguments for building the graph\n        edge_importance_weighting (bool): If ``True``, adds a learnable\n            importance weighting to the edges of the graph\n        **kwargs (optional): Other parameters for graph convolution units\n\n    Shape:\n        - Input: :math:`(N, in_channels, T_{in}, V_{in}, M_{in})`\n        - Output: :math:`(N, num_class)` where\n            :math:`N` is a batch size,\n            :math:`T_{in}` is a length of input sequence,\n            :math:`V_{in}` is the number of graph nodes,\n            :math:`M_{in}` is the number of instance in a frame.\n    """"""\n    def __init__(self,\n                 in_channels,\n                 num_class,\n                 graph_cfg,\n                 edge_importance_weighting=True,\n                 data_bn=True,\n                 **kwargs):\n        super().__init__()\n\n        # load graph\n        self.graph = Graph(**graph_cfg)\n        A = torch.tensor(self.graph.A,\n                         dtype=torch.float32,\n                         requires_grad=False)\n        self.register_buffer(\'A\', A)\n\n        # build networks\n        spatial_kernel_size = A.size(0)\n        temporal_kernel_size = 9\n        kernel_size = (temporal_kernel_size, spatial_kernel_size)\n        self.data_bn = nn.BatchNorm1d(in_channels *\n                                      A.size(1)) if data_bn else lambda x: x\n        kwargs0 = {k: v for k, v in kwargs.items() if k != \'dropout\'}\n        self.st_gcn_networks = nn.ModuleList((\n            st_gcn_block(in_channels,\n                         64,\n                         kernel_size,\n                         1,\n                         residual=False,\n                         **kwargs0),\n            st_gcn_block(64, 64, kernel_size, 1, **kwargs),\n            st_gcn_block(64, 64, kernel_size, 1, **kwargs),\n            st_gcn_block(64, 64, kernel_size, 1, **kwargs),\n            st_gcn_block(64, 128, kernel_size, 2, **kwargs),\n            st_gcn_block(128, 128, kernel_size, 1, **kwargs),\n            st_gcn_block(128, 128, kernel_size, 1, **kwargs),\n            st_gcn_block(128, 256, kernel_size, 2, **kwargs),\n            st_gcn_block(256, 256, kernel_size, 1, **kwargs),\n            st_gcn_block(256, 256, kernel_size, 1, **kwargs),\n        ))\n\n        # initialize parameters for edge importance weighting\n        if edge_importance_weighting:\n            self.edge_importance = nn.ParameterList([\n                nn.Parameter(torch.ones(self.A.size()))\n                for i in self.st_gcn_networks\n            ])\n        else:\n            self.edge_importance = [1] * len(self.st_gcn_networks)\n\n        # fcn for prediction\n        self.fcn = nn.Conv2d(256, num_class, kernel_size=1)\n\n    def forward(self, x):\n        # data normalization\n        N, C, T, V, M = x.size()\n        x = x.permute(0, 4, 3, 1, 2).contiguous()\n        x = x.view(N * M, V * C, T)\n        x = self.data_bn(x)\n        x = x.view(N, M, V, C, T)\n        x = x.permute(0, 1, 3, 4, 2).contiguous()\n        x = x.view(N * M, C, T, V)\n\n        # forward\n        for gcn, importance in zip(self.st_gcn_networks, self.edge_importance):\n            x, _ = gcn(x, self.A * importance)\n\n        # global pooling\n        x = F.avg_pool2d(x, x.size()[2:])\n        x = x.view(N, M, -1, 1, 1).mean(dim=1)\n\n        # prediction\n        x = self.fcn(x)\n        x = x.view(x.size(0), -1)\n\n        return x\n\n    def extract_feature(self, x):\n\n        # data normalization\n        N, C, T, V, M = x.size()\n        x = x.permute(0, 4, 3, 1, 2).contiguous()\n        x = x.view(N * M, V * C, T)\n        x = self.data_bn(x)\n        x = x.view(N, M, V, C, T)\n        x = x.permute(0, 1, 3, 4, 2).contiguous()\n        x = x.view(N * M, C, T, V)\n\n        # forwad\n        for gcn, importance in zip(self.st_gcn_networks, self.edge_importance):\n            x, _ = gcn(x, self.A * importance)\n\n        _, c, t, v = x.size()\n        feature = x.view(N, M, c, t, v).permute(0, 2, 3, 4, 1)\n\n        # prediction\n        x = self.fcn(x)\n        output = x.view(N, M, -1, t, v).permute(0, 2, 3, 4, 1)\n\n        return output, feature\n\n\nclass st_gcn_block(nn.Module):\n    r""""""Applies a spatial temporal graph convolution over an input graph sequence.\n\n    Args:\n        in_channels (int): Number of channels in the input sequence data\n        out_channels (int): Number of channels produced by the convolution\n        kernel_size (tuple): Size of the temporal convolving kernel and graph convolving kernel\n        stride (int, optional): Stride of the temporal convolution. Default: 1\n        dropout (int, optional): Dropout rate of the final output. Default: 0\n        residual (bool, optional): If ``True``, applies a residual mechanism. Default: ``True``\n\n    Shape:\n        - Input[0]: Input graph sequence in :math:`(N, in_channels, T_{in}, V)` format\n        - Input[1]: Input graph adjacency matrix in :math:`(K, V, V)` format\n        - Output[0]: Outpu graph sequence in :math:`(N, out_channels, T_{out}, V)` format\n        - Output[1]: Graph adjacency matrix for output data in :math:`(K, V, V)` format\n\n        where\n            :math:`N` is a batch size,\n            :math:`K` is the spatial kernel size, as :math:`K == kernel_size[1]`,\n            :math:`T_{in}/T_{out}` is a length of input/output sequence,\n            :math:`V` is the number of graph nodes.\n\n    """"""\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 dropout=0,\n                 residual=True):\n        super().__init__()\n\n        assert len(kernel_size) == 2\n        assert kernel_size[0] % 2 == 1\n        padding = ((kernel_size[0] - 1) // 2, 0)\n\n        self.gcn = ConvTemporalGraphical(in_channels, out_channels,\n                                         kernel_size[1])\n\n        self.tcn = nn.Sequential(\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(\n                out_channels,\n                out_channels,\n                (kernel_size[0], 1),\n                (stride, 1),\n                padding,\n            ),\n            nn.BatchNorm2d(out_channels),\n            nn.Dropout(dropout, inplace=True),\n        )\n\n        if not residual:\n            self.residual = lambda x: 0\n\n        elif (in_channels == out_channels) and (stride == 1):\n            self.residual = lambda x: x\n\n        else:\n            self.residual = nn.Sequential(\n                nn.Conv2d(in_channels,\n                          out_channels,\n                          kernel_size=1,\n                          stride=(stride, 1)),\n                nn.BatchNorm2d(out_channels),\n            )\n\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x, A):\n\n        res = self.residual(x)\n        x, A = self.gcn(x, A)\n        x = self.tcn(x) + res\n\n        return self.relu(x), A'"
mmskeleton/models/estimator/__init__.py,0,b'from .base import BaseEstimator\r\nfrom .hrnet_pose import HRPoseEstimator\r\nfrom .twodim_pose import TwoDimPoseEstimator'
mmskeleton/models/estimator/base.py,1,"b'import logging\r\nfrom abc import ABCMeta, abstractmethod\r\nimport torch.nn as nn\r\n\r\nclass BaseEstimator(nn.Module):\r\n    """"""Base class for pose estimation""""""\r\n\r\n    __metaclass__ = ABCMeta\r\n\r\n    def __init__(self):\r\n        super(BaseEstimator, self).__init__()\r\n\r\n    @property\r\n    def with_neck(self):\r\n        return hasattr(self, \'neck\') and self.neck is not None\r\n\r\n    @abstractmethod\r\n    def extract_feat(self, imgs):\r\n        pass\r\n\r\n    @abstractmethod\r\n    def extract_feats(self, imgs):\r\n        assert isinstance(imgs, list)\r\n        for img in imgs:\r\n            yield self.extract_feat(img)\r\n\r\n    @abstractmethod\r\n    def forward_train(self, input, meta, **kwargs):\r\n        pass\r\n\r\n    @abstractmethod\r\n    def simple_test(self, input, meta, **kwargs):\r\n        pass\r\n\r\n    @abstractmethod\r\n    def aug_test(self, input, meta, **kwargs):\r\n        pass\r\n\r\n    def init_weights(self, pretrained=None):\r\n        if pretrained is not None:\r\n            logger = logging.getLogger()\r\n            logger.info(\'load model from: {}\'.format(pretrained))\r\n\r\n    def forward_test(self, input, **kwargs):\r\n        pass\r\n\r\n    def forward(self, image, meta = None, targets = None, target_weights = None, return_loss=True, **kwargs):\r\n        if return_loss:\r\n            return  self.forward_train(image, meta, targets, target_weights, **kwargs)\r\n        else:\r\n            return  self.forward_test(image, **kwargs)\r\n'"
mmskeleton/models/estimator/hrnet_pose.py,0,"b'from .twodim_pose import TwoDimPoseEstimator\r\n\r\nclass HRPoseEstimator(TwoDimPoseEstimator):\r\n    def __init__(self,\r\n                 backbone,\r\n                 neck = None,\r\n                 skeleton_head = None,\r\n                 train_cfg = None,\r\n                 test_cfg = None,\r\n                 pretrained = None):\r\n        super(HRPoseEstimator, self).\\\r\n            __init__(backbone, neck, skeleton_head, train_cfg,\r\n                     test_cfg, pretrained)\r\n\r\n'"
mmskeleton/models/estimator/twodim_pose.py,1,"b'import torch.nn as nn\r\nfrom .base import BaseEstimator\r\nfrom mmskeleton.utils.importer import call_obj\r\nclass TwoDimPoseEstimator(BaseEstimator):\r\n    def __init__(self,\r\n                 backbone,\r\n                 neck=None,\r\n                 skeleton_head=None,\r\n                 train_cfg=None,\r\n                 test_cfg=None,\r\n                 pretrained=None):\r\n        super(TwoDimPoseEstimator,self).__init__()\r\n        self.backbone = call_obj(**backbone)\r\n        if neck is not None:\r\n            self.neck = call_obj(**neck)\r\n        self.skeleton_head = call_obj(**skeleton_head)\r\n        self.train_cfg = train_cfg\r\n        self.test_cfg = test_cfg\r\n        self.init_weights(pretrained=pretrained)\r\n\r\n    def init_weights(self, pretrained=None):\r\n        super(TwoDimPoseEstimator, self).init_weights(pretrained)\r\n        self.backbone.init_weights(pretrained=pretrained)\r\n        if self.with_neck:\r\n            if isinstance(self.neck, nn.Sequential):\r\n                for m in self.neck:\r\n                    m.init_weights()\r\n            else:\r\n                self.neck.init_weights()\r\n        self.skeleton_head.init_weights()\r\n\r\n\r\n    def extract_feat(self, img):\r\n        x = self.backbone(img)\r\n        if self.with_neck:\r\n            x = self.neck(x)\r\n        return x\r\n\r\n    def forward_dummy(self, img):\r\n        x = self.extract_feat(img)\r\n        outs = self.skeleton_head(x[0])\r\n        return outs\r\n\r\n    def forward_train(self,\r\n                      image,\r\n                      meta,\r\n                      targets,\r\n                      target_weights\r\n                      ):\r\n\r\n        x =self.extract_feat(image)\r\n        outs = self.skeleton_head(x)\r\n        loss_inputs = [outs, targets, target_weights]\r\n        losses = self.skeleton_head.loss(\r\n            *loss_inputs)\r\n\r\n        return losses\r\n\r\n    def forward_test(self, image, **kwargs):\r\n        x =self.extract_feat(image)\r\n        outs = self.skeleton_head(x)\r\n        return outs\r\n\r\n\r\n\r\n\r\n\r\n'"
mmskeleton/models/loss/JointsMSELoss.py,1,"b""import torch.nn as nn\r\n\r\nclass JointsMSELoss(nn.Module):\r\n    def __init__(self, use_target_weight):\r\n        super(JointsMSELoss, self).__init__()\r\n        self.criterion = nn.MSELoss(reduction='mean')\r\n        self.use_target_weight = use_target_weight\r\n\r\n    def forward(self, output, target, target_weight):\r\n        batch_size = output.size(0)\r\n        num_joints = output.size(1)\r\n        heatmaps_pred = output.reshape((batch_size, num_joints, -1)).split(1, 1)\r\n        heatmaps_gt = target.reshape((batch_size, num_joints, -1)).split(1, 1)\r\n        loss = 0\r\n\r\n        for idx in range(num_joints):\r\n            heatmap_pred = heatmaps_pred[idx].squeeze()\r\n            heatmap_gt = heatmaps_gt[idx].squeeze()\r\n            if self.use_target_weight:\r\n                loss += 0.5 * self.criterion(\r\n                    heatmap_pred.mul(target_weight[:, idx]),\r\n                    heatmap_gt.mul(target_weight[:, idx])\r\n                )\r\n            else:\r\n                loss += 0.5 * self.criterion(heatmap_pred, heatmap_gt)\r\n\r\n        return loss / num_joints\r\n"""
mmskeleton/models/loss/JointsOHKMMSELoss.py,5,"b""import torch\r\nimport torch.nn as nn\r\n\r\nclass JointsOHKMMSELoss(nn.Module):\r\n    def __init__(self, use_target_weight, topk=8):\r\n        super(JointsOHKMMSELoss, self).__init__()\r\n        self.criterion = nn.MSELoss(reduction='none')\r\n        self.use_target_weight = use_target_weight\r\n        self.topk = topk\r\n\r\n    def ohkm(self, loss):\r\n        ohkm_loss = 0.\r\n        for i in range(loss.size()[0]):\r\n            sub_loss = loss[i]\r\n            topk_val, topk_idx = torch.topk(\r\n                sub_loss, k=self.topk, dim=0, sorted=False\r\n            )\r\n            tmp_loss = torch.gather(sub_loss, 0, topk_idx)\r\n            ohkm_loss += torch.sum(tmp_loss) / self.topk\r\n        ohkm_loss /= loss.size()[0]\r\n        return ohkm_loss\r\n\r\n    def forward(self, outs, targets, target_weights):\r\n        batch_size = outs.size(0)\r\n        num_joints = outs.size(1)\r\n        heatmaps_pred = outs.reshape((batch_size, num_joints, -1)).split(1, 1)\r\n        heatmaps_targets = targets.reshape((batch_size, num_joints, -1)).split(1, 1)\r\n\r\n        loss = []\r\n        for idx in range(num_joints):\r\n            heatmap_pred = heatmaps_pred[idx].squeeze()\r\n            heatmaps_targets = heatmaps_targets[idx].squeeze()\r\n            if self.use_target_weight:\r\n                loss.append(0.5 * self.criterion(\r\n                    heatmap_pred.mul(target_weights[:, idx]),\r\n                    heatmaps_targets.mul(target_weights[:, idx])\r\n                ))\r\n            else:\r\n                loss.append(\r\n                    0.5 * self.criterion(heatmap_pred, heatmaps_targets)\r\n                )\r\n\r\n        loss = [l.mean(dim=1).unsqueeze(dim=1) for l in loss]\r\n        loss = torch.cat(loss, dim=1)\r\n\r\n        return self.ohkm(loss)"""
mmskeleton/models/loss/__init__.py,0,b'from .JointsMSELoss import JointsMSELoss\r\nfrom .JointsOHKMMSELoss import JointsOHKMMSELoss'
mmskeleton/models/skeleton_head/__init__.py,0,b'from .simplehead import SimpleSkeletonHead'
mmskeleton/models/skeleton_head/simplehead.py,1,"b""import torch.nn as nn\r\nfrom mmskeleton.utils.importer import call_obj\r\n\r\n\r\nclass SimpleSkeletonHead(nn.Module):\r\n    def __init__(self,\r\n                 num_convs,\r\n                 in_channels,\r\n                 embed_channels=None,\r\n                 kernel_size=None,\r\n                 num_joints=None,\r\n                 reg_loss=dict(name='JointsMSELoss', use_target_weight=False)):\r\n        super(SimpleSkeletonHead, self).__init__()\r\n        self.num_convs = num_convs\r\n        self.in_channels = in_channels\r\n        self.embed_channels = embed_channels\r\n        self.kernel_size = kernel_size\r\n        self.num_joints = num_joints\r\n        self.skeleton_reg = self.make_layers()\r\n        self.reg_loss = call_obj(**reg_loss)\r\n\r\n    def init_weights(self):\r\n        for m in self.modules():\r\n            if isinstance(m, nn.Conv2d):\r\n                nn.init.normal_(m.weight, std=0.001)\r\n                for name, _ in m.named_parameters():\r\n                    if name in ['bias']:\r\n                        nn.init.constant_(m.bias, 0)\r\n\r\n    def make_layers(self):\r\n\r\n        assert isinstance(self.embed_channels, list) or isinstance(self.embed_channels, int) or \\\r\n               self.embed_channels is None\r\n        assert isinstance(self.kernel_size, list) or isinstance(\r\n            self.kernel_size, int)\r\n\r\n        if isinstance(self.embed_channels, list):\r\n            assert len(self.embed_channels) == self.num_convs - 1\r\n\r\n        if isinstance(self.kernel_size, list):\r\n            assert len(self.kernel_size) == self.num_convs\r\n\r\n        module_list = []\r\n\r\n        for i in range(self.num_convs):\r\n            if i == 0:\r\n                in_channels = self.in_channels\r\n\r\n            if i < self.num_convs - 1:\r\n                if isinstance(self.embed_channels, list):\r\n                    out_channels = self.embed_channels[i]\r\n                elif isinstance(self.embed_channels, int):\r\n                    out_channels = self.embed_channels\r\n            elif (i == self.num_convs - 1) or isinstance(\r\n                    self.embed_channels, None):\r\n                out_channels = self.num_joints\r\n\r\n            if isinstance(self.kernel_size, list):\r\n                kernel_size = self.kernel_size[i]\r\n            else:\r\n                kernel_size = self.kernel_size\r\n\r\n            padding = kernel_size // 2\r\n            module_list.append(\r\n                nn.Conv2d(in_channels=in_channels,\r\n                          out_channels=out_channels,\r\n                          kernel_size=kernel_size,\r\n                          padding=padding,\r\n                          stride=1))\r\n\r\n            in_channels = out_channels\r\n\r\n        return nn.Sequential(*module_list)\r\n\r\n    def forward(self, x):\r\n        reg_pred = self.skeleton_reg(x[0])\r\n        return reg_pred\r\n\r\n    def loss(self, outs, targets, target_weights):\r\n        losses = dict()\r\n        losses['reg_loss'] = self.reg_loss(outs, targets, target_weights)\r\n\r\n        return losses\r\n"""
mmskeleton/ops/nms/__init__.py,0,b''
mmskeleton/ops/nms/nms.py,0,"b'# ------------------------------------------------------------------------------\n# Copyright (c) Microsoft\n# Licensed under the MIT License.\n# Modified from py-faster-rcnn (https://github.com/rbgirshick/py-faster-rcnn)\n# ------------------------------------------------------------------------------\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom .cpu_nms import cpu_nms\nfrom .gpu_nms import gpu_nms\n\n\ndef py_nms_wrapper(thresh):\n    def _nms(dets):\n        return nms(dets, thresh)\n    return _nms\n\n\ndef cpu_nms_wrapper(thresh):\n    def _nms(dets):\n        return cpu_nms(dets, thresh)\n    return _nms\n\n\ndef gpu_nms_wrapper(thresh, device_id):\n    def _nms(dets):\n        return gpu_nms(dets, thresh, device_id)\n    return _nms\n\n\ndef nms(dets, thresh):\n    """"""\n    greedily select boxes with high confidence and overlap with current maximum <= thresh\n    rule out overlap >= thresh\n    :param dets: [[x1, y1, x2, y2 score]]\n    :param thresh: retain overlap < thresh\n    :return: indexes to keep\n    """"""\n    if dets.shape[0] == 0:\n        return []\n\n    x1 = dets[:, 0]\n    y1 = dets[:, 1]\n    x2 = dets[:, 2]\n    y2 = dets[:, 3]\n    scores = dets[:, 4]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        inds = np.where(ovr <= thresh)[0]\n        order = order[inds + 1]\n\n    return keep\n\n\ndef oks_iou(g, d, a_g, a_d, sigmas=None, in_vis_thre=None):\n    if not isinstance(sigmas, np.ndarray):\n        sigmas = np.array([.26, .25, .25, .35, .35, .79, .79, .72, .72, .62, .62, 1.07, 1.07, .87, .87, .89, .89]) / 10.0\n    vars = (sigmas * 2) ** 2\n    xg = g[0::3]\n    yg = g[1::3]\n    vg = g[2::3]\n    ious = np.zeros((d.shape[0]))\n    for n_d in range(0, d.shape[0]):\n        xd = d[n_d, 0::3]\n        yd = d[n_d, 1::3]\n        vd = d[n_d, 2::3]\n        dx = xd - xg\n        dy = yd - yg\n        e = (dx ** 2 + dy ** 2) / vars / ((a_g + a_d[n_d]) / 2 + np.spacing(1)) / 2\n        if in_vis_thre is not None:\n            ind = list(vg > in_vis_thre) and list(vd > in_vis_thre)\n            e = e[ind]\n        ious[n_d] = np.sum(np.exp(-e)) / e.shape[0] if e.shape[0] != 0 else 0.0\n    return ious\n\n\ndef oks_nms(kpts_db, thresh, sigmas=None, in_vis_thre=None):\n    """"""\n    greedily select boxes with high confidence and overlap with current maximum <= thresh\n    rule out overlap >= thresh, overlap = oks\n    :param kpts_db\n    :param thresh: retain overlap < thresh\n    :return: indexes to keep\n    """"""\n    if len(kpts_db) == 0:\n        return []\n\n    scores = np.array([kpts_db[i][\'score\'] for i in range(len(kpts_db))])\n    kpts = np.array([kpts_db[i][\'keypoints\'].flatten() for i in range(len(kpts_db))])\n    areas = np.array([kpts_db[i][\'area\'] for i in range(len(kpts_db))])\n\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n\n        oks_ovr = oks_iou(kpts[i], kpts[order[1:]], areas[i], areas[order[1:]], sigmas, in_vis_thre)\n\n        inds = np.where(oks_ovr <= thresh)[0]\n        order = order[inds + 1]\n\n    return keep\n\n\ndef rescore(overlap, scores, thresh, type=\'gaussian\'):\n    assert overlap.shape[0] == scores.shape[0]\n    if type == \'linear\':\n        inds = np.where(overlap >= thresh)[0]\n        scores[inds] = scores[inds] * (1 - overlap[inds])\n    else:\n        scores = scores * np.exp(- overlap**2 / thresh)\n\n    return scores\n\n\ndef soft_oks_nms(kpts_db, thresh, sigmas=None, in_vis_thre=None):\n    """"""\n    greedily select boxes with high confidence and overlap with current maximum <= thresh\n    rule out overlap >= thresh, overlap = oks\n    :param kpts_db\n    :param thresh: retain overlap < thresh\n    :return: indexes to keep\n    """"""\n    if len(kpts_db) == 0:\n        return []\n\n    scores = np.array([kpts_db[i][\'score\'] for i in range(len(kpts_db))])\n    kpts = np.array([kpts_db[i][\'keypoints\'].flatten() for i in range(len(kpts_db))])\n    areas = np.array([kpts_db[i][\'area\'] for i in range(len(kpts_db))])\n\n    order = scores.argsort()[::-1]\n    scores = scores[order]\n\n    # max_dets = order.size\n    max_dets = 20\n    keep = np.zeros(max_dets, dtype=np.intp)\n    keep_cnt = 0\n    while order.size > 0 and keep_cnt < max_dets:\n        i = order[0]\n\n        oks_ovr = oks_iou(kpts[i], kpts[order[1:]], areas[i], areas[order[1:]], sigmas, in_vis_thre)\n\n        order = order[1:]\n        scores = rescore(oks_ovr, scores[1:], thresh)\n\n        tmp = scores.argsort()[::-1]\n        order = order[tmp]\n        scores = scores[tmp]\n\n        keep[keep_cnt] = i\n        keep_cnt += 1\n\n    keep = keep[:keep_cnt]\n\n    return keep\n    # kpts_db = kpts_db[:keep_cnt]\n\n    # return kpts_db\n'"
mmskeleton/ops/nms/setup_linux.py,0,"b'# --------------------------------------------------------\n# Pose.gluon\n# Copyright (c) 2018-present Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Modified from py-faster-rcnn (https://github.com/rbgirshick/py-faster-rcnn)\n# --------------------------------------------------------\n\nimport os\nfrom os.path import join as pjoin\nfrom setuptools import setup\nfrom distutils.extension import Extension\nfrom Cython.Distutils import build_ext\nimport numpy as np\n\n\ndef find_in_path(name, path):\n    ""Find a file in a search path""\n    # Adapted fom\n    # http://code.activestate.com/recipes/52224-find-a-file-given-a-search-path/\n    for dir in path.split(os.pathsep):\n        binpath = pjoin(dir, name)\n        if os.path.exists(binpath):\n            return os.path.abspath(binpath)\n    return None\n\n\ndef locate_cuda():\n    """"""Locate the CUDA environment on the system\n    Returns a dict with keys \'home\', \'nvcc\', \'include\', and \'lib64\'\n    and values giving the absolute path to each directory.\n    Starts by looking for the CUDAHOME env variable. If not found, everything\n    is based on finding \'nvcc\' in the PATH.\n    """"""\n\n    # first check if the CUDAHOME env variable is in use\n    if \'CUDAHOME\' in os.environ:\n        home = os.environ[\'CUDAHOME\']\n        nvcc = pjoin(home, \'bin\', \'nvcc\')\n    else:\n        # otherwise, search the PATH for NVCC\n        default_path = pjoin(os.sep, \'usr\', \'local\', \'cuda\', \'bin\')\n        nvcc = find_in_path(\'nvcc\',\n                            os.environ[\'PATH\'] + os.pathsep + default_path)\n        if nvcc is None:\n            raise EnvironmentError(\n                \'The nvcc binary could not be \'\n                \'located in your $PATH. Either add it to your path, or set $CUDAHOME\'\n            )\n        home = os.path.dirname(os.path.dirname(nvcc))\n\n    cudaconfig = {\n        \'home\': home,\n        \'nvcc\': nvcc,\n        \'include\': pjoin(home, \'include\'),\n        \'lib64\': pjoin(home, \'lib64\')\n    }\n    for k, v in cudaconfig.items():\n        if not os.path.exists(v):\n            raise EnvironmentError(\n                \'The CUDA %s path could not be located in %s\' % (k, v))\n\n    return cudaconfig\n\n\nCUDA = locate_cuda()\n\n# Obtain the numpy include directory.  This logic works across numpy versions.\ntry:\n    numpy_include = np.get_include()\nexcept AttributeError:\n    numpy_include = np.get_numpy_include()\n\n\ndef customize_compiler_for_nvcc(self):\n    """"""inject deep into distutils to customize how the dispatch\n    to gcc/nvcc works.\n    If you subclass UnixCCompiler, it\'s not trivial to get your subclass\n    injected in, and still have the right customizations (i.e.\n    distutils.sysconfig.customize_compiler) run on it. So instead of going\n    the OO route, I have this. Note, it\'s kindof like a wierd functional\n    subclassing going on.""""""\n\n    # tell the compiler it can processes .cu\n    self.src_extensions.append(\'.cu\')\n\n    # save references to the default compiler_so and _comple methods\n    default_compiler_so = self.compiler_so\n    super = self._compile\n\n    # now redefine the _compile method. This gets executed for each\n    # object but distutils doesn\'t have the ability to change compilers\n    # based on source extension: we add it.\n    def _compile(obj, src, ext, cc_args, extra_postargs, pp_opts):\n        if os.path.splitext(src)[1] == \'.cu\':\n            # use the cuda for .cu files\n            self.set_executable(\'compiler_so\', CUDA[\'nvcc\'])\n            # use only a subset of the extra_postargs, which are 1-1 translated\n            # from the extra_compile_args in the Extension class\n            postargs = extra_postargs[\'nvcc\']\n        else:\n            postargs = extra_postargs[\'gcc\']\n\n        super(obj, src, ext, cc_args, postargs, pp_opts)\n        # reset the default compiler_so, which we might have changed for cuda\n        self.compiler_so = default_compiler_so\n\n    # inject our redefined _compile method into the class\n    self._compile = _compile\n\n\n# run the customize_compiler\nclass custom_build_ext(build_ext):\n    def build_extensions(self):\n        customize_compiler_for_nvcc(self.compiler)\n        build_ext.build_extensions(self)\n\n\next_modules = [\n    Extension(""cpu_nms"", [""cpu_nms.pyx""],\n              extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n              include_dirs=[numpy_include]),\n    Extension(\n        \'gpu_nms\',\n        [\'nms_kernel.cu\', \'gpu_nms.pyx\'],\n        library_dirs=[CUDA[\'lib64\']],\n        libraries=[\'cudart\'],\n        language=\'c++\',\n        runtime_library_dirs=[CUDA[\'lib64\']],\n        # this syntax is specific to this build system\n        # we\'re only going to use certain compiler args with nvcc and not with\n        # gcc the implementation of this trick is in customize_compiler() below\n        extra_compile_args={\n            \'gcc\': [""-Wno-unused-function""],\n            \'nvcc\': [\n                \'-arch=sm_35\', \'--ptxas-options=-v\', \'-c\',\n                \'--compiler-options\', ""\'-fPIC\'""\n            ]\n        },\n        include_dirs=[numpy_include, CUDA[\'include\']]),\n]\n\nif __name__ == ""__main__"":\n    pass\n    setup(\n        name=\'nms\',\n        ext_modules=ext_modules,\n        # inject our custom trigger\n        cmdclass={\'build_ext\': custom_build_ext},\n    )\n'"
mmskeleton/ops/st_gcn/__init__.py,0,b'from .gconv_origin import ConvTemporalGraphical\nfrom .graph import Graph'
mmskeleton/ops/st_gcn/gconv.py,2,"b""# The based unit of graph convolutional networks.\n\nimport torch\nimport torch.nn as nn\n\n\nclass GraphConvND(nn.Module):\n    def __init__(self, N, in_channels, out_channels, kernel_size, stride,\n                 padding, dilation, groups, bias, padding_mode):\n\n        graph_kernel_size = kernel_size[0]\n        graph_stride = stride[0]\n        graph_padding = padding[0]\n        graph_dilation = dilation[0]\n\n        if graph_stride != 1 or graph_padding != 0 or graph_dilation != 1:\n            raise NotImplementedError\n\n        if N == 1:\n            conv_type = nn.Conv1d\n            self.einsum_func = 'nkcv,kvw->ncw'\n        elif N == 2:\n            conv_type = nn.Conv2d\n            self.einsum_func = 'nkcvx,kvw->ncwx'\n        elif N == 3:\n            conv_type = nn.Conv3d\n            self.einsum_func = 'nkcvxy,kvw->ncwxy'\n\n        self.out_channels = out_channels\n        self.graph_kernel_size = graph_kernel_size\n        self.conv = conv_type(in_channels,\n                              out_channels * graph_kernel_size,\n                              kernel_size=[1] + kernel_size[1:],\n                              stride=[1] + stride[1:],\n                              padding=[0] + padding[1:],\n                              dilation=[1] + dilation[1:],\n                              groups=groups,\n                              bias=bias,\n                              padding_mode=padding_mode)\n\n    def forward(self, x, graph):\n\n        # graph is an adjacency matrix\n        if graph.dim() == 2:\n            A, out_graph = self.normalize_adjacency_matrix(graph)\n\n        # graph is a weight matrix\n        elif graph.dim() == 3:\n            A, out_graph = graph, None\n\n        else:\n            raise ValueError('input[1].dim() should be 2 or 3.')\n\n        x = self.conv(x)\n        x = x.view((x.size(0), self.graph_kernel_size, self.out_channels) +\n                   x.size()[2:])\n        x = torch.einsum(self.einsum_func, (x, A))\n\n        return x.contiguous(), out_graph\n\n    def normalize_adjacency_matrix(self, graph):\n        raise NotImplementedError\n        return None, graph\n\n\nclass GraphConv(GraphConvND):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 groups=1,\n                 bias=True,\n                 padding_mode='zeros'):\n\n        super().__init__(1, in_channels, out_channels, kernel_size, stride,\n                         padding, dilation, groups, bias, padding_mode)\n\n\nclass GraphConv2D(GraphConvND):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=(1, 1),\n                 padding=(0, 0),\n                 dilation=(1, 1),\n                 groups=1,\n                 bias=True,\n                 padding_mode='zeros'):\n\n        super().__init__(2, in_channels, out_channels, kernel_size, stride,\n                         padding, dilation, groups, bias, padding_mode)\n\n\nclass GraphConv3D(GraphConvND):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=(1, 1, 1),\n                 padding=(0, 0, 0),\n                 dilation=(1, 1, 1),\n                 groups=1,\n                 bias=True,\n                 padding_mode='zeros'):\n\n        super().__init__(3, in_channels, out_channels, kernel_size, stride,\n                         padding, dilation, groups, bias, padding_mode)"""
mmskeleton/ops/st_gcn/gconv_origin.py,2,"b'# The based unit of graph convolutional networks.\n# This is the original implementation for ST-GCN papers.\n\nimport torch\nimport torch.nn as nn\n\n\nclass ConvTemporalGraphical(nn.Module):\n    r""""""The basic module for applying a graph convolution.\n\n    Args:\n        in_channels (int): Number of channels in the input sequence data\n        out_channels (int): Number of channels produced by the convolution\n        kernel_size (int): Size of the graph convolving kernel\n        t_kernel_size (int): Size of the temporal convolving kernel\n        t_stride (int, optional): Stride of the temporal convolution. Default: 1\n        t_padding (int, optional): Temporal zero-padding added to both sides of\n            the input. Default: 0\n        t_dilation (int, optional): Spacing between temporal kernel elements.\n            Default: 1\n        bias (bool, optional): If ``True``, adds a learnable bias to the output.\n            Default: ``True``\n\n    Shape:\n        - Input[0]: Input graph sequence in :math:`(N, in_channels, T_{in}, V)` format\n        - Input[1]: Input graph adjacency matrix in :math:`(K, V, V)` format\n        - Output[0]: Output graph sequence in :math:`(N, out_channels, T_{out}, V)` format\n        - Output[1]: Graph adjacency matrix for output data in :math:`(K, V, V)` format\n\n        where\n            :math:`N` is a batch size,\n            :math:`K` is the spatial kernel size, as :math:`K == kernel_size[1]`,\n            :math:`T_{in}/T_{out}` is a length of input/output sequence,\n            :math:`V` is the number of graph nodes. \n    """"""\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 t_kernel_size=1,\n                 t_stride=1,\n                 t_padding=0,\n                 t_dilation=1,\n                 bias=True):\n        super().__init__()\n\n        self.kernel_size = kernel_size\n        self.conv = nn.Conv2d(in_channels,\n                              out_channels * kernel_size,\n                              kernel_size=(t_kernel_size, 1),\n                              padding=(t_padding, 0),\n                              stride=(t_stride, 1),\n                              dilation=(t_dilation, 1),\n                              bias=bias)\n\n    def forward(self, x, A):\n        assert A.size(0) == self.kernel_size\n\n        x = self.conv(x)\n\n        n, kc, t, v = x.size()\n        x = x.view(n, self.kernel_size, kc // self.kernel_size, t, v)\n        x = torch.einsum(\'nkctv,kvw->nctw\', (x, A))\n\n        return x.contiguous(), A\n\n\nclass Gconv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size):\n        if isinstance(kernel_size, int):\n            gcn_kernel_size = kernel_size\n            feature_dim = 0\n        if isinstance(kernel_size, list) or isinstance(kernel_size, tuple):\n            gcn_kernel_size = kernel_size[0]\n            cnn_kernel_size = [1] + kernel_size[1:]\n            feature_dim = len(kernel_size) - 1\n        else:\n            raise ValueError(\n                \'The type of kernel_size should be int, list or tuple.\')\n\n        if feature_dim == 1:\n            self.conv = nn.Conv1d(in_channels,\n                                  out_channels * gcn_kernel_size,\n                                  kernel_size=cnn_kernel_size)\n        elif feature_dim == 2:\n            pass\n        elif feature_dim == 3:\n            pass\n        elif feature_dim == 0:\n            pass\n        else:\n            raise ValueError(\n                \'The length of kernel_size should be 1, 2, 3, or 4\')\n\n    def forward(self, X, A):\n        pass'"
mmskeleton/ops/st_gcn/graph.py,0,"b'import numpy as np\n\n\nclass Graph():\n    """""" The Graph to model the skeletons extracted by the openpose\n\n    Args:\n        strategy (string): must be one of the follow candidates\n        - uniform: Uniform Labeling\n        - distance: Distance Partitioning\n        - spatial: Spatial Configuration\n        For more information, please refer to the section \'Partition Strategies\'\n            in our paper (https://arxiv.org/abs/1801.07455).\n\n        layout (string): must be one of the follow candidates\n        - openpose: Is consists of 18 joints. For more information, please\n            refer to https://github.com/CMU-Perceptual-Computing-Lab/openpose#output\n        - ntu-rgb+d: Is consists of 25 joints. For more information, please\n            refer to https://github.com/shahroudy/NTURGB-D\n\n        max_hop (int): the maximal distance between two connected nodes\n        dilation (int): controls the spacing between the kernel points\n\n    """"""\n    def __init__(self,\n                 layout=\'openpose\',\n                 strategy=\'uniform\',\n                 max_hop=1,\n                 dilation=1):\n        self.max_hop = max_hop\n        self.dilation = dilation\n\n        self.get_edge(layout)\n        self.hop_dis = get_hop_distance(self.num_node,\n                                        self.edge,\n                                        max_hop=max_hop)\n        self.get_adjacency(strategy)\n\n    def __str__(self):\n        return self.A\n\n    def get_edge(self, layout):\n        # edge is a list of [child, parent] paris\n\n        if layout == \'openpose\':\n            self.num_node = 18\n            self_link = [(i, i) for i in range(self.num_node)]\n            neighbor_link = [(4, 3), (3, 2), (7, 6), (6, 5),\n                             (13, 12), (12, 11), (10, 9), (9, 8), (11, 5),\n                             (8, 2), (5, 1), (2, 1), (0, 1), (15, 0), (14, 0),\n                             (17, 15), (16, 14)]\n            self.edge = self_link + neighbor_link\n            self.center = 1\n        elif layout == \'ntu-rgb+d\':\n            self.num_node = 25\n            self_link = [(i, i) for i in range(self.num_node)]\n            neighbor_1base = [(1, 2), (2, 21), (3, 21),\n                              (4, 3), (5, 21), (6, 5), (7, 6), (8, 7), (9, 21),\n                              (10, 9), (11, 10), (12, 11), (13, 1), (14, 13),\n                              (15, 14), (16, 15), (17, 1), (18, 17), (19, 18),\n                              (20, 19), (22, 23), (23, 8), (24, 25), (25, 12)]\n            neighbor_link = [(i - 1, j - 1) for (i, j) in neighbor_1base]\n            self.edge = self_link + neighbor_link\n            self.center = 21 - 1\n        elif layout == \'ntu_edge\':\n            self.num_node = 24\n            self_link = [(i, i) for i in range(self.num_node)]\n            neighbor_1base = [(1, 2), (3, 2), (4, 3), (5, 2), (6, 5), (7, 6),\n                              (8, 7), (9, 2), (10, 9), (11, 10), (12, 11),\n                              (13, 1), (14, 13), (15, 14), (16, 15), (17, 1),\n                              (18, 17), (19, 18), (20, 19), (21, 22), (22, 8),\n                              (23, 24), (24, 12)]\n            neighbor_link = [(i - 1, j - 1) for (i, j) in neighbor_1base]\n            self.edge = self_link + neighbor_link\n            self.center = 2\n        elif layout == \'coco\':\n            self.num_node = 17\n            self_link = [(i, i) for i in range(self.num_node)]\n            neighbor_1base = [[16, 14], [14, 12], [17, 15], [15, 13], [12, 13],\n                              [6, 12], [7, 13], [6, 7], [8, 6], [9, 7],\n                              [10, 8], [11, 9], [2, 3], [2, 1], [3, 1], [4, 2],\n                              [5, 3], [4, 6], [5, 7]]\n            neighbor_link = [(i - 1, j - 1) for (i, j) in neighbor_1base]\n            self.edge = self_link + neighbor_link\n            self.center = 0\n        # elif layout==\'customer settings\'\n        #     pass\n        else:\n            raise ValueError(""Do Not Exist This Layout."")\n\n    def get_adjacency(self, strategy):\n        valid_hop = range(0, self.max_hop + 1, self.dilation)\n        adjacency = np.zeros((self.num_node, self.num_node))\n        for hop in valid_hop:\n            adjacency[self.hop_dis == hop] = 1\n        normalize_adjacency = normalize_digraph(adjacency)\n\n        if strategy == \'uniform\':\n            A = np.zeros((1, self.num_node, self.num_node))\n            A[0] = normalize_adjacency\n            self.A = A\n        elif strategy == \'distance\':\n            A = np.zeros((len(valid_hop), self.num_node, self.num_node))\n            for i, hop in enumerate(valid_hop):\n                A[i][self.hop_dis == hop] = normalize_adjacency[self.hop_dis ==\n                                                                hop]\n            self.A = A\n        elif strategy == \'spatial\':\n            A = []\n            for hop in valid_hop:\n                a_root = np.zeros((self.num_node, self.num_node))\n                a_close = np.zeros((self.num_node, self.num_node))\n                a_further = np.zeros((self.num_node, self.num_node))\n                for i in range(self.num_node):\n                    for j in range(self.num_node):\n                        if self.hop_dis[j, i] == hop:\n                            if self.hop_dis[j, self.center] == self.hop_dis[\n                                    i, self.center]:\n                                a_root[j, i] = normalize_adjacency[j, i]\n                            elif self.hop_dis[j, self.center] > self.hop_dis[\n                                    i, self.center]:\n                                a_close[j, i] = normalize_adjacency[j, i]\n                            else:\n                                a_further[j, i] = normalize_adjacency[j, i]\n                if hop == 0:\n                    A.append(a_root)\n                else:\n                    A.append(a_root + a_close)\n                    A.append(a_further)\n            A = np.stack(A)\n            self.A = A\n        else:\n            raise ValueError(""Do Not Exist This Strategy"")\n\n\ndef get_hop_distance(num_node, edge, max_hop=1):\n    A = np.zeros((num_node, num_node))\n    for i, j in edge:\n        A[j, i] = 1\n        A[i, j] = 1\n\n    # compute hop steps\n    hop_dis = np.zeros((num_node, num_node)) + np.inf\n    transfer_mat = [np.linalg.matrix_power(A, d) for d in range(max_hop + 1)]\n    arrive_mat = (np.stack(transfer_mat) > 0)\n    for d in range(max_hop, -1, -1):\n        hop_dis[arrive_mat[d]] = d\n    return hop_dis\n\n\ndef normalize_digraph(A):\n    Dl = np.sum(A, 0)\n    num_node = A.shape[0]\n    Dn = np.zeros((num_node, num_node))\n    for i in range(num_node):\n        if Dl[i] > 0:\n            Dn[i, i] = Dl[i]**(-1)\n    AD = np.dot(A, Dn)\n    return AD\n\n\ndef normalize_undigraph(A):\n    Dl = np.sum(A, 0)\n    num_node = A.shape[0]\n    Dn = np.zeros((num_node, num_node))\n    for i in range(num_node):\n        if Dl[i] > 0:\n            Dn[i, i] = Dl[i]**(-0.5)\n    DAD = np.dot(np.dot(Dn, A), Dn)\n    return DAD'"
mmskeleton/processor/utils/__init__.py,0,b'from . import infernce_utils'
mmskeleton/processor/utils/infernce_utils.py,0,"b""# ------------------------------------------------------------------------------\r\n# Copyright (c) Microsoft\r\n# Licensed under the MIT License.\r\n# Written by Bin Xiao (Bin.Xiao@microsoft.com)\r\n# ------------------------------------------------------------------------------\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport math\r\n\r\nimport numpy as np\r\n\r\nfrom mmskeleton.datasets.utils.coco_transform  import transform_preds\r\n\r\n\r\ndef get_max_preds(batch_heatmaps):\r\n    '''\r\n    get predictions from score maps\r\n    heatmaps: numpy.ndarray([batch_size, num_joints, height, width])\r\n    '''\r\n    assert isinstance(batch_heatmaps, np.ndarray), \\\r\n        'batch_heatmaps should be numpy.ndarray'\r\n    assert batch_heatmaps.ndim == 4, 'batch_images should be 4-ndim'\r\n\r\n    batch_size = batch_heatmaps.shape[0]\r\n    num_joints = batch_heatmaps.shape[1]\r\n    width = batch_heatmaps.shape[3]\r\n    heatmaps_reshaped = batch_heatmaps.reshape((batch_size, num_joints, -1))\r\n    idx = np.argmax(heatmaps_reshaped, 2)\r\n    maxvals = np.amax(heatmaps_reshaped, 2)\r\n\r\n    maxvals = maxvals.reshape((batch_size, num_joints, 1))\r\n    idx = idx.reshape((batch_size, num_joints, 1))\r\n\r\n    preds = np.tile(idx, (1, 1, 2)).astype(np.float32)\r\n\r\n    preds[:, :, 0] = (preds[:, :, 0]) % width\r\n    preds[:, :, 1] = np.floor((preds[:, :, 1]) / width)\r\n\r\n    pred_mask = np.tile(np.greater(maxvals, 0.0), (1, 1, 2))\r\n    pred_mask = pred_mask.astype(np.float32)\r\n\r\n    preds *= pred_mask\r\n    return preds, maxvals\r\n\r\n\r\ndef get_final_preds(post_process, batch_heatmaps, center, scale):\r\n    coords, maxvals = get_max_preds(batch_heatmaps)\r\n\r\n    heatmap_height = batch_heatmaps.shape[2]\r\n    heatmap_width = batch_heatmaps.shape[3]\r\n\r\n    # post-processing\r\n    if post_process:\r\n        for n in range(coords.shape[0]):\r\n            for p in range(coords.shape[1]):\r\n                hm = batch_heatmaps[n][p]\r\n                px = int(math.floor(coords[n][p][0] + 0.5))\r\n                py = int(math.floor(coords[n][p][1] + 0.5))\r\n                if 1 < px < heatmap_width-1 and 1 < py < heatmap_height-1:\r\n                    diff = np.array(\r\n                        [\r\n                            hm[py][px+1] - hm[py][px-1],\r\n                            hm[py+1][px]-hm[py-1][px]\r\n                        ]\r\n                    )\r\n                    coords[n][p] += np.sign(diff) * .25\r\n\r\n    preds = coords.copy()\r\n\r\n    # Transform back\r\n    for i in range(coords.shape[0]):\r\n        preds[i] = transform_preds(\r\n            coords[i], center[i], scale[i], [heatmap_width, heatmap_height]\r\n        )\r\n\r\n    return preds, maxvals\r\n"""
deprecated/origin_stgcn_repo/net/utils/__init__.py,0,b''
deprecated/origin_stgcn_repo/net/utils/graph.py,0,"b'import numpy as np\n\n\nclass Graph():\n    """""" The Graph to model the skeletons extracted by the openpose\n\n    Args:\n        strategy (string): must be one of the follow candidates\n        - uniform: Uniform Labeling\n        - distance: Distance Partitioning\n        - spatial: Spatial Configuration\n        For more information, please refer to the section \'Partition Strategies\'\n            in our paper (https://arxiv.org/abs/1801.07455).\n\n        layout (string): must be one of the follow candidates\n        - openpose: Is consists of 18 joints. For more information, please\n            refer to https://github.com/CMU-Perceptual-Computing-Lab/openpose#output\n        - ntu-rgb+d: Is consists of 25 joints. For more information, please\n            refer to https://github.com/shahroudy/NTURGB-D\n\n        max_hop (int): the maximal distance between two connected nodes\n        dilation (int): controls the spacing between the kernel points\n\n    """"""\n    def __init__(self,\n                 layout=\'openpose\',\n                 strategy=\'uniform\',\n                 max_hop=1,\n                 dilation=1):\n        self.max_hop = max_hop\n        self.dilation = dilation\n\n        self.get_edge(layout)\n        self.hop_dis = get_hop_distance(self.num_node,\n                                        self.edge,\n                                        max_hop=max_hop)\n        self.get_adjacency(strategy)\n\n    def __str__(self):\n        return self.A\n\n    def get_edge(self, layout):\n        if layout == \'openpose\':\n            self.num_node = 18\n            self_link = [(i, i) for i in range(self.num_node)]\n            neighbor_link = [(4, 3), (3, 2), (7, 6), (6, 5),\n                             (13, 12), (12, 11), (10, 9), (9, 8), (11, 5),\n                             (8, 2), (5, 1), (2, 1), (0, 1), (15, 0), (14, 0),\n                             (17, 15), (16, 14)]\n            self.edge = self_link + neighbor_link\n            self.center = 1\n        elif layout == \'ntu-rgb+d\':\n            self.num_node = 25\n            self_link = [(i, i) for i in range(self.num_node)]\n            neighbor_1base = [(1, 2), (2, 21), (3, 21),\n                              (4, 3), (5, 21), (6, 5), (7, 6), (8, 7), (9, 21),\n                              (10, 9), (11, 10), (12, 11), (13, 1), (14, 13),\n                              (15, 14), (16, 15), (17, 1), (18, 17), (19, 18),\n                              (20, 19), (22, 23), (23, 8), (24, 25), (25, 12)]\n            neighbor_link = [(i - 1, j - 1) for (i, j) in neighbor_1base]\n            self.edge = self_link + neighbor_link\n            self.center = 21 - 1\n        elif layout == \'ntu_edge\':\n            self.num_node = 24\n            self_link = [(i, i) for i in range(self.num_node)]\n            neighbor_1base = [(1, 2), (3, 2), (4, 3), (5, 2), (6, 5), (7, 6),\n                              (8, 7), (9, 2), (10, 9), (11, 10), (12, 11),\n                              (13, 1), (14, 13), (15, 14), (16, 15), (17, 1),\n                              (18, 17), (19, 18), (20, 19), (21, 22), (22, 8),\n                              (23, 24), (24, 12)]\n            neighbor_link = [(i - 1, j - 1) for (i, j) in neighbor_1base]\n            self.edge = self_link + neighbor_link\n            self.center = 2\n        # elif layout==\'customer settings\'\n        #     pass\n        else:\n            raise ValueError(""Do Not Exist This Layout."")\n\n    def get_adjacency(self, strategy):\n        valid_hop = range(0, self.max_hop + 1, self.dilation)\n        adjacency = np.zeros((self.num_node, self.num_node))\n        for hop in valid_hop:\n            adjacency[self.hop_dis == hop] = 1\n        normalize_adjacency = normalize_digraph(adjacency)\n\n        if strategy == \'uniform\':\n            A = np.zeros((1, self.num_node, self.num_node))\n            A[0] = normalize_adjacency\n            self.A = A\n        elif strategy == \'distance\':\n            A = np.zeros((len(valid_hop), self.num_node, self.num_node))\n            for i, hop in enumerate(valid_hop):\n                A[i][self.hop_dis == hop] = normalize_adjacency[self.hop_dis ==\n                                                                hop]\n            self.A = A\n        elif strategy == \'spatial\':\n            A = []\n            for hop in valid_hop:\n                a_root = np.zeros((self.num_node, self.num_node))\n                a_close = np.zeros((self.num_node, self.num_node))\n                a_further = np.zeros((self.num_node, self.num_node))\n                for i in range(self.num_node):\n                    for j in range(self.num_node):\n                        if self.hop_dis[j, i] == hop:\n                            if self.hop_dis[j, self.center] == self.hop_dis[\n                                    i, self.center]:\n                                a_root[j, i] = normalize_adjacency[j, i]\n                            elif self.hop_dis[j, self.center] > self.hop_dis[\n                                    i, self.center]:\n                                a_close[j, i] = normalize_adjacency[j, i]\n                            else:\n                                a_further[j, i] = normalize_adjacency[j, i]\n                if hop == 0:\n                    A.append(a_root)\n                else:\n                    A.append(a_root + a_close)\n                    A.append(a_further)\n            A = np.stack(A)\n            self.A = A\n        else:\n            raise ValueError(""Do Not Exist This Strategy"")\n\n\ndef get_hop_distance(num_node, edge, max_hop=1):\n    A = np.zeros((num_node, num_node))\n    for i, j in edge:\n        A[j, i] = 1\n        A[i, j] = 1\n\n    # compute hop steps\n    hop_dis = np.zeros((num_node, num_node)) + np.inf\n    transfer_mat = [np.linalg.matrix_power(A, d) for d in range(max_hop + 1)]\n    arrive_mat = (np.stack(transfer_mat) > 0)\n    for d in range(max_hop, -1, -1):\n        hop_dis[arrive_mat[d]] = d\n    return hop_dis\n\n\ndef normalize_digraph(A):\n    Dl = np.sum(A, 0)\n    num_node = A.shape[0]\n    Dn = np.zeros((num_node, num_node))\n    for i in range(num_node):\n        if Dl[i] > 0:\n            Dn[i, i] = Dl[i]**(-1)\n    AD = np.dot(A, Dn)\n    return AD\n\n\ndef normalize_undigraph(A):\n    Dl = np.sum(A, 0)\n    num_node = A.shape[0]\n    Dn = np.zeros((num_node, num_node))\n    for i in range(num_node):\n        if Dl[i] > 0:\n            Dn[i, i] = Dl[i]**(-0.5)\n    DAD = np.dot(np.dot(Dn, A), Dn)\n    return DAD'"
deprecated/origin_stgcn_repo/net/utils/tgcn.py,2,"b'# The based unit of graph convolutional networks.\n\nimport torch\nimport torch.nn as nn\n\n\nclass ConvTemporalGraphical(nn.Module):\n    r""""""The basic module for applying a graph convolution.\n\n    Args:\n        in_channels (int): Number of channels in the input sequence data\n        out_channels (int): Number of channels produced by the convolution\n        kernel_size (int): Size of the graph convolving kernel\n        t_kernel_size (int): Size of the temporal convolving kernel\n        t_stride (int, optional): Stride of the temporal convolution. Default: 1\n        t_padding (int, optional): Temporal zero-padding added to both sides of\n            the input. Default: 0\n        t_dilation (int, optional): Spacing between temporal kernel elements.\n            Default: 1\n        bias (bool, optional): If ``True``, adds a learnable bias to the output.\n            Default: ``True``\n\n    Shape:\n        - Input[0]: Input graph sequence in :math:`(N, in_channels, T_{in}, V)` format\n        - Input[1]: Input graph adjacency matrix in :math:`(K, V, V)` format\n        - Output[0]: Outpu graph sequence in :math:`(N, out_channels, T_{out}, V)` format\n        - Output[1]: Graph adjacency matrix for output data in :math:`(K, V, V)` format\n\n        where\n            :math:`N` is a batch size,\n            :math:`K` is the spatial kernel size, as :math:`K == kernel_size[1]`,\n            :math:`T_{in}/T_{out}` is a length of input/output sequence,\n            :math:`V` is the number of graph nodes. \n    """"""\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 t_kernel_size=1,\n                 t_stride=1,\n                 t_padding=0,\n                 t_dilation=1,\n                 bias=True):\n        super().__init__()\n\n        self.kernel_size = kernel_size\n        self.conv = nn.Conv2d(in_channels,\n                              out_channels * kernel_size,\n                              kernel_size=(t_kernel_size, 1),\n                              padding=(t_padding, 0),\n                              stride=(t_stride, 1),\n                              dilation=(t_dilation, 1),\n                              bias=bias)\n\n    def forward(self, x, A):\n        assert A.size(0) == self.kernel_size\n\n        x = self.conv(x)\n\n        n, kc, t, v = x.size()\n        x = x.view(n, self.kernel_size, kc // self.kernel_size, t, v)\n        x = torch.einsum(\'nkctv,kvw->nctw\', (x, A))\n\n        return x.contiguous(), A\n'"
deprecated/origin_stgcn_repo/tools/utils/__init__.py,0,b'from . import video\nfrom . import openpose\nfrom . import visualization'
deprecated/origin_stgcn_repo/tools/utils/ntu_read_skeleton.py,0,"b""import numpy as np\nimport os\n\n\ndef read_skeleton(file):\n    with open(file, 'r') as f:\n        skeleton_sequence = {}\n        skeleton_sequence['numFrame'] = int(f.readline())\n        skeleton_sequence['frameInfo'] = []\n        for t in range(skeleton_sequence['numFrame']):\n            frame_info = {}\n            frame_info['numBody'] = int(f.readline())\n            frame_info['bodyInfo'] = []\n            for m in range(frame_info['numBody']):\n                body_info = {}\n                body_info_key = [\n                    'bodyID', 'clipedEdges', 'handLeftConfidence',\n                    'handLeftState', 'handRightConfidence', 'handRightState',\n                    'isResticted', 'leanX', 'leanY', 'trackingState'\n                ]\n                body_info = {\n                    k: float(v)\n                    for k, v in zip(body_info_key,\n                                    f.readline().split())\n                }\n                body_info['numJoint'] = int(f.readline())\n                body_info['jointInfo'] = []\n                for v in range(body_info['numJoint']):\n                    joint_info_key = [\n                        'x', 'y', 'z', 'depthX', 'depthY', 'colorX', 'colorY',\n                        'orientationW', 'orientationX', 'orientationY',\n                        'orientationZ', 'trackingState'\n                    ]\n                    joint_info = {\n                        k: float(v)\n                        for k, v in zip(joint_info_key,\n                                        f.readline().split())\n                    }\n                    body_info['jointInfo'].append(joint_info)\n                frame_info['bodyInfo'].append(body_info)\n            skeleton_sequence['frameInfo'].append(frame_info)\n    return skeleton_sequence\n\n\ndef read_xyz(file, max_body=2, num_joint=25):\n    seq_info = read_skeleton(file)\n    data = np.zeros((3, seq_info['numFrame'], num_joint, max_body))\n    for n, f in enumerate(seq_info['frameInfo']):\n        for m, b in enumerate(f['bodyInfo']):\n            for j, v in enumerate(b['jointInfo']):\n                if m < max_body and j < num_joint:\n                    data[:, n, j, m] = [v['x'], v['y'], v['z']]\n                else:\n                    pass\n    return data"""
deprecated/origin_stgcn_repo/tools/utils/openpose.py,0,"b""from pathlib import Path\nimport json\n\n\ndef json_pack(snippets_dir,\n              video_name,\n              frame_width,\n              frame_height,\n              label='unknown',\n              label_index=-1):\n    sequence_info = []\n    p = Path(snippets_dir)\n    for path in p.glob(video_name + '*.json'):\n        json_path = str(path)\n        print(path)\n        frame_id = int(path.stem.split('_')[-2])\n        frame_data = {'frame_index': frame_id}\n        data = json.load(open(json_path))\n        skeletons = []\n        for person in data['people']:\n            score, coordinates = [], []\n            skeleton = {}\n            keypoints = person['pose_keypoints_2d']\n            for i in range(0, len(keypoints), 3):\n                coordinates += [\n                    keypoints[i] / frame_width, keypoints[i + 1] / frame_height\n                ]\n                score += [keypoints[i + 2]]\n            skeleton['pose'] = coordinates\n            skeleton['score'] = score\n            skeletons += [skeleton]\n        frame_data['skeleton'] = skeletons\n        sequence_info += [frame_data]\n\n    video_info = dict()\n    video_info['data'] = sequence_info\n    video_info['label'] = label\n    video_info['label_index'] = label_index\n\n    return video_info"""
deprecated/origin_stgcn_repo/tools/utils/video.py,0,"b'import skvideo.io\nimport numpy as np\nimport cv2\n\n\ndef video_info_parsing(video_info, num_person_in=5, num_person_out=2):\n    data_numpy = np.zeros((3, len(video_info[\'data\']), 18, num_person_in))\n    for frame_info in video_info[\'data\']:\n        frame_index = frame_info[\'frame_index\']\n        for m, skeleton_info in enumerate(frame_info[""skeleton""]):\n            if m >= num_person_in:\n                break\n            pose = skeleton_info[\'pose\']\n            score = skeleton_info[\'score\']\n            data_numpy[0, frame_index, :, m] = pose[0::2]\n            data_numpy[1, frame_index, :, m] = pose[1::2]\n            data_numpy[2, frame_index, :, m] = score\n\n    # centralization\n    data_numpy[0:2] = data_numpy[0:2] - 0.5\n    data_numpy[0][data_numpy[2] == 0] = 0\n    data_numpy[1][data_numpy[2] == 0] = 0\n\n    sort_index = (-data_numpy[2, :, :, :].sum(axis=1)).argsort(axis=1)\n    for t, s in enumerate(sort_index):\n        data_numpy[:, t, :, :] = data_numpy[:, t, :, s].transpose((1, 2, 0))\n    data_numpy = data_numpy[:, :, :, :num_person_out]\n\n    label = video_info[\'label_index\']\n    return data_numpy, label\n\n\ndef get_video_frames(video_path):\n    vread = skvideo.io.vread(video_path)\n    video = []\n    for frame in vread:\n        video.append(frame)\n    return video\n\n\ndef video_play(video_path, fps=30):\n    cap = cv2.VideoCapture(video_path)\n\n    while (cap.isOpened()):\n        ret, frame = cap.read()\n\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n        cv2.imshow(\'frame\', gray)\n        if cv2.waitKey(1000 / fps) & 0xFF == ord(\'q\'):\n            break\n\n    cap.release()\n    cv2.destroyAllWindows()'"
deprecated/origin_stgcn_repo/tools/utils/visualization.py,0,"b""import cv2\nimport numpy as np\n\n\ndef stgcn_visualize(pose,\n                    edge,\n                    feature,\n                    video,\n                    label=None,\n                    label_sequence=None,\n                    height=1080,\n                    fps=None):\n\n    _, T, V, M = pose.shape\n    T = len(video)\n    pos_track = [None] * M\n    for t in range(T):\n        frame = video[t]\n\n        # image resize\n        H, W, c = frame.shape\n        frame = cv2.resize(frame, (height * W // H // 2, height // 2))\n        H, W, c = frame.shape\n        scale_factor = 2 * height / 1080\n\n        # draw skeleton\n        skeleton = frame * 0\n        text = frame * 0\n        for m in range(M):\n\n            score = pose[2, t, :, m].max()\n            if score < 0.3:\n                continue\n\n            for i, j in edge:\n                xi = pose[0, t, i, m]\n                yi = pose[1, t, i, m]\n                xj = pose[0, t, j, m]\n                yj = pose[1, t, j, m]\n                if xi + yi == 0 or xj + yj == 0:\n                    continue\n                else:\n                    xi = int((xi + 0.5) * W)\n                    yi = int((yi + 0.5) * H)\n                    xj = int((xj + 0.5) * W)\n                    yj = int((yj + 0.5) * H)\n                cv2.line(skeleton, (xi, yi), (xj, yj), (255, 255, 255),\n                         int(np.ceil(2 * scale_factor)))\n\n            if label_sequence is not None:\n                body_label = label_sequence[t // 4][m]\n            else:\n                body_label = ''\n            x_nose = int((pose[0, t, 0, m] + 0.5) * W)\n            y_nose = int((pose[1, t, 0, m] + 0.5) * H)\n            x_neck = int((pose[0, t, 1, m] + 0.5) * W)\n            y_neck = int((pose[1, t, 1, m] + 0.5) * H)\n\n            half_head = int(((x_neck - x_nose)**2 + (y_neck - y_nose)**2)**0.5)\n            pos = (x_nose + half_head, y_nose - half_head)\n            if pos_track[m] is None:\n                pos_track[m] = pos\n            else:\n                new_x = int(pos_track[m][0] + (pos[0] - pos_track[m][0]) * 0.2)\n                new_y = int(pos_track[m][1] + (pos[1] - pos_track[m][1]) * 0.2)\n                pos_track[m] = (new_x, new_y)\n            cv2.putText(text, body_label, pos_track[m],\n                        cv2.FONT_HERSHEY_TRIPLEX, 0.5 * scale_factor,\n                        (255, 255, 255))\n\n        # generate mask\n        mask = frame * 0\n        feature = np.abs(feature)\n        feature = feature / feature.mean()\n        for m in range(M):\n            score = pose[2, t, :, m].max()\n            if score < 0.3:\n                continue\n\n            f = feature[t // 4, :, m]**5\n            if f.mean() != 0:\n                f = f / f.mean()\n            for v in range(V):\n                x = pose[0, t, v, m]\n                y = pose[1, t, v, m]\n                if x + y == 0:\n                    continue\n                else:\n                    x = int((x + 0.5) * W)\n                    y = int((y + 0.5) * H)\n                cv2.circle(mask, (x, y), 0, (255, 255, 255),\n                           int(np.ceil(f[v]**0.5 * 8 * scale_factor)))\n        blurred_mask = cv2.blur(mask, (12, 12))\n\n        skeleton_result = blurred_mask.astype(float) * 0.75\n        skeleton_result += skeleton.astype(float) * 0.25\n        skeleton_result += text.astype(float)\n        skeleton_result[skeleton_result > 255] = 255\n        skeleton_result.astype(np.uint8)\n\n        rgb_result = blurred_mask.astype(float) * 0.75\n        rgb_result += frame.astype(float) * 0.5\n        rgb_result += skeleton.astype(float) * 0.25\n        rgb_result[rgb_result > 255] = 255\n        rgb_result.astype(np.uint8)\n\n        put_text(skeleton, 'inputs of st-gcn', (0.15, 0.5))\n\n        text_1 = cv2.imread('./resource/demo_asset/original_video.png',\n                            cv2.IMREAD_UNCHANGED)\n        text_2 = cv2.imread('./resource/demo_asset/pose_estimation.png',\n                            cv2.IMREAD_UNCHANGED)\n        text_3 = cv2.imread('./resource/demo_asset/attention+prediction.png',\n                            cv2.IMREAD_UNCHANGED)\n        text_4 = cv2.imread('./resource/demo_asset/attention+rgb.png',\n                            cv2.IMREAD_UNCHANGED)\n\n        try:\n            blend(frame, text_1)\n            blend(skeleton, text_2)\n            blend(skeleton_result, text_3)\n            blend(rgb_result, text_4)\n        except:\n            pass\n\n        if label is not None:\n            label_name = 'voting result: ' + label\n            put_text(skeleton_result, label_name, (0.1, 0.5))\n\n        if fps is not None:\n            put_text(skeleton, 'fps:{:.2f}'.format(fps), (0.9, 0.5))\n\n        img0 = np.concatenate((frame, skeleton), axis=1)\n        img1 = np.concatenate((skeleton_result, rgb_result), axis=1)\n        img = np.concatenate((img0, img1), axis=0)\n\n        yield img\n\n\ndef put_text(img, text, position, scale_factor=1):\n    t_w, t_h = cv2.getTextSize(text,\n                               cv2.FONT_HERSHEY_TRIPLEX,\n                               scale_factor,\n                               thickness=1)[0]\n    H, W, _ = img.shape\n    position = (int(W * position[1] - t_w * 0.5),\n                int(H * position[0] - t_h * 0.5))\n    params = (position, cv2.FONT_HERSHEY_TRIPLEX, scale_factor, (255, 255,\n                                                                 255))\n    cv2.putText(img, text, *params)\n\n\ndef blend(background, foreground, dx=20, dy=10, fy=0.7):\n\n    foreground = cv2.resize(foreground, (0, 0), fx=fy, fy=fy)\n    h, w = foreground.shape[:2]\n    b, g, r, a = cv2.split(foreground)\n    mask = np.dstack((a, a, a))\n    rgb = np.dstack((b, g, r))\n\n    canvas = background[-h - dy:-dy, dx:w + dx]\n    imask = mask > 0\n    canvas[imask] = rgb[imask]\n"""
deprecated/origin_stgcn_repo/torchlight/torchlight/__init__.py,0,b'from .io import IO\nfrom .io import str2bool\nfrom .io import str2dict\nfrom .io import DictAction\nfrom .io import import_class\nfrom .gpu import visible_gpu\nfrom .gpu import occupy_gpu\nfrom .gpu import ngpu\n'
deprecated/origin_stgcn_repo/torchlight/torchlight/gpu.py,2,"b'import os\nimport torch\n\n\ndef visible_gpu(gpus):\n    """"""\n        set visible gpu.\n\n        can be a single id, or a list\n\n        return a list of new gpus ids\n    """"""\n    gpus = [gpus] if isinstance(gpus, int) else list(gpus)\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = \',\'.join(list(map(str, gpus)))\n    return list(range(len(gpus)))\n\n\ndef ngpu(gpus):\n    """"""\n        count how many gpus used.\n    """"""\n    gpus = [gpus] if isinstance(gpus, int) else list(gpus)\n    return len(gpus)\n\n\ndef occupy_gpu(gpus=None):\n    """"""\n        make program appear on nvidia-smi.\n    """"""\n    if gpus is None:\n        torch.zeros(1).cuda()\n    else:\n        gpus = [gpus] if isinstance(gpus, int) else list(gpus)\n        for g in gpus:\n            torch.zeros(1).cuda(g)\n'"
deprecated/origin_stgcn_repo/torchlight/torchlight/io.py,5,"b'#!/usr/bin/env python\nimport argparse\nimport os\nimport sys\nimport traceback\nimport time\nimport warnings\nimport pickle\nfrom collections import OrderedDict\nimport yaml\nimport numpy as np\n# torch\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings(""ignore"",category=FutureWarning)\n    import h5py\n\nclass IO():\n    def __init__(self, work_dir, save_log=True, print_log=True):\n        self.work_dir = work_dir\n        self.save_log = save_log\n        self.print_to_screen = print_log\n        self.cur_time = time.time()\n        self.split_timer = {}\n        self.pavi_logger = None\n        self.session_file = None\n        self.model_text = \'\'\n        \n    # PaviLogger is removed in this version\n    def log(self, *args, **kwargs):\n        pass\n    #     try:\n    #         if self.pavi_logger is None:\n    #             from torchpack.runner.hooks import PaviLogger\n    #             url = \'http://pavi.parrotsdnn.org/log\'\n    #             with open(self.session_file, \'r\') as f:\n    #                 info = dict(\n    #                     session_file=self.session_file,\n    #                     session_text=f.read(),\n    #                     model_text=self.model_text)\n    #             self.pavi_logger = PaviLogger(url)\n    #             self.pavi_logger.connect(self.work_dir, info=info)\n    #         self.pavi_logger.log(*args, **kwargs)\n    #     except:  #pylint: disable=W0702\n    #         pass\n\n    def load_model(self, model, **model_args):\n        Model = import_class(model)\n        model = Model(**model_args)\n        self.model_text += \'\\n\\n\' + str(model)\n        return model\n\n    def load_weights(self, model, weights_path, ignore_weights=None):\n        if ignore_weights is None:\n            ignore_weights = []\n        if isinstance(ignore_weights, str):\n            ignore_weights = [ignore_weights]\n\n        self.print_log(\'Load weights from {}.\'.format(weights_path))\n        weights = torch.load(weights_path)\n        weights = OrderedDict([[k.split(\'module.\')[-1],\n                                v.cpu()] for k, v in weights.items()])\n\n        # filter weights\n        for i in ignore_weights:\n            ignore_name = list()\n            for w in weights:\n                if w.find(i) == 0:\n                    ignore_name.append(w)\n            for n in ignore_name:\n                weights.pop(n)\n                self.print_log(\'Filter [{}] remove weights [{}].\'.format(i,n))\n\n        for w in weights:\n            self.print_log(\'Load weights [{}].\'.format(w))\n\n        try:\n            model.load_state_dict(weights)\n        except (KeyError, RuntimeError):\n            state = model.state_dict()\n            diff = list(set(state.keys()).difference(set(weights.keys())))\n            for d in diff:\n                self.print_log(\'Can not find weights [{}].\'.format(d))\n            state.update(weights)\n            model.load_state_dict(state)\n        return model\n\n    def save_pkl(self, result, filename):\n        with open(\'{}/{}\'.format(self.work_dir, filename), \'wb\') as f:\n            pickle.dump(result, f)\n\n    def save_h5(self, result, filename):\n        with h5py.File(\'{}/{}\'.format(self.work_dir, filename), \'w\') as f:\n            for k in result.keys():\n                f[k] = result[k]\n\n    def save_model(self, model, name):\n        model_path = \'{}/{}\'.format(self.work_dir, name)\n        state_dict = model.state_dict()\n        weights = OrderedDict([[\'\'.join(k.split(\'module.\')),\n                                v.cpu()] for k, v in state_dict.items()])\n        torch.save(weights, model_path)\n        self.print_log(\'The model has been saved as {}.\'.format(model_path))\n\n    def save_arg(self, arg):\n\n        self.session_file = \'{}/config.yaml\'.format(self.work_dir)\n\n        # save arg\n        arg_dict = vars(arg)\n        if not os.path.exists(self.work_dir):\n            os.makedirs(self.work_dir)\n        with open(self.session_file, \'w\') as f:\n            f.write(\'# command line: {}\\n\\n\'.format(\' \'.join(sys.argv)))\n            yaml.dump(arg_dict, f, default_flow_style=False, indent=4)\n\n    def print_log(self, str, print_time=True):\n        if print_time:\n            # localtime = time.asctime(time.localtime(time.time()))\n            str = time.strftime(""[%m.%d.%y|%X] "", time.localtime()) + str\n\n        if self.print_to_screen:\n            print(str)\n        if self.save_log:\n            with open(\'{}/log.txt\'.format(self.work_dir), \'a\') as f:\n                print(str, file=f)\n\n    def init_timer(self, *name):\n        self.record_time()\n        self.split_timer = {k: 0.0000001 for k in name}\n\n    def check_time(self, name):\n        self.split_timer[name] += self.split_time()\n\n    def record_time(self):\n        self.cur_time = time.time()\n        return self.cur_time\n\n    def split_time(self):\n        split_time = time.time() - self.cur_time\n        self.record_time()\n        return split_time\n\n    def print_timer(self):\n        proportion = {\n            k: \'{:02d}%\'.format(int(round(v * 100 / sum(self.split_timer.values()))))\n            for k, v in self.split_timer.items()\n        }\n        self.print_log(\'Time consumption:\')\n        for k in proportion:\n            self.print_log(\n                \'\\t[{}][{}]: {:.4f}\'.format(k, proportion[k],self.split_timer[k])\n                )\n\n\ndef str2bool(v):\n    if v.lower() in (\'yes\', \'true\', \'t\', \'y\', \'1\'):\n        return True\n    elif v.lower() in (\'no\', \'false\', \'f\', \'n\', \'0\'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\'Boolean value expected.\')\n\n\ndef str2dict(v):\n    return eval(\'dict({})\'.format(v))  #pylint: disable=W0123\n\n\ndef _import_class_0(name):\n    components = name.split(\'.\')\n    mod = __import__(components[0])\n    for comp in components[1:]:\n        mod = getattr(mod, comp)\n    return mod\n\n\ndef import_class(import_str):\n    mod_str, _sep, class_str = import_str.rpartition(\'.\')\n    __import__(mod_str)\n    try:\n        return getattr(sys.modules[mod_str], class_str)\n    except AttributeError:\n        raise ImportError(\'Class %s cannot be found (%s)\' %\n                          (class_str,\n                           traceback.format_exception(*sys.exc_info())))\n\n\nclass DictAction(argparse.Action):\n    def __init__(self, option_strings, dest, nargs=None, **kwargs):\n        if nargs is not None:\n            raise ValueError(""nargs not allowed"")\n        super(DictAction, self).__init__(option_strings, dest, **kwargs)\n\n    def __call__(self, parser, namespace, values, option_string=None):\n        input_dict = eval(\'dict({})\'.format(values))  #pylint: disable=W0123\n        output_dict = getattr(namespace, self.dest)\n        for k in input_dict:\n            output_dict[k] = input_dict[k]\n        setattr(namespace, self.dest, output_dict)\n'"
mmskeleton/deprecated/datasets/utils/__init__.py,0,b'from . import skeleton\r\n\r\n__all__ = [skeleton]'
mmskeleton/deprecated/datasets/utils/skeleton.py,0,"b'import numpy as np\nimport random\n\n\ndef downsample(data_numpy, step, random_sample=True):\n    # input: C,T,V,M\n    begin = np.random.randint(step) if random_sample else 0\n    return data_numpy[:, begin::step, :, :]\n\n\ndef temporal_slice(data_numpy, step):\n    # input: C,T,V,M\n    C, T, V, M = data_numpy.shape\n    return data_numpy.reshape(C, T / step, step, V, M).transpose(\n        (0, 1, 3, 2, 4)).reshape(C, T / step, V, step * M)\n\n\ndef mean_subtractor(data_numpy, mean):\n    # input: C,T,V,M\n    # naive version\n    if mean == 0:\n        return\n    C, T, V, M = data_numpy.shape\n    valid_frame = (data_numpy != 0).sum(axis=3).sum(axis=2).sum(axis=0) > 0\n    begin = valid_frame.argmax()\n    end = len(valid_frame) - valid_frame[::-1].argmax()\n    data_numpy[:, :end, :, :] = data_numpy[:, :end, :, :] - mean\n    return data_numpy\n\n\ndef auto_pading(data_numpy, size, random_pad=False):\n    C, T, V, M = data_numpy.shape\n    if T < size:\n        begin = random.randint(0, size - T) if random_pad else 0\n        data_numpy_paded = np.zeros((C, size, V, M), dtype=data_numpy.dtype)\n        data_numpy_paded[:, begin:begin + T, :, :] = data_numpy\n        return data_numpy_paded\n    else:\n        return data_numpy\n\n\ndef random_choose(data_numpy, size, auto_pad=True):\n    # input: C,T,V,M\n    C, T, V, M = data_numpy.shape\n    if T == size:\n        return data_numpy\n    elif T < size:\n        if auto_pad:\n            return auto_pading(data_numpy, size, random_pad=True)\n        else:\n            return data_numpy\n    else:\n        begin = random.randint(0, T - size)\n        return data_numpy[:, begin:begin + size, :, :]\n\n\ndef random_move(data_numpy,\n                angle_candidate=[-10., -5., 0., 5., 10.],\n                scale_candidate=[0.9, 1.0, 1.1],\n                transform_candidate=[-0.2, -0.1, 0.0, 0.1, 0.2],\n                move_time_candidate=[1]):\n    # input: C,T,V,M\n    C, T, V, M = data_numpy.shape\n    move_time = random.choice(move_time_candidate)\n    node = np.arange(0, T, T * 1.0 / move_time).round().astype(int)\n    node = np.append(node, T)\n    num_node = len(node)\n\n    A = np.random.choice(angle_candidate, num_node)\n    S = np.random.choice(scale_candidate, num_node)\n    T_x = np.random.choice(transform_candidate, num_node)\n    T_y = np.random.choice(transform_candidate, num_node)\n\n    a = np.zeros(T)\n    s = np.zeros(T)\n    t_x = np.zeros(T)\n    t_y = np.zeros(T)\n\n    # linspace\n    for i in range(num_node - 1):\n        a[node[i]:node[i + 1]] = np.linspace(\n            A[i], A[i + 1], node[i + 1] - node[i]) * np.pi / 180\n        s[node[i]:node[i + 1]] = np.linspace(S[i], S[i + 1],\n                                             node[i + 1] - node[i])\n        t_x[node[i]:node[i + 1]] = np.linspace(T_x[i], T_x[i + 1],\n                                               node[i + 1] - node[i])\n        t_y[node[i]:node[i + 1]] = np.linspace(T_y[i], T_y[i + 1],\n                                               node[i + 1] - node[i])\n\n    theta = np.array([[np.cos(a) * s, -np.sin(a) * s],\n                      [np.sin(a) * s, np.cos(a) * s]])\n\n    # perform transformation\n    for i_frame in range(T):\n        xy = data_numpy[0:2, i_frame, :, :]\n        new_xy = np.dot(theta[:, :, i_frame], xy.reshape(2, -1))\n        new_xy[0] += t_x[i_frame]\n        new_xy[1] += t_y[i_frame]\n        data_numpy[0:2, i_frame, :, :] = new_xy.reshape(2, V, M)\n\n    return data_numpy\n\n\ndef random_shift(data_numpy):\n    # input: C,T,V,M\n    C, T, V, M = data_numpy.shape\n    data_shift = np.zeros(data_numpy.shape)\n    valid_frame = (data_numpy != 0).sum(axis=3).sum(axis=2).sum(axis=0) > 0\n    begin = valid_frame.argmax()\n    end = len(valid_frame) - valid_frame[::-1].argmax()\n\n    size = end - begin\n    bias = random.randint(0, T - size)\n    data_shift[:, bias:bias + size, :, :] = data_numpy[:, begin:end, :, :]\n\n    return data_shift\n\n\ndef openpose_match(data_numpy):\n    C, T, V, M = data_numpy.shape\n    assert (C == 3)\n    score = data_numpy[2, :, :, :].sum(axis=1)\n    # the rank of body confidence in each frame (shape: T-1, M)\n    rank = (-score[0:T - 1]).argsort(axis=1).reshape(T - 1, M)\n\n    # data of frame 1\n    xy1 = data_numpy[0:2, 0:T - 1, :, :].reshape(2, T - 1, V, M, 1)\n    # data of frame 2\n    xy2 = data_numpy[0:2, 1:T, :, :].reshape(2, T - 1, V, 1, M)\n    # square of distance between frame 1&2 (shape: T-1, M, M)\n    distance = ((xy2 - xy1)**2).sum(axis=2).sum(axis=0)\n\n    # match pose\n    forward_map = np.zeros((T, M), dtype=int) - 1\n    forward_map[0] = range(M)\n    for m in range(M):\n        choose = (rank == m)\n        forward = distance[choose].argmin(axis=1)\n        for t in range(T - 1):\n            distance[t, :, forward[t]] = np.inf\n        forward_map[1:][choose] = forward\n    assert (np.all(forward_map >= 0))\n\n    # string data\n    for t in range(T - 1):\n        forward_map[t + 1] = forward_map[t + 1][forward_map[t]]\n\n    # generate data\n    new_data_numpy = np.zeros(data_numpy.shape)\n    for t in range(T):\n        new_data_numpy[:, t, :, :] = data_numpy[:, t, :,\n                                                forward_map[t]].transpose(\n                                                    1, 2, 0)\n    data_numpy = new_data_numpy\n\n    # score sort\n    trace_score = data_numpy[2, :, :, :].sum(axis=1).sum(axis=0)\n    rank = (-trace_score).argsort()\n    data_numpy = data_numpy[:, :, :, rank]\n\n    return data_numpy\n\n\ndef top_k_by_category(label, score, top_k):\n    instance_num, class_num = score.shape\n    rank = score.argsort()\n    hit_top_k = [[] for i in range(class_num)]\n    for i in range(instance_num):\n        l = label[i]\n        hit_top_k[l].append(l in rank[i, -top_k:])\n\n    accuracy_list = []\n    for hit_per_category in hit_top_k:\n        if hit_per_category:\n            accuracy_list.append(\n                sum(hit_per_category) * 1.0 / len(hit_per_category))\n        else:\n            accuracy_list.append(0.0)\n    return accuracy_list\n\n\ndef calculate_recall_precision(label, score):\n    instance_num, class_num = score.shape\n    rank = score.argsort()\n    confusion_matrix = np.zeros([class_num, class_num])\n\n    for i in range(instance_num):\n        true_l = label[i]\n        pred_l = rank[i, -1]\n        confusion_matrix[true_l][pred_l] += 1\n\n    precision = []\n    recall = []\n\n    for i in range(class_num):\n        true_p = confusion_matrix[i][i]\n        false_n = sum(confusion_matrix[i, :]) - true_p\n        false_p = sum(confusion_matrix[:, i]) - true_p\n        precision.append(true_p * 1.0 / (true_p + false_p))\n        recall.append(true_p * 1.0 / (true_p + false_n))\n\n    return precision, recall'"
