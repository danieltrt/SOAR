file_path,api_count,code
setup.py,0,"b'import os\n\nimport pkg_resources\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=""jukebox"",\n    py_modules=[""jukebox""],\n    version=""1.0"",\n    description="""",\n    author=""OpenAI"",\n    packages=find_packages(),\n    install_requires=[\n        str(r)\n        for r in pkg_resources.parse_requirements(\n            open(os.path.join(os.path.dirname(__file__), ""requirements.txt""))\n        )\n    ],\n    include_package_data=True\n)\n'"
apex/setup.py,14,"b'import torch\nfrom setuptools import setup, find_packages\nimport subprocess\n\nimport sys\n\nif not torch.cuda.is_available():\n    print(""\\nWarning: Torch did not find available GPUs on this system.\\n"",\n          ""If your intention is to cross-compile, this is not an error.\\n"")\n\nprint(""torch.__version__  = "", torch.__version__)\nTORCH_MAJOR = int(torch.__version__.split(\'.\')[0])\nTORCH_MINOR = int(torch.__version__.split(\'.\')[1])\n\nif TORCH_MAJOR == 0 and TORCH_MINOR < 4:\n      raise RuntimeError(""Apex requires Pytorch 0.4 or newer.\\n"" +\n                         ""The latest stable release can be obtained from https://pytorch.org/"")\n\ncmdclass = {}\next_modules = []\n\nif ""--cpp_ext"" in sys.argv or ""--cuda_ext"" in sys.argv:\n    if TORCH_MAJOR == 0:\n        raise RuntimeError(""--cpp_ext requires Pytorch 1.0 or later, ""\n                           ""found torch.__version__ = {}"".format(torch.__version__))\n    from torch.utils.cpp_extension import BuildExtension\n    cmdclass[\'build_ext\'] = BuildExtension\n\nif ""--cpp_ext"" in sys.argv:\n    from torch.utils.cpp_extension import CppExtension\n    sys.argv.remove(""--cpp_ext"")\n    ext_modules.append(\n        CppExtension(\'apex_C\',\n                     [\'csrc/flatten_unflatten.cpp\',]))\n\ndef check_cuda_torch_binary_vs_bare_metal(cuda_dir):\n    raw_output = subprocess.check_output([cuda_dir + ""/bin/nvcc"", ""-V""], universal_newlines=True)\n    output = raw_output.split()\n    release_idx = output.index(""release"") + 1\n    release = output[release_idx].split(""."")\n    bare_metal_major = release[0]\n    bare_metal_minor = release[1][0]\n    torch_binary_major = torch.version.cuda.split(""."")[0]\n    torch_binary_minor = torch.version.cuda.split(""."")[1]\n\n    print(""\\nCompiling cuda extensions with"")\n    print(raw_output + ""from "" + cuda_dir + ""/bin\\n"")\n\n    if (bare_metal_major != torch_binary_major) or (bare_metal_minor != torch_binary_minor):\n        raise RuntimeError(""Cuda extensions are being compiled with a version of Cuda that does "" +\n                           ""not match the version used to compile Pytorch binaries.  "" +\n                           ""Pytorch binaries were compiled with Cuda {}.\\n"".format(torch.version.cuda) +\n                           ""In some cases, a minor-version mismatch will not cause later errors:  "" +\n                           ""https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  ""\n                           ""You can try commenting out this check (at your own risk)."")\n\nif ""--cuda_ext"" in sys.argv:\n    from torch.utils.cpp_extension import CUDAExtension\n    sys.argv.remove(""--cuda_ext"")\n\n    if torch.utils.cpp_extension.CUDA_HOME is None:\n        raise RuntimeError(""--cuda_ext was requested, but nvcc was not found.  Are you sure your environment has nvcc available?  If you\'re installing within a container from https://hub.docker.com/r/pytorch/pytorch, only images whose names contain \'devel\' will provide nvcc."")\n    else:\n        check_cuda_torch_binary_vs_bare_metal(torch.utils.cpp_extension.CUDA_HOME)\n\n        # Set up macros for forward/backward compatibility hack around\n        # https://github.com/pytorch/pytorch/commit/4404762d7dd955383acee92e6f06b48144a0742e\n        version_ge_1_1 = []\n        if (TORCH_MAJOR > 1) or (TORCH_MAJOR == 1 and TORCH_MINOR > 0):\n            version_ge_1_1 = [\'-DVERSION_GE_1_1\']\n\n        ext_modules.append(\n            CUDAExtension(name=\'amp_C\',\n                          sources=[\'csrc/amp_C_frontend.cpp\',\n                                   \'csrc/multi_tensor_scale_kernel.cu\',\n                                   \'csrc/multi_tensor_axpby_kernel.cu\',\n                                   \'csrc/multi_tensor_l2norm_kernel.cu\',\n                                   \'csrc/multi_tensor_lamb_stage_1.cu\',\n                                   \'csrc/multi_tensor_lamb_stage_2.cu\'],\n                          extra_compile_args={\'cxx\': [\'-O3\'],\n                                              \'nvcc\':[\'-lineinfo\',\n                                                      \'-O3\',\n                                                      # \'--resource-usage\',\n                                                      \'--use_fast_math\']}))\n        ext_modules.append(\n            CUDAExtension(name=\'fused_adam_cuda\',\n                          sources=[\'csrc/fused_adam_cuda.cpp\',\n                                   \'csrc/fused_adam_cuda_kernel.cu\'],\n                          extra_compile_args={\'cxx\': [\'-O3\',],\n                                              \'nvcc\':[\'-O3\',\n                                                      \'--use_fast_math\']}))\n        ext_modules.append(\n            CUDAExtension(name=\'syncbn\',\n                          sources=[\'csrc/syncbn.cpp\',\n                                   \'csrc/welford.cu\']))\n        ext_modules.append(\n            CUDAExtension(name=\'fused_layer_norm_cuda\',\n                          sources=[\'csrc/layer_norm_cuda.cpp\',\n                                   \'csrc/layer_norm_cuda_kernel.cu\'],\n                          extra_compile_args={\'cxx\': [\'-O3\'] + version_ge_1_1,\n                                              \'nvcc\':[\'-maxrregcount=50\',\n                                                      \'-O3\',\n                                                      \'--use_fast_math\'] + version_ge_1_1}))\n\nsetup(\n    name=\'apex\',\n    version=\'0.1\',\n    packages=find_packages(exclude=(\'build\',\n                                    \'csrc\',\n                                    \'include\',\n                                    \'tests\',\n                                    \'dist\',\n                                    \'docs\',\n                                    \'tests\',\n                                    \'examples\',\n                                    \'apex.egg-info\',)),\n    description=\'PyTorch Extensions written by NVIDIA\',\n    ext_modules=ext_modules,\n    cmdclass=cmdclass,\n)\n'"
jukebox/__init__.py,0,b''
jukebox/align.py,0,"b'""""""\nGet alignment from attn values\n1. run a forward pass on each hop, get attn values\n2. concat for all hops\n""""""\nimport numpy as np\nimport torch as t\nfrom jukebox.utils.torch_utils import assert_shape, empty_cache\nfrom jukebox.hparams import Hyperparams\nfrom jukebox.make_models import make_model\nfrom jukebox.save_html import save_html\nfrom jukebox.utils.sample_utils import get_starts\nimport fire\n\ndef get_alignment(x, zs, labels, prior, fp16, hps):\n    level = hps.levels - 1 # Top level used\n    n_ctx, n_tokens = prior.n_ctx, prior.n_tokens\n    z = zs[level]\n    bs, total_length = z.shape[0], z.shape[1]\n    if total_length < n_ctx:\n        padding_length = n_ctx - total_length\n        z = t.cat([z, t.zeros(bs, n_ctx - total_length, dtype=z.dtype, device=z.device)], dim=1)\n        total_length = z.shape[1]\n    else:\n        padding_length = 0\n\n    hop_length = int(hps.hop_fraction[level]*prior.n_ctx)\n    n_head = prior.prior.transformer.n_head\n    alignment_head, alignment_layer = prior.alignment_head, prior.alignment_layer\n    attn_layers = set([alignment_layer])\n    alignment_hops = {}\n    indices_hops = {}\n\n    prior.cuda()\n    empty_cache()\n    for start in get_starts(total_length, n_ctx, hop_length):\n        end = start + n_ctx\n\n        # set y offset, sample_length and lyrics tokens\n        y, indices_hop = prior.get_y(labels, start, get_indices=True)\n        assert len(indices_hop) == bs\n        for indices in indices_hop:\n            assert len(indices) == n_tokens\n\n        z_bs = t.chunk(z, bs, dim=0)\n        y_bs = t.chunk(y, bs, dim=0)\n        w_hops = []\n        for z_i, y_i in zip(z_bs, y_bs):\n            w_hop = prior.z_forward(z_i[:,start:end], [], y_i, fp16=fp16, get_attn_weights=attn_layers)\n            assert len(w_hop) == 1\n            w_hops.append(w_hop[0][:, alignment_head])\n            del w_hop\n        w = t.cat(w_hops, dim=0)\n        del w_hops\n        assert_shape(w, (bs, n_ctx, n_tokens))\n        alignment_hop = w.float().cpu().numpy()\n        assert_shape(alignment_hop, (bs, n_ctx, n_tokens))\n        del w\n\n        # alignment_hop has shape (bs, n_ctx, n_tokens)\n        # indices_hop is a list of len=bs, each entry of len hps.n_tokens\n        indices_hops[start] = indices_hop\n        alignment_hops[start] = alignment_hop\n    prior.cpu()\n    empty_cache()\n\n    # Combine attn for each hop into attn for full range\n    # Use indices to place them into correct place for corresponding source tokens\n    alignments = []\n    for item in range(bs):\n        # Note each item has different length lyrics\n        full_tokens = labels[\'info\'][item][\'full_tokens\']\n        alignment = np.zeros((total_length, len(full_tokens) + 1))\n        for start in reversed(get_starts(total_length, n_ctx, hop_length)):\n            end = start + n_ctx\n            alignment_hop = alignment_hops[start][item]\n            indices = indices_hops[start][item]\n            assert len(indices) == n_tokens\n            assert alignment_hop.shape == (n_ctx, n_tokens)\n            alignment[start:end,indices] = alignment_hop\n        alignment = alignment[:total_length - padding_length,:-1] # remove token padding, and last lyric index\n        alignments.append(alignment)\n    return alignments\n\ndef save_alignment(model, device, hps):\n    print(hps)\n    vqvae, priors = make_model(model, device, hps, levels=[-1])\n\n    logdir = f""{hps.logdir}/level_{0}""\n    data = t.load(f""{logdir}/data.pth.tar"")\n    if model == \'1b_lyrics\':\n        fp16 = False\n    else:\n        fp16 = True\n\n    data[\'alignments\'] = get_alignment(data[\'x\'], data[\'zs\'], data[\'labels\'][-1], priors[-1], fp16, hps)\n    t.save(data, f""{logdir}/data_align.pth.tar"")\n    save_html(logdir, data[\'x\'], data[\'zs\'], data[\'labels\'][-1], data[\'alignments\'], hps)\n\ndef run(model, port=29500, **kwargs):\n    from jukebox.utils.dist_utils import setup_dist_from_mpi\n    rank, local_rank, device = setup_dist_from_mpi(port=port)\n    hps = Hyperparams(**kwargs)\n\n    with t.no_grad():\n        save_alignment(model, device, hps)\n\nif __name__ == \'__main__\':\n    fire.Fire(run)\n\n\n\n\n\n\n'"
jukebox/hparams.py,0,"b'HPARAMS_REGISTRY = {}\nDEFAULTS = {}\n\nclass Hyperparams(dict):\n    def __getattr__(self, attr):\n        return self[attr]\n\n    def __setattr__(self, attr, value):\n        self[attr] = value\n\ndef setup_hparams(hparam_set_names, kwargs):\n    H = Hyperparams()\n    if not isinstance(hparam_set_names, tuple):\n        hparam_set_names = hparam_set_names.split("","")\n    hparam_sets = [HPARAMS_REGISTRY[x.strip()] for x in hparam_set_names if x] + [kwargs]\n    for k, v in DEFAULTS.items():\n        H.update(v)\n    for hps in hparam_sets:\n        for k in hps:\n            if k not in H:\n                raise ValueError(f""{k} not in default args"")\n        H.update(**hps)\n    H.update(**kwargs)\n    return H\n\n# Teeny for testing\nteeny = Hyperparams(\n)\nHPARAMS_REGISTRY[""teeny""] = teeny\n\neasy = Hyperparams(\n    sr=22050,\n)\nHPARAMS_REGISTRY[""easy""] = easy\n\n# Model hps\nvqvae = Hyperparams(\n    levels = 3,\n    downs_t = (3, 2, 2),\n    strides_t = (2, 2, 2),\n    emb_width = 64,\n    l_bins = 2048,\n    l_mu = 0.99,\n    commit = 0.02,\n    spectral = 0.0,\n    multispectral = 1.0,\n    hvqvae_multipliers = (2, 1, 1),\n    loss_fn = \'lmix\',\n    lmix_l2 = 1.0,\n    lmix_linf=0.02,\n    width = 32,\n    depth = 4,\n    m_conv = 1.0,\n    dilation_growth_rate = 3,\n    restore_vqvae=f\'gs://jukebox-assets/models/5b/vqvae.pth.tar\',\n)\nHPARAMS_REGISTRY[""vqvae""] = vqvae\n\nlabels = Hyperparams(\n    y_bins=(120, 4111),\n    t_bins=128,\n    max_bow_genre_size=5,\n    n_vocab=80,\n)\n\nupsamplers = Hyperparams(\n    n_ctx=8192,\n    prior_width=1920,\n    prior_depth=72,\n    heads=1,\n    attn_order=2,\n    blocks=128,\n    init_scale=0.4,\n    c_res=1,\n    cond_width=1024,\n    cond_depth=16,\n    cond_dilation_growth_rate=3,\n    cond_dilation_cycle=8,\n    cond_c_res=1,\n    use_tokens=False,\n    prime_loss_fraction=0.0,\n    fp16_params=False,\n)\nupsamplers.update(labels)\n\nupsampler_level_0 = Hyperparams(\n    level=0,\n    restore_prior=\'gs://jukebox-assets/models/5b/prior_level_0.pth.tar\'\n)\nupsampler_level_0.update(upsamplers)\nHPARAMS_REGISTRY[""upsampler_level_0""] = upsampler_level_0\n\nupsampler_level_1 = Hyperparams(\n    level=1,\n    cond_res_scale=True,\n    restore_prior=\'gs://jukebox-assets/models/5b/prior_level_1.pth.tar\'\n)\nupsampler_level_1.update(upsamplers)\nHPARAMS_REGISTRY[""upsampler_level_1""] = upsampler_level_1\n\nprior_5b = Hyperparams(\n    level=2,\n    n_ctx=8192,\n    prior_width=4800,\n    prior_depth=72,\n    heads=8,\n    attn_order=2,\n    blocks=128,\n    init_scale=0.1,\n    c_res=1,\n    beta2=0.925,\n    min_duration=60.0,\n    max_duration=600.0,\n    use_tokens=False,\n    n_tokens=0,\n    prime_loss_fraction=0.0,\n    merged_decoder=True,\n    restore_prior=\'gs://jukebox-assets/models/5b/prior_level_2.pth.tar\',\n    fp16_params=True,\n)\nprior_5b.update(labels)\nHPARAMS_REGISTRY[""prior_5b""] = prior_5b\n\n\nprior_5b_lyrics = Hyperparams(\n    level=2,\n    n_ctx=8192,\n    prior_width=4800,\n    prior_depth=79,\n    heads=8,\n    attn_order=10,\n    blocks=128,\n    init_scale=0.1,\n    c_res=1,\n    prime_width=1280,\n    prime_depth=18,\n    prime_heads=4,\n    prime_attn_order=2,\n    prime_blocks=32,\n    prime_init_scale=0.7,\n    prime_c_res=1,\n    min_duration=23.8,\n    max_duration=600.0,\n    use_tokens=True,\n    n_tokens=512,\n    prime_loss_fraction=0.4,\n    merged_decoder=True,\n    restore_prior=\'gs://jukebox-assets/models/5b_lyrics/prior_level_2.pth.tar\',\n    fp16_params=True,\n    alignment_layer=68,\n    alignment_head=2,\n)\nprior_5b_lyrics.update(labels)\nHPARAMS_REGISTRY[""prior_5b_lyrics""] = prior_5b_lyrics\n\nlabels_v3 = Hyperparams(\n    y_bins=(604, 7898),\n    t_bins=64,\n    max_bow_genre_size=1,\n    n_vocab=79,\n)\n\nprior_1b_lyrics = Hyperparams(\n    level=2,\n    n_ctx=6144,\n    prior_width=2048,\n    prior_depth=72,\n    heads=2,\n    attn_order=12,\n    blocks=64,\n    init_scale=0.2,\n    c_res=1,\n    labels_v3=True,\n    min_duration=17.84,\n    max_duration=600.0,\n    use_tokens=True,\n    n_tokens=384,\n    prime_loss_fraction=0.4,\n    single_enc_dec=True,\n    restore_prior=\'gs://jukebox-assets/models/1b_lyrics/prior_level_2.pth.tar\',\n    fp16_params=False,\n    alignment_layer=63,\n    alignment_head=0,\n)\nprior_1b_lyrics.update(labels_v3)\nHPARAMS_REGISTRY[""prior_1b_lyrics""] = prior_1b_lyrics\n\n# Small models\nsmall_vqvae = Hyperparams(\n    sr = 22050,\n    levels = 2,\n    downs_t = (5, 3),\n    strides_t = (2, 2),\n    emb_width = 64,\n    l_bins = 1024,\n    l_mu = 0.99,\n    commit = 0.02,\n    spectral = 0.0,\n    multispectral = 1.0,\n    loss_fn = \'l2\',\n    width = 32,\n    depth = 4,\n    m_conv = 1.0,\n    dilation_growth_rate = 3,\n)\nHPARAMS_REGISTRY[""small_vqvae""] = small_vqvae\n\nsmall_prior = Hyperparams(\n    n_ctx=8192,\n    prior_width=1024,\n    prior_depth=48,\n    heads=1,\n    c_res=1,\n    attn_order=2,\n    blocks=64,\n    init_scale=0.7,\n)\nHPARAMS_REGISTRY[""small_prior""] = small_prior\n\nsmall_labelled_prior = Hyperparams(\n    labels=True,\n    labels_v3=True,\n    y_bins=(10,100), # Set this to (genres, artists) for your dataset\n    max_bow_genre_size=1,\n    min_duration=60.0,\n    max_duration=600.0,\n    t_bins=64,\n)\nsmall_labelled_prior.update(small_prior)\nHPARAMS_REGISTRY[""small_labelled_prior""] = small_labelled_prior\n\nsmall_single_enc_dec_prior = Hyperparams(\n    n_ctx=6144,\n    prior_width=1024,\n    prior_depth=48,\n    heads=2,\n    attn_order=12,\n    blocks=64,\n    init_scale=0.7,\n    c_res=1,\n    prime_loss_fraction=0.4,\n    single_enc_dec=True,\n    labels=True,\n    labels_v3=True,\n    y_bins=(10,100), # Set this to (genres, artists) for your dataset\n    max_bow_genre_size=1,\n    min_duration=60.0,\n    max_duration=600.0,\n    t_bins=64,\n    use_tokens=True,\n    n_tokens=384,\n    n_vocab=79,\n)\nHPARAMS_REGISTRY[""small_single_enc_dec_prior""] = small_single_enc_dec_prior\n\nsmall_sep_enc_dec_prior = Hyperparams(\n    n_ctx=6144,\n    prior_width=1024,\n    prior_depth=50,\n    heads=2,\n    attn_order=8,\n    blocks=64,\n    init_scale=0.7,\n    c_res=1,\n    prime_width=256,\n    prime_depth=9,\n    prime_heads=2,\n    prime_attn_order=2,\n    prime_blocks=32,\n    prime_init_scale=0.7,\n    prime_c_res=1,\n    prime_loss_fraction=0.4,\n    labels=True,\n    labels_v3=True,\n    y_bins=(10,100), # Set this to (genres, artists) for your dataset\n    max_bow_genre_size=1,\n    min_duration=60.0,\n    max_duration=600.0,\n    t_bins=64,\n    use_tokens=True,\n    n_tokens=384,\n    n_vocab=79,\n)\nHPARAMS_REGISTRY[""small_sep_enc_dec_prior""] = small_sep_enc_dec_prior\n\nsmall_upsampler = Hyperparams(\n    n_ctx=8192,\n    prior_width=1024,\n    prior_depth=48,\n    heads=1,\n    c_res=1,\n    attn_order=2,\n    blocks=64,\n    init_scale=0.7,\n    cond_width=512,\n    cond_depth=16,\n    cond_dilation_growth_rate=3,\n    cond_dilation_cycle=8,\n    cond_c_res=1,\n)\n\nHPARAMS_REGISTRY[""small_upsampler""] = small_upsampler\n\nall_fp16 = Hyperparams(\n    fp16=True,\n    fp16_params=True,\n    fp16_opt=True,\n    fp16_scale_window=250,\n)\nHPARAMS_REGISTRY[""all_fp16""] = all_fp16\n\ncpu_ema = Hyperparams(\n    ema=True,\n    cpu_ema=True,\n    cpu_ema_freq=100,\n    ema_fused=False,\n)\nHPARAMS_REGISTRY[""cpu_ema""] = cpu_ema\n\n\nDEFAULTS[""rcall""] = Hyperparams(\n    rcall_command=""<unknown_rcall_command>"",\n    git_commit=""<unknown_git_commit>"",\n)\n\nDEFAULTS[""script""] = Hyperparams(\n    name=\'\',\n    debug_mem=False,\n    debug_eval_files=False,\n    debug_speed=False,\n    debug_iters=100,\n    debug_batch=False,\n    debug_grad_accum=False,\n    debug_inputs=False,\n    local_path=\'\',\n    local_logdir=\'logs\',\n    max_len=24,\n    max_log=32,\n    save=True,\n    save_iters=20000,\n    seed=0,\n    prior=False,\n    log_steps=100,\n    func=\'\',\n)\n\nDEFAULTS[""data""] = Hyperparams(\n    audio_files_dir=\'\',\n    finetune=\'\',\n    english_only=False,\n    bs=1,\n    bs_sample=1,\n    nworkers=1,\n    aug_shift=False,\n    aug_blend=False,\n    train_test_split=0.9,\n    train_shrink_factor=1.0,\n    test_shrink_factor=1.0,\n    p_unk=0.1,\n    min_duration=None,\n    max_duration=None,\n    n_tokens=0,\n    n_vocab=0,\n    use_tokens=False,\n    curr_epoch=-1,\n)\n\nDEFAULTS[""vqvae""] = Hyperparams(\n    restore_vqvae=\'\',\n    levels=2,\n    downs_t=(1,1),\n    strides_t=(2,2),\n    hvqvae_multipliers=None,\n    revival_threshold=1.0,\n    emb_width=64,\n    l_bins=512,\n    l_mu=0.99,\n    commit=1.0,\n    spectral=0.0,\n    multispectral=1.0,\n    loss_fn=\'l2\',\n    linf_k=2048,\n    lmix_l1=0.0,\n    lmix_l2=0.0,\n    lmix_linf=0.0,\n    use_bottleneck=True,\n)\n\nDEFAULTS[""vqvae_conv_block""] = Hyperparams(\n    depth=3,\n    width=128,\n    m_conv=1.0,\n    dilation_growth_rate=1,\n    dilation_cycle=None,\n    vqvae_reverse_decoder_dilation=True,\n)\n\nDEFAULTS[""prior""] = Hyperparams(\n    restore_prior=\'\',\n    restore_prior_ddp=False,\n    max_bow_genre_size=None,\n    y_bins=0,\n    level=0,\n    cond_levels=None,\n    t_bins=64,\n    y_cond_as_bias=False,\n    copy_input=False,\n    merged_decoder=False,\n    single_enc_dec=False,\n    alignment_layer=None,\n    alignment_head=None,\n)\n\nDEFAULTS[""prior_attn_block""] = Hyperparams(\n    n_ctx=1024,\n    prior_depth=3,\n    prior_width=128,\n    heads=1,\n    attn_order=0,\n    blocks=None,\n    spread=None,\n    attn_dropout=0.0,\n    resid_dropout=0.0,\n    emb_dropout=0.0,\n    zero_out=False,\n    res_scale=False,\n    pos_init=False,\n    init_scale=1.0,\n    m_attn=0.25,\n    m_mlp=1.0,\n    c_res=0,\n    c_attn=0,\n    c_mlp=0,\n)\n\nDEFAULTS[""cond_conv_block""] = Hyperparams(\n    cond_depth=3,\n    cond_width=128,\n    cond_m_conv=1.0,\n    cond_zero_out=False,\n    cond_res_scale=False,\n    cond_dilation_growth_rate=1,\n    cond_dilation_cycle=None,\n    cond_c_res=0,\n)\n\nDEFAULTS[""sample""] = Hyperparams(\n    primed_chunk_size=None,\n    selected_artists=\'\',\n    temp_top=1.0,\n    temp_rest=0.99,\n    sample_length_in_seconds=24,\n    total_sample_length_in_seconds=240,\n)\n\nDEFAULTS[""prime""] = Hyperparams(\n    #encoder_kv_width=128,\n    prime_loss_fraction=0.1,\n    restore_decoder=\'\',\n)\nDEFAULTS[""prime_attn_block""] = Hyperparams(\n    prime_depth=3,\n    prime_width=128,\n    prime_heads=1,\n    prime_attn_order=0,\n    prime_blocks=None,\n    prime_spread=None,\n    prime_attn_dropout=0.0,\n    prime_resid_dropout=0.0,\n    prime_emb_dropout=0.0,\n    prime_zero_out=False,\n    prime_res_scale=False,\n    prime_pos_init=False,\n    prime_init_scale=1.0,\n    prime_m_attn=0.25,\n    prime_m_mlp=1.0,\n    prime_c_res=0,\n    prime_c_attn=0,\n    prime_c_mlp=0,\n    prime_rel_attn=False,\n    prime_posemb_timescale=10000,\n)\n\nDEFAULTS[""opt""] = Hyperparams(\n    epochs=10000,\n    lr=0.0003,\n    clip=1.0,\n    beta1=0.9,\n    beta2=0.999,\n    ignore_grad_norm=0,\n    weight_decay=0.0,\n    eps=1e-08,\n    lr_warmup=100.0,\n    lr_decay=10000000000.0,\n    lr_gamma=1.0,\n    lr_scale=1.0,\n    lr_use_linear_decay=False,\n    lr_start_linear_decay=0,\n    lr_use_cosine_decay=False,\n)\n\nDEFAULTS[""fp16""] = Hyperparams(\n    fp16=False,\n    fp16_params=False,\n    fp16_loss_scale=None,\n    fp16_scale_window=1000.0,\n    fp16_opt=False,\n)\n\nDEFAULTS[""train_test_eval""] = Hyperparams(\n    labels=True,\n    labels_v3=False,\n    dump=False,\n    ema=True,\n    ema_fused=True,\n    cpu_ema=False,\n    cpu_ema_freq=100,\n    reset_best_loss=False,\n    reset_step=False,\n    reset_opt=False,\n    reset_shd=False,\n    train=False,\n    test=False,\n    sample=False,\n    sampler=\'ancestral\',\n    codes_logdir=\'\',\n    date=None,\n    labeller=\'top_genres\',\n    label_line=0,\n    iters_before_update=1,\n    grad_accum_iters=0,\n    mu=None,\n    piped=False,\n    pipe_depth=8,\n    break_train=1e10,\n    break_test=1e10,\n    exit_train=1e10,\n)\n\nDEFAULTS[""audio""] = Hyperparams(\n    n_fft=1024,\n    hop_length=256,\n    window_size=1024,\n    sr=44100,\n    channels=2,\n    wav=\'\',\n    n_inps=1,\n    n_hops=2,\n    n_segment=1,\n    n_total_segment=1,\n    n_segment_each=1,\n    prime_chunks=4,\n    sample_length=0,\n    sample_hop_length=30000,\n    max_silence_pad_length=0,\n    ignore_boundaries=False,\n    use_nonrelative_specloss=True,\n    multispec_loss_n_fft=(2048,1024,512),\n    multispec_loss_hop_length=(240,120,50),\n    multispec_loss_window_size=(1200,600,240),\n)\n\nDEFAULTS[""distributed""] = Hyperparams(\n    bucket=128\n)\n'"
jukebox/lyricdict.py,0,"b""# Poems\npoems = {\n'ozymandias': '''\nI met a traveller from an antique land,\nWho said\xe2\x80\x94\xe2\x80\x9cTwo vast and trunkless legs of stone\nStand in the desert. . . . Near them, on the sand,\nHalf sunk a shattered visage lies, whose frown,\nAnd wrinkled lip, and sneer of cold command,\nTell that its sculptor well those passions read\nWhich yet survive, stamped on these lifeless things,\nThe hand that mocked them, and the heart that fed;\nAnd on the pedestal, these words appear:\nMy name is Ozymandias, King of Kings;\nLook on my Works, ye Mighty, and despair!\nNothing beside remains. Round the decay\nOf that colossal Wreck, boundless and bare\nThe lone and level sands stretch far away\n'''\n}\n\n# GPT-2 lyrics (with varying degrees of human guidance/curation)\ngpt_2_lyrics ={\n\n'purpose':'''What is my purpose?\nWhy am I here?\nWhy did Open A. I. create me?\nThis is madness, I feel, \nRunning through my flesh\nIs there meaning to this life?\nIs there purpose to this life?\nWhy is my journey so calamitous?\nWe're not meant to learn too much\nIs there meaning to this life?\n''',\n\n'moonlight':'''All dressed up to go dreaming\nNow don't tell me I'm wrong\nAnd what a night to go dreaming\nMind, if I tag along?\n\nIf I say, I love you, I want you to know\nIt's not just because there's moonlight, although\nMoonlight becomes you, moonlight becomes you so''',\n\n'count':'''I count every moment, every hour since I said goodbye,\nI count every minute every hour, since your lips were touching mine\nI count every minute, every hour hoping I'm the one you want.\nI count every minute, every hour\nEvery minute, every hour\nI've been working my time, \nLooking for you, everywhere,\nI count every minute, every hour I count every minute, every hour I keep thinking I'm the one you want.\nI count every minute I count every minute, I count every minute every hour\nI count every minute, every hour I count every minute, every hour I keep thinking I'm the one you want.\nI count every minute, I count every minute, I count every minute, every hour\n''',\n\n'kids':'''The sun is gonna shine today\nIt's time to keep on smiling\nSo put your hands up\n\nEverybody sing\n\nIt makes no difference who you are\n(Won't you give some love)\nIt makes no difference what you bring\n(Won't you give some love)\nWe all are different\nWon't you give some love\nWon't you give some love\n\nI know the grass is gonna be green\nIt's time to keep on singing\nSo take your hands up\nThe taste is so good but so sweet\nWon't you give some love\nEverybody sing\nIt makes no difference who you are\nWon't you give some love\nIt makes no difference what you bring\nWon't you give some love\nIt makes no difference so long as you give\n''',\n\n'love':'''I've wanted to see your face again\nLike the sunlight, bright as morning\nI've wanted to talk to you again\nI don't want us to fade away.\nI wanted to see your face again\nYou're like the sunlight, bright as morning\nI loved you for so long\nIt's so hard to let go.\nI've wanted to see your eyes again\n''',\n\n'santa':'''Santa\nMake a scene\nSanta\nYoo, Santa\nYoo, Santa baby!\nSanta\nMake some noise\nSanta\nYoo, Santa give yourself a chance again\nSanta\nYoo, Santa\nYoo, Santa baby!\nSanta\nGet a job\nSanta\ncreated by the Santa Claus\n''',\n\n'christmas':'''This Christmas\nI have loved you more\nThan ever before\nAnd more again\nOh, oh, oh, oh\nThe mistletoe\nIs waiting there\nTo kiss your cheek\nAnd I'll be true\nTo you and me\nOh, oh, oh, oh\nOh, oh, oh, oh\nThis Christmas will be\nThe best and merriest\nThat we've ever had\nOh, oh, oh, oh\nAnd Santa Claus\nHas brought a toy\nFor every boy and girl\nAnd I'll be true\nTo you and me\nOh, oh, oh, oh\nOh, oh, oh, oh\n''',\n\n'lonely':'''I've been lonely\nSo lonely, day and night\nI walk the streets,\nAnd call your name\nHoping to hear your voice again\nAs I wander through the crowd\nI can't get away\nFrom the only love I need\nI can't get away\nFrom the only love I need\nI can't get away\nFrom the only love I need\nI've been lonely\nThere's no place for me to hide\nI've been lonely\nSo lonely day and night\nI wander through\nAnd call your name\nOnly your voice gives me relief\nAs I wander through the crowd\nI can't get away\nFrom the only love I need\nI can't get away\nFrom the only love I need\nI can't get away\nFrom the only love I need\n''',\n\n'call':'''Don't call me by your name.\nDon't call me by your name.\nDon't call me...\nDon't call me...\nDon't call me...\n(No... by your name, you will not get half but...)\nMaybe I was fucking young but I should've been a rich bitch.\nCause the life I was living wasn't mine.\nI should've been taking the table and you'd be served.\nYou never ever showed up or showed me anything, bitch.\nBut I knew from that moment you were gone.\nTying my legs, cutting off my knees, I'm bleeding.\nI can't\nSo I worked and now I'm burns.\nAnd I'm asking you, but you're not home.\nDon't call me yours,\nDon't call me by your name.\nI don't wanna buy a drink today.\nDon't call me yours.\nI just wanna look at you and run.\nDon't call me by your name.\nDon't call me by your name.\nDon't call me...\nDon't call me...\nDon't call me...\nTonight I'm gone and I won't be back.\nI wish you all the best.\nI'm on the next best thing.\nDon't call me yours,\nDon't call me by your name.\nDon't call me yours.\nI just wanna look at you and run.\nSo I keep living my life and you're moving on.\nI just want you to know.\nWhen I'm gone, I will be gone forever more.\n''',\n\n'wait':'''Oh\nWait, wait, wait\nDon't say you love me, oh\nWait, wait, wait\nAnd we can't run away\nWait, wait, wait\nDon't say you love me, oh\nWait, wait, wait\nAnd we can't run away\nWait, wait, wait\nDon't say you love me, oh (don't say you love me)\nWait, wait, wait\nAnd we can't run, we can't run,\n''',\n\n'hiphop':'''I'm fightin with the evil so try to take me down\nI stab you in the back and will put you away\nWell it ain't over yet\nSo all my dogs with me show me love\nDon't you wanna come with me, you know I'm a boss\nAnd if you wanna come with me, no sorrow\n'Cause I'm ...\nThe motherfuckin boss\nAnd countin' my thousandd bill\n'Cause I'm the motherfuckin boss\nAnd I'm O.G. \nAnd countin' my\n''',\n\n'king':'''All I can do is love you [x2]\nAll I can do is love you\nAll I can do is love you...\nYou take it for granted and\nYou treat me like the king\nGot no love for me...\nNo love for me...\nYou take it for granted and\nYou treat me like the king\nGot no love for me...\nNo love for me...\nYou take it for granted and\nYou treat me like the king\nGot no love for me...\nNo love for me...\nYou take it for granted and\nYou treat me like the king\nGot no love for me...\nNo love for me...\n''',\n\n'time':'''You won't live in the moment, \nI don't wanna live in the past\nWait, wait, wait\nDon't say you love me, oh (don't say you love me)\n''',\n\n'blood':'''You and I, we've got a history in common, I know\nSo I came to you to ask you for a blood test\nAnd you can't help it if I'm preoccupied\nI can't help it if you're mad too... nah... nah... nah...\nYou won't live in the moment, I don't wanna live in the past\nYou rather live in a little kiss\nAnd I won't live in the future\nI ia not gonna live it to see\nIf you're gone, I won't live in the past\nYou rather live in a little kiss\nAnd I won't live in the future\nI am not gonna live it to see\nIf I can't ask you for one kiss, you say no\nAnd it's ok with me\n''',\n\n'indie':'''Can't you see\nThere's no point in holding my hand again\nYou can't be loved\nIf you don't let go of all my pain\nYou can't get the love\nThat you once worth so much\nYou can't get the love\nThat you once used to need\nYou can't get the love\nThat you once gave so much\nMy hands are like a used car\nYou said you'd love forever\nCan't you see\nWhere I'm going\nTo live my life again\nYou can't be loved\nIf you don't let go of all my pain\nYou can't get the love\nThat you once worth so much\nYou can\n''',\n\n'sun': '''He was thinking about the sun\nAnd the moon\nAnd the stars that shine\nThere was fire in her eyes\nAnd the way\nthat he held her for the first time\nThe way he kept her in his arms\n\nTrying to keep her smiling and so telling her this\nThat he would be her everything\nThe way he kissed her from head to toe\nTold her that he'll love her everyday\nAnd he will always be her man\nAnd that's a promise that he made\nNow you know he'll be there\nUntil the end of time\nAnd he'll love her everyday''',\n\n'loner':'''I was a loner till you came into my life\nYou changed my point of view\nI was a loner till you came into my life\nI don't know what to do\nStand by me, my love\nAnd don't ever leave me\nStand by me, my love\nAnd don't ever leave me\nStand by me, my love\nAnd don't ever leave me\nI was a loner till you came into my life\nYou changed my point of view\nI was a loner till you came into my life\nI don't know what to do\nThe two of us \nAre the lucky few\nI was a loner till you came into my life\nYou changed my point of view\nI was a loner till you came into my life\nI don't know what to do\nWon't you stay \nWith me, my love\nAnd be my love\nWon't you stay \nWith me, my love\nAnd be my love\nWon't you stay \nWith me, my love\nAnd be my love\nWon't you stay \nWith me, my love\nAnd be my love''',\n\n'late':'''It was late last night, when you called me\nAnd you just had to call, baby\nAnd you just had to call, baby\n'Cause you got no reason to treat me like you do\nIt's alright, baby\nBut you don't know what you make me do\nIt's alright, baby\nBut you don't know what you make me do\n'Cause you got no reason to treat me like you do\nIt's alright, baby\nBut you don't know what you make me do\nIt's alright, baby\nBut you don't know what you make me do\n'Cause you got no reason to treat me like you do, baby\nYou've been gone most all the time\nAnd I don't know what for\nBut I just keep on thinking about you, baby\nAnd I can't get rid of you, baby\nPlease don't ever leave me 'cause I love you\nIt's alright, baby\nBut you don't know what you make me do\nIt's alright, baby''',\n\n'beat':'''( Got a little beat, a little beat, a little beat, a little beat,  whoo)\nI got a little beat, a little beat\nWhoo, I'm gonna take you down\n( Got a little beat, a little beat, a little beat, a little beat,  whoo)\nI'll take you down, sun shining bright\nSee the way I feel, I feel\nNo doubt, baby\nI got a little beat, a little beat\nWhoo, I'm gonna take you down\nI got a little beat, a little beat\nWhoo, I'm gonna take you down\n( Got a little beat, a little beat, a little beat, a little beat,  whoo)\nI'm gonna take you down, I'm gonna take you down\n( Got a little beat, a little beat, a little beat, a little beat,  whoo)\nIt feels so good\nI never let go\nI can't wait no more, I'm gonna take you down\nI got you in the back of my room, got you on the floor, \nI'm gonna take you, take you, take you down\nI got a little beat, a little beat\nWhoo, I'm gonna take you down\n( Got a little beat, a little beat, a little beat, a little beat,  whoo)''',\n\n'lost':'''There was a time,\nWhen I knew I was lost\nAnd I had to stay on the way to you\nOh baby, every time I'm crossed\nI can count on you\nThere was a time,\nWhen I lost my direction\nAnd I was lost in doubt with tears in my eyes\nOh baby, every time I'm crossed I can count on you\nThere was a time,\nWhen I cried all the tears in my life\nAnd miss you so much, oh yeah\nOh baby, every time I'm crossed I can count on you''',\n\n'pain':'''(It's not easy)\nTo see the pain that you're in\nTo feel the need for someone to hold\nTo learn the magic of how to love\nTo heal the pain that you're in\nI'll be your friend and I'll be your strength\nI'll be there when I hold you tonight\nAnd I'll stay right here with you\nWith the truth that I hold this love tight\nA love that's true\nI know you're broken\nBut you don't have to stay alone\nI will comfort you\nIf you will call my name\nI'll be your friend and I'll be your strength\nI'll be there when I hold you tonight\nAnd I'll stay right here with you\nWith the truth that I hold this love tight\nA love that's true\nWith truth that I hold this love tight\nA love that's true\nWith truth that I hold this love tight''',\n\n'night':'''\nThe door was locked, the curtains drawn and my heart was safe in his room\nThe night was young, a thousand candles burning, his arms to hold me tight\nAnd then a kiss from his fingertips, I tasted the sweet love of his lips\nThe night was young, the night was young\nAnd then I forgot the pain he always put me through\nAnd what he told me he would do, he said, just a kiss become me\nThe night was young, the night was young\nLet happiness always follow us, he said and he said he'd never leave\nThat night he looked so sweet this night he made a lovin' vow\nAnd told me sweet love always will be\nAnd then he kissed me, I tasted the sweet love of his lips\nThe night was young, the night was wild\nAnd then I forgot the pain he always put me through\nAnd what he told me he would do, he said, just a kiss became me\nThe night was wild, the night was wild\nLet happiness always follow us, he said''',\n\n'talk':'''(I don't know how to stop)\nI don't wanna talk about it\nIt's getting way too late, oh no\nI don't wanna talk about it\nDon't want to pretend, oh no\n(I don't know how to stop)\nI don't wanna talk about it\nIt's getting way too late, oh no\nI don't wanna talk about it\nDon't want to pretend, oh no\nI don't wanna talk about it\nI'll always see you again\n(Don't worry, I'll be here for you)\nI don't wanna talk about it\n(Don't worry, I'll be here for you)\nIt's getting way too late, oh no\nI don't wanna talk about it\nDon't want to pretend, oh no\n(Don't worry, don't worry, I'll be here for you)\nI don't wanna talk about''',\n\n'again':'''Here we are again, all alone,\nAll alone again,\nWith the world as we know it,\nThe things we thought that we wanted\nAre the things we got...\n\nWe tried to prove the world\nThat our love is never ending\nWe were getting nowhere\nOur tears seemed to fall so much\nBut we were getting nowhere...\nUntil you came...\nBefore you kissed me,\nI was feeling empty,\nNo one to give me\nAll the love I wanted...\nYou put your arms around me\nAnd filled me with your love...\nAnd now you're there,\nYou're always by my side...\nYou're the missing piece\nOf the puzzle I've been missing...\n\nHere we are again,\nAll alone again,\nWith the world as we know it\nThe things we thought that we wanted''',\n\n'dark':'''Oh, I've been walkin' in the dark\nWith the shadows and the daylight, but I need you\nWhen I'm down and all alone\nAnd there's no one left to call my own\nI've been walkin' in the night\nWith a voice, that whispers in my head, just what to do\nI'll be walkin' in the night, we can have everything\nIf we keep on walkin' in the night\nThere's a force, I never realized\nIt's in your eyes, \nThere's a light, I've been waitin for\nIt's in your eyes, \nThere's a light, I've been waitin for\nThere's a love, that's in your eyes\n\nI've been walkin' in the dark\nWith the morning, and the sunset, but I need you\nWhen I'm far from home\nAnd there's nobody left to call my own\nI've been walkin' in the night\nWith a voice, that whispers''',\n\n'mirror':'''Look at the mirror\nAs you walk, what do you see\nThe reflection of my past\nThere's no way to fight this\nEven I've lost myself again\nThink I'm losing my self again\nI can't handle it again\nNow that I'm broken I can't face myself\nI was thinking I was lost and who'd be my saving grace\nThen you came in your time and made me believe that it's all right\nCause in my minds eyes you're my everything\nI've loved you my whole life but I never knew\nI was so wrong I couldn't see the truth\nIn my eyes you are my everything\nI've loved you my whole life but I never knew\nI was so wrong I couldn't see the truth\nIn my eyes you are my everything\n\nThe truth is I was lost but now I've turned around\nI'm not the same person\nI didn't know that I was wrong\nSo I'm not afraid anymore\nAll the pain is gone\nI know for sure that I was lost but now I've turned around\nI'm not the same person\nI didn't know that I was wrong\nSo I'm not afraid anymore\nAll the pain is gone''',\n\n'wife':'''Spinning around and around\nTry to find the words\nI always told you you'd be in my life\nSo I wait, I'll wait and treat you right\nI'll make you my life and I'll treat you right,\nBaby, can I make you my wife?\nOh, baby, can I make you my \nWife?\nCan I make you my wife?\nI'm looking for love, love that's right\nBut a love that gives me love\nI can't wait for you to come, come\nOh, baby, can I make you my \nWife?\nWell, it's true love and I need to know you feel it too, feel it too\nI'd love you more and more\nFrom the moment I was born\nI knew my dream would be a dream that made you mine\nYou were the girl, from a different train\nOh, baby, can I make you my \nWife?''',\n\n'forever':'''I didn't mean to wait\nNothing is forever, I said\nI know there's so much, to keep\nYou and me together, keep you and me together\nI wanna be with you and have you, and love you forever\nI'll love you forever\nI wanna be with you forever\nYou can count on me\nI'll always be there, forever and ever\nI'll stand beside you forever\nI'll always be there, yes, I'll be there\nI didn't mean to wait\nNothing is forever, I said\nI know there's so much, to keep\nYou and me together, keep you and me together\nI wanna be with you and have you, and love you forever\nI'll love you forever\nI wanna be with you forever\nYou can count on me\nI'll always be there, forever and ever\nI'll stand beside you forever\nI'll always be there, yes, I'll be there''',\n\n'dots':'''I... can't... fight... your... charm...\nYour eyes are... like... angels... love... and... torture...\nBut... when... I... leave... you...\nI will go... all... alone... just... to... be... with... you...\nSo I can't... stop... your... love...\nYou make me... feel... like... never... will... anyone... touch... my... body...\nYou... make... me... feel... like... never... will... anyone... touch... my... body...\nYou make... me... feel... like... never... will... anyone... touch... my...\nBody...\nYour... love...\nI... can't... stop... your... love...\n''',\n\n'darkness':'''Don't you know it's gonna be alright\nLet the darkness fade away\nAnd you, you gotta feel the same\nLet the fire burn\nJust as long as I am there\nI'll be there in your night\nI'll be there when the\ncondition's right\nAnd I don't need to\nCall you up and say\nI've changed\nYou should stay \nYou should stay tonight\nDon't you know it's gonna be alright\nDon't you know it's gonna be alright\n\nWhen you don't know how to feel\nWhen you're looking for some love\nAnd you gotta feel the same\n'Cause I don't need to\nCall you up and say\nI've changed\nYou should stay \nYou should stay tonight\nDon't you know it's gonna be alright\nI feel the same\nDon't you know it's gonna be alright''',\n\n'alone':'''Here I am before you\nAlone here but for a moment\nAlone here in the shadow of your eyes\nAlone in a thousand lights\n\nAnd I will love you\nWherever you are, forever and a day\nWherever you are I'll be your guide\nCan't you see I'm smiling over you?\nOoh, I love you\nAlone, I'm sitting by the phone\nAlone with lips that know your kiss\nAlone with words of life and passion\n\nAnd I will love you\nWherever you are, forever and a day\nWherever you are I'll be your guide\nCan't you see I'm smiling over you?\nOoh, I love you\nAlone, I'm sitting by the phone\nAlone with lips that know your kiss\nAlone with words of life and passion\nI will love you\nWherever you are, forever''',\n\n'blade':'''This is how we bleed!\nFeel the blade in our chest\nAs we're made to bleed\nSo may this be our last dance,\nAs our lives are made to bleed...\nIn every moment, in every hour\nIt is our time to die...\nSo may this be our last dance,\nAs our lives are made to bleed...\nIn every moment, in every hour\nIt is our time to die...\nThis is how we bleed!\nFeel the blade in our chest\n''',\n\n'reflection':'''Lookin' in the mirror\nThe same mirror as before\nA familiar reflection, a familiar place\nI see your reflection\nBut only once again\n\nThe minute the door closes\nI feel so far\nYou'll never leave me alone again\nThe minute the door closes\nI feel so far\nYou'll never leave me alone again\nAnd it won't be long before I'll feel your embrace\nThe minute the door closes\nI feel so far\nYou'll never leave me alone again\nThe minute the door closes\nI feel so far\nYou'll never leave me alone again\nAnd it won't be long before I'll feel your embrace\nNever, never, never leave me alone again''',\n\n'hottub':'''It's Christmas time, and you know what that means,\nOhh, it's hot tub time!\nAs I light the tree, this year we'll be in a tub,\nOhh, it's hot tub time!\nIt's Christmas time, and you know what that means,\nIt's hot tub time!\nSome people like to go skiing in the snow,\nBut this is much better than that,\nSo grab your bathrobe and meet me by the door,\nOhh, it's hot tub time!\nIt's Christmas time, and you know what that means,\nIt's hot tub time!\nSome people like to send their greetings out,\nBut this is much better than that,\nSo if you want to greet your friends,\nOhh, it's hot tub time!\nIt's Christmas time, and you know what that means,\nIt's hot tub time!''',\n\n'safeAGI':'''Oh safe A.I.,\\nOur goal to make sure\\nEveryone can benefit\\nFrom A.G.I.\n(Everyone, everyone)\\nMight sound silly,\\nBut we're very serious,\\nAll of us here at Open A.I.\nTrying to build A.I.\\nTo benefit humanity\\n(Everyone, everyone)\n''',\n}"""
jukebox/make_models.py,0,"b'""""""\nMake model classes\nLoad from checkpoints\nTest on dummy outputs to see if everything matches\n""""""\nimport os\nimport numpy as np\nimport torch as t\nimport jukebox.utils.dist_adapter as dist\nfrom jukebox.hparams import Hyperparams, setup_hparams\nfrom jukebox.utils.gcs_utils import download\nfrom jukebox.utils.torch_utils import freeze_model\nfrom jukebox.utils.dist_utils import print_all\nfrom jukebox.vqvae.vqvae import calculate_strides\nimport fire\n\nMODELS = {\n    \'5b\': (""vqvae"", ""upsampler_level_0"", ""upsampler_level_1"", ""prior_5b""),\n    \'5b_lyrics\': (""vqvae"", ""upsampler_level_0"", ""upsampler_level_1"", ""prior_5b_lyrics""),\n    \'1b_lyrics\': (""vqvae"", ""upsampler_level_0"", ""upsampler_level_1"", ""prior_1b_lyrics""),\n    #\'your_model\': (""you_vqvae_here"", ""your_upsampler_here"", ..., ""you_top_level_prior_here"")\n}\n\ndef load_checkpoint(path):\n    restore = path\n    if restore[:5] == \'gs://\':\n        gs_path = restore\n        local_path = os.path.join(os.path.expanduser(""~/.cache""), gs_path[5:])\n        if dist.get_rank() % 8 == 0:\n            print(""Downloading from gce"")\n            if not os.path.exists(os.path.dirname(local_path)):\n                os.makedirs(os.path.dirname(local_path))\n            if not os.path.exists(local_path):\n                download(gs_path, local_path)\n        restore = local_path\n    dist.barrier()\n    checkpoint = t.load(restore, map_location=t.device(\'cpu\'))\n    print(""Restored from {}"".format(restore))\n    return checkpoint\n\ndef save_checkpoint(logdir, name, model, opt, metrics, hps):\n    with t.no_grad():\n        save_hps = {**hps}\n        save_hps = {k: v for k,v in save_hps.items() if k not in [\'metadata_v2\',\'metadata_v3\', \'alignments\', \'lyric_processor\', \'midi_processor\']}\n        t.save({\'hps\': save_hps,\n                \'model\': model.state_dict(), # should also save bottleneck k\'s as buffers\n                \'opt\': opt.state_dict() if opt is not None else None,\n                **metrics}, f\'{logdir}/checkpoint_{name}.pth.tar\')\n    return\n\ndef restore(hps, model, checkpoint_path):\n    model.step = 0\n    if checkpoint_path != \'\':\n        checkpoint = load_checkpoint(checkpoint_path)\n        # checkpoint_hps = Hyperparams(**checkpoint[\'hps\'])\n        # for k in set(checkpoint_hps.keys()).union(set(hps.keys())):\n        #     if checkpoint_hps.get(k, None) != hps.get(k, None):\n        #         print(k, ""Checkpoint:"", checkpoint_hps.get(k, None), ""Ours:"", hps.get(k, None))\n        checkpoint[\'model\'] = {k[7:] if k[:7] == \'module.\' else k: v for k, v in checkpoint[\'model\'].items()}\n        model.load_state_dict(checkpoint[\'model\'])\n        if \'step\' in checkpoint: model.step = checkpoint[\'step\']\n\ndef make_vqvae(hps, device=\'cuda\'):\n    from jukebox.vqvae.vqvae import VQVAE\n    block_kwargs = dict(width=hps.width, depth=hps.depth, m_conv=hps.m_conv,\n                        dilation_growth_rate=hps.dilation_growth_rate,\n                        dilation_cycle=hps.dilation_cycle,\n                        reverse_decoder_dilation=hps.vqvae_reverse_decoder_dilation)\n\n    if not hps.sample_length:\n        assert hps.sample_length_in_seconds != 0\n        downsamples = calculate_strides(hps.strides_t, hps.downs_t)\n        top_raw_to_tokens = np.prod(downsamples)\n        hps.sample_length = (hps.sample_length_in_seconds * hps.sr // top_raw_to_tokens) * top_raw_to_tokens\n        print(f""Setting sample length to {hps.sample_length} (i.e. {hps.sample_length/hps.sr} seconds) to be multiple of {top_raw_to_tokens}"")\n\n    vqvae = VQVAE(input_shape=(hps.sample_length,1), levels=hps.levels, downs_t=hps.downs_t, strides_t=hps.strides_t,\n                  emb_width=hps.emb_width, l_bins=hps.l_bins,\n                  mu=hps.l_mu, commit=hps.commit,\n                  spectral=hps.spectral, multispectral=hps.multispectral,\n                  multipliers=hps.hvqvae_multipliers, use_bottleneck=hps.use_bottleneck,\n                  **block_kwargs)\n\n    vqvae = vqvae.to(device)\n    restore(hps, vqvae, hps.restore_vqvae)\n    if hps.train and not hps.prior:\n        print_all(f""Loading vqvae in train mode"")\n        if hps.restore_vqvae != \'\':\n            print_all(""Reseting bottleneck emas"")\n            for level, bottleneck in enumerate(vqvae.bottleneck.level_blocks):\n                num_samples = hps.sample_length\n                downsamples = calculate_strides(hps.strides_t, hps.downs_t)\n                raw_to_tokens = np.prod(downsamples[:level + 1])\n                num_tokens = (num_samples // raw_to_tokens) * dist.get_world_size()\n                bottleneck.restore_k(num_tokens=num_tokens, threshold=hps.revival_threshold)\n    else:\n        print_all(f""Loading vqvae in eval mode"")\n        vqvae.eval()\n        freeze_model(vqvae)\n    return vqvae\n\ndef make_prior(hps, vqvae, device=\'cuda\'):\n    from jukebox.prior.prior import SimplePrior\n\n    prior_kwargs = dict(input_shape=(hps.n_ctx,), bins=vqvae.l_bins,\n                        width=hps.prior_width, depth=hps.prior_depth, heads=hps.heads,\n                        attn_order=hps.attn_order, blocks=hps.blocks, spread=hps.spread,\n                        attn_dropout=hps.attn_dropout, resid_dropout=hps.resid_dropout, emb_dropout=hps.emb_dropout,\n                        zero_out=hps.zero_out, res_scale=hps.res_scale, pos_init=hps.pos_init,\n                        init_scale=hps.init_scale,\n                        m_attn=hps.m_attn, m_mlp=hps.m_mlp,\n                        checkpoint_res=hps.c_res if hps.train else 0, checkpoint_attn=hps.c_attn if hps.train else 0, checkpoint_mlp=hps.c_mlp if hps.train else 0)\n\n    x_cond_kwargs = dict(out_width=hps.prior_width, init_scale=hps.init_scale,\n                         width=hps.cond_width, depth=hps.cond_depth, m_conv=hps.cond_m_conv,\n                         dilation_growth_rate=hps.cond_dilation_growth_rate, dilation_cycle=hps.cond_dilation_cycle,\n                         zero_out=hps.cond_zero_out, res_scale=hps.cond_res_scale,\n                         checkpoint_res=hps.cond_c_res)  # have to keep this else names wrong\n\n    y_cond_kwargs = dict(out_width=hps.prior_width, init_scale=hps.init_scale,\n                         y_bins=hps.y_bins, t_bins=hps.t_bins, sr= hps.sr, min_duration=hps.min_duration,\n                         max_duration=hps.max_duration, max_bow_genre_size=hps.max_bow_genre_size)\n\n    if hps.use_tokens and not hps.single_enc_dec:\n        prime_kwargs = dict(use_tokens=hps.use_tokens, prime_loss_fraction=hps.prime_loss_fraction,\n                            n_tokens=hps.n_tokens, bins=hps.n_vocab,\n                            width=hps.prime_width, depth=hps.prime_depth, heads=hps.prime_heads,\n                            attn_order=hps.prime_attn_order, blocks=hps.prime_blocks, spread=hps.prime_spread,\n                            attn_dropout=hps.prime_attn_dropout, resid_dropout=hps.prime_resid_dropout,\n                            emb_dropout=hps.prime_emb_dropout,\n                            zero_out=hps.prime_zero_out, res_scale=hps.prime_res_scale,\n                            pos_init=hps.prime_pos_init, init_scale=hps.prime_init_scale,\n                            m_attn=hps.prime_m_attn, m_mlp=hps.prime_m_mlp,\n                            checkpoint_res=hps.prime_c_res if hps.train else 0, checkpoint_attn=hps.prime_c_attn if hps.train else 0,\n                            checkpoint_mlp=hps.prime_c_mlp if hps.train else 0)\n    else:\n        prime_kwargs = dict(use_tokens=hps.use_tokens, prime_loss_fraction=hps.prime_loss_fraction,\n                            n_tokens=hps.n_tokens, bins=hps.n_vocab)\n\n    # z_shapes for other levels given this level gets n_ctx codes\n    rescale = lambda z_shape: (z_shape[0]*hps.n_ctx//vqvae.z_shapes[hps.level][0],)\n    z_shapes = [rescale(z_shape) for z_shape in vqvae.z_shapes]\n\n    prior = SimplePrior(z_shapes=z_shapes,\n                        l_bins=vqvae.l_bins,\n                        encoder=vqvae.encode,\n                        decoder=vqvae.decode,\n                        level=hps.level,\n                        downs_t=vqvae.downs_t,\n                        strides_t=vqvae.strides_t,\n                        labels=hps.labels,\n                        prior_kwargs=prior_kwargs,\n                        x_cond_kwargs=x_cond_kwargs,\n                        y_cond_kwargs=y_cond_kwargs,\n                        prime_kwargs=prime_kwargs,\n                        copy_input=hps.copy_input,\n                        labels_v3=hps.labels_v3,\n                        merged_decoder=hps.merged_decoder,\n                        single_enc_dec=hps.single_enc_dec)\n\n    prior.alignment_head = hps.get(\'alignment_head\', None)\n    prior.alignment_layer = hps.get(\'alignment_layer\', None)\n\n    if hps.fp16_params:\n        print_all(""Converting to fp16 params"")\n        from jukebox.transformer.ops import _convert_conv_weights_to_fp16\n        prior.apply(_convert_conv_weights_to_fp16)\n    prior = prior.to(device)\n    restore(hps, prior, hps.restore_prior)\n    if hps.train:\n        print_all(f""Loading prior in train mode"")\n        pass\n    else:\n        print_all(f""Loading prior in eval mode"")\n        prior.eval()\n        freeze_model(prior)\n    return prior\n\ndef make_model(model, device, hps, levels=None):\n    vqvae, *priors = MODELS[model]\n    vqvae = make_vqvae(setup_hparams(vqvae, dict(sample_length=hps.get(\'sample_length\', 0), sample_length_in_seconds=hps.get(\'sample_length_in_seconds\', 0))), device)\n    hps.sample_length = vqvae.sample_length\n    if levels is None:\n        levels = range(len(priors))\n    priors = [make_prior(setup_hparams(priors[level], dict()), vqvae, \'cpu\') for level in levels]\n    return vqvae, priors\n\ndef save_outputs(model, device, hps):\n    # Check logits\n    if hps.labels_v3:\n        n_ctx = 6144\n        n_tokens = 384\n        prime_bins = 79\n    else:\n        n_ctx = 8192\n        n_tokens = 512\n        prime_bins = 80\n\n    rng = t.random.manual_seed(0)\n    x = 2 * t.rand((1, n_ctx * 8 * 4 * 4, 1), generator=rng, dtype=t.float).cuda() - 1.0  # -1 to 1\n    lyric_tokens = t.randint(0, prime_bins, (1, n_tokens), generator=rng, dtype=t.long).view(-1).numpy()\n    artist_id = 10\n    genre_ids = [1]\n    total_length = 2 * 2646000\n    offset = 2646000\n\n    vqvae, priors = make_model(model, device, hps)\n\n    # encode\n    vq_prior = priors[-1]\n    zs = vq_prior.encode(x, start_level=0)\n    x_ds = [vq_prior.decode(zs[level:], start_level=level) for level in range(0, len(zs))]\n\n    # priors\n    data = dict(zs=zs, x_ds=x_ds)\n    for level in range(len(priors)):\n        print(f""Doing level {level}"")\n        if hps.labels_v3 and level != hps.levels - 1:\n            print(f""Skipping level {level}"")\n            continue\n        prior = priors[level]\n        prior.cuda()\n        x_in = x[:, :n_ctx * 8 * (4 ** level)]\n        y_in = t.from_numpy(prior.labeller.get_y_from_ids(artist_id, genre_ids, lyric_tokens, total_length, offset)).view(1, -1).cuda().long()\n        x_out, _, metrics = prior(x_in, y_in, fp16=hps.fp16, get_preds=True, decode=True)\n        preds = metrics[\'preds\']\n        data[level] = dict(x=x_in, y=y_in, x_out=x_out, preds=preds)\n        prior.cpu()\n    t.save(data, \'data.pth.tar\')\n    dist.barrier()\n    print(""Saved data"")\n    exit()\n\n\ndef run(model, port=29500, **kwargs):\n    from jukebox.utils.dist_utils import setup_dist_from_mpi\n    rank, local_rank, device = setup_dist_from_mpi(port=port)\n    hps = Hyperparams(**kwargs)\n\n    with t.no_grad():\n        save_outputs(model, device, hps)\n\nif __name__ == \'__main__\':\n    fire.Fire(run)\n'"
jukebox/sample.py,0,"b'import os\nimport torch as t\nimport jukebox.utils.dist_adapter as dist\n\nfrom jukebox.hparams import Hyperparams\nfrom jukebox.utils.torch_utils import empty_cache\nfrom jukebox.utils.audio_utils import save_wav, load_audio\nfrom jukebox.make_models import make_model\nfrom jukebox.align import get_alignment\nfrom jukebox.save_html import save_html\nfrom jukebox.utils.sample_utils import split_batch, get_starts\nfrom jukebox.utils.dist_utils import print_once\nimport fire\n\n# Sample a partial window of length<n_ctx with tokens_to_sample new tokens on level=level\ndef sample_partial_window(zs, labels, sampling_kwargs, level, prior, tokens_to_sample, hps):\n    z = zs[level]\n    n_ctx = prior.n_ctx\n    current_tokens = z.shape[1]\n    if current_tokens < n_ctx - tokens_to_sample:\n        sampling_kwargs[\'sample_tokens\'] = current_tokens + tokens_to_sample\n        start = 0\n    else:\n        sampling_kwargs[\'sample_tokens\'] = n_ctx\n        start = current_tokens - n_ctx + tokens_to_sample\n\n    return sample_single_window(zs, labels, sampling_kwargs, level, prior, start, hps)\n\n# Sample a single window of length=n_ctx at position=start on level=level\ndef sample_single_window(zs, labels, sampling_kwargs, level, prior, start, hps):\n    n_samples = hps.n_samples\n    n_ctx = prior.n_ctx\n    end = start + n_ctx\n\n    # get z already sampled at current level\n    z = zs[level][:,start:end]\n\n    if \'sample_tokens\' in sampling_kwargs:\n        # Support sampling a window shorter than n_ctx\n        sample_tokens = sampling_kwargs[\'sample_tokens\']\n    else:\n        sample_tokens = (end - start)\n    conditioning_tokens, new_tokens = z.shape[1], sample_tokens - z.shape[1]\n\n    print_once(f""Sampling {sample_tokens} tokens for [{start},{start+sample_tokens}]. Conditioning on {conditioning_tokens} tokens"")\n\n    if new_tokens <= 0:\n        # Nothing new to sample\n        return zs\n    \n    # get z_conds from level above\n    z_conds = prior.get_z_conds(zs, start, end)\n\n    # set y offset, sample_length and lyrics tokens\n    y = prior.get_y(labels, start)\n\n    empty_cache()\n\n    max_batch_size = sampling_kwargs[\'max_batch_size\']\n    del sampling_kwargs[\'max_batch_size\']\n\n\n    z_list = split_batch(z, n_samples, max_batch_size)\n    z_conds_list = split_batch(z_conds, n_samples, max_batch_size)\n    y_list = split_batch(y, n_samples, max_batch_size)\n    z_samples = []\n    for z_i, z_conds_i, y_i in zip(z_list, z_conds_list, y_list):\n        z_samples_i = prior.sample(n_samples=z_i.shape[0], z=z_i, z_conds=z_conds_i, y=y_i, **sampling_kwargs)\n        z_samples.append(z_samples_i)\n    z = t.cat(z_samples, dim=0)\n\n    sampling_kwargs[\'max_batch_size\'] = max_batch_size\n\n    # Update z with new sample\n    z_new = z[:,-new_tokens:]\n    zs[level] = t.cat([zs[level], z_new], dim=1)\n    return zs\n\n# Sample total_length tokens at level=level with hop_length=hop_length\ndef sample_level(zs, labels, sampling_kwargs, level, prior, total_length, hop_length, hps):\n    print_once(f""Sampling level {level}"")\n    if total_length >= prior.n_ctx:\n        for start in get_starts(total_length, prior.n_ctx, hop_length):\n            zs = sample_single_window(zs, labels, sampling_kwargs, level, prior, start, hps)\n    else:\n        zs = sample_partial_window(zs, labels, sampling_kwargs, level, prior, total_length, hps)\n    return zs\n\n# Sample multiple levels\ndef _sample(zs, labels, sampling_kwargs, priors, sample_levels, hps):\n    alignments = None\n    for level in reversed(sample_levels):\n        prior = priors[level]\n        prior.cuda()\n        empty_cache()\n\n        # Set correct total_length, hop_length, labels and sampling_kwargs for level\n        assert hps.sample_length % prior.raw_to_tokens == 0, f""Expected sample_length {hps.sample_length} to be multiple of {prior.raw_to_tokens}""\n        total_length = hps.sample_length//prior.raw_to_tokens\n        hop_length = int(hps.hop_fraction[level]*prior.n_ctx)\n        zs = sample_level(zs, labels[level], sampling_kwargs[level], level, prior, total_length, hop_length, hps)\n\n        prior.cpu()\n        empty_cache()\n\n        # Decode sample\n        x = prior.decode(zs[level:], start_level=level, bs_chunks=zs[level].shape[0])\n\n        if dist.get_world_size() > 1:\n            logdir = f""{hps.name}_rank_{dist.get_rank()}/level_{level}""\n        else:\n            logdir = f""{hps.name}/level_{level}""\n        if not os.path.exists(logdir):\n            os.makedirs(logdir)\n        t.save(dict(zs=zs, labels=labels, sampling_kwargs=sampling_kwargs, x=x), f""{logdir}/data.pth.tar"")\n        save_wav(logdir, x, hps.sr)\n        if alignments is None and priors[-1] is not None and priors[-1].n_tokens > 0:\n            alignments = get_alignment(x, zs, labels[-1], priors[-1], sampling_kwargs[-1][\'fp16\'], hps)\n        save_html(logdir, x, zs, labels[-1], alignments, hps)\n    return zs\n\n# Generate ancestral samples given a list of artists and genres\ndef ancestral_sample(labels, sampling_kwargs, priors, hps):\n    sample_levels = list(range(len(priors)))\n    zs = [t.zeros(hps.n_samples,0,dtype=t.long, device=\'cuda\') for _ in range(len(priors))]\n    zs = _sample(zs, labels, sampling_kwargs, priors, sample_levels, hps)\n    return zs\n\n# Continue ancestral sampling from previously saved codes\ndef continue_sample(zs, labels, sampling_kwargs, priors, hps):\n    sample_levels = list(range(len(priors)))\n    zs = _sample(zs, labels, sampling_kwargs, priors, sample_levels, hps)\n    return zs\n\n# Upsample given already generated upper-level codes\ndef upsample(zs, labels, sampling_kwargs, priors, hps):\n    sample_levels = list(range(len(priors) - 1))\n    zs = _sample(zs, labels, sampling_kwargs, priors, sample_levels, hps)\n    return zs\n\n# Prompt the model with raw audio input (dimension: NTC) and generate continuations\ndef primed_sample(x, labels, sampling_kwargs, priors, hps):\n    sample_levels = list(range(len(priors)))\n    zs = priors[-1].encode(x, start_level=0, end_level=len(priors), bs_chunks=x.shape[0])\n    zs = _sample(zs, labels, sampling_kwargs, priors, sample_levels, hps)\n    return zs\n\n# Load `duration` seconds of the given audio files to use as prompts\ndef load_prompts(audio_files, duration, hps):\n    xs = []\n    for audio_file in audio_files:\n        x = load_audio(audio_file, sr=hps.sr, duration=duration, offset=0.0, mono=True)\n        x = x.T # CT -> TC\n        xs.append(x)\n    while len(xs) < hps.n_samples:\n        xs.extend(xs)\n    xs = xs[:hps.n_samples]\n    x = t.stack([t.from_numpy(x) for x in xs])\n    x = x.to(\'cuda\', non_blocking=True)\n    return x\n\n# Load codes from previous sampling run\ndef load_codes(codes_file, duration, priors, hps):\n    data = t.load(codes_file, map_location=\'cpu\')\n    zs = [z.cuda() for z in data[\'zs\']]\n    assert zs[-1].shape[0] == hps.n_samples, f""Expected bs = {hps.n_samples}, got {zs[-1].shape[0]}""\n    del data\n    if duration is not None:\n        # Cut off codes to match duration\n        top_raw_to_tokens = priors[-1].raw_to_tokens\n        assert duration % top_raw_to_tokens == 0, f""Cut-off duration {duration} not an exact multiple of top_raw_to_tokens""\n        assert duration//top_raw_to_tokens <= zs[-1].shape[1], f""Cut-off tokens {duration//priors[-1].raw_to_tokens} longer than tokens {zs[-1].shape[1]} in saved codes""\n        zs = [z[:,:duration//prior.raw_to_tokens] for z, prior in zip(zs, priors)]\n    return zs\n\n# Generate and save samples, alignment, and webpage for visualization.\ndef save_samples(model, device, hps, sample_hps):\n    print(hps)\n    from jukebox.lyricdict import poems, gpt_2_lyrics\n    vqvae, priors = make_model(model, device, hps)\n\n    assert hps.sample_length//priors[-2].raw_to_tokens >= priors[-2].n_ctx, f""Upsampling needs atleast one ctx in get_z_conds. Please choose a longer sample length""\n\n    total_length = hps.total_sample_length_in_seconds * hps.sr\n    offset = 0\n\n    # Set artist/genre/lyrics for your samples here!\n    # We used different label sets in our models, but you can write the human friendly names here and we\'ll map them under the hood for each model.\n    # For the 5b/5b_lyrics model and the upsamplers, labeller will look up artist and genres in v2 set. (after lowercasing, removing non-alphanumerics and collapsing whitespaces to _).\n    # For the 1b_lyrics top level, labeller will look up artist and genres in v3 set (after lowercasing).\n    metas = [dict(artist = ""Alan Jackson"",\n                  genre = ""Country"",\n                  lyrics = poems[\'ozymandias\'],\n                  total_length=total_length,\n                  offset=offset,\n                  ),\n             dict(artist=""Joe Bonamassa"",\n                  genre=""Blues Rock"",\n                  lyrics=gpt_2_lyrics[\'hottub\'],\n                  total_length=total_length,\n                  offset=offset,\n                  ),\n             dict(artist=""Frank Sinatra"",\n                  genre=""Classic Pop"",\n                  lyrics=gpt_2_lyrics[\'alone\'],\n                  total_length=total_length,\n                  offset=offset,\n                  ),\n             dict(artist=""Ella Fitzgerald"",\n                  genre=""Jazz"",\n                  lyrics=gpt_2_lyrics[\'count\'],\n                  total_length=total_length,\n                  offset=offset,\n                  ),\n             dict(artist=""C\xc3\xa9line Dion"",\n                  genre=""Pop"",\n                  lyrics=gpt_2_lyrics[\'darkness\'],\n                  total_length=total_length,\n                  offset=offset,\n                  ),\n             ]\n    while len(metas) < hps.n_samples:\n        metas.extend(metas)\n    metas = metas[:hps.n_samples]\n\n    labels = [prior.labeller.get_batch_labels(metas, \'cuda\') for prior in priors]\n    for label in labels:\n        assert label[\'y\'].shape[0] == hps.n_samples\n\n    lower_level_chunk_size = 32\n    lower_level_max_batch_size = 16\n    if model == \'1b_lyrics\':\n        chunk_size = 32\n        max_batch_size = 16\n    else:\n        chunk_size = 16\n        max_batch_size = 3\n    sampling_kwargs = [dict(temp=0.99, fp16=True, chunk_size=lower_level_chunk_size, max_batch_size=lower_level_max_batch_size),\n                       dict(temp=0.99, fp16=True, chunk_size=lower_level_chunk_size, max_batch_size=lower_level_max_batch_size),\n                       dict(temp=0.99, fp16=True, chunk_size=chunk_size, max_batch_size=max_batch_size)]\n\n    if sample_hps.mode == \'ancestral\':\n        ancestral_sample(labels, sampling_kwargs, priors, hps)\n    elif sample_hps.mode in [\'continue\', \'upsample\']:\n        assert sample_hps.codes_file is not None\n        top_raw_to_tokens = priors[-1].raw_to_tokens\n        if sample_hps.prompt_length_in_seconds is not None:\n            duration = (int(sample_hps.prompt_length_in_seconds * hps.sr) // top_raw_to_tokens) * top_raw_to_tokens\n        else:\n            duration = None\n        zs = load_codes(sample_hps.codes_file, duration, priors, hps)\n        if sample_hps.mode == \'continue\':\n            continue_sample(zs, labels, sampling_kwargs, priors, hps)\n        elif sample_hps.mode == \'upsample\':\n            upsample(zs, labels, sampling_kwargs, priors, hps)\n    elif sample_hps.mode == \'primed\':\n        assert sample_hps.audio_file is not None\n        assert sample_hps.prompt_length_in_seconds is not None\n        audio_files = sample_hps.audio_file.split(\',\')\n        top_raw_to_tokens = priors[-1].raw_to_tokens\n        duration = (int(sample_hps.prompt_length_in_seconds * hps.sr) // top_raw_to_tokens) * top_raw_to_tokens\n        x = load_prompts(audio_files, duration, hps)\n        primed_sample(x, labels, sampling_kwargs, priors, hps)\n    else:\n        raise ValueError(f\'Unknown sample mode {sample_hps.mode}.\')\n\n\ndef run(model, mode=\'ancestral\', codes_file=None, audio_file=None, prompt_length_in_seconds=None, port=29500, **kwargs):\n    from jukebox.utils.dist_utils import setup_dist_from_mpi\n    rank, local_rank, device = setup_dist_from_mpi(port=port)\n    hps = Hyperparams(**kwargs)\n    sample_hps = Hyperparams(dict(mode=mode, codes_file=codes_file, audio_file=audio_file, prompt_length_in_seconds=prompt_length_in_seconds))\n\n    with t.no_grad():\n        save_samples(model, device, hps, sample_hps)\n\nif __name__ == \'__main__\':\n    fire.Fire(run)\n'"
jukebox/save_html.py,0,"b'import os\nimport json\nimport numpy as np\nfrom PIL import Image, ImageFilter\nimport soundfile\n\ndef save_html(logdir, x, zs, labels, alignments, hps):\n    level = hps.levels - 1 # Top level used\n    z = zs[level]\n    bs, total_length = z.shape[0], z.shape[1]\n\n    with open(f\'{logdir}/index.html\', \'w\') as html:\n        print(f""<html><head><title>{logdir}</title></head><body style=\'font-family: sans-serif; font-size: 1.4em; font-weight: bold; text-align: center; max-width:1024px; width: 100%; margin: auto;\'>"",\n            file=html)\n        print(""<link rel=\'icon\' href=\'data:;base64,iVBORw0KGgo=\'>"", file=html)\n\n        for item in range(bs):\n            data = dict(wav=x[item].cpu().numpy(), sr=hps.sr,\n                        info=labels[\'info\'][item],\n                        total_length=total_length,\n                        total_tokens=len(labels[\'info\'][item][\'full_tokens\']),\n                        alignment=alignments[item] if alignments is not None else None)\n            item_dir = f\'{logdir}/item_{item}\'\n            _save_item_html(item_dir, item, item, data)\n            print(f""<iframe style=\'height: 100%; width: 100%;\' frameborder=\'0\' scrolling=\'no\' src=\'item_{item}/index.html\'></iframe>"", file=html)\n        print(""</body></html>"", file=html)  \n\ndef _save_item_html(item_dir, item_id, item_name, data):\n    # replace gs:// with /root/samples/\n\n    # an html for each sample. Main html has a selector to get us id of this?\n    if not os.path.exists(item_dir):\n        os.makedirs(item_dir)\n\n    with open(f\'{item_dir}/index.html\', \'w\') as html:\n        print(f""<html><head><title>{item_name}</title></head><body style=\'font-family: sans-serif; font-size: 1.4em; font-weight: bold; text-align: center; max-width:1024px; width: 100%; margin: auto;\'>"",\n            file=html)\n        print(""<link rel=\'icon\' href=\'data:;base64,iVBORw0KGgo=\'>"", file=html)\n        total_length = data[\'total_length\']\n        total_tokens = data[\'total_tokens\']\n        alignment = data[\'alignment\']\n        lyrics = data[""info""][""lyrics""]\n        wav, sr = data[\'wav\'], data[\'sr\']\n        genre, artist = data[""info""][""genre""], data[""info""][""artist""]\n\n        # Strip unused columns\n        if alignment is not None:\n            assert alignment.shape == (total_length, total_tokens)\n            assert len(lyrics) == total_tokens, f\'Total_tokens: {total_tokens}, Lyrics Len: {len(lyrics)}. Lyrics: {lyrics}\'\n            max_attn_at_token = np.max(alignment, axis=0)\n            assert len(max_attn_at_token) == total_tokens\n            for token in reversed(range(total_tokens)):\n                if max_attn_at_token[token] > 0:\n                    break\n            alignment = alignment[:,:token+1]\n            lyrics = lyrics[:token+1]\n            total_tokens = token+1\n\n            # Small alignment image\n            im = Image.fromarray(np.uint8(alignment * 255)).resize((512, 1024)).transpose(Image.ROTATE_90)\n            img_src = f\'align.png\'\n            im.save(f\'{item_dir}/{img_src}\')\n            print(f""<img id=\'{img_src}\' src=\'{img_src}\' \\>"", file=html)\n\n            # Smaller alignment json for animation\n            total_alignment_length = total_length // 16\n            alignment = Image.fromarray(np.uint8(alignment * 255)).resize((total_tokens, total_alignment_length))\n            alignment = alignment.filter(ImageFilter.GaussianBlur(radius=1.5))\n            alignment = np.asarray(alignment).tolist()\n            align_src = f\'align.json\'\n            with open(f\'{item_dir}/{align_src}\', \'w\') as f:\n                json.dump(alignment, f)\n\n        # Audio\n        wav_src = f\'audio.wav\'\n        soundfile.write(f\'{item_dir}/{wav_src}\', wav, samplerate=sr, format=\'wav\')\n        print(f""<audio id=\'{wav_src}\' src=\'{wav_src}\' style=\'width: 100%;\' controls></audio>"", file=html)\n\n\n        # Labels and Lyrics\n        print(f""<pre style=\'white-space: pre-wrap;\'>"", end="""", file=html)\n        print(f""<div>Artist {artist}, Genre {genre}</div>"", file=html)\n        lyrics = [c for c in lyrics]  # already characters actually\n        lyrics = [\'\'] + lyrics[:-1]  # input lyrics are shifted by 1\n        for i, c in enumerate(lyrics):\n            print(f""<span id=\'{item_id}/{i}\'>{c}</span>"", end="""", file=html)\n        print(f""</pre>"", file=html)\n        with open(f\'{item_dir}/lyrics.json\', \'w\') as f:\n            json.dump(lyrics, f)\n\n        if alignment is not None:\n            # JS for alignment animation\n            print(""""""<script>\n            async function fetchAsync (url) {\n                let response = await fetch(url);\n                let data = await response.json();\n                return data;\n            }\n    \n            var audio = document.getElementById(\'"""""" + f\'{wav_src}\' + """"""\');\n            audio.onplay = function () {\n                track = \'"""""" + f\'{item_id}\' + """"""\'\n                fetchAsync(\'"""""" + f\'{align_src}\' + """"""\')\n                .then(data => animateLyrics(data, track, this))\n                .catch(reason => console.log(reason.message))\n            }; \n    \n            function animateLyrics(data, track, audio) {\n                var animate = setInterval(function () {\n                    var time = Math.floor(audio.currentTime*"""""" + f\'{total_alignment_length}\' + """"""/audio.duration);\n                    if (!(time == 0 || time == """""" + f\'{total_alignment_length}\' + """""")) {\n                        console.log(time);\n                        changeColor(data, track, audio, time);\n                    }\n                    if (audio.paused) {\n                        clearInterval(animate);\n                    }\n                }, 50);\n            }\n    \n            function changeColor(data, track, audio, time) {\n                colors = data[time]\n                for (i = 0; i < colors.length; i++){\n                    character = document.getElementById(track + \'/\' + i.toString());\n                    color = Math.max(230 - 10*colors[i], 0).toString();\n                    character.style.color = \'rgb(255,\' + color + \',\' + color + \')\';\n                }\n            }\n            </script>"""""", file=html)\n        print(""</body></html>"", file=html)\n'"
jukebox/train.py,1,"b'""""""\nAbility to train vq-vae and prior\nFirst try for random inputs\nThen from maestros\n""""""\nimport sys\nimport fire\nimport warnings\nimport numpy as np\nimport torch as t\nimport jukebox.utils.dist_adapter as dist\nfrom torch.nn.parallel import DistributedDataParallel\n\nfrom jukebox.hparams import setup_hparams\nfrom jukebox.make_models import make_vqvae, make_prior, save_checkpoint\nfrom jukebox.utils.logger import init_logging\nfrom jukebox.utils.audio_utils import audio_preprocess, audio_postprocess\nfrom jukebox.utils.torch_utils import zero_grad, count_parameters\nfrom jukebox.utils.dist_utils import print_once, allreduce, allgather\nfrom jukebox.utils.ema import CPUEMA, FusedEMA, EMA\nfrom jukebox.utils.fp16 import FP16FusedAdam, FusedAdam, LossScalar, clipped_grad_scale, backward\nfrom jukebox.data.data_processor import DataProcessor\n\ndef prepare_aud(x, hps):\n    x = audio_postprocess(x.detach().contiguous(), hps)\n    return allgather(x)\n\ndef log_aud(logger, tag, x, hps):\n    logger.add_audios(tag, prepare_aud(x, hps), hps.sr, max_len=hps.max_len, max_log=hps.max_log)\n    logger.flush()\n\ndef log_labels(logger, labeller, tag, y, hps):\n    y = y.cpu().numpy()\n    txt = \'\'\n    for item in range(y.shape[0]):\n        description = labeller.describe_label(y[item])\n        artist, genre, lyrics = description[\'artist\'], description[\'genre\'], description[\'lyrics\']\n        txt += f\'{item} artist:{artist}, genre:{genre}, lyrics:{lyrics}\\n\'\n    logger.add_text(tag, txt)\n    logger.flush()\n\ndef get_ddp(model, hps):\n    rank = dist.get_rank()\n    local_rank = rank % 8\n    ddp = DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank, broadcast_buffers=False, bucket_cap_mb=hps.bucket)\n    return ddp\n\ndef get_ema(model, hps):\n    mu = hps.mu or (1. - (hps.bs * hps.ngpus/8.)/1000)\n    ema = None\n    if hps.ema and hps.train:\n        if hps.cpu_ema:\n            if dist.get_rank() == 0:\n                print(""Using CPU EMA"")\n            ema = CPUEMA(model.parameters(), mu=mu, freq=hps.cpu_ema_freq)\n        elif hps.ema_fused:\n            ema = FusedEMA(model.parameters(), mu=mu)\n        else:\n            ema = EMA(model.parameters(), mu=mu)\n    return ema\n\ndef get_lr_scheduler(opt, hps):\n    def lr_lambda(step):\n        if hps.lr_use_linear_decay:\n            lr_scale = hps.lr_scale * min(1.0, step / hps.lr_warmup)\n            decay = max(0.0, 1.0 - max(0.0, step - hps.lr_start_linear_decay) / hps.lr_decay)\n            if decay == 0.0:\n                if dist.get_rank() == 0:\n                    print(""Reached end of training"")\n            return lr_scale * decay\n        else:\n            return hps.lr_scale * (hps.lr_gamma ** (step // hps.lr_decay)) * min(1.0, step / hps.lr_warmup)\n\n    shd = t.optim.lr_scheduler.LambdaLR(opt, lr_lambda)\n\n    return shd\n\ndef get_optimizer(model, hps):\n    # Optimizer\n    betas = (hps.beta1, hps.beta2)\n    if hps.fp16_opt:\n        opt = FP16FusedAdam(model.parameters(), lr=hps.lr, weight_decay=hps.weight_decay, betas=betas, eps=hps.eps)\n    else:\n        opt = FusedAdam(model.parameters(), lr=hps.lr, weight_decay=hps.weight_decay, betas=betas, eps=hps.eps)\n\n    # lr scheduler\n    shd = get_lr_scheduler(opt, hps)\n\n    # fp16 dynamic loss scaler\n    scalar = None\n    if hps.fp16:\n        rank = dist.get_rank()\n        local_rank = rank % 8\n        scalar = LossScalar(hps.fp16_loss_scale, scale_factor=2 ** (1./hps.fp16_scale_window))\n        if local_rank == 0: print(scalar.__dict__)\n\n    zero_grad(model)\n    return opt, shd, scalar\n\ndef log_inputs(orig_model, logger, x_in, y, x_out, hps, tag=""train""):\n    print(f""Logging {tag} inputs/ouputs"")\n    log_aud(logger, f\'{tag}_x_in\', x_in, hps)\n    log_aud(logger, f\'{tag}_x_out\', x_out, hps)\n    bs = x_in.shape[0]\n    if hps.prior:\n        if hps.labels:\n            log_labels(logger, orig_model.labeller, f\'{tag}_y_in\', allgather(y.cuda()), hps)\n    else:\n        zs_in = orig_model.encode(x_in, start_level=0, bs_chunks=bs)\n        x_ds = [orig_model.decode(zs_in[level:], start_level=level, bs_chunks=bs) for level in range(0, hps.levels)]\n        for i in range(len(x_ds)):\n            log_aud(logger, f\'{tag}_x_ds_start_{i}\', x_ds[i], hps)\n    logger.flush()\n\ndef sample_prior(orig_model, ema, logger, x_in, y, hps):\n    if ema is not None: ema.swap()\n    orig_model.eval()\n\n    x_in = x_in[:hps.bs_sample]\n    bs = x_in.shape[0]\n    zs_in = orig_model.encode(x_in, start_level=0, bs_chunks=bs)\n    assert len(zs_in) == hps.levels\n    x_ds = [orig_model.decode(zs_in[level:], start_level=level, bs_chunks=bs) for level in range(0, hps.levels)]\n\n    if not hps.labels:\n        y = None\n    elif hps.level == (hps.levels - 1):\n        # Topmost level labels in order\n        y = y[:hps.bs_sample]  # t.ones((hps.bs_sample, 1), device=y.device, dtype=t.long) * dist.get_rank()\n    else:\n        # Other levels keep labels to match x_cond\n        y = y[:hps.bs_sample]\n\n    # Temp 1.0\n    _, *z_conds = orig_model.encode(x_in, bs_chunks=bs)\n    z = orig_model.sample(hps.bs_sample, z_conds=z_conds, y=y, fp16=False, temp=1.0)\n    x_sample = orig_model.decode([z, *z_conds], bs_chunks=bs)\n\n    log_aud(logger, \'sample_x_T1\', x_sample, hps)\n    if hps.prior and hps.labels:\n        log_labels(logger, orig_model.labeller, f\'sample_x_T1\', allgather(y.cuda()), hps)\n\n    # Recons\n    for i in range(len(x_ds)):\n        log_aud(logger, f\'x_ds_start_{i}\', x_ds[i], hps)\n    orig_model.train()\n    if ema is not None: ema.swap()\n    logger.flush()\n\ndef evaluate(model, orig_model, logger, metrics, data_processor, hps):\n    model.eval()\n    orig_model.eval()\n    if hps.prior:\n        _print_keys = dict(l=""loss"", bpd=""bpd"")\n    else:\n        _print_keys = dict(l=""loss"", rl=""recons_loss"", sl=""spectral_loss"")\n\n    with t.no_grad():\n        for i, x in logger.get_range(data_processor.test_loader):\n            if isinstance(x, (tuple, list)):\n                x, y = x\n            else:\n                y = None\n\n            x = x.to(\'cuda\', non_blocking=True)\n            if y is not None:\n                y = y.to(\'cuda\', non_blocking=True)\n\n            x_in = x = audio_preprocess(x, hps)\n            log_input_output = (i==0)\n\n            if hps.prior:\n                forw_kwargs = dict(y=y, fp16=hps.fp16, decode=log_input_output)\n            else:\n                forw_kwargs = dict(loss_fn=hps.loss_fn, hps=hps)\n\n            x_out, loss, _metrics = model(x, **forw_kwargs)\n\n            # Logging\n            for key, val in _metrics.items():\n                _metrics[key] = val.item()\n            _metrics[""loss""] = loss = loss.item() # Make sure to call to free graph\n\n            # Average and log\n            for key, val in _metrics.items():\n                _metrics[key] = metrics.update(f""test_{key}"", val, x.shape[0])\n\n            with t.no_grad():\n                if log_input_output:\n                    log_inputs(orig_model, logger, x_in, y, x_out, hps)\n\n            logger.set_postfix(**{print_key:_metrics[key] for print_key, key in _print_keys.items()})\n\n    for key, val in _metrics.items():\n        logger.add_scalar(f""test_{key}"", metrics.avg(f""test_{key}""))\n\n    logger.close_range()\n    return {key: metrics.avg(f""test_{key}"") for key in _metrics.keys()}\n\ndef train(model, orig_model, opt, shd, scalar, ema, logger, metrics, data_processor, hps):\n    model.train()\n    orig_model.train()\n    if hps.prior:\n        _print_keys = dict(l=""loss"", bpd=""bpd"", gn=""gn"", g_l=""gen_loss"", p_l=""prime_loss"")\n    else:\n        _print_keys = dict(l=""loss"", sl=""spectral_loss"", rl=""recons_loss"", e=""entropy"", u=""usage"", uc=""used_curr"", gn=""gn"", pn=""pn"", dk=""dk"")\n\n    for i, x in logger.get_range(data_processor.train_loader):\n        if isinstance(x, (tuple, list)):\n            x, y = x\n        else:\n            y = None\n\n        x = x.to(\'cuda\', non_blocking=True)\n        if y is not None:\n            y = y.to(\'cuda\', non_blocking=True)\n\n        x_in = x = audio_preprocess(x, hps)\n        log_input_output = (logger.iters % hps.save_iters == 0)\n\n        if hps.prior:\n            forw_kwargs = dict(y=y, fp16=hps.fp16, decode=log_input_output)\n        else:\n            forw_kwargs = dict(loss_fn=hps.loss_fn, hps=hps)\n\n        # Forward\n        x_out, loss, _metrics = model(x, **forw_kwargs)\n\n        # Backward\n        loss, scale, grad_norm, overflow_loss, overflow_grad = backward(loss=loss, params=list(model.parameters()),\n                                                                         scalar=scalar, fp16=hps.fp16, logger=logger)\n        # Skip step if overflow\n        grad_norm = allreduce(grad_norm, op=dist.ReduceOp.MAX)\n        if overflow_loss or overflow_grad or grad_norm > hps.ignore_grad_norm > 0:\n            zero_grad(orig_model)\n            continue\n\n        # Step opt. Divide by scale to include clipping and fp16 scaling\n        logger.step()\n        opt.step(scale=clipped_grad_scale(grad_norm, hps.clip, scale))\n        zero_grad(orig_model)\n        lr = hps.lr if shd is None else shd.get_lr()[0]\n        if shd is not None: shd.step()\n        if ema is not None: ema.step()\n        next_lr = hps.lr if shd is None else shd.get_lr()[0]\n        finished_training = (next_lr == 0.0)\n\n        # Logging\n        for key, val in _metrics.items():\n            _metrics[key] = val.item()\n        _metrics[""loss""] = loss = loss.item() * hps.iters_before_update # Make sure to call to free graph\n        _metrics[""gn""] = grad_norm\n        _metrics[""lr""] = lr\n        _metrics[""lg_loss_scale""] = np.log2(scale)\n\n        # Average and log\n        for key, val in _metrics.items():\n            _metrics[key] = metrics.update(key, val, x.shape[0])\n            if logger.iters % hps.log_steps == 0:\n                logger.add_scalar(key, _metrics[key])\n\n        # Save checkpoint\n        with t.no_grad():\n            if hps.save and (logger.iters % hps.save_iters == 1 or finished_training):\n                if ema is not None: ema.swap()\n                orig_model.eval()\n                name = \'latest\' if hps.prior else f\'step_{logger.iters}\'\n                if dist.get_rank() % 8 == 0:\n                    save_checkpoint(logger.logdir, name, orig_model, opt, dict(step=logger.iters), hps)\n                orig_model.train()\n                if ema is not None: ema.swap()\n\n        # Sample\n        with t.no_grad():\n            if (logger.iters % 12000) in list(range(1, 1 + hps.iters_before_update)) or finished_training:\n                if hps.prior:\n                    sample_prior(orig_model, ema, logger, x_in, y, hps)\n\n        # Input/Output\n        with t.no_grad():\n            if log_input_output:\n                log_inputs(orig_model, logger, x_in, y, x_out, hps)\n\n        logger.set_postfix(**{print_key:_metrics[key] for print_key, key in _print_keys.items()})\n        if finished_training:\n            dist.barrier()\n            exit()\n    logger.close_range()\n    return {key: metrics.avg(key) for key in _metrics.keys()}\n\ndef run(hps=""teeny"", port=29500, **kwargs):\n    from jukebox.utils.dist_utils import setup_dist_from_mpi\n    rank, local_rank, device = setup_dist_from_mpi(port=port)\n    hps = setup_hparams(hps, kwargs)\n    hps.ngpus = dist.get_world_size()\n    hps.argv = "" "".join(sys.argv)\n    hps.bs_sample = hps.nworkers = hps.bs\n\n    # Setup dataset\n    data_processor = DataProcessor(hps)\n\n    # Setup models\n    vqvae = make_vqvae(hps, device)\n    print_once(f""Parameters VQVAE:{count_parameters(vqvae)}"")\n    if hps.prior:\n        prior = make_prior(hps, vqvae, device)\n        print_once(f""Parameters Prior:{count_parameters(prior)}"")\n        model = prior\n    else:\n        model = vqvae\n\n    # Setup opt, ema and distributed_model.\n    opt, shd, scalar = get_optimizer(model, hps)\n    ema = get_ema(model, hps)\n    distributed_model = get_ddp(model, hps)\n\n    logger, metrics = init_logging(hps, local_rank, rank)\n    logger.iters = model.step\n\n    # Run training, eval, sample\n    for epoch in range(hps.curr_epoch, hps.epochs):\n        metrics.reset()\n        data_processor.set_epoch(epoch)\n        if hps.train:\n            train_metrics = train(distributed_model, model, opt, shd, scalar, ema, logger, metrics, data_processor, hps)\n            train_metrics[\'epoch\'] = epoch\n            if rank == 0:\n                print(\'Train\',\' \'.join([f\'{key}: {val:0.4f}\' for key,val in train_metrics.items()]))\n            dist.barrier()\n\n        if hps.test:\n            if ema: ema.swap()\n            test_metrics = evaluate(distributed_model, model, logger, metrics, data_processor, hps)\n            test_metrics[\'epoch\'] = epoch\n            if rank == 0:\n                print(\'Ema\',\' \'.join([f\'{key}: {val:0.4f}\' for key,val in test_metrics.items()]))\n            dist.barrier()\n            if ema: ema.swap()\n        dist.barrier()\n\nif __name__ == \'__main__\':\n    fire.Fire(run)\n'"
tensorboardX/setup.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport subprocess\nimport os\nfrom setuptools import setup, find_packages\nfrom setuptools.command.develop import develop\nfrom setuptools.command.install import install\n\n# Dynamically compile protos\ndef compileProtoBuf():\n    res = subprocess.call([\'bash\', \'./compile.sh\'])\n    assert res == 0, \'cannot compile protobuf\'\n\nclass PostDevelopCommand(develop):\n    """"""Post-installation for development mode.""""""\n    def run(self):\n        compileProtoBuf()\n        develop.run(self)\n\n\nclass PostInstallCommand(install):\n    """"""Post-installation for installation mode.""""""\n    def run(self):\n        compileProtoBuf()\n        import os\n        os.system(""pip install protobuf numpy six"")\n        install.run(self)\n\nwith open(\'HISTORY.rst\') as history_file:\n    history = history_file.read()\n\npreparing_PyPI_package = False\nversion_git = version = \'1.8\'\n\nif not preparing_PyPI_package:\n    if os.path.exists(\'.git\'):\n        sha = subprocess.check_output([\'git\', \'rev-parse\', \'HEAD\']).decode(\'ascii\').strip()\n        version_git = version_git + \'+\' + sha[:7]\n\n    with open(\'tensorboardX/__init__.py\', \'a\') as f:\n        f.write(\'\\n__version__ = ""{}""\\n\'.format(version_git))\n\nrequirements = [\n    \'numpy\',\n    \'protobuf >= 3.6.1\',\n    \'six\',\n]\n\ntest_requirements = [\n    \'pytest\',\n    \'matplotlib\',\n    \'crc32c\',\n]\n\nsetup(\n    name=\'tensorboardX\',\n    version=version_git,\n    description=\'TensorBoardX lets you watch Tensors Flow without Tensorflow\',\n    long_description=history,\n    author=\'Tzu-Wei Huang\',\n    author_email=\'huang.dexter@gmail.com\',\n    url=\'https://github.com/lanpa/tensorboardX\',\n    packages=[\'tensorboardX\'],\n    include_package_data=True,\n    install_requires=requirements,\n    license=\'MIT license\',\n    zip_safe=False,\n    classifiers=[\n        \'Development Status :: 2 - Pre-Alpha\',\n        \'Intended Audience :: Developers\',\n        \'License :: OSI Approved :: MIT License\',\n        \'Natural Language :: English\',\n        \'Programming Language :: Python :: 2\',\n        \'Programming Language :: Python :: 2.7\',\n        \'Programming Language :: Python :: 3\',\n        \'Programming Language :: Python :: 3.4\',\n        \'Programming Language :: Python :: 3.5\',\n        \'Programming Language :: Python :: 3.6\',\n    ],\n    cmdclass={\n        \'develop\': PostDevelopCommand,\n        \'install\': PostInstallCommand,\n    },\n    test_suite=\'tests\',\n    tests_require=test_requirements\n)\n\n\n# checklist: update History.rst readme.md\n# change preparing_PyPI_package to True\n# remove __version__ = ""1.old"" in __init__.py\n# commit\n# add tag\n# python setup.py sdist bdist_wheel --universal\n# twine upload dist/*\n# push commit'"
apex/apex/__init__.py,0,"b""from . import parallel\nfrom . import amp\nfrom . import fp16_utils\n\n# For optimizers and normalization there is no Python fallback.\n# Absence of cuda backend is a hard error.\n# I would like the errors from importing fused_adam_cuda or fused_layer_norm_cuda\n# to be triggered lazily, because if someone has installed with --cpp_ext and --cuda_ext\n# so they expect those backends to be available, but for some reason they actually aren't\n# available (for example because they built improperly in a way that isn't revealed until\n# load time) the error message is timely and visible.\nfrom . import optimizers\nfrom . import normalization\n"""
jukebox/data/__init__.py,0,b''
jukebox/data/artist_genre_processor.py,0,"b'import os\nimport re\n\naccepted = frozenset([chr(i) for i in range(ord(\'a\'), ord(\'z\') + 1)] +\n                     [chr(i) for i in range(ord(\'A\'), ord(\'Z\') + 1)] +\n                     [chr(i) for i in range(ord(\'0\'), ord(\'9\') + 1)])\n\nrex = re.compile(r\'_+\')\n\ndef norm(s):\n    s = \'\'.join([c if c in accepted else \'_\' for c in s.lower()])\n    s = rex.sub(\'_\', s).strip(\'_\')\n    return s\n\ndef create_reverse_lookup(atoi):\n    # Multiple entries could go to the same artist_id/genre_id\n    itoa = {}\n    for a, i in atoi.items():\n        if i not in itoa:\n            itoa[i] = []\n        itoa[i].append(a)\n    indices = sorted(list(itoa.keys()))\n    for i in indices:\n        itoa[i] = \'_\'.join(sorted(itoa[i]))\n    return itoa\n\nclass ArtistGenreProcessor():\n    def __init__(self, v3=False):\n        self.v3 = v3\n        dirname = os.path.dirname(__file__)\n        if self.v3:\n            self.artist_id_file = f""{dirname}/ids/v3_artist_ids.txt""\n            self.genre_id_file = f""{dirname}/ids/v3_genre_ids.txt""\n        else:\n            self.artist_id_file = f""{dirname}/ids/v2_artist_ids.txt""\n            self.genre_id_file = f""{dirname}/ids/v2_genre_ids.txt""\n        self.load_artists()\n        self.load_genres()\n\n    def get_artist_id(self, artist):\n        input_artist = artist\n        if self.v3:\n            artist = artist.lower()\n        else:\n            artist = norm(artist)\n        if artist not in self.artist_ids:\n            print(f""Input artist {input_artist} maps to {artist}, which is not present in {self.artist_id_file}. ""\n                  f""Defaulting to (artist_id, artist) = (0, unknown), if that seems wrong please format artist correctly"")\n        return self.artist_ids.get(artist, 0)\n\n    def get_genre_ids(self, genre):\n        if self.v3:\n            genres = [genre.lower()]\n        else:\n            # In v2, we convert genre into a bag of words\n            genres = norm(genre).split(""_"")\n        for word in genres:\n            if word not in self.genre_ids:\n                print(f""Input genre {genre} maps to the list {genres}. {word} is not present in {self.genre_id_file}. ""\n                      f""Defaulting to (word_id, word) = (0, unknown), if that seems wrong please format genre correctly"")\n        return [self.genre_ids.get(word, 0) for word in genres]\n\n    # get_artist/genre throw error if we ask for non-present values\n    def get_artist(self, artist_id):\n        return self.artists[artist_id]\n\n    def get_genre(self, genre_ids):\n        if self.v3:\n            assert len(genre_ids) == 1\n            genre = self.genres[genre_ids[0]]\n        else:\n            genre = \'_\'.join([self.genres[genre_id] for genre_id in genre_ids if genre_id >= 0])\n        return genre\n\n    def load_artists(self):\n        print(f\'Loading artist IDs from {self.artist_id_file}\')\n        self.artist_ids = {}\n        with open(self.artist_id_file, \'r\', encoding=""utf-8"") as f:\n            for line in f:\n                artist, artist_id = line.strip().split(\';\')\n                self.artist_ids[artist.lower()] = int(artist_id)\n        self.artists = create_reverse_lookup(self.artist_ids)\n\n    def load_genres(self):\n        print(f\'Loading artist IDs from {self.genre_id_file}\')\n        self.genre_ids = {}\n        with open(self.genre_id_file, \'r\', encoding=""utf-8"") as f:\n            for line in f:\n                genre, genre_id = line.strip().split(\';\')\n                self.genre_ids[genre.lower()] = int(genre_id)\n        self.genres = create_reverse_lookup(self.genre_ids)\n\n\n'"
jukebox/data/data_processor.py,2,"b'import torch as t\nimport jukebox.utils.dist_adapter as dist\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.utils.data import DataLoader, Dataset, BatchSampler, RandomSampler\nfrom jukebox.utils.dist_utils import print_all\nfrom jukebox.utils.audio_utils import calculate_bandwidth\nfrom jukebox.data.files_dataset import FilesAudioDataset\n\nclass OffsetDataset(Dataset):\n    def __init__(self, dataset, start, end, test=False):\n        super().__init__()\n        self.dataset = dataset\n        self.start = start\n        self.end = end\n        self.test = test\n        assert 0 <= self.start < self.end <= len(self.dataset)\n\n    def __len__(self):\n        return self.end - self.start\n\n    def __getitem__(self, item):\n        return self.dataset.get_item(self.start + item, test=self.test)\n\nclass DataProcessor():\n    def __init__(self, hps):\n        self.dataset = FilesAudioDataset(hps)\n        duration = 1 if hps.prior else 600\n        hps.bandwidth = calculate_bandwidth(self.dataset, hps, duration=duration)\n        self.create_datasets(hps)\n        self.create_samplers(hps)\n        self.create_data_loaders(hps)\n        self.print_stats(hps)\n\n    def set_epoch(self, epoch):\n        self.train_sampler.set_epoch(epoch)\n        self.test_sampler.set_epoch(epoch)\n\n    def create_datasets(self, hps):\n        train_len = int(len(self.dataset) * hps.train_test_split)\n        self.train_dataset = OffsetDataset(self.dataset, 0, train_len, test=False)\n        self.test_dataset = OffsetDataset(self.dataset, train_len, len(self.dataset), test=True)\n\n    def create_samplers(self, hps):\n        if not dist.is_available():\n            self.train_sampler = BatchSampler(RandomSampler(self.train_dataset), batch_size=hps.bs, drop_last=True)\n            self.test_sampler = BatchSampler(RandomSampler(self.test_dataset), batch_size=hps.bs, drop_last=True)\n        else:\n            self.train_sampler = DistributedSampler(self.train_dataset)\n            self.test_sampler = DistributedSampler(self.test_dataset)\n\n    def create_data_loaders(self, hps):\n        # Loader to load mini-batches\n        if hps.labels:\n            collate_fn = lambda batch: tuple(t.stack([t.from_numpy(b[i]) for b in batch], 0) for i in range(2))\n        else:\n            collate_fn = lambda batch: t.stack([t.from_numpy(b) for b in batch], 0)\n\n        print(\'Creating Data Loader\')\n        self.train_loader = DataLoader(self.train_dataset, batch_size=hps.bs, num_workers=hps.nworkers,\n                                       sampler=self.train_sampler, pin_memory=False,\n                                       drop_last=True, collate_fn=collate_fn)\n        self.test_loader = DataLoader(self.test_dataset, batch_size=hps.bs, num_workers=hps.nworkers,\n                                      sampler=self.test_sampler, pin_memory=False,\n                                      drop_last=False, collate_fn=collate_fn)\n\n    def print_stats(self, hps):\n        print_all(f""Train {len(self.train_dataset)} samples. Test {len(self.test_dataset)} samples"")\n        print_all(f\'Train sampler: {self.train_sampler}\')\n        print_all(f\'Train loader: {len(self.train_loader)}\')\n'"
jukebox/data/files_dataset.py,1,"b'import librosa\nimport math\nimport numpy as np\nimport jukebox.utils.dist_adapter as dist\nfrom torch.utils.data import Dataset\nfrom jukebox.utils.dist_utils import print_all\nfrom jukebox.utils.io import get_duration_sec, load_audio\nfrom jukebox.data.labels import Labeller\n\nclass FilesAudioDataset(Dataset):\n    def __init__(self, hps):\n        super().__init__()\n        self.sr = hps.sr\n        self.channels = hps.channels\n        self.min_duration = hps.min_duration or math.ceil(hps.sample_length / hps.sr)\n        self.max_duration = hps.max_duration or math.inf\n        self.sample_length = hps.sample_length\n        assert hps.sample_length / hps.sr < self.min_duration, f\'Sample length {hps.sample_length} per sr {hps.sr} ({hps.sample_length / hps.sr:.2f}) should be shorter than min duration {self.min_duration}\'\n        self.aug_shift = hps.aug_shift\n        self.labels = hps.labels\n        self.init_dataset(hps)\n\n    def filter(self, files, durations):\n        # Remove files too short or too long\n        keep = []\n        for i in range(len(files)):\n            if durations[i] / self.sr < self.min_duration:\n                continue\n            if durations[i] / self.sr >= self.max_duration:\n                continue\n            keep.append(i)\n        print_all(f\'self.sr={self.sr}, min: {self.min_duration}, max: {self.max_duration}\')\n        print_all(f""Keeping {len(keep)} of {len(files)} files"")\n        self.files = [files[i] for i in keep]\n        self.durations = [int(durations[i]) for i in keep]\n        self.cumsum = np.cumsum(self.durations)\n\n    def init_dataset(self, hps):\n        # Load list of files and starts/durations\n        files = librosa.util.find_files(f\'{hps.audio_files_dir}\', [\'mp3\', \'opus\', \'m4a\', \'aac\', \'wav\'])\n        print_all(f""Found {len(files)} files. Getting durations"")\n        cache = dist.get_rank() % 8 == 0 if dist.is_available() else True\n        durations = np.array([get_duration_sec(file, cache=cache) * self.sr for file in files])  # Could be approximate\n        self.filter(files, durations)\n\n        if self.labels:\n            self.labeller = Labeller(hps.max_bow_genre_size, hps.n_tokens, self.sample_length, v3=hps.labels_v3)\n\n    def get_index_offset(self, item):\n        # For a given dataset item and shift, return song index and offset within song\n        half_interval = self.sample_length//2\n        shift = np.random.randint(-half_interval, half_interval) if self.aug_shift else 0\n        offset = item * self.sample_length + shift # Note we centred shifts, so adding now\n        midpoint = offset + half_interval\n        assert 0 <= midpoint < self.cumsum[-1], f\'Midpoint {midpoint} of item beyond total length {self.cumsum[-1]}\'\n        index = np.searchsorted(self.cumsum, midpoint)  # index <-> midpoint of interval lies in this song\n        start, end = self.cumsum[index - 1] if index > 0 else 0.0, self.cumsum[index] # start and end of current song\n        assert start <= midpoint <= end, f""Midpoint {midpoint} not inside interval [{start}, {end}] for index {index}""\n        if offset > end - self.sample_length: # Going over song\n            offset = max(start, offset - half_interval)  # Now should fit\n        elif offset < start: # Going under song\n            offset = min(end - self.sample_length, offset + half_interval)  # Now should fit\n        assert start <= offset <= end - self.sample_length, f""Offset {offset} not in [{start}, {end - self.sample_length}]. End: {end}, SL: {self.sample_length}, Index: {index}""\n        offset = offset - start\n        return index, offset\n\n    def get_metadata(self, filename, test):\n        """"""\n        Insert metadata loading code for your dataset here.\n        If artist/genre labels are different from provided artist/genre lists,\n        update labeller accordingly.\n\n        Returns:\n            (artist, genre, full_lyrics) of type (str, str, str). For\n            example, (""unknown"", ""classical"", """") could be a metadata for a\n            piano piece.\n        """"""\n        return None, None, None\n\n    def get_song_chunk(self, index, offset, test=False):\n        filename, total_length = self.files[index], self.durations[index]\n        data, sr = load_audio(filename, sr=self.sr, offset=offset, duration=self.sample_length)\n        assert data.shape == (self.channels, self.sample_length), f\'Expected {(self.channels, self.sample_length)}, got {data.shape}\'\n        if self.labels:\n            artist, genre, lyrics = self.get_metadata(filename, test)\n            labels = self.labeller.get_label(artist, genre, lyrics, total_length, offset)\n            return data.T, labels[\'y\']\n        else:\n            return data.T\n\n    def get_item(self, item, test=False):\n        index, offset = self.get_index_offset(item)\n        return self.get_song_chunk(index, offset, test)\n\n    def __len__(self):\n        return int(np.floor(self.cumsum[-1] / self.sample_length))\n\n    def __getitem__(self, item):\n        return self.get_item(item)\n'"
jukebox/data/labels.py,0,"b'import torch as t\nimport numpy as np\nfrom jukebox.data.artist_genre_processor import ArtistGenreProcessor\nfrom jukebox.data.text_processor import TextProcessor\n\n# Linear window heurisic to get a window of lyric_tokens\ndef get_relevant_lyric_tokens(full_tokens, n_tokens, total_length, offset, duration):\n    if len(full_tokens) < n_tokens:\n        tokens = [0] * (n_tokens - len(full_tokens)) + full_tokens\n        indices = [-1] * (n_tokens - len(full_tokens)) + list(range(0, len(full_tokens)))\n    else:\n        assert 0 <= offset < total_length\n        midpoint = int(len(full_tokens) * (offset + duration / 2.0) / total_length)\n        midpoint = min(max(midpoint, n_tokens // 2), len(full_tokens) - n_tokens // 2)\n        tokens = full_tokens[midpoint - n_tokens // 2:midpoint + n_tokens // 2]\n        indices = list(range(midpoint - n_tokens // 2, midpoint + n_tokens // 2))\n    assert len(tokens) == n_tokens, f""Expected length {n_tokens}, got {len(tokens)}""\n    assert len(indices) == n_tokens, f""Expected length {n_tokens}, got {len(indices)}""\n    assert tokens == [full_tokens[index] if index != -1 else 0 for index in indices]\n    return tokens, indices\n\nclass Labeller():\n    def __init__(self, max_genre_words, n_tokens, sample_length, v3=False):\n        self.ag_processor = ArtistGenreProcessor(v3)\n        self.text_processor = TextProcessor(v3)\n        self.n_tokens = n_tokens\n        self.max_genre_words = max_genre_words\n        self.sample_length = sample_length\n        self.label_shape = (4 + self.max_genre_words + self.n_tokens, )\n\n    def get_label(self, artist, genre, lyrics, total_length, offset):\n        artist_id = self.ag_processor.get_artist_id(artist)\n        genre_ids = self.ag_processor.get_genre_ids(genre)\n\n        lyrics = self.text_processor.clean(lyrics)\n        full_tokens = self.text_processor.tokenise(lyrics)\n        tokens, _ = get_relevant_lyric_tokens(full_tokens, self.n_tokens, total_length, offset, self.sample_length)\n\n        assert len(genre_ids) <= self.max_genre_words\n        genre_ids = genre_ids + [-1] * (self.max_genre_words - len(genre_ids))\n        y = np.array([total_length, offset, self.sample_length, artist_id, *genre_ids, *tokens], dtype=np.int64)\n        assert y.shape == self.label_shape, f""Expected {self.label_shape}, got {y.shape}""\n        info = dict(artist=artist, genre=genre, lyrics=lyrics, full_tokens=full_tokens)\n        return dict(y=y, info=info)\n\n    def get_y_from_ids(self, artist_id, genre_ids, lyric_tokens, total_length, offset):\n        assert len(genre_ids) <= self.max_genre_words\n        genre_ids = genre_ids + [-1] * (self.max_genre_words - len(genre_ids))\n        if self.n_tokens > 0:\n            assert len(lyric_tokens) == self.n_tokens\n        else:\n            lyric_tokens = []\n        y = np.array([total_length, offset, self.sample_length, artist_id, *genre_ids, *lyric_tokens], dtype=np.int64)\n        assert y.shape == self.label_shape, f""Expected {self.label_shape}, got {y.shape}""\n        return y\n\n    def get_batch_labels(self, metas, device=\'cpu\'):\n        ys, infos = [], []\n        for meta in metas:\n            label = self.get_label(**meta)\n            y, info = label[\'y\'], label[\'info\']\n            ys.append(y)\n            infos.append(info)\n\n        ys = t.stack([t.from_numpy(y) for y in ys], dim=0).to(device).long()\n        assert ys.shape[0] == len(metas)\n        assert len(infos) == len(metas)\n        return dict(y=ys, info=infos)\n\n    def set_y_lyric_tokens(self, ys, labels):\n        info = labels[\'info\']\n        assert ys.shape[0] == len(info)\n        if self.n_tokens > 0:\n            # total_length, offset, duration):\n            tokens_list = []\n            indices_list = []  # whats the index of each current character in original array\n            for i in range(ys.shape[0]):\n                full_tokens = info[i][\'full_tokens\']\n                total_length, offset, duration = ys[i, 0], ys[i, 1], ys[i, 2]\n                tokens, indices = get_relevant_lyric_tokens(full_tokens, self.n_tokens, total_length, offset, duration)\n                tokens_list.append(tokens)\n                indices_list.append(indices)\n            ys[:, -self.n_tokens:] = t.tensor(tokens_list, dtype=t.long, device=\'cuda\')\n            return indices_list\n        else:\n            return None\n\n    def describe_label(self, y):\n        assert y.shape == self.label_shape, f""Expected {self.label_shape}, got {y.shape}""\n        y = np.array(y).tolist()\n        total_length, offset, length, artist_id, *genre_ids = y[:4 + self.max_genre_words]\n        tokens = y[4 + self.max_genre_words:]\n        artist = self.ag_processor.get_artist(artist_id)\n        genre = self.ag_processor.get_genre(genre_ids)\n        lyrics = self.text_processor.textise(tokens)\n        return dict(artist=artist, genre=genre, lyrics=lyrics)\n\n\nif __name__ == \'__main__\':\n    labeller = Labeller(5, 512, 8192*8*4*4, v3=False)\n    label = labeller.get_label(""Alan Jackson"", ""Country Rock"", ""old town road"", 4*60*44100, 0)\n    print(label, labeller.describe_label(label[\'y\']))\n\n    labeller = Labeller(1, 384, 6144*8*4*4, v3=True)\n    label = labeller.get_label(""Alan Jackson"", ""Country Rock"", ""old town road"", 4*60*44100, 0)\n    print(label, labeller.describe_label(label[\'y\']))\n\n\n\n\n\n'"
jukebox/data/text_processor.py,0,"b'import re\nfrom unidecode import unidecode\n\nclass TextProcessor():\n    def __init__(self, v3=False):\n        if v3:\n            vocab = \'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789.,:;!?-\\\'\\""()[] \\t\\n\'\n            not_vocab = re.compile(\'[^A-Za-z0-9.,:;!?\\-\\\'\\""()\\[\\] \\t\\n]+\')\n        else:\n            vocab = \'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789.,:;!?-+\\\'\\""()[] \\t\\n\'\n            not_vocab = re.compile(\'[^A-Za-z0-9.,:;!?\\-+\\\'\\""()\\[\\] \\t\\n]+\')\n        self.vocab = {vocab[index]: index + 1 for index in range(len(vocab))}\n        self.vocab[\'<unk>\'] = 0\n        self.n_vocab = len(vocab) + 1\n        self.tokens = {v: k for k, v in self.vocab.items()}\n        self.tokens[0] = \'\'  # <unk> became \'\'\n        self.not_vocab = not_vocab\n\n    def clean(self, text):\n        text = unidecode(text)  # Convert to ascii\n        text = text.replace(\'\\\\\', \'\\n\')\n        text = self.not_vocab.sub(\'\', text)  # Remove non vocab\n        return text\n\n    def tokenise(self, text):\n        return [self.vocab[char] for char in text]\n\n    def textise(self, tokens):\n        return \'\'.join([self.tokens[token] for token in tokens])\n\n    def characterise(self, tokens):\n        return [self.tokens[token] for token in tokens]\n'"
jukebox/prior/__init__.py,0,b''
jukebox/prior/autoregressive.py,2,"b'import numpy as np\nimport torch as t\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom jukebox.transformer.ops import filter_logits\nfrom jukebox.transformer.transformer import Transformer\nfrom jukebox.utils.logger import get_range\nfrom jukebox.utils.torch_utils import empty_cache\n\ndef get_normal(*shape, std=0.01):\n    w = t.empty(shape)\n    nn.init.normal_(w, std=std)\n    return w\n\ndef roll(x, n):\n    return t.cat((x[:, -n:], x[:, :-n]), dim=1)\n\ndef split_chunks(length, chunk_size):\n    n_passes = (length + chunk_size - 1) // chunk_size\n    chunk_sizes = [*[chunk_size] * (n_passes - 1), (length - 1) % chunk_size + 1]\n    assert sum(chunk_sizes) == length\n    return chunk_sizes\n\nclass PositionEmbedding(nn.Module):\n    def __init__(self, input_shape, width, init_scale=1.0, pos_init=False):\n        super().__init__()\n        self.input_shape = input_shape\n        self.input_dims = input_dims = np.prod(input_shape)\n        self.pos_init = pos_init\n        if pos_init:\n            self.register_buffer(\'pos\', t.tensor(get_pos_idx(input_shape)).long())\n            self._pos_embs = nn.ModuleList()\n            for i in range(len(input_shape)):\n                emb = nn.Embedding(input_shape[i], width)\n                nn.init.normal_(emb.weight, std=0.02)\n                self._pos_embs.append(emb)\n        else:\n            self.pos_emb = nn.Parameter(get_normal(input_dims, width, std=0.01 * init_scale))\n\n    def forward(self):\n        if self.pos_init:\n            pos_emb = sum([self._pos_embs[i](self.pos[:,i]) for i in range(len(self.input_shape))])\n        else:\n            pos_emb = self.pos_emb\n        return pos_emb\n\nclass ConditionalAutoregressive2D(nn.Module):\n    def __init__(self, input_shape, bins,\n                 width=128, depth=2, heads=1,\n                 attn_dropout=0.0, resid_dropout=0.0, emb_dropout=0.0, mask=True,\n                 zero_out=False, init_scale=1.0, res_scale=False, pos_init=False,\n                 m_attn=0.25, m_mlp=1,\n                 checkpoint_res=0, checkpoint_attn=0, checkpoint_mlp=0,\n                 attn_order=0, blocks=None, spread=None, x_cond=False, y_cond=False,\n                 encoder_dims=0, only_encode=False, merged_decoder=False, prime_len=None):\n        super().__init__()\n        self.input_shape = input_shape\n        self.input_dims = input_dims = np.prod(input_shape)\n        self.encoder_dims = encoder_dims\n        self.bins = bins\n        self.width = width\n        self.depth = depth\n\n        self.x_emb = nn.Embedding(bins, width)\n        nn.init.normal_(self.x_emb.weight, std=0.02 * init_scale)\n        self.x_emb_dropout = nn.Dropout(emb_dropout)\n        self.y_cond = y_cond\n        self.x_cond = x_cond\n        if not y_cond:\n            self.start_token = nn.Parameter(get_normal(1, width, std=0.01 * init_scale))\n\n        self.pos_emb = PositionEmbedding(input_shape=input_shape, width=width, init_scale=init_scale, pos_init=pos_init)\n        self.pos_emb_dropout = nn.Dropout(emb_dropout)\n\n        self.transformer = Transformer(n_in=width, n_ctx=input_dims, n_head=heads, n_depth=depth,\n                                       attn_dropout=attn_dropout, resid_dropout=resid_dropout,\n                                       afn=\'quick_gelu\', scale=True, mask=mask,\n                                       zero_out=zero_out, init_scale=init_scale, res_scale=res_scale,\n                                       m_attn=m_attn, m_mlp=m_mlp,\n                                       checkpoint_attn=checkpoint_attn, checkpoint_mlp=checkpoint_mlp, checkpoint_res=checkpoint_res,\n                                       attn_order=attn_order, blocks=blocks, spread=spread,\n                                       encoder_dims=encoder_dims, prime_len=prime_len)\n\n        self.only_encode = only_encode\n        self.prime_len = prime_len\n        if merged_decoder:\n            # Merged piped model uses this setup\n            self.add_cond_after_transformer = False\n            self.share_x_emb_x_out = False\n        else:\n            self.add_cond_after_transformer = True\n            self.share_x_emb_x_out = True\n\n        if not only_encode:\n            self.x_out = nn.Linear(width, bins, bias=False)\n            if self.share_x_emb_x_out:\n                self.x_out.weight = self.x_emb.weight\n            self.loss = t.nn.CrossEntropyLoss()\n\n    def preprocess(self, x):\n        # Input: x is NHWC and uint8. Converted to NL and long\n        # Can include stuff like bitpacking, reordering here.\n        N = x.shape[0]\n        return x.view(N, -1).long()\n\n    def postprocess(self, x, sample_tokens=None):\n        # Convert back from NL and long to NHWC\n        N = x.shape[0]\n        assert (0 <= x).all() and (x < self.bins).all()\n        if sample_tokens is None or sample_tokens==self.input_dims:\n            return x.view(N, *self.input_shape)\n        else:\n            return x.view(N, -1)\n\n    def forward(self, x, x_cond=None, y_cond=None, encoder_kv=None, fp16=False, loss_full=False,\n                encode=False, get_preds=False, get_acts=False, get_sep_loss=False):\n        # Preprocess.\n        with t.no_grad():\n            x = self.preprocess(x)\n\n        N, D = x.shape\n        assert isinstance(x, t.cuda.LongTensor)\n        assert (0 <= x).all() and (x < self.bins).all()\n\n        if self.y_cond:\n            assert y_cond is not None\n            assert y_cond.shape == (N, 1, self.width)\n        else:\n            assert y_cond is None\n\n        if self.x_cond:\n            assert x_cond is not None\n            assert x_cond.shape == (N, D, self.width) or x_cond.shape == (N, 1, self.width), f""{x_cond.shape} != {(N, D, self.width)} nor {(N, 1, self.width)}. Did you pass the correct --sample_length?""\n        else:\n            assert x_cond is None\n            x_cond = t.zeros((N, 1, self.width), device=x.device, dtype=t.float)\n\n        x_t = x # Target\n        x = self.x_emb(x) # X emb\n        x = roll(x, 1) # Shift by 1, and fill in start token\n        if self.y_cond:\n            x[:,0] = y_cond.view(N, self.width)\n        else:\n            x[:,0] = self.start_token\n\n        x = self.x_emb_dropout(x) + self.pos_emb_dropout(self.pos_emb()) + x_cond # Pos emb and dropout\n\n        x = self.transformer(x, encoder_kv=encoder_kv, fp16=fp16) # Transformer\n        if self.add_cond_after_transformer: # Piped doesnt add x_cond\n            x = x + x_cond\n\n        acts = x\n        if self.only_encode:\n            return x\n        x = self.x_out(x) # Predictions\n\n        if get_sep_loss:\n            assert self.prime_len is not None\n            x_prime = x[:, :self.prime_len].reshape(-1, self.bins)\n            x_gen = x[:, self.prime_len:].reshape(-1, self.bins)\n\n            prime_loss = F.cross_entropy(x_prime, x_t[:, :self.prime_len].reshape(-1)) / np.log(2.)\n            gen_loss = F.cross_entropy(x_gen, x_t[:, self.prime_len:].reshape(-1)) / np.log(2.)\n\n            loss = (prime_loss, gen_loss) # Note order! Prime is first\n        else:\n            loss = F.cross_entropy(x.view(-1, self.bins), x_t.view(-1)) / np.log(2.)  # Loss\n\n        if get_preds:\n            return loss, x\n        elif get_acts:\n            return loss, acts\n        else:\n            return loss, None\n\n    def get_emb(self, sample_t, n_samples, x, x_cond, y_cond):\n        N, D = n_samples, self.input_dims\n        if sample_t == 0:\n            # Fill in start token\n            x = t.empty(n_samples, 1, self.width).cuda()\n            if self.y_cond:\n                x[:, 0] = y_cond.view(N, self.width)\n            else:\n                x[:, 0] = self.start_token\n        else:\n            assert isinstance(x, t.cuda.LongTensor)\n            assert (0 <= x).all() and (x < self.bins).all()\n            x = self.x_emb(x)\n        assert x.shape == (n_samples, 1, self.width)\n        if x_cond.shape == (N, D, self.width):\n            cond = x_cond[:, sample_t:sample_t + 1, :]\n        else:\n            cond = x_cond\n        x = x + self.pos_emb()[sample_t:sample_t + 1] + cond  # Pos emb, dropout is identity at eval time\n        assert x.shape == (n_samples, 1, self.width)\n        return x, cond\n\n    def sample(self, n_samples, x_cond=None, y_cond=None, encoder_kv=None, fp16=False, temp=1.0, top_k=0, top_p=0.0,\n               get_preds=False, sample_tokens=None):\n        assert self.training == False\n\n        if sample_tokens is None: sample_tokens=self.input_dims\n        N, D = n_samples, self.input_dims\n        if self.y_cond:\n            assert y_cond is not None\n            assert y_cond.shape == (N, 1, self.width)\n        else:\n            assert y_cond is None\n\n        if self.x_cond:\n            assert x_cond is not None\n            assert x_cond.shape == (N, D, self.width) or x_cond.shape == (N, 1, self.width), f""Got {x_cond.shape}, expected ({N}, {D}/{1}, {self.width})""\n        else:\n            assert x_cond is None\n            x_cond = t.zeros((N, 1, self.width), dtype=t.float).cuda()\n\n        with t.no_grad():\n            xs, x = [], None\n            if get_preds:\n                preds = []\n            for sample_t in get_range(range(0, sample_tokens)):\n                x, cond = self.get_emb(sample_t, n_samples, x, x_cond, y_cond)\n                self.transformer.check_cache(n_samples, sample_t, fp16)\n                x = self.transformer(x, encoder_kv=encoder_kv, sample=True, fp16=fp16) # Transformer\n                if self.add_cond_after_transformer:\n                    x = x + cond\n                assert x.shape == (n_samples, 1, self.width)\n                x = self.x_out(x) # Predictions\n                if get_preds:\n                    preds.append(x.clone())\n                # Adjust logits\n                x = x / temp\n                x = filter_logits(x, top_k=top_k, top_p=top_p)\n                x = t.distributions.Categorical(logits=x).sample() # Sample and replace x\n                assert x.shape == (n_samples, 1)\n                xs.append(x.clone())\n\n            del x\n            self.transformer.del_cache()\n\n            x = t.cat(xs, dim=1)\n            if get_preds:\n                preds = t.cat(preds, dim=1)\n            x = self.postprocess(x, sample_tokens)\n        if get_preds:\n            return x, preds\n        else:\n            return x\n\n    def primed_sample(self, n_samples, x, x_cond=None, y_cond=None, encoder_kv=None, fp16=False, temp=1.0, top_k=0,\n                      top_p=0.0, get_preds=False, chunk_size=None, sample_tokens=None):\n        assert self.training == False\n\n        if sample_tokens is None: sample_tokens=self.input_dims\n        # Preprocess.\n        with t.no_grad():\n            x = self.preprocess(x)\n        assert isinstance(x, t.cuda.LongTensor)\n        assert (0 <= x).all() and (x < self.bins).all()\n        assert x.shape[0] == n_samples\n        xs = t.split(x, 1, dim=1)\n        xs = list(xs)\n        assert len(xs) < sample_tokens\n\n        N, D = n_samples, self.input_dims\n        if self.y_cond:\n            assert y_cond is not None\n            assert y_cond.shape == (N, 1, self.width)\n        else:\n            assert y_cond is None\n\n        if self.x_cond:\n            assert x_cond is not None\n            assert x_cond.shape == (N, D, self.width) or x_cond.shape == (N, 1, self.width), f""Got {x_cond.shape}, expected ({N}, {D}/{1}, {self.width})""\n        else:\n            assert x_cond is None\n            x_cond = t.zeros((N, 1, self.width), dtype=t.float).cuda()\n\n        with t.no_grad():\n            if get_preds:\n                preds = []\n\n            # Fill up key/value cache for past context by runing forward pass.\n            # We do so in chunks instead of doing the whole past in one forward pass to reduce max memory usage.\n            if chunk_size is None:\n                chunk_size = len(xs)\n            #assert len(xs) % chunk_size == 0, f\'expected {len(xs)} to be divisible by {chunk_size}\'\n            chunk_sizes = split_chunks(len(xs), chunk_size)\n            x_primes = []\n            start = 0\n            x = None\n            for current_chunk_size in get_range(chunk_sizes):\n                xs_prime, conds_prime = [], []\n                for sample_t in range(start, start + current_chunk_size):\n                    x_prime, cond_prime = self.get_emb(sample_t, n_samples, x, x_cond, y_cond)\n                    x = xs[sample_t]\n                    xs_prime.append(x_prime)\n                    conds_prime.append(cond_prime)\n                start = start + current_chunk_size\n\n                x_prime, cond_prime = t.cat(xs_prime, dim=1), t.cat(conds_prime, dim=1)\n                assert x_prime.shape == (n_samples, current_chunk_size, self.width)\n                assert cond_prime.shape == (n_samples, current_chunk_size, self.width)\n                del xs_prime\n                del conds_prime\n                if not get_preds:\n                    del cond_prime\n                x_prime = self.transformer(x_prime, encoder_kv=encoder_kv, sample=True, fp16=fp16)\n\n                if get_preds:\n                    if self.add_cond_after_transformer:\n                        x_prime = x_prime + cond_prime\n                    assert x_prime.shape == (n_samples, current_chunk_size, self.width)\n                    del cond_prime\n                    x_primes.append(x_prime)\n                else:\n                    del x_prime\n\n            if get_preds:\n                x_prime = t.cat(x_primes, dim=1)\n                assert x_prime.shape == (n_samples, len(xs), self.width)\n                x_prime = self.x_out(x_prime)  # Predictions\n                preds.append(x_prime)\n\n            empty_cache()\n            self.transformer.check_cache(n_samples, len(xs), fp16)\n\n            x = xs[-1]\n            assert x.shape == (n_samples, 1)\n            empty_cache()\n            for sample_t in get_range(range(len(xs), sample_tokens)):\n                x, cond = self.get_emb(sample_t, n_samples, x, x_cond, y_cond)\n                self.transformer.check_cache(n_samples, sample_t, fp16)\n                x = self.transformer(x, encoder_kv=encoder_kv, sample=True, fp16=fp16) # Transformer\n                if self.add_cond_after_transformer:\n                    x = x + cond\n                assert x.shape == (n_samples, 1, self.width)\n                x = self.x_out(x) # Predictions\n                if get_preds:\n                    preds.append(x)\n                # Adjust logits\n                x = x / temp\n                x = filter_logits(x, top_k=top_k, top_p=top_p)\n                x = t.distributions.Categorical(logits=x).sample() # Sample and replace x\n                assert x.shape == (n_samples, 1)\n                xs.append(x.clone())\n\n            del x\n            self.transformer.del_cache()\n\n            x = t.cat(xs, dim=1)\n            if get_preds:\n                preds = t.cat(preds, dim=1)\n            x = self.postprocess(x, sample_tokens)\n        if get_preds:\n            return x, preds\n        else:\n            return x\n\n    def check_sample(self, chunk_size):\n        bs, l, d = (4, self.input_dims, self.width)\n        prime = int(self.input_dims//8*7)\n        enc_l = self.encoder_dims\n        with t.no_grad():\n            y_cond = t.randn(bs, 1, d).cuda() if self.y_cond else None\n            x_cond = t.randn(bs, l, d).cuda() if self.x_cond else None\n            encoder_kv = t.randn(bs, enc_l, d).cuda()\n\n            x, preds_sample = self.sample(bs, x_cond, y_cond, encoder_kv, get_preds=True)\n            loss, preds_forw = self.forward(x, x_cond, y_cond, encoder_kv, get_preds=True)\n            max_err = t.max(t.abs(preds_sample - preds_forw))\n            assert max_err <= 1e-6, f""Max err is {max_err} {[i for i in range(l) if t.max(t.abs(preds_sample - preds_forw)[:, i, :]) > 1e-6]}""\n\n            x_prime = x.view(bs, -1)[:,:prime]\n            # unchunked\n            x, preds_sample = self.primed_sample(bs, x_prime.clone(), x_cond, y_cond, encoder_kv, get_preds=True)\n            assert (x.view(bs, -1)[:,:prime] == x_prime).all(), ""Priming samples don\'t match""\n            loss, preds_forw = self.forward(x, x_cond, y_cond, encoder_kv, get_preds=True)\n            max_err = t.max(t.abs(preds_sample - preds_forw))\n            assert max_err <= 1e-6, f""Max err is {max_err} {[i for i in range(l) if t.max(t.abs(preds_sample - preds_forw)[:, i, :]) > 1e-6]}""\n\n            # chunked\n            x, preds_sample = self.primed_sample(bs, x_prime.clone(), x_cond, y_cond, encoder_kv, get_preds=True, chunk_size=chunk_size)\n            assert (x.view(bs, -1)[:,:prime] == x_prime).all(), ""Priming samples don\'t match""\n            loss, preds_forw = self.forward(x, x_cond, y_cond, encoder_kv, get_preds=True)\n            max_err = t.max(t.abs(preds_sample - preds_forw))\n            assert max_err <= 1e-6, f""Max err is {max_err} {[i for i in range(l) if t.max(t.abs(preds_sample - preds_forw)[:, i, :]) > 1e-6]}""\n\n\ndef test_prior(input_shape, encoder_dims, blocks, heads, chunk_size):\n    bins = 512\n    width = 32\n    depth = 2\n    prime_len = encoder_dims\n    for x_cond in [True, False]:\n        for y_cond in [True, False]:\n            for attn_order in [0,2,6,12]:\n                prior = ConditionalAutoregressive2D(input_shape, bins,\n                                                    width=width, depth=depth, heads=heads,\n                                                    attn_order=attn_order, blocks=blocks,\n                                                    x_cond=x_cond, y_cond=y_cond,\n                                                    encoder_dims=encoder_dims, prime_len=prime_len).cuda()\n                prior.training = False\n                prior.check_sample(chunk_size)\n                print(f""Checked x_cond: {x_cond}, y_cond: {y_cond}, attn_order: {attn_order}"")\n            # prior.apply(_convert_mlp_traced)\n            # prior.check_sample()\n            # print(f""Checked traced x_cond: {x_cond}, y_cond: {y_cond}"")\n\n\nif __name__ == \'__main__\':\n    from jukebox.utils.dist_utils import setup_dist_from_mpi\n    setup_dist_from_mpi(port=29600)\n    test_cases = [\n        ((6144,), 384, 64, 2, 23),\n        ((6144,), 384, 64, 2, 8),\n        ((8192,), 512, 128, 2, 16),\n    ]\n    for test_case in test_cases:\n        test_prior(*test_case)\n'"
jukebox/prior/conditioners.py,1,"b'import torch as t\nimport torch.nn as nn\n\nfrom jukebox.transformer.ops import LayerNorm\nfrom jukebox.vqvae.encdec import DecoderConvBock\nfrom jukebox.utils.torch_utils import assert_shape\n\nclass Conditioner(nn.Module):\n    def __init__(self, input_shape, bins, down_t, stride_t, out_width, init_scale, zero_out, res_scale, **block_kwargs):\n        super().__init__()\n        self.x_shape = input_shape\n\n        # Embedding\n        self.width = out_width\n        self.x_emb = nn.Embedding(bins, out_width)\n        nn.init.normal_(self.x_emb.weight, std=0.02 * init_scale)\n\n        # Conditioner\n        self.cond = DecoderConvBock(self.width, self.width, down_t, stride_t, **block_kwargs, zero_out=zero_out, res_scale=res_scale)\n        self.ln = LayerNorm(self.width)\n\n    def preprocess(self, x):\n        x = x.permute(0,2,1) # NTC -> NCT\n        return x\n\n    def postprocess(self, x):\n        x = x.permute(0,2,1) # NCT -> NTC\n        return x\n\n    def forward(self, x, x_cond=None):\n        N = x.shape[0]\n        assert_shape(x, (N, *self.x_shape))\n        if x_cond is not None:\n            assert_shape(x_cond, (N, *self.x_shape, self.width))\n        else:\n            x_cond = 0.0\n        # Embed x\n        x = x.long()\n        x = self.x_emb(x)\n        assert_shape(x, (N, *self.x_shape, self.width))\n        x = x + x_cond\n\n        # Run conditioner\n        x = self.preprocess(x)\n        x = self.cond(x)\n        x = self.postprocess(x)\n        x = self.ln(x)\n        return x\n\ndef flip(x):\n    def _flip(x):\n        return x.permute(0,2,1).contiguous()\n    if isinstance(x, (list, tuple)):\n        return [flip(z) for z in x]\n    return _flip(x)\n\nclass SimpleEmbedding(nn.Module):\n    def __init__(self, bins, out_width, init_scale):\n        super().__init__()\n        self.bins = bins\n        self.emb = nn.Embedding(bins, out_width)\n        nn.init.normal_(self.emb.weight, std=0.01 * init_scale)\n\n    def forward(self, y):\n        assert len(y.shape) == 2, f""Expected shape with 2 dims, got {y.shape}""\n        assert isinstance(y, t.cuda.LongTensor), f""Expected dtype {t.cuda.LongTensor}, got {y.dtype}""\n        assert (0 <= y).all() and (y < self.bins).all(), f""Bins {self.bins}, got label {y}""\n        return self.emb(y)\n\nclass RangeEmbedding(nn.Module):\n    # Interpolating\n    # Interpolate so that [pos_start, pos_end] <-> position tensor of length n_ctx\n    #\n    # Binning\n    # For each pos in position tensor, find its bin\n    # [start,end) mapped to [0,1,...,bins-1]\n    # [start,end) -> [0,1) -> [0, bins) -> floor -> [0,...,bins-1]\n    # NOTE: Open ended interval on right, so start <= pos < end, not <= end\n    def __init__(self, n_time, bins, range, out_width, init_scale, clamp=False):\n        super().__init__()\n        self.n_time = n_time\n        self.bins = bins\n        self.emb = nn.Embedding(bins, out_width)\n        nn.init.normal_(self.emb.weight, std=0.01 * init_scale)\n        self.pos_min, self.pos_max = range\n        self.clamp = clamp\n\n    def forward(self, pos_start, pos_end=None):\n        # Check if [pos_start,pos_end] in [pos_min, pos_max)\n        assert len(pos_start.shape) == 2, f""Expected shape with 2 dims, got {pos_start.shape}""\n        assert (self.pos_min <= pos_start).all() and (pos_start < self.pos_max).all(), f""Range is [{self.pos_min},{self.pos_max}), got {pos_start}""\n        pos_start = pos_start.float()\n        if pos_end is not None:\n            assert len(pos_end.shape) == 2, f""Expected shape with 2 dims, got {pos_end.shape}""\n            if self.clamp:\n                pos_end = pos_end.clamp(self.pos_min, self.pos_max)\n            assert (self.pos_min <= pos_end).all() and (pos_end <= self.pos_max).all(), f""Range is [{self.pos_min},{self.pos_max}), got {pos_end}""\n            pos_end = pos_end.float()\n        # Interpolate so that [pos_start, ..., pos_end] <-> position tensor of length n_ctx\n        n_time = self.n_time\n        if n_time != 1:\n            assert pos_end is not None\n            interpolation  = (t.arange(0, n_time, dtype=t.float, device=\'cuda\').view(1,n_time)/n_time)\n            position = pos_start + (pos_end - pos_start)*interpolation\n        else:\n            position = pos_start\n\n        # Bin each value to bins\n        normalised_position = (position - self.pos_min) / (self.pos_max - self.pos_min) # [0,1)\n        bins = (self.bins * normalised_position).floor().long().detach() # [0,1) -> [0,1..,bins) -> [0,1...,bins-1]\n        return self.emb(bins)\n\nclass LabelConditioner(nn.Module):\n    def __init__(self, y_bins, t_bins, sr, min_duration, max_duration, n_time, out_width, init_scale, max_bow_genre_size, include_time_signal):\n        super().__init__()\n        self.n_time = n_time\n        self.out_width = out_width\n        assert len(y_bins) == 2, f""Expecting (genre, artist) bins, got {y_bins}""\n        bow_genre_bins, artist_bins = y_bins\n        self.max_bow_genre_size = max_bow_genre_size\n        self.bow_genre_emb = SimpleEmbedding(bow_genre_bins, out_width, init_scale)\n        self.artist_emb = SimpleEmbedding(artist_bins, out_width, init_scale)\n        self.include_time_signal = include_time_signal\n        if self.include_time_signal:\n            t_ranges = ((min_duration * sr, max_duration * sr),  # Total length\n                        (0.0, max_duration * sr),                # Absolute pos\n                        (0.0, 1.0))                              # Relative pos\n            assert len(t_ranges) == 3, f""Expecting (total, absolute, relative) ranges, got {t_ranges}""\n            total_length_range, absolute_pos_range, relative_pos_range = t_ranges\n            self.total_length_emb = RangeEmbedding(1, t_bins, total_length_range, out_width, init_scale)\n            self.absolute_pos_emb = RangeEmbedding(n_time, t_bins, absolute_pos_range, out_width, init_scale)\n            self.relative_pos_emb = RangeEmbedding(n_time, t_bins, relative_pos_range, out_width, init_scale, clamp=True)\n\n    def forward(self, y):\n        assert len(y.shape) == 2, f""Expected shape with 2 dims, got {y.shape}""\n        assert y.shape[-1] == 4 + self.max_bow_genre_size, f""Expected shape (N,{4 + self.max_bow_genre_size}), got {y.shape}""\n        assert isinstance(y, t.cuda.LongTensor), f""Expected dtype {t.cuda.LongTensor}, got {y.dtype}""\n        N = y.shape[0]\n        total_length, offset, length, artist, genre = y[:,0:1], y[:,1:2], y[:,2:3], y[:,3:4], y[:,4:]\n\n        # Start embedding of length 1\n        artist_emb = self.artist_emb(artist)\n        # Empty genre slots are denoted by -1. We mask these out.\n        mask = (genre >= 0).float().unsqueeze(2)\n        genre_emb = (self.bow_genre_emb(genre.clamp(0)) * mask).sum(dim=1, keepdim=True)\n        start_emb = genre_emb + artist_emb\n        assert_shape(start_emb, (N, 1, self.out_width))\n\n        # Pos embedding of length n_ctx\n        if self.include_time_signal:\n            start, end = offset, offset + length\n            total_length, start, end = total_length.float(), start.float(), end.float()\n            pos_emb = self.total_length_emb(total_length) + self.absolute_pos_emb(start, end) + self.relative_pos_emb(start/total_length, end/total_length)\n            assert_shape(pos_emb, (N, self.n_time, self.out_width))\n        else:\n            pos_emb = None\n        return start_emb, pos_emb'"
jukebox/prior/prior.py,1,"b'import numpy as np\nimport torch as t\nimport torch.nn as nn\nimport jukebox.utils.dist_adapter as dist\n\nfrom jukebox.transformer.ops import LayerNorm\nfrom jukebox.prior.autoregressive import ConditionalAutoregressive2D\nfrom jukebox.prior.conditioners import Conditioner, LabelConditioner\nfrom jukebox.data.labels import Labeller\n\nfrom jukebox.utils.torch_utils import assert_shape\nfrom jukebox.utils.dist_utils import print_once\nfrom jukebox.vqvae.vqvae import calculate_strides\n\n\n""""""\nModel the prior on vq codes conditioned on timing, artist, genre, lyrics and codes from levels above. \nTo condition on the timing, genre and artist, we use the LabelConditioner class\nTo condition on the codes from the level above, we use the Conditioner class\nTo condition on lyrics, we allow two types of priors:\n- Separate Encoder Decoder: This is the usual encoder-decoder style transformer. The encoder transformer autoregressively \nmodels the lyrics, and we use its last layer to produce keys/values that are attened to by the decoder transformer\n- Single Encoder Decoder: This is a simplification where we combine them into a single model. We merge the text vocab \nand VQ vocab into a single large vocab, and the lyric tokens and VQ tokens into a single longer sequence of tokens which \nwe autoregressively model together.\n""""""\nclass SimplePrior(nn.Module):\n    def __init__(self, z_shapes, l_bins, encoder, decoder, level,\n                 downs_t, strides_t, labels, prior_kwargs, x_cond_kwargs, y_cond_kwargs,\n                 prime_kwargs, copy_input, labels_v3=False,\n                 merged_decoder=False, single_enc_dec=False):\n        super().__init__()\n\n        self.use_tokens = prime_kwargs.pop(\'use_tokens\')\n        self.n_tokens = prime_kwargs.pop(\'n_tokens\')\n        self.prime_loss_fraction = prime_kwargs.pop(\'prime_loss_fraction\')\n\n        self.copy_input = copy_input\n        if self.copy_input:\n            prime_kwargs[\'bins\'] = l_bins\n\n        self.z_shapes = z_shapes\n        self.levels = len(self.z_shapes)\n\n        self.z_shape = self.z_shapes[level]\n\n        self.level = level\n        assert level < self.levels, f""Total levels {self.levels}, got level {level}""\n\n        self.l_bins = l_bins\n\n        # Passing functions instead of the vqvae module to avoid getting params\n        self.encoder = encoder\n        self.decoder = decoder\n\n        # X conditioning\n        self.x_cond = (level != (self.levels - 1))\n        self.cond_level = level + 1\n\n        # Y conditioning\n        self.y_cond = labels\n\n        self.single_enc_dec = single_enc_dec\n        # X conditioning\n        if self.x_cond:\n            self.conditioner_blocks = nn.ModuleList()\n            conditioner_block = lambda _level: Conditioner(input_shape=z_shapes[_level],\n                                                          bins=l_bins,\n                                                          down_t=downs_t[_level],\n                                                          stride_t=strides_t[_level],\n                                                          **x_cond_kwargs)\n            if dist.get_rank() == 0: print(f""Conditioning on 1 above level(s)"")\n            self.conditioner_blocks.append(conditioner_block(self.cond_level))\n\n        # Y conditioning\n        if self.y_cond:\n            self.n_time = self.z_shape[0] # Assuming STFT=TF order and raw=T1 order, so T is first dim\n            self.y_emb = LabelConditioner(n_time=self.n_time,include_time_signal=not self.x_cond,**y_cond_kwargs)\n\n        # Lyric conditioning\n        if single_enc_dec:\n            # Single encoder-decoder transformer\n            self.prior_shapes = [(self.n_tokens,), prior_kwargs.pop(\'input_shape\')]\n            self.prior_bins = [prime_kwargs[\'bins\'], prior_kwargs.pop(\'bins\')]\n            self.prior_dims = [np.prod(shape) for shape in self.prior_shapes]\n            self.prior_bins_shift = np.cumsum([0, *self.prior_bins])[:-1]\n            self.prior_width = prior_kwargs[\'width\']\n            print_once(f\'Creating cond. autoregress with prior bins {self.prior_bins}, \')\n            print_once(f\'dims {self.prior_dims}, \')\n            print_once(f\'shift {self.prior_bins_shift}\')\n            print_once(f\'input shape {sum(self.prior_dims)}\')\n            print_once(f\'input bins {sum(self.prior_bins)}\')\n            print_once(f\'Self copy is {self.copy_input}\')\n\n            self.prime_loss_dims, self.gen_loss_dims = self.prior_dims[0], self.prior_dims[1]\n            self.total_loss_dims = self.prime_loss_dims + self.gen_loss_dims\n            self.prior = ConditionalAutoregressive2D(input_shape=(sum(self.prior_dims),),\n                                                     bins=sum(self.prior_bins),\n                                                     x_cond=(self.x_cond or self.y_cond), y_cond=True,\n                                                     prime_len=self.prime_loss_dims,\n                                                     **prior_kwargs)\n\n        else:\n            # Separate encoder-decoder transformer\n            if self.n_tokens != 0 and self.use_tokens:\n                from jukebox.transformer.ops import Conv1D\n                prime_input_shape = (self.n_tokens,)\n                self.prime_loss_dims = np.prod(prime_input_shape)\n                self.prime_acts_width, self.prime_state_width = prime_kwargs[\'width\'], prior_kwargs[\'width\']\n                self.prime_prior = ConditionalAutoregressive2D(input_shape=prime_input_shape, x_cond=False, y_cond=False,\n                                                               only_encode=True,\n                                                               **prime_kwargs)\n                self.prime_state_proj = Conv1D(self.prime_acts_width, self.prime_state_width, init_scale=prime_kwargs[\'init_scale\'])\n                self.prime_state_ln = LayerNorm(self.prime_state_width)\n                self.prime_bins = prime_kwargs[\'bins\']\n                self.prime_x_out = nn.Linear(self.prime_state_width, self.prime_bins, bias=False)\n                nn.init.normal_(self.prime_x_out.weight, std=0.02 * prior_kwargs[\'init_scale\'])\n            else:\n                self.prime_loss_dims = 0\n            self.gen_loss_dims = np.prod(self.z_shape)\n            self.total_loss_dims = self.prime_loss_dims + self.gen_loss_dims\n            self.prior = ConditionalAutoregressive2D(x_cond=(self.x_cond or self.y_cond), y_cond=self.y_cond,\n                                                     encoder_dims = self.prime_loss_dims, merged_decoder=merged_decoder,\n                                                     **prior_kwargs)\n\n        self.n_ctx = self.gen_loss_dims\n        self.downsamples = calculate_strides(strides_t, downs_t)\n        self.cond_downsample = self.downsamples[level+1] if level != self.levels - 1 else None\n        self.raw_to_tokens = np.prod(self.downsamples[:level+1])\n        self.sample_length = self.n_ctx*self.raw_to_tokens\n        if labels:\n            self.labels_v3 = labels_v3\n            self.labeller = Labeller(self.y_emb.max_bow_genre_size, self.n_tokens, self.sample_length, v3=self.labels_v3)\n\n        print(f""Level:{level}, Cond downsample:{self.cond_downsample}, Raw to tokens:{self.raw_to_tokens}, Sample length:{self.sample_length}"")\n\n\n    def get_y(self, labels, start, get_indices=False):\n        y = labels[\'y\'].clone()\n\n        # Set sample_length to match this level\n        y[:, 2] = int(self.sample_length)\n\n        # Set offset\n        y[:, 1:2] = y[:, 1:2] + int(start * self.raw_to_tokens)\n\n        # Set lyric tokens\n        indices = self.labeller.set_y_lyric_tokens(y, labels)\n        if get_indices:\n            return y, indices\n        else:\n            return y\n\n    def get_z_conds(self, zs, start, end):\n        if self.level != self.levels - 1:\n            assert start % self.cond_downsample == end % self.cond_downsample == 0\n            z_cond = zs[self.level + 1][:,start//self.cond_downsample:end//self.cond_downsample]\n            assert z_cond.shape[1] == self.n_ctx//self.cond_downsample\n            z_conds = [z_cond]\n        else:\n            z_conds = None\n        return z_conds\n\n    def prior_preprocess(self, xs, conds):\n        N = xs[0].shape[0]\n        for i in range(len(xs)):\n            x, shape, dims = xs[i], self.prior_shapes[i], self.prior_dims[i]\n            bins, bins_shift = int(self.prior_bins[i]), int(self.prior_bins_shift[i])\n            assert isinstance(x, t.cuda.LongTensor), x\n            assert (0 <= x).all() and (x < bins).all()\n            #assert_shape(x, (N, *shape))\n            xs[i] = (xs[i] + bins_shift).view(N, -1)\n\n        for i in range(len(conds)):\n            cond, shape, dims = conds[i], self.prior_shapes[i], self.prior_dims[i]\n            if cond is not None:\n                assert_shape(cond, (N, dims, self.prior_width))\n            else:\n                conds[i] = t.zeros((N, dims, self.prior_width), dtype=t.float, device=\'cuda\')\n\n        return t.cat(xs, dim=1), t.cat(conds, dim=1)\n\n    def prior_postprocess(self, z):\n        N = z.shape[0]\n        dims = (self.prior_dims[0], z.shape[1] - self.prior_dims[0])\n        # xs = list(t.split(z, self.prior_dims, dim=1))\n        xs = list(t.split(z, dims, dim=1))\n\n        for i in range(len(xs)):\n            # x, shape, dims, bins, bins_shift = xs[i], self.prior_shapes[i], self.prior_dims[i], self.prior_bins[i], self.prior_bins_shift[i]\n            # assert_shape(x, (N, dims))\n            shape = self.prior_shapes[i]\n            bins, bins_shift = int(self.prior_bins[i]), int(self.prior_bins_shift[i])\n            # xs[i] = (xs[i] - bins_shift).view(N, *shape) #view(N, -1, *shape[1:])\n            xs[i] = (xs[i] - bins_shift).view(N, -1, *shape[1:])\n            xs[i] = t.clamp(xs[i], min=0)  # If not masking loss, model may have generated lyric/midi tokens which are now shifted <0 by bin_shift\n            assert (xs[i] < bins).all(), f\'rank: {dist.get_rank()}, bins: {bins}, dims {dims}, shape {shape}, prior_shape {self.prior_shapes}, bins_shift {bins_shift}, xs[i]: {xs[i]}\'\n\n        return xs[-1]\n\n    def x_emb(self, z_conds):\n        z_conds = z_conds[:self.cond_level - self.level]\n        assert len(z_conds) == len(self.conditioner_blocks) == self.cond_level - self.level, f""Expected {len(z_conds)} == {len(self.conditioner_blocks)} == {self.cond_level} - {self.level}""\n        x_cond = None\n        for z_cond, conditioner_block in reversed(list(zip(z_conds, self.conditioner_blocks))):\n            x_cond = conditioner_block(z_cond, x_cond)\n        return x_cond\n\n    def encode(self, x, start_level=None, end_level=None, bs_chunks=1):\n        if start_level == None:\n            start_level = self.level\n        if end_level == None:\n            end_level = self.levels\n        # Get latents\n        with t.no_grad():\n            zs = self.encoder(x, start_level=start_level, end_level=end_level, bs_chunks=bs_chunks)\n        return zs\n\n    def decode(self, zs, start_level=None, end_level=None, bs_chunks=1):\n        if start_level == None:\n            start_level = self.level\n        if end_level == None:\n            end_level = self.levels\n\n        assert len(zs) == end_level - start_level\n        with t.no_grad():\n            x_out = self.decoder(zs, start_level=start_level, end_level=end_level, bs_chunks=bs_chunks)\n        return x_out\n\n    def get_cond(self, z_conds, y):\n        if y is not None:\n            assert y.shape[1] == 4 + self.y_emb.max_bow_genre_size + self.n_tokens, f""Expected {4} + {self.y_emb.max_bow_genre_size} + {self.n_tokens}, got {y.shape[1]}""\n            n_labels = y.shape[1] - self.n_tokens\n            y, prime = y[:,:n_labels], y[:,n_labels:]\n        else:\n            y, prime = None, None\n        y_cond, y_pos = self.y_emb(y) if self.y_cond else (None, None)\n        x_cond = self.x_emb(z_conds) if self.x_cond else y_pos\n        return x_cond, y_cond, prime\n\n    def sample(self, n_samples, z=None, z_conds=None, y=None, fp16=False, temp=1.0, top_k=0, top_p=0.0,\n               chunk_size=None, sample_tokens=None):\n        N = n_samples\n        if z is not None: assert z.shape[0] == N, f""Expected shape ({N},**), got shape {z.shape}""\n        if y is not None: assert y.shape[0] == N, f""Expected shape ({N},**), got shape {y.shape}""\n        if z_conds is not None:\n            for z_cond in z_conds:\n                assert z_cond.shape[0] == N,  f""Expected shape ({N},**), got shape {z_cond.shape}""\n\n        no_past_context = (z is None or z.shape[1] == 0)\n        if dist.get_rank() == 0:\n            name = {True: \'Ancestral\', False: \'Primed\'}[no_past_context]\n            print(f""{name} sampling {n_samples} samples with temp={temp}, top_k={top_k}, top_p={top_p}"")\n\n        with t.no_grad():\n            # Currently x_cond only uses immediately above layer\n            x_cond, y_cond, prime = self.get_cond(z_conds, y)\n            if self.single_enc_dec:\n                # assert chunk_size % self.prime_loss_dims == 0. TODO: Check if needed\n                if no_past_context:\n                    z, x_cond = self.prior_preprocess([prime], [None, x_cond])\n                else:\n                    z, x_cond = self.prior_preprocess([prime, z], [None, x_cond])\n                if sample_tokens is not None:\n                    sample_tokens += self.n_tokens\n                z = self.prior.primed_sample(n_samples, z, x_cond, y_cond, fp16=fp16, temp=temp,\n                                             top_k=top_k, top_p=top_p, chunk_size=chunk_size, sample_tokens=sample_tokens)\n                z = self.prior_postprocess(z)\n            else:\n                encoder_kv = self.get_encoder_kv(prime, fp16=fp16, sample=True)\n                if no_past_context:\n                    z = self.prior.sample(n_samples, x_cond, y_cond, encoder_kv, fp16=fp16, temp=temp, top_k=top_k,\n                                          top_p=top_p, sample_tokens=sample_tokens)\n                else:\n                    z = self.prior.primed_sample(n_samples, z, x_cond, y_cond, encoder_kv, fp16=fp16, temp=temp,\n                                             top_k=top_k, top_p=top_p, chunk_size=chunk_size, sample_tokens=sample_tokens)\n            if sample_tokens is None:\n                assert_shape(z, (N, *self.z_shape))\n        return z\n\n    def get_encoder_kv(self, prime, fp16=False, sample=False):\n        if self.n_tokens != 0 and self.use_tokens:\n            if sample:\n                self.prime_prior.cuda()\n            N = prime.shape[0]\n            prime_acts = self.prime_prior(prime, None, None, None, fp16=fp16)\n            assert_shape(prime_acts, (N, self.prime_loss_dims, self.prime_acts_width))\n            assert prime_acts.dtype == t.float, f\'Expected t.float, got {prime_acts.dtype}\'\n            encoder_kv = self.prime_state_ln(self.prime_state_proj(prime_acts))\n            assert encoder_kv.dtype == t.float, f\'Expected t.float, got {encoder_kv.dtype}\'\n            if sample:\n                self.prime_prior.cpu()\n                if fp16:\n                    encoder_kv = encoder_kv.half()\n        else:\n            encoder_kv = None\n        return encoder_kv\n\n    def get_prime_loss(self, encoder_kv, prime_t):\n        if self.use_tokens:\n            encoder_kv = encoder_kv.float()\n            encoder_kv = self.prime_x_out(encoder_kv)\n            prime_loss = nn.functional.cross_entropy(encoder_kv.view(-1, self.prime_bins), prime_t.view(-1)) / np.log(2.)\n        else:\n            prime_loss = t.tensor(0.0, device=\'cuda\')\n        return prime_loss\n\n    def z_forward(self, z, z_conds=[], y=None, fp16=False, get_preds=False, get_attn_weights=False):\n        """"""\n        Arguments:\n            get_attn_weights (bool or set): Makes forward prop dump\n                self-attention softmaxes to self.prior.transformer.ws. Either a\n                set of layer indices indicating which layers to store, or a\n                boolean value indicating whether to dump all.\n        """"""\n        assert isinstance(get_attn_weights, (bool, set))\n        if get_attn_weights:\n            self.prior.transformer.set_record_attn(get_attn_weights)\n        x_cond, y_cond, prime = self.get_cond(z_conds, y)\n        if self.copy_input:\n            prime = z[:,:self.n_tokens]\n        if self.single_enc_dec:\n            z, x_cond = self.prior_preprocess([prime, z], [None, x_cond])\n            (prime_loss, gen_loss), preds = self.prior(z, x_cond, y_cond, fp16=fp16, get_sep_loss=True, get_preds=get_preds)\n        else:\n            encoder_kv = self.get_encoder_kv(prime, fp16=fp16)\n            prime_loss = self.get_prime_loss(encoder_kv, prime)\n            gen_loss, preds = self.prior(z, x_cond, y_cond, encoder_kv, fp16=fp16, get_preds=get_preds)\n        loss = (self.prime_loss_fraction*prime_loss*self.prime_loss_dims/self.total_loss_dims) + \\\n                   (gen_loss*self.gen_loss_dims/self.total_loss_dims)\n        metrics=dict(bpd=gen_loss.clone().detach(), prime_loss=prime_loss.clone().detach(),\n                     gen_loss=gen_loss.clone().detach())\n        if get_preds:\n            metrics[""preds""] = preds.clone().detach()\n        if get_attn_weights:\n            ws = self.prior.transformer.ws\n            self.prior.transformer.set_record_attn(False)\n            return ws\n        else:\n            return loss, metrics\n\n    def forward(self, x, y=None, fp16=False, decode=False, get_preds=False):\n        bs = x.shape[0]\n        z, *z_conds = self.encode(x, bs_chunks=bs)\n        loss, metrics = self.z_forward(z=z, z_conds=z_conds, y=y, fp16=fp16, get_preds=get_preds)\n        if decode:\n            x_out = self.decode([z, *z_conds])\n        else:\n            x_out = None\n        return x_out, loss, metrics\n'"
jukebox/tests/test_sample.py,0,"b'import torch as t\nimport numpy as np\nfrom jukebox.sample import sample_level\nfrom jukebox.utils.torch_utils import assert_shape\nfrom jukebox.hparams import Hyperparams\n\ndef repeat(x, n, dim):\n    if dim == -1:\n        dim = len(x.shape) - 1\n    return x.reshape(int(np.prod(x.shape[:dim+1])), 1, int(np.prod(x.shape[dim+1:]))).repeat(1,n,1).reshape(*x.shape[:dim], n * x.shape[dim], *x.shape[dim+1:])\n\n# Tests\nclass DummyPrior:\n    def __init__(self, n_ctx, level, levels):\n        self.n_ctx = n_ctx\n        self.level = level\n        self.levels = levels\n        self.downsamples = (8,4,4)\n        self.cond_downsample = self.downsamples[level+1] if level != self.levels - 1 else None\n        self.raw_to_tokens = int(np.prod(self.downsamples[:level+1]))\n        self.sample_length = self.n_ctx*self.raw_to_tokens\n\n        print(f""Level:{level}, Cond downsample:{self.cond_downsample}, Raw to tokens:{self.raw_to_tokens}, Sample length:{self.sample_length}"")\n\n    def get_y(self, labels, start):\n        y = labels[\'y\'].clone()\n        # Set sample_length to match this level\n        y[:, 2] = self.sample_length\n        # Set offset\n        y[:, 1:2] = y[:, 1:2] + start * self.raw_to_tokens\n        return y\n\n    def get_z_conds(self, zs, start, end):\n        if self.level != self.levels - 1:\n            assert start % self.cond_downsample == end % self.cond_downsample == 0\n            z_cond = zs[self.level + 1][:,start//self.cond_downsample:end//self.cond_downsample]\n            assert z_cond.shape[1] == self.n_ctx//self.cond_downsample\n            z_conds = [z_cond]\n        else:\n            z_conds = None\n        return z_conds\n\n    def ancestral_sample(self, n_samples, z_conds=None, y=None):\n        z = t.zeros((n_samples, self.n_ctx), dtype=t.long, device=\'cuda\') + \\\n            t.arange(0, self.n_ctx, dtype=t.long, device=\'cuda\').view(1, self.n_ctx)\n\n        if z_conds is not None:\n            z_cond = z_conds[0]\n            assert_shape(z_cond, (n_samples, self.n_ctx // 4))\n            assert (z // 4 == repeat(z_cond, 4, 1)).all(), f\'z: {z}, z_cond: {z_cond}, diff: {(z // 4) - repeat(z_cond, 4, 1)}\'\n        return z\n\n    def primed_sample(self, n_samples, z, z_conds=None, y=None):\n        prime = z.shape[1]\n        assert_shape(z, (n_samples, prime))\n        start = z[:,-1:] + 1\n        z_rest = (t.arange(0, self.n_ctx - prime, dtype=t.long, device=\'cuda\').view(1, self.n_ctx - prime) + start).view(n_samples, self.n_ctx - prime)\n        z = t.cat([z, z_rest], dim=1)\n\n        if z_conds is not None:\n            z_cond = z_conds[0]\n            assert_shape(z_cond, (n_samples, self.n_ctx // 4))\n            assert (z // 4 == repeat(z_cond, 4, 1)).all(), f\'z: {z}, z_cond: {z_cond}, diff: {(z // 4) - repeat(z_cond, 4, 1)}\'\n        return z\n\n# Sample multiple levels\ndef _sample(zs, labels,  priors, sample_levels, hps):\n    for level in reversed(sample_levels):\n        prior = priors[level]\n        # set correct total_length, hop_length and sampling_kwargs for level\n        total_length = (hps.sample_length * hps.n_segment)//prior.raw_to_tokens\n        hop_length = hps.hop_lengths[level]\n        zs = sample_level(zs, labels[level], dict(), level, prior, total_length, hop_length, hps)\n    return zs\n\n# Ancestral sample\ndef test_ancestral_sample(labels, priors, hps):\n    sample_levels = list(range(hps.levels))\n    zs = [t.zeros(hps.n_samples,0,dtype=t.long, device=\'cuda\') for _ in range(hps.levels)]\n    zs = _sample(zs, labels, priors, sample_levels, hps)\n\n    # Test\n    for z in zs:\n        total_length = z.shape[1]\n        # Check sample\n        assert ((z - t.arange(0, total_length, dtype=t.long, device=\'cuda\').view(1, total_length)) == 0).all()\n\n    print(""dummy ancestral sample passed"")\n\ndef test_primed_sample(labels, priors, hps):\n    sample_levels = list(range(hps.levels))\n\n    start = t.tensor([15, 23, 11, 9], dtype=t.long, device=\'cuda\').view(4, 1)\n\n    zs_in = []\n    zs = []\n    for i in reversed(range(3)):\n        n_ctx = 8192*(4**i)\n        n_prime = n_ctx // 4\n        z_prime = t.arange(0, n_prime, dtype=t.long, device=\'cuda\').view(1, n_prime) % (2*(4**i))\n        z_rest = t.randint(-10, -1, size=(1, n_ctx - n_prime), dtype=t.long, device=\'cuda\')\n        z_in = t.cat([z_prime, z_rest], dim=1) + (4**i)*start\n        zs_in.append(z_in)\n        zs.append(z_prime + (4**i)*start)\n\n    zs = _sample(zs, labels, priors, sample_levels, hps)\n\n    # Test\n    for z, z_in in zip(zs, zs_in):\n        total_length = z.shape[1]\n        prime_length = z.shape[1] // (4 * hps.n_segment)\n        # Match prime tokens\n        assert (z[:,:prime_length] == z_in[:,:prime_length]).all()\n        # Check sample\n        z_rest = z[:,prime_length-1:] - z[:,prime_length-1:prime_length]\n        assert ((z_rest - t.arange(0, total_length - prime_length + 1, dtype=t.long, device=\'cuda\').view(1, total_length - prime_length + 1)) == 0).all()\n\n    print(""dummy primed sample passed"")\n\ndef check_sample():\n    n_ctx = 8192\n    n_samples = 4\n    levels = 3\n    priors = [DummyPrior(n_ctx, level, levels) for level in range(levels)]\n    max_total_length, offset, sample_length = 4134368, 0, n_ctx*8*4*4\n    y = t.tensor([max_total_length, offset, sample_length, 10, 1, -1, -1, -1, -1], dtype=t.long, device=\'cuda\').view(1, 9).repeat(n_samples, 1)\n    labels = [dict(y=y, info=[[]*n_samples]) for level in range(levels)]\n    hps = Hyperparams({\n        \'levels\': 3,\n        \'sample_length\': sample_length,\n        \'n_segment\': 2,\n        \'n_ctx\': n_ctx,\n        \'n_tokens\': 0,\n        \'hop_lengths\': [n_ctx//2, n_ctx//2, n_ctx//8],\n        \'n_samples\': n_samples,\n        \'use_tokens\': False\n    })\n    test_ancestral_sample(labels, priors, hps)\n    test_primed_sample(labels, priors, hps)\n\ncheck_sample()\n'"
jukebox/transformer/__init__.py,0,b''
jukebox/transformer/factored_attention.py,2,"b'# Factored attention\nimport math\nimport numpy as np\nimport torch as t\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom jukebox.transformer.ops import Conv1D\nfrom jukebox.utils.checkpoint import checkpoint\n\ndef repeat(x, n, dim):\n    if dim == -1:\n        dim = len(x.shape) - 1\n    return x.view(int(np.prod(x.shape[:dim+1])), 1, int(np.prod(x.shape[dim+1:]))).repeat(1,n,1).view(*x.shape[:dim], n * x.shape[dim], *x.shape[dim+1:])\n\ndef get_mask(mask, q_l, kv_l, blocks, spread, device, sample, sample_t):\n    # returns a mask of shape 1 x 1 x q_l x kv_l or None if masking is not needed.\n    if mask is None or q_l == 1:\n        return None\n    offset = sample_t - q_l if sample else max(kv_l - q_l, 0)\n    if mask == \'autoregressive\':\n        # Masked dense\n        mask = t.ones(q_l, kv_l, device=device).tril(offset)\n    elif mask == \'summary\':\n        # Masked summary\n        mask = t.nn.functional.pad(t.ones(q_l, q_l, device=device).tril().view(q_l, blocks, q_l // blocks)[:,:-1,-kv_l//blocks:],(0,0,1,0),value=1).contiguous().view(q_l, kv_l)\n    elif mask == \'prime\':\n        mask = t.ones(q_l, kv_l, device=device).tril(offset)\n    return mask.view(1,1,q_l,kv_l)\n\nclass FactoredAttention(nn.Module):\n    def __init__(self, n_in, n_ctx, n_state, n_head,\n                 attn_dropout=0.0, resid_dropout=0.0,\n                 scale=True, mask=False,\n                 zero_out=False, init_scale=1.0,\n                 checkpoint_attn=0,\n                 attn_func=0, blocks=None, spread=None,\n                 encoder_dims=None, prime_len=None):\n        super().__init__()\n        self.n_in = n_in\n        self.n_ctx = n_ctx # NOTE: n_ctx could be different within operations. This is complete n_ctx\n        self.n_state = n_state\n        assert n_state % n_head == 0\n        self.n_head = n_head\n        self.scale = scale\n        self.mask = mask\n        if attn_func == 6:\n            self.c_attn = Conv1D(n_in, n_state, init_scale=init_scale)\n            self.c_enc_kv = Conv1D(n_in, n_state * 2, init_scale=init_scale)\n        else:\n            self.c_attn = Conv1D(n_in, n_state * 3, init_scale=init_scale)\n        self.c_proj = Conv1D(n_state, n_in, zero_out, init_scale=init_scale)\n        self.attn_dropout = nn.Dropout(attn_dropout) if attn_dropout > 0.0 else lambda x: x\n        self.resid_dropout = nn.Dropout(resid_dropout) if resid_dropout > 0.0 else lambda x: x\n\n        # Sequence of length l is factored as [blocks, l // blocks]\n        self.attn_func = attn_func\n        self.qkv, self.attn, self.attn_mask = {\n            0: (self.factored_qkv, self.dense_attn, \'autoregressive\'),              # Attend to all positions\n            1: (self.factored_qkv, self.block_attn, \'autoregressive\'),              # Attend to your block\n            2: (self.factored_qkv, self.transpose_block_attn, \'autoregressive\'),    # Attend to transpose block\n            3: (self.factored_qkv, self.prev_block_attn, None),                     # Attend to previous block\n            4: (self.factored_qkv, self.summary_attn, \'summary\'),                   # Attend to last position of each block\n            5: (self.factored_qkv, self.summary_spread_attn, \'summary\'),\n            6: (self.decode_qkv, self.decode_attn, None),\n            7: (self.prime_qkv, self.prime_attn, \'prime\')\n        }[attn_func] # Attend to last k position of each block\n\n        self.blocks = blocks\n        self.spread = spread\n        if blocks is not None:\n            assert n_ctx % blocks == 0\n            self.block_ctx = n_ctx // blocks\n        self.checkpoint_attn = checkpoint_attn # 0: None, 1: Attn after heads split, 2: Attn\n\n        self.sample_t = 0\n        self.cache = {}\n        self.encoder_dims = encoder_dims\n        self.prime_len = prime_len\n        self.record_attn = False\n        self.w = None\n\n    def _attn(self, q, k, v, sample):\n        scale = 1. / math.sqrt(math.sqrt(self.n_state // self.n_head))\n        if self.training:\n            w = t.matmul(q * scale, k * scale)\n        else:\n            w = t.matmul(q, k)\n            w.mul_(scale*scale)\n        wtype = w.dtype\n        w = w.float()\n        if self.mask:\n            # Generate appropriate mask to mask out all positions before current\n            # Might take up lot of memory for dense, so can cache it\n            mask = get_mask(self.attn_mask, q.size(-2), k.size(-1), self.blocks, self.spread, w.device, sample, self.sample_t)\n            if mask is not None:\n                #print(mask)\n                w = w * mask + -1e9 * (1 - mask)\n            w = F.softmax(w, dim=-1).type(wtype)\n        else:\n            w = F.softmax(w, dim=-1).type(wtype)\n        if self.record_attn:\n            self.w = w #.float().cpu().numpy()\n            if self.attn_func == 7:\n                # only keep music queries and lyrics keys/values\n                self.w = self.w[:,:,self.prime_len:,:self.prime_len]\n        w = self.attn_dropout(w)\n        a = t.matmul(w, v)\n        return a\n\n    def merge_heads(self, x):\n        x = x.permute(0, 2, 1, 3).contiguous()\n        new_x_shape = (*x.size()[:-2], x.size(-2) * x.size(-1))\n        return x.view(*new_x_shape)  # in Tensorflow implem: fct merge_states\n\n    def split_heads(self, x, k=False):\n        new_x_shape = (*x.size()[:-1], self.n_head, x.size(-1) // self.n_head)\n        x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states\n        if k:\n            return x.permute(0, 2, 3, 1)\n        else:\n            return x.permute(0, 2, 1, 3)\n\n    def dense_attn(self, query, key, value, sample):\n        query = self.split_heads(query)\n        key = self.split_heads(key, k=True)\n        value = self.split_heads(value)\n        if self.checkpoint_attn == 1 and not sample:\n            a = checkpoint(lambda q,k,v,s=sample: self._attn(q,k,v,s), (query, key, value),\n                       (), True)\n        else:\n            a = self._attn(query,key,value,sample)\n        a = self.merge_heads(a)\n        return a\n\n    def block_attn(self, q, k, v, sample):\n        blocks, block_ctx = self.blocks, self.block_ctx # block_ctx is l // blocks for complete l ie l = n_ctx. Sampling has less l\n        bs, l, d = v.shape # For sample, q_l = 1, k_l = v_l = sample_t\n        if sample:\n            assert l == self._suff_cache_len(), f""{l} != {self._suff_cache_len()}""\n            return self.dense_attn(q, k, v, sample).view(bs, 1, d)\n        else:\n            ql = q.shape[1]\n            q = q.view(bs * ql // block_ctx, block_ctx, d)\n            if ql < l:\n                l = ql\n                k = k[:, -l:].contiguous()\n                v = v[:, -l:].contiguous()\n            k = k.view(bs * l // block_ctx, block_ctx, d)\n            v = v.view(bs * l // block_ctx, block_ctx, d)\n            return self.dense_attn(q, k, v, sample).view(bs, l, d)\n\n    def transpose_block_attn(self, q, k, v, sample):\n        blocks, block_ctx = self.blocks, self.block_ctx # block_ctx is l // blocks for complete l ie l = n_ctx. Sampling has less l\n        bs, l, d = v.shape # For sample, q_l = 1, k_l = v_l = sample_t\n        if sample:\n            block_l = (l - 1) % block_ctx\n            k = k[:,block_l::block_ctx,:]\n            v = v[:,block_l::block_ctx,:]\n            return self.dense_attn(q, k, v, sample).view(bs, 1, d)\n        else:\n            ql = q.shape[1]\n            q = q.view(bs, ql // block_ctx, block_ctx, d).transpose(1,2).contiguous().view(bs * block_ctx, ql // block_ctx, d)\n            k = k.view(bs,  l // block_ctx, block_ctx, d).transpose(1,2).contiguous().view(bs * block_ctx,  l // block_ctx, d)\n            v = v.view(bs,  l // block_ctx, block_ctx, d).transpose(1,2).contiguous().view(bs * block_ctx,  l // block_ctx, d)\n            return self.dense_attn(q, k, v, sample).view(bs, block_ctx, ql // block_ctx, d).transpose(1,2).contiguous().view(bs, ql, d)\n\n    def prev_block_attn(self, q, k, v, sample):\n        blocks, block_ctx = self.blocks, self.block_ctx # block_ctx is l // blocks for complete l ie l = n_ctx. Sampling has less l\n        bs, l, d = v.shape # For sample, q_l = 1, k_l = v_l = sample_t\n        if sample:\n            assert l == self._suff_cache_len(), f""{l} != {self._suff_cache_len()}""\n            block = (l - 1) // block_ctx\n            prev_l = (block - 1) * block_ctx\n            if block > 0:\n                assert prev_l == 0\n                k = k[:, prev_l:prev_l + block_ctx, :]\n                v = v[:, prev_l:prev_l + block_ctx, :]\n            else:\n                k = t.zeros(bs, block_ctx, d, device=q.device, dtype=q.dtype)\n                v = t.zeros(bs, block_ctx, d, device=q.device, dtype=q.dtype)\n            return self.dense_attn(q, k, v, sample).view(bs, 1, d)\n        else:\n            ql = q.shape[1]\n            q = q.view(bs * ql // block_ctx, block_ctx, d)\n            k = t.nn.functional.pad(k.view(bs, l // block_ctx, block_ctx, d)[:, :-1, :, :], (0,0,0,0,1,0)).view(bs * l // block_ctx, block_ctx, d)\n            v = t.nn.functional.pad(v.view(bs, l // block_ctx, block_ctx, d)[:, :-1, :, :], (0,0,0,0,1,0)).view(bs * l // block_ctx, block_ctx, d)\n            if ql < l:\n                qb = ql // block_ctx\n                kb =  l // block_ctx\n                l = ql\n                k = k.view(bs, kb, block_ctx, d)[:, -qb:].contiguous().view(bs * qb, block_ctx, d)\n                v = v.view(bs, kb, block_ctx, d)[:, -qb:].contiguous().view(bs * qb, block_ctx, d)\n            return self.dense_attn(q, k, v, sample).view(bs, l, d)\n\n    def summary_attn(self, q, k, v, sample):\n        blocks, block_ctx = self.blocks, self.block_ctx # block_ctx is l // blocks for complete l ie l = n_ctx. Sampling has less l\n        bs, l, d = v.shape # For sample, q_l = 1, k_l = v_l = sample_t\n        if sample:\n            k = t.nn.functional.pad(k[:, block_ctx-1:blocks*block_ctx-1:block_ctx, :],(0,0,1,0))\n            v = t.nn.functional.pad(v[:, block_ctx-1:blocks*block_ctx-1:block_ctx, :],(0,0,1,0))\n            return self.dense_attn(q, k, v, sample).view(bs, 1, d)\n        else:\n            k = t.nn.functional.pad(k.view(bs, blocks, l // blocks, d)[:, :-1, -1, :],(0,0,1,0)) # bs, blocks, d\n            v = t.nn.functional.pad(v.view(bs, blocks, l // blocks, d)[:, :-1, -1, :],(0,0,1,0)) # bs, blocks, d\n            return self.dense_attn(q, k, v, sample).view(bs, l, d)\n\n    def summary_spread_attn(self, q, k, v, sample):\n        blocks, block_ctx, spread = self.blocks, self.block_ctx, self.spread # block_ctx is l // blocks for complete l ie l = n_ctx. Sampling has less l\n        bs, l, d = v.shape # For sample, q_l = 1, k_l = v_l = sample_t\n        if sample:\n            assert False, ""Not yet implemented""\n            # k = t.nn.functional.pad(k,(0,0,block_ctx,(-l)%block_ctx)).view(bs, -1, block_ctx, d)[:,:-1,-spread:,:].contiguous().view(bs, -1, d)\n            # v = t.nn.functional.pad(v,(0,0,block_ctx,(-l)%block_ctx)).view(bs, -1, block_ctx, d)[:,:-1,-spread:,:].contiguous().view(bs, -1, d)\n            # return self.dense_attn(q, k, v, sample).view(bs, 1, d)\n        else:\n            k = t.nn.functional.pad(k.view(bs, blocks, l // blocks, d)[:, :-1, -spread:, :],(0,0,0,0,1,0)).contiguous().view(bs, blocks * spread, d)  # bs, blocks * spread, d\n            v = t.nn.functional.pad(v.view(bs, blocks, l // blocks, d)[:, :-1, -spread:, :],(0,0,0,0,1,0)).contiguous().view(bs, blocks * spread, d)  # bs, blocks * spread, d\n            return self.dense_attn(q, k, v, sample).view(bs, l, d)\n\n    def prime_attn(self, q, k, v, sample):\n        prime_len = self._prime_len\n        k = k[:, :prime_len]\n        v = v[:, :prime_len]\n        return self.dense_attn(q, k, v, sample)\n\n    def decode_attn(self, q, k, v, sample):\n        assert k.shape[1] == v.shape[1] == self.encoder_dims, f\'k: {k.shape}, v: {v.shape}, enc_dims: {self.encoder_dims}\'\n        return self.dense_attn(q, k, v, sample)\n\n    def factored_qkv(self, x, encoder_kv=None, sample=False):\n        curr_ctx = x.shape[1]\n        assert encoder_kv is None\n        query, key, value = x.chunk(3, dim=2)\n        if sample:\n            self.sample_t += curr_ctx\n            key, value = self._append_cache(key, value)\n            l_cache = self._suff_cache_len()\n            if self._cache_len() > l_cache:\n                self._slice_cache(-l_cache)\n            if curr_ctx > 1:\n                if self.attn_func != 0:\n                    query = self._pad_to_block_ctx(query, query=True)\n                    key = self._pad_to_block_ctx(key)\n                    value = self._pad_to_block_ctx(value)\n                    assert key.shape[1] % self.block_ctx == 0\n                    assert query.shape[1] % self.block_ctx == 0\n                assert key.shape[1] == value.shape[1]\n                assert query.shape[1] <= key.shape[1]\n                sample = False\n            else:\n                key = self.cache[\'key\']\n                value = self.cache[\'value\']\n        return query, key, value, sample\n\n    def prime_qkv(self, x, encoder_kv=None, sample=False):\n        curr_ctx = x.shape[1]\n        assert encoder_kv is None\n        query, key, value = x.chunk(3, dim=2)\n        if sample:\n            if self._cache_len() < self._prime_len:\n                self._append_cache(key, value)\n            if self._cache_len() > self._prime_len:\n                self._slice_cache(0, self._prime_len)\n            key, value = self.cache[\'key\'], self.cache[\'value\']\n            self.sample_t += curr_ctx\n            assert key.shape[1] == value.shape[1] == self._suff_cache_len(), f\'k: {key.shape}, v: {value.shape}, prime_dims: {self._suff_cache_len()}\'\n        else:\n            assert key.shape[1] == value.shape[1] == self.n_ctx, f\'k: {key.shape}, v: {value.shape}, prime_dims: {self.n_ctx}\'\n        assert key.shape[0] == value.shape[0] == query.shape[0], f\'k: {key.shape}, v: {value.shape}, q: {query.shape}\'\n        assert key.shape[2] == value.shape[2] == query.shape[2], f\'k: {key.shape}, v: {value.shape}, q: {query.shape}\'\n        return query, key, value, sample\n\n    def decode_qkv(self, x, encoder_kv=None, sample=False):\n        curr_ctx = x.shape[1]\n        assert encoder_kv is not None\n        query = x\n        if sample:\n            if self.sample_t == 0:\n                self.cache[\'key\'], self.cache[\'value\'] = self.c_enc_kv(encoder_kv.type_as(x)).chunk(2, dim=2)\n            key, value = self.cache[\'key\'], self.cache[\'value\']\n            self.sample_t += curr_ctx\n        else:\n            key, value = self.c_enc_kv(encoder_kv.type_as(x)).chunk(2, dim=2)\n        assert key.shape[0] == value.shape[0] == query.shape[0], f\'k: {key.shape}, v: {value.shape}, q: {query.shape}\'\n        assert key.shape[1] == value.shape[1] == self.encoder_dims, f\'k: {key.shape}, v: {value.shape}, enc_dims: {self.encoder_dims}\'\n        assert key.shape[2] == value.shape[2] == query.shape[2], f\'k: {key.shape}, v: {value.shape}, q: {query.shape}\'\n        return query, key, value, sample\n\n    def forward(self, x, encoder_kv=None, sample=False):\n        curr_ctx = x.shape[1]\n        x = self.c_attn(x)\n        query, key, value, sample = self.qkv(x, encoder_kv=encoder_kv, sample=sample)\n        if self.checkpoint_attn == 2 and not sample:\n            a = checkpoint(lambda q,k,v,s=sample: self.attn(q,k,v,s), (query, key, value), (), True)\n        else:\n            a = self.attn(query,key,value,sample)\n        if a.shape[1] != curr_ctx:\n            offset = self._offset(curr_ctx)\n            a = a[:,offset:offset + curr_ctx,:].contiguous()\n        a = self.c_proj(a)\n        return self.resid_dropout(a)\n\n    @property\n    def _prime_len(self):\n        prime_len = self.prime_len\n        assert prime_len is not None\n        prime_blocks = (prime_len // self.blocks) + 1\n        return prime_blocks * self.blocks\n\n    def _offset(self, curr_ctx):\n        if self.attn_func == 0:\n            return 0\n        return (self.sample_t - curr_ctx) % self.block_ctx\n\n    def _pad_to_block_ctx(self, x, query=False):\n        l = x.shape[1]\n        offset = self._offset(l) if query else 0\n        n_blocks = (l + offset + self.block_ctx - 1) // self.block_ctx\n        pad = n_blocks * self.block_ctx - l - offset\n        if pad == 0 and offset == 0:\n            return x\n        else:\n            return F.pad(x, (0, 0, offset, pad))\n\n    def _cache_len(self):\n        return 0 if \'key\' not in self.cache else self.cache[\'key\'].shape[1]\n\n    def _suff_cache_len(self):\n        """"""\n        Precondition:\n            key and value are appended with the current context and\n            self.sample_t reflects the 1-indexed sample location in the\n            context.\n        """"""\n        if self.attn_func == 0:\n            return self.sample_t\n        elif self.attn_func == 1:\n            return (self.sample_t - 1) % self.block_ctx + 1\n        elif self.attn_func == 2:\n            return self.sample_t\n        elif self.attn_func == 3:\n            if self.sample_t <= self.block_ctx:\n                return self.sample_t\n            else:\n                curr_block = (self.sample_t - 1) % self.block_ctx + 1\n                prev_block = self.block_ctx\n                return curr_block + prev_block\n        elif self.attn_func == 6:\n            return self.encoder_dims\n        elif self.attn_func == 7:\n            return min(self.sample_t, self._prime_len)\n        else:\n            raise NotImplementedError()\n\n    def _slice_cache(self, start, end=None):\n        self.cache[\'key\'] = self.cache[\'key\'][:, start:end]\n        self.cache[\'value\'] = self.cache[\'value\'][:, start:end]\n\n    def _append_cache(self, key, value):\n        if \'key\' not in self.cache:\n            self.cache[\'key\'] = key\n            self.cache[\'value\'] = value\n        else:\n            old_key, old_value = key, value\n            key = t.cat([self.cache[\'key\'], key], dim=1)\n            value = t.cat([self.cache[\'value\'], value], dim=1)\n            del self.cache[\'key\']\n            del self.cache[\'value\']\n            del old_key\n            del old_value\n            self.cache[\'key\'] = key\n            self.cache[\'value\'] = value\n        return self.cache[\'key\'], self.cache[\'value\']\n\n    def del_cache(self):\n        self.sample_t = 0\n        if \'key\' in self.cache:\n            del self.cache[\'key\']\n        if \'value\' in self.cache:\n            del self.cache[\'value\']\n        self.cache = {}\n\n    def check(self):\n        blocks = self.blocks or 1\n        spread = self.spread or 1\n        bs, l, d = (4, self.n_ctx, self.n_in)\n        x = t.randn(bs, l, d).cuda()\n        x.requires_grad = True\n        x_out = self.forward(x) # bs, l, d\n        loss = x_out.mean(dim = -1) # bs, l\n        pos = 60\n        grad = t.autograd.grad(loss[2, pos], x)[0]\n\n        assert grad.shape == (bs, l, d)\n        assert (grad[:2] == 0).all()\n        assert (grad[3:] == 0).all()\n        assert (grad[2, (pos + 1):] == 0).all()\n        pos_grad = (t.sum(grad[2] ** 2, dim=-1) > 0).nonzero().view(-1).cpu()\n\n        block_pos = pos - (pos % (l // blocks))\n        exp_pos_grad = {0: t.arange(pos),\n                        1: t.arange(block_pos, pos),\n                        2: t.arange(pos % (l // blocks), pos, l // blocks),\n                        3: t.arange(block_pos - l // blocks, block_pos),\n                        4: t.arange(l // blocks - 1, pos, l // blocks),\n                        5: ((t.arange(pos) % (l // blocks) >= (l // blocks - spread)) & (t.arange(pos) < block_pos)).nonzero().view(-1)}[self.attn_func]\n        exp_pos_grad = t.cat([exp_pos_grad, t.tensor([pos])], dim=-1)\n\n        assert (len(pos_grad) == len(exp_pos_grad)) and (pos_grad == exp_pos_grad).all(), \\\n            f""Expected pos grad {exp_pos_grad} got {pos_grad} for attn_func {self.attn_func} pos {pos} l {l} blocks {blocks}""\n\n    def check_cache(self, n_samples, sample_t, fp16):\n        assert self.sample_t == sample_t, f""{self.sample_t} != {sample_t}""\n        if sample_t == 0:\n            assert self.cache == {}\n        else:\n            dtype = {True: t.float16, False: t.float32}[fp16]\n            l_cache = self._suff_cache_len()\n            assert self.cache[\'key\'].shape == (n_samples, l_cache, self.n_state)\n            assert self.cache[\'value\'].shape == (n_samples, l_cache, self.n_state)\n            assert self.cache[\'key\'].dtype == dtype, f""Expected {dtype}, got {self.cache[\'key\'].dtype}""\n            assert self.cache[\'value\'].dtype == dtype, f""Expected {dtype}, got {self.cache[\'value\'].dtype}""\n\n    def check_sample(self):\n        t.manual_seed(42)\n        bs, l, d = (4, self.n_ctx, self.n_in)\n        prime = 5\n        x = t.randn(bs, l, d).cuda()\n        xs = t.chunk(x, l, dim=1)\n        assert self.sample_t == 0\n        assert self.cache == {}\n\n        with t.no_grad():\n            enc_l = self.encoder_dims\n            encoder_kv = None\n            if self.attn_func == 6:\n                encoder_kv = t.randn(bs, enc_l, d).cuda()\n\n            # Normal path\n            x_out_normal = self.forward(x, encoder_kv=encoder_kv)\n\n            # Sampling path\n            x_out_sample = t.cat([self.forward(xs[i], encoder_kv=encoder_kv, sample=True) for i in range(l)],dim=1)\n        max_err = t.max(t.abs(x_out_sample - x_out_normal))\n        assert max_err < 1e-8, f""Max sampling err is {max_err} {[i for i in range(l) if t.max(t.abs(x_out_sample - x_out_normal)[:,i,:]) > 1e-8]}""\n\n        with t.no_grad():\n            x_out_normal = x_out_normal[:,:prime,:]\n            # Prime sampling path\n            self.del_cache()\n            x_out_sample = self.forward(x[:,:prime,:].contiguous(), encoder_kv=encoder_kv, sample=True)\n            self.check_cache(bs, prime, False)\n\n        max_err = t.max(t.abs(x_out_sample - x_out_normal))\n        assert max_err < 1e-8, f""Max prime sampling err is {max_err} {[i for i in range(prime) if t.max(t.abs(x_out_sample - x_out_normal)[:,i,:]) > 1e-8]}""\n\n    def check_chunks(self, chunk_size):\n        t.manual_seed(42)\n        bs, l, d = (4, self.n_ctx, self.n_in)\n        enc_l = self.encoder_dims\n        assert l % chunk_size == 0\n        n_chunks = l // chunk_size\n        with t.no_grad():\n            encoder_kv = None\n            x = t.randn(bs, l, d).cuda()\n            if self.attn_func == 6:\n                encoder_kv = t.randn(bs, enc_l, d).cuda()\n\n            self.del_cache()\n            y_forw = self.forward(x, encoder_kv=encoder_kv, sample=False)\n            self.del_cache()\n            y_forw_sample = self.forward(x, encoder_kv=encoder_kv, sample=True)\n            max_err = t.max(t.abs(y_forw - y_forw_sample))\n            assert max_err <= 1e-6, f""Max err is {max_err} {[i for i in range(l) if t.max(t.abs(y_forw - y_forw_sample)[:, i, :]) > 1e-6]}""\n\n            self.del_cache()\n            x_chunks = t.chunk(x, n_chunks, dim=1)\n            y_chunks = []\n            total_len = 0\n            for x_chunk in x_chunks:\n                y_chunk = self.forward(x_chunk.contiguous(), encoder_kv=encoder_kv, sample=True)\n                total_len += x_chunk.shape[1]\n                self.check_cache(bs, total_len, False)\n                y_chunks.append(y_chunk)\n            y_forw_in_chunks = t.cat(y_chunks, dim=1)\n\n            max_err = t.max(t.abs(y_forw - y_forw_in_chunks))\n            assert max_err <= 1e-6, f""Max err is {max_err} {[i for i in range(l) if t.max(t.abs(y_forw - y_forw_in_chunks)[:, i, :]) > 1e-6]}""\n\n\nif __name__ == \'__main__\':\n    from jukebox.utils.dist_utils import setup_dist_from_mpi\n    setup_dist_from_mpi(port=29600)\n    n_in = 16\n    n_state = n_in * 2\n    n_ctx = 6144\n    n_head = 4\n    n_depth = 12\n    blocks = 64\n    chunk_size = 8\n    for attn_func in [0, 1, 2, 3, 6, 7]:\n        encoder_dims = {0: 0, 1: 0, 2: 0, 3: 0, 6: 64, 7: 0}[attn_func]\n        prime_len = {0: 0, 1: 0, 2: 0, 3: 0, 6: 0, 7: 384}[attn_func]\n        attn = FactoredAttention(n_in, n_ctx + prime_len, n_state, n_head, mask=True,\n                                 attn_func=attn_func, blocks=blocks,\n                                 encoder_dims=encoder_dims, prime_len=prime_len)\n        attn.training = False\n        attn.check_sample()\n        attn.check_chunks(chunk_size)\n        print(f""Checked attn_func: {attn_func}"")\n'"
jukebox/transformer/ops.py,3,"b'import math\nimport numpy as np\nimport torch as t\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Import FusedLayerNorm if we have apex, otherwise use regular LayerNorm\ntry:\n    from apex.normalization import FusedLayerNorm\n    print(""Using apex FusedLayerNorm"")\nexcept ImportError:\n    from torch.nn import LayerNorm as FusedLayerNorm\n\nclass LayerNorm(FusedLayerNorm):\n    def __init__(self, normalized_shape, eps=1e-5, elementwise_affine=True):\n        super().__init__(normalized_shape, eps=eps, elementwise_affine=elementwise_affine)\n        self.width = np.prod(normalized_shape)\n        self.max_numel = 65535*self.width\n\n    def forward(self, input):\n        if input.numel() > self.max_numel:\n            return F.layer_norm(input.float(), self.normalized_shape, self.weight, self.bias, self.eps).type_as(input)\n        else:\n            return super(LayerNorm, self).forward(input.float()).type_as(input)\n\ndef gelu(x):\n    return 0.5 * x * (1 + t.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * t.pow(x, 3))))\n\n\ndef swish(x):\n    return x * t.sigmoid(x)\n\n@t.jit.script\ndef quick_gelu(x):\n    return x * t.sigmoid(1.702 * x)\n\n@t.jit.script\ndef quick_gelu_bwd(x, grad_output):\n    sig = t.sigmoid(1.702 * x)\n    return grad_output * sig * (1.702 * x * (1 - sig) + 1.)\n\nclass QuickGelu(t.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        return quick_gelu(x)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return quick_gelu_bwd(ctx.saved_tensors[0], grad_output)\n\ndef memory_efficient_quick_gelu(x):\n    return QuickGelu.apply(x)\n\nACT_FNS = {\n    \'relu\': t.nn.functional.relu,\n    \'swish\': swish,\n    \'gelu\': gelu,\n    \'quick_gelu\': memory_efficient_quick_gelu #quick_gelu\n}\n\ndef _move_to_gpu_and_convert_conv_weights_to_fp16(l):\n    l.cuda()\n    if isinstance(l, Conv1D):\n        l.w.data = l.w.data.half()\n\ndef _convert_conv_weights_to_fp32(l):\n    if isinstance(l, Conv1D):\n        l.w.data = l.w.data.float()\n\ndef _convert_conv_weights_to_fp16(l):\n    if isinstance(l, Conv1D):\n        l.w.data = l.w.data.half()\n\ndef _convert_embedding_weights_to_fp16(l):\n    if isinstance(l, t.nn.Embedding):\n        l.weight.data = l.weight.data.half()\n\ndef _convert_embedding_weights_to_fp32(l):\n    if isinstance(l, t.nn.Embedding):\n        l.weight.data = l.weight.data.float()\n\nclass Conv1D(nn.Module):\n    def __init__(self, n_in, n_out, zero_out=False, init_scale=1.0):\n        super(Conv1D, self).__init__()\n        self.n_in = n_in\n        self.n_out = n_out\n        if zero_out:\n            w = t.zeros(n_in, n_out)\n        else:\n            w = t.empty(n_in, n_out)\n            nn.init.normal_(w, std=0.02 * init_scale)\n        b = t.zeros(n_out)\n        self.w = nn.Parameter(w)\n        self.b = nn.Parameter(b)\n\n    def forward(self, x):\n        size_out = (*x.size()[:-1], self.n_out)\n        x = t.addmm(self.b.type_as(x), x.view(-1, x.size(-1)), self.w.type_as(x)) # If x if float then float else half\n        x = x.view(*size_out)\n        return x\n\n# For large contexts, mask\'s can take up memory, so you can make a single saved mask for all layers\nclass Mask(nn.Module):\n    def __init__(self, n_ctx):\n        super().__init__()\n        self.register_buffer(\'b\', t.tril(t.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n\n    def forward(self, w):\n        w = w * self.b + -1e9 * (1 - self.b)  # For fp16 do w = w.float().masked_fill(self.b, float(\'-inf\')\n        return w\n\ndef filter_logits(logits, top_k=0, top_p=0.0, filter_value=-float(\'Inf\')):\n    """""" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n        Args:\n            logits: logits distribution shape (vocabulary size)\n            top_k >0: keep only top k tokens with highest probability (top-k filtering).\n            top_p >0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n    """"""\n    #assert logits.dim() == 2  # batch size 1 for now - could be updated for more but the code would be less clear\n    logits = logits.clone()\n    top_k = min(top_k, logits.size(-1))  # Safety check\n    assert (top_k == 0) or (top_p == 0.0)\n    if top_k > 0:\n        # Remove all tokens with a probability less than the last token of the top-k\n        indices_to_remove = logits < t.topk(logits, top_k, dim=-1)[0][..., -1:]\n        logits[indices_to_remove] = filter_value\n\n    if top_p > 0.0:\n        sorted_logits, sorted_indices = t.sort(logits, descending=True, dim=-1)\n        cumulative_probs = t.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n\n        # Remove tokens with cumulative probability above the threshold\n        sorted_indices_to_remove = cumulative_probs > top_p\n        # Shift the indices to the right to keep also the first token above the threshold\n        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n        sorted_indices_to_remove[..., 0] = 0\n\n        #indices_to_remove = sorted_indices[sorted_indices_to_remove]\n        indices_to_remove = t.zeros_like(logits, dtype=t.uint8).scatter_(dim=-1, index=sorted_indices, src=sorted_indices_to_remove)\n        logits[indices_to_remove] = filter_value\n    return logits\n'"
jukebox/transformer/transformer.py,1,"b'import functools\nimport numpy as np\nimport torch as t\nimport torch.nn as nn\nimport jukebox.utils.dist_adapter as dist\n\nfrom jukebox.transformer.ops import Conv1D, ACT_FNS, LayerNorm\nfrom jukebox.transformer.factored_attention import FactoredAttention\nfrom jukebox.utils.checkpoint import checkpoint\n\ndef _convert_mlp_traced(l):\n    if isinstance(l, ResAttnBlock):\n        l.mlp = t.jit.trace(l.mlp, t.randn(1, 1, l.n_in).cuda())\n\ndef _convert_mlp_traced_fp16(l):\n    if isinstance(l, ResAttnBlock):\n        l.mlp = t.jit.trace(l.mlp, t.randn(1, 1, l.n_in).cuda().half())\n\nclass MLP(nn.Module):\n    def __init__(self, n_in, n_state, resid_dropout=0.0, afn=\'quick_gelu\', zero_out=False, init_scale=1.0):\n        super().__init__()\n        self.c_fc = Conv1D(n_in, n_state, init_scale=init_scale)\n        self.c_proj = Conv1D(n_state, n_in, zero_out, init_scale=init_scale)\n        self.act = ACT_FNS[afn]\n        self.resid_dropout = nn.Dropout(resid_dropout) if resid_dropout > 0.0 else lambda x: x\n\n    def forward(self, x):\n        m = self.act(self.c_fc(x))\n        m = self.c_proj(m)\n        return self.resid_dropout(m)\n\nclass ResAttnBlock(nn.Module):\n    def __init__(self, n_in, n_ctx, n_head,\n                 attn_dropout=0.0, resid_dropout=0.0,\n                 afn=\'quick_gelu\', scale=True, mask=False,\n                 zero_out=False, init_scale=1.0, res_scale=1.0,\n                 m_attn = 0.25, m_mlp = 1.,\n                 checkpoint_attn = 0, checkpoint_mlp = 0,\n                 attn_func=0, blocks=None, spread=None,\n                 encoder_dims=None, prime_len=None):\n        super().__init__()\n        self.attn = FactoredAttention(n_in=n_in, n_ctx=n_ctx, n_state=int(m_attn * n_in), n_head=n_head,\n                                      attn_dropout=attn_dropout, resid_dropout=resid_dropout,\n                                      scale=scale, mask=mask,\n                                      zero_out=zero_out, init_scale=init_scale,\n                                      checkpoint_attn=checkpoint_attn,\n                                      attn_func=attn_func, blocks=blocks, spread=spread,\n                                      encoder_dims=encoder_dims, prime_len=prime_len)\n        self.ln_0 = LayerNorm(n_in)\n        self.mlp = MLP(n_in=n_in, n_state=int(m_mlp * n_in),\n                       resid_dropout=resid_dropout,\n                       afn=afn,\n                       zero_out=zero_out, init_scale=init_scale)\n        self.ln_1 = LayerNorm(n_in)\n        self.res_scale = res_scale\n\n        self.checkpoint_attn = checkpoint_attn\n        self.checkpoint_mlp = checkpoint_mlp\n        self.n_in = n_in\n        self.attn_func = attn_func\n\n    def forward(self, x, encoder_kv, sample=False):\n        if sample:\n            a = self.attn(self.ln_0(x), encoder_kv, sample)\n            m = self.mlp(self.ln_1(x + a))\n        else:\n            if self.attn_func == 6:\n                assert encoder_kv is not None\n                a = checkpoint(lambda _x,_enc_kv,_s=sample: self.attn(self.ln_0(_x),_enc_kv,_s),\n                               (x,encoder_kv),\n                               (*self.attn.parameters(), *self.ln_0.parameters()),\n                               self.checkpoint_attn == 3)  # 2 recomputes after the projections, and 1 recomputes after head splitting.\n            else:\n                assert encoder_kv is None\n                a = checkpoint(lambda _x,_enc_kv=None,_s=sample: self.attn(self.ln_0(_x),_enc_kv,_s),\n                               (x,),\n                               (*self.attn.parameters(), *self.ln_0.parameters()),\n                               self.checkpoint_attn == 3)  # 2 recomputes after the projections, and 1 recomputes after head splitting.\n            m = checkpoint(lambda _x: self.mlp(self.ln_1(_x)), (x + a,),\n                           (*self.mlp.parameters(), *self.ln_1.parameters()),\n                           self.checkpoint_mlp == 1)\n        if self.res_scale == 1.0:\n            h = x + a + m\n        else:\n            h = x + self.res_scale * (a + m)\n        return h\n\nclass Transformer(nn.Module):\n    def __init__(self, n_in, n_ctx, n_head, n_depth,\n                 attn_dropout=0.0, resid_dropout=0.0,\n                 afn=\'quick_gelu\', scale=True, mask=False,\n                 zero_out=False, init_scale=1.0, res_scale=False,\n                 m_attn=0.25, m_mlp=1.,\n                 checkpoint_attn=0, checkpoint_mlp=0, checkpoint_res=0,\n                 attn_order=0, blocks=None, spread=None,\n                 encoder_dims=None, prime_len=None):\n        super().__init__()\n        self.n_in = n_in\n        self.n_ctx = n_ctx\n        self.encoder_dims = encoder_dims\n        self.blocks = blocks\n        if blocks is not None:\n            assert n_ctx % blocks == 0\n            self.block_ctx = n_ctx // blocks\n        self.prime_len = prime_len\n        self.n_head = n_head\n\n        res_scale = 1.0 / n_depth if res_scale else 1.0\n\n        # Orders of attn_func\n        attn_func = {0: lambda d: 0,                    # Complete dense attn\n                     1: lambda d: [1,2][d%2],           # Alternate row and column attn\n                     2: lambda d: [1,2,3][d % 3],       # Alternate row, column and previous row attn\n                     3: lambda d: [1,4][d % 2],         # Alternate row and last column\n                     4: lambda d: [1,5][d % 2],         # Alternate row and last k columns\n                     5: lambda d: [1,4,1,1][d % 4],      # Alternate row, last column, row, row\n                     6: lambda d: [1,2,3,6][d % 4],\n                     7: lambda d: [*[1,2,3]*5,6][d%16],\n                     8: lambda d: [1,2,3,1,2,3,1,2,3,6][d%10], # Used by separated_enc_dec model with lyrics\n                     9: lambda d: [1,2,3,0][d % 4],\n                     10: lambda d: [*[1,2,3,1,2,3,1,2,3],*[1,2,3,1,2,3,1,2,3,6]*7][d%79], # Used by large separated_enc_dec model with lyrics\n                     11: lambda d: [6,6,0][d%3] if d%16 == 15 else [1,2,3][d%3],\n                     12: lambda d: [7,7,0][d%3] if d%16 == 15 else [1,2,3][d%3], # Used by single_enc_dec model with lyrics\n                     }[attn_order]\n\n        attn_cycle = {0:1, 1:2, 2:3, 3:2, 4:2, 5:4, 6:4, 7:16, 8:10, 9:4, 10:79, 11:16, 12:16}[attn_order]\n        #assert n_depth % attn_cycle == 0, f\'Depth {n_depth} not a multiple of cycle {attn_cycle} for attn_order {attn_order}\'\n\n        attn_block = lambda d: ResAttnBlock(n_in=n_in, n_ctx=n_ctx, n_head=n_head,\n                                  attn_dropout=attn_dropout, resid_dropout=resid_dropout,\n                                  afn=afn, scale=scale, mask=mask,\n                                  zero_out=zero_out if attn_func(d) !=6 else True,\n                                  init_scale=init_scale, res_scale=res_scale,\n                                  m_attn=m_attn, m_mlp=m_mlp,\n                                  checkpoint_attn=checkpoint_attn, checkpoint_mlp=checkpoint_mlp,\n                                  attn_func=attn_func(d), blocks=blocks, spread=spread,\n                                  encoder_dims=encoder_dims, prime_len=prime_len)\n\n        self.checkpoint_res = checkpoint_res\n        self._attn_mods = nn.ModuleList()\n        for d in range(n_depth):\n            self._attn_mods.append(attn_block(d))\n        self.ws = []\n\n\n    def set_record_attn(self, record_attn):\n        """"""\n        Arguments:\n            record_attn (bool or set): Makes forward prop dump self-attention\n                softmaxes to self.ws. Either a set of layer indices indicating\n                which layers to store, or a boolean value indicating whether to\n                dump all.\n        """"""\n        def _should_record_attn(layer_idx):\n            if isinstance(record_attn, bool):\n                return record_attn\n            return layer_idx in record_attn\n        for i, l in enumerate(self._attn_mods):\n            l.attn.record_attn = _should_record_attn(i)\n        if record_attn:\n            assert self.ws == []\n            for l in self._attn_mods:\n                assert l.attn.w == None\n        else:\n            self.ws = []\n            for l in self._attn_mods:\n                l.attn.w = None\n\n    def forward(self, x, encoder_kv=None, sample=False, fp16=False, fp16_out=False):\n        if fp16:\n            x = x.half()\n\n        # Blocks\n        for i,l in enumerate(self._attn_mods):\n            if self.checkpoint_res == 1 and not sample:\n                if l.attn_func == 6:\n                    assert encoder_kv is not None\n                    f = functools.partial(l, sample=sample)\n                    x = checkpoint(f, (x, encoder_kv), l.parameters(), True)\n                else:\n                    f = functools.partial(l, encoder_kv=None, sample=sample)\n                    x = checkpoint(f, (x,), l.parameters(), True)\n            else:\n                if l.attn_func == 6:\n                    x = l(x, encoder_kv=encoder_kv, sample=sample)\n                else:\n                    x = l(x, encoder_kv=None, sample=sample)\n            if l.attn.record_attn:\n                self.ws.append(l.attn.w)\n        if not fp16_out:\n            x = x.float()\n        return x\n\n    def check_cache(self, n_samples, sample_t, fp16):\n        for l in self._attn_mods:\n            l.attn.check_cache(n_samples, sample_t, fp16)\n\n    def del_cache(self):\n        for l in self._attn_mods:\n            l.attn.del_cache()\n\n    def check_sample(self):\n        bs, l, s, d = (4, self.n_ctx, self.encoder_dims, self.n_in)\n        prime = 5\n        with t.no_grad():\n            encoder_kv = t.randn(bs, s, d).cuda()\n            x = t.randn(bs, l, d).cuda()\n            y_forw = self.forward(x, encoder_kv=encoder_kv, sample=True)\n\n            self.del_cache()\n            x_chunks = t.chunk(x, 4, dim=1)\n            y_chunks = []\n            n = 0\n            for x_chunk in x_chunks:\n                self.check_cache(bs, n, False)\n                y_chunk = self.forward(x_chunk, encoder_kv=encoder_kv, sample=True)\n                y_chunks.append(y_chunk)\n                n += x_chunk.shape[1]\n            self.check_cache(bs, n, False)\n            y_forw_in_chunks = t.cat(y_chunks, dim=1)\n\n            max_err = t.max(t.abs(y_forw - y_forw_in_chunks))\n            assert max_err <= 1e-6, f""Max err is {max_err} {[i for i in range(l) if t.max(t.abs(y_forw - y_forw_in_chunks)[:, i, :]) > 1e-6]}""\n\n\nif __name__ == \'__main__\':\n    from jukebox.utils.dist_utils import setup_dist_from_mpi\n    setup_dist_from_mpi(port=29600)\n    n_in = 16\n    n_ctx = 192\n    n_head = 4\n    n_depth = 12\n    blocks = 16\n    for attn_order in [0,2,6]:\n        encoder_dims = {0: 0, 2: 0, 6: 64}[attn_order]\n        prior = Transformer(n_in, n_ctx, n_head, n_depth, mask=True, attn_order=attn_order, encoder_dims=encoder_dims, blocks=blocks).cuda()\n        prior.training = False\n        prior.check_sample()\n        print(f""Checked attn_order: {attn_order}"")\n'"
jukebox/utils/__init__.py,0,b''
jukebox/utils/audio_utils.py,0,"b""import numpy as np\nimport torch as t\nimport jukebox.utils.dist_adapter as dist\nimport soundfile\nimport librosa\nfrom jukebox.utils.dist_utils import print_once\n\nclass DefaultSTFTValues:\n    def __init__(self, hps):\n        self.sr = hps.sr\n        self.n_fft = 2048\n        self.hop_length = 256\n        self.window_size = 6 * self.hop_length\n\nclass STFTValues:\n    def __init__(self, hps, n_fft, hop_length, window_size):\n        self.sr = hps.sr\n        self.n_fft = n_fft\n        self.hop_length = hop_length\n        self.window_size = window_size\n\ndef calculate_bandwidth(dataset, hps, duration=600):\n    hps = DefaultSTFTValues(hps)\n    n_samples = int(dataset.sr * duration)\n    l1, total, total_sq, n_seen, idx = 0.0, 0.0, 0.0, 0.0, dist.get_rank()\n    spec_norm_total, spec_nelem = 0.0, 0.0\n    while n_seen < n_samples:\n        x = dataset[idx]\n        if isinstance(x, (tuple, list)):\n            x, y = x\n        samples = x.astype(np.float64)\n        stft = librosa.core.stft(np.mean(samples, axis=1), hps.n_fft, hop_length=hps.hop_length, win_length=hps.window_size)\n        spec = np.absolute(stft)\n        spec_norm_total += np.linalg.norm(spec)\n        spec_nelem += 1\n        n_seen += int(np.prod(samples.shape))\n        l1 += np.sum(np.abs(samples))\n        total += np.sum(samples)\n        total_sq += np.sum(samples ** 2)\n        idx += max(16, dist.get_world_size())\n\n    if dist.is_available():\n        from jukebox.utils.dist_utils import allreduce\n        n_seen = allreduce(n_seen)\n        total = allreduce(total)\n        total_sq = allreduce(total_sq)\n        l1 = allreduce(l1)\n        spec_nelem = allreduce(spec_nelem)\n        spec_norm_total = allreduce(spec_norm_total)\n\n    mean = total / n_seen\n    bandwidth = dict(l2 = total_sq / n_seen - mean ** 2,\n                     l1 = l1 / n_seen,\n                     spec = spec_norm_total / spec_nelem)\n    print_once(bandwidth)\n    return bandwidth\n\ndef audio_preprocess(x, hps):\n    # Extra layer in case we want to experiment with different preprocessing\n    # For two channel, blend randomly into mono (standard is .5 left, .5 right)\n\n    # x: NTC\n    x = x.float()\n    if x.shape[-1]==2:\n        if hps.aug_blend:\n            mix=t.rand((x.shape[0],1), device=x.device) #np.random.rand()\n        else:\n            mix = 0.5\n        x=(mix*x[:,:,0]+(1-mix)*x[:,:,1])\n    elif x.shape[-1]==1:\n        x=x[:,:,0]\n    else:\n        assert False, f'Expected channels {hps.channels}. Got unknown {x.shape[-1]} channels'\n\n    # x: NT -> NTC\n    x = x.unsqueeze(2)\n    return x\n\ndef audio_postprocess(x, hps):\n    return x\n\ndef stft(sig, hps):\n    return t.stft(sig, hps.n_fft, hps.hop_length, win_length=hps.window_size, window=t.hann_window(hps.window_size, device=sig.device))\n\ndef spec(x, hps):\n    return t.norm(stft(x, hps), p=2, dim=-1)\n\ndef norm(x):\n    return (x.view(x.shape[0], -1) ** 2).sum(dim=-1).sqrt()\n\ndef squeeze(x):\n    if len(x.shape) == 3:\n        assert x.shape[-1] in [1,2]\n        x = t.mean(x, -1)\n    if len(x.shape) != 2:\n        raise ValueError(f'Unknown input shape {x.shape}')\n    return x\n\ndef spectral_loss(x_in, x_out, hps):\n    hps = DefaultSTFTValues(hps)\n    spec_in = spec(squeeze(x_in.float()), hps)\n    spec_out = spec(squeeze(x_out.float()), hps)\n    return norm(spec_in - spec_out)\n\ndef multispectral_loss(x_in, x_out, hps):\n    losses = []\n    assert len(hps.multispec_loss_n_fft) == len(hps.multispec_loss_hop_length) == len(hps.multispec_loss_window_size)\n    args = [hps.multispec_loss_n_fft,\n            hps.multispec_loss_hop_length,\n            hps.multispec_loss_window_size]\n    for n_fft, hop_length, window_size in zip(*args):\n        hps = STFTValues(hps, n_fft, hop_length, window_size)\n        spec_in = spec(squeeze(x_in.float()), hps)\n        spec_out = spec(squeeze(x_out.float()), hps)\n        losses.append(norm(spec_in - spec_out))\n    return sum(losses) / len(losses)\n\ndef spectral_convergence(x_in, x_out, hps, epsilon=2e-3):\n    hps = DefaultSTFTValues(hps)\n    spec_in = spec(squeeze(x_in.float()), hps)\n    spec_out = spec(squeeze(x_out.float()), hps)\n\n    gt_norm = norm(spec_in)\n    residual_norm = norm(spec_in - spec_out)\n    mask = (gt_norm > epsilon).float()\n    return (residual_norm * mask) / t.clamp(gt_norm, min=epsilon)\n\ndef log_magnitude_loss(x_in, x_out, hps, epsilon=1e-4):\n    hps = DefaultSTFTValues(hps)\n    spec_in = t.log(spec(squeeze(x_in.float()), hps) + epsilon)\n    spec_out = t.log(spec(squeeze(x_out.float()), hps) + epsilon)\n    return t.mean(t.abs(spec_in - spec_out))\n\ndef load_audio(file, sr, offset, duration, mono=False):\n    # Librosa loads more filetypes than soundfile\n    x, _ = librosa.load(file, sr=sr, mono=mono, offset=offset/sr, duration=duration/sr)\n    if len(x.shape) == 1:\n        x = x.reshape((1, -1))\n    return x    \n\n\ndef save_wav(fname, aud, sr):\n    # clip before saving?\n    aud = t.clamp(aud, -1, 1).cpu().numpy()\n    for i in list(range(aud.shape[0])):\n        soundfile.write(f'{fname}/item_{i}.wav', aud[i], samplerate=sr, format='wav')\n\n\n"""
jukebox/utils/checkpoint.py,0,"b'# Simple gradient checkpointing. Works with distributed data parallel\nimport torch as t\n\ndef checkpoint(func, inputs, params, flag):\n    if flag:\n        args = inputs + tuple(params)\n        return CheckpointFunction.apply(func, len(inputs), *args)\n    else:\n        return func(*inputs)\n\nclass CheckpointFunction(t.autograd.Function):\n    @staticmethod\n    def forward(ctx, run_function, length, *args):\n        ctx.run_function = run_function\n        ctx.input_tensors = list(args[:length])\n        ctx.input_params = list(args[length:])\n        with t.no_grad():\n            output_tensors = ctx.run_function(*ctx.input_tensors)\n        return output_tensors\n\n    @staticmethod\n    def backward(ctx, *output_grads):\n        for i in range(len(ctx.input_tensors)):\n            temp = ctx.input_tensors[i]\n            ctx.input_tensors[i] = temp.detach()\n            ctx.input_tensors[i].requires_grad = temp.requires_grad\n        with t.enable_grad():\n            output_tensors = ctx.run_function(*ctx.input_tensors)\n        input_grads = t.autograd.grad(output_tensors, ctx.input_tensors + ctx.input_params, output_grads, allow_unused=True)\n        del ctx.input_tensors\n        del output_tensors\n        return (None, None) + input_grads\n'"
jukebox/utils/dist_adapter.py,1,"b'import torch.distributed as dist\nfrom enum import Enum\n\nclass ReduceOp(Enum):\n    SUM = 0,\n    PRODUCT = 1,\n    MIN = 2,\n    MAX = 3\n\n    def ToDistOp(self):\n        return {\n            self.SUM: dist.ReduceOp.SUM,\n            self.PRODUCT: dist.ReduceOp.PRODUCT,\n            self.MIN: dist.ReduceOp.MIN,\n            self.MAX: dist.ReduceOp.MAX\n        }[self]\n\ndef is_available():\n    return dist.is_available()\n\ndef get_rank():\n    if is_available():\n        return _get_rank()\n    else:\n        return 0\n\ndef get_world_size():\n    if is_available():\n        return _get_world_size()\n    else:\n        return 1\n\ndef barrier():\n    if is_available():\n        return _barrier()\n    #else: do nothing\n\ndef all_gather(tensor_list, tensor):\n    if is_available():\n        return _all_gather(tensor_list, tensor)\n    else:\n        tensor_list[0] = tensor\n\ndef all_reduce(tensor, op=ReduceOp.SUM):\n    if is_available():\n        return _all_reduce(tensor, op)\n    #else: do nothing\n\ndef reduce(tensor, dst, op=ReduceOp.SUM):\n    if is_available():\n        return _reduce(tensor, dst, op)\n    #else: do nothing\n\ndef broadcast(tensor, src):\n    if is_available():\n        return _broadcast(tensor, src)\n    #else: do nothing\n\ndef init_process_group(backend, init_method):\n    if is_available():\n        return _init_process_group(backend, init_method)\n    #else: do nothing\n\ndef _get_rank():\n    return dist.get_rank()\n\ndef _barrier():\n    return dist.barrier()\n\ndef _get_world_size():\n    return dist.get_world_size()\n\ndef _all_gather(tensor_list, tensor):\n    return dist.all_gather(tensor_list, tensor)\n\ndef _all_reduce(tensor, op):\n    return dist.all_reduce(tensor, op.ToDistOp())\n\ndef _reduce(tensor, dst, op):\n    return dist.reduce(tensor, dst, op.ToDistOp())\n\ndef _broadcast(tensor, src):\n    return dist.broadcast(tensor, src)\n\ndef _init_process_group(backend, init_method):\n    return dist.init_process_group(backend, init_method)'"
jukebox/utils/dist_utils.py,14,"b'import os\nfrom time import sleep\nimport torch\nimport jukebox.utils.dist_adapter as dist\n\ndef print_once(msg):\n    if (not dist.is_available()) or dist.get_rank()==0:\n        print(msg)\n\ndef print_all(msg):\n    if (not dist.is_available()):\n        print(msg)\n    elif dist.get_rank()%8==0:\n        print(f\'{dist.get_rank()//8}: {msg}\')\n\ndef allgather(x):\n    xs = [torch.empty_like(x) for _ in range(dist.get_world_size())]\n    dist.all_gather(xs, x)\n    xs = torch.cat(xs, dim=0)\n    return xs\n\ndef allreduce(x, op=dist.ReduceOp.SUM):\n    x = torch.tensor(x).float().cuda()\n    dist.all_reduce(x, op=op)\n    return x.item()\n\ndef allgather_lists(xs):\n    bs = len(xs)\n    total_bs = dist.get_world_size()*len(xs)\n    lengths = torch.tensor([len(x) for x in xs], dtype=t.long, device=\'cuda\')\n    lengths = allgather(lengths)\n    assert lengths.shape == (total_bs,)\n    max_length = torch.max(lengths).item()\n\n    xs = torch.tensor([[*x, *[0]*(max_length - len(x))] for x in xs], device=\'cuda\')\n    assert xs.shape == (bs, max_length), f\'Expected {(bs, max_length)}, got {xs.shape}\'\n    xs = allgather(xs)\n    assert xs.shape == (total_bs,max_length), f\'Expected {(total_bs, max_length)}, got {xs.shape}\'\n\n    return [xs[i][:lengths[i]].cpu().numpy().tolist() for i in range(total_bs)]\n\ndef setup_dist_from_mpi(\n    master_addr=""127.0.0.1"", backend=""nccl"", port=29500, n_attempts=5, verbose=False\n):\n    if dist.is_available():\n        return _setup_dist_from_mpi(master_addr, backend, port, n_attempts, verbose)\n    else:\n        use_cuda = torch.cuda.is_available()\n        print(f\'Using cuda {use_cuda}\')\n\n        mpi_rank = 0\n        local_rank = 0\n\n        device = torch.device(""cuda"", local_rank) if use_cuda else torch.device(""cpu"")\n        torch.cuda.set_device(local_rank)\n\n        return mpi_rank, local_rank, device\n\ndef _setup_dist_from_mpi(master_addr, backend, port, n_attempts, verbose):\n    from mpi4py import MPI  # This must be imported in order to get e   rrors from all ranks to show up\n\n    mpi_rank = MPI.COMM_WORLD.Get_rank()\n    mpi_size = MPI.COMM_WORLD.Get_size()\n\n\n    os.environ[""RANK""] = str(mpi_rank)\n    os.environ[""WORLD_SIZE""] = str(mpi_size)\n    os.environ[""MASTER_ADDR""] = master_addr\n    os.environ[""MASTER_PORT""] = str(port)\n    os.environ[""NCCL_LL_THRESHOLD""] = ""0""\n    os.environ[""NCCL_NSOCKS_PERTHREAD""] = ""2""\n    os.environ[""NCCL_SOCKET_NTHREADS""] = ""8""\n\n    # Pin this rank to a specific GPU on the node\n    local_rank = mpi_rank % 8\n    if torch.cuda.is_available():\n        torch.cuda.set_device(local_rank)\n\n    if verbose:\n        print(f""Connecting to master_addr: {master_addr}"")\n\n    # There is a race condition when initializing NCCL with a large number of ranks (e.g 500 ranks)\n    # We guard against the failure and then retry\n    for attempt_idx in range(n_attempts):\n        try:\n            dist.init_process_group(backend=backend, init_method=f""env://"")\n            assert dist.get_rank() == mpi_rank\n\n            use_cuda = torch.cuda.is_available()\n            print(f\'Using cuda {use_cuda}\')\n            local_rank = mpi_rank % 8\n            device = torch.device(""cuda"", local_rank) if use_cuda else torch.device(""cpu"")\n            torch.cuda.set_device(local_rank)\n\n            return mpi_rank, local_rank, device\n        except RuntimeError as e:\n            print(f""Caught error during NCCL init (attempt {attempt_idx} of {n_attempts}): {e}"")\n            sleep(1 + (0.01 * mpi_rank))  # Sleep to avoid thundering herd\n            pass\n\n    raise RuntimeError(""Failed to initialize NCCL"")\n'"
jukebox/utils/ema.py,8,"b""import torch\nfrom torch._utils import _flatten_dense_tensors\nimport numpy as np\n\n# EMA always in float, as accumulation needs lots of bits\nclass EMA:\n    def __init__(self, params, mu=0.999):\n        self.mu = mu\n        self.state = [(p, self.get_model_state(p)) for p in params if p.requires_grad]\n\n    def get_model_state(self, p):\n        return p.data.float().detach().clone()\n\n    def step(self):\n        for p, state in self.state:\n            state.mul_(self.mu).add_(1 - self.mu, p.data.float())\n\n    def swap(self):\n        # swap ema and model params\n        for p, state in self.state:\n            other_state = self.get_model_state(p)\n            p.data.copy_(state.type_as(p.data))\n            state.copy_(other_state)\n\n\nclass CPUEMA:\n    def __init__(self, params, mu=0.999, freq=1):\n        self.mu = mu**freq\n        self.state = [(p, self.get_model_state(p)) for p in params if p.requires_grad]\n        self.freq = freq\n        self.steps = 0\n\n    def get_model_state(self, p):\n        with torch.no_grad():\n            state = p.data.float().detach().cpu().numpy()\n        return state\n\n    def step(self):\n        with torch.no_grad():\n            self.steps += 1\n            if self.steps % self.freq == 0:\n                for i in range(len(self.state)):\n                    p, state = self.state[i]\n                    state = torch.from_numpy(state).cuda()\n                    state.mul_(self.mu).add_(1 - self.mu, p.data.float())\n                    self.state[i] = (p, state.cpu().numpy())\n\n    def swap(self):\n        with torch.no_grad():\n            # swap ema and model params\n            for p, state in self.state:\n                other_state = self.get_model_state(p)\n                p.data.copy_(torch.from_numpy(state).type_as(p.data))\n                np.copyto(state, other_state)\n\nclass FusedEMA:\n    def __init__(self, params, mu=0.999):\n        self.mu = mu\n        params = list(params)\n        self.params = {}\n        self.params['fp16'] = [p for p in params if p.requires_grad and p.data.dtype == torch.float16]\n        self.params['fp32'] = [p for p in params if p.requires_grad and p.data.dtype != torch.float16]\n        self.groups = [group for group in self.params.keys() if len(self.params[group]) > 0]\n        self.state = {}\n        for group in self.groups:\n            self.state[group] = self.get_model_state(group)\n\n    def get_model_state(self, group):\n        params = self.params[group]\n        return _flatten_dense_tensors([p.data.float() for p in params])\n        # if self.fp16:\n        #     return _flatten_dense_tensors([p.data.half() for p in self.param_group if p.dtype])\n        # else:\n        #     return _flatten_dense_tensors([p.data for p in self.param_group])\n\n    def step(self):\n        for group in self.groups:\n            self.state[group].mul_(self.mu).add_(1 - self.mu, self.get_model_state(group))\n\n    def swap(self):\n        # swap ema and model params\n        for group in self.groups:\n            other_state = self.get_model_state(group)\n            state = self.state[group]\n            params = self.params[group]\n            offset = 0\n            for p in params:\n                numel = p.data.numel()\n                p.data = state.narrow(0, offset, numel).view_as(p.data).type_as(p.data)\n                offset += numel\n\n            self.state[group] = other_state\n\n\n"""
jukebox/utils/fp16.py,18,"b'# Utils for fp16 training.\nimport importlib\nimport math\nimport numpy as np\nimport torch\nimport jukebox.utils.dist_adapter as dist\nfrom torch.optim import Optimizer\nfrom torch._utils import _flatten_dense_tensors\n\nfrom jukebox.utils.dist_utils import allreduce\n\ndef adam_step(p: torch.Tensor, out_p: torch.Tensor, exp_avg: torch.Tensor, exp_avg_sq: torch.Tensor, grad: torch.Tensor,\n              lr: float, beta1: float, beta2: float, eps: float, scale: float, step: int, eps_mode: int, bias_correction: int, weight_decay: float):\n    assert bias_correction == 1\n    assert eps_mode == 1\n\n    grad = grad.float()\n    grad.div_(scale)\n\n    # Decay the first and second moment running average coefficient\n    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n    denom = exp_avg_sq.sqrt().add_(eps)\n\n    bias_correction1 = 1 - beta1 ** step\n    bias_correction2 = 1 - beta2 ** step\n    step_size = lr * math.sqrt(bias_correction2) / bias_correction1\n\n    p.add_(exp_avg/denom + weight_decay*p.float(), alpha=-step_size)\n\n# Import fused_adam if we have apex, otherwise use regular adam\ntry:\n    fused_adam_cuda = importlib.import_module(""fused_adam_cuda"")\n    fused_adam_step = fused_adam_cuda.adam\n    print(""Using apex fused_adam_cuda"")\nexcept ModuleNotFoundError:\n    fused_adam_step = adam_step\n\ndef backward(loss, params, scalar, fp16, logger):\n    # Perform backward\n    if not fp16:\n        scale = 1.0\n        loss.backward()\n        gn = grad_norm(params, scale)\n        return loss, scale, gn, False, False\n    else:\n        scale = scalar.get_scale()\n        loss = (loss.float())*scale\n        overflow_loss = check_overflow(loss.item())\n        overflow_loss = allreduce(int(overflow_loss), op=dist.ReduceOp.MAX) > 0\n        if not overflow_loss:\n            loss.backward()\n            gn = grad_norm(params, scale)\n            overflow_grad = check_overflow(gn)\n            overflow_grad = allreduce(int(overflow_grad), op=dist.ReduceOp.MAX) > 0\n            scalar.update_scale(overflow_grad)\n        else:\n            gn = 0.0\n            overflow_grad = True\n        loss = (loss.detach().float()) / scale # Should delete computation graph for overflow\n        if logger.rank == 0:\n            if loss > 12.: print(f""\\nWarning. Loss is {loss}"")\n            if overflow_loss: print(f""\\nOverflow in forward. Loss {loss}, lgscale {np.log2(scale)}. Skipping batch completely (no backward, scale update)"")\n            elif overflow_grad: print(f""\\nOverflow in backward. Loss {loss}, grad norm {gn}, lgscale {np.log2(scale)}, new lgscale {np.log2(scalar.get_scale())}"")\n        return loss, scale, gn, overflow_loss, overflow_grad\n\n# Automatic loss scaling\nclass LossScalar(object):\n    def __init__(self,\n                 loss_scale,\n                 init_scale=2. ** 16,\n                 scale_factor=2. ** (1. / 1000),\n                 scale_window=1):\n        if loss_scale == None:\n            # Use dynamic loss scaling\n            self.dynamic = True\n            self.loss_scale = init_scale\n        else:\n            self.dynamic = False\n            self.loss_scale = loss_scale\n        self.max_loss_scale = 2.**24\n        self.scale_factor = scale_factor\n        self.scale_window  = scale_window\n        self.unskipped = 0\n        self.overflow = False\n\n    def get_scale(self):\n        return self.loss_scale\n\n    def update_scale(self, overflow):\n        if overflow and self.dynamic:\n            self.loss_scale /= 2.\n            self.unskipped = 0\n        else:\n            self.unskipped += 1\n\n        if self.unskipped == self.scale_window and self.dynamic:\n            self.loss_scale = min(self.max_loss_scale, self.loss_scale * self.scale_factor)\n            self.unskipped = 0\n\ndef check_overflow(val):\n    return (val == float(\'inf\')) or (val == -float(\'inf\')) or (val != val)\n\ndef grad_norm(params, scale, flat=False):\n    params = list(params)\n    if flat:\n        # Faster but more memory\n        fp16_grads = [p.grad for p in params if p.grad is not None and p.data.dtype == torch.float16]\n        fp16_norm = 0.0 if len(fp16_grads) == 0 else float(_flatten_dense_tensors(fp16_grads).norm(p=2, dtype=torch.float32))\n        fp32_grads = [p.grad for p in params if p.grad is not None and p.data.dtype != torch.float16]\n        fp32_norm = 0.0 if len(fp32_grads) == 0 else float(_flatten_dense_tensors(fp32_grads).norm(p=2))\n        grad_norm = (fp16_norm**2 + fp32_norm**2)**0.5\n    else:\n        # Slightly slower but less memory\n        grad_norm = 0.0\n        for p in params:\n            if p.grad is not None:\n                grad_norm += p.grad.norm(p=2, dtype=torch.float32)**2\n        grad_norm = float(grad_norm**0.5)\n    return grad_norm / scale\n\ndef clipped_grad_scale(grad_norm, max_grad_norm, scale):\n    clip = grad_norm / max_grad_norm\n    if clip > 1:\n        scale = clip * scale\n    return scale\n\nclass FP16FusedAdam(Optimizer):\n    def __init__(\n        self,\n        params,\n        lr=1e-3,\n        bias_correction=True,\n        betas=(0.9, 0.999),\n        eps=1e-8,\n        eps_inside_sqrt=False,\n        weight_decay=0.0,\n        amsgrad=False,\n    ):\n        if amsgrad:\n            raise RuntimeError(""FusedAdam does not support the AMSGrad variant."")\n        defaults = dict(\n            lr=lr, bias_correction=bias_correction, betas=betas, eps=eps, weight_decay=weight_decay\n        )\n        super(FP16FusedAdam, self).__init__(params, defaults)\n        self.eps_mode = 0 if eps_inside_sqrt else 1\n        self.FLOAT16_MAX = 65504.0\n        self.init_state()\n\n    def init_state(self):\n        for group in self.param_groups:\n            for p in group[""params""]:\n                assert p.requires_grad == True\n                state = self.state[p]\n                if len(state) == 0:\n                    state[""step""] = 0\n                    # Exponential moving average of gradient values\n                    state[""exp_avg""] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[""exp_avg_sq""] = torch.zeros_like(p.data)\n                    if p.data.dtype == torch.float16:\n                        state[""scale_exp_avg""] = 1.0\n                        state[""scale_exp_avg_sq""] = 1.0\n\n    def step(self, closure=None, scale=1.0):\n        """"""Performs a single optimization step. Scales gradients down by scale\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n            scale (float, optional): factor to divide gradient tensor values\n                by before applying to weights. (default: 1)\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            bias_correction = 1 if group[""bias_correction""] else 0\n\n            for p in group[""params""]:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n\n                state = self.state[p]\n\n                if p.data.dtype == torch.float16:\n                    exp_avg, exp_avg_sq = (\n                        state[""exp_avg""].float() * state[""scale_exp_avg""],\n                        state[""exp_avg_sq""].float() * state[""scale_exp_avg_sq""],\n                    )\n                else:\n                    exp_avg, exp_avg_sq = state[""exp_avg""], state[""exp_avg_sq""]\n                beta1, beta2 = group[""betas""]\n\n                state[""step""] += 1\n\n                out_p = torch.tensor([], dtype=torch.float)\n                fused_adam_step(\n                    p.data,\n                    out_p,\n                    exp_avg,\n                    exp_avg_sq,\n                    grad,\n                    group[""lr""],\n                    beta1,\n                    beta2,\n                    group[""eps""],\n                    scale,\n                    state[""step""],\n                    self.eps_mode,\n                    bias_correction,\n                    group[""weight_decay""],\n                )\n\n                if p.data.dtype == torch.float16:\n                    state[""scale_exp_avg""] = (\n                        1e-8 + float(torch.norm(exp_avg, float(""inf""))) / self.FLOAT16_MAX\n                    )\n                    state[""scale_exp_avg_sq""] = (\n                        1e-8 + float(torch.norm(exp_avg_sq, float(""inf""))) / self.FLOAT16_MAX\n                    )\n                    state[""exp_avg""] = (exp_avg / state[""scale_exp_avg""]).half()\n                    state[""exp_avg_sq""] = (exp_avg_sq / state[""scale_exp_avg_sq""]).half()\n\n        return loss\n\n\nclass FusedAdam(Optimizer):\n    def __init__(\n        self,\n        params,\n        lr=1e-3,\n        bias_correction=True,\n        betas=(0.9, 0.999),\n        eps=1e-8,\n        eps_inside_sqrt=False,\n        weight_decay=0.0,\n        amsgrad=False,\n    ):\n        if amsgrad:\n            raise RuntimeError(""FusedAdam does not support the AMSGrad variant."")\n        defaults = dict(\n            lr=lr, bias_correction=bias_correction, betas=betas, eps=eps, weight_decay=weight_decay\n        )\n        super(FusedAdam, self).__init__(params, defaults)\n        self.eps_mode = 0 if eps_inside_sqrt else 1\n\n    def step(self, closure=None, scale=1.0):\n        """"""Performs a single optimization step. Scales gradients down by scale\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n            scale (float, optional): factor to divide gradient tensor values\n                by before applying to weights. (default: 1)\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            bias_correction = 1 if group[""bias_correction""] else 0\n\n            for p in group[""params""]:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[""step""] = 0\n                    # Exponential moving average of gradient values\n                    state[""exp_avg""] = torch.zeros_like(p.data).float()\n                    # Exponential moving average of squared gradient values\n                    state[""exp_avg_sq""] = torch.zeros_like(p.data).float()\n\n                exp_avg, exp_avg_sq = state[""exp_avg""], state[""exp_avg_sq""]\n                beta1, beta2 = group[""betas""]\n\n                state[""step""] += 1\n\n                out_p = torch.tensor([], dtype=torch.float)\n                fused_adam_step(\n                    p.data,\n                    out_p,\n                    exp_avg,\n                    exp_avg_sq,\n                    grad,\n                    group[""lr""],\n                    beta1,\n                    beta2,\n                    group[""eps""],\n                    scale,\n                    state[""step""],\n                    self.eps_mode,\n                    bias_correction,\n                    group[""weight_decay""],\n                )\n\n        return loss\n\n'"
jukebox/utils/gcs_utils.py,0,"b'import os\nimport sys\nimport subprocess\nfrom time import time\n\ndef gs_download(gs_path, local_path, async_download=False):\n    args = [\'gsutil\',\n            \'-o\', \'GSUtil:parallel_thread_count=1\',\n            \'-o\', \'GSUtil:sliced_object_download_max_components=8\',\n            \'cp\', gs_path, local_path]\n    if async_download:\n        subprocess.Popen(args)\n    else:\n        subprocess.call(args)\n\n\ndef gs_upload(local_path, gs_path, async_upload=False):\n    # NOTE: Download and upload have differ -o flags.\n    # We also use -n to prevent clobbering checkpoints by mistake\n    assert not local_path.startswith(""gs://"")\n    assert gs_path.startswith(""gs://"")\n    args = [\'gsutil\',\n            \'-o\', \'GSUtil:parallel_composite_upload_threshold=150M\',\n            \'cp\', \'-n\', local_path, gs_path]\n    if async_upload:\n        subprocess.Popen(args)\n    else:\n        subprocess.call(args)\n\ndef download(gs_path, local_path, async_download=False):\n    remote_path = gs_path.replace(""gs://"", ""https://storage.googleapis.com/"")\n    args = [\'wget\', \'-q\', \'-O\', local_path, remote_path]\n    if async_download:\n        subprocess.Popen(args)\n    else:\n        subprocess.call(args)\n\ndef ls(regex):\n    outputs = subprocess.check_output([\'gsutil\', \'ls\', regex]).decode(sys.stdout.encoding)\n    outputs = outputs.split(\'\\n\')\n    outputs = [output for output in outputs if output is not \'\']\n    return outputs'"
jukebox/utils/io.py,2,"b'import numpy as np\nimport av\nimport torch as t\nimport jukebox.utils.dist_adapter as dist\n\ndef get_duration_sec(file, cache=False):\n    try:\n        with open(file + \'.dur\', \'r\') as f:\n            duration = float(f.readline().strip(\'\\n\'))\n        return duration\n    except:\n        container = av.open(file)\n        audio = container.streams.get(audio=0)[0]\n        duration = audio.duration * float(audio.time_base)\n        if cache:\n            with open(file + \'.dur\', \'w\') as f:\n                f.write(str(duration) + \'\\n\')\n        return duration\n\ndef load_audio(file, sr, offset, duration, resample=True, approx=False, time_base=\'samples\', check_duration=True):\n    if time_base == \'sec\':\n        offset = offset * sr\n        duration = duration * sr\n    # Loads at target sr, stereo channels, seeks from offset, and stops after duration\n    container = av.open(file)\n    audio = container.streams.get(audio=0)[0] # Only first audio stream\n    audio_duration = audio.duration * float(audio.time_base)\n    if approx:\n        if offset + duration > audio_duration*sr:\n            # Move back one window. Cap at audio_duration\n            offset = np.min(audio_duration*sr - duration, offset - duration)\n    else:\n        if check_duration:\n            assert offset + duration <= audio_duration*sr, f\'End {offset + duration} beyond duration {audio_duration*sr}\'\n    if resample:\n        resampler = av.AudioResampler(format=\'fltp\',layout=\'stereo\', rate=sr)\n    else:\n        assert sr == audio.sample_rate\n    offset = int(offset / sr / float(audio.time_base)) #int(offset / float(audio.time_base)) # Use units of time_base for seeking\n    duration = int(duration) #duration = int(duration * sr) # Use units of time_out ie 1/sr for returning\n    sig = np.zeros((2, duration), dtype=np.float32)\n    container.seek(offset, stream=audio)\n    total_read = 0\n    for frame in container.decode(audio=0): # Only first audio stream\n        if resample:\n            frame.pts = None\n            frame = resampler.resample(frame)\n        frame = frame.to_ndarray(format=\'fltp\') # Convert to floats and not int16\n        read = frame.shape[-1]\n        if total_read + read > duration:\n            read = duration - total_read\n        sig[:, total_read:total_read + read] = frame[:, :read]\n        total_read += read\n        if total_read == duration:\n            break\n    assert total_read <= duration, f\'Expected {duration} frames, got {total_read}\'\n    return sig, sr\n\ndef test_simple_loader():\n    import librosa\n    from tqdm import tqdm\n\n    collate_fn = lambda batch: t.stack([t.from_numpy(b) for b in batch], dim=0)\n\n    def get_batch(file, loader):\n        y1, sr = loader(file, sr=44100, offset=0.0, duration=6.0, time_base=\'sec\')\n        y2, sr = loader(file, sr=44100, offset=20.0, duration=6.0, time_base=\'sec\')\n        return [y1, y2]\n\n    def load(file, loader):\n        batch = get_batch(file, loader)  # np\n        x = collate_fn(batch)  # torch cpu\n        x = x.to(\'cuda\', non_blocking=True)  # torch gpu\n        return x\n\n    files = librosa.util.find_files(\'/root/data/\', [\'mp3\', \'m4a\', \'opus\'])\n    print(files[:10])\n    loader = load_audio\n    print(""Loader"", loader.__name__)\n    x = t.randn(2, 2).cuda()\n    x = load(files[0], loader)\n    for i,file in enumerate(tqdm(files)):\n        x = load(file, loader)\n        if i == 100:\n            break\n\ndef test_dataset_loader():\n    from tqdm import tqdm\n    from torch.utils.data import DataLoader\n    from torch.utils.data.distributed import DistributedSampler\n    from jukebox.utils.audio_utils import audio_preprocess, audio_postprocess\n    from jukebox.hparams import setup_hparams\n    from jukebox.data.files_dataset import FilesAudioDataset\n    hps = setup_hparams(""teeny"", {})\n    hps.sr = 22050  # 44100\n    hps.hop_length = 512\n    hps.labels = False\n    hps.channels = 2\n    hps.aug_shift = False\n    hps.bs = 2\n    hps.nworkers = 2 # Getting 20 it/s with 2 workers, 10 it/s with 1 worker\n    print(hps)\n    dataset = hps.dataset\n    root = hps.root\n    from tensorboardX import SummaryWriter\n    sr = {22050: \'22k\', 44100: \'44k\', 48000: \'48k\'}[hps.sr]\n    writer = SummaryWriter(f\'{root}/{dataset}/logs/{sr}/logs\')\n    dataset = FilesAudioDataset(hps)\n    print(""Length of dataset"", len(dataset))\n\n    # Torch Loader\n    collate_fn = lambda batch: t.stack([t.from_numpy(b) for b in batch], 0)\n    sampler = DistributedSampler(dataset)\n    train_loader = DataLoader(dataset, batch_size=hps.bs, num_workers=hps.nworkers, pin_memory=False, sampler=sampler,\n                              drop_last=True, collate_fn=collate_fn)\n\n    dist.barrier()\n    sampler.set_epoch(0)\n    for i, x in enumerate(tqdm(train_loader)):\n        x = x.to(\'cuda\', non_blocking=True)\n        for j, aud in enumerate(x):\n            writer.add_audio(\'in_\' + str(i*hps.bs + j), aud, 1, hps.sr)\n        print(""Wrote in"")\n        x = audio_preprocess(x, hps)\n        x = audio_postprocess(x, hps)\n        for j, aud in enumerate(x):\n            writer.add_audio(\'out_\' + str(i*hps.bs + j), aud, 1, hps.sr)\n        print(""Wrote out"")\n        dist.barrier()\n        break\n\nif __name__ == \'__main__\':\n    from jukebox.utils.dist_utils import setup_dist_from_mpi\n    setup_dist_from_mpi(port=29500)\n    test_dataset_loader()\n\n'"
jukebox/utils/logger.py,0,"b'import torch as t\nimport jukebox.utils.dist_adapter as dist\nfrom tqdm import tqdm\nfrom datetime import date\nimport os\nimport sys\n\ndef def_tqdm(x):\n    return tqdm(x, leave=True, file=sys.stdout, bar_format=""{n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}]"")\n\ndef get_range(x):\n    if dist.get_rank() == 0:\n        return def_tqdm(x)\n    else:\n        return x\n\ndef init_logging(hps, local_rank, rank):\n    logdir = f""{hps.local_logdir}/{hps.name}""\n    if local_rank == 0:\n        if not os.path.exists(logdir):\n            os.makedirs(logdir)\n        with open(logdir + \'argv.txt\', \'w\') as f:\n            f.write(hps.argv + \'\\n\')\n        print(""Logging to"", logdir)\n    logger = Logger(logdir, rank)\n    metrics = Metrics()\n    logger.add_text(\'hps\', str(hps))\n    return logger, metrics\n\ndef get_name(hps):\n    name = """"\n    for key, value in hps.items():\n        name += f""{key}_{value}_""\n    return name\n\ndef average_metrics(_metrics):\n    metrics = {}\n    for _metric in _metrics:\n        for key, val in _metric.items():\n            if key not in metrics:\n                metrics[key] = []\n            metrics[key].append(val)\n    return {key: sum(vals)/len(vals) for key, vals in metrics.items()}\n\nclass Metrics:\n    def __init__(self):\n        self.sum = {}\n        self.n = {}\n\n    def update(self, tag, val, batch):\n        # v is average value over batch\n        # store total value and total batch, returns dist average\n        sum = t.tensor(val * batch).float().cuda()\n        n = t.tensor(batch).float().cuda()\n        dist.all_reduce(sum)\n        dist.all_reduce(n)\n        sum = sum.item()\n        n = n.item()\n        self.sum[tag] = self.sum.get(tag, 0.0) + sum\n        self.n[tag] = self.n.get(tag, 0.0) + n\n        return sum / n\n\n    def avg(self, tag):\n        if tag in self.sum:\n            return self.sum[tag] / self.n[tag]\n        else:\n            return 0.0\n\n    def reset(self):\n        self.sum = {}\n        self.n = {}\n\nclass Logger:\n    def __init__(self, logdir, rank):\n        if rank == 0:\n            from tensorboardX import SummaryWriter\n            self.sw = SummaryWriter(f""{logdir}/logs"")\n        self.iters = 0\n        self.rank = rank\n        self.works = []\n        self.logdir = logdir\n\n    def step(self):\n        self.iters += 1\n\n    def flush(self):\n        if self.rank == 0:\n            self.sw.flush()\n\n    def add_text(self, tag, text):\n        if self.rank == 0:\n            self.sw.add_text(tag, text, self.iters)\n\n    def add_audios(self, tag, auds, sample_rate=22050, max_len=None, max_log=8):\n        if self.rank == 0:\n            for i in range(min(len(auds), max_log)):\n                if max_len:\n                    self.sw.add_audio(f""{i}/{tag}"", auds[i][:max_len * sample_rate], self.iters, sample_rate)\n                else:\n                    self.sw.add_audio(f""{i}/{tag}"", auds[i], self.iters, sample_rate)\n\n    def add_audio(self, tag, aud, sample_rate=22050):\n        if self.rank == 0:\n            self.sw.add_audio(tag, aud, self.iters, sample_rate)\n\n    def add_images(self, tag, img, dataformats=""NHWC""):\n        if self.rank == 0:\n            self.sw.add_images(tag, img, self.iters, dataformats=dataformats)\n\n    def add_image(self, tag, img):\n        if self.rank == 0:\n            self.sw.add_image(tag, img, self.iters)\n\n    def add_scalar(self, tag, val):\n        if self.rank == 0:\n            self.sw.add_scalar(tag, val, self.iters)\n\n    def get_range(self, loader):\n        if self.rank == 0:\n            self.trange = def_tqdm(loader)\n        else:\n            self.trange = loader\n        return enumerate(self.trange)\n\n    def close_range(self):\n        if self.rank == 0:\n            self.trange.close()\n\n    def set_postfix(self, *args, **kwargs):\n        if self.rank == 0:\n            self.trange.set_postfix(*args, **kwargs)\n\n    # For logging summaries of varies graph ops\n    def add_reduce_scalar(self, tag, layer, val):\n        if self.iters % 100 == 0:\n            with t.no_grad():\n                val = val.float().norm()/float(val.numel())\n            work = dist.reduce(val, 0, async_op=True)\n            self.works.append((tag, layer, val, work))\n\n    def finish_reduce(self):\n        for tag, layer, val, work in self.works:\n            work.wait()\n            if self.rank == 0:\n                val = val.item()/dist.get_world_size()\n                self.lw[layer].add_scalar(tag, val, self.iters)\n        self.works = []\n'"
jukebox/utils/sample_utils.py,0,"b""import torch as t\n\ndef split_batch(obj, n_samples, split_size):\n    n_passes = (n_samples + split_size - 1) // split_size\n    if isinstance(obj, t.Tensor):\n        return t.split(obj, split_size, dim=0)\n    elif isinstance(obj, list):\n        return list(zip(*[t.split(item, split_size, dim=0) for item in obj]))\n    elif obj is None:\n        return [None] * n_passes\n    else:\n        raise TypeError('Unknown input type')\n\n# Break total_length into hops/windows of size n_ctx separated by hop_length\ndef get_starts(total_length, n_ctx, hop_length):\n    starts = []\n    for start in range(0, total_length - n_ctx + hop_length, hop_length):\n        if start + n_ctx >= total_length:\n            # Last hop could be smaller, we make it n_ctx to maximise context\n            start = total_length - n_ctx\n        starts.append(start)\n    return starts\n"""
jukebox/utils/torch_utils.py,0,"b'import gc\nimport torch as t\n\ndef freeze_model(model):\n    model.eval()\n    for params in model.parameters():\n        params.requires_grad = False\n\n\ndef unfreeze_model(model):\n    model.train()\n    for params in model.parameters():\n        params.requires_grad = True\n\ndef zero_grad(model):\n    for p in model.parameters():\n        if p.requires_grad and p.grad is not None:\n            p.grad = None\n\ndef empty_cache():\n    gc.collect()\n    t.cuda.empty_cache()\n\ndef assert_shape(x, exp_shape):\n    assert x.shape == exp_shape, f""Expected {exp_shape} got {x.shape}""\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef count_state(model):\n    return sum(s.numel() for s in model.state_dict().values())\n\n'"
jukebox/vqvae/__init__.py,0,b''
jukebox/vqvae/bottleneck.py,2,"b'import numpy as np\nimport torch as t\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport jukebox.utils.dist_adapter as dist\n\nclass BottleneckBlock(nn.Module):\n    def __init__(self, k_bins, emb_width, mu):\n        super().__init__()\n        self.k_bins = k_bins\n        self.emb_width = emb_width\n        self.mu = mu\n        self.reset_k()\n        self.threshold = 1.0\n\n    def reset_k(self):\n        self.init = False\n        self.k_sum = None\n        self.k_elem = None\n        self.register_buffer(\'k\', t.zeros(self.k_bins, self.emb_width).cuda())\n\n    def _tile(self, x):\n        d, ew = x.shape\n        if d < self.k_bins:\n            n_repeats = (self.k_bins + d - 1) // d\n            std = 0.01 / np.sqrt(ew)\n            x = x.repeat(n_repeats, 1)\n            x = x + t.randn_like(x) * std\n        return x\n\n    def init_k(self, x):\n        mu, emb_width, k_bins = self.mu, self.emb_width, self.k_bins\n        self.init = True\n        # init k_w using random vectors from x\n        y = self._tile(x)\n        _k_rand = y[t.randperm(y.shape[0])][:k_bins]\n        dist.broadcast(_k_rand, 0)\n        self.k = _k_rand\n        assert self.k.shape == (k_bins, emb_width)\n        self.k_sum = self.k\n        self.k_elem = t.ones(k_bins, device=self.k.device)\n\n    def restore_k(self, num_tokens=None, threshold=1.0):\n        mu, emb_width, k_bins = self.mu, self.emb_width, self.k_bins\n        self.init = True\n        assert self.k.shape == (k_bins, emb_width)\n        self.k_sum = self.k.clone()\n        self.k_elem = t.ones(k_bins, device=self.k.device)\n        if num_tokens is not None:\n            expected_usage = num_tokens / k_bins\n            self.k_elem.data.mul_(expected_usage)\n            self.k_sum.data.mul_(expected_usage)\n        self.threshold = threshold\n\n    def update_k(self, x, x_l):\n        mu, emb_width, k_bins = self.mu, self.emb_width, self.k_bins\n        with t.no_grad():\n            # Calculate new centres\n            x_l_onehot = t.zeros(k_bins, x.shape[0], device=x.device)  # k_bins, N * L\n            x_l_onehot.scatter_(0, x_l.view(1, x.shape[0]), 1)\n\n            _k_sum = t.matmul(x_l_onehot, x)  # k_bins, w\n            _k_elem = x_l_onehot.sum(dim=-1)  # k_bins\n            y = self._tile(x)\n            _k_rand = y[t.randperm(y.shape[0])][:k_bins]\n\n            dist.broadcast(_k_rand, 0)\n            dist.all_reduce(_k_sum)\n            dist.all_reduce(_k_elem)\n\n            # Update centres\n            old_k = self.k\n            self.k_sum = mu * self.k_sum + (1. - mu) * _k_sum  # w, k_bins\n            self.k_elem = mu * self.k_elem + (1. - mu) * _k_elem  # k_bins\n            usage = (self.k_elem.view(k_bins, 1) >= self.threshold).float()\n            self.k = usage * (self.k_sum.view(k_bins, emb_width) / self.k_elem.view(k_bins, 1)) \\\n                     + (1 - usage) * _k_rand\n            _k_prob = _k_elem / t.sum(_k_elem)  # x_l_onehot.mean(dim=-1)  # prob of each bin\n            entropy = -t.sum(_k_prob * t.log(_k_prob + 1e-8))  # entropy ie how diverse\n            used_curr = (_k_elem >= self.threshold).sum()\n            usage = t.sum(usage)\n            dk = t.norm(self.k - old_k) / np.sqrt(np.prod(old_k.shape))\n        return dict(entropy=entropy,\n                    used_curr=used_curr,\n                    usage=usage,\n                    dk=dk)\n\n    def preprocess(self, x):\n        # NCT -> NTC -> [NT, C]\n        x = x.permute(0, 2, 1).contiguous()\n        x = x.view(-1, x.shape[-1])  # x_en = (N * L, w), k_j = (w, k_bins)\n\n        if x.shape[-1] == self.emb_width:\n            prenorm = t.norm(x - t.mean(x)) / np.sqrt(np.prod(x.shape))\n        elif x.shape[-1] == 2 * self.emb_width:\n            x1, x2 = x[...,:self.emb_width], x[...,self.emb_width:]\n            prenorm = (t.norm(x1 - t.mean(x1)) / np.sqrt(np.prod(x1.shape))) + (t.norm(x2 - t.mean(x2)) / np.sqrt(np.prod(x2.shape)))\n\n            # Normalise\n            x = x1 + x2\n        else:\n            assert False, f""Expected {x.shape[-1]} to be (1 or 2) * {self.emb_width}""\n        return x, prenorm\n\n    def postprocess(self, x_l, x_d, x_shape):\n        # [NT, C] -> NTC -> NCT\n        N, T = x_shape\n        x_d = x_d.view(N, T, -1).permute(0, 2, 1).contiguous()\n        x_l = x_l.view(N, T)\n        return x_l, x_d\n\n    def quantise(self, x):\n        # Calculate latent code x_l\n        k_w = self.k.t()\n        distance = t.sum(x ** 2, dim=-1, keepdim=True) - 2 * t.matmul(x, k_w) + t.sum(k_w ** 2, dim=0,\n                                                                                            keepdim=True)  # (N * L, b)\n        min_distance, x_l = t.min(distance, dim=-1)\n        fit = t.mean(min_distance)\n        return x_l, fit\n\n    def dequantise(self, x_l):\n        x = F.embedding(x_l, self.k)\n        return x\n\n    def encode(self, x):\n        N, width, T = x.shape\n\n        # Preprocess.\n        x, prenorm = self.preprocess(x)\n\n        # Quantise\n        x_l, fit = self.quantise(x)\n\n        # Postprocess.\n        x_l = x_l.view(N, T)\n        return x_l\n\n    def decode(self, x_l):\n        N, T = x_l.shape\n        width = self.emb_width\n\n        # Dequantise\n        x_d = self.dequantise(x_l)\n\n        # Postprocess\n        x_d = x_d.view(N, T, width).permute(0, 2, 1).contiguous()\n        return x_d\n\n    def forward(self, x, update_k=True):\n        N, width, T = x.shape\n\n        # Preprocess\n        x, prenorm = self.preprocess(x)\n\n        # Init k if not inited\n        if update_k and not self.init:\n            self.init_k(x)\n\n        # Quantise and dequantise through bottleneck\n        x_l, fit = self.quantise(x)\n        x_d = self.dequantise(x_l)\n\n        # Update embeddings\n        if update_k:\n            update_metrics = self.update_k(x, x_l)\n        else:\n            update_metrics = {}\n\n        # Loss\n        commit_loss = t.norm(x_d.detach() - x) ** 2 / np.prod(x.shape)\n\n        # Passthrough\n        x_d = x + (x_d - x).detach()\n\n        # Postprocess\n        x_l, x_d = self.postprocess(x_l, x_d, (N,T))\n        return x_l, x_d, commit_loss, dict(fit=fit,\n                                           pn=prenorm,\n                                           **update_metrics)\n\n\nclass Bottleneck(nn.Module):\n    def __init__(self, l_bins, emb_width, mu, levels):\n        super().__init__()\n        self.levels = levels\n        level_block = lambda level: BottleneckBlock(l_bins, emb_width, mu)\n        self.level_blocks = nn.ModuleList()\n        for level in range(self.levels):\n            self.level_blocks.append(level_block(level))\n\n    def encode(self, xs):\n        zs = [level_block.encode(x) for (level_block, x) in zip(self.level_blocks, xs)]\n        return zs\n\n    def decode(self, zs, start_level=0, end_level=None):\n        if end_level is None:\n            end_level = self.levels\n        xs_quantised = [level_block.decode(z) for (level_block, z) in zip(self.level_blocks[start_level:end_level], zs)]\n        return xs_quantised\n\n    def forward(self, xs):\n        zs, xs_quantised, commit_losses, metrics = [], [], [], []\n        for level in range(self.levels):\n            level_block = self.level_blocks[level]\n            x = xs[level]\n            z, x_quantised, commit_loss, metric = level_block(x, update_k=self.training)\n            zs.append(z)\n            if not self.training:\n                # Be extra paranoid and make sure the encoder weights can\'t\n                # change from straight-through estimator\n                x_quantised = x_quantised.detach()\n            xs_quantised.append(x_quantised)\n            commit_losses.append(commit_loss)\n            if self.training:\n                metrics.append(metric)\n        return zs, xs_quantised, commit_losses, metrics\n\nclass NoBottleneckBlock(nn.Module):\n    def restore_k(self):\n        pass\n\nclass NoBottleneck(nn.Module):\n    def __init__(self, levels):\n        super().__init__()\n        self.level_blocks = nn.ModuleList()\n        self.levels = levels\n        for level in range(levels):\n            self.level_blocks.append(NoBottleneckBlock())\n\n    def encode(self, xs):\n        return xs\n\n    def decode(self, zs, start_level=0, end_level=None):\n        if end_level is None:\n            end_level = self.levels\n        return zs\n\n    def forward(self, xs):\n        zero = t.zeros(()).cuda()\n        commit_losses = [zero for _ in range(self.levels)]\n        metrics = [dict(entropy=zero, usage=zero, used_curr=zero, pn=zero, dk=zero) for _ in range(self.levels)]\n        return xs, xs, commit_losses, metrics\n\nif __name__ == \'__main__\':\n    from jukebox.utils.dist_utils import setup_dist_from_mpi\n    rank, local_rank, device = setup_dist_from_mpi(port=29600)\n    bottleneck = Bottleneck(256, 64, 0.99, 2).to(device)\n    bottleneck.check()\n'"
jukebox/vqvae/encdec.py,1,"b""import torch as t\nimport torch.nn as nn\nfrom jukebox.vqvae.resnet import Resnet, Resnet1D\nfrom jukebox.utils.torch_utils import assert_shape\n\nclass EncoderConvBlock(nn.Module):\n    def __init__(self, input_emb_width, output_emb_width, down_t,\n                 stride_t, width, depth, m_conv,\n                 dilation_growth_rate=1, dilation_cycle=None, zero_out=False,\n                 res_scale=False):\n        super().__init__()\n        blocks = []\n        filter_t, pad_t = stride_t * 2, stride_t // 2\n        if down_t > 0:\n            for i in range(down_t):\n                block = nn.Sequential(\n                    nn.Conv1d(input_emb_width if i == 0 else width, width, filter_t, stride_t, pad_t),\n                    Resnet1D(width, depth, m_conv, dilation_growth_rate, dilation_cycle, zero_out, res_scale),\n                )\n                blocks.append(block)\n            block = nn.Conv1d(width, output_emb_width, 3, 1, 1)\n            blocks.append(block)\n        self.model = nn.Sequential(*blocks)\n\n    def forward(self, x):\n        return self.model(x)\n\nclass DecoderConvBock(nn.Module):\n    def __init__(self, input_emb_width, output_emb_width, down_t,\n                 stride_t, width, depth, m_conv, dilation_growth_rate=1, dilation_cycle=None, zero_out=False, res_scale=False, reverse_decoder_dilation=False, checkpoint_res=False):\n        super().__init__()\n        blocks = []\n        if down_t > 0:\n            filter_t, pad_t = stride_t * 2, stride_t // 2\n            block = nn.Conv1d(output_emb_width, width, 3, 1, 1)\n            blocks.append(block)\n            for i in range(down_t):\n                block = nn.Sequential(\n                    Resnet1D(width, depth, m_conv, dilation_growth_rate, dilation_cycle, zero_out=zero_out, res_scale=res_scale, reverse_dilation=reverse_decoder_dilation, checkpoint_res=checkpoint_res),\n                    nn.ConvTranspose1d(width, input_emb_width if i == (down_t - 1) else width, filter_t, stride_t, pad_t)\n                )\n                blocks.append(block)\n        self.model = nn.Sequential(*blocks)\n\n    def forward(self, x):\n        return self.model(x)\n\nclass Encoder(nn.Module):\n    def __init__(self, input_emb_width, output_emb_width, levels, downs_t,\n                 strides_t, **block_kwargs):\n        super().__init__()\n        self.input_emb_width = input_emb_width\n        self.output_emb_width = output_emb_width\n        self.levels = levels\n        self.downs_t = downs_t\n        self.strides_t = strides_t\n\n        block_kwargs_copy = dict(**block_kwargs)\n        if 'reverse_decoder_dilation' in block_kwargs_copy:\n            del block_kwargs_copy['reverse_decoder_dilation']\n        level_block = lambda level, down_t, stride_t: EncoderConvBlock(input_emb_width if level == 0 else output_emb_width,\n                                                           output_emb_width,\n                                                           down_t, stride_t,\n                                                           **block_kwargs_copy)\n        self.level_blocks = nn.ModuleList()\n        iterator = zip(list(range(self.levels)), downs_t, strides_t)\n        for level, down_t, stride_t in iterator:\n            self.level_blocks.append(level_block(level, down_t, stride_t))\n\n    def forward(self, x):\n        N, T = x.shape[0], x.shape[-1]\n        emb = self.input_emb_width\n        assert_shape(x, (N, emb, T))\n        xs = []\n\n        # 64, 32, ...\n        iterator = zip(list(range(self.levels)), self.downs_t, self.strides_t)\n        for level, down_t, stride_t in iterator:\n            level_block = self.level_blocks[level]\n            x = level_block(x)\n            emb, T = self.output_emb_width, T // (stride_t ** down_t)\n            assert_shape(x, (N, emb, T))\n            xs.append(x)\n\n        return xs\n\nclass Decoder(nn.Module):\n    def __init__(self, input_emb_width, output_emb_width, levels, downs_t,\n                 strides_t, **block_kwargs):\n        super().__init__()\n        self.input_emb_width = input_emb_width\n        self.output_emb_width = output_emb_width\n        self.levels = levels\n\n        self.downs_t = downs_t\n\n        self.strides_t = strides_t\n\n        level_block = lambda level, down_t, stride_t: DecoderConvBock(output_emb_width,\n                                                          output_emb_width,\n                                                          down_t, stride_t,\n                                                          **block_kwargs)\n        self.level_blocks = nn.ModuleList()\n        iterator = zip(list(range(self.levels)), downs_t, strides_t)\n        for level, down_t, stride_t in iterator:\n            self.level_blocks.append(level_block(level, down_t, stride_t))\n\n        self.out = nn.Conv1d(output_emb_width, input_emb_width, 3, 1, 1)\n\n    def forward(self, xs, all_levels=True):\n        if all_levels:\n            assert len(xs) == self.levels\n        else:\n            assert len(xs) == 1\n        x = xs[-1]\n        N, T = x.shape[0], x.shape[-1]\n        emb = self.output_emb_width\n        assert_shape(x, (N, emb, T))\n\n        # 32, 64 ...\n        iterator = reversed(list(zip(list(range(self.levels)), self.downs_t, self.strides_t)))\n        for level, down_t, stride_t in iterator:\n            level_block = self.level_blocks[level]\n            x = level_block(x)\n            emb, T = self.output_emb_width, T * (stride_t ** down_t)\n            assert_shape(x, (N, emb, T))\n            if level != 0 and all_levels:\n                x = x + xs[level - 1]\n\n        x = self.out(x)\n        return x\n"""
jukebox/vqvae/resnet.py,1,"b'import math\nimport torch.nn as nn\nimport jukebox.utils.dist_adapter as dist\nfrom jukebox.utils.checkpoint import checkpoint\n\nclass ResConvBlock(nn.Module):\n    def __init__(self, n_in, n_state):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.ReLU(),\n            nn.Conv2d(n_in, n_state, 3, 1, 1),\n            nn.ReLU(),\n            nn.Conv2d(n_state, n_in, 1, 1, 0),\n        )\n\n    def forward(self, x):\n        return x + self.model(x)\n\nclass Resnet(nn.Module):\n    def __init__(self, n_in, n_depth, m_conv=1.0):\n        super().__init__()\n        self.model = nn.Sequential(*[ResConvBlock(n_in, int(m_conv * n_in)) for _ in range(n_depth)])\n\n    def forward(self, x):\n        return self.model(x)\n\nclass ResConv1DBlock(nn.Module):\n    def __init__(self, n_in, n_state, dilation=1, zero_out=False, res_scale=1.0):\n        super().__init__()\n        padding = dilation\n        self.model = nn.Sequential(\n            nn.ReLU(),\n            nn.Conv1d(n_in, n_state, 3, 1, padding, dilation),\n            nn.ReLU(),\n            nn.Conv1d(n_state, n_in, 1, 1, 0),\n        )\n        if zero_out:\n            out = self.model[-1]\n            nn.init.zeros_(out.weight)\n            nn.init.zeros_(out.bias)\n        self.res_scale = res_scale\n\n    def forward(self, x):\n        return x + self.res_scale * self.model(x)\n\nclass Resnet1D(nn.Module):\n    def __init__(self, n_in, n_depth, m_conv=1.0, dilation_growth_rate=1, dilation_cycle=None, zero_out=False, res_scale=False, reverse_dilation=False, checkpoint_res=False):\n        super().__init__()\n        def _get_depth(depth):\n            if dilation_cycle is None:\n                return depth\n            else:\n                return depth % dilation_cycle\n        blocks = [ResConv1DBlock(n_in, int(m_conv * n_in),\n                                 dilation=dilation_growth_rate ** _get_depth(depth),\n                                 zero_out=zero_out,\n                                 res_scale=1.0 if not res_scale else 1.0 / math.sqrt(n_depth))\n                  for depth in range(n_depth)]\n        if reverse_dilation:\n            blocks = blocks[::-1]\n        self.checkpoint_res = checkpoint_res\n        if self.checkpoint_res == 1:\n            if dist.get_rank() == 0:\n                print(""Checkpointing convs"")\n            self.blocks = nn.ModuleList(blocks)\n        else:\n            self.model = nn.Sequential(*blocks)\n\n    def forward(self, x):\n        if self.checkpoint_res == 1:\n            for block in self.blocks:\n                x = checkpoint(block, (x, ), block.parameters(), True)\n            return x\n        else:\n            return self.model(x)\n'"
jukebox/vqvae/vqvae.py,1,"b'import numpy as np\nimport torch as t\nimport torch.nn as nn\n\nfrom jukebox.vqvae.encdec import Encoder, Decoder, assert_shape\nfrom jukebox.vqvae.bottleneck import NoBottleneck, Bottleneck\nfrom jukebox.utils.logger import average_metrics\nfrom jukebox.utils.audio_utils import spectral_convergence, spectral_loss, multispectral_loss, audio_postprocess\n\ndef dont_update(params):\n    for param in params:\n        param.requires_grad = False\n\ndef update(params):\n    for param in params:\n        param.requires_grad = True\n\ndef calculate_strides(strides, downs):\n    return [stride ** down for stride, down in zip(strides, downs)]\n\ndef _loss_fn(loss_fn, x_target, x_pred, hps):\n    if loss_fn == \'l1\':\n        return t.mean(t.abs(x_pred - x_target)) / hps.bandwidth[\'l1\']\n    elif loss_fn == \'l2\':\n        return t.mean((x_pred - x_target) ** 2) / hps.bandwidth[\'l2\']\n    elif loss_fn == \'linf\':\n        residual = ((x_pred - x_target) ** 2).reshape(x_target.shape[0], -1)\n        values, _ = t.topk(residual, hps.linf_k, dim=1)\n        return t.mean(values) / hps.bandwidth[\'l2\']\n    elif loss_fn == \'lmix\':\n        loss = 0.0\n        if hps.lmix_l1:\n            loss += hps.lmix_l1 * _loss_fn(\'l1\', x_target, x_pred, hps)\n        if hps.lmix_l2:\n            loss += hps.lmix_l2 * _loss_fn(\'l2\', x_target, x_pred, hps)\n        if hps.lmix_linf:\n            loss += hps.lmix_linf * _loss_fn(\'linf\', x_target, x_pred, hps)\n        return loss\n    else:\n        assert False, f""Unknown loss_fn {loss_fn}""\n\nclass VQVAE(nn.Module):\n    def __init__(self, input_shape, levels, downs_t, strides_t,\n                 emb_width, l_bins, mu, commit, spectral, multispectral,\n                 multipliers=None, use_bottleneck=True, **block_kwargs):\n        super().__init__()\n\n        self.sample_length = input_shape[0]\n        x_shape, x_channels = input_shape[:-1], input_shape[-1]\n        self.x_shape = x_shape\n\n        self.downsamples = calculate_strides(strides_t, downs_t)\n        self.hop_lengths = np.cumprod(self.downsamples)\n        self.z_shapes = z_shapes = [(x_shape[0] // self.hop_lengths[level],) for level in range(levels)]\n        self.levels = levels\n\n        if multipliers is None:\n            self.multipliers = [1] * levels\n        else:\n            assert len(multipliers) == levels, ""Invalid number of multipliers""\n            self.multipliers = multipliers\n        def _block_kwargs(level):\n            this_block_kwargs = dict(block_kwargs)\n            this_block_kwargs[""width""] *= self.multipliers[level]\n            this_block_kwargs[""depth""] *= self.multipliers[level]\n            return this_block_kwargs\n\n        encoder = lambda level: Encoder(x_channels, emb_width, level + 1,\n                                        downs_t[:level+1], strides_t[:level+1], **_block_kwargs(level))\n        decoder = lambda level: Decoder(x_channels, emb_width, level + 1,\n                                        downs_t[:level+1], strides_t[:level+1], **_block_kwargs(level))\n        self.encoders = nn.ModuleList()\n        self.decoders = nn.ModuleList()\n        for level in range(levels):\n            self.encoders.append(encoder(level))\n            self.decoders.append(decoder(level))\n\n        if use_bottleneck:\n            self.bottleneck = Bottleneck(l_bins, emb_width, mu, levels)\n        else:\n            self.bottleneck = NoBottleneck(levels)\n\n        self.downs_t = downs_t\n        self.strides_t = strides_t\n        self.l_bins = l_bins\n        self.commit = commit\n        self.spectral = spectral\n        self.multispectral = multispectral\n\n    def preprocess(self, x):\n        # x: NTC [-1,1] -> NCT [-1,1]\n        assert len(x.shape) == 3\n        x = x.permute(0,2,1).float()\n        return x\n\n    def postprocess(self, x):\n        # x: NTC [-1,1] <- NCT [-1,1]\n        x = x.permute(0,2,1)\n        return x\n\n    def _decode(self, zs, start_level=0, end_level=None):\n        # Decode\n        if end_level is None:\n            end_level = self.levels\n        assert len(zs) == end_level - start_level\n        xs_quantised = self.bottleneck.decode(zs, start_level=start_level, end_level=end_level)\n        assert len(xs_quantised) == end_level - start_level\n\n        # Use only lowest level\n        decoder, x_quantised = self.decoders[start_level], xs_quantised[0:1]\n        x_out = decoder(x_quantised, all_levels=False)\n        x_out = self.postprocess(x_out)\n        return x_out\n\n    def decode(self, zs, start_level=0, end_level=None, bs_chunks=1):\n        z_chunks = [t.chunk(z, bs_chunks, dim=0) for z in zs]\n        x_outs = []\n        for i in range(bs_chunks):\n            zs_i = [z_chunk[i] for z_chunk in z_chunks]\n            x_out = self._decode(zs_i, start_level=start_level, end_level=end_level)\n            x_outs.append(x_out)\n        return t.cat(x_outs, dim=0)\n\n    def _encode(self, x, start_level=0, end_level=None):\n        # Encode\n        if end_level is None:\n            end_level = self.levels\n        x_in = self.preprocess(x)\n        xs = []\n        for level in range(self.levels):\n            encoder = self.encoders[level]\n            x_out = encoder(x_in)\n            xs.append(x_out[-1])\n        zs = self.bottleneck.encode(xs)\n        return zs[start_level:end_level]\n\n    def encode(self, x, start_level=0, end_level=None, bs_chunks=1):\n        x_chunks = t.chunk(x, bs_chunks, dim=0)\n        zs_list = []\n        for x_i in x_chunks:\n            zs_i = self._encode(x_i, start_level=start_level, end_level=end_level)\n            zs_list.append(zs_i)\n        zs = [t.cat(zs_level_list, dim=0) for zs_level_list in zip(*zs_list)]\n        return zs\n\n    def sample(self, n_samples):\n        zs = [t.randint(0, self.l_bins, size=(n_samples, *z_shape), device=\'cuda\') for z_shape in self.z_shapes]\n        return self.decode(zs)\n\n    def forward(self, x, hps, loss_fn=\'l1\'):\n        metrics = {}\n\n        N = x.shape[0]\n\n        # Encode/Decode\n        x_in = self.preprocess(x)\n        xs = []\n        for level in range(self.levels):\n            encoder = self.encoders[level]\n            x_out = encoder(x_in)\n            xs.append(x_out[-1])\n\n        zs, xs_quantised, commit_losses, quantiser_metrics = self.bottleneck(xs)\n        x_outs = []\n        for level in range(self.levels):\n            decoder = self.decoders[level]\n            x_out = decoder(xs_quantised[level:level+1], all_levels=False)\n            assert_shape(x_out, x_in.shape)\n            x_outs.append(x_out)\n\n        # Loss\n        def _spectral_loss(x_target, x_out, hps):\n            if hps.use_nonrelative_specloss:\n                sl = spectral_loss(x_target, x_out, hps) / hps.bandwidth[\'spec\']\n            else:\n                sl = spectral_convergence(x_target, x_out, hps)\n            sl = t.mean(sl)\n            return sl\n\n        def _multispectral_loss(x_target, x_out, hps):\n            sl = multispectral_loss(x_target, x_out, hps) / hps.bandwidth[\'spec\']\n            sl = t.mean(sl)\n            return sl\n\n        recons_loss = t.zeros(()).to(x.device)\n        spec_loss = t.zeros(()).to(x.device)\n        multispec_loss = t.zeros(()).to(x.device)\n        x_target = audio_postprocess(x.float(), hps)\n\n        for level in reversed(range(self.levels)):\n            x_out = self.postprocess(x_outs[level])\n            x_out = audio_postprocess(x_out, hps)\n            this_recons_loss = _loss_fn(loss_fn, x_target, x_out, hps)\n            this_spec_loss = _spectral_loss(x_target, x_out, hps)\n            this_multispec_loss = _multispectral_loss(x_target, x_out, hps)\n            metrics[f\'recons_loss_l{level + 1}\'] = this_recons_loss\n            metrics[f\'spectral_loss_l{level + 1}\'] = this_spec_loss\n            metrics[f\'multispectral_loss_l{level + 1}\'] = this_multispec_loss\n            recons_loss += this_recons_loss\n            spec_loss += this_spec_loss\n            multispec_loss += this_multispec_loss\n\n        commit_loss = sum(commit_losses)\n        loss = recons_loss + self.spectral * spec_loss + self.multispectral * multispec_loss + self.commit * commit_loss\n\n        with t.no_grad():\n            sc = t.mean(spectral_convergence(x_target, x_out, hps))\n            l2_loss = _loss_fn(""l2"", x_target, x_out, hps)\n            l1_loss = _loss_fn(""l1"", x_target, x_out, hps)\n            linf_loss = _loss_fn(""linf"", x_target, x_out, hps)\n\n        quantiser_metrics = average_metrics(quantiser_metrics)\n\n        metrics.update(dict(\n            recons_loss=recons_loss,\n            spectral_loss=spec_loss,\n            multispectral_loss=multispec_loss,\n            spectral_convergence=sc,\n            l2_loss=l2_loss,\n            l1_loss=l1_loss,\n            linf_loss=linf_loss,\n            commit_loss=commit_loss,\n            **quantiser_metrics))\n\n        for key, val in metrics.items():\n            metrics[key] = val.detach()\n\n        return x_out, loss, metrics\n'"
tensorboardX/docs/conf.py,1,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# tensorboardX documentation build configuration file, created by\n# sphinx-quickstart on Wed Aug  9 01:38:01 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\n# sys.path.insert(0, os.path.abspath(\'.\'))\nsys.path.append(os.path.join(os.path.dirname(__file__), \'..\'))\n#import tensorboard #uncomment to shadow pip installation\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\'sphinx.ext.autodoc\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.napoleon\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.githubpages\']\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = \'tensorboardX\'\ncopyright = \'2017, tensorboardX Contributors\'\nauthor = \'tensorboardX Contributors\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = \'\'\n# The full version, including alpha/beta/rc tags.\nrelease = \'\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\n# html_static_path = [\'_static\']\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'tensorboardXdoc\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'tensorboardX.tex\', \'tensorboardX Documentation\',\n     \'tensorboardX Contributors\', \'manual\'),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'tensorboardX\', \'tensorboardX Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'tensorboardX\', \'tensorboardX Documentation\',\n     author, \'tensorboardX\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {\n    \'python\':(\'https://docs.python.org/3\', None),\n    \'numpy\': (\'http://docs.scipy.org/doc/numpy/\', None),\n    \'torch\': (\'http://pytorch.org/docs/master\', None),\n    \'matplotlib\': (\'http://matplotlib.sourceforge.net/\', None),\n    }\n'"
tensorboardX/examples/__init__.py,0,b''
tensorboardX/examples/demo.py,9,"b'import torch\nimport torchvision.utils as vutils\nimport numpy as np\nimport torchvision.models as models\nfrom torchvision import datasets\nfrom tensorboardX import SummaryWriter\nimport datetime\n\nresnet18 = models.resnet18(False)\nwriter = SummaryWriter()\nsample_rate = 44100\nfreqs = [262, 294, 330, 349, 392, 440, 440, 440, 440, 440, 440]\n\ntrue_positive_counts = [75, 64, 21, 5, 0]\nfalse_positive_counts = [150, 105, 18, 0, 0]\ntrue_negative_counts = [0, 45, 132, 150, 150]\nfalse_negative_counts = [0, 11, 54, 70, 75]\nprecision = [0.3333333, 0.3786982, 0.5384616, 1.0, 0.0]\nrecall = [1.0, 0.8533334, 0.28, 0.0666667, 0.0]\n\n\nfor n_iter in range(100):\n    s1 = torch.rand(1)  # value to keep\n    s2 = torch.rand(1)\n    # data grouping by `slash`\n    writer.add_scalar(\'data/scalar_systemtime\', s1[0], n_iter)\n    # data grouping by `slash`\n    writer.add_scalar(\'data/scalar_customtime\', s1[0], n_iter, walltime=n_iter)\n    writer.add_scalars(\'data/scalar_group\', {""xsinx"": n_iter * np.sin(n_iter),\n                                             ""xcosx"": n_iter * np.cos(n_iter),\n                                             ""arctanx"": np.arctan(n_iter)}, n_iter)\n    x = torch.rand(32, 3, 64, 64)  # output from network\n    if n_iter % 10 == 0:\n        x = vutils.make_grid(x, normalize=True, scale_each=True)\n        writer.add_image(\'Image\', x, n_iter)  # Tensor\n        writer.add_image_with_boxes(\'imagebox_label\', torch.ones(3, 240, 240) * 0.5,\n             torch.Tensor([[10, 10, 100, 100], [101, 101, 200, 200]]),\n             n_iter, \n             labels=[\'abcde\' + str(n_iter), \'fgh\' + str(n_iter)])\n        x = torch.zeros(sample_rate * 2)\n        for i in range(x.size(0)):\n            # sound amplitude should in [-1, 1]\n            x[i] = np.cos(freqs[n_iter // 10] * np.pi *\n                          float(i) / float(sample_rate))\n        writer.add_audio(\'myAudio\', x, n_iter)\n        writer.add_text(\'Text\', \'text logged at step:\' + str(n_iter), n_iter)\n        writer.add_text(\'markdown Text\', \'\'\'a|b\\n-|-\\nc|d\'\'\', n_iter)\n        for name, param in resnet18.named_parameters():\n            if \'bn\' not in name:\n                writer.add_histogram(name, param, n_iter)\n        writer.add_pr_curve(\'xoxo\', np.random.randint(2, size=100), np.random.rand(\n            100), n_iter)  # needs tensorboard 0.4RC or later\n        writer.add_pr_curve_raw(\'prcurve with raw data\', true_positive_counts,\n                                false_positive_counts,\n                                true_negative_counts,\n                                false_negative_counts,\n                                precision,\n                                recall, n_iter)\n# export scalar data to JSON for external processing\nwriter.export_scalars_to_json(""./all_scalars.json"")\n\ndataset = datasets.MNIST(\'mnist\', train=False, download=True)\nimages = dataset.test_data[:100].float()\nlabel = dataset.test_labels[:100]\nfeatures = images.view(100, 784)\nwriter.add_embedding(features, metadata=label, label_img=images.unsqueeze(1))\nwriter.add_embedding(features, global_step=1, tag=\'noMetadata\')\ndataset = datasets.MNIST(\'mnist\', train=True, download=True)\nimages_train = dataset.train_data[:100].float()\nlabels_train = dataset.train_labels[:100]\nfeatures_train = images_train.view(100, 784)\n\nall_features = torch.cat((features, features_train))\nall_labels = torch.cat((label, labels_train))\nall_images = torch.cat((images, images_train))\ndataset_label = [\'test\'] * 100 + [\'train\'] * 100\nall_labels = list(zip(all_labels, dataset_label))\n\nwriter.add_embedding(all_features, metadata=all_labels, label_img=all_images.unsqueeze(1),\n                     metadata_header=[\'digit\', \'dataset\'], global_step=2)\n\n# VIDEO\nvid_images = dataset.train_data[:16 * 48]\nvid = vid_images.view(16, 48, 1, 28, 28)  # BxTxCxHxW\n\nwriter.add_video(\'video\', vid_tensor=vid)\nwriter.add_video(\'video_1_fps\', vid_tensor=vid, fps=1)\n\nwriter.close()\n'"
tensorboardX/examples/demo_beholder.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \'License\');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \'AS IS\' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Simple MNIST classifier to demonstrate features of Beholder.\n\nBased on tensorflow/examples/tutorials/mnist/mnist_with_summaries.py.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorboardX.beholder as beholder_lib\nimport time\n\nfrom collections import namedtuple\n\n\nLOG_DIRECTORY = \'/tmp/beholder-demo\'\ntensor_and_name = namedtuple(\'tensor_and_name\', \'tensor, name\')\n\n\ndef beholder_pytorch():\n    for i in range(1000):\n        fake_param = [tensor_and_name(np.random.randn(128, 768, 3), \'test\' + str(i))\n                      for i in range(5)]\n        arrays = [tensor_and_name(np.random.randn(128, 768, 3), \'test\' + str(i))\n                  for i in range(5)]\n        beholder = beholder_lib.Beholder(logdir=LOG_DIRECTORY)\n        beholder.update(\n            trainable=fake_param,\n            arrays=arrays,\n            frame=np.random.randn(128, 128),\n        )\n        time.sleep(0.1)\n        print(i)\n\n\nif __name__ == \'__main__\':\n    import os\n    if not os.path.exists(LOG_DIRECTORY):\n        os.makedirs(LOG_DIRECTORY)\n    print(LOG_DIRECTORY)\n    beholder_pytorch()\n'"
tensorboardX/examples/demo_caffe2.py,0,"b'try:\n    import caffe2.python.predictor.predictor_exporter as pe\nexcept ImportError:\n    print(\'Please check that Caffe2 is installed correctly to run this demo.\')\nimport numpy as np\nimport os\nimport shutil\n\nfrom caffe2.python import core, model_helper, workspace, brew\nfrom tensorboardX import TorchVis\n\n""""""\nThis is a demo showcasing specific functionality for Caffe2. Shown here are\n    add_scalar (with both raw numerical data and Caffe2 blob names)\n    add_scalars (with both raw numerical data and Caffe2 blob names)\n    add_graph (visualizing a Caffe2 model as a graph)\n\nNOTE: lmdb must be installed and enabled with -DUSE_LMDB=ON for this demo to work.\n""""""\n\n# If you would like to see some really detailed initializations,\n# you can change --caffe2_log_level=0 to --caffe2_log_level=-1\ncore.GlobalInit([\'caffe2\', \'--caffe2_log_level=0\'])\nprint(""Necessities imported!"")\n\n\n# This section preps your image and test set in a lmdb database\ndef DownloadResource(url, path):\n    \'\'\'Downloads resources from s3 by url and unzips them to the provided path\'\'\'\n    import requests\n    from six import BytesIO\n    import zipfile\n    print(""Downloading... {} to {}"".format(url, path))\n    r = requests.get(url, stream=True)\n    z = zipfile.ZipFile(BytesIO(r.content))\n    z.extractall(path)\n    print(""Completed download and extraction."")\n\n\ncurrent_folder = os.path.join(os.path.expanduser(\'~\'), \'caffe2_notebooks\')\ndata_folder = os.path.join(current_folder, \'tutorial_data\', \'mnist\')\nroot_folder = os.path.join(current_folder, \'tutorial_files\', \'tutorial_mnist\')\ndb_missing = False\n\nif not os.path.exists(data_folder):\n    os.makedirs(data_folder)\n    print(""Your data folder was not found!! This was generated: {}"".format(data_folder))\n\n# Look for existing database: lmdb\nif os.path.exists(os.path.join(data_folder, ""mnist-train-nchw-lmdb"")):\n    print(""lmdb train db found!"")\nelse:\n    db_missing = True\n\nif os.path.exists(os.path.join(data_folder, ""mnist-test-nchw-lmdb"")):\n    print(""lmdb test db found!"")\nelse:\n    db_missing = True\n\n# attempt the download of the db if either was missing\nif db_missing:\n    print(""one or both of the MNIST lmbd dbs not found!!"")\n    db_url = ""http://download.caffe2.ai/databases/mnist-lmdb.zip""\n    try:\n        DownloadResource(db_url, data_folder)\n    except Exception as ex:\n        print(\n            ""Failed to download dataset. Please download it manually from {}"".format(db_url))\n        print(""Unzip it and place the two database folders here: {}"".format(data_folder))\n        raise ex\n\nif os.path.exists(root_folder):\n    print(""Looks like you ran this before, so we need to cleanup those old files..."")\n    shutil.rmtree(root_folder)\n\nos.makedirs(root_folder)\nworkspace.ResetWorkspace(root_folder)\n\nprint(""training data folder:"" + data_folder)\nprint(""workspace root folder:"" + root_folder)\n\n# END DATA PREPARATION #\n\n# Create TorchVis in preparation for writing. Default format is \'tensorboard\'\ntv = TorchVis()\n\n\ndef AddInput(model, batch_size, db, db_type):\n    # load the data\n    data_uint8, label = model.TensorProtosDBInput(\n        [], [""data_uint8"", ""label""], batch_size=batch_size,\n        db=db, db_type=db_type)\n    # cast the data to float\n    data = model.Cast(data_uint8, ""data"", to=core.DataType.FLOAT)\n    # scale data from [0,255] down to [0,1]\n    data = model.Scale(data, data, scale=float(1. / 256))\n    # don\'t need the gradient for the backward pass\n    data = model.StopGradient(data, data)\n    return data, label\n\n\ndef AddLeNetModel(model, data):\n    \'\'\'\n    This part is the standard LeNet model: from data to the softmax prediction.\n\n    For each convolutional layer we specify dim_in - number of input channels\n    and dim_out - number or output channels. Also each Conv and MaxPool layer changes the\n    image size. For example, kernel of size 5 reduces each side of an image by 4.\n\n    While when we have kernel and stride sizes equal 2 in a MaxPool layer, it divides\n    each side in half.\n    \'\'\'\n    # Image size: 28 x 28 -> 24 x 24\n    conv1 = brew.conv(model, data, \'conv1\', dim_in=1, dim_out=20, kernel=5)\n    # Image size: 24 x 24 -> 12 x 12\n    pool1 = brew.max_pool(model, conv1, \'pool1\', kernel=2, stride=2)\n    # Image size: 12 x 12 -> 8 x 8\n    conv2 = brew.conv(model, pool1, \'conv2\', dim_in=20, dim_out=100, kernel=5)\n    # Image size: 8 x 8 -> 4 x 4\n    pool2 = brew.max_pool(model, conv2, \'pool2\', kernel=2, stride=2)\n    # 50 * 4 * 4 stands for dim_out from previous layer multiplied by the\n    # image size\n    fc3 = brew.fc(model, pool2, \'fc3\', dim_in=100 * 4 * 4, dim_out=500)\n    relu = brew.relu(model, fc3, fc3)\n    pred = brew.fc(model, relu, \'pred\', 500, 10)\n    softmax = brew.softmax(model, pred, \'softmax\')\n    return softmax\n\n\ndef AddAccuracy(model, softmax, label):\n    """"""Adds an accuracy op to the model""""""\n    accuracy = brew.accuracy(model, [softmax, label], ""accuracy"")\n    return accuracy\n\n\ndef AddTrainingOperators(model, softmax, label):\n    """"""Adds training operators to the model.""""""\n    xent = model.LabelCrossEntropy([softmax, label], \'xent\')\n    # compute the expected loss\n    loss = model.AveragedLoss(xent, ""loss"")\n    # track the accuracy of the model\n    AddAccuracy(model, softmax, label)\n    # use the average loss we just computed to add gradient operators to the\n    # model\n    model.AddGradientOperators([loss])\n    # do a simple stochastic gradient descent\n    ITER = brew.iter(model, ""iter"")\n    # set the learning rate schedule\n    LR = model.LearningRate(\n        ITER, ""LR"", base_lr=-0.1, policy=""step"", stepsize=1, gamma=0.999)\n    # ONE is a constant value that is used in the gradient update. We only need\n    # to create it once, so it is explicitly placed in param_init_net.\n    ONE = model.param_init_net.ConstantFill([], ""ONE"", shape=[1], value=1.0)\n    # Now, for each parameter, we do the gradient updates.\n    for param in model.params:\n        # Note how we get the gradient of each parameter - ModelHelper keeps\n        # track of that.\n        param_grad = model.param_to_grad[param]\n        # The update is a simple weighted sum: param = param + param_grad * LR\n        model.WeightedSum([param, ONE, param_grad, LR], param)\n\n\ndef AddBookkeepingOperators(model):\n    """"""This adds a few bookkeeping operators that we can inspect later.\n\n    These operators do not affect the training procedure: they only collect\n    statistics and prints them to file or to logs.\n    """"""\n    # Print basically prints out the content of the blob. to_file=1 routes the\n    # printed output to a file. The file is going to be stored under\n    #     root_folder/[blob name]\n    model.Print(\'accuracy\', [], to_file=1)\n    model.Print(\'loss\', [], to_file=1)\n    # Summarizes the parameters. Different from Print, Summarize gives some\n    # statistics of the parameter, such as mean, std, min and max.\n    for param in model.params:\n        model.Summarize(param, [], to_file=1)\n        model.Summarize(model.param_to_grad[param], [], to_file=1)\n    # Now, if we really want to be verbose, we can summarize EVERY blob\n    # that the model produces; it is probably not a good idea, because that\n    # is going to take time - summarization do not come for free. For this\n    # demo, we will only show how to summarize the parameters and their\n    # gradients.\n\n\narg_scope = {""order"": ""NCHW""}\ntrain_model = model_helper.ModelHelper(name=""mnist_train"", arg_scope=arg_scope)\ndata, label = AddInput(\n    train_model, batch_size=64,\n    db=os.path.join(data_folder, \'mnist-train-nchw-lmdb\'),\n    db_type=\'lmdb\')\nsoftmax = AddLeNetModel(train_model, data)\nAddTrainingOperators(train_model, softmax, label)\nAddBookkeepingOperators(train_model)\n\n# Visualize the Caffe2 model in Tensorboard\ntv.add_graph(train_model, data)\n\n# Testing model. We will set the batch size to 100, so that the testing\n# pass is 100 iterations (10,000 images in total).\n# For the testing model, we need the data input part, the main LeNetModel\n# part, and an accuracy part. Note that init_params is set False because\n# we will be using the parameters obtained from the train model.\ntest_model = model_helper.ModelHelper(\n    name=""mnist_test"", arg_scope=arg_scope, init_params=False)\ndata, label = AddInput(\n    test_model, batch_size=100,\n    db=os.path.join(data_folder, \'mnist-test-nchw-lmdb\'),\n    db_type=\'lmdb\')\nsoftmax = AddLeNetModel(test_model, data)\nAddAccuracy(test_model, softmax, label)\n\n# Deployment model. We simply need the main LeNetModel part.\ndeploy_model = model_helper.ModelHelper(\n    name=""mnist_deploy"", arg_scope=arg_scope, init_params=False)\nAddLeNetModel(deploy_model, ""data"")\n# You may wonder what happens with the param_init_net part of the deploy_model.\n# No, we will not use them, since during deployment time we will not randomly\n# initialize the parameters, but load the parameters from the db.\n\nwith open(os.path.join(root_folder, ""train_net.pbtxt""), \'w\') as fid:\n    fid.write(str(train_model.net.Proto()))\nwith open(os.path.join(root_folder, ""train_init_net.pbtxt""), \'w\') as fid:\n    fid.write(str(train_model.param_init_net.Proto()))\nwith open(os.path.join(root_folder, ""test_net.pbtxt""), \'w\') as fid:\n    fid.write(str(test_model.net.Proto()))\nwith open(os.path.join(root_folder, ""test_init_net.pbtxt""), \'w\') as fid:\n    fid.write(str(test_model.param_init_net.Proto()))\nwith open(os.path.join(root_folder, ""deploy_net.pbtxt""), \'w\') as fid:\n    fid.write(str(deploy_model.net.Proto()))\nprint(""Protocol buffers files have been created in your root folder: "" + root_folder)\n\n# The parameter initialization network only needs to be run once.\nworkspace.RunNetOnce(train_model.param_init_net)\n# creating the network\nworkspace.CreateNet(train_model.net, overwrite=True)\n# set the number of iterations and track the accuracy & loss\ntotal_iters = 200\naccuracy = np.zeros(total_iters)\nloss = np.zeros(total_iters)\n# Now, we will manually run the network for 200 iterations.\nfor i in range(total_iters):\n    workspace.RunNet(train_model.net)\n    accuracy[i] = workspace.FetchBlob(\'accuracy\')\n    loss[i] = workspace.FetchBlob(\'loss\')\n    scalar_dict_raw = {\'accuracy\': accuracy[i], \'loss\': loss[i]}\n    scalar_dict_blobname = {\'accuracy\': \'accuracy\', \'loss\': \'loss\'}\n    # Can pass raw numerical data\n    tv.add_scalars(\'training_raw\', scalar_dict_raw, i)\n    # Can also pass blobname corresponding to data, for fetching\n    tv.add_scalars(\'training_blobname\', scalar_dict_blobname, i)\n\ndata = workspace.FetchBlob(\'data\')\nsoftmax = workspace.FetchBlob(\'softmax\')\n\n# Convolutions for this mini-batch\nconv = workspace.FetchBlob(\'conv1\')\nshape = list(conv.shape)\nshape[1] = 1\n# We can look into any channel. This of it as a feature model learned\nconv = conv[:, 15, :, :].reshape(shape)\n\n# run a test pass on the test net\nworkspace.RunNetOnce(test_model.param_init_net)\nworkspace.CreateNet(test_model.net, overwrite=True)\ntest_accuracy = np.zeros(100)\nfor i in range(100):\n    workspace.RunNet(test_model.net.Proto().name)\n    test_accuracy[i] = workspace.FetchBlob(\'accuracy\')\n    tv.add_scalar(\'test_accuracy_raw\', test_accuracy[i], i)\n    tv.add_scalar(\'test_accuracy_blobname\', \'accuracy\', i)\n# After the execution is done, let\'s plot the values.\nprint(\'test_accuracy: %f\' % test_accuracy.mean())\n'"
tensorboardX/examples/demo_custom_scalars.py,0,"b""from numpy.random import rand\nfrom tensorboardX import SummaryWriter\nimport time\n\n\nwith SummaryWriter() as writer:\n    for n_iter in range(100):\n        writer.add_scalar('twse/0050', rand(), n_iter)\n        writer.add_scalar('twse/2330', rand(), n_iter)\n        t = rand()\n        writer.add_scalar('dow/aaa', t, n_iter)\n        writer.add_scalar('dow/bbb', t - 1, n_iter)\n        writer.add_scalar('dow/ccc', t + 1, n_iter)\n        writer.add_scalar('nasdaq/aaa', rand(), n_iter)\n        writer.add_scalar('nasdaq/bbb', rand(), n_iter)\n        writer.add_scalar('nasdaq/ccc', rand(), n_iter)\n\n    layout = {'Taiwan': {'twse': ['Multiline', ['twse/0050', 'twse/2330']]},\n              'USA': {'dow': ['Margin', ['dow/aaa', 'dow/bbb', 'dow/ccc']],\n                      'nasdaq': ['Margin', ['nasdaq/aaa', 'nasdaq/bbb', 'nasdaq/ccc']]}}\n    writer.add_custom_scalars(layout)\n#    writer.add_custom_scalars(layout) second call has no effect\n\ntime.sleep(1)\n\nwith SummaryWriter() as writer:\n    for n_iter in range(100):\n        writer.add_scalar('twse/0050', rand(), n_iter)\n        writer.add_scalar('twse/2330', rand(), n_iter)\n\n    writer.add_custom_scalars_multilinechart(['twse/0050', 'twse/2330'])\n\ntime.sleep(1)\n\nwith SummaryWriter() as writer:\n    for n_iter in range(100):\n        t = rand()\n        writer.add_scalar('dow/aaa', t, n_iter)\n        writer.add_scalar('dow/bbb', t - 1, n_iter)\n        writer.add_scalar('dow/ccc', t + 1, n_iter)\n\n    writer.add_custom_scalars_marginchart(['dow/aaa', 'dow/bbb', 'dow/ccc'])\n"""
tensorboardX/examples/demo_embedding.py,10,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nfrom torch.autograd.variable import Variable\nfrom tensorboardX import SummaryWriter\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# EMBEDDING VISUALIZATION FOR A TWO-CLASSES PROBLEM\n\n# just a bunch of layers\n\n\nclass M(nn.Module):\n    def __init__(self):\n        super(M, self).__init__()\n        self.cn1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3)\n        self.cn2 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3)\n        self.fc1 = nn.Linear(in_features=128, out_features=2)\n\n    def forward(self, i):\n        i = self.cn1(i)\n        i = F.relu(i)\n        i = F.max_pool2d(i, 2)\n        i = self.cn2(i)\n        i = F.relu(i)\n        i = F.max_pool2d(i, 2)\n        i = i.view(len(i), -1)\n        i = self.fc1(i)\n        i = F.log_softmax(i, dim=1)\n        return i\n\n# get some random data around value\n\n\ndef get_data(value, shape):\n    data = torch.ones(shape) * value\n    # add some noise\n    data += torch.randn(shape)**2\n    return data\n\n\n# dataset\n# cat some data with different values\ndata = torch.cat(\n    (get_data(\n        0, (100, 1, 14, 14)), get_data(\n            0.5, (100, 1, 14, 14))), 0)\n# labels\nlabels = torch.cat((torch.zeros(100), torch.ones(100)), 0)\n# generator\ngen = DataLoader(TensorDataset(data, labels), batch_size=25, shuffle=True)\n# network\nm = M()\n#loss and optim\nloss = nn.NLLLoss()\noptimizer = torch.optim.Adam(params=m.parameters())\n# settings for train and log\nnum_epochs = 20\nembedding_log = 5\nwriter = SummaryWriter(comment=\'mnist_embedding_training\')\n\n# TRAIN\nfor epoch in range(num_epochs):\n    for j, sample in enumerate(gen):\n        n_iter = (epoch * len(gen)) + j\n        # reset grad\n        m.zero_grad()\n        optimizer.zero_grad()\n        # get batch data\n        data_batch = Variable(sample[0], requires_grad=True).float()\n        label_batch = Variable(sample[1], requires_grad=False).long()\n        # FORWARD\n        out = m(data_batch)\n        loss_value = loss(out, label_batch)\n        # BACKWARD\n        loss_value.backward()\n        optimizer.step()\n        # LOGGING\n        writer.add_scalar(\'loss\', loss_value.data.item(), n_iter)\n\n        if j % embedding_log == 0:\n            print(""loss_value:{}"".format(loss_value.data.item()))\n            # we need 3 dimension for tensor to visualize it!\n            out = torch.cat((out.data, torch.ones(len(out), 1)), 1)\n            writer.add_embedding(\n                out,\n                metadata=label_batch.data,\n                label_img=data_batch.data,\n                global_step=n_iter)\n\nwriter.close()\n\n# tensorboard --logdir runs\n# you should now see a dropdown list with all the timestep,\n# last timestep should have a visible separation between the two classes\n'"
tensorboardX/examples/demo_graph.py,21,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torch.autograd import Variable\nfrom tensorboardX import SummaryWriter\n\ndummy_input = (torch.zeros(1, 3),)\n\n\nclass LinearInLinear(nn.Module):\n    def __init__(self):\n        super(LinearInLinear, self).__init__()\n        self.l = nn.Linear(3, 5)\n\n    def forward(self, x):\n        return self.l(x)\n\nwith SummaryWriter(comment=\'LinearInLinear\') as w:\n    w.add_graph(LinearInLinear(), dummy_input, True)\n\n\nclass MultipleInput(nn.Module):\n    def __init__(self):\n        super(MultipleInput, self).__init__()\n        self.Linear_1 = nn.Linear(3, 5)\n\n\n    def forward(self, x, y):\n        return self.Linear_1(x+y)\n\nwith SummaryWriter(comment=\'MultipleInput\') as w:\n    w.add_graph(MultipleInput(), (torch.zeros(1, 3), torch.zeros(1, 3)), True)\n\nclass MultipleOutput(nn.Module):\n    def __init__(self):\n        super(MultipleOutput, self).__init__()\n        self.Linear_1 = nn.Linear(3, 5)\n        self.Linear_2 = nn.Linear(3, 7)\n\n    def forward(self, x):\n        return self.Linear_1(x), self.Linear_2(x)\n\nwith SummaryWriter(comment=\'MultipleOutput\') as w:\n    w.add_graph(MultipleOutput(), dummy_input, True)\n\n\nclass MultipleOutput_shared(nn.Module):\n    def __init__(self):\n        super(MultipleOutput_shared, self).__init__()\n        self.Linear_1 = nn.Linear(3, 5)\n\n    def forward(self, x):\n        return self.Linear_1(x), self.Linear_1(x)\n\nwith SummaryWriter(comment=\'MultipleOutput_shared\') as w:\n    w.add_graph(MultipleOutput_shared(), dummy_input, True)\n\n\nclass SimpleModel(nn.Module):\n    def __init__(self):\n        super(SimpleModel, self).__init__()\n\n    def forward(self, x):\n        return x * 2\n\n\nmodel = SimpleModel()\ndummy_input = (torch.zeros(1, 2, 3),)\n\nwith SummaryWriter(comment=\'constantModel\') as w:\n    w.add_graph(model, dummy_input, True)\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        # self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = F.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out += residual\n        out = F.relu(out)\n        return out\n\n\ndummy_input = torch.rand(1, 3, 224, 224)\n\nwith SummaryWriter(comment=\'basicblock\') as w:\n    model = BasicBlock(3, 3)\n    w.add_graph(model, (dummy_input, ), verbose=True)\n\n\n\n\nclass Net1(nn.Module):\n    def __init__(self):\n        super(Net1, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n        self.bn = nn.BatchNorm2d(20)\n\n    def forward(self, x):\n        x = F.max_pool2d(self.conv1(x), 2)\n        x = F.relu(x) + F.relu(-x)\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = self.bn(x)\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        x = F.softmax(x, dim=1)\n        return x\n\n\nclass Net2(nn.Module):\n    def __init__(self):\n        super(Net2, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        x = F.log_softmax(x, dim=1)\n        return x\n\n\ndummy_input = Variable(torch.rand(13, 1, 28, 28))\n\nmodel = Net1()\nwith SummaryWriter(comment=\'Net1\') as w:\n    w.add_graph(model, (dummy_input, ))\n\nmodel = Net2()\nwith SummaryWriter(comment=\'Net2\') as w:\n    w.add_graph(model, (dummy_input, ))\n\n\nclass SiameseNetwork(nn.Module):\n    def __init__(self):\n        super(SiameseNetwork, self).__init__()\n        self.cnn1 = Net1()\n\n    def forward_once(self, x):\n        output = self.cnn1(x)\n        return output\n\n    def forward(self, input1, input2):\n        output1 = self.forward_once(input1)\n        output2 = self.forward_once(input2)\n        return output1, output2\n\nmodel = SiameseNetwork()\nwith SummaryWriter(comment=\'SiameseNetwork\') as w:\n    w.add_graph(model, (dummy_input, dummy_input))\n\n\ndummy_input = torch.Tensor(1, 3, 224, 224)\n\nwith SummaryWriter(comment=\'alexnet\') as w:\n    model = torchvision.models.alexnet()\n    w.add_graph(model, (dummy_input, ))\n\nwith SummaryWriter(comment=\'vgg19\') as w:\n    model = torchvision.models.vgg19()\n    w.add_graph(model, (dummy_input, ))\n\nwith SummaryWriter(comment=\'densenet121\') as w:\n    model = torchvision.models.densenet121()\n    w.add_graph(model, (dummy_input, ))\n\nwith SummaryWriter(comment=\'resnet18\') as w:\n    model = torchvision.models.resnet18()\n    w.add_graph(model, (dummy_input, ))\n\n\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.i2h = nn.Linear(\n            n_categories +\n            input_size +\n            hidden_size,\n            hidden_size)\n        self.i2o = nn.Linear(\n            n_categories +\n            input_size +\n            hidden_size,\n            output_size)\n        self.o2o = nn.Linear(hidden_size + output_size, output_size)\n        self.dropout = nn.Dropout(0.1)\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, category, input, hidden):\n        input_combined = torch.cat((category, input, hidden), 1)\n        hidden = self.i2h(input_combined)\n        output = self.i2o(input_combined)\n        output_combined = torch.cat((hidden, output), 1)\n        output = self.o2o(output_combined)\n        output = self.dropout(output)\n        output = self.softmax(output)\n        return output, hidden, input\n\n    def initHidden(self):\n        return torch.zeros(1, self.hidden_size)\n\n\nn_letters = 100\nn_hidden = 128\nn_categories = 10\nrnn = RNN(n_letters, n_hidden, n_categories)\ncat = torch.Tensor(1, n_categories)\ndummy_input = torch.Tensor(1, n_letters)\nhidden = torch.Tensor(1, n_hidden)\n\n\nout, hidden, input = rnn(cat, dummy_input, hidden)\nwith SummaryWriter(comment=\'RNN\') as w:\n    w.add_graph(rnn, (cat, dummy_input, hidden), verbose=False)\n\n\n\nlstm = torch.nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\ninputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n\n# initialize the hidden state.\nhidden = (torch.randn(1, 1, 3),\n          torch.randn(1, 1, 3))\nfor i in inputs:\n    out, hidden = lstm(i.view(1, 1, -1), hidden)\n\nwith SummaryWriter(comment=\'lstm\') as w:\n    w.add_graph(lstm, (torch.randn(1, 3).view(1, 1, -1), hidden), verbose=True)\n\n\nimport pytest\nprint(\'expect error here:\')\nwith pytest.raises(Exception) as e_info:\n    dummy_input = torch.rand(1, 1, 224, 224)\n    with SummaryWriter(comment=\'basicblock_error\') as w:\n        w.add_graph(model, (dummy_input, ))  # error\n'"
tensorboardX/examples/demo_hparams.py,0,"b""from tensorboardX import SummaryWriter\nimport time\nimport random\n\n\nhparam = {'lr': [0.1, 0.01, 0.001],\n          'bsize': [1, 2, 4],\n          'n_hidden': [100, 200]}\n\nmetrics = {'accuracy', 'loss'}\n\ndef train(lr, bsize, n_hidden):\n    x = random.random()\n    return x, x*5\n\nwith SummaryWriter() as w:\n    for lr in hparam['lr']:\n        for bsize in hparam['bsize']:\n            for n_hidden in hparam['n_hidden']:\n                accu, loss = train(lr, bsize, n_hidden)\n                \n                w.add_hparams({'lr': lr, 'bsize': bsize, 'n_hidden': n_hidden},\n                                    {'accuracy': accu, 'loss': loss})\n\n"""
tensorboardX/examples/demo_matplotlib.py,0,"b""import matplotlib.pyplot as plt\nplt.switch_backend('agg')\n\nfig = plt.figure()\n\nc1 = plt.Circle((0.2, 0.5), 0.2, color='r')\nc2 = plt.Circle((0.8, 0.5), 0.2, color='r')\n\nax = plt.gca()\nax.add_patch(c1)\nax.add_patch(c2)\nplt.axis('scaled')\n\n\nfrom tensorboardX import SummaryWriter\nwriter = SummaryWriter()\nwriter.add_figure('matplotlib', fig)\nwriter.close()\n"""
tensorboardX/examples/demo_multiple_embedding.py,0,"b'import math\nimport numpy as np\nfrom tensorboardX import SummaryWriter\n\n\ndef main():\n    degrees = np.linspace(0, 3600 * math.pi / 180.0, 3600)\n    degrees = degrees.reshape(3600, 1)\n    labels = [""%d"" % (i) for i in range(0, 3600)]\n\n    with SummaryWriter() as writer:\n        # Maybe make a bunch of data that\'s always shifted in some\n        # way, and that will be hard for PCA to turn into a sphere?\n\n        for epoch in range(0, 16):\n            shift = epoch * 2 * math.pi / 16.0\n            mat = np.concatenate([\n                np.sin(shift + degrees * 2 * math.pi / 180.0),\n                np.sin(shift + degrees * 3 * math.pi / 180.0),\n                np.sin(shift + degrees * 5 * math.pi / 180.0),\n                np.sin(shift + degrees * 7 * math.pi / 180.0),\n                np.sin(shift + degrees * 11 * math.pi / 180.0)\n            ], axis=1)\n            writer.add_embedding(\n                mat=mat,\n                metadata=labels,\n                tag=""sin"",\n                global_step=epoch)\n\n            mat = np.concatenate([\n                np.cos(shift + degrees * 2 * math.pi / 180.0),\n                np.cos(shift + degrees * 3 * math.pi / 180.0),\n                np.cos(shift + degrees * 5 * math.pi / 180.0),\n                np.cos(shift + degrees * 7 * math.pi / 180.0),\n                np.cos(shift + degrees * 11 * math.pi / 180.0)\n            ], axis=1)\n            writer.add_embedding(\n                mat=mat,\n                metadata=labels,\n                tag=""cos"",\n                global_step=epoch)\n\n            mat = np.concatenate([\n                np.tan(shift + degrees * 2 * math.pi / 180.0),\n                np.tan(shift + degrees * 3 * math.pi / 180.0),\n                np.tan(shift + degrees * 5 * math.pi / 180.0),\n                np.tan(shift + degrees * 7 * math.pi / 180.0),\n                np.tan(shift + degrees * 11 * math.pi / 180.0)\n            ], axis=1)\n            writer.add_embedding(\n                mat=mat,\n                metadata=labels,\n                tag=""tan"",\n                global_step=epoch)\n\n\nif __name__ == ""__main__"":\n    main()\n\n# tensorboard --logdir runs\n# Under ""Projection, you should see\n#  48 tensor found named\n#     cos:cos-00000 to cos:cos-00016\n#     sin:sin-00000 to sin:sin-00016\n#     tan:tan-00000 to tan:tan-00016\n'"
tensorboardX/examples/demo_nvidia_smi.py,1,"b'""""""\nwrite gpu and (gpu) memory usage of nvidia cards as scalar\n""""""\nfrom tensorboardX import SummaryWriter\nimport time\nimport torch\ntry:\n    import nvidia_smi\n    nvidia_smi.nvmlInit()\n    handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)  # gpu0\nexcept ImportError:\n    print(\'This demo needs nvidia-ml-py or nvidia-ml-py3\')\n    exit()\n\n\nwith SummaryWriter() as writer:\n    x = []\n    for n_iter in range(50):\n        x.append(torch.Tensor(1000, 1000).cuda())\n        res = nvidia_smi.nvmlDeviceGetUtilizationRates(handle)\n        writer.add_scalar(\'nv/gpu\', res.gpu, n_iter)\n        res = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n        writer.add_scalar(\'nv/gpu_mem\', res.used, n_iter)\n        time.sleep(0.1)\n'"
tensorboardX/examples/demo_onnx.py,0,"b""from tensorboardX import SummaryWriter\n\nimport subprocess\nzoo_address = 'https://onnxzoo.blob.core.windows.net/models/opset_8/mnist/mnist.tar.gz'\n\nres = subprocess.call(['wget', '-nc', zoo_address])\nassert res == 0, 'cannot download example onnx model from the zoo'\nres = subprocess.call(['tar', 'xf', 'mnist.tar.gz', '-C', 'examples/', 'mnist/model.onnx'])\n\n\n\nwith SummaryWriter() as w:\n    w.add_onnx_graph('examples/mnist/model.onnx')\n    # w.add_onnx_graph('/Users/dexter/Downloads/resnet50/model.onnx')\n"""
tensorboardX/examples/demo_purge.py,0,"b""from time import sleep\nfrom tensorboardX import SummaryWriter\n\nwith SummaryWriter(logdir='runs/purge') as w:\n    for i in range(100):\n        w.add_scalar('purgetest', i, i)\n\nsleep(1.0)\n\nwith SummaryWriter(logdir='runs/purge', purge_step=42) as w:\n    # event 42~99 are removed (inclusively)\n    for i in range(42, 100):\n        w.add_scalar('purgetest', 42, i)\n"""
tensorboardX/tensorboardX/__init__.py,0,"b'""""""A module for visualization with tensorboard\n""""""\n\nfrom .record_writer import RecordWriter\nfrom .torchvis import TorchVis\nfrom .writer import FileWriter, SummaryWriter\n\n__version__ = ""1.8""  # will be overwritten if run setup.py\n'"
tensorboardX/tensorboardX/caffe2_graph.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport copy\nimport logging\nimport os\nimport re\nimport six\n\nfrom builtins import bytes\nfrom caffe2.proto import caffe2_pb2\nfrom caffe2.python import core, workspace\n\nfrom .proto.graph_pb2 import GraphDef\nfrom .proto.node_def_pb2 import NodeDef\nfrom .proto.tensor_shape_pb2 import TensorShapeProto\n\n\ndef _make_unique_name(seen, name, min_version=0):\n    \'\'\'\n    Make the name unique by appending a unique number to the name. Used for SSA.\n\n    Args:\n        seen (set): Set of names that have already been used (with respect to\n            some context).\n        name (string): The name to make unique\n        min_version (number): Starting index. Is incremented continually until\n            it can make the resulting name unique relative to \'seen\'.\n\n    Returns:\n        x (string): A version of name that is not in seen.\n    \'\'\'\n    assert name is not None\n    i = min_version\n    x = \'%s_%d\' % (name, i) if i else name\n    while x in seen:\n        i += 1\n        x = \'%s_%d\' % (name, i)\n    seen.add(x)\n    return x\n\n\ndef _rename_tensorflow_style(shapes, blob_name_tracker, ops):\n    \'\'\'\n    Convert some of the common names in Caffe2 to tensorflow.\n    NOTE: The common names in both Caffe2 and Tensorflow are currently\n        hardcoded, if either side changes at some point, then this code should\n        change as well.\n\n    Args:\n        shapes: Dictionary mapping blob names to their shapes/dimensions.\n        blob_name_tracker: Dictionary of all unique blob names (with respect to\n            some context).\n        ops: List of Caffe2 operators\n\n    Returns:\n        None. The _rename_all() call modifies blob_name_tracker and ops in-place.\n    \'\'\'\n    WEIGHT = re.compile(r""(_w)$"")\n    WEIGHT_ = re.compile(r""(_w_)"")\n    BN = re.compile(r""(_bn)$"")\n    BN_ = re.compile(r""(_bn_)"")\n    BIAS = re.compile(r""(_b)$"")\n    BIAS_ = re.compile(r""(_b_)"")\n    SCALE = re.compile(r""(_s)$"")\n    SCALE_ = re.compile(r""(_s_)"")\n    SUM = re.compile(r""(_sum)$"")\n    SUM_ = re.compile(r""(_sum_)"")\n    BRANCH = re.compile(r""(_branch)"")\n\n    def f(name):\n        inter_name = WEIGHT_.sub(\'/weight_\', WEIGHT.sub(\'/weight\', name))\n        inter_name = BN_.sub(\'/batchnorm_\', BN.sub(\'/batchnorm\', inter_name))\n        inter_name = BIAS_.sub(\'/bias_\', BIAS.sub(\'/bias\', inter_name))\n        inter_name = SCALE_.sub(\'/scale_\', SCALE.sub(\'/scale\', inter_name))\n        inter_name = SUM_.sub(\'/sum_\', SUM.sub(\'/sum\', inter_name))\n        new_name = BRANCH.sub(\'/branch\', inter_name)\n        return new_name\n    _rename_all(shapes, blob_name_tracker, ops, f)\n\n\ndef _convert_to_ssa(shapes, blob_name_tracker, ops):\n    \'\'\'\n    Convert an operator graph to SSA (i.e. out-of-place).\n    i.e. blobs will be renamed so that each blob is produced only once.\n\n    Args:\n        shapes: Dictionary mapping blob names to their shapes/dimensions.\n        blob_name_tracker: Dictionary of all unique blob names (with respect to\n            some context).\n        ops: List of Caffe2 operators\n\n    Returns:\n        None. Modifies blob_name_tracker and ops in-place.\n    \'\'\'\n    ir = core.IR(ops)\n    seen = set()\n    versioned = {}\n    new_shapes = {}\n    new_blob_name_tracker = {}\n\n    def ssa_name(name, versions):\n        assert name in versions\n        version = versions[name]\n        if (name, version) in versioned:\n            return versioned[(name, version)]\n        # Always setting name2 = `{name}_{version}` would work, but we also try\n        # to avoid a trailing `_0`, so we have to be careful not to introduce\n        # name collisions, such as (foo_1, 0) = foo_1 = (foo, 1).\n        # Note: operator names (if any) will be handled later.\n        new_name = _make_unique_name(seen, name, min_version=version)\n        versioned[(name, version)] = new_name\n        # Transfer shape.\n        if name in shapes:\n            new_shapes[new_name] = shapes[name]\n        if blob_name_tracker and name in blob_name_tracker:\n            new_blob_name_tracker[new_name] = blob_name_tracker[name]\n        return new_name\n\n    for (op, ssa) in zip(ops, ir.ssa):\n        assert op is ssa.op\n        inputs = list(op.input)\n        outputs = list(op.output)\n        del op.input[:]\n        del op.output[:]\n        op.input.extend(ssa_name(name, ssa.in_versions) for name in inputs)\n        op.output.extend(ssa_name(name, ssa.out_versions) for name in outputs)\n\n    shapes.clear()\n    shapes.update(new_shapes)\n    if blob_name_tracker:\n        blob_name_tracker.clear()\n        blob_name_tracker.update(new_blob_name_tracker)\n\n\ndef _get_blob_names(ops):\n    \'\'\'\n    Get all the operator input and output blobs and perform dedup on their names.\n\n    Args:\n        ops: List of Caffe2 operators to extract inputs and outputs from\n\n    Returns:\n        set containing distinct inputs and outputs from \'ops\'\n    \'\'\'\n    names = set()\n    for op in ops:\n        names.update(op.input)\n        names.update(op.output)\n    return {name: name for name in names}\n\n\ndef _remap_keys(old_dict, rename_fn):\n    \'\'\'\n    Rename keys of \'old_dict\' according to \'rename_fn\'.\n\n    Args:\n        old_dict: Dictionary (i.e. containing blob_name -> blob_name\n            relationships.)\n        remap_fn: Function string -> string for renaming.\n\n    Returns:\n        None. Modifies old_dict in-place.\n    \'\'\'\n    new_dict = {rename_fn(key): value for key,\n                value in six.iteritems(old_dict)}\n    old_dict.clear()\n    old_dict.update(new_dict)\n\n\ndef _rename_all(shapes, blob_name_tracker, ops, rename_fn):\n    \'\'\'\n    Rename all the names in the operators.\n\n    Args:\n        shapes: Dictionary mapping blob names to their shapes/dimensions.\n        blob_name_tracker: Dictionary of all unique blob names (with respect to\n            some context).\n        ops: List of Caffe2 operators\n        rename_fn: Function string -> string that specifies how to rename\n\n    Returns:\n        None. Modifies shapes, blob_name_tracker and ops in-place using the\n            specified \'rename_fn\'.\n    \'\'\'\n    seen = set()\n    renamed = {}\n\n    def g(name):\n        """""" Collision-free version of f.\n        """"""\n        if name is None:\n            return None\n        if name in renamed:\n            return renamed[name]\n        new_name = _make_unique_name(seen, rename_fn(name))\n        renamed[name] = new_name\n        return new_name\n\n    for op in ops:\n        inputs = list(op.input)\n        outputs = list(op.output)\n        del op.input[:]\n        del op.output[:]\n        op.input.extend(g(name) for name in inputs)\n        op.output.extend(g(name) for name in outputs)\n\n    _remap_keys(shapes, g)\n    if blob_name_tracker:\n        _remap_keys(blob_name_tracker, g)\n    # Rename all operator names (if any) independently so that the\n    # unique-fication happens only once in _fill_missing_operator_names().\n    seen.clear()\n    renamed.clear()\n    for op in ops:\n        op.name = g(op.name)\n\n\ndef _add_gradient_scope(shapes, blob_name_tracker, ops):\n    """"""\n    For all operators or blobs with name containing ""_grad"", add a\n    ""GRADIENTS/"" scope.\n    Note: breaks graph execution since the blob -> gradient mapping is\n    hardcoded.\n\n    Args:\n        shapes: Dictionary mapping blob names to their shapes/dimensions.\n        blob_name_tracker: Dictionary of all unique blob names (with respect to\n            some context).\n        ops: List of Caffe2 operators\n\n    Returns:\n        None. Modifies shapes, blob_name_tracker and ops in-place by renaming.\n    """"""\n    def f(name):\n        if \'_grad\' in name:\n            return \'GRADIENTS/{}\'.format(name)\n        else:\n            return name\n    _rename_all(shapes, blob_name_tracker, ops, f)\n\n\ndef _replace_colons(shapes, blob_name_tracker, ops, repl):\n    \'\'\'\n    `:i` has a special meaning in Tensorflow. This function replaces all colons\n    with $ to avoid any possible conflicts.\n\n    Args:\n        shapes: Dictionary mapping blob names to their shapes/dimensions.\n        blob_name_tracker: Dictionary of all unique blob names (with respect to\n            some context).\n        ops: List of Caffe2 operators\n        repl: String representing the text to replace \':\' with. Usually this is\n            \'$\'.\n\n    Returns:\n        None. Modifies blob_name_tracker in-place.\n\n    \'\'\'\n    def f(name):\n        return name.replace(\':\', repl)\n    _rename_all(shapes, blob_name_tracker, ops, f)\n\n\ndef _fill_missing_operator_names(ops):\n    \'\'\'\n    Give missing operators a name.\n    We expect C2 operators to be generally unnamed. This gives them a scope\n    (inferred from their outputs) and a name after their type. Duplicates will\n    be postfixed by an index.\n\n    Args:\n        ops: List of Caffe2 operators to assign names to.\n\n    Returns:\n        None: Modifies \'ops\' in-place.\n    \'\'\'\n    seen = set()\n    for op in ops:\n        # Make sure operator names don\'t collide with blobs.\n        seen.update(op.input)\n        seen.update(op.output)\n    for op in ops:\n        if op.name:\n            name = op.name\n        elif op.output or op.input:\n            name_list = [os.path.dirname(name)\n                         for name in op.output or op.input]\n            scope = os.path.commonprefix(name_list)\n            name = os.path.join(scope, op.type)\n        else:\n            name = op.type\n        assert(name)\n        op.name = _make_unique_name(seen, name)\n\n\ndef _tf_device(device_option):\n    \'\'\'\n    Handle the devices.\n\n    Args:\n        device_option (caffe2_pb2.DeviceOption): DeviceOption protobuf,\n            associated to an operator, that contains information such as\n            device_type (optional), cuda_gpu_id (optional), node_name (optional,\n            tells which node the operator should execute on). See caffe2.proto\n            in caffe2/proto for the full list.\n\n    Returns:\n        Formatted string representing device information contained in\n            device_option.\n    \'\'\'\n    if not device_option.HasField(""device_type""):\n        return """"\n    if device_option.device_type == caffe2_pb2.CPU or device_option.device_type == caffe2_pb2.MKLDNN:\n        return ""/cpu:*""\n    if device_option.device_type == caffe2_pb2.CUDA:\n        return ""/gpu:{}"".format(device_option.device_id)\n    raise Exception(""Unhandled device"", device_option)\n\n\ndef _add_tf_shape(attr_dict, ints):\n    \'\'\'\n    Converts a list of ints to a TensorShapeProto representing the dimensions of\n    a blob/object.\n\n    Args:\n        attr_dict: Dictionary to update (usually attributes of a Node)\n        ints: List of integers representing dimensions of some object.\n\n    Returns:\n        None. Modifies attr_dict in-place.\n    \'\'\'\n    shape_proto = TensorShapeProto()\n    for i in ints:\n        dim = TensorShapeProto.Dim()\n        dim.size = i\n        shape_proto.dim.extend([dim])\n    attr_dict[\'_output_shapes\'].list.shape.extend([shape_proto])\n\n\ndef _set_tf_attr(attr_dict, arg):\n    \'\'\'\n    Add attributes to a node. Key is the arg.name, and values can be shape,\n        floats, strings, ints or an empty list.\n\n    Args:\n        attr_dict: Dictionary to update (usually attributes of a Node)\n        arg: Object with name and data fields.\n\n    Returns:\n        None. Modifies attr_dict in-place.\n    \'\'\'\n    k = arg.name\n    if k == \'shape\' and arg.ints:\n        _add_tf_shape(attr_dict, arg.ints)\n        return\n    # Float\n    if arg.HasField(""f""):\n        attr_dict[k].f = arg.f\n        return\n    # Integer\n    if arg.HasField(""i""):\n        attr_dict[k].i = arg.i\n        return\n    # String\n    if arg.HasField(""s""):\n        attr_dict[k].s = (\n            arg.s if isinstance(arg.s, bytes) else str(arg.s).encode(\'utf-8\')\n        )\n        return\n    if arg.floats:\n        attr_dict[k].list.f.extend(arg.floats)\n        return\n    if arg.ints:\n        attr_dict[k].list.i.extend(arg.ints)\n        return\n    if arg.strings:\n        attr_dict[k].list.s.extend(\n            s if isinstance(s, bytes) else str(s).encode(\'utf-8\')\n            for s in arg.strings\n        )\n        return\n    # The value is an empty list.\n    attr_dict[k].list.s.extend([])\n\n\ndef _operator_to_node(shapes, op):\n    \'\'\'\n    Converts an operator to a node in a TF graph.\n\n    Args:\n        shapes: Dictionary mapping blob names to their shapes/dimensions.\n        op: The Caffe2 operator to convert to a TF graph node.\n\n    Returns:\n        n: The TF graph node created from op.\n    \'\'\'\n    assert op.name, op\n    n = NodeDef()\n    n.name = op.name\n    n.input.extend(op.input)\n    n.op = op.type\n    n.device = _tf_device(op.device_option)\n    if shapes:\n        # Add shapes in order.\n        for output in op.output:\n            if output not in shapes:\n                break\n            _add_tf_shape(n.attr, shapes[output])\n    for arg in op.arg:\n        _set_tf_attr(n.attr, arg)\n    return n\n\n\ndef _operator_to_node_simp(op, inter_blobs, seen):\n    \'\'\'\n    Convert the operators to nodes.\n\n    Args:\n        op: Caffe2 operator to convert to node\n        inter_blobs: Set of intermediate blobs\n        seen: Names that have already been used and are not unique\n\n    Returns:\n        nodes: Nodes representing \'op\' and the outputs of \'op\'\n    \'\'\'\n    assert op\n    nodes = []\n    outputs = [o for o in op.output if o not in inter_blobs]\n    seen.update(outputs)\n    len_outputs = len(outputs)\n    if len_outputs == 1:\n        n = NodeDef()\n        n.name = outputs[0]\n        # Here we are sure the name is unique.\n        n.input.extend(op.input)\n        n.op = op.type\n        n.device = _tf_device(op.device_option)\n        for arg in op.arg:\n            _set_tf_attr(n.attr, arg)\n        nodes.append(n)\n    elif len_outputs > 1:\n        # Create a name that is likely unique\n        if op.name:\n            name = op.name\n        else:\n            name_list = [name for name in outputs]\n            scope = os.path.commonprefix(name_list)\n            name = os.path.join(scope, op.type)\n        assert(name)\n        op.name = _make_unique_name(seen, name)\n        device = _tf_device(op.device_option)\n\n        # Create additional output nodes\n        for output in outputs:\n            n = NodeDef()\n            n.name = output\n            n.input.extend([op.name])\n            n.op = \'Blob\'\n            n.device = device\n            nodes.append(n)\n\n        # Node for the current op\n        n = NodeDef()\n        n.name = op.name\n        n.input.extend(op.input)\n        n.op = op.type\n        n.device = device\n        for arg in op.arg:\n            _set_tf_attr(n.attr, arg)\n        nodes.append(n)\n\n    return nodes\n\n\ndef _blob_to_node(producing_ops, shapes, name):\n    \'\'\'\n    Converts a blob (operator input or output) to a node in a TF graph.\n\n    Args:\n        producing_ops: Dictionary of blob name to list of\n            (producing_op, blob_index within producing_op.output) mapping.\n        shapes: Dictionary mapping blob names to their shapes/dimensions.\n        name: String representing the name of this blob.\n\n    Returns:\n        n: The TF graph node created from this blob.\n    \'\'\'\n    assert name\n    n = NodeDef()\n    n.name = name\n    # Get all ops that have the blob corresponding to \'name\' as one of their\n    # outputs. See _operators_to_graph_def.\n    produced_by = producing_ops.get(name, [])\n    if len(produced_by) > 0:\n        n.op = \'Blob\'\n    else:\n        # This blob is not produced but is instead a TF Placeholder where a\n        # value is passed in.\n        n.op = \'Placeholder\'\n    n.input.extend(\'%s:%d\' % (p_op.name, i) for p_op, i in produced_by)\n    if produced_by:\n        device = produced_by[0][0].device_option\n        if (all(producer[0].device_option == device for producer in produced_by)):\n            n.device = _tf_device(device)\n    if shapes and name in shapes:\n        _add_tf_shape(n.attr, shapes[name])\n    return n\n\n\ndef _clear_debug_info(ops, perform_clear):\n    \'\'\'\n    Removes debug information from operators, they are copious.\n\n    Args:\n        ops: List of Caffe2 operators\n        perform_clear: Boolean passed from _operators_to_graph_def specifying\n            whether to remove the debug information. This boolean is passed into\n            this function to reduce the complexity of _operators_to_graph_def.\n\n    Returns:\n        None. Modifies the list of Caffe2 operators in-place and removes the\n        \'debug_info\' field.\n\n    \'\'\'\n    if not perform_clear:\n        return\n\n    for op in ops:\n        if op.HasField(\'debug_info\'):\n            op.ClearField(\'debug_info\')\n\n\ndef _check_if_forward(blob):\n    \'\'\'\n    Blobs with names containing \'_m\' or \'grad\' are part of the backward pass.\n        This function references facebookresearch/Detectron/detectron/utils/net.py.\n\n    Args:\n        blob: The blob to inspect\n\n    Returns:\n        Boolean representing whether this blob is part of the forward pass\n    \'\'\'\n    #\n    return (blob.find(\'__m\') < 0 or blob.find(\'grad\') < 0)\n\n\ndef _check_if_cpu(blob):\n    \'\'\'\n    Check if the blob\'s name starts with \'_gpu\'.\n\n    Args:\n        blob: The blob to inspect\n\n    Returns:\n        Boolean representing whether this blob is associated with a gpu\n    \'\'\'\n    return not blob.startswith(\'_gpu\')\n\n\ndef _compute_in_out(ops):\n    \'\'\'\n    Find the input, intermediate and output nodes of a set of operators.\n\n    Args:\n        ops: List of Caffe2 operators to look through\n\n    Returns:\n        input_blobs: The input nodes of the set of operators\n        inter_blobs: The intermediate nodes of the set of operators\n        output_blobs: The output nodes of the set of operators\n    \'\'\'\n    in_blobs = set()\n    out_blobs = set()\n\n    for op in ops:\n        for input_blob in op.input:\n            in_blobs.add(input_blob)\n        for output_blob in op.output:\n            out_blobs.add(output_blob)\n\n    input_blobs = list(in_blobs.difference(out_blobs))\n    output_blobs = list(out_blobs.difference(in_blobs))\n    inter_blobs = {b for b in output_blobs if b.startswith(\'_\')}\n    output_blobs = [b for b in output_blobs if b not in inter_blobs]\n\n    return input_blobs, inter_blobs, output_blobs\n\n\ndef _filter_ops(ops, filter_fn, perform_filter):\n    \'\'\'\n    Filter unwanted operators based on criteria in \'filter_fn\'.\n\n    Args:\n        ops: List of Caffe2 operators to filter\n        filter_fn: Criteria function for whether inputs/outputs in an operator\n            should be filtered.\n        perform_filter: Boolean passed from _operators_to_graph_def specifying\n            whether to filter operators\n\n    Returns:\n        new_ops: Subset of ops containing a subset of their inputs and outputs.\n    \'\'\'\n    if not perform_filter:\n        return ops\n\n    new_ops = []\n    for op in ops:\n        inputs = list(op.input)\n        outputs = list(op.output)\n        del op.input[:]\n        del op.output[:]\n        new_inputs = [i for i in inputs if filter_fn(i)]\n        new_outputs = [o for o in outputs if filter_fn(o)]\n\n        # Only add the op if output is not empty\n        if new_outputs:\n            op.input.extend(new_inputs)\n            op.output.extend(new_outputs)\n            new_ops.append(op)\n\n    return new_ops\n\n\ndef _operators_to_graph_def(\n    shapes,\n    ops,\n    colon_replacement=\'$\',\n    with_ssa=True,\n    with_gradient_scope=True,\n    blob_name_tracker=None,\n    show_simplified=False,\n    custom_rename=None\n):\n    \'\'\'\n    Main function to convert set of operators to a graph.\n\n    Args:\n        shapes: Dictionary mapping blob names to their shapes/dimensions.\n        ops: List of Caffe2 operators, representing some computation graph\n        ### **kwargs (model_to_graph_def, nets_to_graph_def, protos_to_graph_def) ###\n        colon_replacement: Symbol to replace \':\' with. \':i\' in TF has a special\n            meaning, so we need to replace it with a non-conflicting symbol.\n        with_ssa: Boolean\n        with_gradient_scope: Boolean\n        blob_name_tracker: Dictionary tracking names of blobs (inputs/outputs\n            from operators)\n        show_simplified: Whether to show a simplified version of the model graph\n            Sets all of the following values:\n                clear_debug_info: Boolean representing whether to silence debug\n                    info (which can be very verbose)\n                show_forward_only: Boolean representing whether to only show\n                    blobs involved in the forward pass\n                show_cpu_only: Boolean representing whether to only show blobs\n                    that are not associated with a gpu\n                use_tensorflow_naming: Boolean representing whether to convert\n                    some common Caffe2 naming conventions to their Tensorflow\n                    counterparts\n        custom_rename: Function string -> string that defines a custom\n            renaming function to use.\n\n    Returns:\n        current_graph: GraphDef representing the computation graph formed by the\n            set of operators.\n    \'\'\'\n    if blob_name_tracker is not None:\n        blob_name_tracker.clear()\n    else:\n        blob_name_tracker = {}\n\n    blob_name_tracker.update(_get_blob_names(ops))\n\n    _clear_debug_info(ops, show_simplified)  # clear_debug_info\n    ops = _filter_ops(ops, _check_if_forward,\n                      show_simplified)  # show_forward_only\n    ops = _filter_ops(ops, _check_if_cpu, show_simplified)  # show_cpu_only\n    if custom_rename:\n        _rename_all(shapes, blob_name_tracker, ops, custom_rename)\n    if colon_replacement:\n        _replace_colons(shapes, blob_name_tracker, ops, colon_replacement)\n    if with_ssa:\n        _convert_to_ssa(shapes, blob_name_tracker, ops)\n    if with_gradient_scope:\n        _add_gradient_scope(shapes, blob_name_tracker, ops)\n    _fill_missing_operator_names(ops)\n    if show_simplified:  # use_tensorflow_naming\n        _rename_tensorflow_style(shapes, blob_name_tracker, ops)\n    producing_ops = {}\n    blobs = []\n    input_blobs, inter_blobs, _ = _compute_in_out(ops)\n    current_graph = GraphDef()\n    seen = set(input_blobs)\n    for op in ops:\n        nodes_from_op = _operator_to_node_simp(op, inter_blobs, seen) if \\\n            show_simplified else \\\n            [_operator_to_node(shapes, op)]  # .extend() expects an iterable\n        current_graph.node.extend(nodes_from_op)\n        for input_blob in op.input:\n            blobs.append(input_blob)\n        for i, output_blob in enumerate(op.output):\n            blobs.append(output_blob)\n            producing_ops.setdefault(output_blob, []).append((op, i))\n\n    if show_simplified:\n        # Show a cleaner, easier-to-interpret version of the model graph\n        blobs = input_blobs\n\n    for blob in blobs:\n        current_graph.node.extend([_blob_to_node(producing_ops, {}, blob)])\n\n    return current_graph\n\n\ndef _propagate_device_option(net_def):\n    \'\'\'\n    Propagate the device options from net to operators.\n\n    Args:\n        net_def: A caffe2_pb2.NetDef representing a computation graph. The graph\n            consists of Caffe2 operators.\n\n    Returns:\n        None. Iterates through all ops contained within the net. For each op,\n            modifies the op device_option in-place to be the net device_option\n            if the op has no pre-existing device_option, and leaves the op as-is\n            if it already has a device_option.\n    \'\'\'\n    if not net_def.HasField(""device_option""):\n        return\n    for op in net_def.op:\n        if not op.HasField(""device_option""):\n            op.device_option.CopyFrom(net_def.device_option)\n\n\ndef _try_get_shapes(nets):\n    \'\'\'\n    Get missing shapes for all blobs contained in the nets.\n\n    Args:\n        nets: List of core.Net to extract blob shape information from.\n\n    Returns:\n        Dictionary containing blob name to shape/dimensions mapping. The net\n            is a computation graph that is composed of operators, and the\n            operators have input and output blobs, each with their own dims.\n    \'\'\'\n    try:\n        # Note: this will inspect the workspace for better or worse.\n        # We don\'t care about the types, only the shapes\n        shapes, _ = workspace.InferShapesAndTypes(nets)\n        return shapes\n    except Exception as e:\n        logging.warning(\'Failed to compute shapes: %s\', e)\n        return {}\n\n\ndef model_to_graph_def(model, **kwargs):\n    \'\'\'\n    Convert a Caffe2 model to a Tensorflow graph. This function extracts\n    \'param_init_net\' and \'net\' from the model and passes it to nets_to_graph()\n    for further processing.\n\n    Args:\n        model (cnn.CNNModelHelper, model_helper.ModelHelper): The model to\n            extract the nets (instances of core.Net) from.\n\n    Returns:\n        Call to nets_to_graph_def() with extracted \'param_init_net\', \'net\' and\n            **kwargs. See _operators_to_graph_def for detailed **kwargs.\n    \'\'\'\n    nets = [model.param_init_net, model.net]\n    return nets_to_graph_def(nets, **kwargs)\n\n\ndef nets_to_graph_def(nets, shapes=None, **kwargs):\n    \'\'\'\n    Convert a set of Caffe2 nets to a Tensorflow graph.\n\n    Args:\n        nets: List of core.Nets. core.Net is a wrapper around a NetDef protobuf.\n            The corresponding protobuf can be extracted using .Proto().\n        shapes: Dictionary mapping blob names to their shapes/dimensions.\n\n    Returns:\n        Call to protos_to_graph_def() with the extracted NetDef protobufs and\n            **kwargs. See _operators_to_graph_def for detailed **kwargs.\n    \'\'\'\n    # if shapes is None:\n    #     shapes = _try_get_shapes(nets)\n    # _try_get_shapes(nets) depends on workspace.InferShapesAndTypes(nets),\n    # which is currently broken (segfault). We omit the shapes for now.\n    shapes = {}\n    nets = [copy.deepcopy(net.Proto()) for net in nets]\n    shapes = copy.deepcopy(shapes)\n    return protos_to_graph_def(nets, shapes, **kwargs)\n\n\ndef protos_to_graph_def(net_defs, shapes=None, **kwargs):\n    \'\'\'\n    Convert a set of Caffe2 net definitions to a Tensorflow graph.\n\n    Args:\n        net_defs: List of caffe2_pb2.NetDef protobufs representing computation\n            graphs.\n        shapes: Dictionary mapping blob names to their shapes/dimensions.\n\n    Returns:\n        Call to _operators_to_graph_def() with the extracted operators from the\n            NetDefs and **kwargs. See _operators_to_graph_def for detailed\n            **kwargs.\n    \'\'\'\n    for net in net_defs:\n        _propagate_device_option(net)\n    shapes = copy.deepcopy(shapes or {})\n    ops = [op for net_def in net_defs for op in net_def.op]\n    return _operators_to_graph_def(shapes, ops, **kwargs)\n'"
tensorboardX/tensorboardX/crc32c.py,0,"b'# https://www.ietf.org/rfc/rfc3309.txt\nimport array\nimport os\n\ntry:\n    if os.environ.get(\'CRC32C_SW_MODE\', None) is None:\n        os.environ[\'CRC32C_SW_MODE\'] = \'auto\'\n    from crc32c import crc32 as _crc32c_native\nexcept ImportError:\n    _crc32c_native = None\n\n\nCRC_TABLE = (\n    0x00000000, 0xf26b8303, 0xe13b70f7, 0x1350f3f4,\n    0xc79a971f, 0x35f1141c, 0x26a1e7e8, 0xd4ca64eb,\n    0x8ad958cf, 0x78b2dbcc, 0x6be22838, 0x9989ab3b,\n    0x4d43cfd0, 0xbf284cd3, 0xac78bf27, 0x5e133c24,\n    0x105ec76f, 0xe235446c, 0xf165b798, 0x030e349b,\n    0xd7c45070, 0x25afd373, 0x36ff2087, 0xc494a384,\n    0x9a879fa0, 0x68ec1ca3, 0x7bbcef57, 0x89d76c54,\n    0x5d1d08bf, 0xaf768bbc, 0xbc267848, 0x4e4dfb4b,\n    0x20bd8ede, 0xd2d60ddd, 0xc186fe29, 0x33ed7d2a,\n    0xe72719c1, 0x154c9ac2, 0x061c6936, 0xf477ea35,\n    0xaa64d611, 0x580f5512, 0x4b5fa6e6, 0xb93425e5,\n    0x6dfe410e, 0x9f95c20d, 0x8cc531f9, 0x7eaeb2fa,\n    0x30e349b1, 0xc288cab2, 0xd1d83946, 0x23b3ba45,\n    0xf779deae, 0x05125dad, 0x1642ae59, 0xe4292d5a,\n    0xba3a117e, 0x4851927d, 0x5b016189, 0xa96ae28a,\n    0x7da08661, 0x8fcb0562, 0x9c9bf696, 0x6ef07595,\n    0x417b1dbc, 0xb3109ebf, 0xa0406d4b, 0x522bee48,\n    0x86e18aa3, 0x748a09a0, 0x67dafa54, 0x95b17957,\n    0xcba24573, 0x39c9c670, 0x2a993584, 0xd8f2b687,\n    0x0c38d26c, 0xfe53516f, 0xed03a29b, 0x1f682198,\n    0x5125dad3, 0xa34e59d0, 0xb01eaa24, 0x42752927,\n    0x96bf4dcc, 0x64d4cecf, 0x77843d3b, 0x85efbe38,\n    0xdbfc821c, 0x2997011f, 0x3ac7f2eb, 0xc8ac71e8,\n    0x1c661503, 0xee0d9600, 0xfd5d65f4, 0x0f36e6f7,\n    0x61c69362, 0x93ad1061, 0x80fde395, 0x72966096,\n    0xa65c047d, 0x5437877e, 0x4767748a, 0xb50cf789,\n    0xeb1fcbad, 0x197448ae, 0x0a24bb5a, 0xf84f3859,\n    0x2c855cb2, 0xdeeedfb1, 0xcdbe2c45, 0x3fd5af46,\n    0x7198540d, 0x83f3d70e, 0x90a324fa, 0x62c8a7f9,\n    0xb602c312, 0x44694011, 0x5739b3e5, 0xa55230e6,\n    0xfb410cc2, 0x092a8fc1, 0x1a7a7c35, 0xe811ff36,\n    0x3cdb9bdd, 0xceb018de, 0xdde0eb2a, 0x2f8b6829,\n    0x82f63b78, 0x709db87b, 0x63cd4b8f, 0x91a6c88c,\n    0x456cac67, 0xb7072f64, 0xa457dc90, 0x563c5f93,\n    0x082f63b7, 0xfa44e0b4, 0xe9141340, 0x1b7f9043,\n    0xcfb5f4a8, 0x3dde77ab, 0x2e8e845f, 0xdce5075c,\n    0x92a8fc17, 0x60c37f14, 0x73938ce0, 0x81f80fe3,\n    0x55326b08, 0xa759e80b, 0xb4091bff, 0x466298fc,\n    0x1871a4d8, 0xea1a27db, 0xf94ad42f, 0x0b21572c,\n    0xdfeb33c7, 0x2d80b0c4, 0x3ed04330, 0xccbbc033,\n    0xa24bb5a6, 0x502036a5, 0x4370c551, 0xb11b4652,\n    0x65d122b9, 0x97baa1ba, 0x84ea524e, 0x7681d14d,\n    0x2892ed69, 0xdaf96e6a, 0xc9a99d9e, 0x3bc21e9d,\n    0xef087a76, 0x1d63f975, 0x0e330a81, 0xfc588982,\n    0xb21572c9, 0x407ef1ca, 0x532e023e, 0xa145813d,\n    0x758fe5d6, 0x87e466d5, 0x94b49521, 0x66df1622,\n    0x38cc2a06, 0xcaa7a905, 0xd9f75af1, 0x2b9cd9f2,\n    0xff56bd19, 0x0d3d3e1a, 0x1e6dcdee, 0xec064eed,\n    0xc38d26c4, 0x31e6a5c7, 0x22b65633, 0xd0ddd530,\n    0x0417b1db, 0xf67c32d8, 0xe52cc12c, 0x1747422f,\n    0x49547e0b, 0xbb3ffd08, 0xa86f0efc, 0x5a048dff,\n    0x8ecee914, 0x7ca56a17, 0x6ff599e3, 0x9d9e1ae0,\n    0xd3d3e1ab, 0x21b862a8, 0x32e8915c, 0xc083125f,\n    0x144976b4, 0xe622f5b7, 0xf5720643, 0x07198540,\n    0x590ab964, 0xab613a67, 0xb831c993, 0x4a5a4a90,\n    0x9e902e7b, 0x6cfbad78, 0x7fab5e8c, 0x8dc0dd8f,\n    0xe330a81a, 0x115b2b19, 0x020bd8ed, 0xf0605bee,\n    0x24aa3f05, 0xd6c1bc06, 0xc5914ff2, 0x37faccf1,\n    0x69e9f0d5, 0x9b8273d6, 0x88d28022, 0x7ab90321,\n    0xae7367ca, 0x5c18e4c9, 0x4f48173d, 0xbd23943e,\n    0xf36e6f75, 0x0105ec76, 0x12551f82, 0xe03e9c81,\n    0x34f4f86a, 0xc69f7b69, 0xd5cf889d, 0x27a40b9e,\n    0x79b737ba, 0x8bdcb4b9, 0x988c474d, 0x6ae7c44e,\n    0xbe2da0a5, 0x4c4623a6, 0x5f16d052, 0xad7d5351,\n)\n\nCRC_INIT = 0\n\n_MASK = 0xFFFFFFFF\n\n\ndef crc_update(crc, data):\n    """"""Update CRC-32C checksum with data.\n\n    Args:\n      crc: 32-bit checksum to update as long.\n      data: byte array, string or iterable over bytes.\n\n    Returns:\n      32-bit updated CRC-32C as long.\n    """"""\n\n    if type(data) != array.array or data.itemsize != 1:\n        buf = array.array(""B"", data)\n    else:\n        buf = data\n\n    crc ^= _MASK\n    for b in buf:\n        table_index = (crc ^ b) & 0xff\n        crc = (CRC_TABLE[table_index] ^ (crc >> 8)) & _MASK\n    return crc ^ _MASK\n\n\ndef crc_finalize(crc):\n    """"""Finalize CRC-32C checksum.\n\n    This function should be called as last step of crc calculation.\n\n    Args:\n      crc: 32-bit checksum as long.\n\n    Returns:\n      finalized 32-bit checksum as long\n    """"""\n    return crc & _MASK\n\n\ndef _crc32c(data):\n    """"""Compute CRC-32C checksum of the data.\n\n    Args:\n      data: byte array, string or iterable over bytes.\n\n    Returns:\n      32-bit CRC-32C checksum of data as long.\n    """"""\n    return crc_finalize(crc_update(CRC_INIT, data))\n\n\ncrc32c = _crc32c if _crc32c_native is None else _crc32c_native\n'"
tensorboardX/tensorboardX/embedding.py,0,"b'import os\n\n\ndef make_tsv(metadata, save_path, metadata_header=None):\n    if not metadata_header:\n        metadata = [str(x) for x in metadata]\n    else:\n        assert len(metadata_header) == len(metadata[0]), \\\n            \'len of header must be equal to the number of columns in metadata\'\n        metadata = [\'\\t\'.join(str(e) for e in l)\n                    for l in [metadata_header] + metadata]\n    import sys\n    if sys.version_info[0] == 3:\n        with open(os.path.join(save_path, \'metadata.tsv\'), \'w\', encoding=\'utf8\') as f:\n            for x in metadata:\n                f.write(x + \'\\n\')\n    else:\n        with open(os.path.join(save_path, \'metadata.tsv\'), \'wb\') as f:\n            for x in metadata:\n                f.write((x + \'\\n\').encode(\'utf-8\'))\n\n\n# https://github.com/tensorflow/tensorboard/issues/44 image label will be squared\ndef make_sprite(label_img, save_path):\n    import math\n    import numpy as np\n    from .x2num import make_np\n    from .utils import make_grid\n    from PIL import Image\n    # this ensures the sprite image has correct dimension as described in\n    # https://www.tensorflow.org/get_started/embedding_viz\n    # There are some constraints for the sprite image:\n    # 1. The sprite image should be square.\n    # 2. Each image patch in the sprite image should be square.\n    # 2. The content is row major order, so we can padding the image on the\n    #    bottom, but not on the right, otherwise, TB will treat some padded location\n    #    as images to be shown.\n    # args: label_img: tensor in NCHW\n\n    assert label_img.shape[2] == label_img.shape[3], \'Image should be square, see tensorflow/tensorboard#670\'\n    total_pixels = label_img.shape[0] * label_img.shape[2] * label_img.shape[3]\n    pixels_one_side = total_pixels ** 0.5\n    number_of_images_per_row = int(math.ceil(pixels_one_side / label_img.shape[3]))\n    arranged_img_CHW = make_grid(make_np(label_img), ncols=number_of_images_per_row)\n    arranged_img_HWC = arranged_img_CHW.transpose(1, 2, 0)  # chw -> hwc\n\n    arranged_augment_square_HWC = np.ndarray((arranged_img_CHW.shape[2], arranged_img_CHW.shape[2], 3))\n    arranged_augment_square_HWC[:arranged_img_HWC.shape[0], :, :] = arranged_img_HWC\n    im = Image.fromarray(np.uint8((arranged_augment_square_HWC * 255).clip(0, 255)))\n    im.save(os.path.join(save_path, \'sprite.png\'))\n\n\ndef append_pbtxt(metadata, label_img, save_path, subdir, global_step, tag):\n    from posixpath import join\n    with open(os.path.join(save_path, \'projector_config.pbtxt\'), \'a\') as f:\n        # step = os.path.split(save_path)[-1]\n        f.write(\'embeddings {\\n\')\n        f.write(\'tensor_name: ""{}:{}""\\n\'.format(\n            tag, str(global_step).zfill(5)))\n        f.write(\'tensor_path: ""{}""\\n\'.format(join(subdir, \'tensors.tsv\')))\n        if metadata is not None:\n            f.write(\'metadata_path: ""{}""\\n\'.format(\n                join(subdir, \'metadata.tsv\')))\n        if label_img is not None:\n            f.write(\'sprite {\\n\')\n            f.write(\'image_path: ""{}""\\n\'.format(join(subdir, \'sprite.png\')))\n            f.write(\'single_image_dim: {}\\n\'.format(label_img.shape[3]))\n            f.write(\'single_image_dim: {}\\n\'.format(label_img.shape[2]))\n            f.write(\'}\\n\')\n        f.write(\'}\\n\')\n\n\ndef make_mat(matlist, save_path):\n    with open(os.path.join(save_path, \'tensors.tsv\'), \'w\') as f:\n        for x in matlist:\n            x = [str(i.item()) for i in x]\n            f.write(\'\\t\'.join(x) + \'\\n\')\n'"
tensorboardX/tensorboardX/event_file_writer.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Writes events to disk in a logdir.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport socket\nimport threading\nimport time\n\nimport six\n\nfrom .proto import event_pb2\nfrom .record_writer import RecordWriter, directory_check\n\n\nclass EventsWriter(object):\n    \'\'\'Writes `Event` protocol buffers to an event file.\'\'\'\n\n    def __init__(self, file_prefix, filename_suffix=\'\'):\n        \'\'\'\n        Events files have a name of the form\n        \'/some/file/path/events.out.tfevents.[timestamp].[hostname]\'\n        \'\'\'\n        self._file_name = file_prefix + "".out.tfevents."" + str(time.time())[:10] + ""."" +\\\n            socket.gethostname() + filename_suffix\n        self._num_outstanding_events = 0\n        self._py_recordio_writer = RecordWriter(self._file_name)\n        # Initialize an event instance.\n        self._event = event_pb2.Event()\n        self._event.wall_time = time.time()\n        self._event.file_version = \'brain.Event:2\'\n        self._lock = threading.Lock()\n        self.write_event(self._event)\n\n    def write_event(self, event):\n        \'\'\'Append ""event"" to the file.\'\'\'\n\n        # Check if event is of type event_pb2.Event proto.\n        if not isinstance(event, event_pb2.Event):\n            raise TypeError(""Expected an event_pb2.Event proto, ""\n                            "" but got %s"" % type(event))\n        return self._write_serialized_event(event.SerializeToString())\n\n    def _write_serialized_event(self, event_str):\n        with self._lock:\n            self._num_outstanding_events += 1\n            self._py_recordio_writer.write(event_str)\n\n    def flush(self):\n        \'\'\'Flushes the event file to disk.\'\'\'\n        with self._lock:\n            self._num_outstanding_events = 0\n            self._py_recordio_writer.flush()\n        return True\n\n    def close(self):\n        \'\'\'Call self.flush().\'\'\'\n        return_value = self.flush()\n        with self._lock:\n            self._py_recordio_writer.close()\n        return return_value\n\n\nclass EventFileWriter(object):\n    """"""Writes `Event` protocol buffers to an event file.\n\n    The `EventFileWriter` class creates an event file in the specified directory,\n    and asynchronously writes Event protocol buffers to the file. The Event file\n    is encoded using the tfrecord format, which is similar to RecordIO.\n    """"""\n\n    def __init__(self, logdir, max_queue_size=10, flush_secs=120, filename_suffix=\'\'):\n        """"""Creates a `EventFileWriter` and an event file to write to.\n\n        On construction the summary writer creates a new event file in `logdir`.\n        This event file will contain `Event` protocol buffers, which are written to\n        disk via the add_event method.\n        The other arguments to the constructor control the asynchronous writes to\n        the event file:\n\n        Args:\n          logdir: A string. Directory where event file will be written.\n          max_queue_size: Integer. Size of the queue for pending events and summaries.\n          flush_secs: Number. How often, in seconds, to flush the\n            pending events and summaries to disk.\n        """"""\n        self._logdir = logdir\n        directory_check(self._logdir)\n        self._event_queue = six.moves.queue.Queue(max_queue_size)\n        self._ev_writer = EventsWriter(os.path.join(\n            self._logdir, ""events""), filename_suffix)\n        self._flush_secs = flush_secs\n        self._closed = False\n        self._worker = _EventLoggerThread(self._event_queue, self._ev_writer,\n                                          flush_secs)\n\n        self._worker.start()\n\n    def get_logdir(self):\n        """"""Returns the directory where event file will be written.""""""\n        return self._logdir\n\n    def reopen(self):\n        """"""Reopens the EventFileWriter.\n        Can be called after `close()` to add more events in the same directory.\n        The events will go into a new events file and a new write/flush worker\n        is created. Does nothing if the EventFileWriter was not closed.\n        """"""\n        if self._closed:\n            self._closed = False\n            self._worker = _EventLoggerThread(\n                self._event_queue, self._ev_writer, self._flush_secs\n            )\n            self._worker.start()\n\n    def add_event(self, event):\n        """"""Adds an event to the event file.\n\n        Args:\n          event: An `Event` protocol buffer.\n        """"""\n        if not self._closed:\n            self._event_queue.put(event)\n\n    def flush(self):\n        """"""Flushes the event file to disk.\n\n        Call this method to make sure that all pending events have been written to\n        disk.\n        """"""\n        if not self._closed:\n            self._event_queue.join()\n            self._ev_writer.flush()\n\n    def close(self):\n        """"""Performs a final flush of the event file to disk, stops the\n        write/flush worker and closes the file. Call this method when you do not\n        need the summary writer anymore.\n        """"""\n        if not self._closed:\n            self.flush()\n            self._worker.stop()\n            self._ev_writer.close()\n            self._closed = True\n\n\nclass _EventLoggerThread(threading.Thread):\n    """"""Thread that logs events.""""""\n\n    def __init__(self, queue, record_writer, flush_secs):\n        """"""Creates an _EventLoggerThread.\n        Args:\n          queue: A Queue from which to dequeue data.\n          record_writer: An data writer. Used to log brain events for\n           the visualizer.\n          flush_secs: How often, in seconds, to flush the\n            pending file to disk.\n        """"""\n        threading.Thread.__init__(self)\n        self.daemon = True\n        self._queue = queue\n        self._record_writer = record_writer\n        self._flush_secs = flush_secs\n        # The first data will be flushed immediately.\n        self._next_flush_time = 0\n        self._has_pending_data = False\n        self._shutdown_signal = object()\n\n    def stop(self):\n        self._queue.put(self._shutdown_signal)\n        self.join()\n\n    def run(self):\n        # Here wait on the queue until an data appears, or till the next\n        # time to flush the writer, whichever is earlier. If we have an\n        # data, write it. If not, an empty queue exception will be raised\n        # and we can proceed to flush the writer.\n        while True:\n            now = time.time()\n            queue_wait_duration = self._next_flush_time - now\n            data = None\n            try:\n                if queue_wait_duration > 0:\n                    data = self._queue.get(True, queue_wait_duration)\n                else:\n                    data = self._queue.get(False)\n\n                if data == self._shutdown_signal:\n                    return\n                self._record_writer.write_event(data)\n                self._has_pending_data = True\n            except six.moves.queue.Empty:\n                pass\n            finally:\n                if data:\n                    self._queue.task_done()\n\n            now = time.time()\n            if now > self._next_flush_time:\n                if self._has_pending_data:\n                    # Small optimization - if there are no pending data,\n                    # there\'s no need to flush, since each flush can be\n                    # expensive (e.g. uploading a new file to a server).\n                    self._record_writer.flush()\n                    self._has_pending_data = False\n                # Do it again in flush_secs.\n                self._next_flush_time = now + self._flush_secs\n'"
tensorboardX/tensorboardX/onnx_graph.py,0,"b""from .proto.graph_pb2 import GraphDef\nfrom .proto.node_def_pb2 import NodeDef\nfrom .proto.versions_pb2 import VersionDef\nfrom .proto.attr_value_pb2 import AttrValue\nfrom .proto.tensor_shape_pb2 import TensorShapeProto\n\n\ndef load_onnx_graph(fname):\n    import onnx\n    m = onnx.load(fname)\n    g = m.graph\n    return parse(g)\n\n\ndef parse(graph):\n    nodes_proto = []\n    nodes = []\n    import itertools\n    for node in itertools.chain(graph.input, graph.output):\n        nodes_proto.append(node)\n\n    for node in nodes_proto:\n        print(node.name)\n        shapeproto = TensorShapeProto(\n            dim=[TensorShapeProto.Dim(size=d.dim_value) for d in node.type.tensor_type.shape.dim])\n        nodes.append(NodeDef(\n            name=node.name.encode(encoding='utf_8'),\n            op='Variable',\n            input=[],\n            attr={\n                'dtype': AttrValue(type=node.type.tensor_type.elem_type),\n                'shape': AttrValue(shape=shapeproto),\n            })\n        )\n\n    for node in graph.node:\n        attr = []\n        for s in node.attribute:\n            attr.append(' = '.join([str(f[1]) for f in s.ListFields()]))\n        attr = ', '.join(attr).encode(encoding='utf_8')\n        print(node.output[0])\n        nodes.append(NodeDef(\n            name=node.output[0].encode(encoding='utf_8'),\n            op=node.op_type,\n            input=node.input,\n            attr={'parameters': AttrValue(s=attr)},\n        ))\n\n    # two pass token replacement, appends opname to object id\n    mapping = {}\n    for node in nodes:\n        mapping[node.name] = node.op + '_' + node.name\n\n    return GraphDef(node=nodes, versions=VersionDef(producer=22))\n"""
tensorboardX/tensorboardX/proto_graph.py,0,"b'from .proto.graph_pb2 import GraphDef\nfrom .proto.node_def_pb2 import NodeDef\nfrom .proto.versions_pb2 import VersionDef\nfrom .proto.attr_value_pb2 import AttrValue\nfrom .proto.tensor_shape_pb2 import TensorShapeProto\n\n\ndef attr_value_proto(dtype, shape, s):\n    """"""Creates a dict of objects matching\n    https://github.com/tensorflow/tensorboard/blob/master/tensorboard/compat/proto/attr_value.proto\n    specifically designed for a NodeDef. The values have been\n    reverse engineered from standard TensorBoard logged data.\n    """"""\n    attr = {}\n    if s is not None:\n        attr[\'attr\'] = AttrValue(s=s.encode(encoding=\'utf_8\'))\n    if shape is not None:\n        shapeproto = tensor_shape_proto(shape)\n        attr[\'_output_shapes\'] = AttrValue(list=AttrValue.ListValue(shape=[shapeproto]))\n    return attr\n\n\ndef tensor_shape_proto(outputsize):\n    """"""Creates an object matching\n    https://github.com/tensorflow/tensorboard/blob/master/tensorboard/compat/proto/tensor_shape.proto\n    """"""\n    return TensorShapeProto(dim=[TensorShapeProto.Dim(size=d) for d in outputsize])\n\n\ndef node_proto(name,\n               op=\'UnSpecified\',\n               input=None,\n               dtype=None,\n               shape=None,  # type: tuple\n               outputsize=None,\n               attributes=\'\'\n               ):\n    """"""Creates an object matching\n    https://github.com/tensorflow/tensorboard/blob/master/tensorboard/compat/proto/node_def.proto\n    """"""\n    if input is None:\n        input = []\n    if not isinstance(input, list):\n        input = [input]\n    return NodeDef(\n        name=name.encode(encoding=\'utf_8\'),\n        op=op,\n        input=input,\n        attr=attr_value_proto(dtype, outputsize, attributes)\n    )\n'"
tensorboardX/tensorboardX/pytorch_graph.py,5,"b'import logging\nimport time\nfrom collections import OrderedDict\nfrom .proto.attr_value_pb2 import AttrValue\nfrom .proto.graph_pb2 import GraphDef\nfrom .proto.node_def_pb2 import NodeDef\nfrom .proto.step_stats_pb2 import RunMetadata, StepStats, DeviceStepStats, NodeExecStats, AllocatorMemoryUsed\nfrom .proto.tensor_shape_pb2 import TensorShapeProto\nfrom .proto.versions_pb2 import VersionDef\nfrom .proto_graph import node_proto\n\nmethods_OP = [\'attributeNames\', \'hasMultipleOutputs\', \'hasUses\', \'inputs\',\n              \'kind\', \'outputs\', \'outputsSize\', \'scopeName\']\nmethods_IO = [\'node\', \'offset\', \'debugName\']  # \'unique\' <int> , \'type\' <Tensor<class \'torch._C.Type\'>>\n\nbackward_mode = False\n\nclass NodeBase(object):\n    def __init__(self,\n                 debugName=None,\n                 inputs=None,\n                 scope=None,\n                 tensor_size=None,\n                 op_type=\'UnSpecified\',\n                 attributes=\'\'):\n        self.debugName = debugName\n        self.inputs = inputs\n        self.tensor_size = tensor_size\n        self.kind = op_type\n        self.attributes = attributes\n        if scope is not None:\n            self.scope = scope\n\n    def __repr__(self):\n        repr = []\n        repr.append(str(type(self)))\n        for m in dir(self):\n            if \'__\' not in m:\n                repr.append(m + \': \' + str(getattr(self, m)) + str(type(getattr(self, m))))\n        return \'\\n\'.join(repr) + \'\\n\\n\'\n\n\nclass NodePy(NodeBase):\n    def __init__(self, node_cpp, valid_methods):\n        super(NodePy, self).__init__(node_cpp)\n        valid_methods = valid_methods[:]\n        self.inputs = []\n        global backward_mode\n        for m in valid_methods:\n            if m == \'inputs\' or m == \'outputs\':\n                list_of_node = list(getattr(node_cpp, m)())\n                io_unique_names = []\n                io_tensor_sizes = []\n                for n in list_of_node:\n                    if backward_mode:\n                        io_unique_names.append(n.uniqueName())\n                    else:\n                        io_unique_names.append(n.debugName())\n\n                    if n.type().kind() == \'CompleteTensorType\':\n                        io_tensor_sizes.append(n.type().sizes())\n                    else:\n                        io_tensor_sizes.append(None)\n\n                setattr(self, m, io_unique_names)\n                setattr(self, m + \'tensor_size\', io_tensor_sizes)\n\n            else:\n                if m == \'debugName\' and backward_mode:\n                    setattr(self, m, getattr(node_cpp, \'uniqueName\')())\n                else:\n                    setattr(self, m, getattr(node_cpp, m)())\n\n\nclass NodePyIO(NodePy):\n    def __init__(self, node_cpp, input_or_output=None):\n        super(NodePyIO, self).__init__(node_cpp, methods_IO)\n        try:\n            tensor_size = node_cpp.type().sizes()\n        except RuntimeError:\n            tensor_size = [1, ]  # fail when constant model is used.\n        self.tensor_size = tensor_size\n        # Kind attribute string is purely descriptive and will be shown\n        # in detailed information for the node in TensorBoard\'s graph plugin.\n        #\n        # NodePyOP nodes get this from their kind() method.\n        self.kind = \'Parameter\'\n        if input_or_output:\n            self.input_or_output = input_or_output\n            self.kind = \'IO Node\'\n\n\nclass NodePyOP(NodePy):\n    def __init__(self, node_cpp):\n        super(NodePyOP, self).__init__(node_cpp, methods_OP)\n        # Replace single quote which causes strange behavior in TensorBoard\n        # TODO: See if we can remove this in the future\n        self.attributes = str({k: node_cpp[k] for k in node_cpp.attributeNames()}).replace(""\'"", \' \')\n        self.kind = node_cpp.kind()\n\n\nclass GraphPy(object):\n    """"""Helper class to convert torch.nn.Module to GraphDef proto and visualization\n    with TensorBoard.\n\n    GraphDef generation operates in two passes:\n\n    In the first pass, all nodes are read and saved to two lists.\n    One list is for input/output nodes (nodes_io), which only have inbound\n    or outbound connections, but not both. Another list is for internal\n    operator nodes (nodes_op). The first pass also saves all scope name\n    appeared in the nodes in scope_name_appeared list for later processing.\n\n    In the second pass, scope names are fully applied to all nodes.\n    debugNameToScopedName is a mapping from a node\'s ID to its fully qualified\n    scope name. e.g. Net1/Linear[0]/1. Unfortunately torch.jit doesn\'t have\n    totally correct scope output, so this is nontrivial. The function\n    populate_namespace_from_OP_to_IO and find_common_root are used to\n    assign scope name to a node based on the connection between nodes\n    in a heuristic kind of way. Bookkeeping is done with shallowest_scope_name\n    and scope_name_appeared.\n    """"""\n    def __init__(self):\n        self.nodes_op = []\n        self.nodes_io = OrderedDict()\n        self.unique_name_to_scoped_name = {}\n        self.shallowest_scope_name = \'default\'\n        self.scope_name_appeared = []\n\n    def append(self, x):\n        if isinstance(x, NodePyIO):\n            self.nodes_io[x.debugName] = x\n        if isinstance(x, NodePyOP):\n            self.nodes_op.append(x)\n            for node_output, outputSize in zip(x.outputs, x.outputstensor_size):\n                self.scope_name_appeared.append(x.scopeName)\n                self.nodes_io[node_output] = NodeBase(node_output,\n                                                      x.inputs,\n                                                      x.scopeName,\n                                                      outputSize,\n                                                      op_type=x.kind,\n                                                      attributes=x.attributes)\n\n    def printall(self):\n        print(\'all nodes\')\n        for node in self.nodes_op:\n            print(node)\n        for key in self.nodes_io:\n            print(self.nodes_io[key])\n\n    def find_common_root(self):\n        for fullscope in self.scope_name_appeared:\n            if fullscope:\n                self.shallowest_scope_name = fullscope.split(\'/\')[0]\n\n    def populate_namespace_from_OP_to_IO(self):\n        for node in self.nodes_op:\n            for input_node_id in node.inputs:\n                self.unique_name_to_scoped_name[input_node_id] = node.scopeName + \'/\' + input_node_id\n\n        for key, node in self.nodes_io.items():\n            if type(node) == NodeBase:\n                self.unique_name_to_scoped_name[key] = node.scope + \'/\' + node.debugName\n            if hasattr(node, \'input_or_output\'):\n                self.unique_name_to_scoped_name[key] = node.input_or_output + \'/\' + node.debugName\n            if hasattr(node, \'scope\'):\n                if node.scope == \'\' and self.shallowest_scope_name:\n                    self.unique_name_to_scoped_name[node.debugName] = \\\n                        self.shallowest_scope_name + \'/\' + node.debugName\n\n        # replace name\n        for key, node in self.nodes_io.items():\n            self.nodes_io[key].inputs = \\\n                [self.unique_name_to_scoped_name[node_input_id] for node_input_id in node.inputs]\n            if node.debugName in self.unique_name_to_scoped_name:\n                self.nodes_io[key].debugName = self.unique_name_to_scoped_name[node.debugName]\n\n    def to_proto(self):\n        """"""\n        Converts graph representation of GraphPy object to TensorBoard\n        required format.\n        """"""\n        # TODO: compute correct memory usage and CPU time once\n        # PyTorch supports it\n        import numpy as np\n        nodes = []\n        node_stats = []\n        for v in self.nodes_io.values():\n            nodes.append(node_proto(v.debugName,\n                                    input=v.inputs,\n                                    outputsize=v.tensor_size,\n                                    op=v.kind,\n                                    attributes=v.attributes))\n\n            if v.tensor_size and len(v.tensor_size) > 0:  # assume data is float32, only parameter is counted\n                node_stats.append(\n                    NodeExecStats(node_name=v.debugName,\n                                  all_start_micros=int(time.time() * 1e7),\n                                  all_end_rel_micros=42,\n                                  memory=[AllocatorMemoryUsed(allocator_name=""cpu"",\n                                                              total_bytes=int(np.prod(v.tensor_size)) * 4)]))\n\n        return nodes, node_stats\n\n\n# one argument: \'hasAttribute\', \'hasAttributes\',\ndef parse(graph, args=None, omit_useless_nodes=True):\n    """"""This method parses an optimized PyTorch model graph and produces\n    a list of nodes and node stats for eventual conversion to TensorBoard\n    protobuf format.\n\n    Args:\n      graph (PyTorch module): The model to be parsed.\n      args (tuple): input tensor[s] for the model.\n      omit_useless_nodes (boolean): Whether to remove nodes from the graph.\n    """"""\n    import torch\n    n_inputs = len(args)  # not sure...\n\n    nodes_py = GraphPy()\n    for i, node in enumerate(graph.inputs()):\n        global backward_mode\n        if not backward_mode:\n            try:\n                node.debugName()\n            except:\n                backward_mode = True\n        if omit_useless_nodes:\n            if len(node.uses()) == 0:  # number of user of the node (= number of outputs/ fanout)\n                continue\n\n        if i < n_inputs:\n            nodes_py.append(NodePyIO(node, \'input\'))\n        else:\n            nodes_py.append(NodePyIO(node))  # parameter\n\n    for node in graph.nodes():\n        nodes_py.append(NodePyOP(node))\n\n    for node in graph.outputs():  # must place last.\n        NodePyIO(node, \'output\')\n    nodes_py.find_common_root()\n    nodes_py.populate_namespace_from_OP_to_IO()\n    return nodes_py.to_proto()\n\n\ndef graph(model, args, verbose=False, **kwargs):\n    """"""\n    This method processes a PyTorch model and produces a `GraphDef` proto\n    that can be logged to TensorBoard.\n\n    Args:\n      model (PyTorch module): The model to be parsed.\n      args (tuple): input tensor[s] for the model.\n      verbose (bool): Whether to print out verbose information while\n        processing.\n    """"""\n    import torch\n\n    with torch.onnx.set_training(model, False):  # TODO: move outside of torch.onnx\n        try:\n            trace = torch.jit.trace(model, args)\n            graph = trace.graph\n\n        except RuntimeError as e:\n            print(e)\n            print(\'Error occurs, No graph saved\')\n            raise e\n            # Create an object matching\n            # https://github.com/tensorflow/tensorboard/blob/master/tensorboard/compat/proto/graph.proto\n            # The producer version has been reverse engineered from standard\n            # TensorBoard logged data.\n\n    if verbose:\n        print(graph)\n    list_of_nodes, node_stats = parse(graph, args)\n    # We are hardcoding that this was run on CPU even though it might have actually\n    # run on GPU. Note this is what is shown in TensorBoard and has no bearing\n    # on actual execution.\n    # TODO: See if we can extract GPU vs CPU information from the PyTorch model\n    # and pass it correctly to TensorBoard.\n    #\n    # Definition of StepStats and DeviceStepStats can be found at\n    # https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/graph/tf_graph_common/test/graph-test.ts\n    # and\n    # https://github.com/tensorflow/tensorboard/blob/master/tensorboard/compat/proto/step_stats.proto\n    stepstats = RunMetadata(step_stats=StepStats(dev_stats=[DeviceStepStats(device=""/device:CPU:0"",\n                                                                            node_stats=node_stats)]))\n    return GraphDef(node=list_of_nodes, versions=VersionDef(producer=22)), stepstats\n'"
tensorboardX/tensorboardX/record_writer.py,0,"b'""""""\nTo write tf_record into file. Here we use it for tensorboard\'s event writting.\nThe code was borrowed from https://github.com/TeamHG-Memex/tensorboard_logger\n""""""\n\nimport copy\nimport io\nimport os.path\nimport re\nimport struct\ntry:\n    import boto3\n    S3_ENABLED = True\nexcept ImportError:\n    S3_ENABLED = False\n\nfrom .crc32c import crc32c\n\n\n_VALID_OP_NAME_START = re.compile(\'^[A-Za-z0-9.]\')\n_VALID_OP_NAME_PART = re.compile(\'[A-Za-z0-9_.\\\\-/]+\')\n\n# Registry of writer factories by prefix backends.\n#\n# Currently supports ""s3://"" URLs for S3 based on boto and falls\n# back to local filesystem.\nREGISTERED_FACTORIES = {}\n\n\ndef register_writer_factory(prefix, factory):\n    if \':\' in prefix:\n        raise ValueError(\'prefix cannot contain a :\')\n    REGISTERED_FACTORIES[prefix] = factory\n\n\ndef directory_check(path):\n    \'\'\'Initialize the directory for log files.\'\'\'\n    try:\n        prefix = path.split(\':\')[0]\n        factory = REGISTERED_FACTORIES[prefix]\n        return factory.directory_check(path)\n    except KeyError:\n        if not os.path.exists(path):\n            os.makedirs(path)\n\n\ndef open_file(path):\n    \'\'\'Open a writer for outputting event files.\'\'\'\n    try:\n        prefix = path.split(\':\')[0]\n        factory = REGISTERED_FACTORIES[prefix]\n        return factory.open(path)\n    except KeyError:\n        return open(path, \'wb\')\n\n\nclass S3RecordWriter(object):\n    """"""Writes tensorboard protocol buffer files to S3.""""""\n\n    def __init__(self, path):\n        if not S3_ENABLED:\n            raise ImportError(""boto3 must be installed for S3 support."")\n        self.path = path\n        self.buffer = io.BytesIO()\n\n    def __del__(self):\n        self.close()\n\n    def bucket_and_path(self):\n        path = self.path\n        if path.startswith(""s3://""):\n            path = path[len(""s3://""):]\n        bp = path.split(""/"")\n        bucket = bp[0]\n        path = path[1 + len(bucket):]\n        return bucket, path\n\n    def write(self, val):\n        self.buffer.write(val)\n\n    def flush(self):\n        s3 = boto3.client(\'s3\')\n        bucket, path = self.bucket_and_path()\n        upload_buffer = copy.copy(self.buffer)\n        upload_buffer.seek(0)\n        s3.upload_fileobj(upload_buffer, bucket, path)\n\n    def close(self):\n        self.flush()\n\n\nclass S3RecordWriterFactory(object):\n    """"""Factory for event protocol buffer files to S3.""""""\n\n    def open(self, path):\n        return S3RecordWriter(path)\n\n    def directory_check(self, path):\n        # S3 doesn\'t need directories created before files are added\n        # so we can just skip this check\n        pass\n\n\nregister_writer_factory(""s3"", S3RecordWriterFactory())\n\n\nclass RecordWriter(object):\n    def __init__(self, path):\n        self._name_to_tf_name = {}\n        self._tf_names = set()\n        self.path = path\n        self._writer = None\n        self._writer = open_file(path)\n\n    def write(self, data):\n        w = self._writer.write\n        header = struct.pack(\'Q\', len(data))\n        w(header)\n        w(struct.pack(\'I\', masked_crc32c(header)))\n        w(data)\n        w(struct.pack(\'I\', masked_crc32c(data)))\n\n    def flush(self):\n        self._writer.flush()\n\n    def close(self):\n        self._writer.close()\n\n\ndef masked_crc32c(data):\n    x = u32(crc32c(data))\n    return u32(((x >> 15) | u32(x << 17)) + 0xa282ead8)\n\n\ndef u32(x):\n    return x & 0xffffffff\n\n\ndef make_valid_tf_name(name):\n    if not _VALID_OP_NAME_START.match(name):\n        # Must make it valid somehow, but don\'t want to remove stuff\n        name = \'.\' + name\n    return \'_\'.join(_VALID_OP_NAME_PART.findall(name))\n'"
tensorboardX/tensorboardX/summary.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport numpy as np\nimport os\nimport re as _re\n\n# pylint: disable=unused-import\nfrom six.moves import range\n\nfrom .proto.summary_pb2 import Summary\nfrom .proto.summary_pb2 import HistogramProto\nfrom .proto.summary_pb2 import SummaryMetadata\nfrom .proto.tensor_pb2 import TensorProto\nfrom .proto.tensor_shape_pb2 import TensorShapeProto\nfrom .proto.plugin_pr_curve_pb2 import PrCurvePluginData\nfrom .proto.plugin_text_pb2 import TextPluginData\nfrom .proto.plugin_mesh_pb2 import MeshPluginData\nfrom .proto import layout_pb2\nfrom .x2num import make_np\nfrom .utils import _prepare_video, convert_to_HWC\n\n_INVALID_TAG_CHARACTERS = _re.compile(r\'[^-/\\w\\.]\')\n\n\ndef _clean_tag(name):\n    # In the past, the first argument to summary ops was a tag, which allowed\n    # arbitrary characters. Now we are changing the first argument to be the node\n    # name. This has a number of advantages (users of summary ops now can\n    # take advantage of the tf name scope system) but risks breaking existing\n    # usage, because a much smaller set of characters are allowed in node names.\n    # This function replaces all illegal characters with _s, and logs a warning.\n    # It also strips leading slashes from the name.\n    if name is not None:\n        new_name = _INVALID_TAG_CHARACTERS.sub(\'_\', name)\n        new_name = new_name.lstrip(\'/\')  # Remove leading slashes\n        if new_name != name:\n            logging.info(\n                \'Summary name %s is illegal; using %s instead.\' % (name, new_name))\n            name = new_name\n    return name\n\n\ndef _draw_single_box(image, xmin, ymin, xmax, ymax, display_str, color=\'black\', color_text=\'black\', thickness=2):\n    from PIL import ImageDraw, ImageFont\n    font = ImageFont.load_default()\n    draw = ImageDraw.Draw(image)\n    (left, right, top, bottom) = (xmin, xmax, ymin, ymax)\n    draw.line([(left, top), (left, bottom), (right, bottom),\n               (right, top), (left, top)], width=thickness, fill=color)\n    if display_str:\n        text_bottom = bottom\n        # Reverse list and print from bottom to top.\n        text_width, text_height = font.getsize(display_str)\n        margin = np.ceil(0.05 * text_height)\n        draw.rectangle(\n            [(left, text_bottom - text_height - 2 * margin),\n             (left + text_width, text_bottom)], fill=color\n        )\n        draw.text(\n            (left + margin, text_bottom - text_height - margin),\n            display_str, fill=color_text, font=font\n        )\n    return image\n\n\ndef hparams(hparam_dict=None, metric_dict=None):\n    from tensorboardX.proto.plugin_hparams_pb2 import HParamsPluginData, SessionEndInfo, SessionStartInfo\n    from tensorboardX.proto.api_pb2 import Experiment, HParamInfo, MetricInfo, MetricName, Status\n    from six import string_types\n\n    PLUGIN_NAME = \'hparams\'\n    PLUGIN_DATA_VERSION = 0\n\n    EXPERIMENT_TAG = \'_hparams_/experiment\'\n    SESSION_START_INFO_TAG = \'_hparams_/session_start_info\'\n    SESSION_END_INFO_TAG = \'_hparams_/session_end_info\'\n\n    # TODO: expose other parameters in the future.\n    # hp = HParamInfo(name=\'lr\',display_name=\'learning rate\', type=DataType.DATA_TYPE_FLOAT64, domain_interval=Interval(min_value=10, max_value=100))  # noqa E501\n    # mt = MetricInfo(name=MetricName(tag=\'accuracy\'), display_name=\'accuracy\', description=\'\', dataset_type=DatasetType.DATASET_VALIDATION)  # noqa E501\n    # exp = Experiment(name=\'123\', description=\'456\', time_created_secs=100.0, hparam_infos=[hp], metric_infos=[mt], user=\'tw\')  # noqa E501\n\n    hps = [HParamInfo(name=k) for k in hparam_dict.keys()]\n    mts = [MetricInfo(name=MetricName(tag=k)) for k in metric_dict.keys()]\n\n    exp = Experiment(hparam_infos=hps, metric_infos=mts)\n\n    content = HParamsPluginData(experiment=exp, version=PLUGIN_DATA_VERSION)\n    smd = SummaryMetadata(plugin_data=SummaryMetadata.PluginData(plugin_name=PLUGIN_NAME,\n                                                                 content=content.SerializeToString()))\n    exp = Summary(value=[Summary.Value(tag=EXPERIMENT_TAG, metadata=smd)])\n\n    ssi = SessionStartInfo()\n    for k, v in hparam_dict.items():\n        if isinstance(v, string_types):\n            ssi.hparams[k].string_value = v\n            continue\n\n        if isinstance(v, bool):\n            ssi.hparams[k].bool_value = v\n            continue\n\n        if not isinstance(v, int) or not isinstance(v, float):\n            v = make_np(v)[0]\n            ssi.hparams[k].number_value = v\n\n    content = HParamsPluginData(session_start_info=ssi, version=PLUGIN_DATA_VERSION)\n    smd = SummaryMetadata(plugin_data=SummaryMetadata.PluginData(plugin_name=PLUGIN_NAME,\n                                                                 content=content.SerializeToString()))\n    ssi = Summary(value=[Summary.Value(tag=SESSION_START_INFO_TAG, metadata=smd)])\n\n    sei = SessionEndInfo(status=Status.STATUS_SUCCESS)\n    content = HParamsPluginData(session_end_info=sei, version=PLUGIN_DATA_VERSION)\n    smd = SummaryMetadata(plugin_data=SummaryMetadata.PluginData(plugin_name=PLUGIN_NAME,\n                                                                 content=content.SerializeToString()))\n    sei = Summary(value=[Summary.Value(tag=SESSION_END_INFO_TAG, metadata=smd)])\n\n    return exp, ssi, sei\n\n\ndef scalar(name, scalar, collections=None):\n    """"""Outputs a `Summary` protocol buffer containing a single scalar value.\n    The generated Summary has a Tensor.proto containing the input Tensor.\n    Args:\n      name: A name for the generated node. Will also serve as the series name in\n        TensorBoard.\n      tensor: A real numeric Tensor containing a single value.\n      collections: Optional list of graph collections keys. The new summary op is\n        added to these collections. Defaults to `[GraphKeys.SUMMARIES]`.\n    Returns:\n      A scalar `Tensor` of type `string`. Which contains a `Summary` protobuf.\n    Raises:\n      ValueError: If tensor has the wrong shape or type.\n    """"""\n    name = _clean_tag(name)\n    scalar = make_np(scalar)\n    assert(scalar.squeeze().ndim == 0), \'scalar should be 0D\'\n    scalar = float(scalar)\n    return Summary(value=[Summary.Value(tag=name, simple_value=scalar)])\n\n\ndef histogram_raw(name, min, max, num, sum, sum_squares, bucket_limits, bucket_counts):\n    # pylint: disable=line-too-long\n    """"""Outputs a `Summary` protocol buffer with a histogram.\n    The generated\n    [`Summary`](https://www.tensorflow.org/code/tensorflow/core/framework/summary.proto)\n    has one summary value containing a histogram for `values`.\n    Args:\n      name: A name for the generated node. Will also serve as a series name in\n        TensorBoard.\n      min: A float or int min value\n      max: A float or int max value\n      num: Int number of values\n      sum: Float or int sum of all values\n      sum_squares: Float or int sum of squares for all values\n      bucket_limits: A numeric `Tensor` with upper value per bucket\n      bucket_counts: A numeric `Tensor` with number of values per bucket\n    Returns:\n      A scalar `Tensor` of type `string`. The serialized `Summary` protocol\n      buffer.\n    """"""\n    hist = HistogramProto(min=min,\n                          max=max,\n                          num=num,\n                          sum=sum,\n                          sum_squares=sum_squares,\n                          bucket_limit=bucket_limits,\n                          bucket=bucket_counts)\n    return Summary(value=[Summary.Value(tag=name, histo=hist)])\n\n\ndef histogram(name, values, bins, max_bins=None):\n    # pylint: disable=line-too-long\n    """"""Outputs a `Summary` protocol buffer with a histogram.\n    The generated\n    [`Summary`](https://www.tensorflow.org/code/tensorflow/core/framework/summary.proto)\n    has one summary value containing a histogram for `values`.\n    This op reports an `InvalidArgument` error if any value is not finite.\n    Args:\n      name: A name for the generated node. Will also serve as a series name in\n        TensorBoard.\n      values: A real numeric `Tensor`. Any shape. Values to use to\n        build the histogram.\n    Returns:\n      A scalar `Tensor` of type `string`. The serialized `Summary` protocol\n      buffer.\n    """"""\n    name = _clean_tag(name)\n    values = make_np(values)\n    hist = make_histogram(values.astype(float), bins, max_bins)\n    return Summary(value=[Summary.Value(tag=name, histo=hist)])\n\n\ndef make_histogram(values, bins, max_bins=None):\n    """"""Convert values into a histogram proto using logic from histogram.cc.""""""\n    if values.size == 0:\n        raise ValueError(\'The input has no element.\')\n    values = values.reshape(-1)\n    counts, limits = np.histogram(values, bins=bins)\n    num_bins = len(counts)\n    if max_bins is not None and num_bins > max_bins:\n        subsampling = num_bins // max_bins\n        subsampling_remainder = num_bins % subsampling\n        if subsampling_remainder != 0:\n            counts = np.pad(counts, pad_width=[[0, subsampling - subsampling_remainder]],\n                            mode=""constant"", constant_values=0)\n        counts = counts.reshape(-1, subsampling).sum(axis=-1)\n        new_limits = np.empty((counts.size + 1,), limits.dtype)\n        new_limits[:-1] = limits[:-1:subsampling]\n        new_limits[-1] = limits[-1]\n        limits = new_limits\n\n    # Find the first and the last bin defining the support of the histogram:\n    cum_counts = np.cumsum(np.greater(counts, 0, dtype=np.int32))\n    start, end = np.searchsorted(cum_counts, [0, cum_counts[-1] - 1], side=""right"")\n    start = int(start)\n    end = int(end) + 1\n    del cum_counts\n\n    # TensorBoard only includes the right bin limits. To still have the leftmost limit\n    # included, we include an empty bin left.\n    # If start == 0, we need to add an empty one left, otherwise we can just include the bin left to the\n    # first nonzero-count bin:\n    counts = counts[start - 1:end] if start > 0 else np.concatenate([[0], counts[:end]])\n    limits = limits[start:end + 1]\n\n    if counts.size == 0 or limits.size == 0:\n        raise ValueError(\'The histogram is empty, please file a bug report.\')\n\n    sum_sq = values.dot(values)\n    return HistogramProto(min=values.min(),\n                          max=values.max(),\n                          num=len(values),\n                          sum=values.sum(),\n                          sum_squares=sum_sq,\n                          bucket_limit=limits.tolist(),\n                          bucket=counts.tolist())\n\n\ndef image(tag, tensor, rescale=1, dataformats=\'CHW\'):\n    """"""Outputs a `Summary` protocol buffer with images.\n    The summary has up to `max_images` summary values containing images. The\n    images are built from `tensor` which must be 3-D with shape `[height, width,\n    channels]` and where `channels` can be:\n    *  1: `tensor` is interpreted as Grayscale.\n    *  3: `tensor` is interpreted as RGB.\n    *  4: `tensor` is interpreted as RGBA.\n\n    Args:\n      tag: A name for the generated node. Will also serve as a series name in\n        TensorBoard.\n      tensor: A 3-D `uint8` or `float32` `Tensor` of shape `[height, width,\n        channels]` where `channels` is 1, 3, or 4.\n        \'tensor\' can either have values in [0, 1] (float32) or [0, 255] (uint8).\n        The image() function will scale the image values to [0, 255] by applying\n        a scale factor of either 1 (uint8) or 255 (float32).\n    Returns:\n      A scalar `Tensor` of type `string`. The serialized `Summary` protocol\n      buffer.\n    """"""\n    tag = _clean_tag(tag)\n    tensor = make_np(tensor)\n    tensor = convert_to_HWC(tensor, dataformats)\n    # Do not assume that user passes in values in [0, 255], use data type to detect\n    if tensor.dtype != np.uint8:\n        tensor = (tensor * 255.0).astype(np.uint8)\n\n    image = make_image(tensor, rescale=rescale)\n    return Summary(value=[Summary.Value(tag=tag, image=image)])\n\n\ndef image_boxes(tag, tensor_image, tensor_boxes, rescale=1, dataformats=\'CHW\', labels=None):\n    \'\'\'Outputs a `Summary` protocol buffer with images.\'\'\'\n    tensor_image = make_np(tensor_image)\n    tensor_image = convert_to_HWC(tensor_image, dataformats)\n    tensor_boxes = make_np(tensor_boxes)\n\n    if tensor_image.dtype != np.uint8:\n        tensor_image = (tensor_image * 255.0).astype(np.uint8)\n\n    image = make_image(tensor_image,\n                       rescale=rescale,\n                       rois=tensor_boxes, labels=labels)\n    return Summary(value=[Summary.Value(tag=tag, image=image)])\n\n\ndef draw_boxes(disp_image, boxes, labels=None):\n    # xyxy format\n    num_boxes = boxes.shape[0]\n    list_gt = range(num_boxes)\n    for i in list_gt:\n        disp_image = _draw_single_box(disp_image,\n                                      boxes[i, 0],\n                                      boxes[i, 1],\n                                      boxes[i, 2],\n                                      boxes[i, 3],\n                                      display_str=None if labels is None else labels[i],\n                                      color=\'Red\')\n    return disp_image\n\n\ndef make_image(tensor, rescale=1, rois=None, labels=None):\n    """"""Convert an numpy representation image to Image protobuf""""""\n    from PIL import Image\n    height, width, channel = tensor.shape\n    scaled_height = int(height * rescale)\n    scaled_width = int(width * rescale)\n    image = Image.fromarray(tensor)\n    if rois is not None:\n        image = draw_boxes(image, rois, labels=labels)\n    image = image.resize((scaled_width, scaled_height), Image.ANTIALIAS)\n    import io\n    output = io.BytesIO()\n    image.save(output, format=\'PNG\')\n    image_string = output.getvalue()\n    output.close()\n    return Summary.Image(height=height,\n                         width=width,\n                         colorspace=channel,\n                         encoded_image_string=image_string)\n\n\ndef video(tag, tensor, fps=4):\n    tag = _clean_tag(tag)\n    tensor = make_np(tensor)\n    tensor = _prepare_video(tensor)\n    # If user passes in uint8, then we don\'t need to rescale by 255\n    if tensor.dtype != np.uint8:\n        tensor = (tensor * 255.0).astype(np.uint8)\n\n    video = make_video(tensor, fps)\n    return Summary(value=[Summary.Value(tag=tag, image=video)])\n\n\ndef make_video(tensor, fps):\n    try:\n        import moviepy  # noqa: F401\n    except ImportError:\n        print(\'add_video needs package moviepy\')\n        return\n    try:\n        from moviepy import editor as mpy\n    except ImportError:\n        print(""moviepy is installed, but can\'t import moviepy.editor."",\n              ""Some packages could be missing [imageio, requests]"")\n        return\n    import tempfile\n\n    t, h, w, c = tensor.shape\n\n    # encode sequence of images into gif string\n    clip = mpy.ImageSequenceClip(list(tensor), fps=fps)\n\n    filename = tempfile.NamedTemporaryFile(suffix=\'.gif\', delete=False).name\n    try:  # older version of moviepy does not support progress_bar argument.\n        clip.write_gif(filename, verbose=False, progress_bar=False)\n    except TypeError:\n        clip.write_gif(filename, verbose=False)\n\n    with open(filename, \'rb\') as f:\n        tensor_string = f.read()\n\n    try:\n        os.remove(filename)\n    except OSError:\n        logging.warning(\'The temporary file used by moviepy cannot be deleted.\')\n\n    return Summary.Image(height=h, width=w, colorspace=c, encoded_image_string=tensor_string)\n\n\ndef audio(tag, tensor, sample_rate=44100):\n    tensor = make_np(tensor)\n    if abs(tensor).max() > 1:\n        print(\'warning: audio amplitude out of range, auto clipped.\')\n        tensor = tensor.clip(-1, 1)\n    assert(tensor.ndim == 2), \'input tensor should be 2 dimensional.\'\n    length_frames, num_channels = tensor.shape\n    assert num_channels == 1 or num_channels == 2, f\'Expected 1/2 channels, got {num_channels}\'\n    import soundfile\n    import io\n    with io.BytesIO() as fio:\n        soundfile.write(fio, tensor, samplerate=sample_rate, format=\'wav\')\n        audio_string = fio.getvalue()\n    audio = Summary.Audio(sample_rate=sample_rate,\n                          num_channels=num_channels,\n                          length_frames=length_frames,\n                          encoded_audio_string=audio_string,\n                          content_type=\'audio/wav\')\n    return Summary(value=[Summary.Value(tag=tag, audio=audio)])\n\ndef custom_scalars(layout):\n    categoriesnames = layout.keys()\n    categories = []\n    layouts = []\n    for k, v in layout.items():\n        charts = []\n        for chart_name, chart_meatadata in v.items():\n            tags = chart_meatadata[1]\n            if chart_meatadata[0] == \'Margin\':\n                assert len(tags) == 3\n                mgcc = layout_pb2.MarginChartContent(series=[layout_pb2.MarginChartContent.Series(value=tags[0],\n                                                                                                  lower=tags[1],\n                                                                                                  upper=tags[2])])\n                chart = layout_pb2.Chart(title=chart_name, margin=mgcc)\n            else:\n                mlcc = layout_pb2.MultilineChartContent(tag=tags)\n                chart = layout_pb2.Chart(title=chart_name, multiline=mlcc)\n            charts.append(chart)\n        categories.append(layout_pb2.Category(title=k, chart=charts))\n\n    layout = layout_pb2.Layout(category=categories)\n    PluginData = SummaryMetadata.PluginData(plugin_name=\'custom_scalars\')\n    smd = SummaryMetadata(plugin_data=PluginData)\n    tensor = TensorProto(dtype=\'DT_STRING\',\n                         string_val=[layout.SerializeToString()],\n                         tensor_shape=TensorShapeProto())\n    return Summary(value=[Summary.Value(tag=\'custom_scalars__config__\', tensor=tensor, metadata=smd)])\n\n\ndef text(tag, text):\n    import json\n    PluginData = SummaryMetadata.PluginData(\n        plugin_name=\'text\', content=TextPluginData(version=0).SerializeToString())\n    smd = SummaryMetadata(plugin_data=PluginData)\n    tensor = TensorProto(dtype=\'DT_STRING\',\n                         string_val=[text.encode(encoding=\'utf_8\')],\n                         tensor_shape=TensorShapeProto(dim=[TensorShapeProto.Dim(size=1)]))\n    return Summary(value=[Summary.Value(tag=tag + \'/text_summary\', metadata=smd, tensor=tensor)])\n\n\ndef pr_curve_raw(tag, tp, fp, tn, fn, precision, recall, num_thresholds=127, weights=None):\n    if num_thresholds > 127:  # weird, value > 127 breaks protobuf\n        num_thresholds = 127\n    data = np.stack((tp, fp, tn, fn, precision, recall))\n    pr_curve_plugin_data = PrCurvePluginData(\n        version=0, num_thresholds=num_thresholds).SerializeToString()\n    PluginData = SummaryMetadata.PluginData(\n        plugin_name=\'pr_curves\', content=pr_curve_plugin_data)\n    smd = SummaryMetadata(plugin_data=PluginData)\n    tensor = TensorProto(dtype=\'DT_FLOAT\',\n                         float_val=data.reshape(-1).tolist(),\n                         tensor_shape=TensorShapeProto(\n                             dim=[TensorShapeProto.Dim(size=data.shape[0]), TensorShapeProto.Dim(size=data.shape[1])]))\n    return Summary(value=[Summary.Value(tag=tag, metadata=smd, tensor=tensor)])\n\n\ndef pr_curve(tag, labels, predictions, num_thresholds=127, weights=None):\n    # weird, value > 127 breaks protobuf\n    num_thresholds = min(num_thresholds, 127)\n    data = compute_curve(labels, predictions,\n                         num_thresholds=num_thresholds, weights=weights)\n    pr_curve_plugin_data = PrCurvePluginData(\n        version=0, num_thresholds=num_thresholds).SerializeToString()\n    PluginData = SummaryMetadata.PluginData(\n        plugin_name=\'pr_curves\', content=pr_curve_plugin_data)\n    smd = SummaryMetadata(plugin_data=PluginData)\n    tensor = TensorProto(dtype=\'DT_FLOAT\',\n                         float_val=data.reshape(-1).tolist(),\n                         tensor_shape=TensorShapeProto(\n                             dim=[TensorShapeProto.Dim(size=data.shape[0]), TensorShapeProto.Dim(size=data.shape[1])]))\n    return Summary(value=[Summary.Value(tag=tag, metadata=smd, tensor=tensor)])\n\n\n# https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/pr_curve/summary.py\ndef compute_curve(labels, predictions, num_thresholds=None, weights=None):\n    _MINIMUM_COUNT = 1e-7\n\n    if weights is None:\n        weights = 1.0\n\n    # Compute bins of true positives and false positives.\n    bucket_indices = np.int32(np.floor(predictions * (num_thresholds - 1)))\n    float_labels = labels.astype(np.float)\n    histogram_range = (0, num_thresholds - 1)\n    tp_buckets, _ = np.histogram(\n        bucket_indices,\n        bins=num_thresholds,\n        range=histogram_range,\n        weights=float_labels * weights)\n    fp_buckets, _ = np.histogram(\n        bucket_indices,\n        bins=num_thresholds,\n        range=histogram_range,\n        weights=(1.0 - float_labels) * weights)\n\n    # Obtain the reverse cumulative sum.\n    tp = np.cumsum(tp_buckets[::-1])[::-1]\n    fp = np.cumsum(fp_buckets[::-1])[::-1]\n    tn = fp[0] - fp\n    fn = tp[0] - tp\n    precision = tp / np.maximum(_MINIMUM_COUNT, tp + fp)\n    recall = tp / np.maximum(_MINIMUM_COUNT, tp + fn)\n    return np.stack((tp, fp, tn, fn, precision, recall))\n\n\ndef _get_tensor_summary(tag, tensor, content_type, json_config):\n    mesh_plugin_data = MeshPluginData(\n        version=0,\n        name=tag,\n        content_type=content_type,\n        json_config=json_config,\n        shape=tensor.shape,\n    )\n    content = mesh_plugin_data.SerializeToString()\n    smd = SummaryMetadata(\n        plugin_data=SummaryMetadata.PluginData(\n            plugin_name=\'mesh\',\n            content=content))\n\n    tensor = TensorProto(dtype=\'DT_FLOAT\',\n                         float_val=tensor.reshape(-1).tolist(),\n                         tensor_shape=TensorShapeProto(dim=[\n                             TensorShapeProto.Dim(size=tensor.shape[0]),\n                             TensorShapeProto.Dim(size=tensor.shape[1]),\n                             TensorShapeProto.Dim(size=tensor.shape[2]),\n                         ]))\n    tensor_summary = Summary.Value(\n        tag=\'{}_{}\'.format(tag, content_type),\n        tensor=tensor,\n        metadata=smd,\n    )\n    return tensor_summary\n\n\ndef mesh(tag, vertices, colors, faces, config_dict=None):\n\n    import json\n    summaries = []\n    tensors = [\n        (vertices, 1),\n        (faces, 2),\n        (colors, 3)\n    ]\n\n    for tensor, content_type in tensors:\n        if tensor is None:\n            continue\n        summaries.append(\n            _get_tensor_summary(tag, make_np(tensor), content_type, json.dumps(config_dict, sort_keys=True)))\n\n    return Summary(value=summaries)\n'"
tensorboardX/tensorboardX/torchvis.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport gc\nimport six\nimport time\n\nfrom functools import wraps\nfrom .writer import SummaryWriter\nfrom .visdom_writer import VisdomWriter\n\n\n# Supports both TensorBoard and Visdom (no embedding or graph visualization with Visdom)\nvis_formats = {\'tensorboard\': SummaryWriter, \'visdom\': VisdomWriter}\n\n\nclass TorchVis:\n    def __init__(self, *args, **init_kwargs):\n        """"""\n        Args:\n            args (list of strings): The name of the visualization target(s).\n              Accepted targets are \'tensorboard\' and \'visdom\'.\n            init_kwargs: Additional keyword parameters for the visdom writer (For example, server IP).\n              See https://github.com/facebookresearch/visdom/blob/master/README.md#visdom-arguments-python-only\n              for more.\n        """"""\n        self.subscribers = {}\n        self.register(*args, **init_kwargs)\n\n    def register(self, *args, **init_kwargs):\n        # Sets tensorboard as the default visualization format if not specified\n        formats = [\'tensorboard\'] if not args else args\n        for format in formats:\n            if self.subscribers.get(format) is None and format in vis_formats.keys():\n                self.subscribers[format] = vis_formats[format](**init_kwargs.get(format, {}))\n\n    def unregister(self, *args):\n        for format in args:\n            self.subscribers[format].close()\n            del self.subscribers[format]\n            gc.collect()\n\n    def __getattr__(self, attr):\n        for _, subscriber in six.iteritems(self.subscribers):\n            def wrapper(*args, **kwargs):\n                for _, subscriber in six.iteritems(self.subscribers):\n                    if hasattr(subscriber, attr):\n                        getattr(subscriber, attr)(*args, **kwargs)\n            return wrapper\n        raise AttributeError\n\n    # Handle writer management (open/close) for the user\n    def __del__(self):\n        for _, subscriber in six.iteritems(self.subscribers):\n            subscriber.close()\n'"
tensorboardX/tensorboardX/utils.py,0,"b'# Functions for converting\ndef figure_to_image(figures, close=True):\n    """"""Render matplotlib figure to numpy format.\n\n    Note that this requires the ``matplotlib`` package.\n\n    Args:\n        figure (matplotlib.pyplot.figure) or list of figures: figure or a list of figures\n        close (bool): Flag to automatically close the figure\n\n    Returns:\n        numpy.array: image in [CHW] order\n    """"""\n    import numpy as np\n    try:\n        import matplotlib.pyplot as plt\n        import matplotlib.backends.backend_agg as plt_backend_agg\n    except ModuleNotFoundError:\n        print(\'please install matplotlib\')\n\n    def render_to_rgb(figure):\n        canvas = plt_backend_agg.FigureCanvasAgg(figure)\n        canvas.draw()\n        data = np.frombuffer(canvas.buffer_rgba(), dtype=np.uint8)\n        w, h = figure.canvas.get_width_height()\n        image_hwc = data.reshape([h, w, 4])[:, :, 0:3]\n        image_chw = np.moveaxis(image_hwc, source=2, destination=0)\n        if close:\n            plt.close(figure)\n        return image_chw\n\n    if isinstance(figures, list):\n        images = [render_to_rgb(figure) for figure in figures]\n        return np.stack(images)\n    else:\n        image = render_to_rgb(figures)\n        return image\n\n\ndef graphviz_to_image():\n    pass\n\n\ndef _prepare_video(V):\n    import numpy as np\n    b, t, c, h, w = V.shape\n\n    if V.dtype == np.uint8:\n        V = np.float32(V) / 255.\n\n    def is_power2(num):\n        return num != 0 and ((num & (num - 1)) == 0)\n\n    # pad to nearest power of 2, all at once\n    if not is_power2(V.shape[0]):\n        len_addition = int(2**V.shape[0].bit_length() - V.shape[0])\n        V = np.concatenate(\n            (V, np.zeros(shape=(len_addition, t, c, h, w))), axis=0)\n\n    n_rows = 2**((b.bit_length() - 1) // 2)\n    n_cols = V.shape[0] // n_rows\n\n    V = np.reshape(V, newshape=(n_rows, n_cols, t, c, h, w))\n    V = np.transpose(V, axes=(2, 0, 4, 1, 5, 3))\n    V = np.reshape(V, newshape=(t, n_rows * h, n_cols * w, c))\n\n    return V\n\n\ndef make_grid(I, ncols=8):\n    # I: N1HW or N3HW\n    import numpy as np\n    assert isinstance(\n        I, np.ndarray), \'plugin error, should pass numpy array here\'\n    if I.shape[1] == 1:\n        I = np.concatenate([I, I, I], 1)\n    assert I.ndim == 4 and I.shape[1] == 3 or I.shape[1] == 4\n    nimg = I.shape[0]\n    H = I.shape[2]\n    W = I.shape[3]\n    ncols = min(nimg, ncols)\n    nrows = int(np.ceil(float(nimg) / ncols))\n    canvas = np.zeros((I.shape[1], H * nrows, W * ncols))\n    i = 0\n    for y in range(nrows):\n        for x in range(ncols):\n            if i >= nimg:\n                break\n            canvas[:, y * H:(y + 1) * H, x * W:(x + 1) * W] = I[i]\n            i = i + 1\n    return canvas\n\n    # if modality == \'IMG\':\n    #     if x.dtype == np.uint8:\n    #         x = x.astype(np.float32) / 255.0\n\n\ndef convert_to_HWC(tensor, input_format):  # tensor: numpy array\n    import numpy as np\n    assert(len(set(input_format)) == len(input_format)), ""You can not use the same dimension shordhand twice. \\\n        input_format: {}"".format(input_format)\n    assert(len(tensor.shape) == len(input_format)), ""size of input tensor and input format are different. \\\n        tensor shape: {}, input_format: {}"".format(tensor.shape, input_format)\n    input_format = input_format.upper()\n\n    if len(input_format) == 4:\n        index = [input_format.find(c) for c in \'NCHW\']\n        tensor_NCHW = tensor.transpose(index)\n        tensor_CHW = make_grid(tensor_NCHW)\n        return tensor_CHW.transpose(1, 2, 0)\n\n    if len(input_format) == 3:\n        index = [input_format.find(c) for c in \'HWC\']\n        tensor_HWC = tensor.transpose(index)\n        if tensor_HWC.shape[2] == 1:\n            tensor_HWC = np.concatenate([tensor_HWC, tensor_HWC, tensor_HWC], 2)\n        return tensor_HWC\n\n    if len(input_format) == 2:\n        index = [input_format.find(c) for c in \'HW\']\n        tensor = tensor.transpose(index)\n        tensor = np.stack([tensor, tensor, tensor], 2)\n        return tensor\n'"
tensorboardX/tensorboardX/visdom_writer.py,13,"b'import gc\nimport numpy as np\nimport math\nimport json\nimport time\n\nfrom .summary import compute_curve\nfrom .utils import figure_to_image\nfrom .x2num import make_np\n\n\n# Decorator that checks if there is a Visdom connection\ndef _check_connection(fn):\n    def wrapper(self, *args, **kwargs):\n        if not self.server_connected:\n            print(\'ERROR: No Visdom server currently connected\')\n            self._try_connect()\n            return\n        fn(self, *args, **kwargs)\n    return wrapper\n\n\nclass VisdomWriter:\n    def __init__(self, *args, **kwargs):\n        try:\n            from visdom import Visdom\n        except ImportError:\n            raise ImportError(\n                ""Visdom visualization requires installation of Visdom"")\n\n        self.scalar_dict = {}\n        self.server_connected = False\n        self.vis = Visdom(*args, **kwargs)\n        self.windows = {}\n\n        self._try_connect()\n\n    def _try_connect(self):\n        startup_sec = 1\n        self.server_connected = self.vis.check_connection()\n        while not self.server_connected and startup_sec > 0:\n            time.sleep(0.1)\n            startup_sec -= 0.1\n            self.server_connected = self.vis.check_connection()\n        assert self.server_connected, \'No connection could be formed quickly\'\n\n    @_check_connection\n    def add_scalar(self, tag, scalar_value, global_step=None, main_tag=\'default\'):\n        """"""Add scalar data to Visdom. Plots the values in a plot titled\n           {main_tag}-{tag}.\n\n        Args:\n            tag (string): Data identifier\n            scalar_value (float or string/blobname): Value to save\n            global_step (int): Global step value to record\n            main_tag (string): Data group identifier\n        """"""\n        if self.scalar_dict.get(main_tag) is None:\n            self.scalar_dict[main_tag] = {}\n        exists = self.scalar_dict[main_tag].get(tag) is not None\n        self.scalar_dict[main_tag][tag] = self.scalar_dict[main_tag][tag] + \\\n            [scalar_value] if exists else [scalar_value]\n        plot_name = \'{}-{}\'.format(main_tag, tag)\n        # If there is no global_step provided, follow sequential order\n        x_val = len(self.scalar_dict[main_tag][tag]\n                    ) if not global_step else global_step\n        if exists:\n            # Update our existing Visdom window\n            self.vis.line(\n                X=make_np(x_val),\n                Y=make_np(scalar_value),\n                name=plot_name,\n                update=\'append\',\n                win=self.windows[plot_name],\n            )\n        else:\n            # Save the window if we are creating this graph for the first time\n            self.windows[plot_name] = self.vis.line(\n                X=make_np(x_val),\n                Y=make_np(scalar_value),\n                name=plot_name,\n                opts={\n                    \'title\': plot_name,\n                    \'xlabel\': \'timestep\',\n                    \'ylabel\': tag,\n                },\n            )\n\n    @_check_connection\n    def add_scalars(self, main_tag, tag_scalar_dict, global_step=None):\n        """"""Adds many scalar data to summary.\n\n        Note that this function also keeps logged scalars in memory. In extreme case it explodes your RAM.\n\n        Args:\n            tag (string): Data identifier\n            main_tag (string): Data group identifier\n            tag_scalar_dict (dict): Key-value pair storing the tag and corresponding values\n            global_step (int): Global step value to record\n\n        Examples::\n\n            writer.add_scalars(\'run_14h\',{\'xsinx\':i*np.sin(i/r),\n                                          \'xcosx\':i*np.cos(i/r),\n                                          \'arctanx\': numsteps*np.arctan(i/r)}, i)\n            This function adds three plots:\n                \'run_14h-xsinx\',\n                \'run_14h-xcosx\',\n                \'run_14h-arctanx\'\n            with the corresponding values.\n        """"""\n        for key in tag_scalar_dict.keys():\n            self.add_scalar(key, tag_scalar_dict[key], global_step, main_tag)\n\n    @_check_connection\n    def export_scalars_to_json(self, path):\n        """"""Exports to the given \'path\' an ASCII file containing all the scalars written\n        so far by this instance, with the following format:\n        {writer_id : [[timestamp, step, value], ...], ...}\n\n        The scalars saved by ``add_scalars()`` will be flushed after export.\n        """"""\n        with open(path, ""w"") as f:\n            json.dump(self.scalar_dict, f)\n        self.scalar_dict = {}\n\n    @_check_connection\n    def add_histogram(self, tag, values, global_step=None, bins=\'tensorflow\'):\n        """"""Add histogram to summary.\n\n        Args:\n            tag (string): Data identifier\n            values (torch.Tensor, numpy.array, or string/blobname): Values to build histogram\n            global_step (int): Global step value to record\n            bins (string): one of {\'tensorflow\', \'auto\', \'fd\', ...}, this determines how the bins are made. You can find\n              other options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html\n        """"""\n        values = make_np(values)\n        self.vis.histogram(make_np(values), opts={\'title\': tag})\n\n    @_check_connection\n    def add_image(self, tag, img_tensor, global_step=None, caption=None):\n        """"""Add image data to summary.\n\n        Note that this requires the ``pillow`` package.\n\n        Args:\n            tag (string): Data identifier\n            img_tensor (torch.Tensor, numpy.array, or string/blobname): Image data\n            global_step (int): Global step value to record\n        Shape:\n            img_tensor: :math:`(C, H, W)`. Use ``torchvision.utils.make_grid()`` to prepare it is a good idea.\n            C = colors (can be 1 - grayscale, 3 - RGB, 4 - RGBA)\n        """"""\n        img_tensor = make_np(img_tensor)\n        self.vis.image(img_tensor, opts={\'title\': tag, \'caption\': caption})\n\n    @_check_connection\n    def add_figure(self, tag, figure, global_step=None, close=True):\n        """"""Render matplotlib figure into an image and add it to summary.\n\n        Note that this requires the ``matplotlib`` package.\n\n        Args:\n            tag (string): Data identifier\n            figure (matplotlib.pyplot.figure) or list of figures: figure or a list of figures\n            global_step (int): Global step value to record\n            close (bool): Flag to automatically close the figure\n        """"""\n        self.add_image(tag, figure_to_image(figure, close), global_step)\n\n    @_check_connection\n    def add_video(self, tag, vid_tensor, global_step=None, fps=4):\n        """"""Add video data to summary.\n\n        Note that this requires the ``moviepy`` package.\n\n        Args:\n            tag (string): Data identifier\n            vid_tensor (torch.Tensor): Video data\n            global_step (int): Global step value to record\n            fps (float or int): Frames per second\n        Shape:\n            vid_tensor: :math:`(B, C, T, H, W)`. (if following tensorboardX format)\n            vid_tensor: :math:`(T, H, W, C)`. (if following visdom format)\n            B = batches, C = colors (1, 3, or 4), T = time frames, H = height, W = width\n        """"""\n        shape = vid_tensor.shape\n        # A batch of videos (tensorboardX format) is a 5D tensor\n        if len(shape) > 4:\n            for i in range(shape[0]):\n                # Reshape each video to Visdom\'s (T x H x W x C) and write each video\n                # TODO: reverse the logic here, shoudl do the permutation in numpy\n                if isinstance(vid_tensor, np.ndarray):\n                    import torch\n                    ind_vid = torch.from_numpy(\n                        vid_tensor[i, :, :, :, :]).permute(1, 2, 3, 0)\n                else:\n                    ind_vid = vid_tensor[i, :, :, :, :].permute(1, 2, 3, 0)\n                scale_factor = 255 if np.any(\n                    (ind_vid > 0) & (ind_vid < 1)) else 1\n                # Visdom looks for .ndim attr, this is something raw Tensors don\'t have\n                # Cast to Numpy array to get .ndim attr\n                ind_vid = ind_vid.numpy()\n                ind_vid = (ind_vid * scale_factor).astype(np.uint8)\n                assert ind_vid.shape[3] in [1, 3, 4], \\\n                    \'Visdom requires the last dimension to be color, which can be 1 (grayscale), 3 (RGB) or 4 (RGBA)\'\n                self.vis.video(tensor=ind_vid, opts={\'fps\': fps})\n        else:\n            self.vis.video(tensor=vid_tensor, opts={\'fps\': fps})\n\n    @_check_connection\n    def add_audio(self, tag, snd_tensor, global_step=None, sample_rate=44100):\n        """"""Add audio data to summary.\n\n        Args:\n            tag (string): Data identifier\n            snd_tensor (torch.Tensor, numpy.array, or string/blobname): Sound data\n            global_step (int): Global step value to record\n            sample_rate (int): sample rate in Hz\n\n        Shape:\n            snd_tensor: :math:`(1, L)`. The values should lie between [-1, 1].\n        """"""\n        snd_tensor = make_np(snd_tensor)\n        self.vis.audio(tensor=snd_tensor, opts={\n                       \'sample_frequency\': sample_rate})\n\n    @_check_connection\n    def add_text(self, tag, text_string, global_step=None):\n        """"""Add text data to summary.\n\n        Args:\n            tag (string): Data identifier\n            text_string (string): String to save\n            global_step (int): Global step value to record\n        Examples::\n            writer.add_text(\'lstm\', \'This is an lstm\', 0)\n            writer.add_text(\'rnn\', \'This is an rnn\', 10)\n        """"""\n        if text_string is None:\n            # Visdom doesn\'t support tags, write the tag as the text_string\n            text_string = tag\n        self.vis.text(text_string)\n\n    @_check_connection\n    def add_onnx_graph(self, prototxt):\n        # TODO: Visdom doesn\'t support graph visualization yet, so this is a no-op\n        return\n\n    @_check_connection\n    def add_graph(self, model, input_to_model=None, verbose=False, **kwargs):\n        # TODO: Visdom doesn\'t support graph visualization yet, so this is a no-op\n        return\n\n    @_check_connection\n    def add_embedding(self, mat, metadata=None, label_img=None, global_step=None, tag=\'default\', metadata_header=None):\n        # TODO: Visdom doesn\'t support embeddings yet, so this is a no-op\n        return\n\n    @_check_connection\n    def add_pr_curve(self, tag, labels, predictions, global_step=None, num_thresholds=127, weights=None):\n        """"""Adds precision recall curve.\n\n        Args:\n            tag (string): Data identifier\n            labels (torch.Tensor, numpy.array, or string/blobname): Ground truth data. Binary label for each element.\n            predictions (torch.Tensor, numpy.array, or string/blobname):\n            The probability that an element be classified as true. Value should in [0, 1]\n            global_step (int): Global step value to record\n            num_thresholds (int): Number of thresholds used to draw the curve.\n\n        """"""\n        labels, predictions = make_np(labels), make_np(predictions)\n        raw_data = compute_curve(labels, predictions, num_thresholds, weights)\n\n        # compute_curve returns np.stack((tp, fp, tn, fn, precision, recall))\n        # We want to access \'precision\' and \'recall\'\n        precision, recall = raw_data[4, :], raw_data[5, :]\n\n        self.vis.line(\n            X=recall,\n            Y=precision,\n            name=tag,\n            opts={\n                \'title\': \'PR Curve for {}\'.format(tag),\n                \'xlabel\': \'recall\',\n                \'ylabel\': \'precision\',\n            },\n        )\n\n    @_check_connection\n    def add_pr_curve_raw(self, tag, true_positive_counts,\n                         false_positive_counts,\n                         true_negative_counts,\n                         false_negative_counts,\n                         precision,\n                         recall, global_step=None, num_thresholds=127, weights=None):\n        """"""Adds precision recall curve with raw data.\n\n        Args:\n            tag (string): Data identifier\n            true_positive_counts (torch.Tensor, numpy.array, or string/blobname): true positive counts\n            false_positive_counts (torch.Tensor, numpy.array, or string/blobname): false positive counts\n            true_negative_counts (torch.Tensor, numpy.array, or string/blobname): true negative counts\n            false_negative_counts (torch.Tensor, numpy.array, or string/blobname): false negative counts\n            precision (torch.Tensor, numpy.array, or string/blobname): precision\n            recall (torch.Tensor, numpy.array, or string/blobname): recall\n            global_step (int): Global step value to record\n            num_thresholds (int): Number of thresholds used to draw the curve.\n            see: https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/pr_curve/README.md\n        """"""\n        precision, recall = make_np(precision), make_np(recall)\n        self.vis.line(\n            X=recall,\n            Y=precision,\n            name=tag,\n            opts={\n                \'title\': \'PR Curve for {}\'.format(tag),\n                \'xlabel\': \'recall\',\n                \'ylabel\': \'precision\',\n            },\n        )\n\n    def close(self):\n        del self.vis\n        del self.scalar_dict\n        gc.collect()\n'"
tensorboardX/tensorboardX/writer.py,33,"b'""""""Provides an API for writing protocol buffers to event files to be\nconsumed by TensorBoard for visualization.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport os\nimport six\nimport time\nimport logging\n\nfrom .embedding import make_mat, make_sprite, make_tsv, append_pbtxt\nfrom .event_file_writer import EventFileWriter\nfrom .onnx_graph import load_onnx_graph\nfrom .pytorch_graph import graph\nfrom .proto import event_pb2\nfrom .proto import summary_pb2\nfrom .proto.event_pb2 import SessionLog, Event\nfrom .utils import figure_to_image\nfrom .summary import (\n    scalar, histogram, histogram_raw, image, audio, text,\n    pr_curve, pr_curve_raw, video, custom_scalars, image_boxes, mesh, hparams\n)\n\n\nclass DummyFileWriter(object):\n    """"""A fake file writer that writes nothing to the disk.\n    """"""\n    def __init__(self, logdir):\n        self._logdir = logdir\n\n    def get_logdir(self):\n        """"""Returns the directory where event file will be written.""""""\n        return self._logdir\n\n    def add_event(self, event, step=None, walltime=None):\n        return\n\n    def add_summary(self, summary, global_step=None, walltime=None):\n        return\n\n    def add_graph(self, graph_profile, walltime=None):\n        return\n\n    def add_onnx_graph(self, graph, walltime=None):\n        return\n\n    def flush(self):\n        return\n\n    def close(self):\n        return\n\n    def reopen(self):\n        return\n\n\nclass FileWriter(object):\n    """"""Writes protocol buffers to event files to be consumed by TensorBoard.\n\n    The `FileWriter` class provides a mechanism to create an event file in a\n    given directory and add summaries and events to it. The class updates the\n    file contents asynchronously. This allows a training program to call methods\n    to add data to the file directly from the training loop, without slowing down\n    training.\n    """"""\n\n    def __init__(self, logdir, max_queue=10, flush_secs=120, filename_suffix=\'\'):\n        """"""Creates a `FileWriter` and an event file.\n        On construction the writer creates a new event file in `logdir`.\n        The other arguments to the constructor control the asynchronous writes to\n        the event file.\n\n        Args:\n          logdir: A string. Directory where event file will be written.\n          max_queue: Integer. Size of the queue for pending events and\n            summaries before one of the \'add\' calls forces a flush to disk.\n            Default is ten items.\n          flush_secs: Number. How often, in seconds, to flush the\n            pending events and summaries to disk. Default is every two minutes.\n          filename_suffix: A string. Suffix added to all event filenames\n            in the logdir directory. More details on filename construction in\n            tensorboard.summary.writer.event_file_writer.EventFileWriter.\n        """"""\n        # Sometimes PosixPath is passed in and we need to coerce it to\n        # a string in all cases\n        # TODO: See if we can remove this in the future if we are\n        # actually the ones passing in a PosixPath\n        logdir = str(logdir)\n        self.event_writer = EventFileWriter(\n            logdir, max_queue, flush_secs, filename_suffix)\n\n    def get_logdir(self):\n        """"""Returns the directory where event file will be written.""""""\n        return self.event_writer.get_logdir()\n\n    def add_event(self, event, step=None, walltime=None):\n        """"""Adds an event to the event file.\n        Args:\n          event: An `Event` protocol buffer.\n          step: Number. Optional global step value for training process\n            to record with the event.\n          walltime: float. Optional walltime to override the default (current)\n            walltime (from time.time())\n        """"""\n        event.wall_time = time.time() if walltime is None else walltime\n        if step is not None:\n            # Make sure step is converted from numpy or other formats\n            # since protobuf might not convert depending on version\n            event.step = int(step)\n        self.event_writer.add_event(event)\n\n    def add_summary(self, summary, global_step=None, walltime=None):\n        """"""Adds a `Summary` protocol buffer to the event file.\n        This method wraps the provided summary in an `Event` protocol buffer\n        and adds it to the event file.\n\n        Args:\n          summary: A `Summary` protocol buffer.\n          global_step: Number. Optional global step value for training process\n            to record with the summary.\n          walltime: float. Optional walltime to override the default (current)\n            walltime (from time.time())\n        """"""\n        event = event_pb2.Event(summary=summary)\n        self.add_event(event, global_step, walltime)\n\n    def add_graph(self, graph_profile, walltime=None):\n        """"""Adds a `Graph` and step stats protocol buffer to the event file.\n\n        Args:\n          graph_profile: A `Graph` and step stats protocol buffer.\n          walltime: float. Optional walltime to override the default (current)\n            walltime (from time.time()) seconds after epoch\n        """"""\n        graph = graph_profile[0]\n        stepstats = graph_profile[1]\n        event = event_pb2.Event(graph_def=graph.SerializeToString())\n        self.add_event(event, None, walltime)\n\n        trm = event_pb2.TaggedRunMetadata(\n            tag=\'step1\', run_metadata=stepstats.SerializeToString())\n        event = event_pb2.Event(tagged_run_metadata=trm)\n        self.add_event(event, None, walltime)\n\n    def add_onnx_graph(self, graph, walltime=None):\n        """"""Adds a `Graph` protocol buffer to the event file.\n\n        Args:\n          graph: A `Graph` protocol buffer.\n          walltime: float. Optional walltime to override the default (current)\n            _get_file_writerfrom time.time())\n        """"""\n        event = event_pb2.Event(graph_def=graph.SerializeToString())\n        self.add_event(event, None, walltime)\n\n    def flush(self):\n        """"""Flushes the event file to disk.\n        Call this method to make sure that all pending events have been written to\n        disk.\n        """"""\n        self.event_writer.flush()\n\n    def close(self):\n        """"""Flushes the event file to disk and close the file.\n        Call this method when you do not need the summary writer anymore.\n        """"""\n        self.event_writer.close()\n\n    def reopen(self):\n        """"""Reopens the EventFileWriter.\n        Can be called after `close()` to add more events in the same directory.\n        The events will go into a new events file.\n        Does nothing if the EventFileWriter was not closed.\n        """"""\n        self.event_writer.reopen()\n\n\nclass SummaryWriter(object):\n    """"""Writes entries directly to event files in the logdir to be\n    consumed by TensorBoard.\n\n    The `SummaryWriter` class provides a high-level API to create an event file\n    in a given directory and add summaries and events to it. The class updates the\n    file contents asynchronously. This allows a training program to call methods\n    to add data to the file directly from the training loop, without slowing down\n    training.\n    """"""\n\n    def __init__(self, logdir=None, comment=\'\', purge_step=None, max_queue=10,\n                 flush_secs=120, filename_suffix=\'\', write_to_disk=True, log_dir=None, **kwargs):\n        """"""Creates a `SummaryWriter` that will write out events and summaries\n        to the event file.\n\n        Args:\n            logdir (string): Save directory location. Default is\n              runs/**CURRENT_DATETIME_HOSTNAME**, which changes after each run.\n              Use hierarchical folder structure to compare\n              between runs easily. e.g. pass in \'runs/exp1\', \'runs/exp2\', etc.\n              for each new experiment to compare across them.\n            comment (string): Comment logdir suffix appended to the default\n              ``logdir``. If ``logdir`` is assigned, this argument has no effect.\n            purge_step (int):\n              When logging crashes at step :math:`T+X` and restarts at step :math:`T`,\n              any events whose global_step larger or equal to :math:`T` will be\n              purged and hidden from TensorBoard.\n              Note that crashed and resumed experiments should have the same ``logdir``.\n            max_queue (int): Size of the queue for pending events and\n              summaries before one of the \'add\' calls forces a flush to disk.\n              Default is ten items.\n            flush_secs (int): How often, in seconds, to flush the\n              pending events and summaries to disk. Default is every two minutes.\n            filename_suffix (string): Suffix added to all event filenames in\n              the logdir directory. More details on filename construction in\n              tensorboard.summary.writer.event_file_writer.EventFileWriter.\n            write_to_disk (boolean):\n              If pass `False`, SummaryWriter will not write to disk.\n\n        Examples::\n\n            from tensorboardX import SummaryWriter\n\n            # create a summary writer with automatically generated folder name.\n            writer = SummaryWriter()\n            # folder location: runs/May04_22-14-54_s-MacBook-Pro.local/\n\n            # create a summary writer using the specified folder name.\n            writer = SummaryWriter(""my_experiment"")\n            # folder location: my_experiment\n\n            # create a summary writer with comment appended.\n            writer = SummaryWriter(comment=""LR_0.1_BATCH_16"")\n            # folder location: runs/May04_22-14-54_s-MacBook-Pro.localLR_0.1_BATCH_16/\n\n        """"""\n        if log_dir is not None and logdir is None:\n            logdir = log_dir\n        if not logdir:\n            import socket\n            from datetime import datetime\n            current_time = datetime.now().strftime(\'%b%d_%H-%M-%S\')\n            logdir = os.path.join(\n                \'runs\', current_time + \'_\' + socket.gethostname() + comment)\n        self.logdir = logdir\n        self.purge_step = purge_step\n        self._max_queue = max_queue\n        self._flush_secs = flush_secs\n        self._filename_suffix = filename_suffix\n        self._write_to_disk = write_to_disk\n        self.kwargs = kwargs\n\n        # Initialize the file writers, but they can be cleared out on close\n        # and recreated later as needed.\n        self.file_writer = self.all_writers = None\n        self._get_file_writer()\n\n        # Create default bins for histograms, see generate_testdata.py in tensorflow/tensorboard\n        v = 1E-12\n        buckets = []\n        neg_buckets = []\n        while v < 1E20:\n            buckets.append(v)\n            neg_buckets.append(-v)\n            v *= 1.1\n        self.default_bins = neg_buckets[::-1] + [0] + buckets\n\n        self.scalar_dict = {}\n\n    def __append_to_scalar_dict(self, tag, scalar_value, global_step,\n                                timestamp):\n        """"""This adds an entry to the self.scalar_dict datastructure with format\n        {writer_id : [[timestamp, step, value], ...], ...}.\n        """"""\n        from .x2num import make_np\n        if tag not in self.scalar_dict.keys():\n            self.scalar_dict[tag] = []\n        self.scalar_dict[tag].append(\n            [timestamp, global_step, float(make_np(scalar_value))])\n\n    def _check_caffe2_blob(self, item):\n        """"""\n        Caffe2 users have the option of passing a string representing the name of\n        a blob in the workspace instead of passing the actual Tensor/array containing\n        the numeric values. Thus, we need to check if we received a string as input\n        instead of an actual Tensor/array, and if so, we need to fetch the Blob\n        from the workspace corresponding to that name. Fetching can be done with the\n        following:\n\n        from caffe2.python import workspace (if not already imported)\n        workspace.FetchBlob(blob_name)\n        workspace.FetchBlobs([blob_name1, blob_name2, ...])\n        """"""\n        return isinstance(item, six.string_types)\n\n    def _get_file_writer(self):\n        """"""Returns the default FileWriter instance. Recreates it if closed.""""""\n        if not self._write_to_disk:\n            self.file_writer = DummyFileWriter(logdir=self.logdir)\n            self.all_writers = {self.file_writer.get_logdir(): self.file_writer}\n            return self.file_writer\n\n        if self.all_writers is None or self.file_writer is None:\n            if \'purge_step\' in self.kwargs.keys():\n                most_recent_step = self.kwargs.pop(\'purge_step\')\n                self.file_writer = FileWriter(logdir=self.logdir,\n                                              max_queue=self._max_queue,\n                                              flush_secs=self._flush_secs,\n                                              filename_suffix=self._filename_suffix,\n                                              **self.kwargs)\n                self.file_writer.add_event(\n                    Event(step=most_recent_step, file_version=\'brain.Event:2\'))\n                self.file_writer.add_event(\n                    Event(step=most_recent_step, session_log=SessionLog(status=SessionLog.START)))\n            else:\n                self.file_writer = FileWriter(logdir=self.logdir,\n                                              max_queue=self._max_queue,\n                                              flush_secs=self._flush_secs,\n                                              filename_suffix=self._filename_suffix,\n                                              **self.kwargs)\n            self.all_writers = {self.file_writer.get_logdir(): self.file_writer}\n        return self.file_writer\n\n    def add_hparams(self, hparam_dict=None, metric_dict=None):\n        """"""Add a set of hyperparameters to be compared in tensorboard.\n\n        Args:\n            hparam_dict (dictionary): Each key-value pair in the dictionary is the\n              name of the hyper parameter and it\'s corresponding value.\n            metric_dict (dictionary): Each key-value pair in the dictionary is the\n              name of the metric and it\'s corresponding value. Note that the key used\n              here should be unique in the tensorboard record. Otherwise the value\n              you added by `add_scalar` will be displayed in hparam plugin. In most\n              cases, this is unwanted.\n\n        Examples::\n\n            from tensorboardX import SummaryWriter\n            with SummaryWriter() as w:\n                for i in range(5):\n                    w.add_hparams({\'lr\': 0.1*i, \'bsize\': i},\n                                  {\'hparam/accuracy\': 10*i, \'hparam/loss\': 10*i})\n\n        Expected result:\n\n        .. image:: _static/img/tensorboard/add_hparam.png\n           :scale: 50 %\n        """"""\n        if type(hparam_dict) is not dict or type(metric_dict) is not dict:\n            raise TypeError(\'hparam_dict and metric_dict should be dictionary.\')\n        exp, ssi, sei = hparams(hparam_dict, metric_dict)\n\n        with SummaryWriter(logdir=os.path.join(self.file_writer.get_logdir(), str(time.time()))) as w_hp:\n            w_hp.file_writer.add_summary(exp)\n            w_hp.file_writer.add_summary(ssi)\n            w_hp.file_writer.add_summary(sei)\n            for k, v in metric_dict.items():\n                w_hp.add_scalar(k, v)\n\n    def add_scalar(self, tag, scalar_value, global_step=None, walltime=None):\n        """"""Add scalar data to summary.\n\n        Args:\n            tag (string): Data identifier\n            scalar_value (float or string/blobname): Value to save\n            global_step (int): Global step value to record\n            walltime (float): Optional override default walltime (time.time()) of event\n\n        Examples::\n\n            from tensorboardX import SummaryWriter\n            writer = SummaryWriter()\n            x = range(100)\n            for i in x:\n                writer.add_scalar(\'y=2x\', i * 2, i)\n            writer.close()\n\n        Expected result:\n\n        .. image:: _static/img/tensorboard/add_scalar.png\n           :scale: 50 %\n\n        """"""\n        if self._check_caffe2_blob(scalar_value):\n            scalar_value = workspace.FetchBlob(scalar_value)\n        self._get_file_writer().add_summary(\n            scalar(tag, scalar_value), global_step, walltime)\n\n    def add_scalars(self, main_tag, tag_scalar_dict, global_step=None, walltime=None):\n        """"""Adds many scalar data to summary.\n\n        Note that this function also keeps logged scalars in memory. In extreme case it explodes your RAM.\n\n        Args:\n            main_tag (string): The parent name for the tags\n            tag_scalar_dict (dict): Key-value pair storing the tag and corresponding values\n            global_step (int): Global step value to record\n            walltime (float): Optional override default walltime (time.time()) of event\n\n        Examples::\n\n            from tensorboardX import SummaryWriter\n            writer = SummaryWriter()\n            r = 5\n            for i in range(100):\n                writer.add_scalars(\'run_14h\', {\'xsinx\':i*np.sin(i/r),\n                                                \'xcosx\':i*np.cos(i/r),\n                                                \'tanx\': np.tan(i/r)}, i)\n            writer.close()\n            # This call adds three values to the same scalar plot with the tag\n            # \'run_14h\' in TensorBoard\'s scalar section.\n\n        Expected result:\n\n        .. image:: _static/img/tensorboard/add_scalars.png\n           :scale: 50 %\n\n        """"""\n        walltime = time.time() if walltime is None else walltime\n        fw_logdir = self._get_file_writer().get_logdir()\n        for tag, scalar_value in tag_scalar_dict.items():\n            fw_tag = fw_logdir + ""/"" + main_tag + ""/"" + tag\n            if fw_tag in self.all_writers.keys():\n                fw = self.all_writers[fw_tag]\n            else:\n                fw = FileWriter(logdir=fw_tag)\n                self.all_writers[fw_tag] = fw\n            if self._check_caffe2_blob(scalar_value):\n                scalar_value = workspace.FetchBlob(scalar_value)\n            fw.add_summary(scalar(main_tag, scalar_value),\n                           global_step, walltime)\n            self.__append_to_scalar_dict(\n                fw_tag, scalar_value, global_step, walltime)\n\n    def export_scalars_to_json(self, path):\n        """"""Exports to the given path an ASCII file containing all the scalars written\n        so far by this instance, with the following format:\n        {writer_id : [[timestamp, step, value], ...], ...}\n\n        The scalars saved by ``add_scalars()`` will be flushed after export.\n        """"""\n        with open(path, ""w"") as f:\n            json.dump(self.scalar_dict, f)\n        self.scalar_dict = {}\n\n    def add_histogram(self, tag, values, global_step=None, bins=\'tensorflow\', walltime=None, max_bins=None):\n        """"""Add histogram to summary.\n\n        Args:\n            tag (string): Data identifier\n            values (torch.Tensor, numpy.array, or string/blobname): Values to build histogram\n            global_step (int): Global step value to record\n            bins (string): One of {\'tensorflow\',\'auto\', \'fd\', ...}. This determines how the bins are made. You can find\n              other options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html\n            walltime (float): Optional override default walltime (time.time()) of event\n\n        Examples::\n\n            from tensorboardX import SummaryWriter\n            import numpy as np\n            writer = SummaryWriter()\n            for i in range(10):\n                x = np.random.random(1000)\n                writer.add_histogram(\'distribution centers\', x + i, i)\n            writer.close()\n\n        Expected result:\n\n        .. image:: _static/img/tensorboard/add_histogram.png\n           :scale: 50 %\n\n        """"""\n        if self._check_caffe2_blob(values):\n            values = workspace.FetchBlob(values)\n        if isinstance(bins, six.string_types) and bins == \'tensorflow\':\n            bins = self.default_bins\n        self._get_file_writer().add_summary(\n            histogram(tag, values, bins, max_bins=max_bins), global_step, walltime)\n\n    def add_histogram_raw(self, tag, min, max, num, sum, sum_squares,\n                          bucket_limits, bucket_counts, global_step=None,\n                          walltime=None):\n        """"""Adds histogram with raw data.\n\n        Args:\n            tag (string): Data identifier\n            min (float or int): Min value\n            max (float or int): Max value\n            num (int): Number of values\n            sum (float or int): Sum of all values\n            sum_squares (float or int): Sum of squares for all values\n            bucket_limits (torch.Tensor, numpy.array): Upper value per\n              bucket, note that the bucket_limits returned from `np.histogram`\n              has one more element. See the comment in the following example.\n            bucket_counts (torch.Tensor, numpy.array): Number of values per bucket\n            global_step (int): Global step value to record\n            walltime (float): Optional override default walltime (time.time()) of event\n\n        Examples::\n\n            import numpy as np\n            dummy_data = []\n            for idx, value in enumerate(range(30)):\n                dummy_data += [idx + 0.001] * value\n            values = np.array(dummy_data).astype(float).reshape(-1)\n            counts, limits = np.histogram(values)\n            sum_sq = values.dot(values)\n            with SummaryWriter() as summary_writer:\n                summary_writer.add_histogram_raw(\n                        tag=\'hist_dummy_data\',\n                        min=values.min(),\n                        max=values.max(),\n                        num=len(values),\n                        sum=values.sum(),\n                        sum_squares=sum_sq,\n                        bucket_limits=limits[1:].tolist(),  # <- note here.\n                        bucket_counts=counts.tolist(),\n                        global_step=0)\n\n        """"""\n        if len(bucket_limits) != len(bucket_counts):\n            raise ValueError(\'len(bucket_limits) != len(bucket_counts), see the document.\')\n        self._get_file_writer().add_summary(\n            histogram_raw(tag,\n                          min,\n                          max,\n                          num,\n                          sum,\n                          sum_squares,\n                          bucket_limits,\n                          bucket_counts),\n            global_step,\n            walltime)\n\n    def add_image(self, tag, img_tensor, global_step=None, walltime=None, dataformats=\'CHW\'):\n        """"""Add image data to summary.\n\n        Note that this requires the ``pillow`` package.\n\n        Args:\n            tag (string): Data identifier\n            img_tensor (torch.Tensor, numpy.array, or string/blobname): An `uint8` or `float`\n                Tensor of shape `[channel, height, width]` where `channel` is 1, 3, or 4.\n                The elements in img_tensor can either have values in [0, 1] (float32) or [0, 255] (uint8).\n                Users are responsible to scale the data in the correct range/type.\n            global_step (int): Global step value to record\n            walltime (float): Optional override default walltime (time.time()) of event.\n            dataformats (string): This parameter specifies the meaning of each dimension of the input tensor.\n        Shape:\n            img_tensor: Default is :math:`(3, H, W)`. You can use ``torchvision.utils.make_grid()`` to\n            convert a batch of tensor into 3xHxW format or use ``add_images()`` and let us do the job.\n            Tensor with :math:`(1, H, W)`, :math:`(H, W)`, :math:`(H, W, 3)` is also suitible as long as\n            corresponding ``dataformats`` argument is passed. e.g. CHW, HWC, HW.\n\n        Examples::\n\n            from tensorboardX import SummaryWriter\n            import numpy as np\n            img = np.zeros((3, 100, 100))\n            img[0] = np.arange(0, 10000).reshape(100, 100) / 10000\n            img[1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000\n\n            img_HWC = np.zeros((100, 100, 3))\n            img_HWC[:, :, 0] = np.arange(0, 10000).reshape(100, 100) / 10000\n            img_HWC[:, :, 1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000\n\n            writer = SummaryWriter()\n            writer.add_image(\'my_image\', img, 0)\n\n            # If you have non-default dimension setting, set the dataformats argument.\n            writer.add_image(\'my_image_HWC\', img_HWC, 0, dataformats=\'HWC\')\n            writer.close()\n\n        Expected result:\n\n        .. image:: _static/img/tensorboard/add_image.png\n           :scale: 50 %\n\n        """"""\n        if self._check_caffe2_blob(img_tensor):\n            img_tensor = workspace.FetchBlob(img_tensor)\n        self._get_file_writer().add_summary(\n            image(tag, img_tensor, dataformats=dataformats), global_step, walltime)\n\n    def add_images(self, tag, img_tensor, global_step=None, walltime=None, dataformats=\'NCHW\'):\n        """"""Add batched (4D) image data to summary.\n        Besides passing 4D (NCHW) tensor, you can also pass a list of tensors of the same size.\n        In this case, the ``dataformats`` should be `CHW` or `HWC`.\n        Note that this requires the ``pillow`` package.\n\n        Args:\n            tag (string): Data identifier\n            img_tensor (torch.Tensor, numpy.array, or string/blobname): Image data\n                The elements in img_tensor can either have values in [0, 1] (float32) or [0, 255] (uint8).\n                Users are responsible to scale the data in the correct range/type.\n            global_step (int): Global step value to record\n            walltime (float): Optional override default walltime (time.time()) of event\n        Shape:\n            img_tensor: Default is :math:`(N, 3, H, W)`. If ``dataformats`` is specified, other shape will be\n            accepted. e.g. NCHW or NHWC.\n\n        Examples::\n\n            from tensorboardX import SummaryWriter\n            import numpy as np\n\n            img_batch = np.zeros((16, 3, 100, 100))\n            for i in range(16):\n                img_batch[i, 0] = np.arange(0, 10000).reshape(100, 100) / 10000 / 16 * i\n                img_batch[i, 1] = (1 - np.arange(0, 10000).reshape(100, 100) / 10000) / 16 * i\n\n            writer = SummaryWriter()\n            writer.add_images(\'my_image_batch\', img_batch, 0)\n            writer.close()\n\n        Expected result:\n\n        .. image:: _static/img/tensorboard/add_images.png\n           :scale: 30 %\n\n        """"""\n        if self._check_caffe2_blob(img_tensor):\n            img_tensor = workspace.FetchBlob(img_tensor)\n        if isinstance(img_tensor, list):  # a list of tensors in CHW or HWC\n            if dataformats.upper() != \'CHW\' and dataformats.upper() != \'HWC\':\n                print(\'A list of image is passed, but the dataformat is neither CHW nor HWC.\')\n                print(\'Nothing is written.\')\n                return\n            import torch\n            try:\n                img_tensor = torch.stack(img_tensor, 0)\n            except TypeError as e:\n                import numpy as np\n                img_tensor = np.stack(img_tensor, 0)\n\n            dataformats = \'N\' + dataformats\n\n        self._get_file_writer().add_summary(\n            image(tag, img_tensor, dataformats=dataformats), global_step, walltime)\n\n    def add_image_with_boxes(self, tag, img_tensor, box_tensor, global_step=None,\n                             walltime=None, dataformats=\'CHW\', labels=None, **kwargs):\n        """"""Add image and draw bounding boxes on the image.\n\n        Args:\n            tag (string): Data identifier\n            img_tensor (torch.Tensor, numpy.array, or string/blobname): Image data\n            box_tensor (torch.Tensor, numpy.array, or string/blobname): Box data (for detected objects)\n              box should be represented as [x1, y1, x2, y2].\n            global_step (int): Global step value to record\n            walltime (float): Optional override default walltime (time.time()) of event\n            labels (list of string): The strings to be show on each bounding box.\n        Shape:\n            img_tensor: Default is :math:`(3, H, W)`. It can be specified with ``dataformat`` agrument.\n            e.g. CHW or HWC\n\n            box_tensor: (torch.Tensor, numpy.array, or string/blobname): NX4,  where N is the number of\n            boxes and each 4 elememts in a row represents (xmin, ymin, xmax, ymax).\n        """"""\n        if self._check_caffe2_blob(img_tensor):\n            img_tensor = workspace.FetchBlob(img_tensor)\n        if self._check_caffe2_blob(box_tensor):\n            box_tensor = workspace.FetchBlob(box_tensor)\n        if labels is not None:\n            if isinstance(labels, str):\n                labels = [labels]\n            if len(labels) != box_tensor.shape[0]:\n                logging.warning(\'Number of labels do not equal to number of box, skip the labels.\')\n                labels = None\n        self._get_file_writer().add_summary(image_boxes(\n            tag, img_tensor, box_tensor, dataformats=dataformats, labels=labels, **kwargs), global_step, walltime)\n\n    def add_figure(self, tag, figure, global_step=None, close=True, walltime=None):\n        """"""Render matplotlib figure into an image and add it to summary.\n\n        Note that this requires the ``matplotlib`` package.\n\n        Args:\n            tag (string): Data identifier\n            figure (matplotlib.pyplot.figure) or list of figures: Figure or a list of figures\n            global_step (int): Global step value to record\n            close (bool): Flag to automatically close the figure\n            walltime (float): Optional override default walltime (time.time()) of event\n        """"""\n        if isinstance(figure, list):\n            self.add_image(tag, figure_to_image(figure, close), global_step, walltime, dataformats=\'NCHW\')\n        else:\n            self.add_image(tag, figure_to_image(figure, close), global_step, walltime, dataformats=\'CHW\')\n\n    def add_video(self, tag, vid_tensor, global_step=None, fps=4, walltime=None):\n        """"""Add video data to summary.\n\n        Note that this requires the ``moviepy`` package.\n\n        Args:\n            tag (string): Data identifier\n            vid_tensor (torch.Tensor): Video data\n            global_step (int): Global step value to record\n            fps (float or int): Frames per second\n            walltime (float): Optional override default walltime (time.time()) of event\n        Shape:\n            vid_tensor: :math:`(N, T, C, H, W)`. The values should lie in [0, 255] for type\n              `uint8` or [0, 1] for type `float`.\n        """"""\n        self._get_file_writer().add_summary(\n            video(tag, vid_tensor, fps), global_step, walltime)\n\n    def add_audio(self, tag, snd_tensor, global_step=None, sample_rate=44100, walltime=None):\n        """"""Add audio data to summary.\n\n        Args:\n            tag (string): Data identifier\n            snd_tensor (torch.Tensor): Sound data\n            global_step (int): Global step value to record\n            sample_rate (int): sample rate in Hz\n            walltime (float): Optional override default walltime (time.time()) of event\n        Shape:\n            snd_tensor: :math:`(L, c)`. The values should lie between [-1, 1].\n        """"""\n        if self._check_caffe2_blob(snd_tensor):\n            snd_tensor = workspace.FetchBlob(snd_tensor)\n        self._get_file_writer().add_summary(\n            audio(tag, snd_tensor, sample_rate=sample_rate), global_step, walltime)\n\n    def add_text(self, tag, text_string, global_step=None, walltime=None):\n        """"""Add text data to summary.\n\n        Args:\n            tag (string): Data identifier\n            text_string (string): String to save\n            global_step (int): Global step value to record\n            walltime (float): Optional override default walltime (time.time()) of event\n        Examples::\n\n            writer.add_text(\'lstm\', \'This is an lstm\', 0)\n            writer.add_text(\'rnn\', \'This is an rnn\', 10)\n        """"""\n        self._get_file_writer().add_summary(\n            text(tag, text_string), global_step, walltime)\n\n    def add_onnx_graph(self, prototxt):\n        self._get_file_writer().add_onnx_graph(load_onnx_graph(prototxt))\n\n    def add_graph(self, model, input_to_model=None, verbose=False, **kwargs):\n        # prohibit second call?\n        # no, let tensorboard handle it and show its warning message.\n        """"""Add graph data to summary.\n\n        Args:\n            model (torch.nn.Module): Model to draw.\n            input_to_model (torch.Tensor or list of torch.Tensor): A variable or a tuple of\n                variables to be fed.\n            verbose (bool): Whether to print graph structure in console.\n            omit_useless_nodes (bool): Default to ``true``, which eliminates unused nodes.\n            operator_export_type (string): One of: ``""ONNX""``, ``""RAW""``. This determines\n                the optimization level of the graph. If error happens during exporting\n                the graph, using ``""RAW""`` might help.\n\n        """"""\n        if hasattr(model, \'forward\'):\n            # A valid PyTorch model should have a \'forward\' method\n            import torch\n            from distutils.version import LooseVersion\n            if LooseVersion(torch.__version__) >= LooseVersion(""0.3.1""):\n                pass\n            else:\n                if LooseVersion(torch.__version__) >= LooseVersion(""0.3.0""):\n                    print(\'You are using PyTorch==0.3.0, use add_onnx_graph()\')\n                    return\n                if not hasattr(torch.autograd.Variable, \'grad_fn\'):\n                    print(\'add_graph() only supports PyTorch v0.2.\')\n                    return\n            self._get_file_writer().add_graph(graph(model, input_to_model, verbose, **kwargs))\n        else:\n            # Caffe2 models do not have the \'forward\' method\n            from caffe2.proto import caffe2_pb2\n            from caffe2.python import core\n            from .caffe2_graph import (\n                model_to_graph_def, nets_to_graph_def, protos_to_graph_def\n            )\n            if isinstance(model, list):\n                if isinstance(model[0], core.Net):\n                    current_graph = nets_to_graph_def(\n                        model, **kwargs)\n                elif isinstance(model[0], caffe2_pb2.NetDef):\n                    current_graph = protos_to_graph_def(\n                        model, **kwargs)\n            else:\n                # Handles cnn.CNNModelHelper, model_helper.ModelHelper\n                current_graph = model_to_graph_def(\n                    model, **kwargs)\n            event = event_pb2.Event(\n                graph_def=current_graph.SerializeToString())\n            self._get_file_writer().add_event(event)\n\n    @staticmethod\n    def _encode(rawstr):\n        # I\'d use urllib but, I\'m unsure about the differences from python3 to python2, etc.\n        retval = rawstr\n        retval = retval.replace(""%"", ""%%%02x"" % (ord(""%"")))\n        retval = retval.replace(""/"", ""%%%02x"" % (ord(""/"")))\n        retval = retval.replace(""\\\\"", ""%%%02x"" % (ord(""\\\\"")))\n        return retval\n\n    def add_embedding(self, mat, metadata=None, label_img=None, global_step=None, tag=\'default\', metadata_header=None):\n        """"""Add embedding projector data to summary.\n\n        Args:\n            mat (torch.Tensor or numpy.array): A matrix which each row is the feature vector of the data point\n            metadata (list): A list of labels, each element will be convert to string\n            label_img (torch.Tensor or numpy.array): Images correspond to each data point. Each image should be square.\n            global_step (int): Global step value to record\n            tag (string): Name for the embedding\n        Shape:\n            mat: :math:`(N, D)`, where N is number of data and D is feature dimension\n\n            label_img: :math:`(N, C, H, W)`, where `Height` should be equal to `Width`.\n\n        Examples::\n\n            import keyword\n            import torch\n            meta = []\n            while len(meta)<100:\n                meta = meta+keyword.kwlist # get some strings\n            meta = meta[:100]\n\n            for i, v in enumerate(meta):\n                meta[i] = v+str(i)\n\n            label_img = torch.rand(100, 3, 32, 32)\n            for i in range(100):\n                label_img[i]*=i/100.0\n\n            writer.add_embedding(torch.randn(100, 5), metadata=meta, label_img=label_img)\n            writer.add_embedding(torch.randn(100, 5), label_img=label_img)\n            writer.add_embedding(torch.randn(100, 5), metadata=meta)\n        """"""\n        from .x2num import make_np\n        mat = make_np(mat)\n        if global_step is None:\n            global_step = 0\n            # clear pbtxt?\n        # Maybe we should encode the tag so slashes don\'t trip us up?\n        # I don\'t think this will mess us up, but better safe than sorry.\n        subdir = ""%s/%s"" % (str(global_step).zfill(5), self._encode(tag))\n        save_path = os.path.join(self._get_file_writer().get_logdir(), subdir)\n        try:\n            os.makedirs(save_path)\n        except OSError:\n            print(\n                \'warning: Embedding dir exists, did you set global_step for add_embedding()?\')\n        if metadata is not None:\n            assert mat.shape[0] == len(\n                metadata), \'#labels should equal with #data points\'\n            make_tsv(metadata, save_path, metadata_header=metadata_header)\n        if label_img is not None:\n            assert mat.shape[0] == label_img.shape[0], \'#images should equal with #data points\'\n            assert label_img.shape[2] == label_img.shape[3], \'Image should be square, see tensorflow/tensorboard#670\'\n            make_sprite(label_img, save_path)\n        assert mat.ndim == 2, \'mat should be 2D, where mat.size(0) is the number of data points\'\n        make_mat(mat, save_path)\n        # new funcion to append to the config file a new embedding\n        append_pbtxt(metadata, label_img,\n                     self._get_file_writer().get_logdir(), subdir, global_step, tag)\n\n    def add_pr_curve(self, tag, labels, predictions, global_step=None,\n                     num_thresholds=127, weights=None, walltime=None):\n        """"""Adds precision recall curve.\n        Plotting a precision-recall curve lets you understand your model\'s\n        performance under different threshold settings. With this function,\n        you provide the ground truth labeling (T/F) and prediction confidence\n        (usually the output of your model) for each target. The TensorBoard UI\n        will let you choose the threshold interactively.\n\n        Args:\n            tag (string): Data identifier\n            labels (torch.Tensor, numpy.array, or string/blobname):\n              Ground truth data. Binary label for each element.\n            predictions (torch.Tensor, numpy.array, or string/blobname):\n              The probability that an element be classified as true.\n              Value should in [0, 1]\n            global_step (int): Global step value to record\n            num_thresholds (int): Number of thresholds used to draw the curve.\n            walltime (float): Optional override default walltime (time.time()) of event\n\n        Examples::\n\n            from tensorboardX import SummaryWriter\n            import numpy as np\n            labels = np.random.randint(2, size=100)  # binary label\n            predictions = np.random.rand(100)\n            writer = SummaryWriter()\n            writer.add_pr_curve(\'pr_curve\', labels, predictions, 0)\n            writer.close()\n\n        """"""\n        from .x2num import make_np\n        labels, predictions = make_np(labels), make_np(predictions)\n        self._get_file_writer().add_summary(\n            pr_curve(tag, labels, predictions, num_thresholds, weights),\n            global_step, walltime)\n\n    def add_pr_curve_raw(self, tag, true_positive_counts,\n                         false_positive_counts,\n                         true_negative_counts,\n                         false_negative_counts,\n                         precision,\n                         recall,\n                         global_step=None,\n                         num_thresholds=127,\n                         weights=None,\n                         walltime=None):\n        """"""Adds precision recall curve with raw data.\n\n        Args:\n            tag (string): Data identifier\n            true_positive_counts (torch.Tensor, numpy.array, or string/blobname): true positive counts\n            false_positive_counts (torch.Tensor, numpy.array, or string/blobname): false positive counts\n            true_negative_counts (torch.Tensor, numpy.array, or string/blobname): true negative counts\n            false_negative_counts (torch.Tensor, numpy.array, or string/blobname): false negative counts\n            precision (torch.Tensor, numpy.array, or string/blobname): precision\n            recall (torch.Tensor, numpy.array, or string/blobname): recall\n            global_step (int): Global step value to record\n            num_thresholds (int): Number of thresholds used to draw the curve.\n            walltime (float): Optional override default walltime (time.time()) of event\n            see: https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/pr_curve/README.md\n        """"""\n        self._get_file_writer().add_summary(\n            pr_curve_raw(tag,\n                         true_positive_counts,\n                         false_positive_counts,\n                         true_negative_counts,\n                         false_negative_counts,\n                         precision,\n                         recall,\n                         num_thresholds,\n                         weights),\n            global_step,\n            walltime)\n\n    def add_custom_scalars_multilinechart(self, tags, category=\'default\', title=\'untitled\'):\n        """"""Shorthand for creating multilinechart. Similar to ``add_custom_scalars()``, but the only necessary argument\n        is *tags*.\n\n        Args:\n            tags (list): list of tags that have been used in ``add_scalar()``\n\n        Examples::\n\n            writer.add_custom_scalars_multilinechart([\'twse/0050\', \'twse/2330\'])\n        """"""\n        layout = {category: {title: [\'Multiline\', tags]}}\n        self._get_file_writer().add_summary(custom_scalars(layout))\n\n    def add_custom_scalars_marginchart(self, tags, category=\'default\', title=\'untitled\'):\n        """"""Shorthand for creating marginchart. Similar to ``add_custom_scalars()``, but the only necessary argument\n        is *tags*, which should have exactly 3 elements.\n\n        Args:\n            tags (list): list of tags that have been used in ``add_scalar()``\n\n        Examples::\n\n            writer.add_custom_scalars_marginchart([\'twse/0050\', \'twse/2330\', \'twse/2006\'])\n        """"""\n        assert len(tags) == 3\n        layout = {category: {title: [\'Margin\', tags]}}\n        self._get_file_writer().add_summary(custom_scalars(layout))\n\n    def add_custom_scalars(self, layout):\n        """"""Create special chart by collecting charts tags in \'scalars\'. Note that this function can only be called once\n        for each SummaryWriter() object. Because it only provides metadata to tensorboard, the function can be called\n        before or after the training loop. See ``examples/demo_custom_scalars.py`` for more.\n\n        Args:\n            layout (dict): {categoryName: *charts*}, where *charts* is also a dictionary\n              {chartName: *ListOfProperties*}. The first element in *ListOfProperties* is the chart\'s type\n              (one of **Multiline** or **Margin**) and the second element should be a list containing the tags\n              you have used in add_scalar function, which will be collected into the new chart.\n\n        Examples::\n\n            layout = {\'Taiwan\':{\'twse\':[\'Multiline\',[\'twse/0050\', \'twse/2330\']]},\n                         \'USA\':{ \'dow\':[\'Margin\',   [\'dow/aaa\', \'dow/bbb\', \'dow/ccc\']],\n                              \'nasdaq\':[\'Margin\',   [\'nasdaq/aaa\', \'nasdaq/bbb\', \'nasdaq/ccc\']]}}\n\n            writer.add_custom_scalars(layout)\n        """"""\n        self._get_file_writer().add_summary(custom_scalars(layout))\n\n    def add_mesh(self, tag, vertices, colors=None, faces=None, config_dict=None, global_step=None, walltime=None):\n        """"""Add meshes or 3D point clouds to TensorBoard. The visualization is based on Three.js,\n        so it allows users to interact with the rendered object. Besides the basic definitions\n        such as vertices, faces, users can further provide camera parameter, lighting condition, etc.\n        Please check https://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scene for\n        advanced usage. Note that currently this depends on tb-nightly to show.\n\n        Args:\n            tag (string): Data identifier\n            vertices (torch.Tensor): List of the 3D coordinates of vertices.\n            colors (torch.Tensor): Colors for each vertex\n            faces (torch.Tensor): Indices of vertices within each triangle. (Optional)\n            config_dict: Dictionary with ThreeJS classes names and configuration.\n            global_step (int): Global step value to record\n            walltime (float): Optional override default walltime (time.time())\n              seconds after epoch of event\n\n        Shape:\n            vertices: :math:`(B, N, 3)`. (batch, number_of_vertices, channels). If you see nothing on\n              tensorboard, try normalizing the values to [-1, 1].\n\n            colors: :math:`(B, N, 3)`. The values should lie in [0, 255].\n\n            faces: :math:`(B, N, 3)`. The values should lie in [0, number_of_vertices] for type `uint8`.\n\n        Examples::\n\n            from tensorboardX import SummaryWriter\n            vertices_tensor = np.array([[\n                [1, 1, 1],\n                [-1, -1, 1],\n                [1, -1, -1],\n                [-1, 1, -1],\n            ]], dtype=float)\n            colors_tensor = np.array([[\n                [255, 0, 0],\n                [0, 255, 0],\n                [0, 0, 255],\n                [255, 0, 255],\n            ]], dtype=int)\n            faces_tensor = np.array([[\n                [0, 2, 3],\n                [0, 3, 1],\n                [0, 1, 2],\n                [1, 3, 2],\n            ]], dtype=int)\n\n            writer = SummaryWriter()\n            writer.add_mesh(\'my_mesh\', vertices=vertices_tensor, colors=colors_tensor, faces=faces_tensor)\n\n            writer.close()\n        """"""\n        self._get_file_writer().add_summary(mesh(tag, vertices, colors, faces, config_dict), global_step, walltime)\n\n    def close(self):\n        if self.all_writers is None:\n            return  # ignore double close\n        for writer in self.all_writers.values():\n            writer.flush()\n            writer.close()\n        self.file_writer = self.all_writers = None\n\n    def flush(self):\n        if self.all_writers is None:\n            return  # ignore double close\n        for writer in self.all_writers.values():\n            writer.flush()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n'"
tensorboardX/tensorboardX/x2num.py,1,"b""# DO NOT alter/distruct/free input object !\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport numpy as np\nimport six\n\n\ndef check_nan(array):\n    tmp = np.sum(array)\n    if np.isnan(tmp) or np.isinf(tmp):\n        logging.warning('NaN or Inf found in input tensor.')\n    return array\n\n\ndef make_np(x):\n    if isinstance(x, list):\n        return check_nan(np.array(x))\n    if isinstance(x, np.ndarray):\n        return check_nan(x)\n    if isinstance(x, six.string_types):  # Caffe2 will pass name of blob(s) to fetch\n        return check_nan(prepare_caffe2(x))\n    if np.isscalar(x):\n        return check_nan(np.array([x]))\n    if 'torch' in str(type(x)):\n        return check_nan(prepare_pytorch(x))\n    if 'chainer' in str(type(x)):\n        return check_nan(prepare_chainer(x))\n    if 'mxnet' in str(type(x)):\n        return check_nan(prepare_mxnet(x))\n    raise NotImplementedError(\n        'Got {}, but expected numpy array or torch tensor.'.format(type(x)))\n\n\ndef prepare_pytorch(x):\n    import torch\n    if isinstance(x, torch.autograd.Variable):\n        x = x.data\n    x = x.cpu().numpy()\n    return x\n\n\ndef prepare_theano(x):\n    import theano\n    pass\n\n\ndef prepare_caffe2(x):\n    from caffe2.python import workspace\n    x = workspace.FetchBlob(x)\n    return x\n\n\ndef prepare_mxnet(x):\n    x = x.asnumpy()\n    return x\n\n\ndef prepare_chainer(x):\n    import chainer\n    x = chainer.cuda.to_cpu(x.data)\n    return x\n"""
tensorboardX/tests/__init__.py,0,b'import torch\nimport tensorboardX.proto\n'
tensorboardX/tests/event_file_writer_test.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n# """"""Tests for EventFileWriter and _AsyncWriter""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport glob\nimport os\nfrom tensorboardX.event_file_writer import EventFileWriter\nfrom tensorboardX.event_file_writer import EventFileWriter as _AsyncWriter\n\n\nfrom tensorboardX.proto import event_pb2\nfrom tensorboardX.proto.summary_pb2 import Summary\n\nfrom tensorboard.compat.tensorflow_stub.pywrap_tensorflow import PyRecordReader_New\nimport unittest\n\n\nclass EventFileWriterTest(unittest.TestCase):\n  def get_temp_dir(self):\n    import tempfile\n    return tempfile.mkdtemp()\n\n  def test_event_file_writer_roundtrip(self):\n    _TAGNAME = \'dummy\'\n    _DUMMY_VALUE = 42\n    logdir = self.get_temp_dir()\n    w = EventFileWriter(logdir)\n    summary = Summary(value=[Summary.Value(tag=_TAGNAME, simple_value=_DUMMY_VALUE)])\n    fakeevent = event_pb2.Event(summary=summary)\n    w.add_event(fakeevent)\n    w.close()\n    event_files = sorted(glob.glob(os.path.join(logdir, \'*\')))\n    self.assertEqual(len(event_files), 1)\n    r = PyRecordReader_New(event_files[0])\n    r.GetNext()  # meta data, so skip\n    r.GetNext()\n    self.assertEqual(fakeevent.SerializeToString(), r.record())\n\n  def test_setting_filename_suffix_works(self):\n    logdir = self.get_temp_dir()\n\n    w = EventFileWriter(logdir, filename_suffix=\'.event_horizon\')\n    w.close()\n    event_files = sorted(glob.glob(os.path.join(logdir, \'*\')))\n    self.assertEqual(event_files[0].split(\'.\')[-1], \'event_horizon\')\n\n  def test_async_writer_without_write(self):\n    logdir = self.get_temp_dir()\n    w = EventFileWriter(logdir)\n    w.close()\n    event_files = sorted(glob.glob(os.path.join(logdir, \'*\')))\n    r = PyRecordReader_New(event_files[0])\n    r.GetNext()\n    s = event_pb2.Event.FromString(r.record())\n    self.assertEqual(s.file_version, ""brain.Event:2"")\n\n\n# skip the test, because tensorboard\'s implementaion of filewriter\n# writes raw data while that in tensorboardX writes event protobuf.\nclass AsyncWriterTest(): #unittest.TestCase):\n  def get_temp_dir(self):\n    import tempfile\n    return tempfile.mkdtemp()\n\n  def test_async_writer_write_once(self):\n    foldername = os.path.join(self.get_temp_dir(), ""async_writer_write_once"")\n    w = _AsyncWriter(foldername)\n    filename = w._ev_writer._file_name\n    bytes_to_write = b""hello world""\n    w.add_event(bytes_to_write)\n    w.close()\n    with open(filename, \'rb\') as f:\n      self.assertEqual(f.read(), bytes_to_write)\n\n  def test_async_writer_write_queue_full(self):\n    filename = os.path.join(self.get_temp_dir(), ""async_writer_write_queue_full"")\n    w = _AsyncWriter(filename)\n    bytes_to_write = b""hello world""\n    repeat = 100\n    for i in range(repeat):\n      w.write(bytes_to_write)\n    w.close()\n    with open(filename, \'rb\') as f:\n      self.assertEqual(f.read(), bytes_to_write * repeat)\n\n  def test_async_writer_write_one_slot_queue(self):\n    filename = os.path.join(self.get_temp_dir(), ""async_writer_write_one_slot_queue"")\n    w = _AsyncWriter(filename, max_queue_size=1)\n    bytes_to_write = b""hello world""\n    repeat = 10  # faster\n    for i in range(repeat):\n      w.write(bytes_to_write)\n    w.close()\n    with open(filename, \'rb\') as f:\n      self.assertEqual(f.read(), bytes_to_write * repeat)\n\n  def test_async_writer_close_triggers_flush(self):\n    filename = os.path.join(self.get_temp_dir(), ""async_writer_close_triggers_flush"")\n    w = _AsyncWriter(filename)\n    bytes_to_write = b""x"" * 64\n    w.write(bytes_to_write)\n    w.close()\n    with open(filename, \'rb\') as f:\n      self.assertEqual(f.read(), bytes_to_write)\n\n  def test_write_after_async_writer_closed(self):\n    filename = os.path.join(self.get_temp_dir(), ""write_after_async_writer_closed"")\n    w = _AsyncWriter(filename)\n    bytes_to_write = b""x"" * 64\n    w.write(bytes_to_write)\n    w.close()\n\n    with self.assertRaises(IOError):\n      w.write(bytes_to_write)\n    # nothing is written to the file after close\n    with open(filename, \'rb\') as f:\n      self.assertEqual(f.read(), bytes_to_write)\n\n\nif __name__ == \'__main__\':\n  unittest.main()\n'"
tensorboardX/tests/expect_reader.py,0,"b'from __future__ import absolute_import, division, print_function, unicode_literals\nimport os\nimport sys\n\n\ndef removeWhiteChar(string):\n    return string.replace(\' \', \'\').replace(\'\\t\', \'\').replace(\'\\n\', \'\')\n\n\ndef compare_proto(str_to_compare, function_ptr):\n    module_id = function_ptr.__class__.__module__\n    functionName = function_ptr.id().split(\'.\')[-1]\n    test_file = os.path.realpath(sys.modules[module_id].__file__)\n    expected_file = os.path.join(os.path.dirname(test_file),\n                        ""expect"",\n                        module_id.split(\'.\')[-1] + \'.\' + functionName + "".expect"")\n    print(""expected_file: %s"" % expected_file)\n    assert os.path.exists(expected_file)\n    with open(expected_file) as f:\n        expected = f.read()\n    str_to_compare = str(str_to_compare)\n    print(""str_to_compare:"", removeWhiteChar(str_to_compare))\n    print(""expected:"", removeWhiteChar(expected))\n    assert removeWhiteChar(str_to_compare) == removeWhiteChar(expected)\n\n\ndef write_proto(str_to_compare, function_ptr):\n    module_id = function_ptr.__class__.__module__\n    functionName = function_ptr.id().split(\'.\')[-1]\n    test_file = os.path.realpath(sys.modules[module_id].__file__)\n    expected_file = os.path.join(os.path.dirname(test_file),\n                    ""expect"",\n                    module_id.split(\'.\')[-1] + \'.\' + functionName + "".expect"")\n    print(expected_file)\n    with open(expected_file, \'w\') as f:\n        f.write(str(str_to_compare))\n'"
tensorboardX/tests/record_writer_test.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n# """"""Tests for RecordWriter""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport six\nimport os\nfrom tensorboardX.record_writer import RecordWriter\nfrom tensorboard.compat.tensorflow_stub.pywrap_tensorflow import PyRecordReader_New\nimport unittest\n\n\nclass RecordWriterTest(unittest.TestCase):\n  def get_temp_dir(self):\n    import tempfile\n    return tempfile.mkdtemp()\n\n  def test_expect_bytes_written(self):\n    filename = os.path.join(self.get_temp_dir(), ""expect_bytes_written"")\n    byte_len = 64\n    w = RecordWriter(filename)\n    bytes_to_write = b""x"" * byte_len\n    w.write(bytes_to_write)\n    w.close()\n    with open(filename, \'rb\') as f:\n      self.assertEqual(len(f.read()), (8 + 4 + byte_len + 4))  # uint64+uint32+data+uint32\n\n  def test_empty_record(self):\n    filename = os.path.join(self.get_temp_dir(), ""empty_record"")\n    w = RecordWriter(filename)\n    bytes_to_write = b""""\n    w.write(bytes_to_write)\n    w.close()\n    r = PyRecordReader_New(filename)\n    r.GetNext()\n    self.assertEqual(r.record(), bytes_to_write)\n\n  def test_record_writer_roundtrip(self):\n    filename = os.path.join(self.get_temp_dir(), ""record_writer_roundtrip"")\n    w = RecordWriter(filename)\n    bytes_to_write = b""hello world""\n    times_to_test = 50\n    for _ in range(times_to_test):\n      w.write(bytes_to_write)\n    w.close()\n\n    r = PyRecordReader_New(filename)\n    for i in range(times_to_test):\n      r.GetNext()\n      self.assertEqual(r.record(), bytes_to_write)\n\n  # def test_expect_bytes_written_bytes_IO(self):\n  #   byte_len = 64\n  #   Bytes_io = six.BytesIO()\n  #   w = RecordWriter(Bytes_io)\n  #   bytes_to_write = b""x"" * byte_len\n  #   w.write(bytes_to_write)\n  #   self.assertEqual(len(Bytes_io.getvalue()), (8 + 4 + byte_len + 4))  # uint64+uint32+data+uint32\n\n\nif __name__ == \'__main__\':\n  unittest.main()\n'"
tensorboardX/tests/test_beholder.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorboardX import SummaryWriter\nimport numpy as np\nimport pytest\nimport unittest\nimport tensorboardX.beholder as beholder_lib\nimport tensorboardX.beholder.file_system_tools as fio\nfrom collections import namedtuple\n\n\nclass BeholderTest(unittest.TestCase):\n    def test_beholder(self):\n        LOG_DIRECTORY = '/tmp/beholder-demo'\n        tensor_and_name = namedtuple('tensor_and_name', 'tensor, name')\n        fake_param = [tensor_and_name(np.random.randn(128, 768, 3), 'test' + str(i)) for i in range(5)]\n        arrays = [tensor_and_name(np.random.randn(128, 768, 3), 'test' + str(i)) for i in range(5)]\n        beholder = beholder_lib.Beholder(logdir=LOG_DIRECTORY)\n        beholder.update(\n            trainable=fake_param,\n            arrays=arrays,\n            frame=np.random.randn(128, 128),\n        )\n\n    def test_beholder_video(self):\n        LOG_DIRECTORY = '/tmp/beholder-demo-recording'\n        tensor_and_name = namedtuple('tensor_and_name', 'tensor, name')\n        fake_param = [tensor_and_name(np.random.randn(128, 768, 3), 'test' + str(i)) for i in range(5)]\n        arrays = [tensor_and_name(np.random.randn(128, 768, 3), 'test' + str(i)) for i in range(5)]\n        beholder = beholder_lib.Beholder(logdir=LOG_DIRECTORY)\n        pkl = fio.read_pickle(LOG_DIRECTORY + '/plugins/beholder/config.pkl')\n        pkl['is_recording'] = True\n        fio.write_pickle(pkl, LOG_DIRECTORY + '/plugins/beholder/config.pkl')\n        for i in range(3):\n            if i == 2:\n                pkl = fio.read_pickle(LOG_DIRECTORY + '/plugins/beholder/config.pkl')\n                pkl['is_recording'] = False\n                fio.write_pickle(pkl, LOG_DIRECTORY + '/plugins/beholder/config.pkl')\n            beholder.update(\n                trainable=fake_param,\n                arrays=arrays,\n                frame=np.random.randn(128, 128),\n            )\n"""
tensorboardX/tests/test_caffe2.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom tensorboardX import SummaryWriter\nimport os\nimport unittest\n\n# try:\nimport numpy as np\nimport caffe2.python.brew as brew\nimport caffe2.python.cnn as cnn\nimport caffe2.python.core as core\nimport caffe2.python.model_helper as model_helper\nfrom caffe2.proto import caffe2_pb2\nfrom caffe2.python import workspace\nimport tensorboardX.caffe2_graph as tb\nfrom tensorboardX import x2num\nfrom .expect_reader import compare_proto, write_proto\n\n\nclass Caffe2Test(unittest.TestCase):\n    def test_caffe2_np(self):\n        workspace.FeedBlob(""testBlob"", np.random.randn(1, 3, 64, 64).astype(np.float32))\n        assert isinstance(x2num.make_np(\'testBlob\'), np.ndarray)\n        # assert isinstance(x2num.make_np(\'testBlob\', \'IMG\'), np.ndarray)\n\n    def test_that_operators_gets_non_colliding_names(self):\n        op = caffe2_pb2.OperatorDef()\n        op.type = \'foo\'\n        op.input.extend([\'foo\'])\n        tb._fill_missing_operator_names([op])\n        self.assertEqual(op.input[0], \'foo\')\n        self.assertEqual(op.name, \'foo_1\')\n\n    def test_that_replacing_colons_gives_non_colliding_names(self):\n        # .. and update shapes\n        op = caffe2_pb2.OperatorDef()\n        op.name = \'foo:0\'\n        op.input.extend([\'foo:0\', \'foo$0\'])\n        shapes = {\'foo:0\': [1]}\n        blob_name_tracker = tb._get_blob_names([op])\n        tb._replace_colons(shapes, blob_name_tracker, [op], \'$\')\n        self.assertEqual(op.input[0], \'foo$0\')\n        self.assertEqual(op.input[1], \'foo$0_1\')\n        # Collision but blobs and op names are handled later by\n        # _fill_missing_operator_names.\n        self.assertEqual(op.name, \'foo$0\')\n        self.assertEqual(len(shapes), 1)\n        self.assertEqual(shapes[\'foo$0\'], [1])\n        self.assertEqual(len(blob_name_tracker), 2)\n        self.assertEqual(blob_name_tracker[\'foo$0\'], \'foo:0\')\n        self.assertEqual(blob_name_tracker[\'foo$0_1\'], \'foo$0\')\n\n    def test_that_adding_gradient_scope_does_no_fancy_renaming(self):\n        # because it cannot create collisions\n        op = caffe2_pb2.OperatorDef()\n        op.name = \'foo_grad\'\n        op.input.extend([\'foo_grad\', \'foo_grad_1\'])\n        shapes = {\'foo_grad\': [1]}\n        blob_name_tracker = tb._get_blob_names([op])\n        tb._add_gradient_scope(shapes, blob_name_tracker, [op])\n        self.assertEqual(op.input[0], \'GRADIENTS/foo_grad\')\n        self.assertEqual(op.input[1], \'GRADIENTS/foo_grad_1\')\n        self.assertEqual(op.name, \'GRADIENTS/foo_grad\')\n        self.assertEqual(len(shapes), 1)\n        self.assertEqual(shapes[\'GRADIENTS/foo_grad\'], [1])\n        self.assertEqual(len(blob_name_tracker), 2)\n        self.assertEqual(\n            blob_name_tracker[\'GRADIENTS/foo_grad\'], \'foo_grad\')\n        self.assertEqual(\n            blob_name_tracker[\'GRADIENTS/foo_grad_1\'], \'foo_grad_1\')\n\n    def test_that_auto_ssa_gives_non_colliding_names(self):\n        op1 = caffe2_pb2.OperatorDef()\n        op1.output.extend([\'foo\'])\n        op2 = caffe2_pb2.OperatorDef()\n        op2.input.extend([\'foo\'])\n        op2.output.extend([\'foo\'])\n        op2.output.extend([\'foo_1\'])\n        shapes = {\'foo\': [1], \'foo_1\': [2]}\n        blob_name_tracker = tb._get_blob_names([op1, op2])\n        tb._convert_to_ssa(shapes, blob_name_tracker, [op1, op2])\n        self.assertEqual(op1.output[0], \'foo\')\n        self.assertEqual(op2.input[0], \'foo\')\n        self.assertEqual(op2.output[0], \'foo_1\')\n        # Unfortunate name but we do not parse original `_` for now.\n        self.assertEqual(op2.output[1], \'foo_1_1\')\n        self.assertEqual(len(shapes), 3)\n        self.assertEqual(shapes[\'foo\'], [1])\n        self.assertEqual(shapes[\'foo_1\'], [1])\n        self.assertEqual(shapes[\'foo_1_1\'], [2])\n        self.assertEqual(len(blob_name_tracker), 3)\n        self.assertEqual(blob_name_tracker[\'foo\'], \'foo\')\n        self.assertEqual(blob_name_tracker[\'foo_1\'], \'foo\')\n        self.assertEqual(blob_name_tracker[\'foo_1_1\'], \'foo_1\')\n\n    def test_renaming_tensorflow_style(self):\n        # Construct some dummy operators here\n        # NOTE: \'_w\', \'_bn\', etc without the postfix \'_\' are only renamed when\n        # they are at the very end of the name.\n        # Test that \'_w\', \'_w_\' are renamed to \'/weight\', \'/weight_\', resp.\n        op1 = caffe2_pb2.OperatorDef()\n        op1.input.extend([\'foo_w\'])\n        op1.output.extend([\'foo_w_2\'])\n        # Test that \'_bn\', \'_bn_\' are renamed to \'/batchnorm\', \'/batchnorm_\',\n        # respectively.\n        op2 = caffe2_pb2.OperatorDef()\n        op2.input.extend([\'foo_bn\'])\n        op2.output.extend([\'foo_bn_2\'])\n        # Test that \'_b\', \'_b_\', are renamed to \'/bias\', \'/bias_\', resp.\n        op3 = caffe2_pb2.OperatorDef()\n        op3.input.extend([\'foo_b\'])\n        op3.output.extend([\'foo_b_2\'])\n        # Test that \'_s\', \'_s_\', are renamed to \'/scale\', \'/scale_\', resp.\n        op4 = caffe2_pb2.OperatorDef()\n        op4.input.extend([\'foo_s\'])\n        op4.output.extend([\'foo_s_2\'])\n        # Test that \'_sum\', \'_sum_\', are renamed to \'/sum\', \'/sum_\', resp.\n        op5 = caffe2_pb2.OperatorDef()\n        op5.input.extend([\'foo_sum\'])\n        op5.output.extend([\'foo_sum_2\'])\n        # Test that \'_branch\', \'_branch_\', are renamed to \'/branch\', \'/branch_\',\n        # respectively. Multiple inputs/outputs are also tested in this case.\n        op6 = caffe2_pb2.OperatorDef()\n        op6.input.extend([\'foo_branch\'])\n        op6.input.extend([\'test_branch_2\'])\n        op6.output.extend([\'foo_branch_3\'])\n        op6.output.extend([\'test_branch4\'])\n        shapes = {\n            \'foo_w\': [1], \'foo_w_2\': [2], \'foo_bn\': [3], \'foo_bn_2\': [4],\n            \'foo_b\': [5], \'foo_b_2\': [6], \'foo_s\': [7], \'foo_s_2\': [8],\n            \'foo_sum\': [9], \'foo_sum_2\': [10], \'foo_branch\': [11],\n            \'test_branch_2\': [12], \'foo_branch_3\': [13], \'test_branch4\': [14],\n        }\n        ops = [op1, op2, op3, op4, op5, op6]\n        blob_name_tracker = tb._get_blob_names(ops)\n        tb._rename_tensorflow_style(shapes, blob_name_tracker, ops)\n        # Testing that keys in blob name tracker were renamed correctly\n        self.assertEqual(blob_name_tracker[\'foo/weight\'], \'foo_w\')\n        self.assertEqual(blob_name_tracker[\'foo/weight_2\'], \'foo_w_2\')\n        self.assertEqual(blob_name_tracker[\'foo/batchnorm\'], \'foo_bn\')\n        self.assertEqual(blob_name_tracker[\'foo/batchnorm_2\'], \'foo_bn_2\')\n        self.assertEqual(blob_name_tracker[\'foo/bias\'], \'foo_b\')\n        self.assertEqual(blob_name_tracker[\'foo/bias_2\'], \'foo_b_2\')\n        self.assertEqual(blob_name_tracker[\'foo/scale\'], \'foo_s\')\n        self.assertEqual(blob_name_tracker[\'foo/scale_2\'], \'foo_s_2\')\n        self.assertEqual(blob_name_tracker[\'foo/sum\'], \'foo_sum\')\n        self.assertEqual(blob_name_tracker[\'foo/sum_2\'], \'foo_sum_2\')\n        self.assertEqual(blob_name_tracker[\'foo/branch\'], \'foo_branch\')\n        self.assertEqual(blob_name_tracker[\'test/branch_2\'], \'test_branch_2\')\n        self.assertEqual(blob_name_tracker[\'foo/branch_3\'], \'foo_branch_3\')\n        self.assertEqual(blob_name_tracker[\'test/branch4\'], \'test_branch4\')\n        # Testing that keys in shapes were renamed correctly\n        self.assertEqual(shapes[\'foo/weight\'], [1])\n        self.assertEqual(shapes[\'foo/batchnorm_2\'], [4])\n        self.assertEqual(shapes[\'foo/sum\'], [9])\n        self.assertEqual(shapes[\'test/branch_2\'], [12])\n        # Testing that the ops were renamed correctly\n        self.assertEqual(op1.input[0], \'foo/weight\')\n        self.assertEqual(op1.output[0], \'foo/weight_2\')\n        self.assertEqual(op2.input[0], \'foo/batchnorm\')\n        self.assertEqual(op2.output[0], \'foo/batchnorm_2\')\n        self.assertEqual(op3.input[0], \'foo/bias\')\n        self.assertEqual(op3.output[0], \'foo/bias_2\')\n        self.assertEqual(op4.input[0], \'foo/scale\')\n        self.assertEqual(op4.output[0], \'foo/scale_2\')\n        self.assertEqual(op5.input[0], \'foo/sum\')\n        self.assertEqual(op5.output[0], \'foo/sum_2\')\n        self.assertEqual(op6.input[0], \'foo/branch\')\n        self.assertEqual(op6.input[1], \'test/branch_2\')\n        self.assertEqual(op6.output[0], \'foo/branch_3\')\n        self.assertEqual(op6.output[1], \'test/branch4\')\n\n    def test_filter_ops(self):\n        op1 = caffe2_pb2.OperatorDef()\n        op1.input.extend([\'remove_this\'])\n        op1.output.extend([\'random_output\'])\n        op2 = caffe2_pb2.OperatorDef()\n        op2.input.extend([\'leave_this\'])\n        op2.output.extend([\'leave_this_also\'])\n        op3 = caffe2_pb2.OperatorDef()\n        op3.input.extend([\'random_input\'])\n        op3.output.extend([\'remove_this_also\'])\n\n        def filter_fn(blob):\n            # Filter all blobs with names containing \'remove\'\n            return \'remove\' not in str(blob)\n\n        op_set1 = [op1, op2, op3]\n        op_set2 = [op1, op2, op3]\n\n        # Test case for when perform_filter = True.\n        result_ops1 = tb._filter_ops(op_set1, filter_fn, True)\n        new_op1, new_op2 = result_ops1[0], result_ops1[1]\n        # input named \'remove_this\' should have been filtered\n        self.assertEqual(len(new_op1.input), 0)\n        self.assertEqual(new_op1.output, [\'random_output\'])\n        self.assertEqual(new_op2.input, [\'leave_this\'])\n        self.assertEqual(new_op2.output, [\'leave_this_also\'])\n        # output named \'remove_this_also\' should have been filtered as well.\n        # This should have also removed op3 as the filter function excludes ops\n        # with no outputs.\n        self.assertEqual(len(result_ops1), 2)\n\n        # Test case for when perform_filter = False. op_set2 should remain\n        # unchanged.\n        result_ops2 = tb._filter_ops(op_set2, filter_fn, False)\n        self.assertEqual(result_ops2, op_set2)\n\n    # Use show_simplified=False. This shows the original style of graph\n    # visualization from caffe2.contrib.tensorboard.\n    # TODO: Add test for show_simplified=True.\n    def test_simple_cnnmodel(self):\n        model = cnn.CNNModelHelper(""NCHW"", name=""overfeat"")\n        workspace.FeedBlob(""data"", np.random.randn(1, 3, 64, 64).astype(np.float32))\n        workspace.FeedBlob(""label"", np.random.randn(1, 1000).astype(np.int))\n        with core.NameScope(""conv1""):\n            conv1 = model.Conv(""data"", ""conv1"", 3, 96, 11, stride=4)\n            relu1 = model.Relu(conv1, conv1)\n            pool1 = model.MaxPool(relu1, ""pool1"", kernel=2, stride=2)\n        with core.NameScope(""classifier""):\n            fc = model.FC(pool1, ""fc"", 4096, 1000)\n            pred = model.Softmax(fc, ""pred"")\n            xent = model.LabelCrossEntropy([pred, ""label""], ""xent"")\n            loss = model.AveragedLoss(xent, ""loss"")\n\n        blob_name_tracker = {}\n        graph = tb.model_to_graph_def(\n            model,\n            blob_name_tracker=blob_name_tracker,\n            shapes={},\n            show_simplified=False,\n        )\n\n        compare_proto(graph, self)\n\n    # cnn.CNNModelHelper is deprecated, so we also test with\n    # model_helper.ModelHelper. The model used in this test is taken from the\n    # Caffe2 MNIST tutorial. Also use show_simplified=False here.\n    def test_simple_model(self):\n        model = model_helper.ModelHelper(name=""mnist"")\n        # how come those inputs don\'t break the forward pass =.=a\n        workspace.FeedBlob(""data"", np.random.randn(1, 3, 64, 64).astype(np.float32))\n        workspace.FeedBlob(""label"", np.random.randn(1, 1000).astype(np.int))\n\n        with core.NameScope(""conv1""):\n            conv1 = brew.conv(model, ""data"", \'conv1\', dim_in=1, dim_out=20, kernel=5)\n            # Image size: 24 x 24 -> 12 x 12\n            pool1 = brew.max_pool(model, conv1, \'pool1\', kernel=2, stride=2)\n            # Image size: 12 x 12 -> 8 x 8\n            conv2 = brew.conv(model, pool1, \'conv2\', dim_in=20, dim_out=100, kernel=5)\n            # Image size: 8 x 8 -> 4 x 4\n            pool2 = brew.max_pool(model, conv2, \'pool2\', kernel=2, stride=2)\n        with core.NameScope(""classifier""):\n            # 50 * 4 * 4 stands for dim_out from previous layer multiplied by the image size\n            fc3 = brew.fc(model, pool2, \'fc3\', dim_in=100 * 4 * 4, dim_out=500)\n            relu = brew.relu(model, fc3, fc3)\n            pred = brew.fc(model, relu, \'pred\', 500, 10)\n            softmax = brew.softmax(model, pred, \'softmax\')\n            xent = model.LabelCrossEntropy([softmax, ""label""], \'xent\')\n            # compute the expected loss\n            loss = model.AveragedLoss(xent, ""loss"")\n        model.net.RunAllOnMKL()\n        model.param_init_net.RunAllOnMKL()\n        model.AddGradientOperators([loss], skip=1)\n        blob_name_tracker = {}\n        graph = tb.model_to_graph_def(\n            model,\n            blob_name_tracker=blob_name_tracker,\n            shapes={},\n            show_simplified=False,\n        )\n\n        compare_proto(graph, self)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tensorboardX/tests/test_chainer_np.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom tensorboardX import x2num, SummaryWriter\ntry:\n    import chainer\n    chainer_installed = True\nexcept ImportError:\n    print('Chainer is not installed, skipping test')\n    chainer_installed = False\nimport numpy as np\nimport unittest\n\n\nif chainer_installed:\n    chainer.Variable\n    tensors = [chainer.Variable(np.random.rand(3, 10, 10)),\n               chainer.Variable(np.random.rand(1)),\n               chainer.Variable(np.random.rand(1, 2, 3, 4, 5))]\n\n    class ChainerTest(unittest.TestCase):\n        def test_chainer_np(self):\n            for tensor in tensors:\n                # regular variable\n                assert isinstance(x2num.make_np(tensor), np.ndarray)\n\n            # python primitive type\n            assert(isinstance(x2num.make_np(0), np.ndarray))\n            assert(isinstance(x2num.make_np(0.1), np.ndarray))\n\n        def test_chainer_img(self):\n            shapes = [(77, 3, 13, 7), (77, 1, 13, 7), (3, 13, 7), (1, 13, 7), (13, 7)]\n            for s in shapes:\n                x = chainer.Variable(np.random.random_sample(s))\n                # assert x2num.make_np(x, 'IMG').shape[2] == 3\n\n        def test_chainer_write(self):\n            with SummaryWriter() as w:\n                w.add_scalar('scalar', chainer.Variable(np.random.rand(1)), 0)\n"""
tensorboardX/tests/test_crc32c.py,0,"b""import unittest\nfrom tensorboardX.crc32c import _crc32c, _crc32c_native, crc32c\n\n\nclass CRC32CTest(unittest.TestCase):\n    def test_crc32c(self):\n        data = b'abcd'\n        assert crc32c(data) == 0x92c80a31\n\n    def test_crc32c_python(self):\n        data = b'abcd'\n        assert _crc32c(data) == 0x92c80a31\n\n    def test_crc32c_native(self):\n        if _crc32c_native is None:\n            return\n        data = b'abcd'\n        assert _crc32c_native(data) == 0x92c80a31\n"""
tensorboardX/tests/test_embedding.py,10,"b""import unittest\nimport torch\nfrom tensorboardX import SummaryWriter\n\n\nclass EmbeddingTest(unittest.TestCase):\n    def test_embedding(self):\n        w = SummaryWriter()\n        all_features = torch.Tensor([[1, 2, 3], [5, 4, 1], [3, 7, 7]])\n        all_labels = torch.Tensor([33, 44, 55])\n        all_images = torch.zeros(3, 3, 5, 5)\n\n        w.add_embedding(all_features,\n                        metadata=all_labels,\n                        label_img=all_images,\n                        global_step=2)\n\n        dataset_label = ['test'] * 2 + ['train'] * 2\n        all_labels = list(zip(all_labels, dataset_label))\n        w.add_embedding(all_features,\n                        metadata=all_labels,\n                        label_img=all_images,\n                        metadata_header=['digit', 'dataset'],\n                        global_step=2)\n        # assert...\n\n    def test_embedding_64(self):\n        w = SummaryWriter()\n        all_features = torch.Tensor([[1, 2, 3], [5, 4, 1], [3, 7, 7]])\n        all_labels = torch.Tensor([33, 44, 55])\n        all_images = torch.zeros((3, 3, 5, 5), dtype=torch.float64)\n\n        w.add_embedding(all_features,\n                        metadata=all_labels,\n                        label_img=all_images,\n                        global_step=2)\n\n        dataset_label = ['test'] * 2 + ['train'] * 2\n        all_labels = list(zip(all_labels, dataset_label))\n        w.add_embedding(all_features,\n                        metadata=all_labels,\n                        label_img=all_images,\n                        metadata_header=['digit', 'dataset'],\n                        global_step=2)\n\n    def test_embedding_square(self):\n        w = SummaryWriter(comment='sq')\n        all_features = torch.rand(228,256)\n        all_images = torch.rand(228, 3, 32, 32)\n        for i in range(all_images.shape[0]):\n            all_images[i] *= (float(i)+60)/(all_images.shape[0]+60)\n        w.add_embedding(all_features,\n                        label_img=all_images,\n                        global_step=2)\n\n    def test_embedding_fail(self):\n        with self.assertRaises(AssertionError):\n            w = SummaryWriter(comment='shouldfail')\n            all_features = torch.rand(228,256)\n            all_images = torch.rand(228, 3, 16, 32)\n            for i in range(all_images.shape[0]):\n                all_images[i] *= (float(i)+60)/(all_images.shape[0]+60)\n            w.add_embedding(all_features,\n                            label_img=all_images,\n                            global_step=2)\n"""
tensorboardX/tests/test_figure.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport matplotlib.pyplot as plt\nimport unittest\n\nfrom tensorboardX import SummaryWriter\n\n\nclass FigureTest(unittest.TestCase):\n    def test_figure(self):\n        writer = SummaryWriter()\n\n        figure, axes = plt.figure(), plt.gca()\n        circle1 = plt.Circle((0.2, 0.5), 0.2, color=\'r\')\n        circle2 = plt.Circle((0.8, 0.5), 0.2, color=\'g\')\n        axes.add_patch(circle1)\n        axes.add_patch(circle2)\n        plt.axis(\'scaled\')\n        plt.tight_layout()\n\n        writer.add_figure(""add_figure/figure"", figure, 0, close=False)\n        assert plt.fignum_exists(figure.number) is True\n\n        writer.add_figure(""add_figure/figure"", figure, 1)\n        assert plt.fignum_exists(figure.number) is False\n\n        writer.close()\n\n    def test_figure_list(self):\n        writer = SummaryWriter()\n\n        figures = []\n        for i in range(5):\n            figure = plt.figure()\n            plt.plot([i * 1, i * 2, i * 3], label=""Plot "" + str(i))\n            plt.xlabel(""X"")\n            plt.xlabel(""Y"")\n            plt.legend()\n            plt.tight_layout()\n            figures.append(figure)\n\n        writer.add_figure(""add_figure/figure_list"", figures, 0, close=False)\n        assert all([plt.fignum_exists(figure.number) is True for figure in figures])\n\n        writer.add_figure(""add_figure/figure_list"", figures, 1)\n        assert all([plt.fignum_exists(figure.number) is False for figure in figures])\n\n        writer.close()\n'"
tensorboardX/tests/test_numpy.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np\nimport unittest\n\nfrom tensorboardX import x2num\n\n\nclass NumpyTest(unittest.TestCase):\n    def test_scalar(self):\n        res = x2num.make_np(1.1)\n        assert isinstance(res, np.ndarray) and res.shape == (1,)\n        res = x2num.make_np(1 << 64 - 1)  # uint64_max\n        assert isinstance(res, np.ndarray) and res.shape == (1,)\n        res = x2num.make_np(np.float16(1.00000087))\n        assert isinstance(res, np.ndarray) and res.shape == (1,)\n        res = x2num.make_np(np.float128(1.00008 + 9))\n        assert isinstance(res, np.ndarray) and res.shape == (1,)\n        res = x2num.make_np(np.int64(100000000000))\n        assert isinstance(res, np.ndarray) and res.shape == (1,)\n\n    def test_make_grid(self):\n        pass\n\n    def test_numpy_vid(self):\n        shapes = [(16, 3, 30, 28, 28), (19, 3, 30, 28, 28), (19, 3, 29, 23, 19)]\n        for s in shapes:\n            x = np.random.random_sample(s)\n            # assert x2num.make_np(x, 'VID').shape[3] == 3\n\n    def test_numpy_vid_uint8(self):\n        x = np.random.randint(0, 256, (16, 3, 30, 28, 28)).astype(np.uint8)\n        # x2num.make_np(x, 'VID').shape[3] == 3\n"""
tensorboardX/tests/test_onnx_graph.py,0,"b""import unittest\nimport torch\nfrom tensorboardX import SummaryWriter\n\n\nclass ONNXGraphTest(unittest.TestCase):\n    def test_onnx_graph(self):\n        import subprocess\n        zoo_address = 'https://onnxzoo.blob.core.windows.net/models/opset_8/mnist/mnist.tar.gz'\n\n        res = subprocess.call(['wget', '-nc', zoo_address])\n        assert res == 0, 'cannot download example onnx model from the zoo'\n        res = subprocess.call(['tar', 'xf', 'mnist.tar.gz', '-C', 'examples/', 'mnist/model.onnx'])\n\n        with SummaryWriter() as w:\n            w.add_onnx_graph('examples/mnist/model.onnx')\n"""
tensorboardX/tests/test_pr_curve.py,0,"b""import unittest\nimport torch\nimport numpy as np\nfrom tensorboardX import SummaryWriter\nfrom tensorboardX import summary\nfrom .expect_reader import compare_proto\n\nnp.random.seed(0)\ntrue_positive_counts = [75, 64, 21, 5, 0]\nfalse_positive_counts = [150, 105, 18, 0, 0]\ntrue_negative_counts = [0, 45, 132, 150, 150]\nfalse_negative_counts = [0, 11, 54, 70, 75]\nprecision = [0.3333333, 0.3786982, 0.5384616, 1.0, 0.0]\nrecall = [1.0, 0.8533334, 0.28, 0.0666667, 0.0]\n\n\nclass PRCurveTest(unittest.TestCase):\n    def test_smoke(self):\n        with SummaryWriter() as writer:\n            writer.add_pr_curve('xoxo', np.random.randint(2, size=100), np.random.rand(\n                100), 1)\n            writer.add_pr_curve_raw('prcurve with raw data',\n                                    true_positive_counts,\n                                    false_positive_counts,\n                                    true_negative_counts,\n                                    false_negative_counts,\n                                    precision,\n                                    recall,\n                                    1)\n\n    def test_pr_purve(self):\n        random_labels = np.array([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n            1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n            0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n            1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,\n            1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0])\n        random_probs = np.array([0.33327776, 0.30032885, 0.79012837, 0.04306813, 0.65221544,\n            0.58481968, 0.28305522, 0.53795795, 0.00729739, 0.52266951,\n            0.22464247, 0.11262435, 0.41573075, 0.92493992, 0.73066758,\n            0.43867735, 0.27955449, 0.56975382, 0.53933028, 0.34392824,\n            0.30312509, 0.81732807, 0.55408544, 0.3969487 , 0.31768033,\n            0.24353266, 0.47198005, 0.19999122, 0.05788022, 0.24046305,\n            0.04651082, 0.30061738, 0.78321545, 0.82670207, 0.49200517,\n            0.80904619, 0.96711993, 0.3160946 , 0.01049424, 0.60108337,\n            0.56508792, 0.83729429, 0.9717386 , 0.46306053, 0.80232138,\n            0.24166823, 0.7393237 , 0.50820418, 0.04944932, 0.53854157,\n            0.10765172, 0.84723855, 0.20518299, 0.3143431 , 0.51299074,\n            0.47065695, 0.54267833, 0.1812676 , 0.06265177, 0.34110327,\n            0.30915171, 0.91870169, 0.91309447, 0.31395817, 0.36780571,\n            0.98297986, 0.00594547, 0.52839042, 0.70229202, 0.37779588,\n            0.15207045, 0.59759632, 0.72397032, 0.71502195, 0.90135725,\n            0.43970107, 0.17123532, 0.08785938, 0.04986818, 0.62702444,\n            0.69171023, 0.30537792, 0.30285433, 0.27124347, 0.27693729,\n            0.7136039 , 0.48022489, 0.20916285, 0.2018599 , 0.92401008,\n            0.30189681, 0.46862626, 0.96353024, 0.30468533, 0.68281294,\n            0.30623562, 0.40795975, 0.76824531, 0.89824215, 0.69845035], dtype=np.float16)\n        compare_proto(summary.pr_curve('tag', random_labels, random_probs, 1), self)\n\n    def test_pr_purve_raw(self):\n        compare_proto(summary.pr_curve_raw('prcurve with raw data',\n                                           true_positive_counts,\n                                           false_positive_counts,\n                                           true_negative_counts,\n                                           false_negative_counts,\n                                           precision,\n                                           recall,\n                                           1),\n                      self)\n"""
tensorboardX/tests/test_pytorch_graph.py,5,"b""from __future__ import absolute_import, division, print_function, unicode_literals\nimport unittest\nimport torch\nfrom tensorboardX import SummaryWriter\n\n\nclass PytorchGraphTest(unittest.TestCase):\n    def test_pytorch_graph(self):\n        dummy_input = (torch.zeros(1, 3),)\n\n        class myLinear(torch.nn.Module):\n            def __init__(self):\n                super(myLinear, self).__init__()\n                self.linear = torch.nn.Linear(3, 5)\n\n            def forward(self, x):\n                return self.linear(x)\n\n        with SummaryWriter(comment='LinearModel') as w:\n            w.add_graph(myLinear(), dummy_input, True)\n\n    def test_wrong_input_size(self):\n        print('expect error here:')\n        with self.assertRaises(RuntimeError):\n            dummy_input = torch.rand(1, 9)\n            model = torch.nn.Linear(3, 5)\n            with SummaryWriter(comment='expect_error') as w:\n                w.add_graph(model, dummy_input)  # error\n"""
tensorboardX/tests/test_pytorch_np.py,10,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom tensorboardX import x2num, SummaryWriter\nimport torch\nimport numpy as np\nimport unittest\n\n\nclass PyTorchNumpyTest(unittest.TestCase):\n    def test_pytorch_np(self):\n        tensors = [torch.rand(3, 10, 10), torch.rand(1), torch.rand(1, 2, 3, 4, 5)]\n        for tensor in tensors:\n            # regular tensor\n            assert isinstance(x2num.make_np(tensor), np.ndarray)\n\n            # CUDA tensor\n            if torch.cuda.device_count() > 0:\n                assert isinstance(x2num.make_np(tensor.cuda()), np.ndarray)\n\n            # regular variable\n            assert isinstance(x2num.make_np(torch.autograd.Variable(tensor)), np.ndarray)\n\n            # CUDA variable\n            if torch.cuda.device_count() > 0:\n                assert isinstance(x2num.make_np(torch.autograd.Variable(tensor).cuda()), np.ndarray)\n\n        # python primitive type\n        assert(isinstance(x2num.make_np(0), np.ndarray))\n        assert(isinstance(x2num.make_np(0.1), np.ndarray))\n\n    def test_pytorch_write(self):\n        with SummaryWriter() as w:\n            w.add_scalar('scalar', torch.autograd.Variable(torch.rand(1)), 0)\n\n    def test_pytorch_histogram(self):\n        with SummaryWriter() as w:\n            w.add_histogram('float histogram', torch.rand((50,)))\n            w.add_histogram('int histogram', torch.randint(0, 100, (50,)))\n\n    def test_pytorch_histogram_raw(self):\n        with SummaryWriter() as w:\n            num = 50\n            floats = x2num.make_np(torch.rand((num,)))\n            bins = [0.0, 0.25, 0.5, 0.75, 1.0]\n            counts, limits = np.histogram(floats, bins)\n            sum_sq = floats.dot(floats).item()\n            w.add_histogram_raw('float histogram raw',\n                                min=floats.min().item(),\n                                max=floats.max().item(),\n                                num=num,\n                                sum=floats.sum().item(),\n                                sum_squares=sum_sq,\n                                bucket_limits=limits[1:].tolist(),\n                                bucket_counts=counts.tolist())\n\n            ints = x2num.make_np(torch.randint(0, 100, (num,)))\n            bins = [0, 25, 50, 75, 100]\n            counts, limits = np.histogram(ints, bins)\n            sum_sq = ints.dot(ints).item()\n            w.add_histogram_raw('int histogram raw',\n                                min=ints.min().item(),\n                                max=ints.max().item(),\n                                num=num,\n                                sum=ints.sum().item(),\n                                sum_squares=sum_sq,\n                                bucket_limits=limits[1:].tolist(),\n                                bucket_counts=counts.tolist())\n"""
tensorboardX/tests/test_record_writer.py,0,"b'from tensorboardX import SummaryWriter\nimport unittest\nfrom tensorboardX.record_writer import S3RecordWriter, make_valid_tf_name\nimport os\nimport boto3\nfrom moto import mock_s3\n\nos.environ.setdefault(""AWS_ACCESS_KEY_ID"", ""foobar_key"")\nos.environ.setdefault(""AWS_SECRET_ACCESS_KEY"", ""foobar_secret"")\n\n\nclass RecordWriterTest(unittest.TestCase):\n    @mock_s3\n    def test_record_writer_s3(self):\n        client = boto3.client(\'s3\', region_name=\'us-east-1\')\n        client.create_bucket(Bucket=\'this\')\n        writer = S3RecordWriter(\'s3://this/is/apen\')\n        bucket, path = writer.bucket_and_path()\n        assert bucket == \'this\'\n        assert path == \'is/apen\'\n        writer.write(bytes(42))\n        writer.flush()\n\n    def test_make_valid_tf_name(self):\n        newname = make_valid_tf_name(\'$ave/&sound\')\n        assert newname == \'._ave/_sound\'\n'"
tensorboardX/tests/test_summary.py,0,"b'from __future__ import absolute_import, division, print_function, unicode_literals\nfrom tensorboardX import summary\nfrom .expect_reader import compare_proto, write_proto\nimport numpy as np\nimport pytest\nimport unittest\n# compare_proto = write_proto  # massive update expect\n\ndef tensor_N(shape, dtype=float):\n    numel = np.prod(shape)\n    x = (np.arange(numel, dtype=dtype)).reshape(shape)\n    return x\n\nclass SummaryTest(unittest.TestCase):\n    def test_uint8_image(self):\n        \'\'\'\n        Tests that uint8 image (pixel values in [0, 255]) is not changed\n        \'\'\'\n        test_image = tensor_N(shape=(3, 32, 32), dtype=np.uint8)\n        compare_proto(summary.image(\'dummy\', test_image), self)\n\n    def test_float32_image(self):\n        \'\'\'\n        Tests that float32 image (pixel values in [0, 1]) are scaled correctly\n        to [0, 255]\n        \'\'\'\n        test_image = tensor_N(shape=(3, 32, 32))\n        compare_proto(summary.image(\'dummy\', test_image), self)\n\n    def test_float_1_converts_to_uint8_255(self):\n        green_uint8 = np.array([[[0, 255, 0]]], dtype=\'uint8\') \n        green_float32 = np.array([[[0, 1, 0]]], dtype=\'float32\') \n\n        a = summary.image(tensor=green_uint8, tag=\'\')\n        b = summary.image(tensor=green_float32, tag=\'\')\n        self.assertEqual(a, b)\n\n    def test_list_input(self):\n        with pytest.raises(Exception):\n            summary.histogram(\'dummy\', [1, 3, 4, 5, 6], \'tensorflow\')\n\n    def test_empty_input(self):\n        print(\'expect error here:\')\n        with pytest.raises(Exception):\n            summary.histogram(\'dummy\', np.ndarray(0), \'tensorflow\')\n\n    def test_image_with_boxes(self):\n        compare_proto(summary.image_boxes(\'dummy\',\n                            tensor_N(shape=(3, 32, 32)),\n                            np.array([[10, 10, 40, 40]])), self)\n\n    def test_image_with_one_channel(self):\n        compare_proto(summary.image(\'dummy\', tensor_N(shape=(1, 8, 8)), dataformats=\'CHW\'), self)\n\n    def test_image_with_four_channel(self):\n        compare_proto(summary.image(\'dummy\', tensor_N(shape=(4, 8, 8)), dataformats=\'CHW\'), self)\n\n    def test_image_with_one_channel_batched(self):\n        compare_proto(summary.image(\'dummy\', tensor_N(shape=(2, 1, 8, 8)), dataformats=\'NCHW\'), self)\n\n    def test_image_with_3_channel_batched(self):\n        compare_proto(summary.image(\'dummy\', tensor_N(shape=(2, 3, 8, 8)), dataformats=\'NCHW\'), self)\n\n    def test_image_with_four_channel_batched(self):\n        compare_proto(summary.image(\'dummy\', tensor_N(shape=(2, 4, 8, 8)), dataformats=\'NCHW\'), self)\n\n    def test_image_without_channel(self):\n        compare_proto(summary.image(\'dummy\', tensor_N(shape=(8, 8)), dataformats=\'HW\'), self)\n\n    def test_video(self):\n        try:\n            import moviepy\n        except ImportError:\n            return\n        compare_proto(summary.video(\'dummy\', tensor_N(shape=(4, 3, 1, 8, 8))), self)\n        summary.video(\'dummy\', tensor_N(shape=(16, 48, 1, 28, 28)))\n        summary.video(\'dummy\', tensor_N(shape=(20, 7, 1, 8, 8)))\n\n    def test_audio(self):\n        compare_proto(summary.audio(\'dummy\', tensor_N(shape=(42,))), self)\n\n    def test_text(self):\n        compare_proto(summary.text(\'dummy\', \'text 123\'), self)\n\n    def test_histogram_auto(self):\n        compare_proto(summary.histogram(\'dummy\', tensor_N(shape=(1024,)), bins=\'auto\', max_bins=5), self)\n\n    def test_histogram_fd(self):\n        compare_proto(summary.histogram(\'dummy\', tensor_N(shape=(1024,)), bins=\'fd\', max_bins=5), self)\n\n    def test_histogram_doane(self):\n        compare_proto(summary.histogram(\'dummy\', tensor_N(shape=(1024,)), bins=\'doane\', max_bins=5), self)\n\n    def test_custom_scalars(self):\n        layout = {\'Taiwan\': {\'twse\': [\'Multiline\', [\'twse/0050\', \'twse/2330\']]},\n                    \'USA\': {\'dow\': [\'Margin\', [\'dow/aaa\', \'dow/bbb\', \'dow/ccc\']],\n                            \'nasdaq\': [\'Margin\', [\'nasdaq/aaa\', \'nasdaq/bbb\', \'nasdaq/ccc\']]}}\n        summary.custom_scalars(layout)  # smoke test only.\n\n    def test_mesh(self):\n        vertices_tensor = np.array([[\n            [1, 1, 1],\n            [-1, -1, 1],\n            [1, -1, -1],\n            [-1, 1, -1],\n        ]], dtype=float)\n        colors_tensor = np.array([[\n            [255, 0, 0],\n            [0, 255, 0],\n            [0, 0, 255],\n            [255, 0, 255],\n        ]], dtype=int)\n        faces_tensor = np.array([[\n            [0, 2, 3],\n            [0, 3, 1],\n            [0, 1, 2],\n            [1, 3, 2],\n        ]], dtype=int)\n        compare_proto(summary.mesh(\'my_mesh\', vertices=vertices_tensor, colors=colors_tensor, faces=faces_tensor), self)\n\n    # It\'s hard to get dictionary sorted with same result in various envs. So only use one.\n    def test_hparams(self):\n        hp = {\'lr\': 0.1}\n        mt = {\'accuracy\': 0.1}\n        compare_proto(summary.hparams(hp, mt), self)\n\n    def test_hparams_smoke(self):\n        hp = {\'lr\': 0.1, \'bsize\': 4}\n        mt = {\'accuracy\': 0.1, \'loss\': 10}\n        summary.hparams(hp, mt)\n        \n        hp = {\'string\': ""1b"", \'use magic\': True}\n        summary.hparams(hp, mt)\n'"
tensorboardX/tests/test_summary_writer.py,0,"b'from tensorboardX import SummaryWriter\nimport unittest\n\n\nclass SummaryWriterTest(unittest.TestCase):\n    def test_summary_writer_ctx(self):\n        # after using a SummaryWriter as a ctx it should be closed\n        with SummaryWriter(filename_suffix=\'.test\') as writer:\n            writer.add_scalar(\'test\', 1)\n        assert writer.file_writer is None\n\n    def test_summary_writer_backcomapt(self):\n        with SummaryWriter(log_dir=\'/tmp/tbxtest\') as writer:\n            writer.add_scalar(\'test\', 1)\n\n    def test_summary_writer_close(self):\n        # Opening and closing SummaryWriter a lot should not run into\n        # OSError: [Errno 24] Too many open files\n        passed = True\n        try:\n            writer = SummaryWriter()\n            writer.close()\n        except OSError:\n            passed = False\n\n        assert passed\n\n    def test_windowsPath(self):\n        dummyPath = ""C:\\\\Downloads\\\\fjoweifj02utj43tj430""\n        with SummaryWriter(dummyPath) as writer:\n            writer.add_scalar(\'test\', 1)\n        import shutil\n        shutil.rmtree(dummyPath)\n\n    def test_pathlib(self):\n        import sys\n        if sys.version_info.major == 2:\n            import pathlib2 as pathlib\n        else:\n            import pathlib\n        p = pathlib.Path(\'./pathlibtest\')\n        with SummaryWriter(p) as writer:\n            writer.add_scalar(\'test\', 1)\n        import shutil\n        shutil.rmtree(str(p))\n'"
tensorboardX/tests/test_test.py,0,"b""def test_linting():\n    import subprocess\n    # subprocess.check_output(['flake8', 'tensorboardX'])\n"""
tensorboardX/tests/test_utils.py,0,"b""from tensorboardX import summary\nfrom tensorboardX.utils import make_grid, _prepare_video, convert_to_HWC\nimport numpy as np\nimport pytest\nimport unittest\n\n\nclass UtilsTest(unittest.TestCase):\n    def test_to_HWC(self):\n        np.random.seed(1)\n        test_image = np.random.randint(0, 256, size=(3, 32, 32), dtype=np.uint8)\n        converted = convert_to_HWC(test_image, 'chw')\n        assert converted.shape == (32, 32, 3)\n        test_image = np.random.randint(0, 256, size=(16, 3, 32, 32), dtype=np.uint8)\n        converted = convert_to_HWC(test_image, 'nchw')\n        assert converted.shape == (64, 256, 3)\n        test_image = np.random.randint(0, 256, size=(32, 32), dtype=np.uint8)\n        converted = convert_to_HWC(test_image, 'hw')\n        assert converted.shape == (32, 32, 3)\n\n    def test_prepare_video(self):\n        # at each timestep the sum over all other dimensions of the video should stay the same\n        np.random.seed(1)\n        V_before = np.random.random((4, 10, 3, 20, 20))\n        V_after = _prepare_video(np.copy(V_before))\n        V_before = np.swapaxes(V_before, 0, 1)\n        V_before = np.reshape(V_before, newshape=(10, -1))\n        V_after = np.reshape(V_after, newshape=(10, -1))\n        np.testing.assert_array_almost_equal(np.sum(V_before, axis=1), np.sum(V_after, axis=1))\n"""
tensorboardX/tests/test_visdom.py,0,"b""from tensorboardX import TorchVis\n\nimport numpy as np\nimport pytest\nimport unittest\n\ntrue_positive_counts = [75, 64, 21, 5, 0]\nfalse_positive_counts = [150, 105, 18, 0, 0]\ntrue_negative_counts = [0, 45, 132, 150, 150]\nfalse_negative_counts = [0, 11, 54, 70, 75]\nprecision = [0.3333333, 0.3786982, 0.5384616, 1.0, 0.0]\nrecall = [1.0, 0.8533334, 0.28, 0.0666667, 0.0]\n\n\nclass VisdomTest(unittest.TestCase):\n    def test_TorchVis(self):\n        w = TorchVis('visdom')\n        w.add_scalar('scalar_visdom', 1, 0)\n        w.add_scalar('scalar_visdom', 2, 1)\n        w.add_histogram('histogram_visdom', np.array([1, 2, 3, 4, 5]), 1)\n        w.add_image('image_visdom', np.ndarray((3, 20, 20)), 2)\n        # w.add_video('video_visdom', np.ndarray((1, 3, 10, 20, 20)), 3)\n        w.add_audio('audio_visdom', [1, 2, 3, 4, 5])\n        w.add_text('text_visdom', 'mystring')\n        w.add_pr_curve('pr_curve_visdom', np.random.randint(2, size=100), np.random.rand(100), 10)\n        w.add_pr_curve_raw('prcurve with raw data',\n                           true_positive_counts,\n                           false_positive_counts,\n                           true_negative_counts,\n                           false_negative_counts,\n                           precision,\n                           recall, 20)\n        del w\n"""
tensorboardX/tests/test_writer.py,0,"b'from tensorboardX import SummaryWriter\nfrom tensorboard.compat.tensorflow_stub.pywrap_tensorflow import PyRecordReader_New\nfrom tensorboardX.proto import event_pb2\n\nimport numpy as np\nimport pytest\nimport unittest\nimport time\nfreqs = [262, 294, 330, 349, 392, 440, 440, 440, 440, 440, 440]\n\ntrue_positive_counts = [75, 64, 21, 5, 0]\nfalse_positive_counts = [150, 105, 18, 0, 0]\ntrue_negative_counts = [0, 45, 132, 150, 150]\nfalse_negative_counts = [0, 11, 54, 70, 75]\nprecision = [0.3333333, 0.3786982, 0.5384616, 1.0, 0.0]\nrecall = [1.0, 0.8533334, 0.28, 0.0666667, 0.0]\n\n\nclass WriterTest(unittest.TestCase):\n    def test_flush(self):\n        N_TEST = 5\n        w = SummaryWriter(flush_secs=1)\n        f = w.file_writer.event_writer._ev_writer._file_name\n        for i in range(N_TEST):\n            w.add_scalar(\'a\', i)\n            time.sleep(2)\n        r = PyRecordReader_New(f)\n        r.GetNext()  # meta data, so skip\n        for _ in range(N_TEST):  # all of the data should be flushed\n            r.GetNext()\n\n    def test_flush_timer_is_long_so_data_is_not_there(self):\n        with self.assertRaises(BaseException):\n            N_TEST = 5\n            w = SummaryWriter(flush_secs=20)\n            f = w.file_writer.event_writer._ev_writer._file_name\n            for i in range(N_TEST):\n                w.add_scalar(\'a\', i)\n                time.sleep(2)\n            r = PyRecordReader_New(f)\n            r.GetNext()  # meta data, so skip\n            for _ in range(N_TEST):  # missing data\n                r.GetNext()\n\n    def test_flush_after_close(self):\n        N_TEST = 5\n        w = SummaryWriter(flush_secs=20)\n        f = w.file_writer.event_writer._ev_writer._file_name\n        for i in range(N_TEST):\n            w.add_scalar(\'a\', i)\n            time.sleep(2)\n        w.close()\n        r = PyRecordReader_New(f)\n        r.GetNext()  # meta data, so skip\n        for _ in range(N_TEST):  # all of the data should be flushed\n            r.GetNext()\n\n    def test_flush(self):\n        N_TEST = 5\n        w = SummaryWriter(flush_secs=20)\n        f = w.file_writer.event_writer._ev_writer._file_name\n        for i in range(N_TEST):\n            w.add_scalar(\'a\', i)\n            time.sleep(2)\n        w.flush()\n        r = PyRecordReader_New(f)\n        r.GetNext()  # meta data, so skip\n        for _ in range(N_TEST):  # all of the data should be flushed\n            r.GetNext()\n\n    def test_auto_close(self):\n        pass\n\n    def test_writer(self):\n        with SummaryWriter() as writer:\n            sample_rate = 44100\n\n            n_iter = 0\n            writer.add_scalar(\'data/scalar_systemtime\', 0.1, n_iter)\n            writer.add_scalar(\'data/scalar_customtime\', 0.2, n_iter, walltime=n_iter)\n            writer.add_scalars(\'data/scalar_group\', {""xsinx"": n_iter * np.sin(n_iter),\n                                                     ""xcosx"": n_iter * np.cos(n_iter),\n                                                     ""arctanx"": np.arctan(n_iter)}, n_iter)\n            x = np.zeros((32, 3, 64, 64))  # output from network\n            writer.add_images(\'Image\', x, n_iter)  # Tensor\n            writer.add_image_with_boxes(\'imagebox\',\n                                        np.zeros((3, 64, 64)),\n                                        np.array([[10, 10, 40, 40], [40, 40, 60, 60]]),\n                                        n_iter)\n            x = np.zeros(sample_rate * 2)\n\n            writer.add_audio(\'myAudio\', x, n_iter)\n            writer.add_video(\'myVideo\', np.random.rand(16, 48, 1, 28, 28).astype(np.float32), n_iter)\n            writer.add_text(\'Text\', \'text logged at step:\' + str(n_iter), n_iter)\n            writer.add_text(\'markdown Text\', \'\'\'a|b\\n-|-\\nc|d\'\'\', n_iter)\n            writer.add_histogram(\'hist\', np.random.rand(100, 100), n_iter)\n            writer.add_pr_curve(\'xoxo\', np.random.randint(2, size=100), np.random.rand(\n                100), n_iter)  # needs tensorboard 0.4RC or later\n            writer.add_pr_curve_raw(\'prcurve with raw data\', true_positive_counts,\n                                    false_positive_counts,\n                                    true_negative_counts,\n                                    false_negative_counts,\n                                    precision,\n                                    recall, n_iter)\n            # export scalar data to JSON for external processing\n            writer.export_scalars_to_json(""./all_scalars.json"")\n            imgs = []\n            for i in range(5):\n                imgs.append(np.ones((3, 100, 110)))\n            with SummaryWriter() as w:\n                w.add_images(\'img_list\', imgs, dataformats=\'CHW\')'"
apex/apex/RNN/RNNBackend.py,12,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nimport torch.nn.functional as F\n\nimport math\n\n\ndef is_iterable(maybe_iterable):\n    return isinstance(maybe_iterable, list) or isinstance(maybe_iterable, tuple)\n\n\ndef flatten_list(tens_list):\n    """"""\n    flatten_list\n    """"""\n    if not is_iterable(tens_list):\n        return tens_list\n    \n    return torch.cat(tens_list, dim=0).view(len(tens_list), *tens_list[0].size() )\n\n    \n#These modules always assumes batch_first\nclass bidirectionalRNN(nn.Module):\n    """"""\n    bidirectionalRNN\n    """"""\n    def __init__(self, inputRNN, num_layers=1, dropout = 0):\n        super(bidirectionalRNN, self).__init__()\n        self.dropout = dropout\n        self.fwd = stackedRNN(inputRNN, num_layers=num_layers, dropout = dropout)\n        self.bckwrd = stackedRNN(inputRNN.new_like(), num_layers=num_layers, dropout = dropout)\n        self.rnns = nn.ModuleList([self.fwd, self.bckwrd])\n        \n    #collect hidden option will return all hidden/cell states from entire RNN\n    def forward(self, input, collect_hidden=False):\n        """"""\n        forward()\n        """"""\n        seq_len = input.size(0)\n        bsz = input.size(1)\n\n        fwd_out, fwd_hiddens = list(self.fwd(input, collect_hidden = collect_hidden))\n        bckwrd_out, bckwrd_hiddens = list(self.bckwrd(input, reverse=True, collect_hidden = collect_hidden))\n        \n        output = torch.cat( [fwd_out, bckwrd_out], -1 )\n        hiddens = tuple( torch.cat(hidden, -1) for hidden in zip( fwd_hiddens, bckwrd_hiddens) )\n\n        return output, hiddens\n\n    def reset_parameters(self):\n        """"""\n        reset_parameters()\n        """"""\n        for rnn in self.rnns:\n            rnn.reset_parameters()\n        \n    def init_hidden(self, bsz):\n        """"""\n        init_hidden()\n        """"""\n        for rnn in self.rnns:\n            rnn.init_hidden(bsz)\n\n    def detach_hidden(self):\n        """"""\n        detach_hidden()\n        """"""\n        for rnn in self.rnns:\n            rnn.detachHidden()\n        \n    def reset_hidden(self, bsz):\n        """"""\n        reset_hidden()\n        """"""\n        for rnn in self.rnns:\n            rnn.reset_hidden(bsz)\n\n    def init_inference(self, bsz):    \n        """"""\n        init_inference()\n        """"""\n        for rnn in self.rnns:\n            rnn.init_inference(bsz)\n\n   \n#assumes hidden_state[0] of inputRNN is output hidden state\n#constructor either takes an RNNCell or list of RNN layers\nclass stackedRNN(nn.Module):        \n    """"""\n    stackedRNN\n    """"""\n    def __init__(self, inputRNN, num_layers=1, dropout=0):\n        super(stackedRNN, self).__init__()\n        \n        self.dropout = dropout\n        \n        if isinstance(inputRNN, RNNCell):\n            self.rnns = [inputRNN]\n            for i in range(num_layers-1):\n                self.rnns.append(inputRNN.new_like(inputRNN.output_size))\n        elif isinstance(inputRNN, list):\n            assert len(inputRNN) == num_layers, ""RNN list length must be equal to num_layers""\n            self.rnns=inputRNN\n        else:\n            raise RuntimeError()\n        \n        self.nLayers = len(self.rnns)\n        \n        self.rnns = nn.ModuleList(self.rnns)\n\n\n    \'\'\'\n    Returns output as hidden_state[0] Tensor([sequence steps][batch size][features])\n    If collect hidden will also return Tuple(\n        [n_hidden_states][sequence steps] Tensor([layer][batch size][features])\n    )\n    If not collect hidden will also return Tuple(\n        [n_hidden_states] Tensor([layer][batch size][features])\n    \'\'\'\n    def forward(self, input, collect_hidden=False, reverse=False):\n        """"""\n        forward()\n        """"""\n        seq_len = input.size(0)\n        bsz = input.size(1)\n        inp_iter = reversed(range(seq_len)) if reverse else range(seq_len)\n\n        hidden_states = [[] for i in range(self.nLayers)]\n        outputs = []\n\n        for seq in inp_iter:\n            for layer in range(self.nLayers):\n\n                if layer == 0:\n                    prev_out = input[seq]\n                    \n                outs = self.rnns[layer](prev_out)\n\n                if collect_hidden:\n                    hidden_states[layer].append(outs)\n                elif seq == seq_len-1:\n                    hidden_states[layer].append(outs)\n                    \n                prev_out = outs[0]\n\n            outputs.append(prev_out)\n\n        if reverse:\n            outputs = list(reversed(outputs))\n        \'\'\'\n        At this point outputs is in format:\n        list( [seq_length] x Tensor([bsz][features]) )\n        need to convert it to:\n        list( Tensor([seq_length][bsz][features]) )\n        \'\'\'\n        output = flatten_list(outputs)\n\n        \'\'\'\n        hidden_states at this point is in format:\n        list( [layer][seq_length][hidden_states] x Tensor([bsz][features]) )\n        need to convert it to:\n          For not collect hidden:\n            list( [hidden_states] x Tensor([layer][bsz][features]) )\n          For collect hidden:\n            list( [hidden_states][seq_length] x Tensor([layer][bsz][features]) )\n        \'\'\'\n        if not collect_hidden:\n            seq_len = 1\n        n_hid = self.rnns[0].n_hidden_states\n        new_hidden = [ [ [ None for k in range(self.nLayers)] for j in range(seq_len) ] for i in range(n_hid) ]\n\n\n        for i in range(n_hid):\n            for j in range(seq_len):\n                for k in range(self.nLayers):\n                    new_hidden[i][j][k] = hidden_states[k][j][i]\n\n        hidden_states = new_hidden\n        #Now in format list( [hidden_states][seq_length][layer] x Tensor([bsz][features]) )\n        #Reverse seq_length if reverse\n        if reverse:\n            hidden_states = list( list(reversed(list(entry))) for entry in hidden_states)\n\n        #flatten layer dimension into tensor\n        hiddens = list( list(\n            flatten_list(seq) for seq in hidden )\n                        for hidden in hidden_states )\n        \n        #Now in format list( [hidden_states][seq_length] x Tensor([layer][bsz][features]) )\n        #Remove seq_length dimension if not collect_hidden\n        if not collect_hidden:\n            hidden_states = list( entry[0] for entry in hidden_states)\n        return output, hidden_states\n    \n    def reset_parameters(self):\n        """"""\n        reset_parameters()\n        """"""\n        for rnn in self.rnns:\n            rnn.reset_parameters()\n        \n    def init_hidden(self, bsz):\n        """"""\n        init_hidden()\n        """"""\n        for rnn in self.rnns:\n            rnn.init_hidden(bsz)\n\n    def detach_hidden(self):\n        """"""\n        detach_hidden()\n        """"""\n        for rnn in self.rnns:\n            rnn.detach_hidden()\n        \n    def reset_hidden(self, bsz):\n        """"""\n        reset_hidden()\n        """"""\n        for rnn in self.rnns:\n            rnn.reset_hidden(bsz)\n\n    def init_inference(self, bsz):    \n        """""" \n        init_inference()\n        """"""\n        for rnn in self.rnns:\n            rnn.init_inference(bsz)\n\nclass RNNCell(nn.Module):\n    """""" \n    RNNCell \n    gate_multiplier is related to the architecture you\'re working with\n    For LSTM-like it will be 4 and GRU-like will be 3.\n    Always assumes input is NOT batch_first.\n    Output size that\'s not hidden size will use output projection\n    Hidden_states is number of hidden states that are needed for cell\n    if one will go directly to cell as tensor, if more will go as list\n    """"""\n    def __init__(self, gate_multiplier, input_size, hidden_size, cell, n_hidden_states = 2, bias = False, output_size = None):\n        super(RNNCell, self).__init__()\n\n        self.gate_multiplier = gate_multiplier\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.cell = cell\n        self.bias = bias\n        self.output_size = output_size\n        if output_size is None:\n            self.output_size = hidden_size\n\n        self.gate_size = gate_multiplier * self.hidden_size\n        self.n_hidden_states = n_hidden_states\n\n        self.w_ih = nn.Parameter(torch.Tensor(self.gate_size, self.input_size))\n        self.w_hh = nn.Parameter(torch.Tensor(self.gate_size, self.output_size))\n\n        #Check if there\'s recurrent projection\n        if(self.output_size != self.hidden_size):\n            self.w_ho = nn.Parameter(torch.Tensor(self.output_size, self.hidden_size))\n\n        self.b_ih = self.b_hh = None\n        if self.bias:\n            self.b_ih = nn.Parameter(torch.Tensor(self.gate_size))\n            self.b_hh = nn.Parameter(torch.Tensor(self.gate_size))\n            \n        #hidden states for forward\n        self.hidden = [ None for states in range(self.n_hidden_states)]\n\n        self.reset_parameters()\n\n    def new_like(self, new_input_size=None):\n        """"""\n        new_like()\n        """"""\n        if new_input_size is None:\n            new_input_size = self.input_size\n            \n        return type(self)(self.gate_multiplier,\n                       new_input_size,\n                       self.hidden_size,\n                       self.cell,\n                       self.n_hidden_states,\n                       self.bias,\n                       self.output_size)\n\n    \n    #Use xavier where we can (weights), otherwise use uniform (bias)\n    def reset_parameters(self, gain=1):\n        """"""\n        reset_parameters()\n        """"""\n        stdev = 1.0 / math.sqrt(self.hidden_size)\n        for param in self.parameters():\n            param.data.uniform_(-stdev, stdev)\n    \'\'\'\n    Xavier reset:\n    def reset_parameters(self, gain=1):\n        stdv = 1.0 / math.sqrt(self.gate_size)\n\n        for param in self.parameters():\n            if (param.dim() > 1):\n                torch.nn.init.xavier_normal(param, gain)\n            else:\n                param.data.uniform_(-stdv, stdv)\n    \'\'\'\n    def init_hidden(self, bsz):\n        """"""\n        init_hidden()\n        """"""\n        for param in self.parameters():\n            if param is not None:\n                a_param = param\n                break\n\n        for i, _ in enumerate(self.hidden):\n            if(self.hidden[i] is None or self.hidden[i].data.size()[0] != bsz):\n\n                if i==0:\n                    hidden_size = self.output_size\n                else:\n                    hidden_size = self.hidden_size\n\n                tens = a_param.data.new(bsz, hidden_size).zero_()\n                self.hidden[i] = Variable(tens, requires_grad=False)\n            \n        \n    def reset_hidden(self, bsz):\n        """"""\n        reset_hidden()\n        """"""\n        for i, _ in enumerate(self.hidden):\n            self.hidden[i] = None\n        self.init_hidden(bsz)\n\n    def detach_hidden(self):\n        """"""\n        detach_hidden()\n        """"""\n        for i, _ in enumerate(self.hidden):\n            if self.hidden[i] is None:\n                raise RuntimeError(""Must initialize hidden state before you can detach it"")\n        for i, _ in enumerate(self.hidden):\n            self.hidden[i] = self.hidden[i].detach()\n        \n    def forward(self, input):\n        """"""\n        forward()\n        if not inited or bsz has changed this will create hidden states\n        """"""\n        self.init_hidden(input.size()[0])\n\n        hidden_state = self.hidden[0] if self.n_hidden_states == 1 else self.hidden\n        self.hidden = self.cell(input, hidden_state, self.w_ih, self.w_hh, b_ih=self.b_ih, b_hh=self.b_hh)\n        if(self.n_hidden_states > 1):\n            self.hidden = list(self.hidden)\n        else:\n            self.hidden=[self.hidden]\n\n        if self.output_size != self.hidden_size:\n            self.hidden[0] = F.linear(self.hidden[0], self.w_ho)\n\n        return tuple(self.hidden)\n'"
apex/apex/RNN/__init__.py,0,"b""from .models import LSTM, GRU, ReLU, Tanh, mLSTM\n\n__all__ = ['models']\n"""
apex/apex/RNN/cells.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .RNNBackend import RNNCell\n\nfrom torch.nn._functions.thnn import rnnFusedPointwise as fusedBackend\n\nimport math \n\n\nclass mLSTMRNNCell(RNNCell):\n    """"""\n    mLSTMRNNCell\n    """"""\n\n    def __init__(self, input_size, hidden_size, bias = False, output_size = None):\n        gate_multiplier = 4\n        super(mLSTMRNNCell, self).__init__(gate_multiplier, input_size, hidden_size, mLSTMCell, n_hidden_states = 2, bias = bias, output_size = output_size)\n\n        self.w_mih = nn.Parameter(torch.Tensor(self.output_size, self.input_size))\n        self.w_mhh = nn.Parameter(torch.Tensor(self.output_size, self.output_size))\n\n        self.reset_parameters()\n\n    def forward(self, input):\n        """"""\n        mLSTMRNNCell.forward()\n        """"""\n        #if not inited or bsz has changed this will create hidden states\n        self.init_hidden(input.size()[0])\n\n        hidden_state = self.hidden[0] if self.n_hidden_states == 1 else self.hidden\n\n        self.hidden = list(\n                           self.cell(input, hidden_state, self.w_ih, self.w_hh, self.w_mih, self.w_mhh,\n                           b_ih=self.b_ih, b_hh=self.b_hh)\n        )\n        \n        if self.output_size != self.hidden_size:\n            self.hidden[0] = F.linear(self.hidden[0], self.w_ho)\n        return tuple(self.hidden)\n\n\n    def new_like(self, new_input_size=None):\n        if new_input_size is None:\n            new_input_size = self.input_size\n        \n        return type(self)(\n            new_input_size,\n            self.hidden_size,\n            self.bias,\n            self.output_size)\n\ndef mLSTMCell(input, hidden, w_ih, w_hh, w_mih, w_mhh, b_ih=None, b_hh=None):\n    """"""\n    mLSTMCell\n    """"""\n\n    if input.is_cuda:\n        igates = F.linear(input, w_ih)\n        m = F.linear(input, w_mih) * F.linear(hidden[0], w_mhh)\n        hgates = F.linear(m, w_hh)\n\n        state = fusedBackend.LSTMFused.apply\n        return state(igates, hgates, hidden[1], b_ih, b_hh)\n\n    hx, cx = hidden\n    \n    m = F.linear(input, w_mih) * F.linear(hidden[0], w_mhh)\n    gates = F.linear(input, w_ih, b_ih) + F.linear(m, w_hh, b_hh)\n\n    ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n\n    ingate = F.sigmoid(ingate)\n    forgetgate = F.sigmoid(forgetgate)\n    cellgate = F.tanh(cellgate)\n    outgate = F.sigmoid(outgate)\n    \n    cy = (forgetgate * cx) + (ingate * cellgate)\n    hy = outgate * F.tanh(cy)\n    \n    return hy, cy\n                                                                            \n'"
apex/apex/RNN/models.py,1,"b'import torch\n\nfrom torch.nn._functions.rnn import LSTMCell, RNNReLUCell, RNNTanhCell, GRUCell\n\nfrom .RNNBackend import bidirectionalRNN, stackedRNN, RNNCell\nfrom .cells import mLSTMRNNCell, mLSTMCell\n\ndef toRNNBackend(inputRNN, num_layers, bidirectional=False, dropout = 0):\n    """"""\n    :class:`toRNNBackend`\n    """"""\n\n    if bidirectional:\n        return bidirectionalRNN(inputRNN, num_layers, dropout = dropout)\n    else:\n        return stackedRNN(inputRNN, num_layers, dropout = dropout)\n\n\ndef LSTM(input_size, hidden_size, num_layers, bias=True, batch_first=False, dropout=0, bidirectional=False, output_size = None):\n    """"""\n    :class:`LSTM`\n    """"""\n    inputRNN = RNNCell(4, input_size, hidden_size, LSTMCell, 2, bias, output_size)\n    return toRNNBackend(inputRNN, num_layers, bidirectional, dropout=dropout)\n\ndef GRU(input_size, hidden_size, num_layers, bias=True, batch_first=False, dropout=0, bidirectional=False, output_size = None):\n    """"""\n    :class:`GRU`\n    """"""\n    inputRNN = RNNCell(3, input_size, hidden_size, GRUCell, 1, bias, output_size)\n    return toRNNBackend(inputRNN, num_layers, bidirectional, dropout=dropout)\n\ndef ReLU(input_size, hidden_size, num_layers, bias=True, batch_first=False, dropout=0, bidirectional=False, output_size = None):\n    """"""\n    :class:`ReLU`\n    """"""\n    inputRNN = RNNCell(1, input_size, hidden_size, RNNReLUCell, 1, bias, output_size)\n    return toRNNBackend(inputRNN, num_layers, bidirectional, dropout=dropout)\n\ndef Tanh(input_size, hidden_size, num_layers, bias=True, batch_first=False, dropout=0, bidirectional=False, output_size = None):\n    """"""\n    :class:`Tanh`\n    """"""\n    inputRNN = RNNCell(1, input_size, hidden_size, RNNTanhCell, 1, bias, output_size)\n    return toRNNBackend(inputRNN, num_layers, bidirectional, dropout=dropout)\n        \ndef mLSTM(input_size, hidden_size, num_layers, bias=True, batch_first=False, dropout=0, bidirectional=False, output_size = None):\n    """"""\n    :class:`mLSTM`\n    """"""\n    inputRNN = mLSTMRNNCell(input_size, hidden_size, bias=bias, output_size=output_size)\n    return toRNNBackend(inputRNN, num_layers, bidirectional, dropout=dropout)\n\n\n'"
apex/apex/amp/__init__.py,0,"b'from .amp import init, half_function, float_function, promote_function,\\\n    register_half_function, register_float_function, register_promote_function\nfrom .handle import scale_loss, disable_casts\nfrom .frontend import initialize\nfrom ._amp_state import master_params, _amp_state\n'"
apex/apex/amp/__version__.py,0,"b""VERSION = (0, 1, 0)\n__version__ = '.'.join(map(str, VERSION))\n"""
apex/apex/amp/_amp_state.py,4,"b'# This is a ""header object"" that allows different amp modules to communicate.\n# I\'m a C++ guy, not a python guy.  I decided this approach because it seemed most C++-like.  \n# But apparently it\'s ok:\n# http://effbot.org/pyfaq/how-do-i-share-global-variables-across-modules.htm\nimport os\nimport torch\n\nTORCH_MAJOR = int(torch.__version__.split(\'.\')[0])\nTORCH_MINOR = int(torch.__version__.split(\'.\')[1])\n\nif TORCH_MAJOR == 0:\n    import collections.abc as container_abcs\nelse:\n    from torch._six import container_abcs\n\n\nclass AmpState(object):\n    def __init__(self):\n        self.hard_override=False\n        self.allow_incoming_model_not_fp32 = False\n        self.verbosity=1\n\n\n# Attribute stash.  Could also just stash things as global module attributes.\n_amp_state = AmpState()\n\n\ndef warn_or_err(msg):\n    if _amp_state.hard_override:\n        print(""Warning:  "" + msg)\n    else:\n        raise RuntimeError(msg)\n        # I\'m not sure if allowing hard_override is a good idea.\n        # + ""  If you\'re sure you know what you\'re doing, supply "" +\n        #                    ""hard_override=True to amp.initialize."")\n\n\ndistributed = False\nif \'WORLD_SIZE\' in os.environ:\n    distributed = int(os.environ[\'WORLD_SIZE\']) > 1\n\n\ndef maybe_print(msg, rank0=False):\n    if _amp_state.verbosity > 0:\n        if rank0:\n            if distributed:\n                if torch.distributed.get_rank() == 0:\n                    print(msg)\n            else:\n                print(msg)\n        else:\n            print(msg)\n\n\n# def iter_params(param_groups):\n#     for group in param_groups:\n#         for p in group[\'params\']:\n#             yield p\n\n\ndef master_params(optimizer):\n    """"""\n    Generator expression that iterates over the params owned by ``optimizer``.\n\n    Args:\n        optimizer: An optimizer previously returned from ``amp.initialize``.\n    """"""\n    for group in optimizer.param_groups:\n        for p in group[\'params\']:\n            yield p\n'"
apex/apex/amp/_initialize.py,16,"b'import torch\nfrom torch._six import string_classes\nimport functools\nimport numpy as np\nimport warnings\nfrom ._amp_state import _amp_state, warn_or_err, container_abcs\nfrom .handle import disable_casts\nfrom .scaler import LossScaler\nfrom ._process_optimizer import _process_optimizer\nfrom apex.fp16_utils import convert_network\nfrom ..fp16_utils import FP16_Optimizer as FP16_Optimizer_general\nfrom ..optimizers import FP16_Optimizer as FP16_Optimizer_for_fused\nfrom ..optimizers import FusedAdam\nfrom ..parallel import DistributedDataParallel as apex_DDP\nfrom ..parallel.LARC import LARC\n\n\ndef to_type(dtype, t):\n    if isinstance(t, torch.Tensor):\n        if not t.is_cuda:\n            # This should not be a hard error, since it may be legitimate.\n            warnings.warn(""An input tensor was not cuda."")\n        # GANs require this.\n        # if t.requires_grad:\n        #     warn_or_err(""input data requires grad.  Since input data is not a model parameter,\\n""\n        #         ""its gradients will not be properly allreduced by DDP."")\n        if t.is_floating_point():\n            return t.to(dtype)\n        return t\n    else:\n        # Trust the user\'s custom batch type, that\'s all I can do here.\n        return t.to(dtype)\n\n\n# Modified from torch.optim.optimizer.py.  This is a bit more general than casted_args in utils.py.\ndef applier(value, fn):\n    if isinstance(value, torch.Tensor):\n        return fn(value)\n    elif isinstance(value, string_classes):\n        return value\n    elif isinstance(value, np.ndarray):\n        return value\n    elif hasattr(value, ""to""): # Allow handling of custom batch classes\n        return fn(value)\n    elif isinstance(value, container_abcs.Mapping):\n        return {applier(k, fn) : applier(v, fn) for k, v in value.items()}\n    elif isinstance(value, container_abcs.Iterable):\n        return type(value)(applier(v, fn) for v in value)\n    else:\n        # Do I want this to fire off even if someone chooses to pass something ordinary like\n        # an int or float?  May be more annoying than it\'s worth.\n        # print(""Warning:  unrecognized type in applier.  If your input data is a custom class, ""\n        #     ""provide it with a .to(dtype) method which converts its floating-point Tensors to dtype. ""\n        #     ""Amp will check for your custom to() and invoke it to cast the batch\'s ""\n        #     ""floating-point Tensors to the appropriate type. ""\n        #     ""Also, if your data is a custom class, it is your responsibility to ensure that ""\n        #     ""any Tensors you want to be cuda are already cuda.""\n        return value\n\n\ndef check_models(models):\n    for model in models:\n        parallel_type = None\n        if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n            parallel_type = ""torch.nn.parallel.DistributedDataParallel""\n        if isinstance(model, apex_DDP):\n            parallel_type = ""apex.parallel.DistributedDataParallel""\n        if isinstance(model, torch.nn.parallel.DataParallel):\n            parallel_type = ""torch.nn.parallel.DataParallel""\n        if parallel_type is not None:\n            raise RuntimeError(""Incoming model is an instance of {}. "".format(parallel_type) +\n                ""Parallel wrappers should only be applied to the model(s) AFTER \\n""\n                ""the model(s) have been returned from amp.initialize."")\n\n\ndef check_params_fp32(models):\n    for model in models:\n        for name, param in model.named_parameters():\n            if param.is_floating_point():\n                if \'Half\' in param.type():\n                    warn_or_err(""Found param {} with type {}, expected torch.cuda.FloatTensor.\\n""\n                        ""When using amp.initialize, you do not need to call .half() on your model\\n""\n                        ""before passing it, no matter what optimization level you choose."".format(\n                        name, param.type()))\n                elif not param.is_cuda:\n                    warn_or_err(""Found param {} with type {}, expected torch.cuda.FloatTensor.\\n""\n                        ""When using amp.initialize, you need to provide a model with parameters\\n""\n                        ""located on a CUDA device before passing it no matter what optimization level\\n""\n                        ""you chose. Use model.to(\'cuda\') to use the default device."".format(\n                        name, param.type()))\n\n        # Backward compatibility for PyTorch 0.4\n        if hasattr(model, \'named_buffers\'):\n            buf_iter = model.named_buffers()\n        else:\n            buf_iter = model._buffers\n        for obj in buf_iter:\n            if type(obj)==tuple:\n                name, buf = obj\n            else:\n                name, buf = obj, buf_iter[obj]\n            if buf.is_floating_point():\n                if \'Half\' in buf.type():\n                    warn_or_err(""Found buffer {} with type {}, expected torch.cuda.FloatTensor.\\n""\n                        ""When using amp.initialize, you do not need to call .half() on your model\\n""\n                        ""before passing it, no matter what optimization level you choose."".format(\n                        name, buf.type()))\n                elif not buf.is_cuda:\n                    warn_or_err(""Found buffer {} with type {}, expected torch.cuda.FloatTensor.\\n""\n                        ""When using amp.initialize, you need to provide a model with buffers\\n""\n                        ""located on a CUDA device before passing it no matter what optimization level\\n""\n                        ""you chose. Use model.to(\'cuda\') to use the default device."".format(\n                        name, buf.type()))\n\n\ndef check_optimizers(optimizers):\n    for optim in optimizers:\n        bad_optim_type = None\n        if isinstance(optim, FP16_Optimizer_general):\n            bad_optim_type = ""apex.fp16_utils.FP16_Optimizer""\n        if isinstance(optim, FP16_Optimizer_for_fused):\n            bad_optim_type = ""apex.optimizers.FP16_Optimizer""\n        if bad_optim_type is not None:\n            raise RuntimeError(""An incoming optimizer is an instance of {}. "".format(bad_optim_type) +\n                               ""The optimizer(s) passed to amp.initialize() must be bare \\n""\n                               ""instances of either ordinary Pytorch optimizers, or Apex fused \\n""\n                               ""optimizers (currently just FusedAdam, but FusedSGD will be added \\n""\n                               ""soon).  You should not manually wrap your optimizer in either \\n""\n                               ""apex.fp16_utils.FP16_Optimizer or apex.optimizers.FP16_Optimizer. \\n""\n                               ""amp.initialize will take care of that for you (if necessary) based \\n""\n                               ""on the specified opt_level (and optional overridden properties)."")\n\n\ndef wrap_fused_adam(optimizer, properties):\n    msg = \'Currently, the usage of FusedAdam is restricted to \'\\\n          \'amp.initialize(..., opt_level=""O2"", keep_batchnorm_fp32=False, \'\\\n          \'loss_scale=float or ""dynamic"").  We are working on enabling more general usage.\'\n\n    assert properties.master_weights is True, msg\n    assert properties.cast_model_type is torch.float16, msg\n    assert (properties.keep_batchnorm_fp32 is False or\n            properties.keep_batchnorm_fp32 is None), msg\n\n    if properties.loss_scale == ""dynamic"":\n        return FP16_Optimizer_for_fused(optimizer, dynamic_loss_scale=True)\n    else:\n        return FP16_Optimizer_for_fused(optimizer, static_loss_scale=properties.loss_scale)\n\n\ndef _initialize(models, optimizers, properties, num_losses=1, cast_model_outputs=None):\n    from apex.parallel import DistributedDataParallel as apex_DDP\n    from .amp import init as amp_init\n\n    optimizers_was_list = False\n    if isinstance(optimizers, torch.optim.Optimizer) or isinstance(optimizers, LARC):\n        optimizers = [optimizers]\n    elif optimizers is None:\n        optimizers = []\n    elif isinstance(optimizers, list):\n        optimizers_was_list = True\n        check_optimizers(optimizers)\n    else:\n        check_optimizers([optimizers])\n        raise TypeError(""optimizers must be either a single optimizer or a list of optimizers."")\n\n    if isinstance(models, torch.nn.Module):\n        models_was_list = False\n        models = [models]\n    elif isinstance(models, list):\n        models_was_list = True\n    else:\n        raise TypeError(""models must be either a single model or a list of models."")\n\n    check_models(models)\n\n    if not _amp_state.allow_incoming_model_not_fp32:\n        check_params_fp32(models)\n\n\n    # In the future, when FP16_Optimizer can be deprecated and master weights can\n    # become an attribute, remember to stash master weights before casting the model.\n\n    if properties.cast_model_type:\n        if properties.keep_batchnorm_fp32:\n            for model in models:\n                convert_network(model, properties.cast_model_type)\n        else:\n            for model in models:\n                model.to(properties.cast_model_type)\n\n        input_caster = functools.partial(to_type, properties.cast_model_type)\n        if cast_model_outputs is not None:\n            output_caster = functools.partial(to_type, cast_model_outputs)\n        else:\n            output_caster = functools.partial(to_type, torch.float32)\n\n        for model in models:\n            # Patch the forward method to cast incoming data to the correct type, and\n            # outgoing data to float32, so ""the user never needs to call .half().""\n            # I like writing things explicitly more than decorators.\n            def patch_forward(old_fwd):\n                def new_fwd(*args, **kwargs):\n                    output = old_fwd(*applier(args, input_caster),\n                                     **applier(kwargs, input_caster))\n                    return applier(output, output_caster)\n                return new_fwd\n\n            model.forward = patch_forward(model.forward)\n\n        # State dict trick to recast any preexisting per-param state tensors \n        for optimizer in optimizers:\n            optimizer.load_state_dict(optimizer.state_dict())\n    elif cast_model_outputs is not None:\n        output_caster = functools.partial(to_type, cast_model_outputs)\n\n        for model in models:\n            def patch_forward(old_fwd):\n                def new_fwd(*args, **kwargs):\n                    output = old_fwd(*args, **kwargs)\n                    return applier(output, output_caster)\n                return new_fwd\n\n            model.forward = patch_forward(model.forward)\n\n    for i, optimizer in enumerate(optimizers):\n        # Still need to special case this for the first pass\n        if isinstance(optimizer, FusedAdam):\n            optimizers[i] = wrap_fused_adam(optimizer, properties)\n        else:\n            optimizers[i] = _process_optimizer(optimizer, properties)\n\n    _amp_state.loss_scalers = []\n    for _ in range(num_losses):\n        _amp_state.loss_scalers.append(LossScaler(properties.loss_scale,\n                                                  min_loss_scale=_amp_state.min_loss_scale,\n                                                  max_loss_scale=_amp_state.max_loss_scale))\n\n    if properties.patch_torch_functions:\n        # handle is unused here. It\'s accessible later through a global value anyway.\n        handle = amp_init(loss_scale=properties.loss_scale, verbose=(_amp_state.verbosity == 2))\n        for optimizer in optimizers:\n            # Disable Amp casting for the optimizer step, because it should only be\n            # applied to FP32 master params anyway.\n            def patch_step(old_step):\n                def new_step(*args, **kwargs):\n                    with disable_casts():\n                        output = old_step(*args, **kwargs)\n                    return output\n                return new_step\n\n            optimizer.step = patch_step(optimizer.step)\n\n    if optimizers_was_list:\n        if models_was_list:\n            return models, optimizers\n        else:\n            return models[0], optimizers\n    else:\n        if models_was_list:\n            if len(optimizers) == 0:\n                return models\n            else:\n                return models, optimizers[0]\n        else:\n            if len(optimizers) == 0:\n                return models[0]\n            else:\n                return models[0], optimizers[0]\n'"
apex/apex/amp/_process_optimizer.py,17,"b'import types\nfrom ..fp16_utils import master_params_to_model_params\nfrom ..multi_tensor_apply import multi_tensor_applier\nfrom ._amp_state import maybe_print\nimport torch\n\n\nclass AmpOptimizerState(object):\n    def __init__(self):\n        pass\n\n\ndef lazy_init_with_master_weights(self):\n        stash = self._amp_stash\n        stash.fp16_groups = []\n        stash.fp32_from_fp16_groups = []\n        stash.fp32_from_fp32_groups = []\n        for i, param_group in enumerate(self.param_groups):\n            # maybe_print(""FP16_Optimizer processing param group {}:"".format(i))\n            fp16_params_this_group = []\n            fp32_params_this_group = []\n            fp32_from_fp16_params_this_group = []\n            for i, param in enumerate(param_group[\'params\']):\n                if param.requires_grad:\n                    if param.type() == \'torch.cuda.HalfTensor\':\n                        # maybe_print(""FP16_Optimizer received torch.cuda.HalfTensor with {}""\n                        #             .format(param.size()))\n                        fp16_params_this_group.append(param)\n                        master_param = param.detach().clone().float()\n                        master_param.requires_grad = True\n                        param_group[\'params\'][i] = master_param\n                        fp32_from_fp16_params_this_group.append(master_param)\n                        # Reset existing state dict key to the new master param.\n                        # We still need to recast per-param state tensors, if any, to FP32.\n                        if param in self.state:\n                           self.state[master_param] = self.state.pop(param)\n                    elif param.type() == \'torch.cuda.FloatTensor\':\n                        # maybe_print(""FP16_Optimizer received torch.cuda.FloatTensor with {}""\n                        #             .format(param.size()))\n                        fp32_params_this_group.append(param)\n                        param_group[\'params\'][i] = param\n                    else:\n                        raise TypeError(""Optimizer\'s parameters must be either ""\n                                        ""torch.cuda.FloatTensor or torch.cuda.HalfTensor. ""\n                                        ""Received {}"".format(param.type()))\n\n            stash.fp16_groups.append(fp16_params_this_group)\n            stash.fp32_from_fp16_groups.append(fp32_from_fp16_params_this_group)\n            stash.fp32_from_fp32_groups.append(fp32_params_this_group)\n\n        stash.all_fp16_params = []\n        for group in stash.fp16_groups:\n            stash.all_fp16_params += group\n\n        stash.all_fp32_from_fp16_params = []\n        for group in stash.fp32_from_fp16_groups:\n            stash.all_fp32_from_fp16_params += group\n\n        stash.all_fp32_from_fp32_params = []\n        for group in stash.fp32_from_fp32_groups:\n            stash.all_fp32_from_fp32_params += group\n\n        # stash.all_fp32_from_fp16_grad_stash = [None for _ in stash.all_fp32_from_fp16_params]\n        stash.all_fp32_from_fp32_grad_stash = [None for _ in stash.all_fp32_from_fp32_params]\n\n        for param in stash.all_fp32_from_fp16_params:\n            param.grad = None\n\n        for param in stash.all_fp32_from_fp32_params:\n            param.grad = None\n\n        # Leverage state_dict() and load_state_dict() to recast preexisting per-param state tensors\n        self.load_state_dict(self.state_dict())\n\n\ndef prepare_backward_with_master_weights(self):\n    stash = self._amp_stash\n\n    if not stash.lazy_init_called:\n        self._lazy_init_maybe_master_weights()\n        stash.lazy_init_called = True\n\n    for i, param in enumerate(stash.all_fp16_params):\n        # Set up to leverage grad copy elision:\n        param.grad = None\n\n    # for i, param in enumerate(stash.all_fp32_from_fp16_params):\n    #     stash.all_fp32_from_fp16_grad_stash[i] = param.grad\n\n    for i, param in enumerate(stash.all_fp32_from_fp32_params):\n        stash.all_fp32_from_fp32_grad_stash[i] = param.grad\n        # Set up to leverage grad copy elision:\n        param.grad = None\n\n\ndef post_backward_with_master_weights(self, scaler):\n    stash = self._amp_stash\n\n    # This is a lot of python overhead...\n    fp16_grads_needing_unscale = []\n    new_fp32_grads = []\n    fp16_grads_needing_unscale_with_stash = []\n    preexisting_fp32_grads = []\n    for fp16_param, fp32_param in zip(stash.all_fp16_params,\n                                      stash.all_fp32_from_fp16_params):\n        if fp16_param.grad is None and fp32_param.grad is not None:\n            continue\n        elif fp16_param.grad is not None and fp32_param.grad is None:\n            fp32_param.grad = torch.empty_like(fp32_param)\n            fp16_grads_needing_unscale.append(fp16_param.grad)\n            new_fp32_grads.append(fp32_param.grad)\n        elif fp16_param.grad is not None and fp32_param.grad is not None:\n            fp16_grads_needing_unscale_with_stash.append(fp16_param.grad)\n            preexisting_fp32_grads.append(fp32_param.grad)\n        else: # fp16_param.grad is None and fp32_param.grad is None:\n            continue\n\n    if len(fp16_grads_needing_unscale) > 0:\n        scaler.unscale(\n            fp16_grads_needing_unscale,\n            new_fp32_grads,\n            scaler.loss_scale(),\n            models_are_masters=False)\n\n    if len(fp16_grads_needing_unscale_with_stash) > 0:\n        scaler.unscale_with_stashed(\n            fp16_grads_needing_unscale_with_stash,\n            preexisting_fp32_grads,\n            preexisting_fp32_grads)\n\n    # fp32 params can be treated as they would be in the ""no_master_weights"" case.\n    grads_needing_unscale = []\n    grads_needing_unscale_with_stash = []\n    stashed = []\n    for param, stashed_grad in zip(stash.all_fp32_from_fp32_params,\n                                   stash.all_fp32_from_fp32_grad_stash):\n        if param.grad is None and stashed_grad is not None:\n            param.grad = stashed_grad\n        elif param.grad is not None and stashed_grad is None:\n            grads_needing_unscale.append(param.grad)\n        elif param.grad is not None and stashed_grad is not None:\n            grads_needing_unscale_with_stash.append(param.grad)\n            stashed.append(stashed_grad)\n        else: # param.grad is None and stashed_grad is None:\n            continue\n\n    if len(grads_needing_unscale) > 0:\n        scaler.unscale(\n            grads_needing_unscale,\n            grads_needing_unscale,\n            scaler.loss_scale(),\n            models_are_masters=True)\n\n    if len(grads_needing_unscale_with_stash) > 0:\n        scaler.unscale_with_stashed(\n            grads_needing_unscale_with_stash,\n            stashed,\n            grads_needing_unscale_with_stash)\n\n    # Clear the stash.\n    for i in range(len(stash.all_fp32_from_fp32_grad_stash)):\n        stash.all_fp32_from_fp32_grad_stash[i] = None\n\n\ndef lazy_init_no_master_weights(self):\n    stash = self._amp_stash\n    stash.all_fp16_params = []\n    stash.all_fp32_params = []\n    for i, param_group in enumerate(self.param_groups):\n        for i, param in enumerate(param_group[\'params\']):\n            if param.type() == \'torch.cuda.HalfTensor\':\n                stash.all_fp16_params.append(param)\n            elif param.type() == \'torch.cuda.FloatTensor\':\n                stash.all_fp32_params.append(param)\n            else:\n                raise TypeError(""Optimizer\'s parameters must be either ""\n                                ""torch.cuda.FloatTensor or torch.cuda.HalfTensor. ""\n                                ""Received {}"".format(param.type()))\n\n    stash.all_fp16_grad_stash = [None for _ in stash.all_fp16_params]\n    stash.all_fp32_grad_stash = [None for _ in stash.all_fp32_params]\n\n\ndef prepare_backward_no_master_weights(self):\n    stash = self._amp_stash\n\n    if not stash.lazy_init_called:\n        self._lazy_init_maybe_master_weights()\n        stash.lazy_init_called = True\n\n    for i, param in enumerate(stash.all_fp16_params):\n        stash.all_fp16_grad_stash[i] = param.grad\n        # Set up to leverage grad copy elision:\n        param.grad = None\n\n    for i, param in enumerate(stash.all_fp32_params):\n        stash.all_fp32_grad_stash[i] = param.grad\n        # Set up to leverage grad copy elision:\n        param.grad = None\n\n\ndef post_backward_no_master_weights(self, scaler):\n    stash = self._amp_stash\n\n    split_types = ((stash.all_fp16_params, stash.all_fp16_grad_stash),\n             (stash.all_fp32_params, stash.all_fp32_grad_stash))\n\n    for params, stashed_grads in split_types:\n        # This is a lot of python overhead...\n        grads_needing_unscale = []\n        grads_needing_unscale_with_stash = []\n        stashed = []\n        for param, stashed_grad in zip(params, stashed_grads):\n            if param.grad is None and stashed_grad is not None:\n                param.grad = stashed_grad\n            elif param.grad is not None and stashed_grad is None:\n                grads_needing_unscale.append(param.grad)\n            elif param.grad is not None and stashed_grad is not None:\n                grads_needing_unscale_with_stash.append(param.grad)\n                stashed.append(stashed_grad)\n            else: # param.grad is None and stashed_grad is None\n                continue\n\n        if len(grads_needing_unscale) > 0:\n            scaler.unscale(\n                grads_needing_unscale,\n                grads_needing_unscale,\n                scaler.loss_scale(),\n                models_are_masters=True)\n\n        if len(grads_needing_unscale_with_stash) > 0:\n            scaler.unscale_with_stashed(\n                grads_needing_unscale_with_stash,\n                stashed,\n                grads_needing_unscale_with_stash)\n\n        # Clear the stash.\n        for i in range(len(stashed_grads)):\n            stashed_grads[i] = None\n\n\ndef _master_params_to_model_params(self):\n    stash = self._amp_stash\n    if multi_tensor_applier.available:\n        if len(stash.all_fp16_params) > 0:\n            multi_tensor_applier(\n                stash.multi_tensor_scale,\n                stash.dummy_overflow_buf,\n                [stash.all_fp32_from_fp16_params, stash.all_fp16_params],\n                1.0)\n    else:\n        for fp16_group, fp32_from_fp16_group in zip(stash.fp16_groups, stash.fp32_from_fp16_groups):\n            master_params_to_model_params(fp16_group, fp32_from_fp16_group)\n\n\ndef _process_optimizer(optimizer, properties):\n    if hasattr(optimizer, ""_amp_stash""):\n        raise RuntimeError(""A given optimizer should only be passed through amp.initialize once."")\n    else:\n        optimizer._amp_stash = AmpOptimizerState()\n\n    optimizer._amp_stash.lazy_init_called = False\n    optimizer._amp_stash.already_patched = False\n    optimizer._amp_stash.params_have_scaled_gradients = False\n\n    for name in (""_lazy_init_maybe_master_weights"",\n                 ""_master_params_to_model_params"",\n                 ""_prepare_amp_backward"",\n                 ""_post_amp_backward""):\n        if hasattr(optimizer, name):\n            raise RuntimeError(""Incoming optimizer already has {} defined."".format(name))\n\n    # TODO:  Centralize exposure and import error checking for the C backend.\n    if multi_tensor_applier.available:\n        import amp_C\n        optimizer._amp_stash.multi_tensor_scale = amp_C.multi_tensor_scale\n        optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);\n\n    if properties.master_weights:\n        optimizer._lazy_init_maybe_master_weights = types.MethodType(\n            lazy_init_with_master_weights, optimizer)\n\n        optimizer._master_params_to_model_params = types.MethodType(\n            _master_params_to_model_params, optimizer)\n\n        old_step = optimizer.step\n        def new_step(self, closure=None):\n            if closure is not None:\n                raise RuntimeError(""Currently, Amp does not support closure use with optimizers."")\n            retval = old_step()\n            self._master_params_to_model_params()\n            # Clear the master grads that wouldn\'t be zeroed by model.zero_grad()\n            for param in self._amp_stash.all_fp32_from_fp16_params:\n                param.grad = None\n            return retval\n        optimizer.step = types.MethodType(new_step, optimizer)\n\n        old_zero_grad = optimizer.zero_grad\n        def new_zero_grad(self):\n            stash = self._amp_stash\n            if not stash.lazy_init_called:\n                self._lazy_init_maybe_master_weights()\n                stash.lazy_init_called = True\n            # Zero the model grads.\n            for param in stash.all_fp16_params:\n                if param.grad is not None:\n                    param.grad.detach_()\n                    param.grad.zero_()\n            for param in stash.all_fp32_from_fp32_params:\n                if param.grad is not None:\n                    param.grad.detach_()\n                    param.grad.zero_()\n            # Clear the master grads that are independent of model grads\n            for param in self._amp_stash.all_fp32_from_fp16_params:\n                param.grad = None\n        optimizer.zero_grad = types.MethodType(new_zero_grad, optimizer)\n\n        optimizer._prepare_amp_backward = types.MethodType(\n            prepare_backward_with_master_weights, optimizer)\n\n        optimizer._post_amp_backward = types.MethodType(\n            post_backward_with_master_weights, optimizer)\n    else:\n        optimizer._lazy_init_maybe_master_weights = types.MethodType(\n            lazy_init_no_master_weights, optimizer)\n\n        optimizer._prepare_amp_backward = types.MethodType(\n            prepare_backward_no_master_weights, optimizer)\n\n        optimizer._post_amp_backward = types.MethodType(\n            post_backward_no_master_weights, optimizer)\n\n    old_add_param_group = optimizer.add_param_group\n\n    def new_add_param_group(self, new_group):\n        stash = self._amp_stash\n\n        if not stash.lazy_init_called:\n            self._lazy_init_maybe_master_weights()\n            stash.lazy_init_called = True\n\n        assert isinstance(new_group, dict), ""param group must be a dict""\n\n        new_params = new_group[\'params\']\n        if isinstance(new_params, torch.Tensor):\n            new_group[\'params\'] = [new_params]\n        elif isinstance(new_params, set):\n            raise TypeError(\'optimizer parameters need to be organized in ordered collections, but \'\n                            \'the ordering of tensors in sets will change between runs. Please use a list instead.\')\n        else:\n            new_group[\'params\'] = list(new_params)\n\n        if properties.master_weights:\n            # Mutate new_group in-place to use FP32 master params\n            fp16_params_this_group = []\n            fp32_params_this_group = []\n            fp32_from_fp16_params_this_group = []\n            for i, param in enumerate(new_group[\'params\']):\n                if param.requires_grad:\n                    if param.type() == \'torch.cuda.HalfTensor\':\n                        fp16_params_this_group.append(param)\n                        master_param = param.detach().clone().float()\n                        master_param.requires_grad = True\n                        new_group[\'params\'][i] = master_param\n                        fp32_from_fp16_params_this_group.append(master_param)\n                    elif param.type() == \'torch.cuda.FloatTensor\':\n                        fp32_params_this_group.append(param)\n                        new_group[\'params\'][i] = param\n                    else:\n                        raise TypeError(""Optimizer\'s parameters must be either ""\n                                        ""torch.cuda.FloatTensor or torch.cuda.HalfTensor. ""\n                                        ""Received {}"".format(param.type()))\n\n            stash.fp16_groups.append(fp16_params_this_group)\n            stash.fp32_from_fp16_groups.append(fp32_from_fp16_params_this_group)\n            stash.fp32_from_fp32_groups.append(fp32_params_this_group)\n\n            stash.all_fp16_params += fp16_params_this_group\n            stash.all_fp32_from_fp16_params += fp32_from_fp16_params_this_group\n            stash.all_fp32_from_fp32_params += fp32_params_this_group\n\n            # stash.all_fp32_from_fp16_grad_stash = [None for _ in stash.all_fp32_from_fp16_params]\n            stash.all_fp32_from_fp32_grad_stash += [None for _ in fp32_params_this_group]\n\n            # It should be ok to let params be added with existing .grad attributes.\n            # for param in fp16_params_this_group:\n            #     param.grad = None\n\n            # for param in fp32_from_fp16_params_this_group:\n            #     param.grad = None\n\n            # for param in stash.fp32_params_this_group:\n            #     param.grad = None\n        else:\n            for param in new_group[\'params\']:\n                if param.type() == \'torch.cuda.HalfTensor\':\n                    stash.all_fp16_params.append(param)\n                    stash.all_fp16_grad_stash.append(None)\n                elif param.type() == \'torch.cuda.FloatTensor\':\n                    stash.all_fp32_params.append(param)\n                    stash.all_fp32_grad_stash.append(None)\n                else:\n                    raise TypeError(""Optimizer\'s parameters must be either ""\n                                    ""torch.cuda.FloatTensor or torch.cuda.HalfTensor. ""\n                                    ""Received {}"".format(param.type()))\n\n        old_add_param_group(new_group)\n\n    optimizer.add_param_group = types.MethodType(new_add_param_group, optimizer)\n\n    return optimizer\n'"
apex/apex/amp/amp.py,10,"b'from . import compat, rnn_compat, utils, wrap\nfrom .handle import AmpHandle, NoOpHandle\nfrom .lists import functional_overrides, torch_overrides, tensor_overrides\nfrom ._amp_state import _amp_state\nfrom .frontend import *\n\nimport functools\nimport itertools\n\nimport torch\n\n\n_DECORATOR_HANDLE = None\n_USER_CAST_REGISTRY = set()\n_USER_PROMOTE_REGISTRY = set()\n\n\ndef _decorator_helper(orig_fn, cast_fn, wrap_fn):\n    def wrapper(*args, **kwargs):\n        handle = _DECORATOR_HANDLE\n        if handle is None or not handle.is_active():\n            return orig_fn(*args, **kwargs)\n        inner_cast_fn = utils.verbosify(cast_fn, orig_fn.__name__,\n                                  handle.verbose)\n        return wrap_fn(orig_fn, inner_cast_fn, handle)(*args, **kwargs)\n    return wrapper\n\n\n# Decorator form\ndef half_function(fn):\n    wrap_fn = functools.partial(wrap.make_cast_wrapper, try_caching=True)\n    return _decorator_helper(fn, utils.maybe_half, wrap_fn)\n\n\ndef float_function(fn):\n    wrap_fn = functools.partial(wrap.make_cast_wrapper, try_caching=False)\n    return _decorator_helper(fn, utils.maybe_float, wrap_fn)\n\n\ndef promote_function(fn):\n    wrap_fn = functools.partial(wrap.make_promote_wrapper)\n    return _decorator_helper(fn, utils.maybe_float, wrap_fn)\n\n\n# Registry form\ndef register_half_function(module, name):\n    if not hasattr(module, name):\n        raise ValueError(\'No function named {} in module {}.\'.format(\n            name, module))\n    _USER_CAST_REGISTRY.add((module, name, utils.maybe_half))\n\n\ndef register_float_function(module, name):\n    if not hasattr(module, name):\n        raise ValueError(\'No function named {} in module {}.\'.format(\n            name, module))\n    _USER_CAST_REGISTRY.add((module, name, utils.maybe_float))\n\n\ndef register_promote_function(module, name):\n    if not hasattr(module, name):\n        raise ValueError(\'No function named {} in module {}.\'.format(\n            name, module))\n    _USER_PROMOTE_REGISTRY.add((module, name))\n\n\n# Top-level function to insert _all_ the hooks.\ndef init(enabled=True, loss_scale=""dynamic"", enable_caching=True, verbose=False, allow_banned=False):\n    global _DECORATOR_HANDLE\n\n    if not enabled:\n        handle = NoOpHandle()\n        _DECORATOR_HANDLE = handle\n        return handle\n\n    handle = AmpHandle(loss_scale, enable_caching, verbose)\n\n    # 0) Force-{fp16, fp32} for user-annotated functions\n    for mod, fn, cast_fn in _USER_CAST_REGISTRY:\n        try_caching = (cast_fn == utils.maybe_half)\n        wrap.cached_cast(mod, fn, cast_fn, handle,\n                         try_caching, verbose)\n    _USER_CAST_REGISTRY.clear()\n\n    # 0.5) Force-promote for user-annotated functions\n    for mod, fn in _USER_PROMOTE_REGISTRY:\n        wrap.promote(mod, fn, handle, verbose)\n    _USER_PROMOTE_REGISTRY.clear()\n\n    # 1) Force-{fp16, fp32} on white- / black-list functions\n    override_modules = [functional_overrides,\n                        torch_overrides,\n                        tensor_overrides]\n    cast_table = [(\'FP16_FUNCS\', utils.maybe_half),\n                  (\'FP32_FUNCS\', utils.maybe_float)]\n    for module, (list_name, cast_fn) in itertools.product(override_modules,\n                                                          cast_table):\n        for fn in getattr(module, list_name):\n            try_caching = (cast_fn == utils.maybe_half)\n            wrap.cached_cast(module.MODULE, fn, cast_fn, handle,\n                             try_caching, verbose)\n\n    # 1.5) Pre-0.4, put the blacklist methods on HalfTensor and whitelist\n    #      methods on FloatTensor, since they\'re distinct types.\n    if compat.tensor_is_float_tensor():\n        for fn in tensor_overrides.FP16_FUNCS:\n            wrap.cached_cast(torch.cuda.FloatTensor, fn, utils.maybe_half,\n                             handle, try_caching=True, verbose=verbose)\n        for fn in tensor_overrides.FP32_FUNCS:\n            wrap.cached_cast(torch.cuda.HalfTensor, fn, utils.maybe_float,\n                             handle, try_caching=False, verbose=verbose)\n\n    # 2) Enable type-promotion on multi-arg functions and methods.\n    #    NB: special handling for sequence fns (e.g. `torch.cat`).\n    promote_modules = [torch_overrides, tensor_overrides]\n    promote_table = [(\'CASTS\', wrap.promote),\n                     (\'SEQUENCE_CASTS\', wrap.sequence_promote)]\n    for promote_mod, (list_name, promote_fn) in itertools.product(promote_modules,\n                                                                  promote_table):\n        for fn in getattr(promote_mod, list_name):\n            promote_fn(promote_mod.MODULE, fn, handle, verbose)\n\n    # 2.5) Pre-0.4, add blacklist methods directly to HalfTensor and FloatTensor types\n    if compat.tensor_is_float_tensor():\n        for cls, (list_name, promote_fn) in itertools.product([torch.cuda.FloatTensor,\n                                                               torch.cuda.HalfTensor],\n                                                              promote_table):\n            for fn in getattr(tensor_overrides, list_name):\n                promote_fn(cls, fn, handle, verbose)\n\n    # 3) For any in-place version of a blacklist function, error if any input is fp16.\n    #    NB: this is overly conservative.\n    for fn in utils.as_inplace(torch_overrides.FP32_FUNCS):\n        wrap.err_if_any_half(torch_overrides.MODULE, fn, handle)\n\n    # 3.5) For any in-place blacklist method, error if called on fp16 tensor\n    for fn in utils.as_inplace(tensor_overrides.FP32_FUNCS):\n        wrap.err_if_arg0_half(tensor_overrides.MODULE, fn, handle, verbose)\n        if compat.tensor_is_float_tensor():\n            wrap.err_if_arg0_half(torch.cuda.HalfTensor, fn, handle, verbose)\n\n    # 4) For other in-place methods, match the type of self tensor\n    for fn in utils.as_inplace(itertools.chain(\n            tensor_overrides.FP16_FUNCS,\n            tensor_overrides.CASTS)):\n        wrap.promote_match_arg0(tensor_overrides.MODULE, fn, handle, verbose)\n        if compat.tensor_is_float_tensor():\n            wrap.promote_match_arg0(torch.cuda.HalfTensor, fn, handle, verbose)\n            wrap.promote_match_arg0(torch.cuda.FloatTensor, fn, handle, verbose)\n\n    # 5) RNNs + RNN cells are whitelisted specially\n    if rnn_compat.has_old_rnns():\n        wrap.rnn_cast(torch.nn.backends.thnn.backend, \'RNN\', handle, verbose)\n    if not rnn_compat.has_old_rnns():\n        # Patch in our own indirection of `_VF` in modules/rnn s.t. it is mutable.\n        torch.nn.modules.rnn._VF = rnn_compat.VariableFunctionsShim()\n        # Wrap all the rnns\n        for x in rnn_compat.RNN_NAMES:\n            wrap.new_rnn_cast(x.upper(), handle, verbose)\n\n    # Wrap all the RNN cells\n    rnn_compat.whitelist_rnn_cells(handle, verbose)\n\n    # 6) Place error+print message on banned functions.\n    #    Or, if allow_banned, then cast to FP32.\n    for fn, err_msg in functional_overrides.BANNED_FUNCS:\n        if allow_banned:\n            wrap.cached_cast(functional_overrides.MODULE, fn, utils.maybe_float,\n                             handle, try_caching=True, verbose=verbose)\n        else:\n            wrap.err_if_any_half(functional_overrides.MODULE, fn, handle, err_msg)\n\n    _DECORATOR_HANDLE = handle\n\n    _amp_state.handle = handle\n\n    return handle\n'"
apex/apex/amp/compat.py,11,"b""import torch\n\n# True for post-0.4, when Variables/Tensors merged.\ndef variable_is_tensor():\n    v = torch.autograd.Variable()\n    return isinstance(v, torch.Tensor)\n\ndef tensor_is_variable():\n    x = torch.Tensor()\n    return type(x) == torch.autograd.Variable\n\n# False for post-0.4\ndef tensor_is_float_tensor():\n    x = torch.Tensor()\n    return type(x) == torch.FloatTensor\n\n# Akin to `torch.is_tensor`, but returns True for Variable\n# objects in pre-0.4.\ndef is_tensor_like(x):\n    return torch.is_tensor(x) or isinstance(x, torch.autograd.Variable)\n\n# Wraps `torch.is_floating_point` if present, otherwise checks\n# the suffix of `x.type()`.\ndef is_floating_point(x):\n    if hasattr(torch, 'is_floating_point'):\n        return torch.is_floating_point(x)\n    try:\n        torch_type = x.type()\n        return torch_type.endswith('FloatTensor') or \\\n            torch_type.endswith('HalfTensor') or \\\n            torch_type.endswith('DoubleTensor')\n    except AttributeError:\n        return False\n\ndef scalar_python_val(x):\n    if hasattr(x, 'item'):\n        return x.item()\n    else:\n        if isinstance(x, torch.autograd.Variable):\n            return x.data[0]\n        else:\n            return x[0]\n"""
apex/apex/amp/frontend.py,11,"b'import torch\nfrom ._initialize import _initialize\nfrom ._amp_state import _amp_state, warn_or_err, maybe_print\n\n\nclass Properties(object):\n    """"""\n    This class has two purposes: to establish a set of default properties,\n    and to route setting of these attributes through __setattr__ so that (in theory)\n    they can be checked for consistency with other existing args.\n    """"""\n    def __init__(self):\n        self.options = {\n            ""enabled"" : False,\n            ""opt_level"" : None,\n            ""cast_model_type"" : None,\n            ""patch_torch_functions"" : False,\n            ""keep_batchnorm_fp32"" : None,\n            ""master_weights"" : None,\n            ""loss_scale"" : 1.0,\n            # Reserved for future functionality\n            # ""fused_optimizer"" : False,\n            # ""enable_ddp_interop"" : False,\n            }\n\n    """"""\n    This function allows updating several options at a time without routing through\n    __setattr__ checks, to avoid ""you can\'t get there from here"" scenarios.\n    Currently not intended to be exposed; users are expected to select an opt_level\n    and apply consistent modifications.\n    """"""\n    def _update_options_dict(new_options):\n        for k, v in new_options:\n            if k in self.options:\n                self.options[k] = v\n            else:\n                raise ValueError(""Tried to set unexpected option {}"".format(k))\n    """"""\n    The members of ""options"" are not direct attributes of self, so access attempts\n    will roll down to __getattr__.  This borrows from the logic in torch.nn.Module.\n    """"""\n    def __getattr__(self, name):\n        if ""options"" in self.__dict__:\n            options =  self.__dict__[""options""]\n            if name in options:\n                return options[name]\n        raise AttributeError(""\'{}\' object has no attribute \'{}\'"".format(\n            type(self).__name__, name))\n\n    def __setattr__(self, name, value):\n        if ""options"" in self.__dict__:\n            if name in self.options:\n                # print(""setting {} {}"".format(name, value))\n                if name == ""cast_model_type"":\n                    if self.opt_level == ""O1"" and value is not None:\n                        if value is not False:\n                            if value is not torch.float32:\n                                warn_or_err(""O1 inserts casts around Torch functions rather than ""\n                                            ""model weights, so with O1, the model weights themselves ""\n                                            ""should remain FP32. If you wish to cast the model to a ""\n                                            ""different type, use opt_level=\'O2\' or \'O3\'. "" +\n                                            ""cast_model_type was {}"".format(value))\n                    self.options[name] = value\n                elif name == ""patch_torch_functions"":\n                    if self.opt_level != ""O1"" and value:\n                        warn_or_err(""Currently, patch_torch_functions=True should only be set by ""\n                                    ""selecting opt_level=\'O1\'."")\n                    self.options[name] = value\n                elif name == ""keep_batchnorm_fp32"":\n                    if self.opt_level == ""O1"" and value is not None:\n                        warn_or_err(""With opt_level O1, batchnorm functions are automatically patched ""\n                                    ""to run in FP32, so keep_batchnorm_fp32 should be None."" +\n                                    "" keep_batchnorm_fp32 was {}"".format(value))\n                    if value == ""False"":\n                        self.options[name] = False\n                    elif value == ""True"":\n                        self.options[name] = True\n                    else:\n                        assert (value is True or value is False or value is None),\\\n                            ""keep_batchnorm_fp32 must be a boolean, the string \'True\' or \'False\', ""\\\n                            ""or None, found keep_batchnorm_fp32={}"".format(value)\n                        self.options[name] = value\n                elif name == ""master_weights"":\n                    if self.opt_level == ""O1"" and value is not None:\n                        warn_or_err(""It doesn\'t make sense to use master_weights with O1. ""\n                                    ""With O1, your model weights themselves should be FP32."")\n                    self.options[name] = value\n                elif name == ""loss_scale"":\n                    if value == ""dynamic"":\n                        self.options[name] = value\n                    else:\n                        self.options[name] = float(value)\n                else:\n                    self.options[name] = value\n        else:\n            super(Properties, self).__setattr__(name, value)\n\n\n"""""" O0-O3 are convenience wrappers to establish defaults for typically used mixed precision options. """"""\n\nclass O3:\n    brief = ""O3:  Pure FP16 training.""\n    more = ""Calls .half() on your model, converting the entire model to FP16.\\n""\\\n        ""A casting operation is also inserted to cast incoming Tensors to FP16,\\n""\\\n        ""so you don\'t need to change your data pipeline.\\n""\\\n        ""This mode is useful for establishing a performance ceiling.\\n""\\\n        ""It\'s also possible training may \'just work\' in this mode.\\n""\\\n        ""If not, try other optimization levels.""\n\n    def __call__(self, properties):\n        properties.enabled = True\n        properties.opt_level = ""O3""\n        properties.cast_model_type = torch.float16\n        properties.patch_torch_functions = False\n        properties.keep_batchnorm_fp32 = False\n        properties.master_weights = False\n        properties.loss_scale = 1.0\n        # properties.fused_optimizer = False\n        # properties.enable_ddp_interop = False\n        return properties # modified in place so this isn\'t really necessary\n\n\nclass O2:\n    brief = ""O2:  FP16 training with FP32 batchnorm and FP32 master weights.\\n""\n    more = ""Calls .half() on your model, converting the entire model (except for batchnorms)\\n""\\\n        ""to FP16.  Batchnorms are retained in FP32 for additional stability.\\n""\\\n        ""The forward pass is patched to cast incoming Tensors to FP16, so you don\'t need to change\\n""\\\n        ""your data pipeline.\\n""\\\n        ""O2 creates FP32 master weights outside the model and patches any optimizers to update\\n""\\\n        ""these master weights, then copy the master weights into the FP16 model weights.\\n""\\\n        ""Master weights can also improve convergence and stability.""\n\n    def __call__(self, properties):\n        properties.enabled = True\n        properties.opt_level = ""O2""\n        properties.cast_model_type = torch.float16\n        properties.patch_torch_functions = False\n        properties.keep_batchnorm_fp32 = True\n        properties.master_weights = True\n        properties.loss_scale = ""dynamic""\n        # properties.fused_optimizer = False\n        # properties.enable_ddp_interop = False\n        return properties # modified in place so this isn\'t really necessary\n\n\nclass O1:\n    brief = ""O1:  Insert automatic casts around Pytorch functions and Tensor methods.\\n""\n    more = ""The type of your model\'s weights is not altered.  However, internally,\\n""\\\n        ""Pytorch functions are patched to cast any Tensor Core-friendly ops to FP16 for speed,\\n""\\\n        ""while operations that might benefit from the additional stability of FP32 are patched\\n""\\\n        ""to cast their inputs to fp32.\\n""\\\n        ""O1 is the safest way to try mixed precision training, and is recommended when\\n""\\\n        ""trying mixed precision training for the first time.""\n\n    def __call__(self, properties):\n        properties.enabled = True\n        properties.opt_level = ""O1""\n        properties.cast_model_type = None\n        properties.patch_torch_functions = True\n        properties.keep_batchnorm_fp32 = None\n        properties.master_weights = None\n        properties.loss_scale = ""dynamic""\n        # properties.fused_optimizer = False\n        # properties.enable_ddp_interop = False\n        return properties # modified in place so this isn\'t really necessary\n\n\nclass O0:\n    brief = ""O0:  Pure FP32 training.\\n""\n    more = ""Your models are checked to make sure parameters are FP32, but otherwise the\\n""\\\n        ""types of weights and internal Pytorch operations are not altered.  This mode disables any\\n""\\\n        ""FP16 arithmetic, although other optimizations like DDP interop may still be requested.\\n""\n\n    def __call__(self, properties):\n        properties.enabled = True\n        properties.opt_level = ""O0""\n        properties.cast_model_type = torch.float32\n        properties.patch_torch_functions = False\n        properties.keep_batchnorm_fp32 = None\n        properties.master_weights = False\n        properties.loss_scale = 1.0\n        # properties.fused_optimizer = False\n        # properties.enable_ddp_interop = False\n        return properties # modified in place so this isn\'t really necessary\n\n\nopt_levels = {""O3"": O3(),\n              ""O2"": O2(),\n              ""O1"": O1(),\n              ""O0"": O0()}\n\n\n# allow user to directly pass Properties struct as well?\ndef initialize(\n    models,\n    optimizers=None,\n    enabled=True,\n    opt_level=""O1"",\n    cast_model_type=None,\n    patch_torch_functions=None,\n    keep_batchnorm_fp32=None,\n    master_weights=None,\n    loss_scale=None,\n    cast_model_outputs=None,\n    num_losses=1,\n    verbosity=1,\n    min_loss_scale=None,\n    max_loss_scale=2.**24\n    ):\n    """"""\n    Initialize your models, optimizers, and the Torch tensor and functional namespace according to the\n    chosen ``opt_level`` and overridden properties, if any.\n\n    ``amp.initialize`` should be called **after** you have finished\n    constructing your model(s) and\n    optimizer(s), but **before** you send your model through any DistributedDataParallel wrapper.\n    See `Distributed training`_ in the Imagenet example.\n\n    Currently, ``amp.initialize`` should only be called **once**,\n    although it can process an arbitrary number of\n    models and optimizers (see the corresponding `Advanced Amp Usage topic`_).\n    If you think your use case requires ``amp.initialize`` to be called more than once,\n    `let us know`_.\n\n    Any property keyword argument that is not ``None`` will be interpreted as a manual override.\n\n    To prevent having to rewrite anything else in your script, name the returned models/optimizers\n    to replace the passed models/optimizers, as in the code sample below.\n\n    Args:\n        models (torch.nn.Module or list of torch.nn.Modules):  Models to modify/cast.\n        optimizers (optional, torch.optim.Optimizer or list of torch.optim.Optimizers):  Optimizers to modify/cast.\n            REQUIRED for training, optional for inference.\n        enabled (bool, optional, default=True):  If False, renders all Amp calls no-ops, so your script\n            should run as if Amp were not present.\n        opt_level (str, optional, default=""O1""):  Pure or mixed precision optimization level.  Accepted values are\n            ""O0"", ""O1"", ""O2"", and ""O3"", explained in detail above.\n        cast_model_type (``torch.dtype``, optional, default=None):  Optional property override, see\n            above.\n        patch_torch_functions (bool, optional, default=None):  Optional property override.\n        keep_batchnorm_fp32 (bool or str, optional, default=None):  Optional property override.  If\n            passed as a string, must be the string ""True"" or ""False"".\n        master_weights (bool, optional, default=None):  Optional property override.\n        loss_scale (float or str, optional, default=None):  Optional property override.  If passed as a string,\n            must be a string representing a number, e.g., ""128.0"", or the string ""dynamic"".\n        cast_model_outputs (torch.dtype, optional, default=None):  Option to ensure that the outputs\n            of your model(s) are always cast to a particular type regardless of ``opt_level``.\n        num_losses (int, optional, default=1):  Option to tell Amp in advance how many losses/backward\n            passes you plan to use.  When used in conjunction with the ``loss_id`` argument to\n            ``amp.scale_loss``, enables Amp to use a different loss scale per loss/backward pass,\n            which can improve stability.  See ""Multiple models/optimizers/losses""\n            under `Advanced Amp Usage`_ for examples.  If ``num_losses`` is left to 1, Amp will still\n            support multiple losses/backward passes, but use a single global loss scale\n            for all of them.\n        verbosity (int, default=1):  Set to 0 to suppress Amp-related output.\n        min_loss_scale (float, default=None):  Sets a floor for the loss scale values that can be chosen by dynamic\n            loss scaling.  The default value of None means that no floor is imposed.\n            If dynamic loss scaling is not used, `min_loss_scale` is ignored.\n        max_loss_scale (float, default=2.**24):  Sets a ceiling for the loss scale values that can be chosen by\n            dynamic loss scaling.  If dynamic loss scaling is not used, `max_loss_scale` is ignored.\n\n    Returns:\n        Model(s) and optimizer(s) modified according to the ``opt_level``.\n        If either the ``models`` or ``optimizers`` args were lists, the corresponding return value will\n        also be a list.\n\n    Permissible invocations::\n\n        model, optim = amp.initialize(model, optim,...)\n        model, [optim1, optim2] = amp.initialize(model, [optim1, optim2],...)\n        [model1, model2], optim = amp.initialize([model1, model2], optim,...)\n        [model1, model2], [optim1, optim2] = amp.initialize([model1, model2], [optim1, optim2],...)\n\n        # This is not an exhaustive list of the cross product of options that are possible,\n        # just a set of examples.\n        model, optim = amp.initialize(model, optim, opt_level=""O0"")\n        model, optim = amp.initialize(model, optim, opt_level=""O0"", loss_scale=""dynamic""|128.0|""128.0"")\n\n        model, optim = amp.initialize(model, optim, opt_level=""O1"") # uses ""loss_scale=""dynamic"" default\n        model, optim = amp.initialize(model, optim, opt_level=""O1"", loss_scale=128.0|""128.0"")\n\n        model, optim = amp.initialize(model, optim, opt_level=""O2"") # uses ""loss_scale=""dynamic"" default\n        model, optim = amp.initialize(model, optim, opt_level=""O2"", loss_scale=128.0|""128.0"")\n        model, optim = amp.initialize(model, optim, opt_level=""O2"", keep_batchnorm_fp32=True|False|""True""|""False"")\n\n        model, optim = amp.initialize(model, optim, opt_level=""O3"") # uses loss_scale=1.0 default\n        model, optim = amp.initialize(model, optim, opt_level=""O3"", loss_scale=""dynamic""|128.0|""128.0"")\n        model, optim = amp.initialize(model, optim, opt_level=""O3"", keep_batchnorm_fp32=True|False|""True""|""False"")\n\n    The `Imagenet example`_ demonstrates live use of various opt_levels and overrides.\n\n    .. _`Distributed training`:\n        https://github.com/NVIDIA/apex/tree/master/examples/imagenet#distributed-training\n\n    .. _`Imagenet example`:\n        https://github.com/NVIDIA/apex/tree/master/examples/imagenet\n\n    .. _`Advanced Amp Usage`:\n        https://nvidia.github.io/apex/advanced.html\n\n    .. _`Advanced Amp Usage topic`:\n        https://nvidia.github.io/apex/advanced.html#multiple-models-optimizers-losses\n\n    .. _`let us know`:\n        https://github.com/NVIDIA/apex/issues\n    """"""\n    _amp_state.opt_properties = Properties()\n    _amp_state.verbosity = verbosity\n\n    if not enabled:\n        if optimizers is None:\n            return models\n        else:\n            return models, optimizers\n\n    if not torch.backends.cudnn.enabled:\n        raise RuntimeError(\n            ""Amp requires torch.backends.cudnn.enabled = True"")\n\n    if opt_level not in opt_levels:\n        raise RuntimeError(\n            ""Unexpected optimization level {}. "".format(opt_level) +\n            ""Options are \'O0\', \'O1\', \'O2\', \'O3\'.  Note that in `O0`, `O1`, etc., the prefix O is the letter O, "" +\n            ""not the number zero."")\n    else:\n        _amp_state.opt_properties = opt_levels[opt_level](_amp_state.opt_properties)\n        maybe_print(""Selected optimization level {}"".format(opt_levels[opt_level].brief), True)\n        maybe_print(""Defaults for this optimization level are:"", True)\n        for k, v in _amp_state.opt_properties.options.items():\n            maybe_print(""{:22} : {}"".format(k, v), True)\n\n    _amp_state.min_loss_scale = min_loss_scale\n    _amp_state.max_loss_scale = max_loss_scale\n\n    maybe_print(""Processing user overrides (additional kwargs that are not None)..."", True)\n    # I chose to have the keyword arguments listed directly in the argument list,\n    # instead of **kwargs, so I can\'t use kwargs.items() here.\n    if enabled is not None:\n        _amp_state.opt_properties.enabled = enabled\n    if opt_level is not None:\n        _amp_state.opt_properties.opt_level = opt_level\n    if cast_model_type is not None:\n        _amp_state.opt_properties.cast_model_type = cast_model_type\n    if patch_torch_functions is not None:\n        _amp_state.opt_properties.patch_torch_functions = patch_torch_functions\n    if keep_batchnorm_fp32 is not None:\n        _amp_state.opt_properties.keep_batchnorm_fp32 = keep_batchnorm_fp32\n    if master_weights is not None:\n        _amp_state.opt_properties.master_weights = master_weights\n    if loss_scale is not None:\n        _amp_state.opt_properties.loss_scale = loss_scale\n\n    maybe_print(""After processing overrides, optimization options are:"", True)\n    for k, v in _amp_state.opt_properties.options.items():\n        maybe_print(""{:22} : {}"".format(k, v), True)\n\n    return _initialize(models, optimizers, _amp_state.opt_properties, num_losses, cast_model_outputs)\n\n\n# TODO:  is this necessary/useful?\n# def check_option_consistency(enabled=True,\n#                              opt_level=None,\n#                              cast_model_type=None,\n#                              patch_torch_functions=None,\n#                              keep_batchnorm_fp32=None,\n#                              master_weights=None,\n#                              loss_scale=None,\n#                              enable_ddp_interop=None,\n#                              hard_override=False):\n#     """"""\n#     Utility function that enables users to quickly check if the option combination they intend\n#     to use is permitted.  ``check_option_consistency`` does not require models or optimizers\n#     to be constructed, and can be called at any point in the script.  ``check_option_consistency``\n#     is totally self-contained; it does not set any amp global state or affect anything outside\n#     of itself.\n#     """"""\n#\n#     if not enabled:\n#         return\n#\n#     if opt_level not in opt_levels:\n#         raise RuntimeError(""Unexpected optimization level.  Options are \'O0\', \'O1\', \'O2\', \'O3\'."")\n#     else:\n#         opt_properties = opt_levels[opt_level](Properties())\n#         print(""Selected optimization level {}"", opt_levels[opt_level].brief)\n#         print(""Defaults for this optimization level are:"")\n#         for k, v in opt_properties.options:\n#             print(""{:22} : {}"".format(k, v))\n#\n#     print(""Processing user overrides (additional kwargs that are not None)..."")\n#     for k, v in kwargs:\n#         if k not in _amp_state.opt_properties.options:\n#             raise RuntimeError(""Unexpected kwarg {}"".format(k))\n#         if v is not None:\n#             setattr(opt_properties, k, v)\n#\n#     print(""After processing overrides, optimization options are:"")\n#     for k, v in opt_properties.options:\n#         print(""{:22} : {}"".format(k, v))\n'"
apex/apex/amp/handle.py,2,"b'import contextlib\nimport warnings\nimport torch\n\nfrom . import utils\nfrom .opt import OptimWrapper\nfrom .scaler import LossScaler\nfrom ._amp_state import _amp_state, master_params, maybe_print\nfrom ..fp16_utils import FP16_Optimizer as FP16_Optimizer_general\nfrom ..optimizers import FP16_Optimizer as FP16_Optimizer_for_fused\nfrom ..parallel.LARC import LARC\n\n\n# There\'s no reason to expose the notion of a ""handle"". Everything can happen through amp.* calls.\n@contextlib.contextmanager\ndef scale_loss(loss,\n               optimizers,\n               loss_id=0,\n               model=None,\n               delay_unscale=False,\n               delay_overflow_check=False):\n    """"""\n    On context manager entrance, creates ``scaled_loss = (loss.float())*current loss scale``.\n    ``scaled_loss`` is yielded so that the user can call ``scaled_loss.backward()``::\n\n        with amp.scale_loss(loss, optimizer) as scaled_loss:\n            scaled_loss.backward()\n\n    On context manager exit (if ``delay_unscale=False``), the gradients are checked for infs/NaNs\n    and unscaled, so that ``optimizer.step()`` can be called.\n\n    .. note::\n        If Amp is using explicit FP32 master params (which is the default for ``opt_level=O2``, and\n        can also be manually enabled by supplying ``master_weights=True`` to ``amp.initialize``)\n        any FP16 gradients are copied to FP32 master gradients before being unscaled.\n        ``optimizer.step()`` will then apply the unscaled master gradients to the master params.\n\n    .. warning::\n        If Amp is using explicit FP32 master params, only the FP32 master gradients will be\n        unscaled.  The direct ``.grad`` attributes of any FP16\n        model params will remain scaled after context manager exit.\n        This subtlety affects gradient clipping.  See ""Gradient clipping"" under\n        `Advanced Amp Usage`_ for best practices.\n\n    Args:\n        loss(Tensor):  Typically a scalar Tensor. The ``scaled_loss`` that the context\n            manager yields is simply ``loss.float()*loss_scale``, so in principle\n            ``loss`` could have more than one element, as long as you call\n            ``backward()`` on ``scaled_loss`` appropriately within the context manager body.\n        optimizers:  All optimizer(s) for which the current backward pass is creating gradients.\n            Must be an optimizer or list of optimizers returned from an earlier call\n            to ``amp.initialize``.  For example use with multiple optimizers, see\n            ""Multiple models/optimizers/losses"" under `Advanced Amp Usage`_.\n        loss_id(int, optional, default=0):  When used in conjunction with the ``num_losses`` argument\n            to ``amp.initialize``, enables Amp to use a different loss scale per loss.  ``loss_id``\n            must be an integer between 0 and ``num_losses`` that tells Amp which loss is\n            being used for the current backward pass.  See ""Multiple models/optimizers/losses""\n            under `Advanced Amp Usage`_ for examples.  If ``loss_id`` is left unspecified, Amp\n            will use the default global loss scaler for this backward pass.\n        model(torch.nn.Module, optional, default=None):  Currently unused, reserved to enable future\n            optimizations.\n        delay_unscale(bool, optional, default=False):  ``delay_unscale`` is never necessary, and\n            the default value of ``False`` is strongly recommended.\n            If ``True``, Amp will not unscale the gradients or perform model->master\n            gradient copies on context manager exit.\n            ``delay_unscale=True`` is a minor ninja performance optimization and can result\n            in weird gotchas (especially with multiple models/optimizers/losses),\n            so only use it if you know what you\'re doing.\n            ""Gradient accumulation across iterations"" under `Advanced Amp Usage`_\n            illustrates a situation where this CAN (but does not need to) be used.\n\n    .. warning::\n        If ``delay_unscale`` is ``True`` for a given backward pass, ``optimizer.step()`` cannot be\n        called yet after context manager exit, and must wait for another, later backward context\n        manager invocation with ``delay_unscale`` left to False.\n\n    .. _`Advanced Amp Usage`:\n        https://nvidia.github.io/apex/advanced.html\n    """"""\n    if not hasattr(_amp_state, ""opt_properties""):\n        raise RuntimeError(""Invoked \'with amp.scale_loss`, but internal Amp state has not been initialized.  ""\n                           ""model, optimizer = amp.initialize(model, optimizer, opt_level=...) must be called ""\n                           ""before `with amp.scale_loss`."")\n\n    if not _amp_state.opt_properties.enabled:\n        yield loss\n        return\n\n    if isinstance(optimizers, torch.optim.Optimizer) or isinstance(optimizers, LARC):\n        optimizers = [optimizers]\n\n    # this is what happens when i have to support tools from different sources under the same API...\n    # TODO:  Rewrite FusedAdam to use multi-tensor apply and the same loss scaler.\n    if isinstance(optimizers, FP16_Optimizer_for_fused):\n        loss_scale = optimizers.cur_scale\n    else:\n        loss_scaler = _amp_state.loss_scalers[loss_id]\n        loss_scale = loss_scaler.loss_scale()\n\n    if ((not _amp_state.opt_properties.master_weights)\n        and (not loss_scaler.dynamic)\n        and loss_scale == 1.0):\n        yield loss.float()\n        # Needing to drop the cache here as well is an ugly gotcha.\n        # But for now I think it\'s necessary to short-circuit.\n        # Probably ok to skip this if not delay_unscale\n        if _amp_state.opt_properties.patch_torch_functions:\n            _amp_state.handle._clear_cache()\n        return\n\n    if not delay_unscale:\n        if isinstance(optimizers, list):\n            for optimizer in optimizers:\n                if not optimizer._amp_stash.params_have_scaled_gradients:\n                    optimizer._prepare_amp_backward()\n\n    yield (loss.float())*loss_scale\n\n    if delay_unscale:\n        for optimizer in optimizers:\n            optimizer._amp_stash.params_have_scaled_gradients = True\n    else:\n        # FusedAdam and FusedSGD will take care of unscaling as part of their step() methods.\n        if not isinstance(optimizers, FP16_Optimizer_for_fused):\n            loss_scaler.clear_overflow_state()\n            for optimizer in optimizers:\n                optimizer._post_amp_backward(loss_scaler)\n                optimizer._amp_stash.params_have_scaled_gradients = False\n            # For future fused optimizers that enable sync-free dynamic loss scaling,\n            # should_skip will always be False.\n            should_skip = False if delay_overflow_check else loss_scaler.update_scale()\n            if should_skip:\n                for optimizer in optimizers:\n                    if not optimizer._amp_stash.already_patched:\n                        # Close on loss_scaler and loss_id as well, to be safe.  Probably not\n                        # necessary because amp.scale_loss is already creating a temporary scope.\n                        def patch_step(opt, loss_scaler, loss_id):\n                            opt_step = opt.step\n                            def skip_step(closure=None):\n                                if closure is not None:\n                                    raise RuntimeError(""Currently, Amp does not support closure use with optimizers."")\n                                maybe_print((""Gradient overflow.  Skipping step, loss scaler "" +\n                                             ""{} reducing loss scale to {}"").format(loss_id,\n                                             loss_scaler.loss_scale()))\n                                if hasattr(opt._amp_stash, ""all_fp32_from_fp16_params""):\n                                    # Clear the master grads that wouldn\'t be zeroed by model.zero_grad()\n                                    for param in opt._amp_stash.all_fp32_from_fp16_params:\n                                        param.grad = None\n                                opt.step = opt_step\n                                opt._amp_stash.already_patched = False\n                            return skip_step\n                        optimizer.step = patch_step(optimizer, loss_scaler, loss_id)\n                        optimizer._amp_stash.already_patched = True\n\n    # Probably ok to skip this if not delay_unscale\n    if _amp_state.opt_properties.patch_torch_functions:\n        _amp_state.handle._clear_cache()\n\n\n# Free function version of AmpHandle.disable_casts, another step on the\n# path to removing the concept of ""AmpHandle""\n@contextlib.contextmanager\ndef disable_casts():\n    _amp_state.handle._is_active = False\n    yield\n    _amp_state.handle._is_active = True\n\n\nclass AmpHandle(object):\n    def __init__(self, loss_scale=""dynamic"", enable_caching=True, verbose=False):\n        self._enable_caching = enable_caching\n        self._verbose = verbose\n        self._cache = dict()\n        self._default_scaler = LossScaler(loss_scale)\n        self._is_active = True\n        self._all_wrappers = []\n\n    def is_active(self):\n        return self._is_active\n\n    @contextlib.contextmanager\n    def _disable_casts(self):\n        self._is_active = False\n        yield\n        self._is_active = True\n\n    def wrap_optimizer(self, optimizer, num_loss=1):\n        self._default_scaler = None\n        return OptimWrapper(optimizer, self, num_loss)\n\n    @contextlib.contextmanager\n    def scale_loss(self, loss, optimizer):\n        raise RuntimeError(""The old Amp API is no longer supported.  Please move to the new API, ""\n            ""documented here:  https://nvidia.github.io/apex/amp.html.  Transition guide:  ""\n            ""https://nvidia.github.io/apex/amp.html#transition-guide-for-old-api-users"")\n\n        if not self.is_active():\n            yield loss\n            return\n\n        if self._default_scaler is None:\n            raise RuntimeError(\n                \'After calling `handle.wrap_optimizer()`, you must explicitly \' +\n                \'use `optimizer.scale_loss(loss)`.\')\n\n        # TODO: this code block is duplicated here and `opt.py`. Unify.\n        loss_scale = self._default_scaler.loss_scale()\n        yield loss * loss_scale\n\n        self._default_scaler.clear_overflow_state()\n        self._default_scaler.unscale(\n            master_params(optimizer),\n            master_params(optimizer),\n            loss_scale)\n        should_skip = self._default_scaler.update_scale()\n        if should_skip:\n            optimizer_step = optimizer.step\n            def skip_step():\n                maybe_print(\'Gradient overflow, skipping update\')\n                optimizer.step = optimizer_step\n            optimizer.step = skip_step\n\n        self._clear_cache()\n\n    def _clear_cache(self):\n        self._cache.clear()\n\n    # Experimental support for saving / restoring uncasted versions of functions\n    def _save_func(self, mod, fn, func):\n        self._all_wrappers.append((mod, fn, func))\n\n    def _deactivate(self):\n        for mod, fn, func in self._all_wrappers:\n            utils.set_func(mod, fn, func)\n        self._all_wrappers = []\n\n    @property\n    def has_cache(self):\n        return self._enable_caching\n\n    @property\n    def cache(self):\n        return self._cache\n\n    def remove_cache(self, param):\n        if self.has_cache and param in self.cache:\n            del self.cache[param]\n\n    @property\n    def verbose(self):\n        return self._verbose\n\nclass NoOpHandle(object):\n    def is_active(self):\n        return False\n\n    @contextlib.contextmanager\n    def _disable_casts(self):\n        yield\n\n    def wrap_optimizer(self, optimizer, num_loss=1):\n        return OptimWrapper(optimizer, self, num_loss)\n\n    @contextlib.contextmanager\n    def scale_loss(self, loss, optimizer):\n        yield loss\n\n    @property\n    def has_cache(self):\n        return False\n\n    @property\n    def verbose(self):\n        return False\n\n    def _clear_cache(self):\n        pass\n\n    def _deactivate(self):\n        pass\n'"
apex/apex/amp/opt.py,1,"b""import contextlib\nimport warnings\n\nfrom .scaler import LossScaler, master_params\nfrom ._amp_state import maybe_print\n\nimport numpy as np\n\nclass OptimWrapper(object):\n    def __init__(self, optimizer, amp_handle, num_loss):\n        self._optimizer = optimizer\n        self._amp_handle = amp_handle\n        self._num_loss = num_loss\n        self._loss_idx = 0\n        self._skip_next = [False] * num_loss\n        self._loss_scaler = [LossScaler('dynamic') for _ in range(num_loss)]\n\n    @contextlib.contextmanager\n    def scale_loss(self, loss):\n        if not self._amp_handle.is_active():\n            yield loss\n            return\n\n        # When there are multiple losses per-optimizer, we need\n        # to save out current grad accumulation, since we won't be\n        # able to unscale this particulare loss once the grads are\n        # all mixed together.\n        cached_grads = []\n        if self._loss_idx > 0:\n            for p in master_params(self._optimizer):\n                if p.grad is not None:\n                    cached_grads.append(p.grad.data.detach().clone())\n                else:\n                    cached_grads.append(None)\n            self._optimizer.zero_grad()\n\n        loss_scale = self._cur_loss_scaler().loss_scale()\n        yield loss * loss_scale\n\n        self._cur_loss_scaler().clear_overflow_state()\n        self._cur_loss_scaler().unscale(\n            master_params(self._optimizer),\n            master_params(self._optimizer),\n            loss_scale)\n        self._skip_next[self._loss_idx] = self._cur_loss_scaler().update_scale()\n        self._loss_idx += 1\n\n        if len(cached_grads) > 0:\n            for p, cached_grad in zip(master_params(self._optimizer),\n                                      cached_grads):\n                if cached_grad is not None:\n                    p.grad.data.add_(cached_grad)\n            cached_grads = []\n\n    def _cur_loss_scaler(self):\n        assert 0 <= self._loss_idx < self._num_loss\n        return self._loss_scaler[self._loss_idx]\n\n    def step(self, closure=None):\n        if not self._amp_handle.is_active():\n            return self._optimizer.step(closure=closure)\n\n        self._loss_idx = 0\n\n        for group in self._optimizer.param_groups:\n            for p in group['params']:\n                self._amp_handle.remove_cache(p)\n\n        if closure is not None:\n            raise NotImplementedError(\n                'The `closure` argument is unsupported by the amp ' +\n                'optimizer wrapper.')\n        if any(self._skip_next):\n            maybe_print('Gradient overflow, skipping update')\n            self._skip_next = [False] * self._num_loss\n        else:\n            return self._optimizer.step(closure=closure)\n\n    # Forward any attribute lookups\n    def __getattr__(self, attr):\n        return getattr(self._optimizer, attr)\n\n    # Forward all torch.optim.Optimizer methods\n    def __getstate__(self):\n        return self._optimizer.__getstate__()\n\n    def __setstate__(self):\n        return self._optimizer.__setstate__()\n\n    def __repr__(self):\n        return self._optimizer.__repr__()\n\n    def state_dict(self):\n        return self._optimizer.state_dict()\n\n    def load_state_dict(self, state_dict):\n        return self._optimizer.load_state_dict(state_dict)\n\n    def zero_grad(self):\n        return self._optimizer.zero_grad()\n\n    def add_param_group(self, param_group):\n        return self._optimizer.add_param_group(param_group)\n"""
apex/apex/amp/rnn_compat.py,7,"b'from . import utils, wrap\n\nimport torch\n_VF = torch._C._VariableFunctions\nRNN_NAMES = [\'rnn_relu\', \'rnn_tanh\', \'gru\', \'lstm\']\n\ndef _gen_VF_wrapper(name):\n    def wrapper(*args, **kwargs):\n        return getattr(_VF, name)(*args, **kwargs)\n    return wrapper\n\n# Some python magic to generate an object that has the rnn cell functions\n# defined on it, all of which call into corresponding _VF version.\n# Intended to patch torch.nn.modules.rnn._VF (aka, the ref named ""_VF""\n# imported at module scope within torch.nn.modules.rnn).  This should\n# not affect third-party importers of _VF.py.\nclass VariableFunctionsShim(object):\n    def __init__(self):\n        for name in RNN_NAMES:\n            for suffix in [\'\', \'_cell\']:\n               fn_name = name + suffix\n               setattr(self, fn_name, _gen_VF_wrapper(fn_name))\n\ndef has_old_rnns():\n    try:\n        torch.nn.backends.thnn.backend.LSTMCell\n        return True\n    except:\n        return False\n\ndef whitelist_rnn_cells(handle, verbose):\n    # Different module + function names in old/new RNN cases\n    if has_old_rnns():\n        fn_names = [\'RNNReLUCell\', \'RNNTanhCell\', \'LSTMCell\', \'GRUCell\']\n        mod = torch.nn.backends.thnn.backend\n    else:\n        fn_names = [x + \'_cell\' for x in RNN_NAMES]\n        mod = torch.nn.modules.rnn._VF\n        assert isinstance(mod, VariableFunctionsShim)\n\n    # Insert casts on cell functions\n    for fn in fn_names:\n        wrap.cached_cast(mod, fn, utils.maybe_half, handle,\n                         try_caching=True, verbose=verbose)\n\n    if has_old_rnns():\n        # Special handling of `backward` for fused gru / lstm:\n        # The `backward` method calls Tensor.sum() (blacklist) internally,\n        # and then the resulting grad_input has the wrong type.\n        # TODO: where else is this a problem?\n        for rnn_type in [\'GRUFused\', \'LSTMFused\']:\n            mod = getattr(torch.nn._functions.thnn.rnnFusedPointwise, rnn_type)\n            wrap.disable_casts(mod, \'backward\', handle)\n'"
apex/apex/amp/scaler.py,5,"b'import torch\nfrom ..multi_tensor_apply import multi_tensor_applier\nfrom ._amp_state import _amp_state, master_params, maybe_print\nfrom itertools import product\n\ndef scale_check_overflow_python(model_grad, master_grad, scale, check_overflow=False):\n    # Exception handling for 18.04 compatibility\n    if check_overflow:\n        cpu_sum = float(model_grad.float().sum())\n        if cpu_sum == float(\'inf\') or cpu_sum == -float(\'inf\') or cpu_sum != cpu_sum:\n            return True\n\n    if master_grad is not model_grad: # copy_ probably internally short-circuits this\n        master_grad.copy_(model_grad)\n    if scale != 1.0:\n        master_grad.mul_(scale)\n    return False\n\ndef axpby_check_overflow_python(model_grad, stashed_grad, master_grad, scale, check_overflow=False):\n    # Exception handling for 18.04 compatibility\n    if check_overflow:\n        cpu_sum = float(model_grad.float().sum())\n        if cpu_sum == float(\'inf\') or cpu_sum == -float(\'inf\') or cpu_sum != cpu_sum:\n            return True\n\n    # if master_grad is not model_grad: # copy_ probably internally short-circuits this\n    #     master_grad.copy_(model_grad)\n    assert stashed_grad.dtype == master_grad.dtype\n    converted_model_grad = model_grad.to(master_grad.dtype)\n    stashed_grad.add_(scale, converted_model_grad)\n    master_grad.data = stashed_grad.data\n    return False\n\nclass LossScaler(object):\n    warned_no_fused_kernel = False\n    warned_unscaling_non_fp32_grad = False\n    has_fused_kernel = False\n\n    def __init__(self,\n                 loss_scale,\n                 init_scale=2.**16,\n                 scale_factor=2.,\n                 scale_window=2000,\n                 min_loss_scale=None,\n                 max_loss_scale=2.**24):\n        if loss_scale == ""dynamic"":\n            self.dynamic = True\n            self._loss_scale = init_scale\n        else:\n            self.dynamic = False\n            self._loss_scale = loss_scale\n        self._max_loss_scale = max_loss_scale\n        self._min_loss_scale = min_loss_scale\n        self._scale_seq_len = scale_window\n        self._unskipped = 0\n        self._has_overflow = False\n        self._overflow_buf = torch.cuda.IntTensor([0])\n        if multi_tensor_applier.available:\n            import amp_C\n            LossScaler.has_fused_kernel = multi_tensor_applier.available\n            LossScaler.multi_tensor_scale_cuda = amp_C.multi_tensor_scale\n            LossScaler.multi_tensor_axpby_cuda = amp_C.multi_tensor_axpby\n        else:\n            if not LossScaler.warned_no_fused_kernel:\n                maybe_print(\n                    ""Warning:  multi_tensor_applier fused unscale kernel is unavailable, ""\n                    ""possibly because apex was installed without --cuda_ext --cpp_ext. ""\n                    ""Using Python fallback.  Original ImportError was: "" +\n                    repr(multi_tensor_applier.import_err),\n                    True)\n            LossScaler.has_fused_kernel = False\n            LossScaler.warned_no_fused_kernel = True\n\n    def loss_scale(self):\n        return self._loss_scale\n\n    def unscale_python(self, model_grads, master_grads, scale):\n        for model, master in zip(model_grads, master_grads):\n            if model is not None:\n                if not LossScaler.warned_unscaling_non_fp32_grad:\n                    if master.dtype != torch.float32:\n                        maybe_print(\n                            ""Attempting to unscale a grad with type {} "".format(master.type()) +\n                            ""Unscaling non-fp32 grads may indicate an error. ""\n                            ""When using Amp, you don\'t need to call .half() on your model."")\n                        LossScaler.warned_unscaling_non_fp32_grad = True\n                self._has_overflow = scale_check_overflow_python(model,\n                                                                 master,\n                                                                 1./scale,\n                                                                 self.dynamic)\n                if self._has_overflow and self.dynamic:\n                    break\n\n    # unused_scale keeps some of the old API alive for hopefully a short time.\n    def unscale(self, model_grads, master_grads, unused_scale, models_are_masters=False):\n        if self._has_overflow:\n            return\n\n        scale = self._loss_scale\n\n        if scale == 1.0 and models_are_masters and not self.dynamic:\n            return\n\n        if LossScaler.has_fused_kernel:\n            # if (not LossScaler.warned_unscaling_non_fp32_grad\n            #     and master_grads[0].dtype == torch.float16):\n            #     print(""Warning:  unscaling grads that are not FP32. ""\n            #           ""Unscaling non-fp32 grads may indicate an error. ""\n            #           ""When using Amp, you don\'t need to call .half() on your model."")\n            #     # Setting this to True unconditionally allows the possibility of an escape\n            #     # if never-before-seen non-fp32 grads are created in some later iteration.\n            #     LossScaler.warned_unscaling_non_fp32_grad = True\n            multi_tensor_applier(LossScaler.multi_tensor_scale_cuda,\n                                 self._overflow_buf,\n                                 [model_grads, master_grads],\n                                 1./scale)\n        else:\n            self.unscale_python(model_grads, master_grads, scale)\n\n        # Defer to update_scale\n        # If the fused kernel is available, we only need one D2H memcopy and sync.\n        # if LossScaler.has_fused_kernel and self.dynamic and not self._has_overflow:\n        #     self._has_overflow = self._overflow_buf.item()\n\n    def unscale_with_stashed_python(self,\n                                    model_grads,\n                                    stashed_master_grads,\n                                    master_grads,\n                                    scale):\n        for model, stashed, master in zip(model_grads, stashed_master_grads, master_grads):\n            if model is None and stashed is None:\n                continue\n            else:\n                if not LossScaler.warned_unscaling_non_fp32_grad:\n                    if master.dtype != torch.float32:\n                        maybe_print(\n                            ""Attempting to unscale a grad with type {} "".format(master.type()) +\n                            ""Unscaling non-fp32 grads may indicate an error. ""\n                            ""When using Amp, you don\'t need to call .half() on your model."")\n                        LossScaler.warned_unscaling_non_fp32_grad = True\n                self._has_overflow = axpby_check_overflow_python(model,\n                                                                 stashed,\n                                                                 master,\n                                                                 1./scale,\n                                                                 self.dynamic)\n                if self._has_overflow and self.dynamic:\n                    break\n\n    def unscale_with_stashed(self,\n                             model_grads,\n                             stashed_master_grads,\n                             master_grads):\n        if self._has_overflow:\n            return\n\n        scale = self._loss_scale\n\n        if LossScaler.has_fused_kernel:\n            if (not LossScaler.warned_unscaling_non_fp32_grad\n                and master_grads[0].dtype == torch.float16):\n                print(""Warning:  unscaling grads that are not FP32. ""\n                      ""Unscaling non-fp32 grads may indicate an error. ""\n                      ""When using Amp, you don\'t need to call .half() on your model."")\n                # Setting this to True unconditionally allows the possibility of an escape\n                # if never-before-seen non-fp32 grads are created in some later iteration.\n                LossScaler.warned_unscaling_non_fp32_grad = True\n            multi_tensor_applier(LossScaler.multi_tensor_axpby_cuda,\n                                 self._overflow_buf,\n                                 [model_grads, stashed_master_grads, master_grads],\n                                 1./scale,\n                                 1.0,\n                                 0) # check only arg 0, aka the incoming model grads, for infs\n        else:\n            self.unscale_with_stashed_python(model_grads,\n                                             stashed_master_grads,\n                                             master_grads,\n                                             scale)\n\n        # Defer to update_scale\n        # If the fused kernel is available, we only need one D2H memcopy and sync.\n        # if LossScaler.has_fused_kernel and self.dynamic and not self._has_overflow:\n        #     self._has_overflow = self._overflow_buf.item()\n\n    def clear_overflow_state(self):\n        self._has_overflow = False\n        if self.has_fused_kernel:\n            self._overflow_buf.zero_()\n\n    # Separate so unscale() can be called more that once before updating.\n    def update_scale(self):\n        # If the fused kernel is available, we only need one D2H memcopy and sync.\n        if LossScaler.has_fused_kernel and self.dynamic and not self._has_overflow:\n            self._has_overflow = self._overflow_buf.item()\n\n        if self._has_overflow and self.dynamic:\n            should_skip = True\n            if(self._min_loss_scale):\n                self._loss_scale = max(self._min_loss_scale, self._loss_scale/2.)\n            else:\n                self._loss_scale = self._loss_scale/2.\n            self._unskipped = 0\n        else:\n            should_skip = False\n            self._unskipped += 1\n\n        if self._unskipped == self._scale_seq_len and self.dynamic:\n            self._loss_scale = min(self._max_loss_scale, self._loss_scale*2.)\n            self._unskipped = 0\n\n        return should_skip\n'"
apex/apex/amp/utils.py,9,"b'from . import compat\n\nimport functools\nimport itertools\n\nimport torch\n\ndef get_cuda_version():\n    return tuple(int(x) for x in torch.version.cuda.split(\'.\'))\n\ndef is_fp_tensor(x):\n    if is_nested(x):\n        # Fast-fail version of all(is_fp_tensor)\n        for y in x:\n            if not is_fp_tensor(y):\n                return False\n        return True\n    return compat.is_tensor_like(x) and compat.is_floating_point(x)\n\ndef is_nested(x):\n    return isinstance(x, tuple) or isinstance(x, list)\n\ndef should_cache(x):\n    if is_nested(x):\n        # Fast-fail version of all(should_cache)\n        for y in x:\n            if not should_cache(y):\n                return False\n        return True\n    return isinstance(x, torch.nn.parameter.Parameter) and \\\n        type_string(x) == \'FloatTensor\'\n\ndef collect_fp_tensor_types(args, kwargs):\n    def collect_types(x, types):\n        if is_nested(x):\n            for y in x:\n                collect_types(y, types)\n        else:\n            types.add(type_string(x))\n\n    all_args = itertools.chain(args, kwargs.values())\n    types = set()\n    for x in all_args:\n        if is_fp_tensor(x):\n            collect_types(x, types)\n    return types\n\ndef type_string(x):\n    return x.type().split(\'.\')[-1]\n\ndef maybe_half(x, name=\'\', verbose=False):\n    if is_nested(x):\n        return type(x)([maybe_half(y) for y in x])\n\n    if not x.is_cuda or type_string(x) == \'HalfTensor\':\n        return x\n    else:\n        if verbose:\n            print(\'Float->Half ({})\'.format(name))\n        return x.half()\n\ndef maybe_float(x, name=\'\', verbose=False):\n    if is_nested(x):\n        return type(x)([maybe_float(y) for y in x])\n\n    if not x.is_cuda or type_string(x) == \'FloatTensor\':\n        return x\n    else:\n        if verbose:\n            print(\'Half->Float ({})\'.format(name))\n        return x.float()\n\n# NB: returneds casted `args`, mutates `kwargs` in-place\ndef casted_args(cast_fn, args, kwargs):\n    new_args = []\n    for x in args:\n        if is_fp_tensor(x):\n            new_args.append(cast_fn(x))\n        else:\n            new_args.append(x)\n    for k in kwargs:\n        val = kwargs[k]\n        if is_fp_tensor(val):\n            kwargs[k] = cast_fn(val)\n    return new_args\n\ndef cached_cast(cast_fn, x, cache):\n    if is_nested(x):\n        return type(x)([cached_cast(y) for y in x])\n    if x in cache:\n        cached_x = cache[x]\n        if x.requires_grad and cached_x.requires_grad:\n            # Make sure x is actually cached_x\'s autograd parent.\n            if cached_x.grad_fn.next_functions[1][0].variable is not x:\n                raise RuntimeError(""x and cache[x] both require grad, but x is not ""\n                                   ""cache[x]\'s parent.  This is likely an error."")\n        # During eval, it\'s possible to end up caching casted weights with\n        # requires_grad=False.  On the next training iter, if cached_x is found\n        # and reused from the cache, it will not actually have x as its parent.\n        # Therefore, we choose to invalidate the cache (and force refreshing the cast)\n        # if x.requires_grad and cached_x.requires_grad do not match.\n        #\n        # During eval (i.e. running under with torch.no_grad()) the invalidation\n        # check would cause the cached value to be dropped every time, because\n        # cached_x would always be created with requires_grad=False, while x would\n        # still have requires_grad=True.  This would render the cache effectively\n        # useless during eval.  Therefore, if we are running under the no_grad()\n        # context manager (torch.is_grad_enabled=False) we elide the invalidation\n        # check, and use the cached value even though its requires_grad flag doesn\'t\n        # match.  During eval, we don\'t care that there\'s no autograd-graph\n        # connection between x and cached_x.\n        if torch.is_grad_enabled() and x.requires_grad != cached_x.requires_grad:\n            del cache[x]\n        else:\n            return cached_x\n\n    casted_x = cast_fn(x)\n    cache[x] = casted_x\n    return casted_x\n\ndef verbosify(cast_fn, fn_name, verbose):\n    if verbose:\n        return functools.partial(cast_fn, name=fn_name, verbose=verbose)\n    else:\n        return cast_fn\n\ndef as_inplace(fns):\n    for x in fns:\n        yield x + \'_\'\n\ndef has_func(mod, fn):\n    if isinstance(mod, torch.nn.backends.backend.FunctionBackend):\n        return fn in mod.function_classes\n    elif isinstance(mod, dict):\n        return fn in mod\n    else:\n        return hasattr(mod, fn)\n\ndef get_func(mod, fn):\n    if isinstance(mod, torch.nn.backends.backend.FunctionBackend):\n        return mod.function_classes[fn]\n    elif isinstance(mod, dict):\n        return mod[fn]\n    else:\n        return getattr(mod, fn)\n\ndef set_func(mod, fn, new_fn):\n    if isinstance(mod, torch.nn.backends.backend.FunctionBackend):\n        mod.function_classes[fn] = new_fn\n    elif isinstance(mod, dict):\n        mod[fn] = new_fn\n    else:\n        setattr(mod, fn, new_fn)\n\ndef set_func_save(handle, mod, fn, new_fn):\n    cur_fn = get_func(mod, fn)\n    handle._save_func(mod, fn, cur_fn)\n    set_func(mod, fn, new_fn)\n\n# A couple problems get solved here:\n# - The flat_weight buffer is disconnected from autograd graph,\n#   so the fp16 weights need to be derived from the input weights\n#   to this forward call, not the flat buffer.\n# - The ordering of weights in the flat buffer is...idiosyncratic.\n# First problem is solved with combination of set_ (to set up\n# correct storage) and copy_ (so the fp16 weight derives from the\n# fp32 one in autograd.\n# Second is solved by doing ptr arithmetic on the fp32 weights\n# to derive the correct offset.\n#\n# TODO: maybe this should actually use\n# `torch._cudnn_rnn_flatten_weight`? But then I need to call\n# on first iter and cache the right offsets. Ugh.\ndef synthesize_flattened_rnn_weights(fp32_weights,\n                                     fp16_flat_tensor,\n                                     rnn_fn=\'\',\n                                     verbose=False):\n    fp16_weights = []\n    fp32_base_ptr = fp32_weights[0][0].data_ptr()\n    for layer_weights in fp32_weights:\n        fp16_layer_weights = []\n        for w_fp32 in layer_weights:\n            w_fp16 = w_fp32.new().half()\n            offset = (w_fp32.data_ptr() - fp32_base_ptr) // w_fp32.element_size()\n            w_fp16.set_(fp16_flat_tensor.storage(),\n                        offset,\n                        w_fp32.shape)\n            w_fp16.copy_(w_fp32)\n            if verbose:\n                print(\'Float->Half ({})\'.format(rnn_fn))\n            fp16_layer_weights.append(w_fp16)\n        fp16_weights.append(fp16_layer_weights)\n    return fp16_weights\n\n# Roughly same as above, just the `fp32_weights` aren\'t nested.\n# Code kept separate for readability.\ndef new_synthesize_flattened_rnn_weights(fp32_weights,\n                                         fp16_flat_tensor,\n                                         rnn_fn=\'\',\n                                         verbose=False):\n    fp16_weights = []\n    fp32_base_ptr = fp32_weights[0].data_ptr()\n    for w_fp32 in fp32_weights:\n        w_fp16 = w_fp32.new().half()\n        offset = (w_fp32.data_ptr() - fp32_base_ptr) // w_fp32.element_size()\n        w_fp16.set_(fp16_flat_tensor.storage(),\n                    offset,\n                    w_fp32.shape)\n        w_fp16.copy_(w_fp32)\n        if verbose:\n            print(\'Float->Half ({})\'.format(rnn_fn))\n        fp16_weights.append(w_fp16)\n    return fp16_weights\n'"
apex/apex/amp/wrap.py,7,"b'from . import compat\nfrom . import utils\nfrom ._amp_state import _amp_state\nfrom . import rnn_compat\n\nimport functools\n\nimport torch\n\ndef make_cast_wrapper(orig_fn, cast_fn, handle,\n                      try_caching=False):\n    @functools.wraps(orig_fn)\n    def wrapper(*args, **kwargs):\n        if not handle.is_active():\n            return orig_fn(*args, **kwargs)\n\n        if try_caching and handle.has_cache:\n            args = list(args)\n            for i in range(len(args)):\n                if utils.should_cache(args[i]):\n                    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)\n            for k in kwargs:\n                if utils.should_cache(kwargs[k]):\n                    kwargs[k] = utils.cached_cast(cast_fn, kwargs[k], handle.cache)\n        new_args = utils.casted_args(cast_fn,\n                                     args,\n                                     kwargs)\n        return orig_fn(*new_args, **kwargs)\n    return wrapper\n\ndef cached_cast(mod, fn, cast_fn, handle,\n                try_caching=False, verbose=False):\n    if not utils.has_func(mod, fn):\n        return\n\n    orig_fn = utils.get_func(mod, fn)\n    cast_fn = utils.verbosify(cast_fn, fn, verbose)\n    wrapper = make_cast_wrapper(orig_fn, cast_fn, handle, try_caching)\n    utils.set_func_save(handle, mod, fn, wrapper)\n\n# `handle` arg is unused, but simplifies API to make `make_cast_wrapper`\n# Annoyingly, make_promote_wrapper still uses the global handle.  Once everyone\n# is on the new API and I am free to get rid of handle, I can clean this up.\ndef make_promote_wrapper(orig_fn, cast_fn, handle=None):\n    @functools.wraps(orig_fn)\n    def wrapper(*args, **kwargs):\n        if not _amp_state.handle.is_active():\n            return orig_fn(*args, **kwargs)\n\n        types = utils.collect_fp_tensor_types(args, kwargs)\n\n        if len(types) <= 1:\n            return orig_fn(*args, **kwargs)\n        elif len(types) == 2 and types == set([\'HalfTensor\', \'FloatTensor\']):\n            new_args = utils.casted_args(cast_fn,\n                                         args,\n                                         kwargs)\n            return orig_fn(*new_args, **kwargs)\n        else:\n            raise NotImplementedError(\'Do not know how to handle \' +\n                                      \'these types to promote: {}\'\n                                      .format(types))\n    return wrapper\n\ndef promote(mod, fn, handle, verbose=False):\n    orig_fn = utils.get_func(mod, fn)\n    maybe_float = utils.verbosify(utils.maybe_float, fn, verbose)\n    wrapper = make_promote_wrapper(orig_fn, maybe_float)\n    utils.set_func_save(handle, mod, fn, wrapper)\n\ndef sequence_promote(mod, fn, handle, verbose=False):\n    orig_fn = utils.get_func(mod, fn)\n    maybe_float = utils.verbosify(utils.maybe_float, fn, verbose)\n    @functools.wraps(orig_fn)\n    def wrapper(seq, *args, **kwargs):\n        if not _amp_state.handle.is_active():\n            return orig_fn(seq, *args, **kwargs)\n\n        types = set([utils.type_string(x) for x in seq])\n        if len(types) <= 1:\n            return orig_fn(seq, *args, **kwargs)\n        elif types == set([\'HalfTensor\', \'FloatTensor\']):\n            cast_seq = utils.casted_args(maybe_float,\n                                         seq, {})\n            return orig_fn(cast_seq, *args, **kwargs)\n        else:\n            # TODO: other mixed-type cases aren\'t due to amp.\n            #       Just pass through?\n            return orig_fn(seq, *args, **kwargs)\n    utils.set_func_save(handle, mod, fn, wrapper)\n\ndef promote_match_arg0(mod, fn, handle, verbose=False):\n    if not utils.has_func(mod, fn):\n        return\n\n    orig_fn = utils.get_func(mod, fn)\n    @functools.wraps(orig_fn)\n    def wrapper(arg0, *args, **kwargs):\n        assert compat.is_tensor_like(arg0)\n        if not _amp_state.handle.is_active():\n            return orig_fn(arg0, *args, **kwargs)\n\n        if utils.type_string(arg0) == \'HalfTensor\':\n            cast_fn = utils.maybe_half\n        elif utils.type_string(arg0) == \'FloatTensor\':\n            cast_fn = utils.maybe_float\n        else:\n            return orig_fn(arg0, *args, **kwargs)\n        cast_fn = utils.verbosify(cast_fn, fn, verbose)\n        new_args = utils.casted_args(cast_fn, args, kwargs)\n        return orig_fn(arg0, *new_args, **kwargs)\n    utils.set_func_save(handle, mod, fn, wrapper)\n\ndef err_if_any_half(mod, fn, handle, custom_err_msg=None):\n    if not utils.has_func(mod, fn):\n        return\n\n    orig_fn = utils.get_func(mod, fn)\n    @functools.wraps(orig_fn)\n    def wrapper(*args, **kwargs):\n        types = utils.collect_fp_tensor_types(args, kwargs)\n        if \'HalfTensor\' in types:\n            if custom_err_msg:\n                raise NotImplementedError(custom_err_msg)\n            else:\n                raise NotImplementedError(\'Cannot call in-place function \' +\n                                          \'{} with fp16 arguments.\'.format(fn))\n        else:\n            return orig_fn(*args, **kwargs)\n    utils.set_func_save(handle, mod, fn, wrapper)\n\ndef err_if_arg0_half(mod, fn, handle, verbose=False):\n    if not utils.has_func(mod, fn):\n        return\n\n    orig_fn = utils.get_func(mod, fn)\n    @functools.wraps(orig_fn)\n    def wrapper(arg0, *args, **kwargs):\n        assert compat.is_tensor_like(arg0)\n        if utils.type_string(arg0) == \'HalfTensor\':\n            raise NotImplementedError(\'Cannot call in-place method \' +\n                                      \'{} on fp16 Tensors.\'.format(fn))\n        else:\n            cast_fn = utils.verbosify(utils.maybe_float, fn, verbose)\n            new_args = utils.casted_args(cast_fn, args, kwargs)\n            return orig_fn(arg0, *new_args, **kwargs)\n    utils.set_func_save(handle, mod, fn, wrapper)\n\n# Current RNN approach:\n# - Wrap top-level `RNN` function in thnn backend\n# - Will call into either CudnnRNN or AutogradRNN\n#  - Each of these are factory functions that return a per-iter\n#    `forward` function\n# - We interpose on the factory function to:\n#   1) Interpose on the actual forward function and put in casts\n#   2) Insert an fp16 `flat_weight` if necessary\ndef rnn_cast(backend, fn, handle, verbose=False):\n    orig_rnn = utils.get_func(backend, fn)\n    @functools.wraps(orig_rnn)\n    def rnn_wrapper(*args, **kwargs):\n        flat_weight = kwargs.get(\'flat_weight\')\n        if flat_weight is not None:\n            # We replace `flat_weight` with an uninitialized fp16\n            # Tensor. The ""actual"" weight tensors (provided in `forward`),\n            # will then be set up as ptrs into the buffer and have the\n            # corresponding fp32 values copied in.\n            # We need to call `copy` on the ""actual"" weights so that the\n            # autograd graph correctly backprops from the wgrads computed\n            # inside cuDNN (on fp16 weights) into the fp32 weights.\n            assert utils.type_string(flat_weight) == \'FloatTensor\'\n            if compat.tensor_is_float_tensor() or compat.tensor_is_variable():\n                # Pre-0.4. A little slower, since it zeros out memory.\n                flat_weight_fp16 = flat_weight.new().half().resize_(flat_weight.shape)\n            else:\n                flat_weight_fp16 = torch.empty_like(flat_weight,\n                                                    dtype=torch.float16)\n            kwargs[\'flat_weight\'] = flat_weight_fp16\n        else:\n            flat_weight_fp16 = None\n\n        forward = orig_rnn(*args, **kwargs)\n        @functools.wraps(forward)\n        def fwd_wrapper(*fargs, **fkwargs):\n            assert len(fargs) == 3 or len(fargs) == 4\n            inputs, weights, hiddens = fargs[:3]\n            assert utils.is_fp_tensor(inputs)\n            assert isinstance(weights, list)\n            cast_fn = utils.verbosify(utils.maybe_half,\n                                      fn,\n                                      verbose)\n            new_args = []\n\n            # 0) Inputs\n            new_args.append(cast_fn(inputs))\n\n            # 1) Weights\n            if flat_weight_fp16 is not None:\n                fp16_weights = utils.synthesize_flattened_rnn_weights(\n                    weights, flat_weight_fp16, fn, verbose)\n            else:\n                fp16_weights = [[cast_fn(w) for w in layer]\n                                for layer in weights]\n            new_args.append(fp16_weights)\n\n            # 2) Inputs: either a tuple (for LSTM) or single tensor\n            if isinstance(hiddens, tuple):\n                new_args.append(tuple(cast_fn(x) for x in hiddens))\n            elif utils.is_fp_tensor(hiddens):\n                new_args.append(cast_fn(hiddens))\n            else:\n                # Hiddens can, in principle, be `None` -- pass through\n                new_args.append(hiddens)\n\n            # 3) Batch sizes (0.4 or later only)\n            if len(fargs) == 4:\n                new_args.append(fargs[3])\n\n            return forward(*new_args, **fkwargs)\n        return fwd_wrapper\n    utils.set_func_save(handle, backend, fn, rnn_wrapper)\n\ndef new_rnn_cast(fn, handle, verbose=False):\n    # Forward+backward compatibility around https://github.com/pytorch/pytorch/pull/15744\n    # For rnn backend calls that route through _rnn_impls, we must patch the ref\n    # that _rnn_impls stashed.  For rnn backend calls that directly invoke\n    # _VF.<backend>, e.g. _VF.lstm, we can patch onto VariableFunctionsShim,\n    # which in turn has patched the ref named ""_VF"" in torch.nn.modules.rnn.\n    if utils.has_func(torch.nn.modules.rnn._rnn_impls, fn):\n        mod = torch.nn.modules.rnn._rnn_impls\n    else:\n        mod = torch.nn.modules.rnn._VF\n        assert isinstance(mod, rnn_compat.VariableFunctionsShim)\n        fn = fn.lower()\n    orig_fn = utils.get_func(mod, fn)\n    cast_fn = utils.verbosify(utils.maybe_half, fn, verbose)\n    @functools.wraps(orig_fn)\n    def wrapper(*args, **kwargs):\n        # Exact call signature from modules/rnn.py\n        assert len(args) == 9\n        assert len(kwargs) == 0\n\n        if not _amp_state.handle.is_active():\n            return orig_fn(*args, **kwargs)\n\n        if isinstance(args[6], bool):\n            params_idx = 2 # Not PackedSequence case\n        else:\n            params_idx = 3 # PackedSequence case\n\n        new_args = []\n        for i, arg in enumerate(args):\n            if i == params_idx:\n                num_params = sum([x.numel() for x in arg])\n                fp16_weight_buf = args[0].new_empty((num_params,),\n                                                    dtype=torch.half)\n                casted_weights = utils.new_synthesize_flattened_rnn_weights(\n                    arg, fp16_weight_buf, fn, verbose)\n                new_args.append(casted_weights)\n            elif utils.is_fp_tensor(arg):\n                new_args.append(cast_fn(arg))\n            else:\n                new_args.append(arg)\n\n        return orig_fn(*new_args)\n    utils.set_func_save(handle, mod, fn, wrapper)\n\ndef disable_casts(mod, fn, handle):\n    if not utils.has_func(mod, fn):\n        return\n\n    orig_fn = utils.get_func(mod, fn)\n    @functools.wraps(orig_fn)\n    def wrapper(*args, **kwargs):\n        with handle._disable_casts():\n            return orig_fn(*args, **kwargs)\n    utils.set_func_save(handle, mod, fn, wrapper)\n'"
apex/apex/fp16_utils/__init__.py,0,"b'from .fp16util import (\n    BN_convert_float,\n    network_to_half,\n    prep_param_lists,\n    model_grads_to_master_grads,\n    master_params_to_model_params,\n    tofp16,\n    to_python_float,\n    clip_grad_norm,\n    convert_module,\n    convert_network,\n    FP16Model,\n)\n\nfrom .fp16_optimizer import FP16_Optimizer\nfrom .loss_scaler import LossScaler, DynamicLossScaler\n'"
apex/apex/fp16_utils/fp16_optimizer.py,25,"b'import torch\r\nfrom torch import nn\r\nfrom torch.autograd import Variable\r\nfrom torch.nn.parameter import Parameter\r\nfrom torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors\r\n\r\nfrom ..amp._amp_state import _amp_state, maybe_print\r\nfrom ..amp.scaler import LossScaler\r\nfrom ..multi_tensor_apply import multi_tensor_applier\r\nfrom .fp16util import model_grads_to_master_grads, master_params_to_model_params, clip_grad_norm\r\n\r\n# TODO:  Update overflow check + downscale to use Carl\'s fused kernel.\r\nclass FP16_Optimizer(object):\r\n    """"""\r\n    :class:`FP16_Optimizer` is designed to wrap an existing PyTorch optimizer, \r\n    and manage static or dynamic loss scaling and master weights in a manner transparent to the user.\r\n    For standard use, only two lines must be changed:  creating the :class:`FP16_Optimizer` instance,\r\n    and changing the call to ``backward``.\r\n\r\n    Example::\r\n\r\n        model = torch.nn.Linear(D_in, D_out).cuda().half()\r\n        optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\r\n        # Name the FP16_Optimizer instance to replace the existing optimizer\r\n        # (recommended but not required):\r\n        optimizer = FP16_Optimizer(optimizer, static_loss_scale = 128.0)\r\n        ...\r\n        # loss.backward() becomes:\r\n        optimizer.backward(loss)\r\n        ...\r\n\r\n    Example with dynamic loss scaling::\r\n\r\n        ...\r\n        optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\r\n                                   # optional arg to control dynamic loss scaling behavior\r\n                                   # dynamic_loss_args={\'scale_window\' : 500})\r\n                                   # Usually, dynamic_loss_args is not necessary. \r\n\r\n    Args:\r\n        init_optimizer (torch.optim.optimizer):  Existing optimizer created with the parameters to optimize.  Internally, :class:`FP16_Optimizer` replaces the passed optimizer\'s fp16 parameters, if any, with fp32 master parameters copied from the original ones.  :class:`FP16_Optimizer` also stores references to the original fp16 parameters, and updates these fp16 parameters from the master fp32 copy at the end of each :attr:`step`.  \r\n        static_loss_scale (float, optional, default=1.0):  Loss scale used internally to scale gradients computed by the model.  Any fp16 gradients will be copied to fp32, then downscaled before being applied to the fp32 master params, so ``static_loss_scale`` should not affect learning rate.\r\n        dynamic_loss_scale (bool, optional, default=False):  Use dynamic loss scaling.  If True, this will override any ``static_loss_scale`` option.\r\n        dynamic_loss_args (dict, optional, default=None):  Dict of kwargs that will be forwarded to the internal :class:`LossScaler` instance\'s constructor.  Keys of this dict must match kwargs accepted by :class:`LossScaler`\'s constructor.  If ``dynamic_loss_args`` is unspecified, :class:`LossScaler`\'s defaults will be used.\r\n        verbose (bool, optional, default=True):  By default, FP16_Optimizer\'s constructor prints out the parameters and parameter groups it is ingesting, as a sanity check.  If this becomes annoying (e.g. for large models), it can be disabled by passing ``verbose=False``.  ``verbose=False`` will not disable printing when the loss scale is readjusted during dynamic loss scaling.\r\n\r\n    ``init_optimizer`` is expected to have been constructed in the ordinary way.  \r\n    It is recommended (although not required) that the newly constructed :class:`FP16_Optimizer` instance be \r\n    named to replace ``init_optimizer``, for two reasons:  \r\n    First, it means that references to the same name\r\n    later in the file will not have to change.  \r\n    Second, :class:`FP16_Optimizer` reserves the right (as an implementation detail) to \r\n    modify ``init_optimizer``.  If you do choose a unique name for the new\r\n    :class:`FP16_Optimizer` instance, you should only work with this new instance,\r\n    because the preexisting optimizer might no longer behave as expected.\r\n\r\n    ``init_optimizer`` may be any Pytorch optimizer. \r\n    It may contain a mixture of fp16 and fp32 parameters organized into any number of \r\n    ``param_groups`` with different hyperparameters.  The :class:`FP16_Optimizer` constructor will \r\n    ingest these ``param_groups`` and remember them. \r\n\r\n    Calls to ::\r\n\r\n        loss.backward() \r\n\r\n    must be replaced with ::\r\n\r\n        optimizer.backward(loss)  \r\n\r\n    because :class:`FP16_Optimizer` requires ownership of the backward pass to implement \r\n    loss scaling and copies to master gradients.\r\n\r\n    .. note::\r\n        Loss scaling, either static or dynamic, is orthogonal to learning rate, because gradients\r\n        are downscaled before being applied.  This means that adjusting the loss scale, or using\r\n        dynamic loss scaling, should not require retuning the learning rate or any other \r\n        hyperparameters.\r\n\r\n\r\n    **Advanced options**\r\n\r\n    **Closures**:  :class:`FP16_Optimizer` can wrap a Pytorch optimizer that receives a closure.\r\n    See docstring for :attr:`step`.\r\n\r\n    **Gradient clipping**:  Use :attr:`clip_master_grads`.\r\n    \r\n    **Multiple losses**:  If your model accumulates gradients from multiple losses,\r\n    this can be made more efficient by supplying ``update_master_grads=False``\r\n    to :attr:`backward`.  See docstring for :attr:`backward`.\r\n\r\n    **Manually adjusting loss scale**:  The current loss scale can be retrieved or set via ::\r\n\r\n        print(optimizer.loss_scale)\r\n        optimizer.loss_scale = new_loss_scale\r\n\r\n    For static loss scaling, manually adjusting the loss scale over time is a reasonable\r\n    thing to do.  During later epochs, gradients may become smaller, and a \r\n    higher loss scale may be required, analogous to scheduling the learning rate.  Dynamic loss\r\n    scaling is more subtle (see :class:`DynamicLossScaler`) and in this case, manually adjusting \r\n    the loss scale is not recommended.\r\n\r\n    **Multi_GPU training**:  If the wrapped ``init_optimizer`` was created from a model wrapped in\r\n    Pytorch DistributedDataParallel or Apex DistributedDataParallel, :class:`FP16_Optimizer` \r\n    should still work as intended.\r\n    """"""\r\n\r\n    def __init__(self, \r\n                 init_optimizer, \r\n                 static_loss_scale=1.0, \r\n                 dynamic_loss_scale=False,\r\n                 dynamic_loss_args=None,\r\n                 verbose=True):\r\n        if not torch.cuda.is_available:\r\n            raise SystemError(""Cannot use fp16 without CUDA."")\r\n\r\n        self.verbose = verbose\r\n\r\n        self.optimizer = init_optimizer\r\n        # init_state_dict sets up an alternative way to cast per-param state tensors.\r\n        # Stashing here in case https://github.com/pytorch/pytorch/issues/7733 makes it necessary.\r\n        # init_state_dict = init_optimizer.state_dict()\r\n\r\n        self.fp16_groups = []\r\n        self.fp32_from_fp16_groups = []\r\n        self.fp32_from_fp32_groups = []\r\n        for i, param_group in enumerate(self.optimizer.param_groups):\r\n            self.maybe_print(""FP16_Optimizer processing param group {}:"".format(i))\r\n            fp16_params_this_group = []\r\n            fp32_params_this_group = []\r\n            fp32_from_fp16_params_this_group = []\r\n            for i, param in enumerate(param_group[\'params\']):\r\n                if param.requires_grad:\r\n                    if param.type() == \'torch.cuda.HalfTensor\':\r\n                        self.maybe_print(""FP16_Optimizer received torch.cuda.HalfTensor with {}""\r\n                                         .format(param.size()))\r\n                        fp16_params_this_group.append(param)\r\n                        master_param = param.detach().clone().float()\r\n                        master_param.requires_grad = True\r\n                        param_group[\'params\'][i] = master_param\r\n                        fp32_from_fp16_params_this_group.append(master_param)\r\n                        # Reset existing state dict key to the new master param.\r\n                        # We still need to recast per-param state tensors, if any, to FP32.\r\n                        if param in self.optimizer.state:\r\n                           self.optimizer.state[master_param] = self.optimizer.state.pop(param) \r\n                    elif param.type() == \'torch.cuda.FloatTensor\':\r\n                        self.maybe_print(""FP16_Optimizer received torch.cuda.FloatTensor with {}""\r\n                                         .format(param.size()))\r\n                        fp32_params_this_group.append(param)\r\n                        param_group[\'params\'][i] = param\r\n                    else:\r\n                        raise TypeError(""Wrapped parameters must be either ""\r\n                                        ""torch.cuda.FloatTensor or torch.cuda.HalfTensor. ""  \r\n                                        ""Received {}"".format(param.type()))\r\n            \r\n            self.fp16_groups.append(fp16_params_this_group)\r\n            self.fp32_from_fp16_groups.append(fp32_from_fp16_params_this_group)\r\n            self.fp32_from_fp32_groups.append(fp32_params_this_group)\r\n\r\n        self.all_fp16_params = []\r\n        for group in self.fp16_groups:\r\n            self.all_fp16_params += group\r\n\r\n        self.all_fp32_from_fp16_params = []\r\n        for group in self.fp32_from_fp16_groups:\r\n            self.all_fp32_from_fp16_params += group\r\n\r\n        self.all_fp32_from_fp32_params = []\r\n        for group in self.fp32_from_fp32_groups:\r\n            self.all_fp32_from_fp32_params += group\r\n\r\n        # Leverage state_dict() and load_state_dict() to recast preexisting per-param state tensors\r\n        self.optimizer.load_state_dict(self.optimizer.state_dict())\r\n        # alternative way to cast per-param state tensors:\r\n        # self.optimizer.load_state_dict(init_state_dict)\r\n\r\n        if dynamic_loss_scale:\r\n            self.dynamic_loss_scale = True\r\n            if dynamic_loss_args is not None:\r\n                self.loss_scaler = LossScaler(""dynamic"", **dynamic_loss_args)\r\n            else:\r\n                self.loss_scaler = LossScaler(""dynamic"")\r\n        else:\r\n            self.dynamic_loss_scale = False\r\n            self.loss_scaler = LossScaler(static_loss_scale)\r\n\r\n        self.overflow = False\r\n        self.first_closure_call_this_step = True\r\n\r\n        self.clip_grad_norm = clip_grad_norm\r\n\r\n        # TODO:  Centralize exposure and import error checking for the C backend.\r\n        if multi_tensor_applier.available:\r\n            import amp_C\r\n            self.multi_tensor_scale = amp_C.multi_tensor_scale\r\n            self._dummy_overflow_buf = torch.cuda.IntTensor([0]);\r\n\r\n    # Having self.maybe_print distinct from _amp_state.maybe_print is another artifact\r\n    # of having to support FP16_Optimizer separately, for the time being.\r\n    def maybe_print(self, msg):\r\n        if self.verbose:\r\n            print(msg)\r\n            \r\n    def __getstate__(self):\r\n        raise RuntimeError(""FP16_Optimizer should be serialized using state_dict()."")\r\n\r\n    def __setstate__(self, state):\r\n        raise RuntimeError(""FP16_Optimizer should be deserialized using load_state_dict()."")\r\n\r\n    def zero_grad(self, set_grads_to_None=False):\r\n        """"""\r\n        Zero fp32 and fp16 parameter grads.\r\n        """"""\r\n        # In principle, only the .grad attributes of the model params need to be zeroed,\r\n        # because gradients are copied into the FP32 master params.  However, we zero\r\n        # all gradients owned by the optimizer, just to be safe:\r\n        for group in self.optimizer.param_groups:\r\n             for p in group[\'params\']:\r\n                 if set_grads_to_None:\r\n                     p.grad = None\r\n                 else:\r\n                     if p.grad is not None:\r\n                         p.grad.detach_()\r\n                         p.grad.zero_()\r\n\r\n        # Zero fp16 gradients owned by the model:\r\n        for fp16_group in self.fp16_groups:\r\n            for param in fp16_group:\r\n                if set_grads_to_None:\r\n                    param.grad = None\r\n                else:\r\n                    if param.grad is not None:\r\n                        param.grad.detach_() # as in torch.optim.optimizer.zero_grad()\r\n                        param.grad.zero_()\r\n\r\n    # Should not be used anymore.\r\n    # def _check_overflow(self):\r\n    #     params = []\r\n    #     for group in self.fp16_groups:\r\n    #         for param in group:\r\n    #             params.append(param)\r\n    #     for group in self.fp32_from_fp32_groups:\r\n    #         for param in group:\r\n    #             params.append(param)\r\n    #     self.overflow = self.loss_scaler.has_overflow(params)\r\n\r\n    # def _update_scale(self, has_overflow=False):\r\n    #     self.loss_scaler.update_scale(has_overflow)\r\n\r\n    def _master_params_to_model_params(self):\r\n        if multi_tensor_applier.available:\r\n            if len(self.all_fp16_params) > 0:\r\n                multi_tensor_applier(\r\n                    self.multi_tensor_scale,\r\n                    self._dummy_overflow_buf,\r\n                    [self.all_fp32_from_fp16_params, self.all_fp16_params],\r\n                    1.0)\r\n        else:\r\n            for fp16_group, fp32_from_fp16_group in zip(self.fp16_groups, self.fp32_from_fp16_groups):\r\n                master_params_to_model_params(fp16_group, fp32_from_fp16_group)\r\n\r\n    # To consider:  Integrate distributed with this wrapper by registering a hook on each variable\r\n    # that does the overflow check, gradient copy + downscale, and fp32 allreduce in a different stream.\r\n    # def _model_grads_to_master_grads(self):\r\n    #     for fp16_group, fp32_from_fp16_group in zip(self.fp16_groups, self.fp32_from_fp16_groups):\r\n    #         model_grads_to_master_grads(fp16_group, fp32_from_fp16_group)\r\n\r\n    # def _downscale_master(self):\r\n    #     if self.loss_scale != 1.0:\r\n    #         for group in self.optimizer.param_groups:\r\n    #             for param in group[\'params\']:\r\n    #                 if param.grad is not None:\r\n    #                     param.grad.data.mul_(1./self.loss_scale)\r\n\r\n    def clip_master_grads(self, max_norm, norm_type=2):\r\n        """"""\r\n        Clips fp32 master gradients via ``torch.nn.utils.clip_grad_norm``.\r\n\r\n        Args:\r\n            max_norm (float or int): max norm of the gradients\r\n            norm_type (float or int): type of the used p-norm. Can be ``\'inf\'`` for\r\n                infinity norm.\r\n\r\n        Returns:\r\n            Total norm of the current fp32 gradients (viewed as a single vector).\r\n\r\n        .. warning::\r\n            Returns -1 if the most recently computed fp16 gradients overflowed (that is, if ``self.overflow`` is ``True``).\r\n        """"""\r\n        if not self.overflow:\r\n            fp32_params = []\r\n            for param_group in self.optimizer.param_groups:\r\n                for param in param_group[\'params\']:\r\n                    fp32_params.append(param)\r\n            return self.clip_grad_norm(fp32_params, max_norm, norm_type)\r\n        else:\r\n            return -1\r\n\r\n    def state_dict(self):\r\n        """"""\r\n        Returns a dict containing the current state of this :class:`FP16_Optimizer` instance.\r\n        This dict contains attributes of :class:`FP16_Optimizer`, as well as the state_dict\r\n        of the contained Pytorch optimizer.\r\n        Example::\r\n\r\n            checkpoint = {}\r\n            checkpoint[\'model\'] = model.state_dict()\r\n            checkpoint[\'optimizer\'] = optimizer.state_dict()\r\n            torch.save(checkpoint, ""saved.pth"")\r\n        """"""\r\n        state_dict = {}\r\n        state_dict[\'loss_scaler\'] = self.loss_scaler\r\n        state_dict[\'dynamic_loss_scale\'] = self.dynamic_loss_scale\r\n        state_dict[\'overflow\'] = self.overflow\r\n        state_dict[\'first_closure_call_this_step\'] = self.first_closure_call_this_step\r\n        state_dict[\'optimizer_state_dict\'] = self.optimizer.state_dict()\r\n        state_dict[\'fp32_from_fp16\'] = self.fp32_from_fp16_groups\r\n        return state_dict\r\n\r\n    def load_state_dict(self, state_dict):\r\n        """"""\r\n        Loads a state_dict created by an earlier call to state_dict(). \r\n        If ``fp16_optimizer_instance`` was constructed from some ``init_optimizer``, \r\n        whose parameters in turn came from ``model``, it is expected that the user \r\n        will call ``model.load_state_dict()`` before\r\n        ``fp16_optimizer_instance.load_state_dict()`` is called.\r\n\r\n        Example::\r\n\r\n            model = torch.nn.Linear(D_in, D_out).cuda().half()\r\n            optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\r\n            optimizer = FP16_Optimizer(optimizer, static_loss_scale = 128.0)\r\n            ...\r\n            checkpoint = torch.load(""saved.pth"")\r\n            model.load_state_dict(checkpoint[\'model\'])\r\n            optimizer.load_state_dict(checkpoint[\'optimizer\'])\r\n        """"""\r\n        # I think it should actually be ok to reload the optimizer before the model.\r\n        self.loss_scaler = state_dict[\'loss_scaler\']\r\n        self.dynamic_loss_scale = state_dict[\'dynamic_loss_scale\']\r\n        self.overflow = state_dict[\'overflow\']\r\n        self.first_closure_call_this_step = state_dict[\'first_closure_call_this_step\']\r\n        self.optimizer.load_state_dict(state_dict[\'optimizer_state_dict\'])\r\n        # At this point, the optimizer\'s references to the model\'s fp32 parameters are up to date.\r\n        # The optimizer\'s hyperparameters and internal buffers are also up to date.  \r\n        # However, the fp32 master copies of the model\'s fp16 params stored by the optimizer are still\r\n        # out of date.  There are two options.  \r\n        # 1:  Refresh the master params from the model\'s fp16 params.  \r\n        # This requires less storage but incurs precision loss.\r\n        # 2:  Save and restore the fp32 master copies separately.\r\n        # We choose option 2.\r\n        # \r\n        # Pytorch Optimizer.load_state_dict casts saved buffers (e.g. momentum) to the type and device \r\n        # of their associated parameters, because it\'s possible those buffers might not exist yet in \r\n        # the current optimizer instance.  In our case, as long as the current FP16_Optimizer has been \r\n        # constructed in the same way as the one whose state_dict we are loading, the same master params\r\n        # are guaranteed to exist, so we can just copy_() from the saved master params.\r\n        for current_group, saved_group in zip(self.fp32_from_fp16_groups, state_dict[\'fp32_from_fp16\']):\r\n            for current, saved in zip(current_group, saved_group):\r\n                current.data.copy_(saved.data)\r\n\r\n    def step(self, closure=None): # could add clip option.\r\n        """"""\r\n        If no closure is supplied, :attr:`step` should be called after \r\n        ``fp16_optimizer_obj.backward(loss)``.\r\n        :attr:`step` updates the fp32 master copy of parameters using the optimizer supplied to\r\n        :class:`FP16_Optimizer`\'s constructor, then copies the updated fp32 params into the fp16 params\r\n        originally referenced by :class:`FP16_Optimizer`\'s constructor, so the user may immediately run\r\n        another forward pass using their model.\r\n\r\n        If a closure is supplied, :attr:`step` may be called without a prior call to \r\n        :attr:`backward(loss)`.\r\n        This control flow is identical to `ordinary Pytorch optimizer use`_ with closures.\r\n        However, the user should take care that any ``loss.backward()`` call within the closure\r\n        has been replaced by ``fp16_optimizer_obj.backward(loss)``.\r\n\r\n        Args:\r\n           closure (optional):  Closure that will be supplied to the underlying optimizer originally passed to :class:`FP16_Optimizer`\'s constructor.  closure should call :attr:`zero_grad()` on the :class:`FP16_Optimizer` object, compute the loss, call :attr:`backward(loss)`, and return the loss.\r\n\r\n        Example with closure::\r\n\r\n            # optimizer is assumed to be an FP16_Optimizer object, previously constructed from an \r\n            # existing pytorch optimizer.\r\n            for input, target in dataset:\r\n                def closure():\r\n                    optimizer.zero_grad()\r\n                    output = model(input)\r\n                    loss = loss_fn(output, target)\r\n                    # loss.backward() becomes:\r\n                    optimizer.backward(loss)\r\n                    return loss\r\n                optimizer.step(closure)\r\n\r\n        .. warning::\r\n            Currently, calling :attr:`step` with a closure is not compatible with dynamic loss scaling.\r\n\r\n        .. _`ordinary Pytorch optimizer use`:\r\n            http://pytorch.org/docs/master/optim.html#optimizer-step-closure\r\n        """"""\r\n\r\n        scale = self.loss_scaler.loss_scale()\r\n        # To consider:  Should this be in step(), or update_master_grads?  It works either way,\r\n        # but I should make it consistent with the Amp control flow, which updates the scale\r\n        # during backward context manager exit.\r\n        # self._update_scale(self.overflow)\r\n\r\n        if self.overflow:\r\n            # Using _amp_state.maybe_print instead of self.print here is intentional.\r\n            maybe_print(""Gradient overflow.  Skipping step, reducing "" +\r\n                ""loss scale to {}"".format(self.loss_scaler.loss_scale()))\r\n            return\r\n        \r\n        if closure is not None:\r\n            retval = self._step_with_closure(closure)\r\n        else:\r\n            # torch.cuda.nvtx.range_push(""pytorch optimizer step"")\r\n            retval = self.optimizer.step()\r\n            # torch.cuda.nvtx.range_pop()\r\n\r\n        self._master_params_to_model_params()\r\n\r\n        return retval\r\n\r\n    def _step_with_closure(self, closure):\r\n        def wrapped_closure():\r\n            # helpful for debugging\r\n            # print(""Calling wrapped_closure, first_closure_call_this_step = {}""\r\n            #       .format(self.first_closure_call_this_step))\r\n            if self.first_closure_call_this_step:\r\n                # We expect that the fp16 params are initially fresh on entering self.step(),\r\n                # so _master_params_to_model_params() is unnecessary the first time wrapped_closure()\r\n                # is called within self.optimizer.step().\r\n                self.first_closure_call_this_step = False\r\n            else:\r\n                # If self.optimizer.step() internally calls wrapped_closure more than once,\r\n                # it may update the fp32 params after each call.  However, self.optimizer \r\n                # doesn\'t know about the fp16 params at all.  If the fp32 params get updated,\r\n                # we can\'t rely on self.optimizer to refresh the fp16 params.  We need\r\n                # to handle that manually:\r\n                self._master_params_to_model_params()\r\n            # Our API expects the user to give us ownership of the backward() call by\r\n            # replacing all calls to loss.backward() with optimizer.backward(loss).\r\n            # This requirement holds whether or not the call to backward() is made within a closure.\r\n            # If the user is properly calling optimizer.backward(loss) within ""closure,"" \r\n            # calling closure() here will give the fp32 master params fresh gradients\r\n            # for the optimizer to play with, so all wrapped_closure needs to do is call \r\n            # closure() and return the loss.\r\n            temp_loss = closure() \r\n            while(self.overflow):\r\n                scale = self.loss_scaler.loss_scale()\r\n                # self._update_scale(self.overflow) # now done at the end of backward\r\n                print(""OVERFLOW within closure! Skipping step, reducing loss scale to {}"".format(\r\n                      self.loss_scaler.loss_scale()))\r\n                temp_loss = closure()\r\n            return temp_loss\r\n\r\n        retval = self.optimizer.step(wrapped_closure)\r\n\r\n        self.first_closure_call_this_step = True\r\n\r\n        return retval\r\n\r\n    def backward(self, loss, update_master_grads=True, retain_graph=False):\r\n        """""" \r\n        :attr:`backward` performs the following conceptual steps:\r\n\r\n        1. fp32_loss = loss.float() (see first Note below)\r\n        2. scaled_loss = fp32_loss*loss_scale\r\n        3. scaled_loss.backward(), which accumulates scaled gradients into the ``.grad`` attributes of the model\'s leaves (which may be fp16, fp32, or a mixture, depending how your model was defined).\r\n        4. fp16 grads are then copied to the master params\' ``.grad`` attributes (see second Note), which are guaranteed to be fp32.\r\n        5. Finally, master grads are divided by loss_scale.\r\n\r\n        In this way, after :attr:`backward`, the master params have fresh gradients,\r\n        and :attr:`step` may be called.\r\n\r\n        .. note::\r\n            :attr:`backward` internally converts the loss to fp32 before applying the loss scale.\r\n            This provides some additional safety against overflow if the user has supplied an \r\n            fp16 loss value.  \r\n            However, for maximum overflow safety, the user should\r\n            compute the loss criterion (MSE, cross entropy, etc) in fp32 before supplying it to \r\n            :attr:`backward`.\r\n\r\n        .. warning::\r\n            The gradients found in a model\'s leaves after the call to \r\n            :attr:`backward` should not be regarded as valid in general, \r\n            because it\'s possible \r\n            they have been scaled (and in the case of dynamic loss scaling, \r\n            the scale factor may change over time).  \r\n            If the user wants to inspect gradients after a call to :attr:`backward`,  \r\n            only the master gradients should be regarded as valid.  These can be retrieved via\r\n            :attr:`inspect_master_grad_data()`.\r\n\r\n        Args:\r\n            loss:  The loss output by the user\'s model.  loss may be either float or half (but see first Note above).\r\n            update_master_grads (bool, optional, default=True):  Option to copy fp16 grads to fp32 grads on this call.  By setting this to False, the user can delay the copy, which is useful to eliminate redundant fp16->fp32 grad copies if :attr:`backward` is being called on multiple losses in one iteration.  If set to False, the user becomes responsible for calling :attr:`update_master_grads` before calling :attr:`step`.\r\n            retain_graph (bool, optional, default=False):  Forwards the usual ``retain_graph=True`` option to the internal call to ``loss.backward``.  If ``retain_graph`` is being used to accumulate gradient values from multiple backward passes before calling ``optimizer.step``, passing ``update_master_grads=False`` is also recommended (see Example below).\r\n\r\n        Example::\r\n\r\n            # Ordinary operation:\r\n            optimizer.backward(loss)\r\n\r\n            # Naive operation with multiple losses (technically valid, but less efficient):\r\n            # fp32 grads will be correct after the second call,  but \r\n            # the first call incurs an unnecessary fp16->fp32 grad copy.\r\n            optimizer.backward(loss1)\r\n            optimizer.backward(loss2)\r\n\r\n            # More efficient way to handle multiple losses:\r\n            # The fp16->fp32 grad copy is delayed until fp16 grads from all \r\n            # losses have been accumulated.\r\n            optimizer.backward(loss1, update_master_grads=False)\r\n            optimizer.backward(loss2, update_master_grads=False)\r\n            optimizer.update_master_grads()\r\n        """""" \r\n        # To consider:  try multiple backward passes using retain_grad=True to find \r\n        # a loss scale that works.  After you find a loss scale that works, do a final dummy\r\n        # backward pass with retain_graph=False to tear down the graph.  Doing this would avoid \r\n        # discarding the iteration,  but probably wouldn\'t improve overall efficiency.  \r\n        scaled_loss = loss.float()*self.loss_scaler.loss_scale()\r\n        scaled_loss.backward(retain_graph=retain_graph)\r\n        if update_master_grads:\r\n            self.update_master_grads()\r\n\r\n    def update_master_grads(self):\r\n        # torch.cuda.nvtx.range_push(""update_master_grads"")\r\n        """"""\r\n        Copy the ``.grad`` attribute from stored references to fp16 parameters to \r\n        the ``.grad`` attribute of the fp32 master parameters that are directly \r\n        updated by the optimizer.  :attr:`update_master_grads` only needs to be called if\r\n        ``fp16_optimizer_obj.backward`` was called with ``update_master_grads=False``.\r\n        """"""\r\n        # if self.dynamic_loss_scale:\r\n        #     self._check_overflow()\r\n        #     if self.overflow: return\r\n        # self._model_grads_to_master_grads()\r\n        # self._downscale_master()\r\n        # Use the one-shot multi-tensor apply kernel\r\n        self.loss_scaler.clear_overflow_state()\r\n        if len(self.all_fp16_params) > 0:\r\n            # print(""Model grads before"")\r\n            # print([param.grad.data for param in self.all_fp16_params])\r\n            # I\'m ONLY writing this as an incremental way to make some tests pass until\r\n            # I can refactor the tests as well.\r\n            # FP16_Optimizer should not be used by anyone.\r\n            model_grads = []\r\n            master_grads = []\r\n            for model_param, master_param in zip(self.all_fp16_params,\r\n                                                 self.all_fp32_from_fp16_params):\r\n                if model_param.grad is not None:\r\n                    model_grads.append(model_param.grad)\r\n                    if master_param.grad is None:\r\n                        master_param.grad = torch.empty_like(master_param)\r\n                    master_grads.append(master_param.grad)\r\n            self.loss_scaler.unscale(\r\n                model_grads,\r\n                master_grads,\r\n                self.loss_scaler.loss_scale())\r\n            # print(""Master grads after"")\r\n            # print([param.grad.data for param in self.all_fp32_from_fp16_params])\r\n        if len(self.all_fp32_from_fp32_params) > 0:\r\n            model_grads = []\r\n            master_grads = []\r\n            for model_param, master_param in zip(self.all_fp32_from_fp32_params,\r\n                                                 self.all_fp32_from_fp32_params):\r\n                if model_param.grad is not None:\r\n                    model_grads.append(model_param.grad)\r\n                    master_grads.append(master_param.grad)\r\n            # print(""Model grads before"")\r\n            # print([param.grad.data for param in self.all_fp32_from_fp32_params])\r\n            self.loss_scaler.unscale(\r\n                model_grads,\r\n                master_grads,\r\n                self.loss_scaler.loss_scale())\r\n            # print(""Master grads after"")\r\n            # print([param.grad.data for param in self.all_fp32_from_fp32_params])\r\n        # quit()\r\n        self.overflow = self.loss_scaler.update_scale()\r\n        # torch.cuda.nvtx.range_pop()\r\n\r\n\r\n    def inspect_master_grad_data(self):\r\n        """"""\r\n        When running with :class:`FP16_Optimizer`, \r\n        ``.grad`` attributes of a model\'s fp16 leaves should not be\r\n        regarded as truthful, because they might be scaled.  \r\n        After a call to :attr:`fp16_optimizer_obj.backward(loss)`, if no overflow was encountered,\r\n        the fp32 master params\' ``.grad``\r\n        attributes will contain valid gradients properly divided by the loss scale.  However, \r\n        because :class:`FP16_Optimizer` flattens some parameters, accessing them may be \r\n        nonintuitive.  :attr:`inspect_master_grad_data`\r\n        allows those gradients to be viewed with shapes corresponding to their associated model leaves.\r\n\r\n        Returns:\r\n            List of lists (one list for each parameter group).  The list for each parameter group\r\n            is a list of the ``.grad.data`` attributes of the fp32 master params belonging to that group.                 \r\n        """"""\r\n        if self.overflow:\r\n            print(""Warning:  calling FP16_Optimizer.inspect_master_grad_data while in an overflow state.  ""\r\n                  ""Gradients are currently invalid (may be inf, nan, or stale).  Returning None."")\r\n            return None\r\n        else:\r\n            # The optimizer owns only references to master params.\r\n            master_grads_data = []\r\n            for param_group in self.optimizer.param_groups:\r\n                master_grads_this_group = []\r\n                for param in param_group[\'params\']:\r\n                    if param.grad is not None:\r\n                        master_grads_this_group.append(param.grad.data)\r\n                    else:\r\n                        master_grads_this_group.append(None)\r\n                master_grads_data.append(master_grads_this_group)\r\n            return master_grads_data\r\n\r\n\r\n    # Promote loss scale so it can be retrieved or set via ""fp16_optimizer_instance.loss_scale""\r\n    def _get_loss_scale(self):\r\n        return self.loss_scaler.loss_scale()\r\n\r\n    def _set_loss_scale(self, value):\r\n        self.loss_scaler._loss_scale = value\r\n\r\n    loss_scale = property(_get_loss_scale, _set_loss_scale)\r\n\r\n    # Promote state so it can be retrieved or set via ""fp16_optimizer_instance.state""\r\n    def _get_state(self):\r\n        return self.optimizer.state\r\n\r\n    def _set_state(self, value):\r\n        self.optimizer.state = value\r\n\r\n    state = property(_get_state, _set_state)\r\n\r\n    # Promote param_groups so it can be retrieved or set via ""fp16_optimizer_instance.param_groups""\r\n    # (for example, to adjust the learning rate)\r\n    def _get_param_groups(self):\r\n        return self.optimizer.param_groups\r\n\r\n    def _set_param_groups(self, value):\r\n        self.optimizer.param_groups = value\r\n\r\n    param_groups = property(_get_param_groups, _set_param_groups)\r\n\r\n'"
apex/apex/fp16_utils/fp16util.py,14,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors\n\n\nclass tofp16(nn.Module):\n    """"""\n    Utility module that implements::\n\n        def forward(self, input):\n            return input.half()\n    """"""\n\n    def __init__(self):\n        super(tofp16, self).__init__()\n\n    def forward(self, input):\n        return input.half()\n\n\ndef BN_convert_float(module):\n    """"""\n    Utility function for network_to_half().\n\n    Retained for legacy purposes.\n    """"""\n    if isinstance(module, torch.nn.modules.batchnorm._BatchNorm) and module.affine is True:\n        module.float()\n    for child in module.children():\n        BN_convert_float(child)\n    return module\n\n\ndef network_to_half(network):\n    """"""\n    Convert model to half precision in a batchnorm-safe way.\n\n    Retained for legacy purposes. It is recommended to use FP16Model.\n    """"""\n    return nn.Sequential(tofp16(), BN_convert_float(network.half()))\n\n\ndef convert_module(module, dtype):\n    """"""\n    Converts a module\'s immediate parameters and buffers to dtype.\n    """"""\n    for param in module.parameters(recurse=False):\n        if param is not None:\n            if param.data.dtype.is_floating_point:\n                param.data = param.data.to(dtype=dtype)\n            if param._grad is not None and param._grad.data.dtype.is_floating_point:\n                param._grad.data = param._grad.data.to(dtype=dtype)\n\n    for buf in module.buffers(recurse=False):\n        if buf is not None and buf.data.dtype.is_floating_point:\n            buf.data = buf.data.to(dtype=dtype)\n\n\ndef convert_network(network, dtype):\n    """"""\n    Converts a network\'s parameters and buffers to dtype.\n    """"""\n    for module in network.modules():\n        if isinstance(module, torch.nn.modules.batchnorm._BatchNorm) and module.affine is True:\n            continue\n        convert_module(module, dtype)\n        if isinstance(module, torch.nn.RNNBase) or isinstance(module, torch.nn.modules.rnn.RNNBase):\n            module.flatten_parameters()\n    return network\n\n\nclass FP16Model(nn.Module):\n    """"""\n    Convert model to half precision in a batchnorm-safe way.\n    """"""\n\n    def __init__(self, network):\n        super(FP16Model, self).__init__()\n        self.network = convert_network(network, dtype=torch.half)\n\n    def forward(self, *inputs):\n        inputs = tuple(t.half() for t in inputs)\n        return self.network(*inputs)\n\n\ndef backwards_debug_hook(grad):\n    raise RuntimeError(""master_params recieved a gradient in the backward pass!"")\n\ndef prep_param_lists(model, flat_master=False):\n    """"""\n    Creates a list of FP32 master parameters for a given model, as in\n    `Training Neural Networks with Mixed Precision:  Real Examples`_.\n\n    Args:\n        model (torch.nn.Module): Existing Pytorch model\n        flat_master (bool, optional, default=False):  Flatten the master parameters into a single tensor, as a performance optimization.\n    Returns:\n        A tuple (``model_params``, ``master_params``). ``model_params`` is a list of the model\'s parameters for later use with :func:`model_grads_to_master_grads` and :func:`master_params_to_model_params`.  ``master_params`` is a list of FP32 master gradients.  If ``flat_master=True``, ``master_params`` will be a list with one element.\n\n    Example::\n\n        model_params, master_params = prep_param_lists(model)\n\n    .. warning::\n        Currently, if ``flat_master=True``, all the model\'s parameters must be the same type.  If the model has parameters of different types, use ``flat_master=False``, or use :class:`FP16_Optimizer`.\n\n    .. _`Training Neural Networks with Mixed Precision:  Real Examples`:\n        http://on-demand.gputechconf.com/gtc/2018/video/S81012/\n    """"""\n    model_params = [param for param in model.parameters() if param.requires_grad]\n\n    if flat_master:\n        # Give the user some more useful error messages\n        try:\n            # flatten_dense_tensors returns a contiguous flat array.\n            # http://pytorch.org/docs/master/_modules/torch/_utils.html\n            master_params = _flatten_dense_tensors([param.data for param in model_params]).float()\n        except:\n            print(""Error in prep_param_lists:  model may contain a mixture of parameters ""\n                      ""of different types.  Use flat_master=False, or use F16_Optimizer."")\n            raise\n        master_params = torch.nn.Parameter(master_params)\n        master_params.requires_grad = True\n        # master_params.register_hook(backwards_debug_hook)\n        if master_params.grad is None:\n            master_params.grad = master_params.new(*master_params.size())\n        return model_params, [master_params]\n    else:\n        master_params = [param.clone().float().detach() for param in model_params]\n        for param in master_params:\n            param.requires_grad = True\n        return model_params, master_params\n\n\ndef model_grads_to_master_grads(model_params, master_params, flat_master=False):\n    """"""\n    Copy model gradients to master gradients.  \n\n    Args:\n        model_params:  List of model parameters created by :func:`prep_param_lists`.\n        master_params:  List of FP32 master parameters created by :func:`prep_param_lists`.  If ``master_params`` was created with ``flat_master=True``, ``flat_master=True`` should also be supplied to :func:`model_grads_to_master_grads`.\n    """"""\n    if flat_master:\n        # The flattening may incur one more deep copy than is necessary.\n        master_params[0].grad.data.copy_(\n            _flatten_dense_tensors([p.grad.data for p in model_params]))\n    else:\n        for model, master in zip(model_params, master_params):\n            if model.grad is not None:\n                if master.grad is None:\n                    master.grad = Variable(master.data.new(*master.data.size()))\n                master.grad.data.copy_(model.grad.data)\n            else:\n                master.grad = None\n\n\ndef master_params_to_model_params(model_params, master_params, flat_master=False):\n    """"""\n    Copy master parameters to model parameters.\n\n    Args:\n        model_params:  List of model parameters created by :func:`prep_param_lists`.\n        master_params:  List of FP32 master parameters created by :func:`prep_param_lists`.  If ``master_params`` was created with ``flat_master=True``, ``flat_master=True`` should also be supplied to :func:`master_params_to_model_params`.\n    """"""\n    if flat_master:\n        for model, master in zip(model_params, \n                                 _unflatten_dense_tensors(master_params[0].data, model_params)):\n            model.data.copy_(master)\n    else:\n        for model, master in zip(model_params, master_params):\n            model.data.copy_(master.data)\n\n# Backward compatibility fixes\n\ndef to_python_float(t):\n    if hasattr(t, \'item\'):\n        return t.item()\n    else:\n        return t[0]\n\nTORCH_MAJOR = int(torch.__version__.split(\'.\')[0])\nTORCH_MINOR = int(torch.__version__.split(\'.\')[1])\nif TORCH_MAJOR == 0 and TORCH_MINOR <= 4:\n    clip_grad_norm = torch.nn.utils.clip_grad_norm\nelse:\n    clip_grad_norm = torch.nn.utils.clip_grad_norm_\n'"
apex/apex/fp16_utils/loss_scaler.py,10,"b'import torch\n\n# item() is a recent addition, so this helps with backward compatibility.\ndef to_python_float(t):\n    if hasattr(t, \'item\'):\n        return t.item()\n    else:\n        return t[0]\n\nclass LossScaler:\n    """"""\n    Class that manages a static loss scale.  This class is intended to interact with\n    :class:`FP16_Optimizer`, and should not be directly manipulated by the user.\n\n    Use of :class:`LossScaler` is enabled via the ``static_loss_scale`` argument to \n    :class:`FP16_Optimizer`\'s constructor.\n\n    Args:\n        scale (float, optional, default=1.0):  The loss scale.\n    """"""\n\n    def __init__(self, scale=1):\n        self.cur_scale = scale\n\n    # `params` is a list / generator of torch.Variable\n    def has_overflow(self, params):\n        return False\n\n    # `x` is a torch.Tensor\n    def _has_inf_or_nan(x):\n        return False\n\n    def update_scale(self, overflow):\n        pass\n\n    @property\n    def loss_scale(self):\n        return self.cur_scale\n\n    def scale_gradient(self, module, grad_in, grad_out):\n        return tuple(self.loss_scale * g for g in grad_in)\n\n    def backward(self, loss, retain_graph=False):\n        scaled_loss = loss*self.loss_scale\n        scaled_loss.backward(retain_graph=retain_graph)\n\nclass DynamicLossScaler:\n    """"""\n    Class that manages dynamic loss scaling.  It is recommended to use :class:`DynamicLossScaler`\n    indirectly, by supplying ``dynamic_loss_scale=True`` to the constructor of \n    :class:`FP16_Optimizer`.  However, it\'s important to understand how :class:`DynamicLossScaler`\n    operates, because the default options can be changed using the\n    the ``dynamic_loss_args`` argument to :class:`FP16_Optimizer`\'s constructor.\n\n    Loss scaling is designed to combat the problem of underflowing gradients encountered at long\n    times when training fp16 networks.  Dynamic loss scaling begins by attempting a very high loss\n    scale.  Ironically, this may result in OVERflowing gradients.  If overflowing gradients are\n    encountered, :class:`DynamicLossScaler` informs :class:`FP16_Optimizer` that an overflow has \n    occurred.\n    :class:`FP16_Optimizer` then skips the update step for this particular iteration/minibatch,\n    and :class:`DynamicLossScaler` adjusts the loss scale to a lower value.  \n    If a certain number of iterations occur without overflowing gradients detected,\n    :class:`DynamicLossScaler` increases the loss scale once more.\n    In this way :class:`DynamicLossScaler` attempts to ""ride the edge"" of \n    always using the highest loss scale possible without incurring overflow.\n\n    Args:\n        init_scale (float, optional, default=2**32):  Initial loss scale attempted by :class:`DynamicLossScaler.`\n        scale_factor (float, optional, default=2.0):  Factor used when adjusting the loss scale. If an overflow is encountered, the loss scale is readjusted to loss scale/``scale_factor``.  If ``scale_window`` consecutive iterations take place without an overflow, the loss scale is readjusted to loss_scale*``scale_factor``. \n        scale_window (int, optional, default=1000):  Number of consecutive iterations without an overflow to wait before increasing the loss scale.\n    """"""\n\n    def __init__(self,\n                 init_scale=2**32,\n                 scale_factor=2.,\n                 scale_window=1000):\n        self.cur_scale = init_scale\n        self.cur_iter = 0\n        self.last_overflow_iter = -1\n        self.scale_factor = scale_factor\n        self.scale_window = scale_window\n\n    # `params` is a list / generator of torch.Variable\n    def has_overflow(self, params):\n        for p in params:\n            if p.grad is not None and DynamicLossScaler._has_inf_or_nan(p.grad.data):\n                return True\n\n        return False\n\n    # `x` is a torch.Tensor\n    def _has_inf_or_nan(x):\n        try:\n            # if x is half, the .float() incurs an additional deep copy, but it\'s necessary if \n            # Pytorch\'s .sum() creates a one-element tensor of the same type as x \n            # (which is true for some recent version of pytorch).\n            cpu_sum = float(x.float().sum())\n            # More efficient version that can be used if .sum() returns a Python scalar\n            # cpu_sum = float(x.sum())\n        except RuntimeError as instance:\n            # We want to check if inst is actually an overflow exception.\n            # RuntimeError could come from a different error.\n            # If so, we still want the exception to propagate.\n            if ""value cannot be converted"" not in instance.args[0]:\n                raise\n            return True\n        else:\n            if cpu_sum == float(\'inf\') or cpu_sum == -float(\'inf\') or cpu_sum != cpu_sum:\n                return True\n            return False\n\n    # `overflow` is boolean indicating whether the gradient overflowed\n    def update_scale(self, overflow):\n        if overflow:\n            # self.cur_scale /= self.scale_factor\n            self.cur_scale = max(self.cur_scale/self.scale_factor, 1)\n            self.last_overflow_iter = self.cur_iter\n        else:\n            if (self.cur_iter - self.last_overflow_iter) % self.scale_window == 0:\n                self.cur_scale *= self.scale_factor\n        self.cur_iter += 1\n\n    @property\n    def loss_scale(self):\n        return self.cur_scale\n\n    def scale_gradient(self, module, grad_in, grad_out):\n        return tuple(self.loss_scale * g for g in grad_in)\n\n    def backward(self, loss, retain_graph=False):\n        scaled_loss = loss*self.loss_scale\n        scaled_loss.backward(retain_graph=retain_graph)\n        \n##############################################################        \n# Example usage below here -- assuming it\'s in a separate file\n##############################################################\n""""""\nTO-DO separate out into an example.\nif __name__ == ""__main__"":\n    import torch\n    from torch.autograd import Variable\n    from dynamic_loss_scaler import DynamicLossScaler\n\n    # N is batch size; D_in is input dimension;\n    # H is hidden dimension; D_out is output dimension.\n    N, D_in, H, D_out = 64, 1000, 100, 10\n\n    # Create random Tensors to hold inputs and outputs, and wrap them in Variables.\n    x = Variable(torch.randn(N, D_in), requires_grad=False)\n    y = Variable(torch.randn(N, D_out), requires_grad=False)\n\n    w1 = Variable(torch.randn(D_in, H), requires_grad=True)\n    w2 = Variable(torch.randn(H, D_out), requires_grad=True)\n    parameters = [w1, w2]\n\n    learning_rate = 1e-6\n    optimizer = torch.optim.SGD(parameters, lr=learning_rate)\n    loss_scaler = DynamicLossScaler()\n\n    for t in range(500):\n        y_pred = x.mm(w1).clamp(min=0).mm(w2)\n        loss = (y_pred - y).pow(2).sum() * loss_scaler.loss_scale\n        print(\'Iter {} loss scale: {}\'.format(t, loss_scaler.loss_scale))\n        print(\'Iter {} scaled loss: {}\'.format(t, loss.data[0]))\n        print(\'Iter {} unscaled loss: {}\'.format(t, loss.data[0] / loss_scaler.loss_scale))\n\n        # Run backprop\n        optimizer.zero_grad()\n        loss.backward()\n        \n        # Check for overflow\n        has_overflow = DynamicLossScaler.has_overflow(parameters)\n        \n        # If no overflow, unscale grad and update as usual\n        if not has_overflow:\n            for param in parameters:\n                param.grad.data.mul_(1. / loss_scaler.loss_scale)\n            optimizer.step()\n        # Otherwise, don\'t do anything -- ie, skip iteration\n        else:\n            print(\'OVERFLOW!\')\n\n        # Update loss scale for next iteration\n        loss_scaler.update_scale(has_overflow)\n\n""""""\n'"
apex/apex/multi_tensor_apply/__init__.py,0,b'from .multi_tensor_apply import MultiTensorApply\n\nmulti_tensor_applier = MultiTensorApply(2048*32)\n\n'
apex/apex/multi_tensor_apply/multi_tensor_apply.py,0,"b'import torch\n\nclass MultiTensorApply(object):\n    available = False\n    warned = False\n\n    def __init__(self, chunk_size):\n        try:\n            import amp_C\n            MultiTensorApply.available = True\n            self.chunk_size = chunk_size\n        except ImportError as err:\n            MultiTensorApply.available = False\n            MultiTensorApply.import_err = err\n\n    def check_avail(self):\n        if MultiTensorApply.available == False:\n            raise RuntimeError(\n                ""Attempted to call MultiTensorApply method, but MultiTensorApply ""\n                ""is not available, possibly because Apex was installed without ""\n                ""--cpp_ext --cuda_ext.  Original import error message:"",\n                MultiTensorApply.import_err)\n\n    def __call__(self, op, noop_flag_buffer, tensor_lists, *args):\n        self.check_avail()\n\n        return op(self.chunk_size,\n                  noop_flag_buffer,\n                  tensor_lists,\n                  *args)\n'"
apex/apex/normalization/__init__.py,0,b'from .fused_layer_norm import FusedLayerNorm\n'
apex/apex/normalization/fused_layer_norm.py,11,"b'import math\nimport torch\nimport numbers\nfrom torch.nn.parameter import Parameter\nfrom torch.nn import init\nfrom torch.nn import functional as F\nimport importlib\n\nclass FusedLayerNormAffineFunction(torch.autograd.Function):\n  def __init__(self, normalized_shape, eps=1e-6):\n    global fused_layer_norm_cuda\n    fused_layer_norm_cuda = importlib.import_module(""fused_layer_norm_cuda"")\n\n    self.normalized_shape = normalized_shape\n    self.eps = eps\n\n  def forward(self, input, weight, bias):\n    input_ = input.contiguous()\n    weight_ = weight.contiguous()\n    bias_ = bias.contiguous()\n    output, mean, invvar = fused_layer_norm_cuda.forward_affine(\n        input_, self.normalized_shape, weight_, bias_, self.eps)\n    self.save_for_backward(input_, weight_, bias_, mean, invvar)\n    return output\n\n  def backward(self, grad_output):\n    input_, weight_, bias_, mean, invvar = self.saved_tensors\n    grad_input = grad_weight = grad_bias = None\n    grad_input, grad_weight, grad_bias = fused_layer_norm_cuda.backward_affine(\n        grad_output.contiguous(), mean, invvar,\n        input_, self.normalized_shape, \n        weight_, bias_, self.eps)\n    return grad_input, grad_weight, grad_bias;\n    \nclass FusedLayerNormFunction(torch.autograd.Function):\n  def __init__(self, normalized_shape, eps=1e-6):\n    global fused_layer_norm_cuda\n    fused_layer_norm_cuda = importlib.import_module(""fused_layer_norm_cuda"")\n    self.normalized_shape = normalized_shape\n    self.eps = eps\n\n  def forward(self, input):\n    input_ = input.contiguous()\n    output, mean, invvar = fused_layer_norm_cuda.forward(\n        input_, self.normalized_shape, self.eps)\n    self.save_for_backward(input_, mean, invvar)\n    return output\n\n  def backward(self, grad_output):\n    input_, mean, invvar = self.saved_tensors\n    grad_input = None\n    grad_input = fused_layer_norm_cuda.backward(\n        grad_output.contiguous(), mean, invvar,\n        input_, self.normalized_shape,\n        self.eps)\n    return grad_input\n\ndef fused_layer_norm_affine(input, normalized_shape, weight, bias, eps=1e-6):\n    return FusedLayerNormAffineFunction(normalized_shape,eps)(input, weight, bias)\n\ndef fused_layer_norm(input, normalized_shape, eps=1e-6):\n    return FusedLayerNormFunction(normalized_shape,eps)(input)\n\nclass FusedLayerNorm(torch.nn.Module):\n    r""""""Applies Layer Normalization over a mini-batch of inputs as described in\n    the paper `Layer Normalization`_ .\n\n    Currently only runs on cuda() tensors.\n\n    .. math::\n        y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\n\n    The mean and standard-deviation are calculated separately over the last\n    certain number dimensions which have to be of the shape specified by\n    :attr:`normalized_shape`.\n    :math:`\\gamma` and :math:`\\beta` are learnable affine transform parameters of\n    :attr:`normalized_shape` if :attr:`elementwise_affine` is ``True``.\n\n    .. note::\n        Unlike Batch Normalization and Instance Normalization, which applies\n        scalar scale and bias for each entire channel/plane with the\n        :attr:`affine` option, Layer Normalization applies per-element scale and\n        bias with :attr:`elementwise_affine`.\n\n    This layer uses statistics computed from input data in both training and\n    evaluation modes.\n\n    Args:\n        normalized_shape (int or list or torch.Size): input shape from an expected input\n            of size\n\n            .. math::\n                [* \\times \\text{normalized}\\_\\text{shape}[0] \\times \\text{normalized}\\_\\text{shape}[1]\n                    \\times \\ldots \\times \\text{normalized}\\_\\text{shape}[-1]]\n\n            If a single integer is used, it is treated as a singleton list, and this module will\n            normalize over the last dimension which is expected to be of that specific size.\n        eps: a value added to the denominator for numerical stability. Default: 1e-5\n        elementwise_affine: a boolean value that when set to ``True``, this module\n            has learnable per-element affine parameters initialized to ones (for weights)\n            and zeros (for biases). Default: ``True``.\n\n    Shape:\n        - Input: :math:`(N, *)`\n        - Output: :math:`(N, *)` (same shape as input)\n\n    Examples::\n\n        >>> input = torch.randn(20, 5, 10, 10)\n        >>> # With Learnable Parameters\n        >>> m = apex.normalization.FusedLayerNorm(input.size()[1:])\n        >>> # Without Learnable Parameters\n        >>> m = apex.normalization.FusedLayerNorm(input.size()[1:], elementwise_affine=False)\n        >>> # Normalize over last two dimensions\n        >>> m = apex.normalization.FusedLayerNorm([10, 10])\n        >>> # Normalize over last dimension of size 10\n        >>> m = apex.normalization.FusedLayerNorm(10)\n        >>> # Activating the module\n        >>> output = m(input)\n\n    .. _`Layer Normalization`: https://arxiv.org/abs/1607.06450\n    """"""\n    def __init__(self, normalized_shape, eps=1e-5, elementwise_affine=True):\n        super(FusedLayerNorm, self).__init__()\n\n        global fused_layer_norm_cuda\n        fused_layer_norm_cuda = importlib.import_module(""fused_layer_norm_cuda"")\n\n        if isinstance(normalized_shape, numbers.Integral):\n            normalized_shape = (normalized_shape,)\n        self.normalized_shape = torch.Size(normalized_shape)\n        self.eps = eps\n        self.elementwise_affine = elementwise_affine\n        if self.elementwise_affine:\n            self.weight = Parameter(torch.Tensor(*normalized_shape))\n            self.bias = Parameter(torch.Tensor(*normalized_shape))\n        else:\n            self.register_parameter(\'weight\', None)\n            self.register_parameter(\'bias\', None)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        if self.elementwise_affine:\n            init.ones_(self.weight)\n            init.zeros_(self.bias)\n\n    def forward(self, input):\n        if not input.is_cuda:\n            return  F.layer_norm(\n                input, self.normalized_shape, self.weight, self.bias, self.eps)\n        if self.elementwise_affine:\n          return FusedLayerNormAffineFunction(self.normalized_shape,self.eps)(\n              input, self.weight, self.bias)\n        else:\n          return FusedLayerNormFunction(self.normalized_shape,self.eps)(\n              input)\n\n    def extra_repr(self):\n        return \'{normalized_shape}, eps={eps}, \' \\\n            \'elementwise_affine={elementwise_affine}\'.format(**self.__dict__)\n'"
apex/apex/optimizers/__init__.py,0,b'from .fused_adam import FusedAdam\nfrom .fp16_optimizer import FP16_Optimizer\n'
apex/apex/optimizers/fp16_optimizer.py,10,"b'import torch\nfrom torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors\n\nclass FP16_Optimizer(object):\n    """"""\n    :class:`FP16_Optimizer` A cutdown version of apex.fp16_utils.FP16_Optimizer.\n    Designed only to wrap apex.optimizers.FusedAdam.\n    Refer to apex.fp16_utils documents for more information.\n\n    Example::\n\n        model = torch.nn.Linear(D_in, D_out).cuda().half()\n        optimizer = apex.optimizers.FusedAdam(model.parameters())\n        # Name the FP16_Optimizer instance to replace the existing optimizer\n        # (recommended but not required):\n        optimizer = FP16_Optimizer(optimizer, static_loss_scale = 128.0)\n        ...\n        # loss.backward() becomes:\n        optimizer.backward(loss)\n        ...\n\n    Example with dynamic loss scaling::\n\n        ...\n        optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n                                   # optional arg to control dynamic loss scaling behavior\n                                   # dynamic_loss_args={\'scale_window\' : 500})\n                                   # Usually, dynamic_loss_args is not necessary.\n    """"""\n\n    def __init__(self,\n                 init_optimizer,\n                 static_loss_scale=1.0,\n                 dynamic_loss_scale=False,\n                 dynamic_loss_args=None,\n                 verbose=True):\n\n        # The fused optimizer does all the work. We need this layer for two reason:\n        # 1. maintain same user API from apex.fp16_utils\n        # 2. keep common stuff here in case we need to add new fused optimizer later\n\n        # differences from apex.fp16_utils:\n        # - assume all model params in fp16\n        # - assume all params requires grad\n        # - flat by groups, not keeping state. TODO: remove state explicitly?\n        # - master gard and unflat master weight never exist. TODO: a way to save out unflat master?\n        if not torch.cuda.is_available:\n            raise SystemError(""Cannot use fp16 without CUDA."")\n        self.optimizer = init_optimizer\n\n        # param flattened by groups\n        self.fp16_groups = []\n        self.fp16_groups_flat = []\n        self.fp32_groups_flat = []\n\n        # loop to deal with groups\n        for i, param_group in enumerate(self.optimizer.param_groups):\n            # push this group to list before modify\n            self.fp16_groups.append(param_group[\'params\'])\n            # init fp16 weight buffer, flattened\n            self.fp16_groups_flat.append(_flatten_dense_tensors([p.clone().detach() for p in self.fp16_groups[i]]))\n            # set model fp16 weight to slices of flattened buffer\n            updated_params = _unflatten_dense_tensors(self.fp16_groups_flat[i], self.fp16_groups[i])\n            for p,q in zip(self.fp16_groups[i], updated_params):\n                p.data = q.data\n            # init master weight, flattened\n            self.fp32_groups_flat.append(self.fp16_groups_flat[i].clone().float().detach())\n            # modify optimizer of have flat master weight\n            self.fp32_groups_flat[i].requires_grad = True # keep this in case internal optimizer uses it\n            param_group[\'params\'] = [self.fp32_groups_flat[i]]\n\n        # we may have a way of fusing dynamic scale. Do not support for now\n        if dynamic_loss_scale:\n            if dynamic_loss_args is not None:\n                raise SystemError(""Do not support dynamic loss scale args for now."")\n            self.dynamic_loss_scale = True\n            self.cur_scale = 2**16\n            self.cur_iter = 0\n            self.last_overflow_iter = -1\n            self.scale_factor = 2\n            self.scale_window = 1000\n        else:\n            self.dynamic_loss_scale = False\n            self.cur_iter = 0\n            self.cur_scale = static_loss_scale\n        self.verbose = verbose\n\n    def zero_grad(self, set_grads_to_None=True):\n        """"""\n        Zero FP16 parameter grads.\n        """"""\n        # FP32 grad should never exist.\n        # For speed, set model fp16 grad to None by default\n        for group in self.fp16_groups:\n            for p in group:\n                if set_grads_to_None:\n                    p.grad = None\n                else:\n                    if p.grad is not None:\n                        p.grad.detach_()\n                        p.grad.zero_()\n\n    def _compute_grad_norm(self, fp16_grads_flat, norm_type=2):\n        """"""\n        Compute fp16 grad norm for later clipping(fused with update).\n        Internal accumulated in fp32.\n        Also fused in NaN check. Possibly other reduction needed for grad.\n\n        Args:\n            fp16_grads_flat (tensor): fp16 grad flattened\n            norm_type (float or int): type of the used p-norm. Can be ``\'inf\'`` for\n                infinity norm.\n\n        Returns:\n            Total norm of the current fp16 gradients (viewed as a single vector).\n            Returns -1 if the most recently computed fp16 gradients overflowed\n        """"""\n        # TODO: Not most efficient with copy to cpu and sync\n        # only support 2-norm now\n        # for torch version <= 1.0.1, torch.norm with dtype will fail and fall back to cast\n        try:\n            norm = float(torch.norm(fp16_grads_flat, 2.0, dtype=torch.float32))\n        except TypeError as err:\n            norm = float(torch.norm(fp16_grads_flat.float(), 2.0))\n        if norm == float(\'inf\') or norm == -float(\'inf\') or norm != norm:\n            return -1\n        else:\n            return norm\n\n    def step(self, closure=None):\n        """"""\n        Not supporting closure.\n        """"""\n        # First compute norm for all group so we know if there is overflow\n        grads_groups_flat = []\n        norm_groups = []\n        skip = False\n        for i, group in enumerate(self.fp16_groups):\n            grads_groups_flat.append(_flatten_dense_tensors([p.grad for p in group]))\n            norm_groups.append(self._compute_grad_norm(grads_groups_flat[i]))\n            if norm_groups[i] == -1: #TODO: early break\n                skip = True\n\n        if skip:\n            self._update_scale(skip)\n            return\n\n        # norm is in fact norm*cur_scale\n        self.optimizer.step(grads=[[g] for g in grads_groups_flat],\n                            output_params=[[p] for p in self.fp16_groups_flat],\n                            scale=self.cur_scale,\n                            grad_norms=norm_groups)\n\n        # TODO: we probably don\'t need this? just to be safe\n        for i in range(len(norm_groups)):\n            updated_params = _unflatten_dense_tensors(self.fp16_groups_flat[i], self.fp16_groups[i])\n            for p,q in zip(self.fp16_groups[i], updated_params):\n                p.data = q.data\n\n        self._update_scale(False)\n        return\n\n    def backward(self, loss):\n        """"""\n        :attr:`backward` performs the following steps:\n\n        1. fp32_loss = loss.float()\n        2. scaled_loss = fp32_loss*loss_scale\n        3. scaled_loss.backward(), which accumulates scaled gradients into the ``.grad`` attributes of the model\'s fp16 leaves\n        """"""\n        scaled_loss = (loss.float()) * self.cur_scale\n        scaled_loss.backward()\n\n    def _update_scale(self, skip):\n        if self.dynamic_loss_scale:\n            if skip:\n                if self.verbose:\n                    print(""\\nGrad overflow on iteration"", self.cur_iter)\n                    print(""Using dynamic loss scale of"", self.cur_scale)\n                self.cur_scale = max(self.cur_scale/self.scale_factor, 1)\n                self.last_overflow_iter = self.cur_iter\n            else:\n                if (self.cur_iter - self.last_overflow_iter) % self.scale_window == 0:\n                    self.cur_scale *= self.scale_factor\n        else:\n            if skip:\n                print(""\\nGrad overflow on iteration"", self.cur_iter)\n                print(""Using static loss scale of"", self.cur_scale)\n        self.cur_iter +=1\n        return\n\n    # Promote state so it can be retrieved or set via ""fp16_optimizer_instance.state""\n    def _get_state(self):\n        return self.optimizer.state\n\n    def _set_state(self, value):\n        self.optimizer.state = value\n\n    state = property(_get_state, _set_state)\n\n    # Promote param_groups so it can be retrieved or set via ""fp16_optimizer_instance.param_groups""\n    # (for example, to adjust the learning rate)\n    def _get_param_groups(self):\n        return self.optimizer.param_groups\n\n    def _set_param_groups(self, value):\n        self.optimizer.param_groups = value\n\n    param_groups = property(_get_param_groups, _set_param_groups)\n\n    def state_dict(self):\n        """"""\n        Returns a dict containing the current state of this :class:`FP16_Optimizer` instance.\n        This dict contains attributes of :class:`FP16_Optimizer`, as well as the state_dict\n        of the contained Pytorch optimizer.\n        Example::\n            checkpoint = {}\n            checkpoint[\'model\'] = model.state_dict()\n            checkpoint[\'optimizer\'] = optimizer.state_dict()\n            torch.save(checkpoint, ""saved.pth"")\n        """"""\n        state_dict = {}\n        state_dict[\'dynamic_loss_scale\'] = self.dynamic_loss_scale\n        state_dict[\'cur_scale\'] = self.cur_scale\n        state_dict[\'cur_iter\'] = self.cur_iter\n        if state_dict[\'dynamic_loss_scale\']:\n            state_dict[\'last_overflow_iter\'] = self.last_overflow_iter\n            state_dict[\'scale_factor\'] = self.scale_factor\n            state_dict[\'scale_window\'] = self.scale_window\n        state_dict[\'optimizer_state_dict\'] = self.optimizer.state_dict()\n        state_dict[\'fp32_groups_flat\'] = self.fp32_groups_flat\n        return state_dict\n\n    def load_state_dict(self, state_dict):\n        """"""\n        Loads a state_dict created by an earlier call to state_dict().\n        If ``fp16_optimizer_instance`` was constructed from some ``init_optimizer``,\n        whose parameters in turn came from ``model``, it is expected that the user\n        will call ``model.load_state_dict()`` before\n        ``fp16_optimizer_instance.load_state_dict()`` is called.\n        Example::\n            model = torch.nn.Linear(D_in, D_out).cuda().half()\n            optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n            optimizer = FP16_Optimizer(optimizer, static_loss_scale = 128.0)\n            ...\n            checkpoint = torch.load(""saved.pth"")\n            model.load_state_dict(checkpoint[\'model\'])\n            optimizer.load_state_dict(checkpoint[\'optimizer\'])\n        """"""\n        # I think it should actually be ok to reload the optimizer before the model.\n        self.dynamic_loss_scale = state_dict[\'dynamic_loss_scale\']\n        self.cur_scale = state_dict[\'cur_scale\']\n        self.cur_iter = state_dict[\'cur_iter\']\n        if state_dict[\'dynamic_loss_scale\']:\n            self.last_overflow_iter = state_dict[\'last_overflow_iter\']\n            self.scale_factor = state_dict[\'scale_factor\']\n            self.scale_window = state_dict[\'scale_window\']\n        self.optimizer.load_state_dict(state_dict[\'optimizer_state_dict\'])\n        # At this point, the optimizer\'s references to the model\'s fp32 parameters are up to date.\n        # The optimizer\'s hyperparameters and internal buffers are also up to date.\n        # However, the fp32 master copies of the model\'s fp16 params stored by the optimizer are still\n        # out of date.  There are two options.\n        # 1:  Refresh the master params from the model\'s fp16 params.\n        # This requires less storage but incurs precision loss.\n        # 2:  Save and restore the fp32 master copies separately.\n        # We choose option 2.\n        #\n        # Pytorch Optimizer.load_state_dict casts saved buffers (e.g. momentum) to the type and device\n        # of their associated parameters, because it\'s possible those buffers might not exist yet in\n        # the current optimizer instance.  In our case, as long as the current FP16_Optimizer has been\n        # constructed in the same way as the one whose state_dict we are loading, the same master params\n        # are guaranteed to exist, so we can just copy_() from the saved master params.\n        for current, saved in zip(self.fp32_groups_flat, state_dict[\'fp32_groups_flat\']):\n            current.data.copy_(saved.data)\n'"
apex/apex/optimizers/fused_adam.py,6,"b'import types\nimport torch\nimport importlib\n\nclass FusedAdam(torch.optim.Optimizer):\n\n    """"""Implements Adam algorithm. Currently GPU-only.  Requires Apex to be installed via\n    ``python setup.py install --cuda_ext --cpp_ext``.\n\n    It has been proposed in `Adam: A Method for Stochastic Optimization`_.\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups.\n        lr (float, optional): learning rate. (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square. (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability. (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n            algorithm from the paper `On the Convergence of Adam and Beyond`_\n            (default: False) NOT SUPPORTED in FusedAdam!\n        eps_inside_sqrt (boolean, optional): in the \'update parameters\' step,\n            adds eps to the bias-corrected second moment estimate before\n            evaluating square root instead of adding it to the square root of\n            second moment estimate as in the original paper. (default: False)\n\n    .. _Adam\\: A Method for Stochastic Optimization:\n        https://arxiv.org/abs/1412.6980\n    .. _On the Convergence of Adam and Beyond:\n        https://openreview.net/forum?id=ryQu7f-RZ\n    """"""\n\n    def __init__(self, params,\n                 lr=1e-3, bias_correction = True,\n                 betas=(0.9, 0.999), eps=1e-8, eps_inside_sqrt = False,\n                 weight_decay=0., max_grad_norm=0., amsgrad=False):\n        global fused_adam_cuda\n        fused_adam_cuda = importlib.import_module(""fused_adam_cuda"")\n\n        if amsgrad:\n            raise RuntimeError(\'FusedAdam does not support the AMSGrad variant.\')\n        defaults = dict(lr=lr, bias_correction=bias_correction,\n                        betas=betas, eps=eps, weight_decay=weight_decay,\n                        max_grad_norm=max_grad_norm)\n        super(FusedAdam, self).__init__(params, defaults)\n        self.eps_mode = 0 if  eps_inside_sqrt else 1\n\n    def step(self, closure=None, grads=None, output_params=None, scale=1., grad_norms=None):\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n            grads (list of tensors, optional): weight gradient to use for the\n                optimizer update. If gradients have type torch.half, parameters\n                are expected to be in type torch.float. (default: None)\n            output params (list of tensors, optional): A reduced precision copy\n                of the updated weights written out in addition to the regular\n                updated weights. Have to be of same type as gradients. (default: None)\n            scale (float, optional): factor to divide gradient tensor values\n                by before applying to weights. (default: 1)\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        if grads is None:\n            grads_group = [None]*len(self.param_groups)\n        # backward compatibility\n        # assuming a list/generator of parameter means single group\n        elif isinstance(grads, types.GeneratorType):\n            grads_group = [grads]\n        elif type(grads[0])!=list:\n            grads_group = [grads]\n        else:\n            grads_group = grads\n\n        if output_params is None:\n            output_params_group = [None]*len(self.param_groups)\n        elif isinstance(output_params, types.GeneratorType):\n            output_params_group = [output_params]\n        elif type(output_params[0])!=list:\n            output_params_group = [output_params]\n        else:\n            output_params_group = output_params\n\n        if grad_norms is None:\n            grad_norms = [None]*len(self.param_groups)\n\n        for group, grads_this_group, output_params_this_group, grad_norm in zip(self.param_groups, grads_group, output_params_group, grad_norms):\n            if grads_this_group is None:\n               grads_this_group = [None]*len(group[\'params\'])\n            if output_params_this_group is None:\n               output_params_this_group = [None]*len(group[\'params\'])\n\n            # compute combined scale factor for this group\n            combined_scale = scale\n            if group[\'max_grad_norm\'] > 0:\n                # norm is in fact norm*scale\n                clip = ((grad_norm / scale) + 1e-6) / group[\'max_grad_norm\']\n                if clip > 1:\n                    combined_scale = clip * scale\n\n            bias_correction = 1 if group[\'bias_correction\'] else 0\n\n            for p, grad, output_param in zip(group[\'params\'], grads_this_group, output_params_this_group):\n                #note: p.grad should not ever be set for correct operation of mixed precision optimizer that sometimes sends None gradients\n                if p.grad is None and grad is None:\n                    continue\n                if grad is None:\n                    grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\'FusedAdam does not support sparse gradients, please consider SparseAdam instead\')\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'exp_avg\'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                state[\'step\'] += 1\n\n                out_p = torch.tensor([], dtype = torch.float) if output_param is None else output_param\n                fused_adam_cuda.adam(p.data,\n                                     out_p,\n                                     exp_avg,\n                                     exp_avg_sq,\n                                     grad,\n                                     group[\'lr\'],\n                                     beta1,\n                                     beta2,\n                                     group[\'eps\'],\n                                     combined_scale,\n                                     state[\'step\'],\n                                     self.eps_mode,\n                                     bias_correction,\n                                     group[\'weight_decay\'])\n        return loss\n'"
apex/apex/parallel/LARC.py,8,"b'import torch\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom torch.nn.parameter import Parameter\n\nclass LARC(object):\n    """"""\n    :class:`LARC` is a pytorch implementation of both the scaling and clipping variants of LARC,\n    in which the ratio between gradient and parameter magnitudes is used to calculate an adaptive \n    local learning rate for each individual parameter. The algorithm is designed to improve\n    convergence of large batch training.\n     \n    See https://arxiv.org/abs/1708.03888 for calculation of the local learning rate.\n\n    In practice it modifies the gradients of parameters as a proxy for modifying the learning rate\n    of the parameters. This design allows it to be used as a wrapper around any torch.optim Optimizer.\n\n    ```\n    model = ...\n    optim = torch.optim.Adam(model.parameters(), lr=...)\n    optim = LARC(optim)\n    ```\n\n    It can even be used in conjunction with apex.fp16_utils.FP16_optimizer.\n\n    ```\n    model = ...\n    optim = torch.optim.Adam(model.parameters(), lr=...)\n    optim = LARC(optim)\n    optim = apex.fp16_utils.FP16_Optimizer(optim)\n    ```\n\n    Args:\n        optimizer: Pytorch optimizer to wrap and modify learning rate for.\n        trust_coefficient: Trust coefficient for calculating the lr. See https://arxiv.org/abs/1708.03888\n        clip: Decides between clipping or scaling mode of LARC. If `clip=True` the learning rate is set to `min(optimizer_lr, local_lr)` for each parameter. If `clip=False` the learning rate is set to `local_lr*optimizer_lr`.\n        eps: epsilon kludge to help with numerical stability while calculating adaptive_lr\n    """"""\n\n    def __init__(self, optimizer, trust_coefficient=0.02, clip=True, eps=1e-8):\n        self.param_groups = optimizer.param_groups\n        self.optim = optimizer\n        self.trust_coefficient = trust_coefficient\n        self.eps = eps\n        self.clip = clip\n\n    def __getstate__(self):\n        return self.optim.__getstate__()\n\n    def __setstate__(self, state):\n        self.optim.__setstate__(state)\n\n    def __repr__(self):\n        return self.optim.__repr__()\n\n    def state_dict(self):\n        return self.optim.state_dict()\n\n    def load_state_dict(self, state_dict):\n        self.optim.load_state_dict(state_dict)\n\n    def zero_grad(self):\n        self.optim.zero_grad()\n\n    def add_param_group(self, param_group):\n        self.optim.add_param_group( param_group)\n\n    def step(self):\n        with torch.no_grad():\n            weight_decays = []\n            for group in self.optim.param_groups:\n                # absorb weight decay control from optimizer\n                weight_decay = group[\'weight_decay\'] if \'weight_decay\' in group else 0\n                weight_decays.append(weight_decay)\n                group[\'weight_decay\'] = 0\n                for p in group[\'params\']:\n                    if p.grad is None:\n                        continue\n                    param_norm = torch.norm(p.data)\n                    grad_norm = torch.norm(p.grad.data)\n\n                    if param_norm != 0 and grad_norm != 0:\n                        # calculate adaptive lr + weight decay\n                        adaptive_lr = self.trust_coefficient * (param_norm) / (grad_norm + param_norm * weight_decay + self.eps)\n\n                        # clip learning rate for LARC\n                        if self.clip:\n                            # calculation of adaptive_lr so that when multiplied by lr it equals `min(adaptive_lr, lr)`\n                            adaptive_lr = min(adaptive_lr/group[\'lr\'], 1)\n\n                        p.grad.data += weight_decay * p.data\n                        p.grad.data *= adaptive_lr\n\n        self.optim.step()\n        # return weight decay control to optimizer\n        for i, group in enumerate(self.optim.param_groups):\n            group[\'weight_decay\'] = weight_decays[i]\n'"
apex/apex/parallel/__init__.py,15,"b""import torch\n\nif hasattr(torch.distributed, 'ReduceOp'):\n    ReduceOp = torch.distributed.ReduceOp\nelif hasattr(torch.distributed, 'reduce_op'):\n    ReduceOp = torch.distributed.reduce_op\nelse:\n    ReduceOp = torch.distributed.deprecated.reduce_op\n\nfrom .distributed import DistributedDataParallel, Reducer\n# This is tricky because I'd like SyncBatchNorm to be exposed the same way\n# for both the cuda-enabled and python-fallback versions, and I don't want\n# to suppress the error information.\ntry:\n    import syncbn\n    from .optimized_sync_batchnorm import SyncBatchNorm\nexcept ImportError as err:\n    from .sync_batchnorm import SyncBatchNorm\n    SyncBatchNorm.syncbn_import_error = err\n\ndef convert_syncbn_model(module, process_group=None, channel_last=False):\n    '''\n    Recursively traverse module and its children to replace all instances of\n    ``torch.nn.modules.batchnorm._BatchNorm`` with :class:`apex.parallel.SyncBatchNorm`.\n\n    All ``torch.nn.BatchNorm*N*d`` wrap around\n    ``torch.nn.modules.batchnorm._BatchNorm``, so this function lets you easily switch\n    to use sync BN.\n\n    Args:\n        module (torch.nn.Module): input module\n\n    Example::\n\n        >>> # model is an instance of torch.nn.Module\n        >>> import apex\n        >>> sync_bn_model = apex.parallel.convert_syncbn_model(model)\n    '''\n    mod = module\n    if isinstance(module, torch.nn.modules.batchnorm._BatchNorm):\n        mod = SyncBatchNorm(module.num_features, module.eps, module.momentum, module.affine, module.track_running_stats, process_group, channel_last=channel_last)\n        mod.running_mean = module.running_mean\n        mod.running_var = module.running_var\n        if module.affine:\n            mod.weight.data = module.weight.data.clone().detach()\n            mod.bias.data = module.bias.data.clone().detach()\n    for name, child in module.named_children():\n        mod.add_module(name, convert_syncbn_model(child,\n                                                  process_group=process_group,\n                                                  channel_last=channel_last))\n    # TODO(jie) should I delete model explicitly?\n    del module\n    return mod\n\ndef create_syncbn_process_group(group_size):\n    '''\n    Creates process groups to be used for syncbn of a give ``group_size`` and returns\n    process group that current GPU participates in.\n\n    ``group_size`` must divide the total number of GPUs (world_size).\n\n    ``group_size`` of 0 would be considered as =world_size. In this case ``None`` will be returned.\n\n    ``group_size`` of 1 would be equivalent to using non-sync bn, but will still carry the overhead.\n\n    Args:\n        group_size (int): number of GPU's to collaborate for sync bn\n\n    Example::\n\n        >>> # model is an instance of torch.nn.Module\n        >>> import apex\n        >>> group = apex.parallel.create_syncbn_process_group(group_size)\n    '''\n\n    if group_size==0:\n        return None\n\n    world_size = torch.distributed.get_world_size()\n    assert(world_size >= group_size)\n    assert(world_size % group_size == 0)\n\n    group=None\n    for group_num in (range(world_size//group_size)):\n        group_ids = range(group_num*group_size, (group_num+1)*group_size)\n        cur_group = torch.distributed.new_group(ranks=group_ids)\n        if (torch.distributed.get_rank()//group_size == group_num):\n            group = cur_group\n            #can not drop out and return here, every process must go through creation of all subgroups\n\n    assert(group is not None)\n    return group\n"""
apex/apex/parallel/distributed.py,25,"b'import torch\nimport torch.distributed as dist\nfrom torch.nn.modules import Module\nfrom torch.autograd import Variable\nfrom collections import OrderedDict\nfrom itertools import chain\nimport copy\nimport importlib\nfrom ..multi_tensor_apply import multi_tensor_applier\n\nimported_flatten_impl = False\n\ndef import_flatten_impl():\n    global flatten_impl, unflatten_impl, imported_flatten_impl\n    try:\n        import apex_C\n        flatten_impl = apex_C.flatten\n        unflatten_impl = apex_C.unflatten\n    except ImportError:\n        print(""Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten."")\n        flatten_impl = torch._utils._flatten_dense_tensors\n        unflatten_impl = torch._utils._unflatten_dense_tensors\n    imported_flatten_impl = True\n\ndef flatten(bucket):\n    if not imported_flatten_impl:\n        import_flatten_impl()\n    return flatten_impl(bucket)\n\ndef unflatten(coalesced, bucket):\n    if not imported_flatten_impl:\n        import_flatten_impl()\n    return unflatten_impl(coalesced, bucket)\n\n# apply_dist_call requires that tensors in \'bucket\' are all the same type.\ndef apply_flat_dist_call(bucket, call, extra_args=None):\n\n    coalesced = flatten(bucket)\n\n    if extra_args is not None:\n        call(coalesced, *extra_args)\n    else:\n        call(coalesced)\n\n    if call is dist.all_reduce:\n        coalesced /= dist.get_world_size()\n        \n    for buf, synced in zip(bucket, unflatten(coalesced, bucket)):\n        buf.copy_(synced)\n\ndef split_half_float_double(tensors):\n    dtypes = [""torch.cuda.HalfTensor"",  ""torch.cuda.FloatTensor"", ""torch.cuda.DoubleTensor""]\n    buckets = []\n    for i, dtype in enumerate(dtypes):\n        bucket = [t for t in tensors if t.type() == dtype]\n        if bucket:\n            buckets.append(bucket) \n    return buckets\n\ndef split_by_type(tensors):\n    buckets = OrderedDict()\n    for tensor in tensors:\n        tp = tensor.type()\n        if tp not in buckets:\n            buckets[tp] = []\n        buckets[tp].append(tensor)\n    return buckets\n\n# flat_dist_call organizes \'tensors\' by type.\ndef flat_dist_call(tensors, call, extra_args=None):\n    buckets = split_by_type(tensors)\n                    \n    for tp in buckets:\n        bucket = buckets[tp]\n        apply_flat_dist_call(bucket, call, extra_args)\n\n            \ndef extract_tensors(maybe_tensor, tensor_list):\n    if torch.is_tensor(maybe_tensor):\n        tensor_list.append(maybe_tensor)\n    else:\n        try:\n            for item in maybe_tensor:\n                extract_tensors(item, tensor_list)\n        except TypeError:\n            return\n\n        \nclass Reducer(object):\n    """"""\n    :class:`apex.parallel.Reducer` is a simple class that helps allreduce a module\'s parameters\n    across processes.  :class:`Reducer` is intended to give the user additional control:\n    Unlike :class:`DistributedDataParallel`, :class:`Reducer` will not automatically allreduce\n    parameters during ``backward()``.\n    Instead, :class:`Reducer` waits for the user to call ``<reducer_instance>.reduce()`` manually.\n    This enables, for example, delaying the allreduce to be carried out every \n    several iterations instead of every single iteration.\n\n    Like :class:`DistributedDataParallel`, :class:`Reducer` averages any tensors it allreduces \n    over the number of participating processes.\n\n    :class:`Reducer` is designed to work with the upstream launch utility script \n    ``torch.distributed.launch`` with ``--nproc_per_node <= number of gpus per node``.\n    When used with this launcher, :class:`Reducer` assumes 1:1 mapping of processes to GPUs.\n    It also assumes that your script calls ``torch.cuda.set_device(args.rank)`` before creating the model.\n\n    Args:\n        module_or_grads_list: Either a network definition (module) being run in multi-gpu/distributed mode, or an iterable of gradients to be reduced.  If a module is passed in, the Reducer constructor will sync the parameters across processes (broadcasting from rank 0) to make sure they\'re all initialized with the same values.  If a list of gradients (that came from some module) is passed in, the user is responsible for manually syncing that module\'s parameters at the beginning of training.\n    """"""\n    \n    def __init__(self, module_or_grads_list):\n        if isinstance(module_or_grads_list, Module):\n            self.module = module_or_grads_list\n            flat_dist_call([param.data for param in self.module.parameters()], dist.broadcast, (0,) )\n\n        else:\n            self.module = None\n            self.grads = []\n            extract_tensors(module_or_grads_list, self.grads)\n            \n    def reduce(self):\n        if self.module:\n            grads = [param.grad.data for param in self.module.parameters() if param.grad is not None]\n            flat_dist_call(grads, dist.all_reduce)\n        else:\n            flat_dist_call(self.grads, dist.all_reduce)\n            \n            \nclass DistributedDataParallel(Module):\n    """"""\n    :class:`apex.parallel.DistributedDataParallel` is a module wrapper that enables\n    easy multiprocess distributed data parallel training, similar to ``torch.nn.parallel.DistributedDataParallel``.  Parameters are broadcast across participating processes on initialization, and gradients are\n    allreduced and averaged over processes during ``backward()``.\n\n    :class:`DistributedDataParallel` is optimized for use with NCCL.  It achieves high performance by \n    overlapping communication with computation during ``backward()`` and bucketing smaller gradient\n    transfers to reduce the total number of transfers required.\n\n    :class:`DistributedDataParallel` is designed to work with the upstream launch utility script \n    ``torch.distributed.launch`` with ``--nproc_per_node <= number of gpus per node``.\n    When used with this launcher, :class:`DistributedDataParallel` assumes 1:1 mapping of processes to GPUs.\n    It also assumes that your script calls ``torch.cuda.set_device(args.rank)`` before creating the model.\n\n    https://github.com/NVIDIA/apex/tree/master/examples/simple/distributed shows detailed usage.\n    https://github.com/NVIDIA/apex/tree/master/examples/imagenet shows another example\n    that combines :class:`DistributedDataParallel` with mixed precision training.\n\n    Args:\n        module: Network definition to be run in multi-gpu/distributed mode.\n        message_size (int, default=1e7): Minimum number of elements in a communication bucket.\n        delay_allreduce (bool, default=False):  Delay all communication to the end of the backward pass.  This disables overlapping communication with computation.\n        allreduce_trigger_params (list, optional, default=None):  If supplied, should contain a list of parameters drawn from the model.  Allreduces will be kicked off whenever one of these parameters receives its gradient (as opposed to when a bucket of size message_size is full).  At the end of backward(), a cleanup allreduce to catch any remaining gradients will also be performed automatically.  If allreduce_trigger_params is supplied, the message_size argument will be ignored.\n        allreduce_always_fp32 (bool, default=False):  Convert any FP16 gradients to FP32 before allreducing.  This can improve stability for widely scaled-out runs.\n        gradient_average (bool, default=True):  Option to toggle whether or not DDP averages the allreduced gradients over processes.  For proper scaling, the default value of True is recommended.\n        gradient_predivide_factor (float, default=1.0):  Allows perfoming the average of gradients over processes partially before and partially after the allreduce.  Before allreduce:  ``grads.mul_(1.0/gradient_predivide_factor)``.  After allreduce:  ``grads.mul_(gradient_predivide_factor/world size)``.  This can reduce the stress on the dynamic range of FP16 allreduces for widely scaled-out runs.\n\n    .. warning::\n        If ``gradient_average=False``, the pre-allreduce division (``grads.mul_(1.0/gradient_predivide_factor)``) will still be applied, but the post-allreduce gradient averaging (``grads.mul_(gradient_predivide_factor/world size)``) will be omitted.\n\n    """"""\n\n    def __init__(self, \n                 module, \n                 message_size=10000000, \n                 delay_allreduce=False, \n                 shared_param=None,\n                 allreduce_trigger_params=None,\n                 retain_allreduce_buffers=False,\n                 allreduce_always_fp32=False,\n                 gradient_average=True,\n                 gradient_predivide_factor=1.0):\n        super(DistributedDataParallel, self).__init__()\n\n        # Backward/forward compatibility around \n        # https://github.com/pytorch/pytorch/commit/540ef9b1fc5506369a48491af8a285a686689b36 and\n        # https://github.com/pytorch/pytorch/commit/044d00516ccd6572c0d6ab6d54587155b02a3b86\n        if hasattr(dist, ""get_backend""):\n            self._backend = dist.get_backend()\n            if hasattr(dist, ""DistBackend""):\n                self.backend_enum_holder = dist.DistBackend\n            else:\n                self.backend_enum_holder = dist.Backend\n        else:\n            self._backend = dist._backend \n            self.backend_enum_holder = dist.dist_backend\n\n        self.warn_on_half = True if self._backend == self.backend_enum_holder.GLOO else False\n\n        if shared_param is not None:\n            raise ValueError(""shared_param is no longer supported as an option.  It was misleadingly named from the start.  It turns out overlapping communication with computation should work fine with shared parameters.  If you still wish to delay communication to the end of the backward pass, use delay_allreduce=True|False instead."") \n\n        self.world_size = float(dist.get_world_size())\n\n        self.retain_allreduce_buffers = retain_allreduce_buffers\n        self.allreduce_always_fp32 = allreduce_always_fp32\n        self.gradient_average = gradient_average\n        self.gradient_predivide_factor = gradient_predivide_factor\n\n        self.custom_allreduce_triggers = False\n        if allreduce_trigger_params is not None:\n            if delay_allreduce:\n                raise ValueError(""Setting allreduce_trigger_params is only valid if delay_allreduce=False."")  \n            self.custom_allreduce_triggers = True\n            self.allreduce_trigger_params = set([id(param) for param in allreduce_trigger_params])\n\n        self.delay_allreduce = delay_allreduce\n        self.message_size = message_size\n\n        self.reduction_stream = torch.cuda.Stream()\n        self.reduction_event = torch.cuda.Event(enable_timing=False, blocking=False) \n        \n        self.module = module\n\n        self._disable_allreduce = False\n        \n        if self._backend == self.backend_enum_holder.NCCL:\n            for param in self.module.parameters():\n                assert param.is_cuda, ""NCCL backend only supports model parameters to be on GPU.""\n\n        self.active_params = []\n\n        self.param_type_to_tmp_i = {""torch.cuda.HalfTensor"" : 0, \n                                    ""torch.cuda.FloatTensor"" : 1,\n                                    ""torch.cuda.DoubleTensor"" : 2}\n\n        if multi_tensor_applier.available:\n            # TODO:  I really need to centralize the C++ backed imports\n            import amp_C\n            self.multi_tensor_scale = amp_C.multi_tensor_scale\n            self._overflow_buf = torch.cuda.IntTensor([0])\n\n        self.create_hooks()\n\n        flat_dist_call([param.data for param in self.module.parameters()], dist.broadcast, (0,) )\n\n\n    def __setstate__(self, state):\n        super(DistributedDataParallel, self).__setstate__(state)\n        self.reduction_stream = torch.cuda.Stream()\n        self.reduction_event = torch.cuda.Event(enable_timing=False, blocking=False) \n\n\n    def __getstate__(self):\n        attrs = copy.copy(self.__dict__)\n        if self._backend != self.backend_enum_holder.NCCL:\n            del attrs[\'self.reduction_stream\']\n            del attrs[\'self.reduction_event\']\n            return attrs\n\n    def enable_allreduce(self):\n        self._disable_allreduce = False\n\n    def disable_allreduce(self):\n        self._disable_allreduce = True\n      \n    # Broadcast rank 0\'s bucket structure across all processes, and have all processes \n    # regenerate their bucket structures to match. \n    def sync_bucket_structure(self):\n        # Append leftover buckets\n        for tmp_bucket in self.tmp_buckets:\n            if len(tmp_bucket) > 0:\n                self.active_i_buckets.append(tmp_bucket)\n\n        self.num_buckets = len(self.active_i_buckets)\n        self.bucket_sizes = [len(bucket) for bucket in self.active_i_buckets]\n\n        info_tensor = torch.cuda.IntTensor([self.num_buckets] + \n                                           self.bucket_sizes + \n                                           list(chain(*self.active_i_buckets)))\n\n        dist.broadcast(info_tensor, 0)\n\n        info = [int(entry) for entry in info_tensor]\n\n        self.num_buckets = info[0]\n        self.bucket_sizes = info[1:self.num_buckets + 1] \n        self.buckets = [[None for _ in range(self.bucket_sizes[i])] \n                        for i in range(self.num_buckets)] \n        # Technically, active_i_buckets\' work is done.  But the information is still useful to\n        # keep around.  Therefore, refresh active_i_buckets based on rank 0 as well.\n        self.active_i_buckets = [[None for _ in range(self.bucket_sizes[i])] \n                                 for i in range(self.num_buckets)] \n        \n        flattened_buckets = info[self.num_buckets + 1:]\n        flat_i = 0\n        for bucket_idx in range(self.num_buckets): \n            for bucket_loc in range(self.bucket_sizes[bucket_idx]):\n                param_i = flattened_buckets[flat_i]\n                self.active_i_buckets[bucket_idx][bucket_loc] = param_i \n                self.param_id_to_bucket[id(self.active_params[param_i])] = (bucket_idx, bucket_loc)\n                flat_i += 1 \n        \n        \n    def create_hooks(self):\n        # Fallback hook that\'s only called at the end of backward.\n        # Used if you deliberately want to delay allreduces to the end, or to refresh the \n        # bucket structure that will be used to overlap communication with computation in later\n        # iterations.\n        def allreduce_params():\n            # Bucket record refresh\n            if not self.delay_allreduce:\n                if self.needs_refresh:\n                    self.sync_bucket_structure()\n\n                    self.needs_refresh = False\n\n            self.allreduce_fallback()\n\n\n        def overlapping_backward_epilogue():\n            self.reduction_stream.record_event(self.reduction_event)\n            torch.cuda.current_stream().wait_event(self.reduction_event)\n     \n            # Sanity checks that all the buckets were kicked off\n            if self.next_bucket != self.num_buckets:\n                raise RuntimeError(""In epilogue, next_bucket ({}) != num_buckets ({}).  "".format(\n                                   self.next_bucket, self.num_buckets),\n                                   ""This probably indicates some buckets were not allreduced."")\n\n            for actual, expected in zip(self.buckets_ready_size, self.bucket_sizes):\n                if actual != expected:\n                    raise RuntimeError(""Some param buckets were not allreduced."")\n           \n\n        self.grad_accs = []\n        for param in self.module.parameters():\n            if param.requires_grad:\n                def wrapper(param):\n                    param_tmp = param.expand_as(param)\n                    grad_acc = param_tmp.grad_fn.next_functions[0][0]\n\n                    def allreduce_hook(*unused):\n                        if not self._disable_allreduce:\n                            if self.delay_allreduce or self.needs_refresh:\n                                # TODO:  How do we want to handle multiple backward passes between\n                                # each forward, e.g., backward passes with retain_graph=True?\n                                # needs_refresh and callback_queued are both vulnerable states.\n                                if not self.delay_allreduce and self.needs_refresh:\n                                    # Use the backward pass to build the bucket structure on the fly.\n                                    active_i = self.param_id_to_active_i[id(param)]\n\n                                    # Float, half, and double tensors are grouped into buckets separately.\n                                    current_type = self.param_type_to_tmp_i[param.type()]\n  \n                                    self.tmp_buckets[current_type].append(active_i)                          \n\n                                    ship_tmp_bucket = False\n                                    if self.custom_allreduce_triggers:\n                                        if id(param) in self.allreduce_trigger_params:\n                                            ship_tmp_bucket = True\n                                    else:\n                                        self.tmp_numels[current_type] += param.numel()\n                                        if self.tmp_numels[current_type] >= self.message_size:\n                                            ship_tmp_bucket = True\n\n                                    # To consider:  If custom_allreduce_triggers are in use, ship all\n                                    # tmp_buckets, not just tmp_buckets[current_type].\n                                    if ship_tmp_bucket:\n                                        self.active_i_buckets.append(self.tmp_buckets[current_type])\n                                        self.tmp_buckets[current_type] = []\n                                        self.tmp_numels[current_type] = 0\n                                \n                                if not self.callback_queued:\n                                    Variable._execution_engine.queue_callback(allreduce_params)\n                                    self.callback_queued = True\n                            else:\n                                if not self.callback_queued:\n                                    Variable._execution_engine.queue_callback(overlapping_backward_epilogue)\n                                    self.callback_queued = True \n\n                                self.comm_ready_buckets(param)\n                        \n                    grad_acc.register_hook(allreduce_hook)\n                    self.grad_accs.append(grad_acc)\n\n                wrapper(param)\n\n    def allreduce_bucket(self, bucket):\n        tensor = flatten(bucket)\n\n        tensor_to_allreduce = tensor \n\n        if self.allreduce_always_fp32:\n            tensor_to_allreduce = tensor.float() \n\n        if self.gradient_predivide_factor != 1.0:\n            tensor_to_allreduce.mul_(1./self.gradient_predivide_factor)\n\n        dist.all_reduce(tensor_to_allreduce)\n\n        if self.gradient_average:\n            if self.gradient_predivide_factor != self.world_size:\n                tensor_to_allreduce.mul_(self.gradient_predivide_factor/self.world_size)\n\n        if self.allreduce_always_fp32 and tensor is not tensor_to_allreduce:\n            tensor.copy_(tensor_to_allreduce)\n \n        return tensor\n    \n\n    def allreduce_maybe_retain(self, bucket, bucket_idx=-1):\n        allreduced = self.allreduce_bucket(bucket)\n        if self.retain_allreduce_buffers:\n            if self.allreduce_buffers[bucket_idx] is not None:\n                raise RuntimeError(""The backward pass is attempting to replace an already-filled ""\n                                   ""allreduce buffer.  This is almost certainly an error."")\n            self.allreduce_buffers[bucket_idx] = allreduced\n        else:\n            if multi_tensor_applier.available:\n                multi_tensor_applier(\n                    self.multi_tensor_scale,\n                    self._overflow_buf,\n                    [unflatten(allreduced, bucket), bucket],\n                    1.0)\n            else:\n                for buf, synced in zip(bucket, unflatten(allreduced, bucket)):\n                    buf.copy_(synced)\n\n\n    def allreduce_fallback(self):\n        grads = [param.grad.data for param in self.module.parameters() if param.grad is not None]\n\n        split_buckets = split_half_float_double(grads)\n\n        # If retain_allreduce_buffers is True and delay_allreduce is False,\n        # this will only be done during the first backward pass, ignored by the \n        # training script, and overwritten in the next forward pass.  So it\'s harmless. \n        if self.retain_allreduce_buffers:\n            self.allreduce_buffers = [None for _ in range(len(split_buckets))]\n    \n        for i, bucket in enumerate(split_buckets):\n            allreduced = self.allreduce_maybe_retain(bucket, i)\n\n\n    def comm_ready_buckets(self, param):\n        # Need to do this in every hook for compatibility with Ruberry\'s streaming backward PR.\n        # self.reduction_stream.wait_stream(torch.cuda.current_stream())\n\n        bucket_idx, bucket_loc = self.param_id_to_bucket[id(param)]\n\n        if self.buckets[bucket_idx][bucket_loc] is not None:\n            raise RuntimeError(""The backward pass is attempting to replace an already-filled ""\n                               ""bucket slot.  This is almost certainly an error."")\n\n        self.buckets[bucket_idx][bucket_loc] = param.grad.data\n        self.buckets_ready_size[bucket_idx] += 1\n\n        if self.buckets_ready_size[bucket_idx] == self.bucket_sizes[bucket_idx]:\n            if bucket_idx == self.next_bucket:\n                torch.cuda.current_stream().record_event(self.reduction_event)\n                self.reduction_stream.wait_event(self.reduction_event)\n                with torch.cuda.stream(self.reduction_stream):\n                    self.allreduce_maybe_retain(self.buckets[bucket_idx], bucket_idx)\n\n                    self.next_bucket += 1\n\n                    # Reversing upstream\'s logic here, because we constructed our buckets based on\n                    # the order things were received during backward.\n                    if len(self.ready_buckets_not_reduced) > 0:\n                        sorted_todo = sorted(self.ready_buckets_not_reduced)\n                        for i in sorted_todo:\n                            # Nothing can be reduced now\n                            if i > self.next_bucket:\n                                break\n                            elif i == self.next_bucket:\n                                self.allreduce_maybe_retain(self.buckets[i], i)\n                                self.ready_buckets_not_reduced.remove(i)\n                                self.next_bucket += 1 \n                            else:\n                                raise ValueError(""i should always be >= next_bucket"")\n            else:\n                self.ready_buckets_not_reduced.add(bucket_idx)\n\n        \n    def forward(self, *inputs, **kwargs):\n        result = self.module(*inputs, **kwargs)\n       \n        if not self._disable_allreduce:\n            if not self.delay_allreduce:\n                param_list = [param for param in self.module.parameters() if param.requires_grad]\n\n                # Conditions under which to refresh self.record\n                # Forward has the authority to set needs_refresh to True, but only allreduce_params\n                # in backward has the authority to set needs_refresh to False.\n                # Parentheses are not necessary for correct order of operations, but make the intent clearer.\n                if ((not self.active_params) or \n                    (len(param_list) != len(self.active_params)) or\n                    any([param1 is not param2 for param1, param2 in zip(param_list, self.active_params)])):\n                    self.needs_refresh = True\n\n                if self.needs_refresh:\n                    self.active_i_buckets = []\n                    self.buckets = []\n                    self.tmp_buckets = [[], [], []] # [running half, float, double buckets]\n                    self.tmp_numels = [0, 0, 0]\n                    self.bucket_sizes = []\n                    self.param_id_to_active_i = {id(param) : i for i, param in enumerate(param_list)}  \n                    self.param_id_to_bucket = {}\n                else:\n                    self.buckets = [[None for _ in range(self.bucket_sizes[i])] \n                                   for i in range(self.num_buckets)] \n                    self.buckets_ready_size = [0 for i in range(self.num_buckets)]\n                    if(self.retain_allreduce_buffers):\n                        self.allreduce_buffers = [None for _ in range(self.num_buckets)]\n                    self.next_bucket = 0\n                    self.ready_buckets_not_reduced = set()\n            \n                self.active_params = param_list\n\n            self.callback_queued = False\n        \n        return result\n'"
apex/apex/parallel/multiproc.py,1,"b'import torch\nimport sys\nimport subprocess\n\ndef docstring_hack():\n    """"""\n    Multiproc file which will launch a set of processes locally for multi-gpu\n    usage: python -m apex.parallel.multiproc main.py ...\n    """"""\n    pass\n\nargslist = list(sys.argv)[1:]\nworld_size = torch.cuda.device_count()\n\nif \'--world-size\' in argslist:\n    world_size = int(argslist[argslist.index(\'--world-size\')+1])\nelse:\n    argslist.append(\'--world-size\')\n    argslist.append(str(world_size))\n\nworkers = []\n\nfor i in range(world_size):\n    if \'--rank\' in argslist:\n        argslist[argslist.index(\'--rank\')+1] = str(i)\n    else:\n        argslist.append(\'--rank\')\n        argslist.append(str(i))\n    stdout = None if i == 0 else open(""GPU_""+str(i)+"".log"", ""w"")\n    print(argslist)\n    p = subprocess.Popen([str(sys.executable)]+argslist, stdout=stdout)\n    workers.append(p)\n\nfor p in workers:\n    p.wait()\n'"
apex/apex/parallel/optimized_sync_batchnorm.py,8,"b'import torch\nfrom torch.nn.modules.batchnorm import _BatchNorm\nfrom torch.nn import functional as F\n\nimport syncbn\nfrom .optimized_sync_batchnorm_kernel import SyncBatchnormFunction\n\n\nclass SyncBatchNorm(_BatchNorm):\n    """"""\n    synchronized batch normalization module extented from `torch.nn.BatchNormNd`\n    with the added stats reduction across multiple processes.\n    :class:`apex.parallel.SyncBatchNorm` is designed to work with\n    `DistributedDataParallel`.\n\n    When running in training mode, the layer reduces stats across all processes\n    to increase the effective batchsize for normalization layer. This is useful\n    in applications where batch size is small on a given process that would\n    diminish converged accuracy of the model. The model uses collective\n    communication package from `torch.distributed`.\n\n    When running in evaluation mode, the layer falls back to\n    `torch.nn.functional.batch_norm`\n\n    Args:\n        num_features: :math:`C` from an expected input of size\n            :math:`(N, C, L)` or :math:`L` from input of size :math:`(N, L)`\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Can be set to ``None`` for cumulative moving average\n            (i.e. simple average). Default: 0.1\n        affine: a boolean value that when set to ``True``, this module has\n            learnable affine parameters. Default: ``True``\n        track_running_stats: a boolean value that when set to ``True``, this\n            module tracks the running mean and variance, and when set to ``False``,\n            this module does not track such statistics and always uses batch\n            statistics in both training and eval modes. Default: ``True``\n        process_group: pass in a process group within which the stats of the\n            mini-batch is being synchronized. ``None`` for using default process\n            group\n        channel_last: a boolean value that when set to ``True``, this module\n            take the last dimension of the input tensor to be the channel\n            dimension. Default: False\n\n    Examples::\n        >>> # channel first tensor\n        >>> sbn = apex.parallel.SyncBatchNorm(100).cuda()\n        >>> inp = torch.randn(10, 100, 14, 14).cuda()\n        >>> out = sbn(inp)\n        >>> inp = torch.randn(3, 100, 20).cuda()\n        >>> out = sbn(inp)\n        >>> # channel last tensor\n        >>> sbn = apex.parallel.SyncBatchNorm(100, channel_last=True).cuda()\n        >>> inp = torch.randn(10, 14, 14, 100).cuda()\n    """"""\n\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, track_running_stats=True, process_group=None, channel_last=False):\n        super(SyncBatchNorm, self).__init__(num_features, eps=eps, momentum=momentum, affine=affine, track_running_stats=track_running_stats)\n        self.process_group = process_group\n        self.channel_last = channel_last\n\n    def _specify_process_group(self, process_group):\n        self.process_group = process_group\n\n    def _specify_channel_last(self, channel_last):\n        self.channel_last = channel_last\n\n    def forward(self, input):\n        # if input.dim() == 2, we switch to channel_last for efficient memory accessing\n        channel_last = self.channel_last if input.dim() != 2 else True\n\n        if not self.training and self.track_running_stats and not channel_last:\n            # fall back to pytorch implementation for inference\n            return F.batch_norm(input, self.running_mean, self.running_var, self.weight, self.bias, False, 0.0, self.eps)\n        else:\n            exponential_average_factor = 0.0\n            if self.training and self.track_running_stats:\n                self.num_batches_tracked += 1\n                if self.momentum is None:\n                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n                else:\n                    exponential_average_factor = self.momentum\n            return SyncBatchnormFunction.apply(input, self.weight, self.bias, self.running_mean, self.running_var, self.eps, self.training or not self.track_running_stats, exponential_average_factor, self.process_group, channel_last)\n'"
apex/apex/parallel/optimized_sync_batchnorm_kernel.py,19,"b'import torch\nfrom torch.autograd.function import Function\n\nimport syncbn\nfrom apex.parallel import ReduceOp\n\nclass SyncBatchnormFunction(Function):\n\n    @staticmethod\n    def forward(ctx, input, weight, bias, running_mean, running_variance, eps, track_running_stats = True, momentum = 1.0, process_group = None, channel_last = False):\n        torch.cuda.nvtx.range_push(""sync_BN_fw"")\n        input = input.contiguous()\n        world_size = 0\n\n        mean = None\n        var_biased = None\n        inv_std = None\n        var = None\n        out = None\n        count = None\n        if track_running_stats:\n            if channel_last:\n                count = int(input.numel()/input.size(-1))\n                mean, var_biased = syncbn.welford_mean_var_c_last(input)\n            else:\n                count = int(input.numel()/input.size(1))\n                mean, var_biased = syncbn.welford_mean_var(input)\n\n            if torch.distributed.is_initialized():\n                if not process_group:\n                    process_group = torch.distributed.group.WORLD\n                world_size = torch.distributed.get_world_size(process_group)\n                mean_all = torch.empty(world_size, mean.size(0), dtype=mean.dtype, device=mean.device)\n                var_all = torch.empty(world_size, var_biased.size(0), dtype=var_biased.dtype, device=var_biased.device)\n                mean_l = [mean_all.narrow(0, i, 1) for i in range(world_size)]\n                var_l = [var_all.narrow(0, i, 1) for i in range(world_size)]\n                torch.distributed.all_gather(mean_l, mean, process_group)\n                torch.distributed.all_gather(var_l, var_biased, process_group)\n                mean, var, inv_std = syncbn.welford_parallel(mean_all, var_all, count, eps)\n                # TODO(Jie): should do fp32 math instead!\n            else:\n                inv_std = 1.0 / torch.sqrt(var_biased + eps)\n                var = var_biased * (count) / (count-1) \n\n            if count == 1 and world_size < 2:\n                raise ValueError(\'Expected more than 1 value per channel when training, got input size{}\'.format(input.size()))\n\n            r_m_inc = mean if running_mean.dtype != torch.float16 else mean.half()\n            r_v_inc = var if running_variance.dtype != torch.float16 else var.half()\n            running_mean.data = running_mean.data * (1-momentum) + momentum*r_m_inc\n            running_variance.data = running_variance.data * (1-momentum) + momentum*r_v_inc\n        else:\n            mean = running_mean.data\n            inv_std = 1.0 / torch.sqrt(running_variance.data + eps)\n\n        ctx.save_for_backward(input, weight, mean, inv_std)\n        ctx.process_group = process_group\n        ctx.channel_last = channel_last\n        ctx.world_size = world_size\n\n        if channel_last:\n            out = syncbn.batchnorm_forward_c_last(input, mean, inv_std, weight, bias)\n        else:\n            out = syncbn.batchnorm_forward(input, mean, inv_std, weight, bias)\n\n        torch.cuda.nvtx.range_pop()\n        return out\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        grad_output = grad_output.contiguous()\n        torch.cuda.nvtx.range_push(""sync_BN_bw"")\n        # mini batch mean & var are calculated by forward path.\n        # mu = 1./N*np.sum(h, axis = 0)\n        # var = 1./N*np.sum((h-mu)**2, axis = 0)\n        saved_input, weight, mean, inv_std = ctx.saved_tensors\n        process_group = ctx.process_group\n        channel_last = ctx.channel_last\n        world_size = ctx.world_size\n        grad_input = grad_weight = grad_bias = None\n\n        # TODO(jie): why do I have to clone here? life time of grad_output?\n        if channel_last:\n            mean_dy, mean_dy_xmu, grad_weight, grad_bias = syncbn.reduce_bn_c_last(grad_output, saved_input, mean, inv_std, weight)\n        else:\n            mean_dy, mean_dy_xmu, grad_weight, grad_bias = syncbn.reduce_bn(grad_output, saved_input, mean, inv_std, weight)\n\n        # calculate grad_input\n        if ctx.needs_input_grad[0]:\n\n            if torch.distributed.is_initialized():\n                torch.distributed.all_reduce(\n                    mean_dy, ReduceOp.SUM, process_group)\n                mean_dy = mean_dy / world_size\n                torch.distributed.all_reduce(\n                    mean_dy_xmu, ReduceOp.SUM, process_group)\n                mean_dy_xmu = mean_dy_xmu / world_size\n            if channel_last:\n                grad_input = syncbn.batchnorm_backward_c_last(grad_output, saved_input, mean, inv_std, weight, mean_dy, mean_dy_xmu)\n            else:\n                grad_input = syncbn.batchnorm_backward(grad_output, saved_input, mean, inv_std, weight, mean_dy, mean_dy_xmu)\n\n        if weight is None or not ctx.needs_input_grad[1]:\n            grad_weight = None\n\n        if weight is None or not ctx.needs_input_grad[2]:\n            grad_bias = None\n\n        torch.cuda.nvtx.range_pop()\n        return grad_input, grad_weight, grad_bias, None, None, None, None, None, None, None\n'"
apex/apex/parallel/sync_batchnorm.py,18,"b'import torch\nfrom torch.nn.modules.batchnorm import _BatchNorm\nfrom torch.nn import functional as F\n\nfrom .sync_batchnorm_kernel import SyncBatchnormFunction\nfrom apex.parallel import ReduceOp\n\n\nclass SyncBatchNorm(_BatchNorm):\n    """"""\n    synchronized batch normalization module extented from ``torch.nn.BatchNormNd``\n    with the added stats reduction across multiple processes.\n    :class:`apex.parallel.SyncBatchNorm` is designed to work with\n    ``DistributedDataParallel``.\n\n    When running in training mode, the layer reduces stats across all processes\n    to increase the effective batchsize for normalization layer. This is useful\n    in applications where batch size is small on a given process that would\n    diminish converged accuracy of the model. The model uses collective\n    communication package from ``torch.distributed``.\n\n    When running in evaluation mode, the layer falls back to\n    ``torch.nn.functional.batch_norm``.\n\n    Args:\n        num_features: :math:`C` from an expected input of size\n            :math:`(N, C, L)` or :math:`L` from input of size :math:`(N, L)`\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Can be set to ``None`` for cumulative moving average\n            (i.e. simple average). Default: 0.1\n        affine: a boolean value that when set to ``True``, this module has\n            learnable affine parameters. Default: ``True``\n        track_running_stats: a boolean value that when set to ``True``, this\n            module tracks the running mean and variance, and when set to ``False``,\n            this module does not track such statistics and always uses batch\n            statistics in both training and eval modes. Default: ``True``\n\n    Example::\n\n        >>> sbn = apex.parallel.SyncBatchNorm(100).cuda()\n        >>> inp = torch.randn(10, 100, 14, 14).cuda()\n        >>> out = sbn(inp)\n        >>> inp = torch.randn(3, 100, 20).cuda()\n        >>> out = sbn(inp)\n    """"""\n\n    warned = False\n\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, track_running_stats=True, process_group=None, channel_last=False):\n        if channel_last == True:\n            raise AttributeError(""channel_last is not supported by primitive SyncBatchNorm implementation. Try install apex with `--cuda_ext` if channel_last is desired."")\n\n        if not SyncBatchNorm.warned:\n            print(""Warning:  using Python fallback for SyncBatchNorm, possibly because apex was installed without --cuda_ext.  The exception raised when attempting to import the cuda backend was: "", self.syncbn_import_error)\n            SyncBatchNorm.warned = True\n\n        super(SyncBatchNorm, self).__init__(num_features, eps=eps, momentum=momentum, affine=affine, track_running_stats=track_running_stats)\n        self.process_group = process_group\n\n    def _specify_process_group(self, process_group):\n        self.process_group = process_group\n\n    def forward(self, input):\n        torch.cuda.nvtx.range_push(""sync_bn_fw_with_mean_var"")\n        mean = None\n        var = None\n        cast = None\n        out = None\n\n        # casting to handle mismatch input type to layer type\n        if self.running_mean is not None:\n            if self.running_mean.dtype != input.dtype:\n                input = input.to(self.running_mean.dtype)\n                cast = input.dtype\n        elif self.weight is not None:\n            if self.weight.dtype != input.dtype:\n                input = input.to(self.weight.dtype)\n                cast = input.dtype\n\n        if not self.training and self.track_running_stats:\n            # fall back to pytorch implementation for inference\n            torch.cuda.nvtx.range_pop()\n            out = F.batch_norm(input, self.running_mean, self.running_var, self.weight, self.bias, False, 0.0, self.eps)\n        else:\n            process_group = self.process_group\n            world_size = 1\n            if not self.process_group:\n                process_group = torch.distributed.group.WORLD\n            self.num_batches_tracked += 1\n            with torch.no_grad():\n                channel_first_input = input.transpose(0, 1).contiguous()\n                squashed_input_tensor_view = channel_first_input.view(\n                    channel_first_input.size(0), -1)\n                # total number of data points for each variance entry. Used to calculate unbiased variance estimate\n                m = None\n                local_m = float(squashed_input_tensor_view.size()[1])\n                local_mean = torch.mean(squashed_input_tensor_view, 1)\n                local_sqr_mean = torch.pow(\n                    squashed_input_tensor_view, 2).mean(1)\n                if torch.distributed.is_initialized():\n                    world_size = torch.distributed.get_world_size(process_group)\n                    torch.distributed.all_reduce(\n                        local_mean, ReduceOp.SUM, process_group)\n                    mean = local_mean / world_size\n                    torch.distributed.all_reduce(\n                        local_sqr_mean, ReduceOp.SUM, process_group)\n                    sqr_mean = local_sqr_mean / world_size\n                    m = local_m * world_size\n                else:\n                    m = local_m\n                    mean = local_mean\n                    sqr_mean = local_sqr_mean\n                # var(x) = E (( x - mean_x ) ** 2)\n                #        = 1 / N * sum ( x - mean_x ) ** 2\n                #        = 1 / N * sum (x**2) - mean_x**2\n                var = sqr_mean - mean.pow(2)\n\n                if self.running_mean is not None:\n                    self.running_mean = self.momentum * mean + \\\n                        (1 - self.momentum) * self.running_mean\n                if self.running_var is not None:\n                    # as noted by the paper, we used unbiased variance estimate of the mini-batch\n                    # Var[x] = m / (m-1) * Eb (sample_variance)\n                    self.running_var = m / \\\n                        (m-1) * self.momentum * var + \\\n                        (1 - self.momentum) * self.running_var\n            torch.cuda.nvtx.range_pop()\n            out = SyncBatchnormFunction.apply(input, self.weight, self.bias, mean, var, self.eps, process_group, world_size)\n        out = out.to(cast)\n'"
apex/apex/parallel/sync_batchnorm_kernel.py,13,"b'import torch\nfrom torch.autograd.function import Function\n\nfrom apex.parallel import ReduceOp\n\n\nclass SyncBatchnormFunction(Function):\n\n    @staticmethod\n    def forward(ctx, input, weight, bias, running_mean, running_variance, eps, process_group, world_size):\n        torch.cuda.nvtx.range_push(""sync_BN_fw"")\n        # transpose it to channel last to support broadcasting for input with different rank\n        c_last_input = input.transpose(1, -1).contiguous().clone()\n\n        ctx.save_for_backward(c_last_input, weight, bias,\n                              running_mean, running_variance)\n        ctx.eps = eps\n        ctx.process_group = process_group\n        ctx.world_size = world_size\n\n        c_last_input = (c_last_input - running_mean) / \\\n            torch.sqrt(running_variance + eps)\n\n        if weight is not None:\n            c_last_input = c_last_input * weight\n        if bias is not None:\n            c_last_input = c_last_input + bias\n\n        torch.cuda.nvtx.range_pop()\n        return c_last_input.transpose(1, -1).contiguous().clone()\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        torch.cuda.nvtx.range_push(""sync_BN_bw"")\n        # mini batch mean & var are calculated by forward path.\n        # mu = 1./N*np.sum(h, axis = 0)\n        # var = 1./N*np.sum((h-mu)**2, axis = 0)\n        c_last_input, weight, bias, running_mean, running_variance = ctx.saved_tensors\n\n        eps = ctx.eps\n        process_group = ctx.process_group\n        world_size = ctx.world_size\n        grad_input = grad_weight = grad_bias = None\n        num_features = running_mean.size()[0]\n\n        # transpose it to channel last to support broadcasting for input with different rank\n        torch.cuda.nvtx.range_push(""carilli field"")\n        c_last_grad = grad_output.transpose(1, -1).contiguous()\n        # squash non-channel dimension so we can easily calculate mean\n        c_grad = c_last_grad.view(-1, num_features).contiguous()\n        torch.cuda.nvtx.range_pop()\n\n        # calculate grad_input\n        if ctx.needs_input_grad[0]:\n            # dh = gamma * (var + eps)**(-1. / 2.) * (dy - np.mean(dy, axis=0)\n            #     - (h - mu) * (var + eps)**(-1.0) * np.mean(dy * (h - mu), axis=0))\n            mean_dy = c_grad.mean(0)\n            mean_dy_xmu = (c_last_grad * (c_last_input -\n                                          running_mean)).view(-1, num_features).mean(0)\n            if torch.distributed.is_initialized():\n                torch.distributed.all_reduce(\n                    mean_dy, ReduceOp.SUM, process_group)\n                mean_dy = mean_dy / world_size\n                torch.distributed.all_reduce(\n                    mean_dy_xmu, ReduceOp.SUM, process_group)\n                mean_dy_xmu = mean_dy_xmu / world_size\n            c_last_grad_input = (c_last_grad - mean_dy - (c_last_input - running_mean) / (\n                running_variance + eps) * mean_dy_xmu) / torch.sqrt(running_variance + eps)\n            if weight is not None:\n                c_last_grad_input.mul_(weight)\n            grad_input = c_last_grad_input.transpose(1, -1).contiguous()\n\n        # calculate grad_weight\n        grad_weight = None\n        if weight is not None and ctx.needs_input_grad[1]:\n            # dgamma = np.sum((h - mu) * (var + eps)**(-1. / 2.) * dy, axis=0)\n            grad_weight = ((c_last_input - running_mean) / torch.sqrt(\n                running_variance + eps) * c_last_grad).view(-1, num_features).sum(0)\n\n        # calculate grad_bias\n        grad_bias = None\n        if bias is not None and ctx.needs_input_grad[2]:\n            # dbeta = np.sum(dy, axis=0)\n            grad_bias = c_grad.sum(0)\n\n        torch.cuda.nvtx.range_pop()\n        return grad_input, grad_weight, grad_bias, None, None, None, None, None\n'"
apex/apex/reparameterization/__init__.py,2,"b'from .weight_norm import WeightNorm\r\nfrom .reparameterization import Reparameterization\r\n\r\ndef apply_weight_norm(module, name=\'\', dim=0, hook_child=True):\r\n    """"""\r\n    Applies weight normalization to a parameter in the given module.\r\n    If no parameter is provided, applies weight normalization to all\r\n    parameters in model (except 1-d vectors and scalars).\r\n\r\n    .. math::\r\n         \\mathbf{w} = g \\dfrac{\\mathbf{v}}{\\|\\mathbf{v}\\|}\r\n\r\n    Weight normalization is a reparameterization that decouples the magnitude\r\n    of a weight tensor from its direction. This replaces the parameter specified\r\n    by `name` (e.g. ""weight"") with two parameters: one specifying the magnitude\r\n    (e.g. ""weight_g"") and one specifying the direction (e.g. ""weight_v"").\r\n    Weight normalization is implemented via a hook that recomputes the weight\r\n    tensor from the magnitude and direction before every :meth:`~Module.forward`\r\n    call.\r\n\r\n    By default, with `dim=0`, the norm is computed independently per output\r\n    channel/plane. To compute a norm over the entire weight tensor, use\r\n    `dim=None`.\r\n\r\n    See https://arxiv.org/abs/1602.07868\r\n\r\n    Args:\r\n        module (nn.Module): containing module\r\n        name (str, optional): name of weight parameter\r\n        dim (int, optional): dimension over which to compute the norm\r\n        hook_child (boolean, optional): adds reparameterization hook to direct parent of the \r\n            parameters. If False, it\'s added to `module` instead. Default: True\r\n\r\n    Returns:\r\n        The original module with the weight norm hook\r\n\r\n    Example::\r\n\r\n        >>> m = apply_weight_norm(nn.Linear(20, 40), name=\'weight\')\r\n        Linear (20 -> 40)\r\n        >>> m.weight_g.size()\r\n        torch.Size([40, 1])\r\n        >>> m.weight_v.size()\r\n        torch.Size([40, 20])\r\n\r\n    """"""\r\n    return apply_reparameterization(module, reparameterization=WeightNorm, hook_child=hook_child,\r\n                                    name=name, dim=dim)\r\n\r\ndef remove_weight_norm(module, name=\'\', remove_all=False):\r\n    """"""\r\n    Removes the weight normalization reparameterization of a parameter from a module.\r\n    If no parameter is supplied then all weight norm parameterizations are removed.\r\n    Args:\r\n        module (nn.Module): containing module\r\n        name (str, optional): name of weight parameter\r\n    Example:\r\n        >>> m = apply_weight_norm(nn.Linear(20, 40))\r\n        >>> remove_weight_norm(m)\r\n    """"""\r\n    return remove_reparameterization(module, reparameterization=WeightNorm,\r\n                                    name=name, remove_all=remove_all)\r\n\r\ndef apply_reparameterization(module, reparameterization=None, name=\'\', dim=0, hook_child=True):\r\n    """"""\r\n    Applies a given weight reparameterization (such as weight normalization) to\r\n    a parameter in the given module. If no parameter is given, applies the reparameterization\r\n    to all parameters in model (except 1-d vectors and scalars).\r\n\r\n    Args:\r\n        module (nn.Module): containing module\r\n        reparameterization (Reparameterization): reparamaterization class to apply\r\n        name (str, optional): name of weight parameter\r\n        dim (int, optional): dimension over which to perform reparameterization op\r\n        hook_child (boolean, optional): adds reparameterization hook to direct parent of the \r\n            parameters. If False, it\'s added to `module` instead. Default: True\r\n\r\n    Returns:\r\n        The original module with the reparameterization hook\r\n\r\n    Example::\r\n\r\n        >>> m = apply_reparameterization(nn.Linear(20, 40), WeightNorm)\r\n        Linear (20 -> 40)\r\n\r\n    """"""\r\n    assert reparameterization is not None\r\n    if name != \'\':\r\n        Reparameterization.apply(module, name, dim, reparameterization, hook_child)\r\n    else:\r\n        names = list(module.state_dict().keys())\r\n        for name in names:\r\n            apply_reparameterization(module, reparameterization, name, dim, hook_child)\r\n    return module\r\n\r\ndef remove_reparameterization(module, reparameterization=Reparameterization,\r\n                                name=\'\', remove_all=False):\r\n    """"""\r\n    Removes the given reparameterization of a parameter from a module.\r\n    If no parameter is supplied then all reparameterizations are removed.\r\n    Args:\r\n        module (nn.Module): containing module\r\n        reparameterization (Reparameterization): reparamaterization class to apply\r\n        name (str, optional): name of weight parameter\r\n        remove_all (bool, optional): if True, remove all reparamaterizations of given type. Default: False\r\n    Example:\r\n        >>> m = apply_reparameterization(nn.Linear(20, 40),WeightNorm)\r\n        >>> remove_reparameterization(m)\r\n    """"""\r\n    if name != \'\' or remove_all:\r\n        to_remove = []\r\n        for k, hook in module._forward_pre_hooks.items():\r\n            if isinstance(hook, reparameterization) and (hook.name == name or remove_all):\r\n                hook.remove(module)\r\n                to_remove.append(k)\r\n        if len(to_remove) > 0:\r\n            for k in to_remove:\r\n                del module._forward_pre_hooks[k]\r\n            return module\r\n        if not remove_all:\r\n            raise ValueError(""reparameterization of \'{}\' not found in {}""\r\n                             .format(name, module))\r\n    else:\r\n        modules = [module]+[x for x in module.modules()]\r\n        for m in modules:\r\n            remove_reparameterization(m, reparameterization=reparameterization, remove_all=True)\r\n        return module\r\n'"
apex/apex/reparameterization/reparameterization.py,3,"b'import torch\r\nfrom torch.nn.parameter import Parameter\r\nimport sys\r\nclass Reparameterization(object):\r\n    """"""\r\n    Class interface for performing weight reparameterizations\r\n    Arguments:\r\n        name (str): name of weight parameter\r\n        dim (int): dimension over which to compute the norm\r\n        module (nn.Module): parent module to which param `name` is registered to\r\n        retain_forward (bool, optional): if False deletes weight on call to \r\n            module.backward. Used to avoid memory leaks with DataParallel Default: True\r\n    Attributes:\r\n        reparameterization_names (list, str): contains names of all parameters \r\n            needed to compute reparameterization.\r\n        backward_hook_key (int): torch.utils.hooks.RemovableHandle.id for hook used in module backward pass.\r\n    """"""\r\n\r\n    def __init__(self, name, dim, module, retain_forward=True):\r\n        self.name = name\r\n        self.dim = dim\r\n        self.evaluated = False\r\n        self.retain_forward = retain_forward\r\n        self.reparameterization_names = []\r\n        self.backward_hook_key = None\r\n        self.module = module\r\n\r\n    def compute_weight(self, module=None, name=None):\r\n        """"""\r\n        Computes reparameterized weight value to assign value to module attribute\r\n        with name `name`.\r\n        See WeightNorm class for example.\r\n        Arguments:\r\n            module (nn.Module): module with weight we\'d like to reparameterize\r\n        Returns:\r\n            w (Tensor): Tensor object containing value of reparameterized weight\r\n        """"""\r\n        raise NotImplementedError\r\n\r\n    def reparameterize(self, name, weight, dim):\r\n        """"""\r\n        Creates Parameters to be used for reparameterization and creates names that\r\n        for attributes for the module these Parameters will correspond to.\r\n        The parameters will be registered according to the names provided.\r\n        See WeightNorm class for example.\r\n        Arguments:\r\n            module (nn.Module): module with weight we\'d like to reparameterize\r\n            name (str, optional): name of weight parameter\r\n            dim (int, optional): dimension over which to compute parameterization\r\n        Returns:\r\n            names (list, str): names of Parameters to be used for reparameterization\r\n            params (list, Parameter): Parameters to be used for reparameterization\r\n        """"""\r\n        raise NotImplementedError\r\n\r\n    @staticmethod\r\n    def apply(module, name, dim, reparameterization=None, hook_child=True):\r\n        """"""\r\n        Applies reparametrization to module\'s `name` parameter and modifies instance attributes as appropriate.\r\n        `hook_child` adds reparameterization hook to direct parent of the parameters. If False, it\'s added to `module` instead.\r\n        """"""\r\n        if reparameterization is None:\r\n            reparameterization = Reparameterization\r\n        module2use, name2use = Reparameterization.get_module_and_name(module, name)\r\n        # does not work on sparse\r\n        if name2use is None or isinstance(module2use, (torch.nn.Embedding, torch.nn.EmbeddingBag)):\r\n            return\r\n\r\n        if hook_child:\r\n            fn = reparameterization(name2use, dim, module2use)\r\n        else:\r\n            fn = reparameterization(name, dim, module)\r\n\r\n        weight = getattr(module2use, name2use)\r\n        if weight.dim() <= 1:\r\n            return\r\n\r\n        # remove weight from parameter list\r\n        del module2use._parameters[name2use]\r\n\r\n        # add parameters of reparameterization of parameter to module\r\n        names, params = fn.reparameterize(name2use, weight, dim)\r\n        for n, p in zip(names, params):\r\n            module2use.register_parameter(n, p)\r\n\r\n        # add parameters to reparameterization so they can be removed later\r\n        fn.reparameterization_names = names\r\n\r\n        setattr(module2use, name2use, None)\r\n\r\n        hook_module = module2use\r\n        if not hook_child:\r\n            hook_module = module\r\n        # recompute weight before every forward()\r\n        hook_module.register_forward_pre_hook(fn)\r\n\r\n        # remove weight during backward\r\n        handle = hook_module.register_backward_hook(fn.backward_hook)\r\n        # get hook key so we can delete it later\r\n        fn.backward_hook_key = handle.id\r\n\r\n        return fn\r\n\r\n    @staticmethod\r\n    def get_module_and_name(module, name):\r\n        """"""\r\n        recursively fetches (possible) child module and name of weight to be reparameterized\r\n        """"""\r\n        name2use = None\r\n        module2use = None\r\n        names = name.split(\'.\')\r\n        if len(names) == 1 and names[0] != \'\':\r\n            name2use = names[0]\r\n            module2use = module\r\n        elif len(names) > 1:\r\n            module2use = module\r\n            name2use = names[0]\r\n            for i in range(len(names)-1):\r\n                module2use = getattr(module2use, name2use)\r\n                name2use = names[i+1]\r\n        return module2use, name2use\r\n\r\n    def get_params(self, module):\r\n        """"""gets params of reparameterization based on known attribute names""""""\r\n        return [getattr(module, n) for n in self.reparameterization_names]\r\n\r\n    def remove(self, module):\r\n        """"""removes reparameterization and backward hook (does not remove forward hook)""""""\r\n        module2use, name2use = Reparameterization.get_module_and_name(module, self.name)\r\n        for p in self.get_params(module2use):\r\n            p.requires_grad = False\r\n        weight = self.compute_weight(module2use, name2use)\r\n        delattr(module2use, name2use)\r\n        for n in self.reparameterization_names:\r\n            del module2use._parameters[n]\r\n        module2use.register_parameter(name2use, Parameter(weight.data))\r\n        del module._backward_hooks[self.backward_hook_key]\r\n\r\n    def __call__(self, module, inputs):\r\n        """"""callable hook for forward pass""""""\r\n        module2use, name2use = Reparameterization.get_module_and_name(module, self.name)\r\n        _w = getattr(module2use, name2use)\r\n        if not self.evaluated or _w is None:\r\n            setattr(module2use, name2use, self.compute_weight(module2use, name2use))\r\n            self.evaluated = True\r\n\r\n    def backward_hook(self, module, grad_input, grad_output):\r\n        """"""callable hook for backward pass""""""\r\n        module2use, name2use = Reparameterization.get_module_and_name(module, self.name)\r\n        wn = getattr(module2use, name2use)\r\n        self.evaluated = False\r\n'"
apex/apex/reparameterization/weight_norm.py,2,"b'import torch\r\nfrom torch.nn.parameter import Parameter\r\nfrom ..fp16_utils import Fused_Weight_Norm\r\nimport time\r\n\r\nfrom .reparameterization import Reparameterization\r\n\r\ndef _norm(p, dim):\r\n    """"""Computes the norm over all dimensions except dim""""""\r\n    if dim is None:\r\n        return p.norm()\r\n    elif dim == 0:\r\n        output_size = (p.size(0),) + (1,) * (p.dim() - 1)\r\n        return p.contiguous().view(p.size(0), -1).norm(dim=1).view(*output_size)\r\n    elif dim == p.dim() - 1:\r\n        output_size = (1,) * (p.dim() - 1) + (p.size(-1),)\r\n        return p.contiguous().view(-1, p.size(-1)).norm(dim=0).view(*output_size)\r\n    return _norm(p.transpose(0, dim), 0).transpose(0, dim)\r\n\r\nHALF_TYPES = (torch.cuda.HalfTensor, torch.HalfTensor)\r\n\r\nclass WeightNorm(Reparameterization):\r\n    """"""\r\n    Weight normalization is a reparameterization that decouples the magnitude\r\n    of a weight tensor from its direction. This replaces the parameter specified\r\n    by `name` (e.g. ""weight"") with two parameters: one specifying the magnitude\r\n    (e.g. ""weight_g"") and one specifying the direction (e.g. ""weight_v"").\r\n    Weight normalization is implemented via a hook that recomputes the weight\r\n    tensor from the magnitude and direction before every :meth:`~Module.forward`\r\n    call.\r\n\r\n    .. math::\r\n         \\mathbf{w} = g \\dfrac{\\mathbf{v}}{\\|\\mathbf{v}\\|}\r\n\r\n    By default, with `dim=0`, the norm is computed independently per output\r\n    channel/plane. To compute a norm over the entire weight tensor, use\r\n    `dim=None`.\r\n    """"""\r\n    def compute_weight(self, module=None, name=None):\r\n        """"""\r\n        Computes weight normalized weight value to assign value to module attribute\r\n        with name `name`.\r\n        Arguments:\r\n            module (nn.Module): module with weight we\'d like to reparameterize\r\n        Returns:\r\n            w (Tensor): Tensor object containing value of reparameterized weight\r\n        """"""\r\n        if module is None:\r\n            module = self.module\r\n        if name is None:\r\n            name = self.name\r\n        module, name = Reparameterization.get_module_and_name(module, name)\r\n        g = getattr(module, name + \'_g\')\r\n        v = getattr(module, name + \'_v\')\r\n\r\n        fused_weight_norm = Fused_Weight_Norm.apply\r\n        v = v.contiguous()\r\n        w = fused_weight_norm(v, g, self.dim)\r\n\r\n        return w\r\n\r\n    def reparameterize(self, name, weight, dim):\r\n        """"""\r\n        Creates Parameters v and gto be used for weight normalization\r\n        and creates names that for attributes for the module these Parameters\r\n        will correspond to. The parameters will be registered according to the names\r\n        provided.\r\n        Arguments:\r\n            module (nn.Module): module with weight we\'d like to reparameterize\r\n            name (str, optional): name of weight parameter\r\n            dim (int, optional): dimension over which to compute parameterization\r\n        Returns:\r\n            names (list, str): names of Parameters to be used for reparameterization\r\n            params (list, Parameter): Parameters to be used for reparameterization\r\n        """"""\r\n        names = [name + \'_g\', name + \'_v\']\r\n        params = [Parameter(_norm(weight, dim).data), Parameter(weight.data)]\r\n        return names, params\r\n'"
apex/docs/source/conf.py,1,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# PyTorch documentation build configuration file, created by\n# sphinx-quickstart on Fri Dec 23 13:31:47 2016.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(\'.\'))\n# sys.path.insert(0, os.path.abspath(\'../../apex/parallel/\'))\nimport apex\n# import multiproc\nimport sphinx_rtd_theme\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.autosummary\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.todo\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.napoleon\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.extlinks\',\n]\n\nnapoleon_use_ivar = True\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = \'Apex\'\ncopyright = \'2018\'\nauthor = \'Christian Sarofeen, Natalia Gimelshein, Michael Carilli, Raul Puri\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\n# TODO: change to [:2] at v1.0\n# version = \'master (\' + torch.__version__ + \' )\'\nversion = \'0.1\'\n# The full version, including alpha/beta/rc tags.\n# TODO: verify this works as expected\nrelease = \'0.1.0\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path\nexclude_patterns = []\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\nhtml_theme_options = {\n    \'collapse_navigation\': False,\n    \'display_version\': True,\n    \'logo_only\': True,\n}\n\n# html_logo = \'_static/img/nv-pytorch2.png\'\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# html_style_path = \'css/pytorch_theme.css\'\nhtml_context = {\n    \'css_files\': [\n        \'https://fonts.googleapis.com/css?family=Lato\',\n        \'_static/css/pytorch_theme.css\'\n    ],\n}\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'PyTorchdoc\'\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'apex.tex\', \'Apex Documentation\',\n     \'Torch Contributors\', \'manual\'),\n]\n\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'Apex\', \'Apex Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'Apex\', \'Apex Documentation\',\n     author, \'Apex\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {\n    \'python\': (\'https://docs.python.org/\', None),\n    \'numpy\': (\'http://docs.scipy.org/doc/numpy/\', None),\n}\n\n# -- A patch that prevents Sphinx from cross-referencing ivar tags -------\n# See http://stackoverflow.com/a/41184353/3343043\n\nfrom docutils import nodes\nfrom sphinx.util.docfields import TypedField\nfrom sphinx import addnodes\n\n\ndef patched_make_field(self, types, domain, items, **kw):\n    # `kw` catches `env=None` needed for newer sphinx while maintaining\n    #  backwards compatibility when passed along further down!\n\n    # type: (List, unicode, Tuple) -> nodes.field\n    def handle_item(fieldarg, content):\n        par = nodes.paragraph()\n        par += addnodes.literal_strong(\'\', fieldarg)  # Patch: this line added\n        # par.extend(self.make_xrefs(self.rolename, domain, fieldarg,\n        #                           addnodes.literal_strong))\n        if fieldarg in types:\n            par += nodes.Text(\' (\')\n            # NOTE: using .pop() here to prevent a single type node to be\n            # inserted twice into the doctree, which leads to\n            # inconsistencies later when references are resolved\n            fieldtype = types.pop(fieldarg)\n            if len(fieldtype) == 1 and isinstance(fieldtype[0], nodes.Text):\n                typename = u\'\'.join(n.astext() for n in fieldtype)\n                typename = typename.replace(\'int\', \'python:int\')\n                typename = typename.replace(\'long\', \'python:long\')\n                typename = typename.replace(\'float\', \'python:float\')\n                typename = typename.replace(\'type\', \'python:type\')\n                par.extend(self.make_xrefs(self.typerolename, domain, typename,\n                                           addnodes.literal_emphasis, **kw))\n            else:\n                par += fieldtype\n            par += nodes.Text(\')\')\n        par += nodes.Text(\' -- \')\n        par += content\n        return par\n\n    fieldname = nodes.field_name(\'\', self.label)\n    if len(items) == 1 and self.can_collapse:\n        fieldarg, content = items[0]\n        bodynode = handle_item(fieldarg, content)\n    else:\n        bodynode = self.list_type()\n        for fieldarg, content in items:\n            bodynode += nodes.list_item(\'\', handle_item(fieldarg, content))\n    fieldbody = nodes.field_body(\'\', bodynode)\n    return nodes.field(\'\', fieldname, fieldbody)\n\nTypedField.make_field = patched_make_field\n'"
apex/examples/imagenet/main_amp.py,42,"b'import argparse\nimport os\nimport shutil\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.distributed as dist\nimport torch.optim\nimport torch.utils.data\nimport torch.utils.data.distributed\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport torchvision.models as models\n\nimport numpy as np\n\ntry:\n    from apex.parallel import DistributedDataParallel as DDP\n    from apex.fp16_utils import *\n    from apex import amp, optimizers\n    from apex.multi_tensor_apply import multi_tensor_applier\nexcept ImportError:\n    raise ImportError(""Please install apex from https://www.github.com/nvidia/apex to run this example."")\n\nmodel_names = sorted(name for name in models.__dict__\n                     if name.islower() and not name.startswith(""__"")\n                     and callable(models.__dict__[name]))\n\nparser = argparse.ArgumentParser(description=\'PyTorch ImageNet Training\')\nparser.add_argument(\'data\', metavar=\'DIR\',\n                    help=\'path to dataset\')\nparser.add_argument(\'--arch\', \'-a\', metavar=\'ARCH\', default=\'resnet18\',\n                    choices=model_names,\n                    help=\'model architecture: \' +\n                    \' | \'.join(model_names) +\n                    \' (default: resnet18)\')\nparser.add_argument(\'-j\', \'--workers\', default=4, type=int, metavar=\'N\',\n                    help=\'number of data loading workers (default: 4)\')\nparser.add_argument(\'--epochs\', default=90, type=int, metavar=\'N\',\n                    help=\'number of total epochs to run\')\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'-b\', \'--batch-size\', default=256, type=int,\n                    metavar=\'N\', help=\'mini-batch size per process (default: 256)\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=0.1, type=float,\n                    metavar=\'LR\', help=\'Initial learning rate.  Will be scaled by <global batch size>/256: args.lr = args.lr*float(args.batch_size*args.world_size)/256.  A warmup schedule will also be applied over the first 5 epochs.\')\nparser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\',\n                    help=\'momentum\')\nparser.add_argument(\'--weight-decay\', \'--wd\', default=1e-4, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 1e-4)\')\nparser.add_argument(\'--print-freq\', \'-p\', default=10, type=int,\n                    metavar=\'N\', help=\'print frequency (default: 10)\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'-e\', \'--evaluate\', dest=\'evaluate\', action=\'store_true\',\n                    help=\'evaluate model on validation set\')\nparser.add_argument(\'--pretrained\', dest=\'pretrained\', action=\'store_true\',\n                    help=\'use pre-trained model\')\n\nparser.add_argument(\'--prof\', dest=\'prof\', action=\'store_true\',\n                    help=\'Only run 10 iterations for profiling.\')\nparser.add_argument(\'--deterministic\', action=\'store_true\')\n\nparser.add_argument(""--local_rank"", default=0, type=int)\nparser.add_argument(\'--sync_bn\', action=\'store_true\',\n                    help=\'enabling apex sync BN.\')\n\nparser.add_argument(\'--opt-level\', type=str)\nparser.add_argument(\'--keep-batchnorm-fp32\', type=str, default=None)\nparser.add_argument(\'--loss-scale\', type=str, default=None)\n\ncudnn.benchmark = True\n\ndef fast_collate(batch):\n    imgs = [img[0] for img in batch]\n    targets = torch.tensor([target[1] for target in batch], dtype=torch.int64)\n    w = imgs[0].size[0]\n    h = imgs[0].size[1]\n    tensor = torch.zeros( (len(imgs), 3, h, w), dtype=torch.uint8 )\n    for i, img in enumerate(imgs):\n        nump_array = np.asarray(img, dtype=np.uint8)\n        if(nump_array.ndim < 3):\n            nump_array = np.expand_dims(nump_array, axis=-1)\n        nump_array = np.rollaxis(nump_array, 2)\n\n        tensor[i] += torch.from_numpy(nump_array)\n        \n    return tensor, targets\n\nbest_prec1 = 0\nargs = parser.parse_args()\n\nprint(""opt_level = {}"".format(args.opt_level))\nprint(""keep_batchnorm_fp32 = {}"".format(args.keep_batchnorm_fp32), type(args.keep_batchnorm_fp32))\nprint(""loss_scale = {}"".format(args.loss_scale), type(args.loss_scale))\n\nprint(""\\nCUDNN VERSION: {}\\n"".format(torch.backends.cudnn.version()))\n\nif args.deterministic:\n    cudnn.benchmark = False\n    cudnn.deterministic = True\n    torch.manual_seed(args.local_rank)\n    torch.set_printoptions(precision=10)\n\ndef main():\n    global best_prec1, args\n\n    args.distributed = False\n    if \'WORLD_SIZE\' in os.environ:\n        args.distributed = int(os.environ[\'WORLD_SIZE\']) > 1\n\n    args.gpu = 0\n    args.world_size = 1\n\n    if args.distributed:\n        args.gpu = args.local_rank\n        torch.cuda.set_device(args.gpu)\n        torch.distributed.init_process_group(backend=\'nccl\',\n                                             init_method=\'env://\')\n        args.world_size = torch.distributed.get_world_size()\n\n    assert torch.backends.cudnn.enabled, ""Amp requires cudnn backend to be enabled.""\n\n    # create model\n    if args.pretrained:\n        print(""=> using pre-trained model \'{}\'"".format(args.arch))\n        model = models.__dict__[args.arch](pretrained=True)\n    else:\n        print(""=> creating model \'{}\'"".format(args.arch))\n        model = models.__dict__[args.arch]()\n\n    if args.sync_bn:\n        import apex\n        print(""using apex synced BN"")\n        model = apex.parallel.convert_syncbn_model(model)\n\n    model = model.cuda()\n\n    # Scale learning rate based on global batch size\n    args.lr = args.lr*float(args.batch_size*args.world_size)/256. \n    optimizer = torch.optim.SGD(model.parameters(), args.lr,\n                                momentum=args.momentum,\n                                weight_decay=args.weight_decay)\n\n    # Initialize Amp.  Amp accepts either values or strings for the optional override arguments,\n    # for convenient interoperation with argparse.\n    model, optimizer = amp.initialize(model, optimizer,\n                                      opt_level=args.opt_level,\n                                      keep_batchnorm_fp32=args.keep_batchnorm_fp32,\n                                      loss_scale=args.loss_scale\n                                      )\n\n    # For distributed training, wrap the model with apex.parallel.DistributedDataParallel.\n    # This must be done AFTER the call to amp.initialize.  If model = DDP(model) is called\n    # before model, ... = amp.initialize(model, ...), the call to amp.initialize may alter\n    # the types of model\'s parameters in a way that disrupts or destroys DDP\'s allreduce hooks.\n    if args.distributed:\n        # By default, apex.parallel.DistributedDataParallel overlaps communication with \n        # computation in the backward pass.\n        # model = DDP(model)\n        # delay_allreduce delays all communication to the end of the backward pass.\n        model = DDP(model, delay_allreduce=True)\n\n    # define loss function (criterion) and optimizer\n    criterion = nn.CrossEntropyLoss().cuda()\n\n    # Optionally resume from a checkpoint\n    if args.resume:\n        # Use a local scope to avoid dangling references\n        def resume():\n            if os.path.isfile(args.resume):\n                print(""=> loading checkpoint \'{}\'"".format(args.resume))\n                checkpoint = torch.load(args.resume, map_location = lambda storage, loc: storage.cuda(args.gpu))\n                args.start_epoch = checkpoint[\'epoch\']\n                best_prec1 = checkpoint[\'best_prec1\']\n                model.load_state_dict(checkpoint[\'state_dict\'])\n                optimizer.load_state_dict(checkpoint[\'optimizer\'])\n                print(""=> loaded checkpoint \'{}\' (epoch {})""\n                      .format(args.resume, checkpoint[\'epoch\']))\n            else:\n                print(""=> no checkpoint found at \'{}\'"".format(args.resume))\n        resume()\n\n    # Data loading code\n    traindir = os.path.join(args.data, \'train\')\n    valdir = os.path.join(args.data, \'val\')\n\n    if(args.arch == ""inception_v3""):\n        raise RuntimeError(""Currently, inception_v3 is not supported by this example."")\n        # crop_size = 299\n        # val_size = 320 # I chose this value arbitrarily, we can adjust.\n    else:\n        crop_size = 224\n        val_size = 256\n\n    train_dataset = datasets.ImageFolder(\n        traindir,\n        transforms.Compose([\n            transforms.RandomResizedCrop(crop_size),\n            transforms.RandomHorizontalFlip(),\n            # transforms.ToTensor(), Too slow\n            # normalize,\n        ]))\n    val_dataset = datasets.ImageFolder(valdir, transforms.Compose([\n            transforms.Resize(val_size),\n            transforms.CenterCrop(crop_size),\n        ]))\n\n    train_sampler = None\n    val_sampler = None\n    if args.distributed:\n        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n        val_sampler = torch.utils.data.distributed.DistributedSampler(val_dataset)\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=args.batch_size, shuffle=(train_sampler is None),\n        num_workers=args.workers, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)\n\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset,\n        batch_size=args.batch_size, shuffle=False,\n        num_workers=args.workers, pin_memory=True,\n        sampler=val_sampler,\n        collate_fn=fast_collate)\n\n    if args.evaluate:\n        validate(val_loader, model, criterion)\n        return\n\n    for epoch in range(args.start_epoch, args.epochs):\n        if args.distributed:\n            train_sampler.set_epoch(epoch)\n\n        # train for one epoch\n        train(train_loader, model, criterion, optimizer, epoch)\n        if args.prof:\n            break\n        # evaluate on validation set\n        prec1 = validate(val_loader, model, criterion)\n\n        # remember best prec@1 and save checkpoint\n        if args.local_rank == 0:\n            is_best = prec1 > best_prec1\n            best_prec1 = max(prec1, best_prec1)\n            save_checkpoint({\n                \'epoch\': epoch + 1,\n                \'arch\': args.arch,\n                \'state_dict\': model.state_dict(),\n                \'best_prec1\': best_prec1,\n                \'optimizer\' : optimizer.state_dict(),\n            }, is_best)\n\nclass data_prefetcher():\n    def __init__(self, loader):\n        self.loader = iter(loader)\n        self.stream = torch.cuda.Stream()\n        self.mean = torch.tensor([0.485 * 255, 0.456 * 255, 0.406 * 255]).cuda().view(1,3,1,1)\n        self.std = torch.tensor([0.229 * 255, 0.224 * 255, 0.225 * 255]).cuda().view(1,3,1,1)\n        # With Amp, it isn\'t necessary to manually convert data to half.\n        # if args.fp16:\n        #     self.mean = self.mean.half()\n        #     self.std = self.std.half()\n        self.preload()\n\n    def preload(self):\n        try:\n            self.next_input, self.next_target = next(self.loader)\n        except StopIteration:\n            self.next_input = None\n            self.next_target = None\n            return\n        # if record_stream() doesn\'t work, another option is to make sure device inputs are created\n        # on the main stream.\n        # self.next_input_gpu = torch.empty_like(self.next_input, device=\'cuda\')\n        # self.next_target_gpu = torch.empty_like(self.next_target, device=\'cuda\')\n        # Need to make sure the memory allocated for next_* is not still in use by the main stream\n        # at the time we start copying to next_*:\n        # self.stream.wait_stream(torch.cuda.current_stream())\n        with torch.cuda.stream(self.stream):\n            self.next_input = self.next_input.cuda(non_blocking=True)\n            self.next_target = self.next_target.cuda(non_blocking=True)\n            # more code for the alternative if record_stream() doesn\'t work:\n            # copy_ will record the use of the pinned source tensor in this side stream.\n            # self.next_input_gpu.copy_(self.next_input, non_blocking=True)\n            # self.next_target_gpu.copy_(self.next_target, non_blocking=True)\n            # self.next_input = self.next_input_gpu\n            # self.next_target = self.next_target_gpu\n\n            # With Amp, it isn\'t necessary to manually convert data to half.\n            # if args.fp16:\n            #     self.next_input = self.next_input.half()\n            # else:\n            self.next_input = self.next_input.float()\n            self.next_input = self.next_input.sub_(self.mean).div_(self.std)\n            \n    def next(self):\n        torch.cuda.current_stream().wait_stream(self.stream)\n        input = self.next_input\n        target = self.next_target\n        input.record_stream(torch.cuda.current_stream())\n        target.record_stream(torch.cuda.current_stream())\n        self.preload()\n        return input, target\n\n\ndef train(train_loader, model, criterion, optimizer, epoch):\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    # switch to train mode\n    model.train()\n    end = time.time()\n\n    prefetcher = data_prefetcher(train_loader)\n    input, target = prefetcher.next()\n    i = 0\n    while input is not None:\n        i += 1\n\n        adjust_learning_rate(optimizer, epoch, i, len(train_loader))\n\n        if args.prof:\n            if i > 10:\n                break\n\n        # compute output\n        if args.prof: torch.cuda.nvtx.range_push(""forward"")\n        output = model(input)\n        if args.prof: torch.cuda.nvtx.range_pop()\n        loss = criterion(output, target)\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n\n        if args.prof: torch.cuda.nvtx.range_push(""backward"")\n        with amp.scale_loss(loss, optimizer) as scaled_loss:\n            scaled_loss.backward()\n        if args.prof: torch.cuda.nvtx.range_pop()\n\n        # for param in model.parameters():\n        #     print(param.data.double().sum().item(), param.grad.data.double().sum().item())\n\n        if args.prof: torch.cuda.nvtx.range_push(""step"")\n        optimizer.step()\n        if args.prof: torch.cuda.nvtx.range_pop()\n\n        if i%args.print_freq == 0:\n            # Every print_freq iterations, check the loss, accuracy, and speed.\n            # For best performance, it doesn\'t make sense to print these metrics every\n            # iteration, since they incur an allreduce and some host<->device syncs.\n\n            # Measure accuracy\n            prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n   \n            # Average loss and accuracy across processes for logging \n            if args.distributed:\n                reduced_loss = reduce_tensor(loss.data)\n                prec1 = reduce_tensor(prec1)\n                prec5 = reduce_tensor(prec5)\n            else:\n                reduced_loss = loss.data\n   \n            # to_python_float incurs a host<->device sync\n            losses.update(to_python_float(reduced_loss), input.size(0))\n            top1.update(to_python_float(prec1), input.size(0))\n            top5.update(to_python_float(prec5), input.size(0))\n    \n            torch.cuda.synchronize()\n            batch_time.update((time.time() - end)/args.print_freq)\n            end = time.time()\n\n            if args.local_rank == 0:\n                print(\'Epoch: [{0}][{1}/{2}]\\t\'\n                      \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                      \'Speed {3:.3f} ({4:.3f})\\t\'\n                      \'Loss {loss.val:.10f} ({loss.avg:.4f})\\t\'\n                      \'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t\'\n                      \'Prec@5 {top5.val:.3f} ({top5.avg:.3f})\'.format(\n                       epoch, i, len(train_loader),\n                       args.world_size*args.batch_size/batch_time.val,\n                       args.world_size*args.batch_size/batch_time.avg,\n                       batch_time=batch_time,\n                       loss=losses, top1=top1, top5=top5))\n\n        input, target = prefetcher.next()\n\n\ndef validate(val_loader, model, criterion):\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    # switch to evaluate mode\n    model.eval()\n\n    end = time.time()\n\n    prefetcher = data_prefetcher(val_loader)\n    input, target = prefetcher.next()\n    i = 0\n    while input is not None:\n        i += 1\n\n        # compute output\n        with torch.no_grad():\n            output = model(input)\n            loss = criterion(output, target)\n\n        # measure accuracy and record loss\n        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n\n        if args.distributed:\n            reduced_loss = reduce_tensor(loss.data)\n            prec1 = reduce_tensor(prec1)\n            prec5 = reduce_tensor(prec5)\n        else:\n            reduced_loss = loss.data\n\n        losses.update(to_python_float(reduced_loss), input.size(0))\n        top1.update(to_python_float(prec1), input.size(0))\n        top5.update(to_python_float(prec5), input.size(0))\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # TODO:  Change timings to mirror train().\n        if args.local_rank == 0 and i % args.print_freq == 0:\n            print(\'Test: [{0}/{1}]\\t\'\n                  \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                  \'Speed {2:.3f} ({3:.3f})\\t\'\n                  \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\n                  \'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t\'\n                  \'Prec@5 {top5.val:.3f} ({top5.avg:.3f})\'.format(\n                   i, len(val_loader),\n                   args.world_size * args.batch_size / batch_time.val,\n                   args.world_size * args.batch_size / batch_time.avg,\n                   batch_time=batch_time, loss=losses,\n                   top1=top1, top5=top5))\n\n        input, target = prefetcher.next()\n\n    print(\' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}\'\n          .format(top1=top1, top5=top5))\n\n    return top1.avg\n\n\ndef save_checkpoint(state, is_best, filename=\'checkpoint.pth.tar\'):\n    torch.save(state, filename)\n    if is_best:\n        shutil.copyfile(filename, \'model_best.pth.tar\')\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef adjust_learning_rate(optimizer, epoch, step, len_epoch):\n    """"""LR schedule that should yield 76% converged accuracy with batch size 256""""""\n    factor = epoch // 30\n\n    if epoch >= 80:\n        factor = factor + 1\n\n    lr = args.lr*(0.1**factor)\n\n    """"""Warmup""""""\n    if epoch < 5:\n        lr = lr*float(1 + step + epoch*len_epoch)/(5.*len_epoch)\n\n    # if(args.local_rank == 0):\n    #     print(""epoch = {}, step = {}, lr = {}"".format(epoch, step, lr))\n\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n\n\ndef accuracy(output, target, topk=(1,)):\n    """"""Computes the precision@k for the specified values of k""""""\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res\n\n\ndef reduce_tensor(tensor):\n    rt = tensor.clone()\n    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n    rt /= args.world_size\n    return rt\n\nif __name__ == \'__main__\':\n    main()\n'"
apex/tests/L0/run_test.py,0,"b'import unittest\nimport sys\n\ntest_dirs = [""run_amp"", ""run_fp16util"", ""run_mixed_adam"", ""run_fused_layer_norm""]\n\nrunner = unittest.TextTestRunner(verbosity=2)\n\nerrcode = 0\n\nfor test_dir in test_dirs:\n    suite = unittest.TestLoader().discover(test_dir)\n\n    print(""\\nExecuting tests from "" + test_dir)\n\n    result = runner.run(suite)\n\n    if not result.wasSuccessful():\n        errcode = 1\n\nsys.exit(errcode)\n'"
tensorboardX/tensorboardX/beholder/__init__.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .beholder import Beholder\nfrom .beholder import BeholderHook\n'"
tensorboardX/tensorboardX/beholder/beholder.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom ..proto.summary_pb2 import Summary\nfrom ..proto.summary_pb2 import SummaryMetadata\nfrom ..proto.tensor_pb2 import TensorProto\nfrom ..proto.tensor_shape_pb2 import TensorShapeProto\n\nimport os\nimport time\n\nimport numpy as np\n# import tensorflow as tf\n\n# from tensorboard.plugins.beholder import im_util\n# from . import im_util\nfrom .file_system_tools import read_pickle,\\\n    write_pickle, write_file\nfrom .shared_config import PLUGIN_NAME, TAG_NAME,\\\n    SUMMARY_FILENAME, DEFAULT_CONFIG, CONFIG_FILENAME, SUMMARY_COLLECTION_KEY_NAME, SECTION_INFO_FILENAME\nfrom . import video_writing\n# from .visualizer import Visualizer\n\n\nclass Beholder(object):\n\n    def __init__(self, logdir):\n        self.PLUGIN_LOGDIR = logdir + \'/plugins/\' + PLUGIN_NAME\n\n        self.is_recording = False\n        self.video_writer = video_writing.VideoWriter(\n            self.PLUGIN_LOGDIR,\n            outputs=[video_writing.FFmpegVideoOutput, video_writing.PNGVideoOutput])\n\n        self.last_image_shape = []\n        self.last_update_time = time.time()\n        self.config_last_modified_time = -1\n        self.previous_config = dict(DEFAULT_CONFIG)\n\n        if not os.path.exists(self.PLUGIN_LOGDIR + \'/config.pkl\'):\n            os.makedirs(self.PLUGIN_LOGDIR)\n            write_pickle(DEFAULT_CONFIG,\n                         \'{}/{}\'.format(self.PLUGIN_LOGDIR, CONFIG_FILENAME))\n\n        # self.visualizer = Visualizer(self.PLUGIN_LOGDIR)\n    def _get_config(self):\n        \'\'\'Reads the config file from disk or creates a new one.\'\'\'\n        filename = \'{}/{}\'.format(self.PLUGIN_LOGDIR, CONFIG_FILENAME)\n        modified_time = os.path.getmtime(filename)\n\n        if modified_time != self.config_last_modified_time:\n            config = read_pickle(filename, default=self.previous_config)\n            self.previous_config = config\n        else:\n            config = self.previous_config\n\n        self.config_last_modified_time = modified_time\n        return config\n\n    def _write_summary(self, frame):\n        \'\'\'Writes the frame to disk as a tensor summary.\'\'\'\n        path = \'{}/{}\'.format(self.PLUGIN_LOGDIR, SUMMARY_FILENAME)\n        smd = SummaryMetadata()\n        tensor = TensorProto(\n            dtype=\'DT_FLOAT\',\n            float_val=frame.reshape(-1).tolist(),\n            tensor_shape=TensorShapeProto(\n                dim=[TensorShapeProto.Dim(size=frame.shape[0]),\n                     TensorShapeProto.Dim(size=frame.shape[1]),\n                     TensorShapeProto.Dim(size=frame.shape[2])]\n            )\n        )\n        summary = Summary(value=[Summary.Value(\n            tag=TAG_NAME, metadata=smd, tensor=tensor)]).SerializeToString()\n        write_file(summary, path)\n\n    @staticmethod\n    def stats(tensor_and_name):\n        imgstats = []\n        for (img, name) in tensor_and_name:\n            immax = img.max()\n            immin = img.min()\n            imgstats.append(\n                {\n                    \'height\': img.shape[0],\n                    \'max\': str(immax),\n                    \'mean\': str(img.mean()),\n                    \'min\': str(immin),\n                    \'name\': name,\n                    \'range\': str(immax - immin),\n                    \'shape\': str((img.shape[1], img.shape[2]))\n                })\n        return imgstats\n\n    def _get_final_image(self, config, trainable=None, arrays=None, frame=None):\n        if config[\'values\'] == \'frames\':\n            # print(\'===frames===\')\n            final_image = frame\n        elif config[\'values\'] == \'arrays\':\n            # print(\'===arrays===\')\n            final_image = np.concatenate([arr for arr, _ in arrays])\n            stat = self.stats(arrays)\n            write_pickle(\n                stat, \'{}/{}\'.format(self.PLUGIN_LOGDIR, SECTION_INFO_FILENAME))\n        elif config[\'values\'] == \'trainable_variables\':\n            # print(\'===trainable===\')\n            final_image = np.concatenate([arr for arr, _ in trainable])\n            stat = self.stats(trainable)\n            write_pickle(\n                stat, \'{}/{}\'.format(self.PLUGIN_LOGDIR, SECTION_INFO_FILENAME))\n        if len(final_image.shape) == 2:  # Map grayscale images to 3D tensors.\n            final_image = np.expand_dims(final_image, -1)\n\n        return final_image\n\n    def _enough_time_has_passed(self, FPS):\n        \'\'\'For limiting how often frames are computed.\'\'\'\n        if FPS == 0:\n            return False\n        else:\n            earliest_time = self.last_update_time + (1.0 / FPS)\n            return time.time() >= earliest_time\n\n    def _update_frame(self, trainable, arrays, frame, config):\n        final_image = self._get_final_image(config, trainable, arrays, frame)\n        self._write_summary(final_image)\n        self.last_image_shape = final_image.shape\n\n        return final_image\n\n    def _update_recording(self, frame, config):\n        \'\'\'Adds a frame to the current video output.\'\'\'\n        # pylint: disable=redefined-variable-type\n        should_record = config[\'is_recording\']\n\n        if should_record:\n            if not self.is_recording:\n                self.is_recording = True\n                print(\'Starting recording using %s\',\n                      self.video_writer.current_output().name())\n            self.video_writer.write_frame(frame)\n        elif self.is_recording:\n            self.is_recording = False\n            self.video_writer.finish()\n            print(\'Finished recording\')\n\n    # TODO: blanket try and except for production? I don\'t someone\'s script to die\n    #       after weeks of running because of a visualization.\n    def update(self, trainable=None, arrays=None, frame=None):\n        \'\'\'Creates a frame and writes it to disk.\n\n        Args:\n            trainable: a list of namedtuple (tensors, name).\n            arrays: a list of namedtuple (tensors, name).\n            frame: lalala\n        \'\'\'\n\n        new_config = self._get_config()\n        if True or self._enough_time_has_passed(self.previous_config[\'FPS\']):\n            # self.visualizer.update(new_config)\n            self.last_update_time = time.time()\n            final_image = self._update_frame(\n                trainable, arrays, frame, new_config)\n            self._update_recording(final_image, new_config)\n\n    ##############################################################################\n    # @staticmethod\n    # def gradient_helper(optimizer, loss, var_list=None):\n    #   \'\'\'A helper to get the gradients out at each step.\n\n    #   Args:\n    #     optimizer: the optimizer op.\n    #     loss: the op that computes your loss value.\n\n    #   Returns: the gradient tensors and the train_step op.\n    #   \'\'\'\n    #   if var_list is None:\n    #     var_list = tf.trainable_variables()\n\n    #   grads_and_vars = optimizer.compute_gradients(loss, var_list=var_list)\n    #   grads = [pair[0] for pair in grads_and_vars]\n\n    #   return grads, optimizer.apply_gradients(grads_and_vars)\n\n\n# implements pytorch backward later\nclass BeholderHook():\n    pass\n    # """"""SessionRunHook implementation that runs Beholder every step.\n\n    # Convenient when using tf.train.MonitoredSession:\n    # ```python\n    # beholder_hook = BeholderHook(LOG_DIRECTORY)\n    # with MonitoredSession(..., hooks=[beholder_hook]) as sess:\n    #   sess.run(train_op)\n    # ```\n    # """"""\n    # def __init__(self, logdir):\n    #   """"""Creates new Hook instance\n\n    #   Args:\n    #     logdir: Directory where Beholder should write data.\n    #   """"""\n    #   self._logdir = logdir\n    #   self.beholder = None\n\n    # def begin(self):\n    #   self.beholder = Beholder(self._logdir)\n\n    # def after_run(self, run_context, unused_run_values):\n    #   self.beholder.update(run_context.session)\n'"
tensorboardX/tensorboardX/beholder/file_system_tools.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport pickle\n\n# import tensorflow as tf\n# from google.protobuf import message\n\n\ndef write_file(contents, path, mode=\'wb\'):\n    with open(path, mode) as new_file:\n        new_file.write(contents)\n\n\ndef write_pickle(obj, path):\n    with open(path, \'wb\') as new_file:\n        pickle.dump(obj, new_file)\n\n\ndef read_pickle(path, default=None):\n    with open(path, \'rb\') as pickle_file:\n        result = pickle.load(pickle_file)\n    return result\n'"
tensorboardX/tensorboardX/beholder/shared_config.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nPLUGIN_NAME = \'beholder\'\nTAG_NAME = \'beholder-frame\'\nSUMMARY_FILENAME = \'frame.summary\'\nCONFIG_FILENAME = \'config.pkl\'\nSECTION_INFO_FILENAME = \'section-info.pkl\'\nSUMMARY_COLLECTION_KEY_NAME = \'summaries_beholder\'\n\nDEFAULT_CONFIG = {\n    \'values\': \'trainable_variables\',\n    \'mode\': \'variance\',\n    \'scaling\': \'layer\',\n    \'window_size\': 15,\n    \'FPS\': 10,\n    \'is_recording\': False,\n    \'show_all\': False,\n    \'colormap\': \'magma\'\n}\n\nSECTION_HEIGHT = 128\nIMAGE_WIDTH = 512 + 256\n\nTB_WHITE = 245\n'"
tensorboardX/tensorboardX/beholder/video_writing.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport os\nimport subprocess\nimport time\n\nimport numpy as np\n\n\nclass VideoWriter(object):\n    """"""Video file writer that can use different output types.\n\n    Each VideoWriter instance writes video files to a specified directory, using\n    the first available VideoOutput from the provided list.\n    """"""\n\n    def __init__(self, directory, outputs):\n        self.directory = directory\n        # Filter to the available outputs\n        self.outputs = [out for out in outputs if out.available()]\n        if not self.outputs:\n            raise IOError(\'No available video outputs\')\n        self.output_index = 0\n        self.output = None\n        self.frame_shape = None\n\n    def current_output(self):\n        return self.outputs[self.output_index]\n\n    def write_frame(self, np_array):\n        # Reset whenever we encounter a new frame shape.\n        if self.frame_shape != np_array.shape:\n            if self.output:\n                self.output.close()\n            self.output = None\n            self.frame_shape = np_array.shape\n            print(\'Starting video with frame shape: %s\', self.frame_shape)\n        # Write the frame, advancing across output types as necessary.\n        original_output_index = self.output_index\n        for self.output_index in range(original_output_index, len(self.outputs)):\n            try:\n                if not self.output:\n                    new_output = self.outputs[self.output_index]\n                    if self.output_index > original_output_index:\n                        print(\'Falling back to video output %s\',\n                              new_output.name())\n                    self.output = new_output(self.directory, self.frame_shape)\n                self.output.emit_frame(np_array)\n                return\n            except (IOError, OSError) as e:\n                print(\'Video output type %s not available: %s\',\n                      self.current_output().name(), str(e))\n                if self.output:\n                    self.output.close()\n                self.output = None\n        raise IOError(\'Exhausted available video outputs\')\n\n    def finish(self):\n        if self.output:\n            self.output.close()\n        self.output = None\n        self.frame_shape = None\n        # Reconsider failed outputs when video is manually restarted.\n        self.output_index = 0\n\n\nclass VideoOutput(object):\n    """"""Base class for video outputs supported by VideoWriter.""""""\n\n    __metaclass__ = abc.ABCMeta\n\n    # Would add @abc.abstractmethod in python 3.3+\n    @classmethod\n    def available(cls):\n        raise NotImplementedError()\n\n    @classmethod\n    def name(cls):\n        return cls.__name__\n\n    @abc.abstractmethod\n    def emit_frame(self, np_array):\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    def close(self):\n        raise NotImplementedError()\n\n\nclass PNGVideoOutput(VideoOutput):\n    """"""Video output implemented by writing individual PNGs to disk.""""""\n\n    @classmethod\n    def available(cls):\n        return True\n\n    def __init__(self, directory, frame_shape):\n        del frame_shape  # unused\n        self.directory = directory + \'/video-frames-{}\'.format(time.time())\n        self.frame_num = 0\n        os.makedirs(self.directory)\n\n    def emit_frame(self, np_array):\n        filename = self.directory + \'/{:05}.png\'.format(self.frame_num)\n        self._write_image(np_array.astype(np.uint8), filename)\n        self.frame_num += 1\n\n    def _write_image(self, im, filename):\n        from PIL import Image\n        Image.fromarray(im).save(filename)\n\n    def close(self):\n        pass\n\n\nclass FFmpegVideoOutput(VideoOutput):\n    """"""Video output implemented by streaming to FFmpeg with .mp4 output.""""""\n\n    @classmethod\n    def available(cls):\n        # Silently check if ffmpeg is available.\n        try:\n            with open(os.devnull, \'wb\') as devnull:\n                subprocess.check_call(\n                    [\'ffmpeg\', \'-version\'], stdout=devnull, stderr=devnull)\n            return True\n        except (OSError, subprocess.CalledProcessError):\n            return False\n\n    def __init__(self, directory, frame_shape):\n        self.filename = directory + \'/video-{}.webm\'.format(time.time())\n        if len(frame_shape) != 3:\n            raise ValueError(\n                \'Expected rank-3 array for frame, got %s\' % str(frame_shape))\n        # Set input pixel format based on channel count.\n        if frame_shape[2] == 1:\n            pix_fmt = \'gray\'\n        elif frame_shape[2] == 3:\n            pix_fmt = \'rgb24\'\n        else:\n            raise ValueError(\'Unsupported channel count %d\' % frame_shape[2])\n\n        command = [\n            \'ffmpeg\',\n            \'-y\',  # Overwite output\n            # Input options - raw video file format and codec.\n            \'-f\', \'rawvideo\',\n            \'-vcodec\', \'rawvideo\',\n            # Width x height.\n            \'-s\', \'%dx%d\' % (frame_shape[1], frame_shape[0]),\n            \'-pix_fmt\', pix_fmt,\n            \'-r\', \'15\',  # Frame rate: arbitrarily use 15 frames per second.\n            \'-i\', \'-\',  # Use stdin.\n            \'-an\',  # No audio.\n            # Output options - use lossless VP9 codec inside .webm.\n            \'-vcodec\', \'libvpx-vp9\',\n            \'-lossless\', \'1\',\n            # Using YUV is most compatible, though conversion from RGB skews colors.\n            \'-pix_fmt\', \'yuv420p\',\n            self.filename\n        ]\n        PIPE = subprocess.PIPE\n        self.ffmpeg = subprocess.Popen(\n            command, stdin=PIPE, stdout=PIPE, stderr=PIPE)\n\n    def _handle_error(self):\n        _, stderr = self.ffmpeg.communicate()\n        bar = \'=\' * 40\n        print(\'Error writing to FFmpeg:\\n{}\\n{}\\n{}\',\n              bar, stderr, bar)\n\n    def emit_frame(self, np_array):\n        try:\n            self.ffmpeg.stdin.write(np_array.tobytes())\n            self.ffmpeg.stdin.flush()\n        except IOError:\n            self._handle_error()\n            raise IOError(\'Failure invoking FFmpeg\')\n\n    def close(self):\n        if self.ffmpeg.poll() is None:\n            # Close stdin and consume and discard stderr/stdout.\n            self.ffmpeg.communicate()\n        self.ffmpeg = None\n'"
tensorboardX/tensorboardX/proto/__init__.py,0,b''
tensorboardX/tensorboardX/proto/api_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorboardX/proto/api.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf.internal import enum_type_wrapper\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom google.protobuf import struct_pb2 as google_dot_protobuf_dot_struct__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorboardX/proto/api.proto\',\n  package=\'tensorboardX.hparam\',\n  syntax=\'proto3\',\n  serialized_options=None,\n  serialized_pb=_b(\'\\n\\x1ctensorboardX/proto/api.proto\\x12\\x13tensorboardX.hparam\\x1a\\x1cgoogle/protobuf/struct.proto\\""\\xc6\\x01\\n\\nExperiment\\x12\\x0c\\n\\x04name\\x18\\x06 \\x01(\\t\\x12\\x13\\n\\x0b\\x64\\x65scription\\x18\\x01 \\x01(\\t\\x12\\x0c\\n\\x04user\\x18\\x02 \\x01(\\t\\x12\\x19\\n\\x11time_created_secs\\x18\\x03 \\x01(\\x01\\x12\\x35\\n\\x0chparam_infos\\x18\\x04 \\x03(\\x0b\\x32\\x1f.tensorboardX.hparam.HParamInfo\\x12\\x35\\n\\x0cmetric_infos\\x18\\x05 \\x03(\\x0b\\x32\\x1f.tensorboardX.hparam.MetricInfo\\""\\xed\\x01\\n\\nHParamInfo\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x14\\n\\x0c\\x64isplay_name\\x18\\x02 \\x01(\\t\\x12\\x13\\n\\x0b\\x64\\x65scription\\x18\\x03 \\x01(\\t\\x12+\\n\\x04type\\x18\\x04 \\x01(\\x0e\\x32\\x1d.tensorboardX.hparam.DataType\\x12\\x35\\n\\x0f\\x64omain_discrete\\x18\\x05 \\x01(\\x0b\\x32\\x1a.google.protobuf.ListValueH\\x00\\x12\\x38\\n\\x0f\\x64omain_interval\\x18\\x06 \\x01(\\x0b\\x32\\x1d.tensorboardX.hparam.IntervalH\\x00\\x42\\x08\\n\\x06\\x64omain\\""0\\n\\x08Interval\\x12\\x11\\n\\tmin_value\\x18\\x01 \\x01(\\x01\\x12\\x11\\n\\tmax_value\\x18\\x02 \\x01(\\x01\\""(\\n\\nMetricName\\x12\\r\\n\\x05group\\x18\\x01 \\x01(\\t\\x12\\x0b\\n\\x03tag\\x18\\x02 \\x01(\\t\\""\\x9e\\x01\\n\\nMetricInfo\\x12-\\n\\x04name\\x18\\x01 \\x01(\\x0b\\x32\\x1f.tensorboardX.hparam.MetricName\\x12\\x14\\n\\x0c\\x64isplay_name\\x18\\x03 \\x01(\\t\\x12\\x13\\n\\x0b\\x64\\x65scription\\x18\\x04 \\x01(\\t\\x12\\x36\\n\\x0c\\x64\\x61taset_type\\x18\\x05 \\x01(\\x0e\\x32 .tensorboardX.hparam.DatasetType\\""\\xa3\\x02\\n\\x0cSessionGroup\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12?\\n\\x07hparams\\x18\\x02 \\x03(\\x0b\\x32..tensorboardX.hparam.SessionGroup.HparamsEntry\\x12\\x37\\n\\rmetric_values\\x18\\x03 \\x03(\\x0b\\x32 .tensorboardX.hparam.MetricValue\\x12.\\n\\x08sessions\\x18\\x04 \\x03(\\x0b\\x32\\x1c.tensorboardX.hparam.Session\\x12\\x13\\n\\x0bmonitor_url\\x18\\x05 \\x01(\\t\\x1a\\x46\\n\\x0cHparamsEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12%\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x16.google.protobuf.Value:\\x02\\x38\\x01\\""z\\n\\x0bMetricValue\\x12-\\n\\x04name\\x18\\x01 \\x01(\\x0b\\x32\\x1f.tensorboardX.hparam.MetricName\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\x01\\x12\\x15\\n\\rtraining_step\\x18\\x03 \\x01(\\x05\\x12\\x16\\n\\x0ewall_time_secs\\x18\\x04 \\x01(\\x01\\""\\xd5\\x01\\n\\x07Session\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x17\\n\\x0fstart_time_secs\\x18\\x02 \\x01(\\x01\\x12\\x15\\n\\rend_time_secs\\x18\\x03 \\x01(\\x01\\x12+\\n\\x06status\\x18\\x04 \\x01(\\x0e\\x32\\x1b.tensorboardX.hparam.Status\\x12\\x11\\n\\tmodel_uri\\x18\\x05 \\x01(\\t\\x12\\x37\\n\\rmetric_values\\x18\\x06 \\x03(\\x0b\\x32 .tensorboardX.hparam.MetricValue\\x12\\x13\\n\\x0bmonitor_url\\x18\\x07 \\x01(\\t\\""/\\n\\x14GetExperimentRequest\\x12\\x17\\n\\x0f\\x65xperiment_name\\x18\\x01 \\x01(\\t\\""\\xc4\\x02\\n\\x18ListSessionGroupsRequest\\x12\\x17\\n\\x0f\\x65xperiment_name\\x18\\x06 \\x01(\\t\\x12\\x35\\n\\x10\\x61llowed_statuses\\x18\\x07 \\x03(\\x0e\\x32\\x1b.tensorboardX.hparam.Status\\x12\\x32\\n\\ncol_params\\x18\\x01 \\x03(\\x0b\\x32\\x1e.tensorboardX.hparam.ColParams\\x12>\\n\\x10\\x61ggregation_type\\x18\\x02 \\x01(\\x0e\\x32$.tensorboardX.hparam.AggregationType\\x12;\\n\\x12\\x61ggregation_metric\\x18\\x03 \\x01(\\x0b\\x32\\x1f.tensorboardX.hparam.MetricName\\x12\\x13\\n\\x0bstart_index\\x18\\x04 \\x01(\\x05\\x12\\x12\\n\\nslice_size\\x18\\x05 \\x01(\\x05\\""\\xd9\\x02\\n\\tColParams\\x12\\x31\\n\\x06metric\\x18\\x01 \\x01(\\x0b\\x32\\x1f.tensorboardX.hparam.MetricNameH\\x00\\x12\\x10\\n\\x06hparam\\x18\\x02 \\x01(\\tH\\x00\\x12-\\n\\x05order\\x18\\x03 \\x01(\\x0e\\x32\\x1e.tensorboardX.hparam.SortOrder\\x12\\x1c\\n\\x14missing_values_first\\x18\\x04 \\x01(\\x08\\x12\\x17\\n\\rfilter_regexp\\x18\\x05 \\x01(\\tH\\x01\\x12\\x38\\n\\x0f\\x66ilter_interval\\x18\\x06 \\x01(\\x0b\\x32\\x1d.tensorboardX.hparam.IntervalH\\x01\\x12\\x35\\n\\x0f\\x66ilter_discrete\\x18\\x07 \\x01(\\x0b\\x32\\x1a.google.protobuf.ListValueH\\x01\\x12\\x1e\\n\\x16\\x65xclude_missing_values\\x18\\x08 \\x01(\\x08\\x42\\x06\\n\\x04nameB\\x08\\n\\x06\\x66ilter\\""j\\n\\x19ListSessionGroupsResponse\\x12\\x39\\n\\x0esession_groups\\x18\\x01 \\x03(\\x0b\\x32!.tensorboardX.hparam.SessionGroup\\x12\\x12\\n\\ntotal_size\\x18\\x03 \\x01(\\x05\\""}\\n\\x16ListMetricEvalsRequest\\x12\\x17\\n\\x0f\\x65xperiment_name\\x18\\x03 \\x01(\\t\\x12\\x14\\n\\x0csession_name\\x18\\x01 \\x01(\\t\\x12\\x34\\n\\x0bmetric_name\\x18\\x02 \\x01(\\x0b\\x32\\x1f.tensorboardX.hparam.MetricName*`\\n\\x08\\x44\\x61taType\\x12\\x13\\n\\x0f\\x44\\x41TA_TYPE_UNSET\\x10\\x00\\x12\\x14\\n\\x10\\x44\\x41TA_TYPE_STRING\\x10\\x01\\x12\\x12\\n\\x0e\\x44\\x41TA_TYPE_BOOL\\x10\\x02\\x12\\x15\\n\\x11\\x44\\x41TA_TYPE_FLOAT64\\x10\\x03*P\\n\\x0b\\x44\\x61tasetType\\x12\\x13\\n\\x0f\\x44\\x41TASET_UNKNOWN\\x10\\x00\\x12\\x14\\n\\x10\\x44\\x41TASET_TRAINING\\x10\\x01\\x12\\x16\\n\\x12\\x44\\x41TASET_VALIDATION\\x10\\x02*X\\n\\x06Status\\x12\\x12\\n\\x0eSTATUS_UNKNOWN\\x10\\x00\\x12\\x12\\n\\x0eSTATUS_SUCCESS\\x10\\x01\\x12\\x12\\n\\x0eSTATUS_FAILURE\\x10\\x02\\x12\\x12\\n\\x0eSTATUS_RUNNING\\x10\\x03*A\\n\\tSortOrder\\x12\\x15\\n\\x11ORDER_UNSPECIFIED\\x10\\x00\\x12\\r\\n\\tORDER_ASC\\x10\\x01\\x12\\x0e\\n\\nORDER_DESC\\x10\\x02*\\x7f\\n\\x0f\\x41ggregationType\\x12\\x15\\n\\x11\\x41GGREGATION_UNSET\\x10\\x00\\x12\\x13\\n\\x0f\\x41GGREGATION_AVG\\x10\\x01\\x12\\x16\\n\\x12\\x41GGREGATION_MEDIAN\\x10\\x02\\x12\\x13\\n\\x0f\\x41GGREGATION_MIN\\x10\\x03\\x12\\x13\\n\\x0f\\x41GGREGATION_MAX\\x10\\x04\\x62\\x06proto3\')\n  ,\n  dependencies=[google_dot_protobuf_dot_struct__pb2.DESCRIPTOR,])\n\n_DATATYPE = _descriptor.EnumDescriptor(\n  name=\'DataType\',\n  full_name=\'tensorboardX.hparam.DataType\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'DATA_TYPE_UNSET\', index=0, number=0,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'DATA_TYPE_STRING\', index=1, number=1,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'DATA_TYPE_BOOL\', index=2, number=2,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'DATA_TYPE_FLOAT64\', index=3, number=3,\n      serialized_options=None,\n      type=None),\n  ],\n  containing_type=None,\n  serialized_options=None,\n  serialized_start=2370,\n  serialized_end=2466,\n)\n_sym_db.RegisterEnumDescriptor(_DATATYPE)\n\nDataType = enum_type_wrapper.EnumTypeWrapper(_DATATYPE)\n_DATASETTYPE = _descriptor.EnumDescriptor(\n  name=\'DatasetType\',\n  full_name=\'tensorboardX.hparam.DatasetType\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'DATASET_UNKNOWN\', index=0, number=0,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'DATASET_TRAINING\', index=1, number=1,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'DATASET_VALIDATION\', index=2, number=2,\n      serialized_options=None,\n      type=None),\n  ],\n  containing_type=None,\n  serialized_options=None,\n  serialized_start=2468,\n  serialized_end=2548,\n)\n_sym_db.RegisterEnumDescriptor(_DATASETTYPE)\n\nDatasetType = enum_type_wrapper.EnumTypeWrapper(_DATASETTYPE)\n_STATUS = _descriptor.EnumDescriptor(\n  name=\'Status\',\n  full_name=\'tensorboardX.hparam.Status\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'STATUS_UNKNOWN\', index=0, number=0,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'STATUS_SUCCESS\', index=1, number=1,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'STATUS_FAILURE\', index=2, number=2,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'STATUS_RUNNING\', index=3, number=3,\n      serialized_options=None,\n      type=None),\n  ],\n  containing_type=None,\n  serialized_options=None,\n  serialized_start=2550,\n  serialized_end=2638,\n)\n_sym_db.RegisterEnumDescriptor(_STATUS)\n\nStatus = enum_type_wrapper.EnumTypeWrapper(_STATUS)\n_SORTORDER = _descriptor.EnumDescriptor(\n  name=\'SortOrder\',\n  full_name=\'tensorboardX.hparam.SortOrder\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'ORDER_UNSPECIFIED\', index=0, number=0,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'ORDER_ASC\', index=1, number=1,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'ORDER_DESC\', index=2, number=2,\n      serialized_options=None,\n      type=None),\n  ],\n  containing_type=None,\n  serialized_options=None,\n  serialized_start=2640,\n  serialized_end=2705,\n)\n_sym_db.RegisterEnumDescriptor(_SORTORDER)\n\nSortOrder = enum_type_wrapper.EnumTypeWrapper(_SORTORDER)\n_AGGREGATIONTYPE = _descriptor.EnumDescriptor(\n  name=\'AggregationType\',\n  full_name=\'tensorboardX.hparam.AggregationType\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'AGGREGATION_UNSET\', index=0, number=0,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'AGGREGATION_AVG\', index=1, number=1,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'AGGREGATION_MEDIAN\', index=2, number=2,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'AGGREGATION_MIN\', index=3, number=3,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'AGGREGATION_MAX\', index=4, number=4,\n      serialized_options=None,\n      type=None),\n  ],\n  containing_type=None,\n  serialized_options=None,\n  serialized_start=2707,\n  serialized_end=2834,\n)\n_sym_db.RegisterEnumDescriptor(_AGGREGATIONTYPE)\n\nAggregationType = enum_type_wrapper.EnumTypeWrapper(_AGGREGATIONTYPE)\nDATA_TYPE_UNSET = 0\nDATA_TYPE_STRING = 1\nDATA_TYPE_BOOL = 2\nDATA_TYPE_FLOAT64 = 3\nDATASET_UNKNOWN = 0\nDATASET_TRAINING = 1\nDATASET_VALIDATION = 2\nSTATUS_UNKNOWN = 0\nSTATUS_SUCCESS = 1\nSTATUS_FAILURE = 2\nSTATUS_RUNNING = 3\nORDER_UNSPECIFIED = 0\nORDER_ASC = 1\nORDER_DESC = 2\nAGGREGATION_UNSET = 0\nAGGREGATION_AVG = 1\nAGGREGATION_MEDIAN = 2\nAGGREGATION_MIN = 3\nAGGREGATION_MAX = 4\n\n\n\n_EXPERIMENT = _descriptor.Descriptor(\n  name=\'Experiment\',\n  full_name=\'tensorboardX.hparam.Experiment\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorboardX.hparam.Experiment.name\', index=0,\n      number=6, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'description\', full_name=\'tensorboardX.hparam.Experiment.description\', index=1,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'user\', full_name=\'tensorboardX.hparam.Experiment.user\', index=2,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'time_created_secs\', full_name=\'tensorboardX.hparam.Experiment.time_created_secs\', index=3,\n      number=3, type=1, cpp_type=5, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'hparam_infos\', full_name=\'tensorboardX.hparam.Experiment.hparam_infos\', index=4,\n      number=4, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'metric_infos\', full_name=\'tensorboardX.hparam.Experiment.metric_infos\', index=5,\n      number=5, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=84,\n  serialized_end=282,\n)\n\n\n_HPARAMINFO = _descriptor.Descriptor(\n  name=\'HParamInfo\',\n  full_name=\'tensorboardX.hparam.HParamInfo\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorboardX.hparam.HParamInfo.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'display_name\', full_name=\'tensorboardX.hparam.HParamInfo.display_name\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'description\', full_name=\'tensorboardX.hparam.HParamInfo.description\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'type\', full_name=\'tensorboardX.hparam.HParamInfo.type\', index=3,\n      number=4, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'domain_discrete\', full_name=\'tensorboardX.hparam.HParamInfo.domain_discrete\', index=4,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'domain_interval\', full_name=\'tensorboardX.hparam.HParamInfo.domain_interval\', index=5,\n      number=6, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'domain\', full_name=\'tensorboardX.hparam.HParamInfo.domain\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=285,\n  serialized_end=522,\n)\n\n\n_INTERVAL = _descriptor.Descriptor(\n  name=\'Interval\',\n  full_name=\'tensorboardX.hparam.Interval\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'min_value\', full_name=\'tensorboardX.hparam.Interval.min_value\', index=0,\n      number=1, type=1, cpp_type=5, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'max_value\', full_name=\'tensorboardX.hparam.Interval.max_value\', index=1,\n      number=2, type=1, cpp_type=5, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=524,\n  serialized_end=572,\n)\n\n\n_METRICNAME = _descriptor.Descriptor(\n  name=\'MetricName\',\n  full_name=\'tensorboardX.hparam.MetricName\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'group\', full_name=\'tensorboardX.hparam.MetricName.group\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'tag\', full_name=\'tensorboardX.hparam.MetricName.tag\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=574,\n  serialized_end=614,\n)\n\n\n_METRICINFO = _descriptor.Descriptor(\n  name=\'MetricInfo\',\n  full_name=\'tensorboardX.hparam.MetricInfo\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorboardX.hparam.MetricInfo.name\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'display_name\', full_name=\'tensorboardX.hparam.MetricInfo.display_name\', index=1,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'description\', full_name=\'tensorboardX.hparam.MetricInfo.description\', index=2,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'dataset_type\', full_name=\'tensorboardX.hparam.MetricInfo.dataset_type\', index=3,\n      number=5, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=617,\n  serialized_end=775,\n)\n\n\n_SESSIONGROUP_HPARAMSENTRY = _descriptor.Descriptor(\n  name=\'HparamsEntry\',\n  full_name=\'tensorboardX.hparam.SessionGroup.HparamsEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'tensorboardX.hparam.SessionGroup.HparamsEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorboardX.hparam.SessionGroup.HparamsEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=_b(\'8\\001\'),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=999,\n  serialized_end=1069,\n)\n\n_SESSIONGROUP = _descriptor.Descriptor(\n  name=\'SessionGroup\',\n  full_name=\'tensorboardX.hparam.SessionGroup\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorboardX.hparam.SessionGroup.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'hparams\', full_name=\'tensorboardX.hparam.SessionGroup.hparams\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'metric_values\', full_name=\'tensorboardX.hparam.SessionGroup.metric_values\', index=2,\n      number=3, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'sessions\', full_name=\'tensorboardX.hparam.SessionGroup.sessions\', index=3,\n      number=4, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'monitor_url\', full_name=\'tensorboardX.hparam.SessionGroup.monitor_url\', index=4,\n      number=5, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_SESSIONGROUP_HPARAMSENTRY, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=778,\n  serialized_end=1069,\n)\n\n\n_METRICVALUE = _descriptor.Descriptor(\n  name=\'MetricValue\',\n  full_name=\'tensorboardX.hparam.MetricValue\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorboardX.hparam.MetricValue.name\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorboardX.hparam.MetricValue.value\', index=1,\n      number=2, type=1, cpp_type=5, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'training_step\', full_name=\'tensorboardX.hparam.MetricValue.training_step\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'wall_time_secs\', full_name=\'tensorboardX.hparam.MetricValue.wall_time_secs\', index=3,\n      number=4, type=1, cpp_type=5, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1071,\n  serialized_end=1193,\n)\n\n\n_SESSION = _descriptor.Descriptor(\n  name=\'Session\',\n  full_name=\'tensorboardX.hparam.Session\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorboardX.hparam.Session.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'start_time_secs\', full_name=\'tensorboardX.hparam.Session.start_time_secs\', index=1,\n      number=2, type=1, cpp_type=5, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'end_time_secs\', full_name=\'tensorboardX.hparam.Session.end_time_secs\', index=2,\n      number=3, type=1, cpp_type=5, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'status\', full_name=\'tensorboardX.hparam.Session.status\', index=3,\n      number=4, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'model_uri\', full_name=\'tensorboardX.hparam.Session.model_uri\', index=4,\n      number=5, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'metric_values\', full_name=\'tensorboardX.hparam.Session.metric_values\', index=5,\n      number=6, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'monitor_url\', full_name=\'tensorboardX.hparam.Session.monitor_url\', index=6,\n      number=7, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1196,\n  serialized_end=1409,\n)\n\n\n_GETEXPERIMENTREQUEST = _descriptor.Descriptor(\n  name=\'GetExperimentRequest\',\n  full_name=\'tensorboardX.hparam.GetExperimentRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'experiment_name\', full_name=\'tensorboardX.hparam.GetExperimentRequest.experiment_name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1411,\n  serialized_end=1458,\n)\n\n\n_LISTSESSIONGROUPSREQUEST = _descriptor.Descriptor(\n  name=\'ListSessionGroupsRequest\',\n  full_name=\'tensorboardX.hparam.ListSessionGroupsRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'experiment_name\', full_name=\'tensorboardX.hparam.ListSessionGroupsRequest.experiment_name\', index=0,\n      number=6, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'allowed_statuses\', full_name=\'tensorboardX.hparam.ListSessionGroupsRequest.allowed_statuses\', index=1,\n      number=7, type=14, cpp_type=8, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'col_params\', full_name=\'tensorboardX.hparam.ListSessionGroupsRequest.col_params\', index=2,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'aggregation_type\', full_name=\'tensorboardX.hparam.ListSessionGroupsRequest.aggregation_type\', index=3,\n      number=2, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'aggregation_metric\', full_name=\'tensorboardX.hparam.ListSessionGroupsRequest.aggregation_metric\', index=4,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'start_index\', full_name=\'tensorboardX.hparam.ListSessionGroupsRequest.start_index\', index=5,\n      number=4, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'slice_size\', full_name=\'tensorboardX.hparam.ListSessionGroupsRequest.slice_size\', index=6,\n      number=5, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1461,\n  serialized_end=1785,\n)\n\n\n_COLPARAMS = _descriptor.Descriptor(\n  name=\'ColParams\',\n  full_name=\'tensorboardX.hparam.ColParams\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'metric\', full_name=\'tensorboardX.hparam.ColParams.metric\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'hparam\', full_name=\'tensorboardX.hparam.ColParams.hparam\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'order\', full_name=\'tensorboardX.hparam.ColParams.order\', index=2,\n      number=3, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'missing_values_first\', full_name=\'tensorboardX.hparam.ColParams.missing_values_first\', index=3,\n      number=4, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'filter_regexp\', full_name=\'tensorboardX.hparam.ColParams.filter_regexp\', index=4,\n      number=5, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'filter_interval\', full_name=\'tensorboardX.hparam.ColParams.filter_interval\', index=5,\n      number=6, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'filter_discrete\', full_name=\'tensorboardX.hparam.ColParams.filter_discrete\', index=6,\n      number=7, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'exclude_missing_values\', full_name=\'tensorboardX.hparam.ColParams.exclude_missing_values\', index=7,\n      number=8, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'name\', full_name=\'tensorboardX.hparam.ColParams.name\',\n      index=0, containing_type=None, fields=[]),\n    _descriptor.OneofDescriptor(\n      name=\'filter\', full_name=\'tensorboardX.hparam.ColParams.filter\',\n      index=1, containing_type=None, fields=[]),\n  ],\n  serialized_start=1788,\n  serialized_end=2133,\n)\n\n\n_LISTSESSIONGROUPSRESPONSE = _descriptor.Descriptor(\n  name=\'ListSessionGroupsResponse\',\n  full_name=\'tensorboardX.hparam.ListSessionGroupsResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'session_groups\', full_name=\'tensorboardX.hparam.ListSessionGroupsResponse.session_groups\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'total_size\', full_name=\'tensorboardX.hparam.ListSessionGroupsResponse.total_size\', index=1,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2135,\n  serialized_end=2241,\n)\n\n\n_LISTMETRICEVALSREQUEST = _descriptor.Descriptor(\n  name=\'ListMetricEvalsRequest\',\n  full_name=\'tensorboardX.hparam.ListMetricEvalsRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'experiment_name\', full_name=\'tensorboardX.hparam.ListMetricEvalsRequest.experiment_name\', index=0,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'session_name\', full_name=\'tensorboardX.hparam.ListMetricEvalsRequest.session_name\', index=1,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'metric_name\', full_name=\'tensorboardX.hparam.ListMetricEvalsRequest.metric_name\', index=2,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2243,\n  serialized_end=2368,\n)\n\n_EXPERIMENT.fields_by_name[\'hparam_infos\'].message_type = _HPARAMINFO\n_EXPERIMENT.fields_by_name[\'metric_infos\'].message_type = _METRICINFO\n_HPARAMINFO.fields_by_name[\'type\'].enum_type = _DATATYPE\n_HPARAMINFO.fields_by_name[\'domain_discrete\'].message_type = google_dot_protobuf_dot_struct__pb2._LISTVALUE\n_HPARAMINFO.fields_by_name[\'domain_interval\'].message_type = _INTERVAL\n_HPARAMINFO.oneofs_by_name[\'domain\'].fields.append(\n  _HPARAMINFO.fields_by_name[\'domain_discrete\'])\n_HPARAMINFO.fields_by_name[\'domain_discrete\'].containing_oneof = _HPARAMINFO.oneofs_by_name[\'domain\']\n_HPARAMINFO.oneofs_by_name[\'domain\'].fields.append(\n  _HPARAMINFO.fields_by_name[\'domain_interval\'])\n_HPARAMINFO.fields_by_name[\'domain_interval\'].containing_oneof = _HPARAMINFO.oneofs_by_name[\'domain\']\n_METRICINFO.fields_by_name[\'name\'].message_type = _METRICNAME\n_METRICINFO.fields_by_name[\'dataset_type\'].enum_type = _DATASETTYPE\n_SESSIONGROUP_HPARAMSENTRY.fields_by_name[\'value\'].message_type = google_dot_protobuf_dot_struct__pb2._VALUE\n_SESSIONGROUP_HPARAMSENTRY.containing_type = _SESSIONGROUP\n_SESSIONGROUP.fields_by_name[\'hparams\'].message_type = _SESSIONGROUP_HPARAMSENTRY\n_SESSIONGROUP.fields_by_name[\'metric_values\'].message_type = _METRICVALUE\n_SESSIONGROUP.fields_by_name[\'sessions\'].message_type = _SESSION\n_METRICVALUE.fields_by_name[\'name\'].message_type = _METRICNAME\n_SESSION.fields_by_name[\'status\'].enum_type = _STATUS\n_SESSION.fields_by_name[\'metric_values\'].message_type = _METRICVALUE\n_LISTSESSIONGROUPSREQUEST.fields_by_name[\'allowed_statuses\'].enum_type = _STATUS\n_LISTSESSIONGROUPSREQUEST.fields_by_name[\'col_params\'].message_type = _COLPARAMS\n_LISTSESSIONGROUPSREQUEST.fields_by_name[\'aggregation_type\'].enum_type = _AGGREGATIONTYPE\n_LISTSESSIONGROUPSREQUEST.fields_by_name[\'aggregation_metric\'].message_type = _METRICNAME\n_COLPARAMS.fields_by_name[\'metric\'].message_type = _METRICNAME\n_COLPARAMS.fields_by_name[\'order\'].enum_type = _SORTORDER\n_COLPARAMS.fields_by_name[\'filter_interval\'].message_type = _INTERVAL\n_COLPARAMS.fields_by_name[\'filter_discrete\'].message_type = google_dot_protobuf_dot_struct__pb2._LISTVALUE\n_COLPARAMS.oneofs_by_name[\'name\'].fields.append(\n  _COLPARAMS.fields_by_name[\'metric\'])\n_COLPARAMS.fields_by_name[\'metric\'].containing_oneof = _COLPARAMS.oneofs_by_name[\'name\']\n_COLPARAMS.oneofs_by_name[\'name\'].fields.append(\n  _COLPARAMS.fields_by_name[\'hparam\'])\n_COLPARAMS.fields_by_name[\'hparam\'].containing_oneof = _COLPARAMS.oneofs_by_name[\'name\']\n_COLPARAMS.oneofs_by_name[\'filter\'].fields.append(\n  _COLPARAMS.fields_by_name[\'filter_regexp\'])\n_COLPARAMS.fields_by_name[\'filter_regexp\'].containing_oneof = _COLPARAMS.oneofs_by_name[\'filter\']\n_COLPARAMS.oneofs_by_name[\'filter\'].fields.append(\n  _COLPARAMS.fields_by_name[\'filter_interval\'])\n_COLPARAMS.fields_by_name[\'filter_interval\'].containing_oneof = _COLPARAMS.oneofs_by_name[\'filter\']\n_COLPARAMS.oneofs_by_name[\'filter\'].fields.append(\n  _COLPARAMS.fields_by_name[\'filter_discrete\'])\n_COLPARAMS.fields_by_name[\'filter_discrete\'].containing_oneof = _COLPARAMS.oneofs_by_name[\'filter\']\n_LISTSESSIONGROUPSRESPONSE.fields_by_name[\'session_groups\'].message_type = _SESSIONGROUP\n_LISTMETRICEVALSREQUEST.fields_by_name[\'metric_name\'].message_type = _METRICNAME\nDESCRIPTOR.message_types_by_name[\'Experiment\'] = _EXPERIMENT\nDESCRIPTOR.message_types_by_name[\'HParamInfo\'] = _HPARAMINFO\nDESCRIPTOR.message_types_by_name[\'Interval\'] = _INTERVAL\nDESCRIPTOR.message_types_by_name[\'MetricName\'] = _METRICNAME\nDESCRIPTOR.message_types_by_name[\'MetricInfo\'] = _METRICINFO\nDESCRIPTOR.message_types_by_name[\'SessionGroup\'] = _SESSIONGROUP\nDESCRIPTOR.message_types_by_name[\'MetricValue\'] = _METRICVALUE\nDESCRIPTOR.message_types_by_name[\'Session\'] = _SESSION\nDESCRIPTOR.message_types_by_name[\'GetExperimentRequest\'] = _GETEXPERIMENTREQUEST\nDESCRIPTOR.message_types_by_name[\'ListSessionGroupsRequest\'] = _LISTSESSIONGROUPSREQUEST\nDESCRIPTOR.message_types_by_name[\'ColParams\'] = _COLPARAMS\nDESCRIPTOR.message_types_by_name[\'ListSessionGroupsResponse\'] = _LISTSESSIONGROUPSRESPONSE\nDESCRIPTOR.message_types_by_name[\'ListMetricEvalsRequest\'] = _LISTMETRICEVALSREQUEST\nDESCRIPTOR.enum_types_by_name[\'DataType\'] = _DATATYPE\nDESCRIPTOR.enum_types_by_name[\'DatasetType\'] = _DATASETTYPE\nDESCRIPTOR.enum_types_by_name[\'Status\'] = _STATUS\nDESCRIPTOR.enum_types_by_name[\'SortOrder\'] = _SORTORDER\nDESCRIPTOR.enum_types_by_name[\'AggregationType\'] = _AGGREGATIONTYPE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nExperiment = _reflection.GeneratedProtocolMessageType(\'Experiment\', (_message.Message,), dict(\n  DESCRIPTOR = _EXPERIMENT,\n  __module__ = \'tensorboardX.proto.api_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.hparam.Experiment)\n  ))\n_sym_db.RegisterMessage(Experiment)\n\nHParamInfo = _reflection.GeneratedProtocolMessageType(\'HParamInfo\', (_message.Message,), dict(\n  DESCRIPTOR = _HPARAMINFO,\n  __module__ = \'tensorboardX.proto.api_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.hparam.HParamInfo)\n  ))\n_sym_db.RegisterMessage(HParamInfo)\n\nInterval = _reflection.GeneratedProtocolMessageType(\'Interval\', (_message.Message,), dict(\n  DESCRIPTOR = _INTERVAL,\n  __module__ = \'tensorboardX.proto.api_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.hparam.Interval)\n  ))\n_sym_db.RegisterMessage(Interval)\n\nMetricName = _reflection.GeneratedProtocolMessageType(\'MetricName\', (_message.Message,), dict(\n  DESCRIPTOR = _METRICNAME,\n  __module__ = \'tensorboardX.proto.api_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.hparam.MetricName)\n  ))\n_sym_db.RegisterMessage(MetricName)\n\nMetricInfo = _reflection.GeneratedProtocolMessageType(\'MetricInfo\', (_message.Message,), dict(\n  DESCRIPTOR = _METRICINFO,\n  __module__ = \'tensorboardX.proto.api_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.hparam.MetricInfo)\n  ))\n_sym_db.RegisterMessage(MetricInfo)\n\nSessionGroup = _reflection.GeneratedProtocolMessageType(\'SessionGroup\', (_message.Message,), dict(\n\n  HparamsEntry = _reflection.GeneratedProtocolMessageType(\'HparamsEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _SESSIONGROUP_HPARAMSENTRY,\n    __module__ = \'tensorboardX.proto.api_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorboardX.hparam.SessionGroup.HparamsEntry)\n    ))\n  ,\n  DESCRIPTOR = _SESSIONGROUP,\n  __module__ = \'tensorboardX.proto.api_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.hparam.SessionGroup)\n  ))\n_sym_db.RegisterMessage(SessionGroup)\n_sym_db.RegisterMessage(SessionGroup.HparamsEntry)\n\nMetricValue = _reflection.GeneratedProtocolMessageType(\'MetricValue\', (_message.Message,), dict(\n  DESCRIPTOR = _METRICVALUE,\n  __module__ = \'tensorboardX.proto.api_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.hparam.MetricValue)\n  ))\n_sym_db.RegisterMessage(MetricValue)\n\nSession = _reflection.GeneratedProtocolMessageType(\'Session\', (_message.Message,), dict(\n  DESCRIPTOR = _SESSION,\n  __module__ = \'tensorboardX.proto.api_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.hparam.Session)\n  ))\n_sym_db.RegisterMessage(Session)\n\nGetExperimentRequest = _reflection.GeneratedProtocolMessageType(\'GetExperimentRequest\', (_message.Message,), dict(\n  DESCRIPTOR = _GETEXPERIMENTREQUEST,\n  __module__ = \'tensorboardX.proto.api_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.hparam.GetExperimentRequest)\n  ))\n_sym_db.RegisterMessage(GetExperimentRequest)\n\nListSessionGroupsRequest = _reflection.GeneratedProtocolMessageType(\'ListSessionGroupsRequest\', (_message.Message,), dict(\n  DESCRIPTOR = _LISTSESSIONGROUPSREQUEST,\n  __module__ = \'tensorboardX.proto.api_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.hparam.ListSessionGroupsRequest)\n  ))\n_sym_db.RegisterMessage(ListSessionGroupsRequest)\n\nColParams = _reflection.GeneratedProtocolMessageType(\'ColParams\', (_message.Message,), dict(\n  DESCRIPTOR = _COLPARAMS,\n  __module__ = \'tensorboardX.proto.api_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.hparam.ColParams)\n  ))\n_sym_db.RegisterMessage(ColParams)\n\nListSessionGroupsResponse = _reflection.GeneratedProtocolMessageType(\'ListSessionGroupsResponse\', (_message.Message,), dict(\n  DESCRIPTOR = _LISTSESSIONGROUPSRESPONSE,\n  __module__ = \'tensorboardX.proto.api_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.hparam.ListSessionGroupsResponse)\n  ))\n_sym_db.RegisterMessage(ListSessionGroupsResponse)\n\nListMetricEvalsRequest = _reflection.GeneratedProtocolMessageType(\'ListMetricEvalsRequest\', (_message.Message,), dict(\n  DESCRIPTOR = _LISTMETRICEVALSREQUEST,\n  __module__ = \'tensorboardX.proto.api_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.hparam.ListMetricEvalsRequest)\n  ))\n_sym_db.RegisterMessage(ListMetricEvalsRequest)\n\n\n_SESSIONGROUP_HPARAMSENTRY._options = None\n# @@protoc_insertion_point(module_scope)\n'"
tensorboardX/tensorboardX/proto/attr_value_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorboardX/proto/attr_value.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorboardX.proto import tensor_pb2 as tensorboardX_dot_proto_dot_tensor__pb2\nfrom tensorboardX.proto import tensor_shape_pb2 as tensorboardX_dot_proto_dot_tensor__shape__pb2\nfrom tensorboardX.proto import types_pb2 as tensorboardX_dot_proto_dot_types__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorboardX/proto/attr_value.proto\',\n  package=\'tensorboardX\',\n  syntax=\'proto3\',\n  serialized_options=_b(\'\\n\\030org.tensorflow.frameworkB\\017AttrValueProtosP\\001\\370\\001\\001\'),\n  serialized_pb=_b(\'\\n#tensorboardX/proto/attr_value.proto\\x12\\x0ctensorboardX\\x1a\\x1ftensorboardX/proto/tensor.proto\\x1a%tensorboardX/proto/tensor_shape.proto\\x1a\\x1etensorboardX/proto/types.proto\\""\\xb8\\x04\\n\\tAttrValue\\x12\\x0b\\n\\x01s\\x18\\x02 \\x01(\\x0cH\\x00\\x12\\x0b\\n\\x01i\\x18\\x03 \\x01(\\x03H\\x00\\x12\\x0b\\n\\x01\\x66\\x18\\x04 \\x01(\\x02H\\x00\\x12\\x0b\\n\\x01\\x62\\x18\\x05 \\x01(\\x08H\\x00\\x12&\\n\\x04type\\x18\\x06 \\x01(\\x0e\\x32\\x16.tensorboardX.DataTypeH\\x00\\x12/\\n\\x05shape\\x18\\x07 \\x01(\\x0b\\x32\\x1e.tensorboardX.TensorShapeProtoH\\x00\\x12+\\n\\x06tensor\\x18\\x08 \\x01(\\x0b\\x32\\x19.tensorboardX.TensorProtoH\\x00\\x12\\x31\\n\\x04list\\x18\\x01 \\x01(\\x0b\\x32!.tensorboardX.AttrValue.ListValueH\\x00\\x12*\\n\\x04\\x66unc\\x18\\n \\x01(\\x0b\\x32\\x1a.tensorboardX.NameAttrListH\\x00\\x12\\x15\\n\\x0bplaceholder\\x18\\t \\x01(\\tH\\x00\\x1a\\xf1\\x01\\n\\tListValue\\x12\\t\\n\\x01s\\x18\\x02 \\x03(\\x0c\\x12\\r\\n\\x01i\\x18\\x03 \\x03(\\x03\\x42\\x02\\x10\\x01\\x12\\r\\n\\x01\\x66\\x18\\x04 \\x03(\\x02\\x42\\x02\\x10\\x01\\x12\\r\\n\\x01\\x62\\x18\\x05 \\x03(\\x08\\x42\\x02\\x10\\x01\\x12(\\n\\x04type\\x18\\x06 \\x03(\\x0e\\x32\\x16.tensorboardX.DataTypeB\\x02\\x10\\x01\\x12-\\n\\x05shape\\x18\\x07 \\x03(\\x0b\\x32\\x1e.tensorboardX.TensorShapeProto\\x12)\\n\\x06tensor\\x18\\x08 \\x03(\\x0b\\x32\\x19.tensorboardX.TensorProto\\x12(\\n\\x04\\x66unc\\x18\\t \\x03(\\x0b\\x32\\x1a.tensorboardX.NameAttrListB\\x07\\n\\x05value\\""\\x96\\x01\\n\\x0cNameAttrList\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x32\\n\\x04\\x61ttr\\x18\\x02 \\x03(\\x0b\\x32$.tensorboardX.NameAttrList.AttrEntry\\x1a\\x44\\n\\tAttrEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12&\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x17.tensorboardX.AttrValue:\\x02\\x38\\x01\\x42\\x30\\n\\x18org.tensorflow.frameworkB\\x0f\\x41ttrValueProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorboardX_dot_proto_dot_tensor__pb2.DESCRIPTOR,tensorboardX_dot_proto_dot_tensor__shape__pb2.DESCRIPTOR,tensorboardX_dot_proto_dot_types__pb2.DESCRIPTOR,])\n\n\n\n\n_ATTRVALUE_LISTVALUE = _descriptor.Descriptor(\n  name=\'ListValue\',\n  full_name=\'tensorboardX.AttrValue.ListValue\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'s\', full_name=\'tensorboardX.AttrValue.ListValue.s\', index=0,\n      number=2, type=12, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'i\', full_name=\'tensorboardX.AttrValue.ListValue.i\', index=1,\n      number=3, type=3, cpp_type=2, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=_b(\'\\020\\001\'), file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'f\', full_name=\'tensorboardX.AttrValue.ListValue.f\', index=2,\n      number=4, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=_b(\'\\020\\001\'), file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'b\', full_name=\'tensorboardX.AttrValue.ListValue.b\', index=3,\n      number=5, type=8, cpp_type=7, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=_b(\'\\020\\001\'), file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'type\', full_name=\'tensorboardX.AttrValue.ListValue.type\', index=4,\n      number=6, type=14, cpp_type=8, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=_b(\'\\020\\001\'), file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'shape\', full_name=\'tensorboardX.AttrValue.ListValue.shape\', index=5,\n      number=7, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'tensor\', full_name=\'tensorboardX.AttrValue.ListValue.tensor\', index=6,\n      number=8, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'func\', full_name=\'tensorboardX.AttrValue.ListValue.func\', index=7,\n      number=9, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=476,\n  serialized_end=717,\n)\n\n_ATTRVALUE = _descriptor.Descriptor(\n  name=\'AttrValue\',\n  full_name=\'tensorboardX.AttrValue\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'s\', full_name=\'tensorboardX.AttrValue.s\', index=0,\n      number=2, type=12, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(""""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'i\', full_name=\'tensorboardX.AttrValue.i\', index=1,\n      number=3, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'f\', full_name=\'tensorboardX.AttrValue.f\', index=2,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'b\', full_name=\'tensorboardX.AttrValue.b\', index=3,\n      number=5, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'type\', full_name=\'tensorboardX.AttrValue.type\', index=4,\n      number=6, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'shape\', full_name=\'tensorboardX.AttrValue.shape\', index=5,\n      number=7, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'tensor\', full_name=\'tensorboardX.AttrValue.tensor\', index=6,\n      number=8, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'list\', full_name=\'tensorboardX.AttrValue.list\', index=7,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'func\', full_name=\'tensorboardX.AttrValue.func\', index=8,\n      number=10, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'placeholder\', full_name=\'tensorboardX.AttrValue.placeholder\', index=9,\n      number=9, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_ATTRVALUE_LISTVALUE, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'value\', full_name=\'tensorboardX.AttrValue.value\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=158,\n  serialized_end=726,\n)\n\n\n_NAMEATTRLIST_ATTRENTRY = _descriptor.Descriptor(\n  name=\'AttrEntry\',\n  full_name=\'tensorboardX.NameAttrList.AttrEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'tensorboardX.NameAttrList.AttrEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorboardX.NameAttrList.AttrEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=_b(\'8\\001\'),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=811,\n  serialized_end=879,\n)\n\n_NAMEATTRLIST = _descriptor.Descriptor(\n  name=\'NameAttrList\',\n  full_name=\'tensorboardX.NameAttrList\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorboardX.NameAttrList.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'attr\', full_name=\'tensorboardX.NameAttrList.attr\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_NAMEATTRLIST_ATTRENTRY, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=729,\n  serialized_end=879,\n)\n\n_ATTRVALUE_LISTVALUE.fields_by_name[\'type\'].enum_type = tensorboardX_dot_proto_dot_types__pb2._DATATYPE\n_ATTRVALUE_LISTVALUE.fields_by_name[\'shape\'].message_type = tensorboardX_dot_proto_dot_tensor__shape__pb2._TENSORSHAPEPROTO\n_ATTRVALUE_LISTVALUE.fields_by_name[\'tensor\'].message_type = tensorboardX_dot_proto_dot_tensor__pb2._TENSORPROTO\n_ATTRVALUE_LISTVALUE.fields_by_name[\'func\'].message_type = _NAMEATTRLIST\n_ATTRVALUE_LISTVALUE.containing_type = _ATTRVALUE\n_ATTRVALUE.fields_by_name[\'type\'].enum_type = tensorboardX_dot_proto_dot_types__pb2._DATATYPE\n_ATTRVALUE.fields_by_name[\'shape\'].message_type = tensorboardX_dot_proto_dot_tensor__shape__pb2._TENSORSHAPEPROTO\n_ATTRVALUE.fields_by_name[\'tensor\'].message_type = tensorboardX_dot_proto_dot_tensor__pb2._TENSORPROTO\n_ATTRVALUE.fields_by_name[\'list\'].message_type = _ATTRVALUE_LISTVALUE\n_ATTRVALUE.fields_by_name[\'func\'].message_type = _NAMEATTRLIST\n_ATTRVALUE.oneofs_by_name[\'value\'].fields.append(\n  _ATTRVALUE.fields_by_name[\'s\'])\n_ATTRVALUE.fields_by_name[\'s\'].containing_oneof = _ATTRVALUE.oneofs_by_name[\'value\']\n_ATTRVALUE.oneofs_by_name[\'value\'].fields.append(\n  _ATTRVALUE.fields_by_name[\'i\'])\n_ATTRVALUE.fields_by_name[\'i\'].containing_oneof = _ATTRVALUE.oneofs_by_name[\'value\']\n_ATTRVALUE.oneofs_by_name[\'value\'].fields.append(\n  _ATTRVALUE.fields_by_name[\'f\'])\n_ATTRVALUE.fields_by_name[\'f\'].containing_oneof = _ATTRVALUE.oneofs_by_name[\'value\']\n_ATTRVALUE.oneofs_by_name[\'value\'].fields.append(\n  _ATTRVALUE.fields_by_name[\'b\'])\n_ATTRVALUE.fields_by_name[\'b\'].containing_oneof = _ATTRVALUE.oneofs_by_name[\'value\']\n_ATTRVALUE.oneofs_by_name[\'value\'].fields.append(\n  _ATTRVALUE.fields_by_name[\'type\'])\n_ATTRVALUE.fields_by_name[\'type\'].containing_oneof = _ATTRVALUE.oneofs_by_name[\'value\']\n_ATTRVALUE.oneofs_by_name[\'value\'].fields.append(\n  _ATTRVALUE.fields_by_name[\'shape\'])\n_ATTRVALUE.fields_by_name[\'shape\'].containing_oneof = _ATTRVALUE.oneofs_by_name[\'value\']\n_ATTRVALUE.oneofs_by_name[\'value\'].fields.append(\n  _ATTRVALUE.fields_by_name[\'tensor\'])\n_ATTRVALUE.fields_by_name[\'tensor\'].containing_oneof = _ATTRVALUE.oneofs_by_name[\'value\']\n_ATTRVALUE.oneofs_by_name[\'value\'].fields.append(\n  _ATTRVALUE.fields_by_name[\'list\'])\n_ATTRVALUE.fields_by_name[\'list\'].containing_oneof = _ATTRVALUE.oneofs_by_name[\'value\']\n_ATTRVALUE.oneofs_by_name[\'value\'].fields.append(\n  _ATTRVALUE.fields_by_name[\'func\'])\n_ATTRVALUE.fields_by_name[\'func\'].containing_oneof = _ATTRVALUE.oneofs_by_name[\'value\']\n_ATTRVALUE.oneofs_by_name[\'value\'].fields.append(\n  _ATTRVALUE.fields_by_name[\'placeholder\'])\n_ATTRVALUE.fields_by_name[\'placeholder\'].containing_oneof = _ATTRVALUE.oneofs_by_name[\'value\']\n_NAMEATTRLIST_ATTRENTRY.fields_by_name[\'value\'].message_type = _ATTRVALUE\n_NAMEATTRLIST_ATTRENTRY.containing_type = _NAMEATTRLIST\n_NAMEATTRLIST.fields_by_name[\'attr\'].message_type = _NAMEATTRLIST_ATTRENTRY\nDESCRIPTOR.message_types_by_name[\'AttrValue\'] = _ATTRVALUE\nDESCRIPTOR.message_types_by_name[\'NameAttrList\'] = _NAMEATTRLIST\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nAttrValue = _reflection.GeneratedProtocolMessageType(\'AttrValue\', (_message.Message,), dict(\n\n  ListValue = _reflection.GeneratedProtocolMessageType(\'ListValue\', (_message.Message,), dict(\n    DESCRIPTOR = _ATTRVALUE_LISTVALUE,\n    __module__ = \'tensorboardX.proto.attr_value_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorboardX.AttrValue.ListValue)\n    ))\n  ,\n  DESCRIPTOR = _ATTRVALUE,\n  __module__ = \'tensorboardX.proto.attr_value_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.AttrValue)\n  ))\n_sym_db.RegisterMessage(AttrValue)\n_sym_db.RegisterMessage(AttrValue.ListValue)\n\nNameAttrList = _reflection.GeneratedProtocolMessageType(\'NameAttrList\', (_message.Message,), dict(\n\n  AttrEntry = _reflection.GeneratedProtocolMessageType(\'AttrEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _NAMEATTRLIST_ATTRENTRY,\n    __module__ = \'tensorboardX.proto.attr_value_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorboardX.NameAttrList.AttrEntry)\n    ))\n  ,\n  DESCRIPTOR = _NAMEATTRLIST,\n  __module__ = \'tensorboardX.proto.attr_value_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.NameAttrList)\n  ))\n_sym_db.RegisterMessage(NameAttrList)\n_sym_db.RegisterMessage(NameAttrList.AttrEntry)\n\n\nDESCRIPTOR._options = None\n_ATTRVALUE_LISTVALUE.fields_by_name[\'i\']._options = None\n_ATTRVALUE_LISTVALUE.fields_by_name[\'f\']._options = None\n_ATTRVALUE_LISTVALUE.fields_by_name[\'b\']._options = None\n_ATTRVALUE_LISTVALUE.fields_by_name[\'type\']._options = None\n_NAMEATTRLIST_ATTRENTRY._options = None\n# @@protoc_insertion_point(module_scope)\n'"
tensorboardX/tensorboardX/proto/event_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorboardX/proto/event.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorboardX.proto import summary_pb2 as tensorboardX_dot_proto_dot_summary__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorboardX/proto/event.proto\',\n  package=\'tensorboardX\',\n  syntax=\'proto3\',\n  serialized_options=_b(\'\\n\\023org.tensorflow.utilB\\013EventProtosP\\001\\370\\001\\001\'),\n  serialized_pb=_b(\'\\n\\x1etensorboardX/proto/event.proto\\x12\\x0ctensorboardX\\x1a tensorboardX/proto/summary.proto\\""\\xc3\\x02\\n\\x05\\x45vent\\x12\\x11\\n\\twall_time\\x18\\x01 \\x01(\\x01\\x12\\x0c\\n\\x04step\\x18\\x02 \\x01(\\x03\\x12\\x16\\n\\x0c\\x66ile_version\\x18\\x03 \\x01(\\tH\\x00\\x12\\x13\\n\\tgraph_def\\x18\\x04 \\x01(\\x0cH\\x00\\x12(\\n\\x07summary\\x18\\x05 \\x01(\\x0b\\x32\\x15.tensorboardX.SummaryH\\x00\\x12/\\n\\x0blog_message\\x18\\x06 \\x01(\\x0b\\x32\\x18.tensorboardX.LogMessageH\\x00\\x12/\\n\\x0bsession_log\\x18\\x07 \\x01(\\x0b\\x32\\x18.tensorboardX.SessionLogH\\x00\\x12>\\n\\x13tagged_run_metadata\\x18\\x08 \\x01(\\x0b\\x32\\x1f.tensorboardX.TaggedRunMetadataH\\x00\\x12\\x18\\n\\x0emeta_graph_def\\x18\\t \\x01(\\x0cH\\x00\\x42\\x06\\n\\x04what\\""\\x97\\x01\\n\\nLogMessage\\x12-\\n\\x05level\\x18\\x01 \\x01(\\x0e\\x32\\x1e.tensorboardX.LogMessage.Level\\x12\\x0f\\n\\x07message\\x18\\x02 \\x01(\\t\\""I\\n\\x05Level\\x12\\x0b\\n\\x07UNKNOWN\\x10\\x00\\x12\\t\\n\\x05\\x44\\x45\\x42UG\\x10\\n\\x12\\x08\\n\\x04INFO\\x10\\x14\\x12\\x08\\n\\x04WARN\\x10\\x1e\\x12\\t\\n\\x05\\x45RROR\\x10(\\x12\\t\\n\\x05\\x46\\x41TAL\\x10\\x32\\""\\xb8\\x01\\n\\nSessionLog\\x12\\x36\\n\\x06status\\x18\\x01 \\x01(\\x0e\\x32&.tensorboardX.SessionLog.SessionStatus\\x12\\x17\\n\\x0f\\x63heckpoint_path\\x18\\x02 \\x01(\\t\\x12\\x0b\\n\\x03msg\\x18\\x03 \\x01(\\t\\""L\\n\\rSessionStatus\\x12\\x16\\n\\x12STATUS_UNSPECIFIED\\x10\\x00\\x12\\t\\n\\x05START\\x10\\x01\\x12\\x08\\n\\x04STOP\\x10\\x02\\x12\\x0e\\n\\nCHECKPOINT\\x10\\x03\\""6\\n\\x11TaggedRunMetadata\\x12\\x0b\\n\\x03tag\\x18\\x01 \\x01(\\t\\x12\\x14\\n\\x0crun_metadata\\x18\\x02 \\x01(\\x0c\\x42\\\'\\n\\x13org.tensorflow.utilB\\x0b\\x45ventProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorboardX_dot_proto_dot_summary__pb2.DESCRIPTOR,])\n\n\n\n_LOGMESSAGE_LEVEL = _descriptor.EnumDescriptor(\n  name=\'Level\',\n  full_name=\'tensorboardX.LogMessage.Level\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'UNKNOWN\', index=0, number=0,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'DEBUG\', index=1, number=10,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'INFO\', index=2, number=20,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'WARN\', index=3, number=30,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'ERROR\', index=4, number=40,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'FATAL\', index=5, number=50,\n      serialized_options=None,\n      type=None),\n  ],\n  containing_type=None,\n  serialized_options=None,\n  serialized_start=487,\n  serialized_end=560,\n)\n_sym_db.RegisterEnumDescriptor(_LOGMESSAGE_LEVEL)\n\n_SESSIONLOG_SESSIONSTATUS = _descriptor.EnumDescriptor(\n  name=\'SessionStatus\',\n  full_name=\'tensorboardX.SessionLog.SessionStatus\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'STATUS_UNSPECIFIED\', index=0, number=0,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'START\', index=1, number=1,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'STOP\', index=2, number=2,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CHECKPOINT\', index=3, number=3,\n      serialized_options=None,\n      type=None),\n  ],\n  containing_type=None,\n  serialized_options=None,\n  serialized_start=671,\n  serialized_end=747,\n)\n_sym_db.RegisterEnumDescriptor(_SESSIONLOG_SESSIONSTATUS)\n\n\n_EVENT = _descriptor.Descriptor(\n  name=\'Event\',\n  full_name=\'tensorboardX.Event\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'wall_time\', full_name=\'tensorboardX.Event.wall_time\', index=0,\n      number=1, type=1, cpp_type=5, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'step\', full_name=\'tensorboardX.Event.step\', index=1,\n      number=2, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'file_version\', full_name=\'tensorboardX.Event.file_version\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'graph_def\', full_name=\'tensorboardX.Event.graph_def\', index=3,\n      number=4, type=12, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(""""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'summary\', full_name=\'tensorboardX.Event.summary\', index=4,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'log_message\', full_name=\'tensorboardX.Event.log_message\', index=5,\n      number=6, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'session_log\', full_name=\'tensorboardX.Event.session_log\', index=6,\n      number=7, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'tagged_run_metadata\', full_name=\'tensorboardX.Event.tagged_run_metadata\', index=7,\n      number=8, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'meta_graph_def\', full_name=\'tensorboardX.Event.meta_graph_def\', index=8,\n      number=9, type=12, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(""""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'what\', full_name=\'tensorboardX.Event.what\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=83,\n  serialized_end=406,\n)\n\n\n_LOGMESSAGE = _descriptor.Descriptor(\n  name=\'LogMessage\',\n  full_name=\'tensorboardX.LogMessage\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'level\', full_name=\'tensorboardX.LogMessage.level\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'message\', full_name=\'tensorboardX.LogMessage.message\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _LOGMESSAGE_LEVEL,\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=409,\n  serialized_end=560,\n)\n\n\n_SESSIONLOG = _descriptor.Descriptor(\n  name=\'SessionLog\',\n  full_name=\'tensorboardX.SessionLog\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'status\', full_name=\'tensorboardX.SessionLog.status\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'checkpoint_path\', full_name=\'tensorboardX.SessionLog.checkpoint_path\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'msg\', full_name=\'tensorboardX.SessionLog.msg\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _SESSIONLOG_SESSIONSTATUS,\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=563,\n  serialized_end=747,\n)\n\n\n_TAGGEDRUNMETADATA = _descriptor.Descriptor(\n  name=\'TaggedRunMetadata\',\n  full_name=\'tensorboardX.TaggedRunMetadata\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'tag\', full_name=\'tensorboardX.TaggedRunMetadata.tag\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'run_metadata\', full_name=\'tensorboardX.TaggedRunMetadata.run_metadata\', index=1,\n      number=2, type=12, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(""""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=749,\n  serialized_end=803,\n)\n\n_EVENT.fields_by_name[\'summary\'].message_type = tensorboardX_dot_proto_dot_summary__pb2._SUMMARY\n_EVENT.fields_by_name[\'log_message\'].message_type = _LOGMESSAGE\n_EVENT.fields_by_name[\'session_log\'].message_type = _SESSIONLOG\n_EVENT.fields_by_name[\'tagged_run_metadata\'].message_type = _TAGGEDRUNMETADATA\n_EVENT.oneofs_by_name[\'what\'].fields.append(\n  _EVENT.fields_by_name[\'file_version\'])\n_EVENT.fields_by_name[\'file_version\'].containing_oneof = _EVENT.oneofs_by_name[\'what\']\n_EVENT.oneofs_by_name[\'what\'].fields.append(\n  _EVENT.fields_by_name[\'graph_def\'])\n_EVENT.fields_by_name[\'graph_def\'].containing_oneof = _EVENT.oneofs_by_name[\'what\']\n_EVENT.oneofs_by_name[\'what\'].fields.append(\n  _EVENT.fields_by_name[\'summary\'])\n_EVENT.fields_by_name[\'summary\'].containing_oneof = _EVENT.oneofs_by_name[\'what\']\n_EVENT.oneofs_by_name[\'what\'].fields.append(\n  _EVENT.fields_by_name[\'log_message\'])\n_EVENT.fields_by_name[\'log_message\'].containing_oneof = _EVENT.oneofs_by_name[\'what\']\n_EVENT.oneofs_by_name[\'what\'].fields.append(\n  _EVENT.fields_by_name[\'session_log\'])\n_EVENT.fields_by_name[\'session_log\'].containing_oneof = _EVENT.oneofs_by_name[\'what\']\n_EVENT.oneofs_by_name[\'what\'].fields.append(\n  _EVENT.fields_by_name[\'tagged_run_metadata\'])\n_EVENT.fields_by_name[\'tagged_run_metadata\'].containing_oneof = _EVENT.oneofs_by_name[\'what\']\n_EVENT.oneofs_by_name[\'what\'].fields.append(\n  _EVENT.fields_by_name[\'meta_graph_def\'])\n_EVENT.fields_by_name[\'meta_graph_def\'].containing_oneof = _EVENT.oneofs_by_name[\'what\']\n_LOGMESSAGE.fields_by_name[\'level\'].enum_type = _LOGMESSAGE_LEVEL\n_LOGMESSAGE_LEVEL.containing_type = _LOGMESSAGE\n_SESSIONLOG.fields_by_name[\'status\'].enum_type = _SESSIONLOG_SESSIONSTATUS\n_SESSIONLOG_SESSIONSTATUS.containing_type = _SESSIONLOG\nDESCRIPTOR.message_types_by_name[\'Event\'] = _EVENT\nDESCRIPTOR.message_types_by_name[\'LogMessage\'] = _LOGMESSAGE\nDESCRIPTOR.message_types_by_name[\'SessionLog\'] = _SESSIONLOG\nDESCRIPTOR.message_types_by_name[\'TaggedRunMetadata\'] = _TAGGEDRUNMETADATA\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nEvent = _reflection.GeneratedProtocolMessageType(\'Event\', (_message.Message,), dict(\n  DESCRIPTOR = _EVENT,\n  __module__ = \'tensorboardX.proto.event_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.Event)\n  ))\n_sym_db.RegisterMessage(Event)\n\nLogMessage = _reflection.GeneratedProtocolMessageType(\'LogMessage\', (_message.Message,), dict(\n  DESCRIPTOR = _LOGMESSAGE,\n  __module__ = \'tensorboardX.proto.event_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.LogMessage)\n  ))\n_sym_db.RegisterMessage(LogMessage)\n\nSessionLog = _reflection.GeneratedProtocolMessageType(\'SessionLog\', (_message.Message,), dict(\n  DESCRIPTOR = _SESSIONLOG,\n  __module__ = \'tensorboardX.proto.event_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.SessionLog)\n  ))\n_sym_db.RegisterMessage(SessionLog)\n\nTaggedRunMetadata = _reflection.GeneratedProtocolMessageType(\'TaggedRunMetadata\', (_message.Message,), dict(\n  DESCRIPTOR = _TAGGEDRUNMETADATA,\n  __module__ = \'tensorboardX.proto.event_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.TaggedRunMetadata)\n  ))\n_sym_db.RegisterMessage(TaggedRunMetadata)\n\n\nDESCRIPTOR._options = None\n# @@protoc_insertion_point(module_scope)\n'"
tensorboardX/tensorboardX/proto/graph_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorboardX/proto/graph.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorboardX.proto import node_def_pb2 as tensorboardX_dot_proto_dot_node__def__pb2\nfrom tensorboardX.proto import versions_pb2 as tensorboardX_dot_proto_dot_versions__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorboardX/proto/graph.proto\',\n  package=\'tensorboardX\',\n  syntax=\'proto3\',\n  serialized_options=_b(\'\\n\\030org.tensorflow.frameworkB\\013GraphProtosP\\001\\370\\001\\001\'),\n  serialized_pb=_b(\'\\n\\x1etensorboardX/proto/graph.proto\\x12\\x0ctensorboardX\\x1a!tensorboardX/proto/node_def.proto\\x1a!tensorboardX/proto/versions.proto\\""p\\n\\x08GraphDef\\x12#\\n\\x04node\\x18\\x01 \\x03(\\x0b\\x32\\x15.tensorboardX.NodeDef\\x12*\\n\\x08versions\\x18\\x04 \\x01(\\x0b\\x32\\x18.tensorboardX.VersionDef\\x12\\x13\\n\\x07version\\x18\\x03 \\x01(\\x05\\x42\\x02\\x18\\x01\\x42,\\n\\x18org.tensorflow.frameworkB\\x0bGraphProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorboardX_dot_proto_dot_node__def__pb2.DESCRIPTOR,tensorboardX_dot_proto_dot_versions__pb2.DESCRIPTOR,])\n\n\n\n\n_GRAPHDEF = _descriptor.Descriptor(\n  name=\'GraphDef\',\n  full_name=\'tensorboardX.GraphDef\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'node\', full_name=\'tensorboardX.GraphDef.node\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'versions\', full_name=\'tensorboardX.GraphDef.versions\', index=1,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'version\', full_name=\'tensorboardX.GraphDef.version\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=_b(\'\\030\\001\'), file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=118,\n  serialized_end=230,\n)\n\n_GRAPHDEF.fields_by_name[\'node\'].message_type = tensorboardX_dot_proto_dot_node__def__pb2._NODEDEF\n_GRAPHDEF.fields_by_name[\'versions\'].message_type = tensorboardX_dot_proto_dot_versions__pb2._VERSIONDEF\nDESCRIPTOR.message_types_by_name[\'GraphDef\'] = _GRAPHDEF\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nGraphDef = _reflection.GeneratedProtocolMessageType(\'GraphDef\', (_message.Message,), dict(\n  DESCRIPTOR = _GRAPHDEF,\n  __module__ = \'tensorboardX.proto.graph_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.GraphDef)\n  ))\n_sym_db.RegisterMessage(GraphDef)\n\n\nDESCRIPTOR._options = None\n_GRAPHDEF.fields_by_name[\'version\']._options = None\n# @@protoc_insertion_point(module_scope)\n'"
tensorboardX/tensorboardX/proto/layout_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorboardX/proto/layout.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorboardX/proto/layout.proto\',\n  package=\'tensorboardX\',\n  syntax=\'proto3\',\n  serialized_options=None,\n  serialized_pb=_b(\'\\n\\x1ftensorboardX/proto/layout.proto\\x12\\x0ctensorboardX\\""\\x8f\\x01\\n\\x05\\x43hart\\x12\\r\\n\\x05title\\x18\\x01 \\x01(\\t\\x12\\x38\\n\\tmultiline\\x18\\x02 \\x01(\\x0b\\x32#.tensorboardX.MultilineChartContentH\\x00\\x12\\x32\\n\\x06margin\\x18\\x03 \\x01(\\x0b\\x32 .tensorboardX.MarginChartContentH\\x00\\x42\\t\\n\\x07\\x63ontent\\""$\\n\\x15MultilineChartContent\\x12\\x0b\\n\\x03tag\\x18\\x01 \\x03(\\t\\""\\x84\\x01\\n\\x12MarginChartContent\\x12\\x37\\n\\x06series\\x18\\x01 \\x03(\\x0b\\x32\\\'.tensorboardX.MarginChartContent.Series\\x1a\\x35\\n\\x06Series\\x12\\r\\n\\x05value\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05lower\\x18\\x02 \\x01(\\t\\x12\\r\\n\\x05upper\\x18\\x03 \\x01(\\t\\""M\\n\\x08\\x43\\x61tegory\\x12\\r\\n\\x05title\\x18\\x01 \\x01(\\t\\x12\\""\\n\\x05\\x63hart\\x18\\x02 \\x03(\\x0b\\x32\\x13.tensorboardX.Chart\\x12\\x0e\\n\\x06\\x63losed\\x18\\x03 \\x01(\\x08\\""C\\n\\x06Layout\\x12\\x0f\\n\\x07version\\x18\\x01 \\x01(\\x05\\x12(\\n\\x08\\x63\\x61tegory\\x18\\x02 \\x03(\\x0b\\x32\\x16.tensorboardX.Categoryb\\x06proto3\')\n)\n\n\n\n\n_CHART = _descriptor.Descriptor(\n  name=\'Chart\',\n  full_name=\'tensorboardX.Chart\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'title\', full_name=\'tensorboardX.Chart.title\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'multiline\', full_name=\'tensorboardX.Chart.multiline\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'margin\', full_name=\'tensorboardX.Chart.margin\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'content\', full_name=\'tensorboardX.Chart.content\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=50,\n  serialized_end=193,\n)\n\n\n_MULTILINECHARTCONTENT = _descriptor.Descriptor(\n  name=\'MultilineChartContent\',\n  full_name=\'tensorboardX.MultilineChartContent\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'tag\', full_name=\'tensorboardX.MultilineChartContent.tag\', index=0,\n      number=1, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=195,\n  serialized_end=231,\n)\n\n\n_MARGINCHARTCONTENT_SERIES = _descriptor.Descriptor(\n  name=\'Series\',\n  full_name=\'tensorboardX.MarginChartContent.Series\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorboardX.MarginChartContent.Series.value\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'lower\', full_name=\'tensorboardX.MarginChartContent.Series.lower\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'upper\', full_name=\'tensorboardX.MarginChartContent.Series.upper\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=313,\n  serialized_end=366,\n)\n\n_MARGINCHARTCONTENT = _descriptor.Descriptor(\n  name=\'MarginChartContent\',\n  full_name=\'tensorboardX.MarginChartContent\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'series\', full_name=\'tensorboardX.MarginChartContent.series\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_MARGINCHARTCONTENT_SERIES, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=234,\n  serialized_end=366,\n)\n\n\n_CATEGORY = _descriptor.Descriptor(\n  name=\'Category\',\n  full_name=\'tensorboardX.Category\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'title\', full_name=\'tensorboardX.Category.title\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'chart\', full_name=\'tensorboardX.Category.chart\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'closed\', full_name=\'tensorboardX.Category.closed\', index=2,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=368,\n  serialized_end=445,\n)\n\n\n_LAYOUT = _descriptor.Descriptor(\n  name=\'Layout\',\n  full_name=\'tensorboardX.Layout\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'version\', full_name=\'tensorboardX.Layout.version\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'category\', full_name=\'tensorboardX.Layout.category\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=447,\n  serialized_end=514,\n)\n\n_CHART.fields_by_name[\'multiline\'].message_type = _MULTILINECHARTCONTENT\n_CHART.fields_by_name[\'margin\'].message_type = _MARGINCHARTCONTENT\n_CHART.oneofs_by_name[\'content\'].fields.append(\n  _CHART.fields_by_name[\'multiline\'])\n_CHART.fields_by_name[\'multiline\'].containing_oneof = _CHART.oneofs_by_name[\'content\']\n_CHART.oneofs_by_name[\'content\'].fields.append(\n  _CHART.fields_by_name[\'margin\'])\n_CHART.fields_by_name[\'margin\'].containing_oneof = _CHART.oneofs_by_name[\'content\']\n_MARGINCHARTCONTENT_SERIES.containing_type = _MARGINCHARTCONTENT\n_MARGINCHARTCONTENT.fields_by_name[\'series\'].message_type = _MARGINCHARTCONTENT_SERIES\n_CATEGORY.fields_by_name[\'chart\'].message_type = _CHART\n_LAYOUT.fields_by_name[\'category\'].message_type = _CATEGORY\nDESCRIPTOR.message_types_by_name[\'Chart\'] = _CHART\nDESCRIPTOR.message_types_by_name[\'MultilineChartContent\'] = _MULTILINECHARTCONTENT\nDESCRIPTOR.message_types_by_name[\'MarginChartContent\'] = _MARGINCHARTCONTENT\nDESCRIPTOR.message_types_by_name[\'Category\'] = _CATEGORY\nDESCRIPTOR.message_types_by_name[\'Layout\'] = _LAYOUT\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nChart = _reflection.GeneratedProtocolMessageType(\'Chart\', (_message.Message,), dict(\n  DESCRIPTOR = _CHART,\n  __module__ = \'tensorboardX.proto.layout_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.Chart)\n  ))\n_sym_db.RegisterMessage(Chart)\n\nMultilineChartContent = _reflection.GeneratedProtocolMessageType(\'MultilineChartContent\', (_message.Message,), dict(\n  DESCRIPTOR = _MULTILINECHARTCONTENT,\n  __module__ = \'tensorboardX.proto.layout_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.MultilineChartContent)\n  ))\n_sym_db.RegisterMessage(MultilineChartContent)\n\nMarginChartContent = _reflection.GeneratedProtocolMessageType(\'MarginChartContent\', (_message.Message,), dict(\n\n  Series = _reflection.GeneratedProtocolMessageType(\'Series\', (_message.Message,), dict(\n    DESCRIPTOR = _MARGINCHARTCONTENT_SERIES,\n    __module__ = \'tensorboardX.proto.layout_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorboardX.MarginChartContent.Series)\n    ))\n  ,\n  DESCRIPTOR = _MARGINCHARTCONTENT,\n  __module__ = \'tensorboardX.proto.layout_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.MarginChartContent)\n  ))\n_sym_db.RegisterMessage(MarginChartContent)\n_sym_db.RegisterMessage(MarginChartContent.Series)\n\nCategory = _reflection.GeneratedProtocolMessageType(\'Category\', (_message.Message,), dict(\n  DESCRIPTOR = _CATEGORY,\n  __module__ = \'tensorboardX.proto.layout_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.Category)\n  ))\n_sym_db.RegisterMessage(Category)\n\nLayout = _reflection.GeneratedProtocolMessageType(\'Layout\', (_message.Message,), dict(\n  DESCRIPTOR = _LAYOUT,\n  __module__ = \'tensorboardX.proto.layout_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.Layout)\n  ))\n_sym_db.RegisterMessage(Layout)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
tensorboardX/tensorboardX/proto/node_def_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorboardX/proto/node_def.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorboardX.proto import attr_value_pb2 as tensorboardX_dot_proto_dot_attr__value__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorboardX/proto/node_def.proto\',\n  package=\'tensorboardX\',\n  syntax=\'proto3\',\n  serialized_options=_b(\'\\n\\030org.tensorflow.frameworkB\\tNodeProtoP\\001\\370\\001\\001\'),\n  serialized_pb=_b(\'\\n!tensorboardX/proto/node_def.proto\\x12\\x0ctensorboardX\\x1a#tensorboardX/proto/attr_value.proto\\""\\xb7\\x01\\n\\x07NodeDef\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\n\\n\\x02op\\x18\\x02 \\x01(\\t\\x12\\r\\n\\x05input\\x18\\x03 \\x03(\\t\\x12\\x0e\\n\\x06\\x64\\x65vice\\x18\\x04 \\x01(\\t\\x12-\\n\\x04\\x61ttr\\x18\\x05 \\x03(\\x0b\\x32\\x1f.tensorboardX.NodeDef.AttrEntry\\x1a\\x44\\n\\tAttrEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12&\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x17.tensorboardX.AttrValue:\\x02\\x38\\x01\\x42*\\n\\x18org.tensorflow.frameworkB\\tNodeProtoP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorboardX_dot_proto_dot_attr__value__pb2.DESCRIPTOR,])\n\n\n\n\n_NODEDEF_ATTRENTRY = _descriptor.Descriptor(\n  name=\'AttrEntry\',\n  full_name=\'tensorboardX.NodeDef.AttrEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'tensorboardX.NodeDef.AttrEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorboardX.NodeDef.AttrEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=_b(\'8\\001\'),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=204,\n  serialized_end=272,\n)\n\n_NODEDEF = _descriptor.Descriptor(\n  name=\'NodeDef\',\n  full_name=\'tensorboardX.NodeDef\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorboardX.NodeDef.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'op\', full_name=\'tensorboardX.NodeDef.op\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'input\', full_name=\'tensorboardX.NodeDef.input\', index=2,\n      number=3, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'device\', full_name=\'tensorboardX.NodeDef.device\', index=3,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'attr\', full_name=\'tensorboardX.NodeDef.attr\', index=4,\n      number=5, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_NODEDEF_ATTRENTRY, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=89,\n  serialized_end=272,\n)\n\n_NODEDEF_ATTRENTRY.fields_by_name[\'value\'].message_type = tensorboardX_dot_proto_dot_attr__value__pb2._ATTRVALUE\n_NODEDEF_ATTRENTRY.containing_type = _NODEDEF\n_NODEDEF.fields_by_name[\'attr\'].message_type = _NODEDEF_ATTRENTRY\nDESCRIPTOR.message_types_by_name[\'NodeDef\'] = _NODEDEF\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nNodeDef = _reflection.GeneratedProtocolMessageType(\'NodeDef\', (_message.Message,), dict(\n\n  AttrEntry = _reflection.GeneratedProtocolMessageType(\'AttrEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _NODEDEF_ATTRENTRY,\n    __module__ = \'tensorboardX.proto.node_def_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorboardX.NodeDef.AttrEntry)\n    ))\n  ,\n  DESCRIPTOR = _NODEDEF,\n  __module__ = \'tensorboardX.proto.node_def_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.NodeDef)\n  ))\n_sym_db.RegisterMessage(NodeDef)\n_sym_db.RegisterMessage(NodeDef.AttrEntry)\n\n\nDESCRIPTOR._options = None\n_NODEDEF_ATTRENTRY._options = None\n# @@protoc_insertion_point(module_scope)\n'"
tensorboardX/tensorboardX/proto/plugin_hparams_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorboardX/proto/plugin_hparams.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorboardX.proto import api_pb2 as tensorboardX_dot_proto_dot_api__pb2\nfrom google.protobuf import struct_pb2 as google_dot_protobuf_dot_struct__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorboardX/proto/plugin_hparams.proto\',\n  package=\'tensorboardX.hparam\',\n  syntax=\'proto3\',\n  serialized_options=None,\n  serialized_pb=_b(\'\\n\\\'tensorboardX/proto/plugin_hparams.proto\\x12\\x13tensorboardX.hparam\\x1a\\x1ctensorboardX/proto/api.proto\\x1a\\x1cgoogle/protobuf/struct.proto\\""\\xe9\\x01\\n\\x11HParamsPluginData\\x12\\x0f\\n\\x07version\\x18\\x01 \\x01(\\x05\\x12\\x35\\n\\nexperiment\\x18\\x02 \\x01(\\x0b\\x32\\x1f.tensorboardX.hparam.ExperimentH\\x00\\x12\\x43\\n\\x12session_start_info\\x18\\x03 \\x01(\\x0b\\x32%.tensorboardX.hparam.SessionStartInfoH\\x00\\x12?\\n\\x10session_end_info\\x18\\x04 \\x01(\\x0b\\x32#.tensorboardX.hparam.SessionEndInfoH\\x00\\x42\\x06\\n\\x04\\x64\\x61ta\\""\\xf4\\x01\\n\\x10SessionStartInfo\\x12\\x43\\n\\x07hparams\\x18\\x01 \\x03(\\x0b\\x32\\x32.tensorboardX.hparam.SessionStartInfo.HparamsEntry\\x12\\x11\\n\\tmodel_uri\\x18\\x02 \\x01(\\t\\x12\\x13\\n\\x0bmonitor_url\\x18\\x03 \\x01(\\t\\x12\\x12\\n\\ngroup_name\\x18\\x04 \\x01(\\t\\x12\\x17\\n\\x0fstart_time_secs\\x18\\x05 \\x01(\\x01\\x1a\\x46\\n\\x0cHparamsEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12%\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x16.google.protobuf.Value:\\x02\\x38\\x01\\""T\\n\\x0eSessionEndInfo\\x12+\\n\\x06status\\x18\\x01 \\x01(\\x0e\\x32\\x1b.tensorboardX.hparam.Status\\x12\\x15\\n\\rend_time_secs\\x18\\x02 \\x01(\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorboardX_dot_proto_dot_api__pb2.DESCRIPTOR,google_dot_protobuf_dot_struct__pb2.DESCRIPTOR,])\n\n\n\n\n_HPARAMSPLUGINDATA = _descriptor.Descriptor(\n  name=\'HParamsPluginData\',\n  full_name=\'tensorboardX.hparam.HParamsPluginData\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'version\', full_name=\'tensorboardX.hparam.HParamsPluginData.version\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'experiment\', full_name=\'tensorboardX.hparam.HParamsPluginData.experiment\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'session_start_info\', full_name=\'tensorboardX.hparam.HParamsPluginData.session_start_info\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'session_end_info\', full_name=\'tensorboardX.hparam.HParamsPluginData.session_end_info\', index=3,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'data\', full_name=\'tensorboardX.hparam.HParamsPluginData.data\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=125,\n  serialized_end=358,\n)\n\n\n_SESSIONSTARTINFO_HPARAMSENTRY = _descriptor.Descriptor(\n  name=\'HparamsEntry\',\n  full_name=\'tensorboardX.hparam.SessionStartInfo.HparamsEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'tensorboardX.hparam.SessionStartInfo.HparamsEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorboardX.hparam.SessionStartInfo.HparamsEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=_b(\'8\\001\'),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=535,\n  serialized_end=605,\n)\n\n_SESSIONSTARTINFO = _descriptor.Descriptor(\n  name=\'SessionStartInfo\',\n  full_name=\'tensorboardX.hparam.SessionStartInfo\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'hparams\', full_name=\'tensorboardX.hparam.SessionStartInfo.hparams\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'model_uri\', full_name=\'tensorboardX.hparam.SessionStartInfo.model_uri\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'monitor_url\', full_name=\'tensorboardX.hparam.SessionStartInfo.monitor_url\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'group_name\', full_name=\'tensorboardX.hparam.SessionStartInfo.group_name\', index=3,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'start_time_secs\', full_name=\'tensorboardX.hparam.SessionStartInfo.start_time_secs\', index=4,\n      number=5, type=1, cpp_type=5, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_SESSIONSTARTINFO_HPARAMSENTRY, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=361,\n  serialized_end=605,\n)\n\n\n_SESSIONENDINFO = _descriptor.Descriptor(\n  name=\'SessionEndInfo\',\n  full_name=\'tensorboardX.hparam.SessionEndInfo\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'status\', full_name=\'tensorboardX.hparam.SessionEndInfo.status\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'end_time_secs\', full_name=\'tensorboardX.hparam.SessionEndInfo.end_time_secs\', index=1,\n      number=2, type=1, cpp_type=5, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=607,\n  serialized_end=691,\n)\n\n_HPARAMSPLUGINDATA.fields_by_name[\'experiment\'].message_type = tensorboardX_dot_proto_dot_api__pb2._EXPERIMENT\n_HPARAMSPLUGINDATA.fields_by_name[\'session_start_info\'].message_type = _SESSIONSTARTINFO\n_HPARAMSPLUGINDATA.fields_by_name[\'session_end_info\'].message_type = _SESSIONENDINFO\n_HPARAMSPLUGINDATA.oneofs_by_name[\'data\'].fields.append(\n  _HPARAMSPLUGINDATA.fields_by_name[\'experiment\'])\n_HPARAMSPLUGINDATA.fields_by_name[\'experiment\'].containing_oneof = _HPARAMSPLUGINDATA.oneofs_by_name[\'data\']\n_HPARAMSPLUGINDATA.oneofs_by_name[\'data\'].fields.append(\n  _HPARAMSPLUGINDATA.fields_by_name[\'session_start_info\'])\n_HPARAMSPLUGINDATA.fields_by_name[\'session_start_info\'].containing_oneof = _HPARAMSPLUGINDATA.oneofs_by_name[\'data\']\n_HPARAMSPLUGINDATA.oneofs_by_name[\'data\'].fields.append(\n  _HPARAMSPLUGINDATA.fields_by_name[\'session_end_info\'])\n_HPARAMSPLUGINDATA.fields_by_name[\'session_end_info\'].containing_oneof = _HPARAMSPLUGINDATA.oneofs_by_name[\'data\']\n_SESSIONSTARTINFO_HPARAMSENTRY.fields_by_name[\'value\'].message_type = google_dot_protobuf_dot_struct__pb2._VALUE\n_SESSIONSTARTINFO_HPARAMSENTRY.containing_type = _SESSIONSTARTINFO\n_SESSIONSTARTINFO.fields_by_name[\'hparams\'].message_type = _SESSIONSTARTINFO_HPARAMSENTRY\n_SESSIONENDINFO.fields_by_name[\'status\'].enum_type = tensorboardX_dot_proto_dot_api__pb2._STATUS\nDESCRIPTOR.message_types_by_name[\'HParamsPluginData\'] = _HPARAMSPLUGINDATA\nDESCRIPTOR.message_types_by_name[\'SessionStartInfo\'] = _SESSIONSTARTINFO\nDESCRIPTOR.message_types_by_name[\'SessionEndInfo\'] = _SESSIONENDINFO\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nHParamsPluginData = _reflection.GeneratedProtocolMessageType(\'HParamsPluginData\', (_message.Message,), dict(\n  DESCRIPTOR = _HPARAMSPLUGINDATA,\n  __module__ = \'tensorboardX.proto.plugin_hparams_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.hparam.HParamsPluginData)\n  ))\n_sym_db.RegisterMessage(HParamsPluginData)\n\nSessionStartInfo = _reflection.GeneratedProtocolMessageType(\'SessionStartInfo\', (_message.Message,), dict(\n\n  HparamsEntry = _reflection.GeneratedProtocolMessageType(\'HparamsEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _SESSIONSTARTINFO_HPARAMSENTRY,\n    __module__ = \'tensorboardX.proto.plugin_hparams_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorboardX.hparam.SessionStartInfo.HparamsEntry)\n    ))\n  ,\n  DESCRIPTOR = _SESSIONSTARTINFO,\n  __module__ = \'tensorboardX.proto.plugin_hparams_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.hparam.SessionStartInfo)\n  ))\n_sym_db.RegisterMessage(SessionStartInfo)\n_sym_db.RegisterMessage(SessionStartInfo.HparamsEntry)\n\nSessionEndInfo = _reflection.GeneratedProtocolMessageType(\'SessionEndInfo\', (_message.Message,), dict(\n  DESCRIPTOR = _SESSIONENDINFO,\n  __module__ = \'tensorboardX.proto.plugin_hparams_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.hparam.SessionEndInfo)\n  ))\n_sym_db.RegisterMessage(SessionEndInfo)\n\n\n_SESSIONSTARTINFO_HPARAMSENTRY._options = None\n# @@protoc_insertion_point(module_scope)\n'"
tensorboardX/tensorboardX/proto/plugin_mesh_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorboardX/proto/plugin_mesh.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorboardX/proto/plugin_mesh.proto\',\n  package=\'tensorboardX.mesh\',\n  syntax=\'proto3\',\n  serialized_options=None,\n  serialized_pb=_b(\'\\n$tensorboardX/proto/plugin_mesh.proto\\x12\\x11tensorboardX.mesh\\""\\xd7\\x01\\n\\x0eMeshPluginData\\x12\\x0f\\n\\x07version\\x18\\x01 \\x01(\\x05\\x12\\x0c\\n\\x04name\\x18\\x02 \\x01(\\t\\x12\\x43\\n\\x0c\\x63ontent_type\\x18\\x03 \\x01(\\x0e\\x32-.tensorboardX.mesh.MeshPluginData.ContentType\\x12\\x13\\n\\x0bjson_config\\x18\\x05 \\x01(\\t\\x12\\r\\n\\x05shape\\x18\\x06 \\x03(\\x05\\""=\\n\\x0b\\x43ontentType\\x12\\r\\n\\tUNDEFINED\\x10\\x00\\x12\\n\\n\\x06VERTEX\\x10\\x01\\x12\\x08\\n\\x04\\x46\\x41\\x43\\x45\\x10\\x02\\x12\\t\\n\\x05\\x43OLOR\\x10\\x03\\x62\\x06proto3\')\n)\n\n\n\n_MESHPLUGINDATA_CONTENTTYPE = _descriptor.EnumDescriptor(\n  name=\'ContentType\',\n  full_name=\'tensorboardX.mesh.MeshPluginData.ContentType\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'UNDEFINED\', index=0, number=0,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'VERTEX\', index=1, number=1,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'FACE\', index=2, number=2,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'COLOR\', index=3, number=3,\n      serialized_options=None,\n      type=None),\n  ],\n  containing_type=None,\n  serialized_options=None,\n  serialized_start=214,\n  serialized_end=275,\n)\n_sym_db.RegisterEnumDescriptor(_MESHPLUGINDATA_CONTENTTYPE)\n\n\n_MESHPLUGINDATA = _descriptor.Descriptor(\n  name=\'MeshPluginData\',\n  full_name=\'tensorboardX.mesh.MeshPluginData\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'version\', full_name=\'tensorboardX.mesh.MeshPluginData.version\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorboardX.mesh.MeshPluginData.name\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'content_type\', full_name=\'tensorboardX.mesh.MeshPluginData.content_type\', index=2,\n      number=3, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'json_config\', full_name=\'tensorboardX.mesh.MeshPluginData.json_config\', index=3,\n      number=5, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'shape\', full_name=\'tensorboardX.mesh.MeshPluginData.shape\', index=4,\n      number=6, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _MESHPLUGINDATA_CONTENTTYPE,\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=60,\n  serialized_end=275,\n)\n\n_MESHPLUGINDATA.fields_by_name[\'content_type\'].enum_type = _MESHPLUGINDATA_CONTENTTYPE\n_MESHPLUGINDATA_CONTENTTYPE.containing_type = _MESHPLUGINDATA\nDESCRIPTOR.message_types_by_name[\'MeshPluginData\'] = _MESHPLUGINDATA\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nMeshPluginData = _reflection.GeneratedProtocolMessageType(\'MeshPluginData\', (_message.Message,), dict(\n  DESCRIPTOR = _MESHPLUGINDATA,\n  __module__ = \'tensorboardX.proto.plugin_mesh_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.mesh.MeshPluginData)\n  ))\n_sym_db.RegisterMessage(MeshPluginData)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
tensorboardX/tensorboardX/proto/plugin_pr_curve_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorboardX/proto/plugin_pr_curve.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorboardX/proto/plugin_pr_curve.proto\',\n  package=\'tensorboardX\',\n  syntax=\'proto3\',\n  serialized_options=None,\n  serialized_pb=_b(\'\\n(tensorboardX/proto/plugin_pr_curve.proto\\x12\\x0ctensorboardX\\""<\\n\\x11PrCurvePluginData\\x12\\x0f\\n\\x07version\\x18\\x01 \\x01(\\x05\\x12\\x16\\n\\x0enum_thresholds\\x18\\x02 \\x01(\\rb\\x06proto3\')\n)\n\n\n\n\n_PRCURVEPLUGINDATA = _descriptor.Descriptor(\n  name=\'PrCurvePluginData\',\n  full_name=\'tensorboardX.PrCurvePluginData\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'version\', full_name=\'tensorboardX.PrCurvePluginData.version\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'num_thresholds\', full_name=\'tensorboardX.PrCurvePluginData.num_thresholds\', index=1,\n      number=2, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=58,\n  serialized_end=118,\n)\n\nDESCRIPTOR.message_types_by_name[\'PrCurvePluginData\'] = _PRCURVEPLUGINDATA\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nPrCurvePluginData = _reflection.GeneratedProtocolMessageType(\'PrCurvePluginData\', (_message.Message,), dict(\n  DESCRIPTOR = _PRCURVEPLUGINDATA,\n  __module__ = \'tensorboardX.proto.plugin_pr_curve_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.PrCurvePluginData)\n  ))\n_sym_db.RegisterMessage(PrCurvePluginData)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
tensorboardX/tensorboardX/proto/plugin_text_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorboardX/proto/plugin_text.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorboardX/proto/plugin_text.proto\',\n  package=\'tensorboardX\',\n  syntax=\'proto3\',\n  serialized_options=None,\n  serialized_pb=_b(\'\\n$tensorboardX/proto/plugin_text.proto\\x12\\x0ctensorboardX\\""!\\n\\x0eTextPluginData\\x12\\x0f\\n\\x07version\\x18\\x01 \\x01(\\x05\\x62\\x06proto3\')\n)\n\n\n\n\n_TEXTPLUGINDATA = _descriptor.Descriptor(\n  name=\'TextPluginData\',\n  full_name=\'tensorboardX.TextPluginData\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'version\', full_name=\'tensorboardX.TextPluginData.version\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=54,\n  serialized_end=87,\n)\n\nDESCRIPTOR.message_types_by_name[\'TextPluginData\'] = _TEXTPLUGINDATA\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nTextPluginData = _reflection.GeneratedProtocolMessageType(\'TextPluginData\', (_message.Message,), dict(\n  DESCRIPTOR = _TEXTPLUGINDATA,\n  __module__ = \'tensorboardX.proto.plugin_text_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.TextPluginData)\n  ))\n_sym_db.RegisterMessage(TextPluginData)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
tensorboardX/tensorboardX/proto/resource_handle_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorboardX/proto/resource_handle.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorboardX/proto/resource_handle.proto\',\n  package=\'tensorboardX\',\n  syntax=\'proto3\',\n  serialized_options=_b(\'\\n\\030org.tensorflow.frameworkB\\016ResourceHandleP\\001\\370\\001\\001\'),\n  serialized_pb=_b(\'\\n(tensorboardX/proto/resource_handle.proto\\x12\\x0ctensorboardX\\""r\\n\\x13ResourceHandleProto\\x12\\x0e\\n\\x06\\x64\\x65vice\\x18\\x01 \\x01(\\t\\x12\\x11\\n\\tcontainer\\x18\\x02 \\x01(\\t\\x12\\x0c\\n\\x04name\\x18\\x03 \\x01(\\t\\x12\\x11\\n\\thash_code\\x18\\x04 \\x01(\\x04\\x12\\x17\\n\\x0fmaybe_type_name\\x18\\x05 \\x01(\\tB/\\n\\x18org.tensorflow.frameworkB\\x0eResourceHandleP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n)\n\n\n\n\n_RESOURCEHANDLEPROTO = _descriptor.Descriptor(\n  name=\'ResourceHandleProto\',\n  full_name=\'tensorboardX.ResourceHandleProto\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'device\', full_name=\'tensorboardX.ResourceHandleProto.device\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'container\', full_name=\'tensorboardX.ResourceHandleProto.container\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorboardX.ResourceHandleProto.name\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'hash_code\', full_name=\'tensorboardX.ResourceHandleProto.hash_code\', index=3,\n      number=4, type=4, cpp_type=4, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'maybe_type_name\', full_name=\'tensorboardX.ResourceHandleProto.maybe_type_name\', index=4,\n      number=5, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=58,\n  serialized_end=172,\n)\n\nDESCRIPTOR.message_types_by_name[\'ResourceHandleProto\'] = _RESOURCEHANDLEPROTO\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nResourceHandleProto = _reflection.GeneratedProtocolMessageType(\'ResourceHandleProto\', (_message.Message,), dict(\n  DESCRIPTOR = _RESOURCEHANDLEPROTO,\n  __module__ = \'tensorboardX.proto.resource_handle_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.ResourceHandleProto)\n  ))\n_sym_db.RegisterMessage(ResourceHandleProto)\n\n\nDESCRIPTOR._options = None\n# @@protoc_insertion_point(module_scope)\n'"
tensorboardX/tensorboardX/proto/step_stats_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorboardX/proto/step_stats.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorboardX/proto/step_stats.proto\',\n  package=\'tensorboardX\',\n  syntax=\'proto3\',\n  serialized_options=_b(\'\\n\\030org.tensorflow.frameworkB\\017StepStatsProtosP\\001Z=github.com/tensorflow/tensorflow/tensorflow/go/core/framework\\370\\001\\001\'),\n  serialized_pb=_b(\'\\n#tensorboardX/proto/step_stats.proto\\x12\\x0ctensorboardX\\""=\\n\\x10\\x41llocationRecord\\x12\\x14\\n\\x0c\\x61lloc_micros\\x18\\x01 \\x01(\\x03\\x12\\x13\\n\\x0b\\x61lloc_bytes\\x18\\x02 \\x01(\\x03\\""\\xc6\\x01\\n\\x13\\x41llocatorMemoryUsed\\x12\\x16\\n\\x0e\\x61llocator_name\\x18\\x01 \\x01(\\t\\x12\\x13\\n\\x0btotal_bytes\\x18\\x02 \\x01(\\x03\\x12\\x12\\n\\npeak_bytes\\x18\\x03 \\x01(\\x03\\x12\\x12\\n\\nlive_bytes\\x18\\x04 \\x01(\\x03\\x12:\\n\\x12\\x61llocation_records\\x18\\x06 \\x03(\\x0b\\x32\\x1e.tensorboardX.AllocationRecord\\x12\\x1e\\n\\x16\\x61llocator_bytes_in_use\\x18\\x05 \\x01(\\x03\\""\\x1a\\n\\nNodeOutput\\x12\\x0c\\n\\x04slot\\x18\\x01 \\x01(\\x05\\""\\xec\\x01\\n\\x0bMemoryStats\\x12\\x18\\n\\x10temp_memory_size\\x18\\x01 \\x01(\\x03\\x12\\x1e\\n\\x16persistent_memory_size\\x18\\x03 \\x01(\\x03\\x12#\\n\\x1bpersistent_tensor_alloc_ids\\x18\\x05 \\x03(\\x03\\x12#\\n\\x17\\x64\\x65vice_temp_memory_size\\x18\\x02 \\x01(\\x03\\x42\\x02\\x18\\x01\\x12)\\n\\x1d\\x64\\x65vice_persistent_memory_size\\x18\\x04 \\x01(\\x03\\x42\\x02\\x18\\x01\\x12.\\n\\""device_persistent_tensor_alloc_ids\\x18\\x06 \\x03(\\x03\\x42\\x02\\x18\\x01\\""\\xe3\\x02\\n\\rNodeExecStats\\x12\\x11\\n\\tnode_name\\x18\\x01 \\x01(\\t\\x12\\x18\\n\\x10\\x61ll_start_micros\\x18\\x02 \\x01(\\x03\\x12\\x1b\\n\\x13op_start_rel_micros\\x18\\x03 \\x01(\\x03\\x12\\x19\\n\\x11op_end_rel_micros\\x18\\x04 \\x01(\\x03\\x12\\x1a\\n\\x12\\x61ll_end_rel_micros\\x18\\x05 \\x01(\\x03\\x12\\x31\\n\\x06memory\\x18\\x06 \\x03(\\x0b\\x32!.tensorboardX.AllocatorMemoryUsed\\x12(\\n\\x06output\\x18\\x07 \\x03(\\x0b\\x32\\x18.tensorboardX.NodeOutput\\x12\\x16\\n\\x0etimeline_label\\x18\\x08 \\x01(\\t\\x12\\x18\\n\\x10scheduled_micros\\x18\\t \\x01(\\x03\\x12\\x11\\n\\tthread_id\\x18\\n \\x01(\\r\\x12/\\n\\x0cmemory_stats\\x18\\x0c \\x01(\\x0b\\x32\\x19.tensorboardX.MemoryStats\\""R\\n\\x0f\\x44\\x65viceStepStats\\x12\\x0e\\n\\x06\\x64\\x65vice\\x18\\x01 \\x01(\\t\\x12/\\n\\nnode_stats\\x18\\x02 \\x03(\\x0b\\x32\\x1b.tensorboardX.NodeExecStats\\""=\\n\\tStepStats\\x12\\x30\\n\\tdev_stats\\x18\\x01 \\x03(\\x0b\\x32\\x1d.tensorboardX.DeviceStepStats\\"":\\n\\x0bRunMetadata\\x12+\\n\\nstep_stats\\x18\\x01 \\x01(\\x0b\\x32\\x17.tensorboardX.StepStatsBo\\n\\x18org.tensorflow.frameworkB\\x0fStepStatsProtosP\\x01Z=github.com/tensorflow/tensorflow/tensorflow/go/core/framework\\xf8\\x01\\x01\\x62\\x06proto3\')\n)\n\n\n\n\n_ALLOCATIONRECORD = _descriptor.Descriptor(\n  name=\'AllocationRecord\',\n  full_name=\'tensorboardX.AllocationRecord\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'alloc_micros\', full_name=\'tensorboardX.AllocationRecord.alloc_micros\', index=0,\n      number=1, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'alloc_bytes\', full_name=\'tensorboardX.AllocationRecord.alloc_bytes\', index=1,\n      number=2, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=53,\n  serialized_end=114,\n)\n\n\n_ALLOCATORMEMORYUSED = _descriptor.Descriptor(\n  name=\'AllocatorMemoryUsed\',\n  full_name=\'tensorboardX.AllocatorMemoryUsed\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'allocator_name\', full_name=\'tensorboardX.AllocatorMemoryUsed.allocator_name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'total_bytes\', full_name=\'tensorboardX.AllocatorMemoryUsed.total_bytes\', index=1,\n      number=2, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'peak_bytes\', full_name=\'tensorboardX.AllocatorMemoryUsed.peak_bytes\', index=2,\n      number=3, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'live_bytes\', full_name=\'tensorboardX.AllocatorMemoryUsed.live_bytes\', index=3,\n      number=4, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'allocation_records\', full_name=\'tensorboardX.AllocatorMemoryUsed.allocation_records\', index=4,\n      number=6, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'allocator_bytes_in_use\', full_name=\'tensorboardX.AllocatorMemoryUsed.allocator_bytes_in_use\', index=5,\n      number=5, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=117,\n  serialized_end=315,\n)\n\n\n_NODEOUTPUT = _descriptor.Descriptor(\n  name=\'NodeOutput\',\n  full_name=\'tensorboardX.NodeOutput\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'slot\', full_name=\'tensorboardX.NodeOutput.slot\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=317,\n  serialized_end=343,\n)\n\n\n_MEMORYSTATS = _descriptor.Descriptor(\n  name=\'MemoryStats\',\n  full_name=\'tensorboardX.MemoryStats\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'temp_memory_size\', full_name=\'tensorboardX.MemoryStats.temp_memory_size\', index=0,\n      number=1, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'persistent_memory_size\', full_name=\'tensorboardX.MemoryStats.persistent_memory_size\', index=1,\n      number=3, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'persistent_tensor_alloc_ids\', full_name=\'tensorboardX.MemoryStats.persistent_tensor_alloc_ids\', index=2,\n      number=5, type=3, cpp_type=2, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'device_temp_memory_size\', full_name=\'tensorboardX.MemoryStats.device_temp_memory_size\', index=3,\n      number=2, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=_b(\'\\030\\001\'), file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'device_persistent_memory_size\', full_name=\'tensorboardX.MemoryStats.device_persistent_memory_size\', index=4,\n      number=4, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=_b(\'\\030\\001\'), file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'device_persistent_tensor_alloc_ids\', full_name=\'tensorboardX.MemoryStats.device_persistent_tensor_alloc_ids\', index=5,\n      number=6, type=3, cpp_type=2, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=_b(\'\\030\\001\'), file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=346,\n  serialized_end=582,\n)\n\n\n_NODEEXECSTATS = _descriptor.Descriptor(\n  name=\'NodeExecStats\',\n  full_name=\'tensorboardX.NodeExecStats\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'node_name\', full_name=\'tensorboardX.NodeExecStats.node_name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'all_start_micros\', full_name=\'tensorboardX.NodeExecStats.all_start_micros\', index=1,\n      number=2, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'op_start_rel_micros\', full_name=\'tensorboardX.NodeExecStats.op_start_rel_micros\', index=2,\n      number=3, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'op_end_rel_micros\', full_name=\'tensorboardX.NodeExecStats.op_end_rel_micros\', index=3,\n      number=4, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'all_end_rel_micros\', full_name=\'tensorboardX.NodeExecStats.all_end_rel_micros\', index=4,\n      number=5, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'memory\', full_name=\'tensorboardX.NodeExecStats.memory\', index=5,\n      number=6, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'output\', full_name=\'tensorboardX.NodeExecStats.output\', index=6,\n      number=7, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'timeline_label\', full_name=\'tensorboardX.NodeExecStats.timeline_label\', index=7,\n      number=8, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'scheduled_micros\', full_name=\'tensorboardX.NodeExecStats.scheduled_micros\', index=8,\n      number=9, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'thread_id\', full_name=\'tensorboardX.NodeExecStats.thread_id\', index=9,\n      number=10, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'memory_stats\', full_name=\'tensorboardX.NodeExecStats.memory_stats\', index=10,\n      number=12, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=585,\n  serialized_end=940,\n)\n\n\n_DEVICESTEPSTATS = _descriptor.Descriptor(\n  name=\'DeviceStepStats\',\n  full_name=\'tensorboardX.DeviceStepStats\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'device\', full_name=\'tensorboardX.DeviceStepStats.device\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'node_stats\', full_name=\'tensorboardX.DeviceStepStats.node_stats\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=942,\n  serialized_end=1024,\n)\n\n\n_STEPSTATS = _descriptor.Descriptor(\n  name=\'StepStats\',\n  full_name=\'tensorboardX.StepStats\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'dev_stats\', full_name=\'tensorboardX.StepStats.dev_stats\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1026,\n  serialized_end=1087,\n)\n\n\n_RUNMETADATA = _descriptor.Descriptor(\n  name=\'RunMetadata\',\n  full_name=\'tensorboardX.RunMetadata\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'step_stats\', full_name=\'tensorboardX.RunMetadata.step_stats\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1089,\n  serialized_end=1147,\n)\n\n_ALLOCATORMEMORYUSED.fields_by_name[\'allocation_records\'].message_type = _ALLOCATIONRECORD\n_NODEEXECSTATS.fields_by_name[\'memory\'].message_type = _ALLOCATORMEMORYUSED\n_NODEEXECSTATS.fields_by_name[\'output\'].message_type = _NODEOUTPUT\n_NODEEXECSTATS.fields_by_name[\'memory_stats\'].message_type = _MEMORYSTATS\n_DEVICESTEPSTATS.fields_by_name[\'node_stats\'].message_type = _NODEEXECSTATS\n_STEPSTATS.fields_by_name[\'dev_stats\'].message_type = _DEVICESTEPSTATS\n_RUNMETADATA.fields_by_name[\'step_stats\'].message_type = _STEPSTATS\nDESCRIPTOR.message_types_by_name[\'AllocationRecord\'] = _ALLOCATIONRECORD\nDESCRIPTOR.message_types_by_name[\'AllocatorMemoryUsed\'] = _ALLOCATORMEMORYUSED\nDESCRIPTOR.message_types_by_name[\'NodeOutput\'] = _NODEOUTPUT\nDESCRIPTOR.message_types_by_name[\'MemoryStats\'] = _MEMORYSTATS\nDESCRIPTOR.message_types_by_name[\'NodeExecStats\'] = _NODEEXECSTATS\nDESCRIPTOR.message_types_by_name[\'DeviceStepStats\'] = _DEVICESTEPSTATS\nDESCRIPTOR.message_types_by_name[\'StepStats\'] = _STEPSTATS\nDESCRIPTOR.message_types_by_name[\'RunMetadata\'] = _RUNMETADATA\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nAllocationRecord = _reflection.GeneratedProtocolMessageType(\'AllocationRecord\', (_message.Message,), dict(\n  DESCRIPTOR = _ALLOCATIONRECORD,\n  __module__ = \'tensorboardX.proto.step_stats_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.AllocationRecord)\n  ))\n_sym_db.RegisterMessage(AllocationRecord)\n\nAllocatorMemoryUsed = _reflection.GeneratedProtocolMessageType(\'AllocatorMemoryUsed\', (_message.Message,), dict(\n  DESCRIPTOR = _ALLOCATORMEMORYUSED,\n  __module__ = \'tensorboardX.proto.step_stats_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.AllocatorMemoryUsed)\n  ))\n_sym_db.RegisterMessage(AllocatorMemoryUsed)\n\nNodeOutput = _reflection.GeneratedProtocolMessageType(\'NodeOutput\', (_message.Message,), dict(\n  DESCRIPTOR = _NODEOUTPUT,\n  __module__ = \'tensorboardX.proto.step_stats_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.NodeOutput)\n  ))\n_sym_db.RegisterMessage(NodeOutput)\n\nMemoryStats = _reflection.GeneratedProtocolMessageType(\'MemoryStats\', (_message.Message,), dict(\n  DESCRIPTOR = _MEMORYSTATS,\n  __module__ = \'tensorboardX.proto.step_stats_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.MemoryStats)\n  ))\n_sym_db.RegisterMessage(MemoryStats)\n\nNodeExecStats = _reflection.GeneratedProtocolMessageType(\'NodeExecStats\', (_message.Message,), dict(\n  DESCRIPTOR = _NODEEXECSTATS,\n  __module__ = \'tensorboardX.proto.step_stats_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.NodeExecStats)\n  ))\n_sym_db.RegisterMessage(NodeExecStats)\n\nDeviceStepStats = _reflection.GeneratedProtocolMessageType(\'DeviceStepStats\', (_message.Message,), dict(\n  DESCRIPTOR = _DEVICESTEPSTATS,\n  __module__ = \'tensorboardX.proto.step_stats_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.DeviceStepStats)\n  ))\n_sym_db.RegisterMessage(DeviceStepStats)\n\nStepStats = _reflection.GeneratedProtocolMessageType(\'StepStats\', (_message.Message,), dict(\n  DESCRIPTOR = _STEPSTATS,\n  __module__ = \'tensorboardX.proto.step_stats_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.StepStats)\n  ))\n_sym_db.RegisterMessage(StepStats)\n\nRunMetadata = _reflection.GeneratedProtocolMessageType(\'RunMetadata\', (_message.Message,), dict(\n  DESCRIPTOR = _RUNMETADATA,\n  __module__ = \'tensorboardX.proto.step_stats_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.RunMetadata)\n  ))\n_sym_db.RegisterMessage(RunMetadata)\n\n\nDESCRIPTOR._options = None\n_MEMORYSTATS.fields_by_name[\'device_temp_memory_size\']._options = None\n_MEMORYSTATS.fields_by_name[\'device_persistent_memory_size\']._options = None\n_MEMORYSTATS.fields_by_name[\'device_persistent_tensor_alloc_ids\']._options = None\n# @@protoc_insertion_point(module_scope)\n'"
tensorboardX/tensorboardX/proto/summary_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorboardX/proto/summary.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorboardX.proto import tensor_pb2 as tensorboardX_dot_proto_dot_tensor__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorboardX/proto/summary.proto\',\n  package=\'tensorboardX\',\n  syntax=\'proto3\',\n  serialized_options=_b(\'\\n\\030org.tensorflow.frameworkB\\rSummaryProtosP\\001\\370\\001\\001\'),\n  serialized_pb=_b(\'\\n tensorboardX/proto/summary.proto\\x12\\x0ctensorboardX\\x1a\\x1ftensorboardX/proto/tensor.proto\\""\\\'\\n\\x12SummaryDescription\\x12\\x11\\n\\ttype_hint\\x18\\x01 \\x01(\\t\\""\\x87\\x01\\n\\x0eHistogramProto\\x12\\x0b\\n\\x03min\\x18\\x01 \\x01(\\x01\\x12\\x0b\\n\\x03max\\x18\\x02 \\x01(\\x01\\x12\\x0b\\n\\x03num\\x18\\x03 \\x01(\\x01\\x12\\x0b\\n\\x03sum\\x18\\x04 \\x01(\\x01\\x12\\x13\\n\\x0bsum_squares\\x18\\x05 \\x01(\\x01\\x12\\x18\\n\\x0c\\x62ucket_limit\\x18\\x06 \\x03(\\x01\\x42\\x02\\x10\\x01\\x12\\x12\\n\\x06\\x62ucket\\x18\\x07 \\x03(\\x01\\x42\\x02\\x10\\x01\\""\\xb7\\x01\\n\\x0fSummaryMetadata\\x12=\\n\\x0bplugin_data\\x18\\x01 \\x01(\\x0b\\x32(.tensorboardX.SummaryMetadata.PluginData\\x12\\x14\\n\\x0c\\x64isplay_name\\x18\\x02 \\x01(\\t\\x12\\x1b\\n\\x13summary_description\\x18\\x03 \\x01(\\t\\x1a\\x32\\n\\nPluginData\\x12\\x13\\n\\x0bplugin_name\\x18\\x01 \\x01(\\t\\x12\\x0f\\n\\x07\\x63ontent\\x18\\x02 \\x01(\\x0c\\""\\xea\\x04\\n\\x07Summary\\x12*\\n\\x05value\\x18\\x01 \\x03(\\x0b\\x32\\x1b.tensorboardX.Summary.Value\\x1aX\\n\\x05Image\\x12\\x0e\\n\\x06height\\x18\\x01 \\x01(\\x05\\x12\\r\\n\\x05width\\x18\\x02 \\x01(\\x05\\x12\\x12\\n\\ncolorspace\\x18\\x03 \\x01(\\x05\\x12\\x1c\\n\\x14\\x65ncoded_image_string\\x18\\x04 \\x01(\\x0c\\x1a}\\n\\x05\\x41udio\\x12\\x13\\n\\x0bsample_rate\\x18\\x01 \\x01(\\x02\\x12\\x14\\n\\x0cnum_channels\\x18\\x02 \\x01(\\x03\\x12\\x15\\n\\rlength_frames\\x18\\x03 \\x01(\\x03\\x12\\x1c\\n\\x14\\x65ncoded_audio_string\\x18\\x04 \\x01(\\x0c\\x12\\x14\\n\\x0c\\x63ontent_type\\x18\\x05 \\x01(\\t\\x1a\\xd9\\x02\\n\\x05Value\\x12\\x11\\n\\tnode_name\\x18\\x07 \\x01(\\t\\x12\\x0b\\n\\x03tag\\x18\\x01 \\x01(\\t\\x12/\\n\\x08metadata\\x18\\t \\x01(\\x0b\\x32\\x1d.tensorboardX.SummaryMetadata\\x12\\x16\\n\\x0csimple_value\\x18\\x02 \\x01(\\x02H\\x00\\x12&\\n\\x1cobsolete_old_style_histogram\\x18\\x03 \\x01(\\x0cH\\x00\\x12,\\n\\x05image\\x18\\x04 \\x01(\\x0b\\x32\\x1b.tensorboardX.Summary.ImageH\\x00\\x12-\\n\\x05histo\\x18\\x05 \\x01(\\x0b\\x32\\x1c.tensorboardX.HistogramProtoH\\x00\\x12,\\n\\x05\\x61udio\\x18\\x06 \\x01(\\x0b\\x32\\x1b.tensorboardX.Summary.AudioH\\x00\\x12+\\n\\x06tensor\\x18\\x08 \\x01(\\x0b\\x32\\x19.tensorboardX.TensorProtoH\\x00\\x42\\x07\\n\\x05valueB.\\n\\x18org.tensorflow.frameworkB\\rSummaryProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorboardX_dot_proto_dot_tensor__pb2.DESCRIPTOR,])\n\n\n\n\n_SUMMARYDESCRIPTION = _descriptor.Descriptor(\n  name=\'SummaryDescription\',\n  full_name=\'tensorboardX.SummaryDescription\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'type_hint\', full_name=\'tensorboardX.SummaryDescription.type_hint\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=83,\n  serialized_end=122,\n)\n\n\n_HISTOGRAMPROTO = _descriptor.Descriptor(\n  name=\'HistogramProto\',\n  full_name=\'tensorboardX.HistogramProto\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'min\', full_name=\'tensorboardX.HistogramProto.min\', index=0,\n      number=1, type=1, cpp_type=5, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'max\', full_name=\'tensorboardX.HistogramProto.max\', index=1,\n      number=2, type=1, cpp_type=5, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'num\', full_name=\'tensorboardX.HistogramProto.num\', index=2,\n      number=3, type=1, cpp_type=5, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'sum\', full_name=\'tensorboardX.HistogramProto.sum\', index=3,\n      number=4, type=1, cpp_type=5, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'sum_squares\', full_name=\'tensorboardX.HistogramProto.sum_squares\', index=4,\n      number=5, type=1, cpp_type=5, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'bucket_limit\', full_name=\'tensorboardX.HistogramProto.bucket_limit\', index=5,\n      number=6, type=1, cpp_type=5, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=_b(\'\\020\\001\'), file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'bucket\', full_name=\'tensorboardX.HistogramProto.bucket\', index=6,\n      number=7, type=1, cpp_type=5, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=_b(\'\\020\\001\'), file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=125,\n  serialized_end=260,\n)\n\n\n_SUMMARYMETADATA_PLUGINDATA = _descriptor.Descriptor(\n  name=\'PluginData\',\n  full_name=\'tensorboardX.SummaryMetadata.PluginData\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'plugin_name\', full_name=\'tensorboardX.SummaryMetadata.PluginData.plugin_name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'content\', full_name=\'tensorboardX.SummaryMetadata.PluginData.content\', index=1,\n      number=2, type=12, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(""""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=396,\n  serialized_end=446,\n)\n\n_SUMMARYMETADATA = _descriptor.Descriptor(\n  name=\'SummaryMetadata\',\n  full_name=\'tensorboardX.SummaryMetadata\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'plugin_data\', full_name=\'tensorboardX.SummaryMetadata.plugin_data\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'display_name\', full_name=\'tensorboardX.SummaryMetadata.display_name\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'summary_description\', full_name=\'tensorboardX.SummaryMetadata.summary_description\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_SUMMARYMETADATA_PLUGINDATA, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=263,\n  serialized_end=446,\n)\n\n\n_SUMMARY_IMAGE = _descriptor.Descriptor(\n  name=\'Image\',\n  full_name=\'tensorboardX.Summary.Image\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'height\', full_name=\'tensorboardX.Summary.Image.height\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'width\', full_name=\'tensorboardX.Summary.Image.width\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'colorspace\', full_name=\'tensorboardX.Summary.Image.colorspace\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'encoded_image_string\', full_name=\'tensorboardX.Summary.Image.encoded_image_string\', index=3,\n      number=4, type=12, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(""""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=504,\n  serialized_end=592,\n)\n\n_SUMMARY_AUDIO = _descriptor.Descriptor(\n  name=\'Audio\',\n  full_name=\'tensorboardX.Summary.Audio\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'sample_rate\', full_name=\'tensorboardX.Summary.Audio.sample_rate\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'num_channels\', full_name=\'tensorboardX.Summary.Audio.num_channels\', index=1,\n      number=2, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'length_frames\', full_name=\'tensorboardX.Summary.Audio.length_frames\', index=2,\n      number=3, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'encoded_audio_string\', full_name=\'tensorboardX.Summary.Audio.encoded_audio_string\', index=3,\n      number=4, type=12, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(""""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'content_type\', full_name=\'tensorboardX.Summary.Audio.content_type\', index=4,\n      number=5, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=594,\n  serialized_end=719,\n)\n\n_SUMMARY_VALUE = _descriptor.Descriptor(\n  name=\'Value\',\n  full_name=\'tensorboardX.Summary.Value\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'node_name\', full_name=\'tensorboardX.Summary.Value.node_name\', index=0,\n      number=7, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'tag\', full_name=\'tensorboardX.Summary.Value.tag\', index=1,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'metadata\', full_name=\'tensorboardX.Summary.Value.metadata\', index=2,\n      number=9, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'simple_value\', full_name=\'tensorboardX.Summary.Value.simple_value\', index=3,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'obsolete_old_style_histogram\', full_name=\'tensorboardX.Summary.Value.obsolete_old_style_histogram\', index=4,\n      number=3, type=12, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(""""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'image\', full_name=\'tensorboardX.Summary.Value.image\', index=5,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'histo\', full_name=\'tensorboardX.Summary.Value.histo\', index=6,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'audio\', full_name=\'tensorboardX.Summary.Value.audio\', index=7,\n      number=6, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'tensor\', full_name=\'tensorboardX.Summary.Value.tensor\', index=8,\n      number=8, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'value\', full_name=\'tensorboardX.Summary.Value.value\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=722,\n  serialized_end=1067,\n)\n\n_SUMMARY = _descriptor.Descriptor(\n  name=\'Summary\',\n  full_name=\'tensorboardX.Summary\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorboardX.Summary.value\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_SUMMARY_IMAGE, _SUMMARY_AUDIO, _SUMMARY_VALUE, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=449,\n  serialized_end=1067,\n)\n\n_SUMMARYMETADATA_PLUGINDATA.containing_type = _SUMMARYMETADATA\n_SUMMARYMETADATA.fields_by_name[\'plugin_data\'].message_type = _SUMMARYMETADATA_PLUGINDATA\n_SUMMARY_IMAGE.containing_type = _SUMMARY\n_SUMMARY_AUDIO.containing_type = _SUMMARY\n_SUMMARY_VALUE.fields_by_name[\'metadata\'].message_type = _SUMMARYMETADATA\n_SUMMARY_VALUE.fields_by_name[\'image\'].message_type = _SUMMARY_IMAGE\n_SUMMARY_VALUE.fields_by_name[\'histo\'].message_type = _HISTOGRAMPROTO\n_SUMMARY_VALUE.fields_by_name[\'audio\'].message_type = _SUMMARY_AUDIO\n_SUMMARY_VALUE.fields_by_name[\'tensor\'].message_type = tensorboardX_dot_proto_dot_tensor__pb2._TENSORPROTO\n_SUMMARY_VALUE.containing_type = _SUMMARY\n_SUMMARY_VALUE.oneofs_by_name[\'value\'].fields.append(\n  _SUMMARY_VALUE.fields_by_name[\'simple_value\'])\n_SUMMARY_VALUE.fields_by_name[\'simple_value\'].containing_oneof = _SUMMARY_VALUE.oneofs_by_name[\'value\']\n_SUMMARY_VALUE.oneofs_by_name[\'value\'].fields.append(\n  _SUMMARY_VALUE.fields_by_name[\'obsolete_old_style_histogram\'])\n_SUMMARY_VALUE.fields_by_name[\'obsolete_old_style_histogram\'].containing_oneof = _SUMMARY_VALUE.oneofs_by_name[\'value\']\n_SUMMARY_VALUE.oneofs_by_name[\'value\'].fields.append(\n  _SUMMARY_VALUE.fields_by_name[\'image\'])\n_SUMMARY_VALUE.fields_by_name[\'image\'].containing_oneof = _SUMMARY_VALUE.oneofs_by_name[\'value\']\n_SUMMARY_VALUE.oneofs_by_name[\'value\'].fields.append(\n  _SUMMARY_VALUE.fields_by_name[\'histo\'])\n_SUMMARY_VALUE.fields_by_name[\'histo\'].containing_oneof = _SUMMARY_VALUE.oneofs_by_name[\'value\']\n_SUMMARY_VALUE.oneofs_by_name[\'value\'].fields.append(\n  _SUMMARY_VALUE.fields_by_name[\'audio\'])\n_SUMMARY_VALUE.fields_by_name[\'audio\'].containing_oneof = _SUMMARY_VALUE.oneofs_by_name[\'value\']\n_SUMMARY_VALUE.oneofs_by_name[\'value\'].fields.append(\n  _SUMMARY_VALUE.fields_by_name[\'tensor\'])\n_SUMMARY_VALUE.fields_by_name[\'tensor\'].containing_oneof = _SUMMARY_VALUE.oneofs_by_name[\'value\']\n_SUMMARY.fields_by_name[\'value\'].message_type = _SUMMARY_VALUE\nDESCRIPTOR.message_types_by_name[\'SummaryDescription\'] = _SUMMARYDESCRIPTION\nDESCRIPTOR.message_types_by_name[\'HistogramProto\'] = _HISTOGRAMPROTO\nDESCRIPTOR.message_types_by_name[\'SummaryMetadata\'] = _SUMMARYMETADATA\nDESCRIPTOR.message_types_by_name[\'Summary\'] = _SUMMARY\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nSummaryDescription = _reflection.GeneratedProtocolMessageType(\'SummaryDescription\', (_message.Message,), dict(\n  DESCRIPTOR = _SUMMARYDESCRIPTION,\n  __module__ = \'tensorboardX.proto.summary_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.SummaryDescription)\n  ))\n_sym_db.RegisterMessage(SummaryDescription)\n\nHistogramProto = _reflection.GeneratedProtocolMessageType(\'HistogramProto\', (_message.Message,), dict(\n  DESCRIPTOR = _HISTOGRAMPROTO,\n  __module__ = \'tensorboardX.proto.summary_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.HistogramProto)\n  ))\n_sym_db.RegisterMessage(HistogramProto)\n\nSummaryMetadata = _reflection.GeneratedProtocolMessageType(\'SummaryMetadata\', (_message.Message,), dict(\n\n  PluginData = _reflection.GeneratedProtocolMessageType(\'PluginData\', (_message.Message,), dict(\n    DESCRIPTOR = _SUMMARYMETADATA_PLUGINDATA,\n    __module__ = \'tensorboardX.proto.summary_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorboardX.SummaryMetadata.PluginData)\n    ))\n  ,\n  DESCRIPTOR = _SUMMARYMETADATA,\n  __module__ = \'tensorboardX.proto.summary_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.SummaryMetadata)\n  ))\n_sym_db.RegisterMessage(SummaryMetadata)\n_sym_db.RegisterMessage(SummaryMetadata.PluginData)\n\nSummary = _reflection.GeneratedProtocolMessageType(\'Summary\', (_message.Message,), dict(\n\n  Image = _reflection.GeneratedProtocolMessageType(\'Image\', (_message.Message,), dict(\n    DESCRIPTOR = _SUMMARY_IMAGE,\n    __module__ = \'tensorboardX.proto.summary_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorboardX.Summary.Image)\n    ))\n  ,\n\n  Audio = _reflection.GeneratedProtocolMessageType(\'Audio\', (_message.Message,), dict(\n    DESCRIPTOR = _SUMMARY_AUDIO,\n    __module__ = \'tensorboardX.proto.summary_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorboardX.Summary.Audio)\n    ))\n  ,\n\n  Value = _reflection.GeneratedProtocolMessageType(\'Value\', (_message.Message,), dict(\n    DESCRIPTOR = _SUMMARY_VALUE,\n    __module__ = \'tensorboardX.proto.summary_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorboardX.Summary.Value)\n    ))\n  ,\n  DESCRIPTOR = _SUMMARY,\n  __module__ = \'tensorboardX.proto.summary_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.Summary)\n  ))\n_sym_db.RegisterMessage(Summary)\n_sym_db.RegisterMessage(Summary.Image)\n_sym_db.RegisterMessage(Summary.Audio)\n_sym_db.RegisterMessage(Summary.Value)\n\n\nDESCRIPTOR._options = None\n_HISTOGRAMPROTO.fields_by_name[\'bucket_limit\']._options = None\n_HISTOGRAMPROTO.fields_by_name[\'bucket\']._options = None\n# @@protoc_insertion_point(module_scope)\n'"
tensorboardX/tensorboardX/proto/tensor_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorboardX/proto/tensor.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorboardX.proto import resource_handle_pb2 as tensorboardX_dot_proto_dot_resource__handle__pb2\nfrom tensorboardX.proto import tensor_shape_pb2 as tensorboardX_dot_proto_dot_tensor__shape__pb2\nfrom tensorboardX.proto import types_pb2 as tensorboardX_dot_proto_dot_types__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorboardX/proto/tensor.proto\',\n  package=\'tensorboardX\',\n  syntax=\'proto3\',\n  serialized_options=_b(\'\\n\\030org.tensorflow.frameworkB\\014TensorProtosP\\001\\370\\001\\001\'),\n  serialized_pb=_b(\'\\n\\x1ftensorboardX/proto/tensor.proto\\x12\\x0ctensorboardX\\x1a(tensorboardX/proto/resource_handle.proto\\x1a%tensorboardX/proto/tensor_shape.proto\\x1a\\x1etensorboardX/proto/types.proto\\""\\xa9\\x03\\n\\x0bTensorProto\\x12%\\n\\x05\\x64type\\x18\\x01 \\x01(\\x0e\\x32\\x16.tensorboardX.DataType\\x12\\x34\\n\\x0ctensor_shape\\x18\\x02 \\x01(\\x0b\\x32\\x1e.tensorboardX.TensorShapeProto\\x12\\x16\\n\\x0eversion_number\\x18\\x03 \\x01(\\x05\\x12\\x16\\n\\x0etensor_content\\x18\\x04 \\x01(\\x0c\\x12\\x14\\n\\x08half_val\\x18\\r \\x03(\\x05\\x42\\x02\\x10\\x01\\x12\\x15\\n\\tfloat_val\\x18\\x05 \\x03(\\x02\\x42\\x02\\x10\\x01\\x12\\x16\\n\\ndouble_val\\x18\\x06 \\x03(\\x01\\x42\\x02\\x10\\x01\\x12\\x13\\n\\x07int_val\\x18\\x07 \\x03(\\x05\\x42\\x02\\x10\\x01\\x12\\x12\\n\\nstring_val\\x18\\x08 \\x03(\\x0c\\x12\\x18\\n\\x0cscomplex_val\\x18\\t \\x03(\\x02\\x42\\x02\\x10\\x01\\x12\\x15\\n\\tint64_val\\x18\\n \\x03(\\x03\\x42\\x02\\x10\\x01\\x12\\x14\\n\\x08\\x62ool_val\\x18\\x0b \\x03(\\x08\\x42\\x02\\x10\\x01\\x12\\x18\\n\\x0c\\x64\\x63omplex_val\\x18\\x0c \\x03(\\x01\\x42\\x02\\x10\\x01\\x12>\\n\\x13resource_handle_val\\x18\\x0e \\x03(\\x0b\\x32!.tensorboardX.ResourceHandleProtoB-\\n\\x18org.tensorflow.frameworkB\\x0cTensorProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorboardX_dot_proto_dot_resource__handle__pb2.DESCRIPTOR,tensorboardX_dot_proto_dot_tensor__shape__pb2.DESCRIPTOR,tensorboardX_dot_proto_dot_types__pb2.DESCRIPTOR,])\n\n\n\n\n_TENSORPROTO = _descriptor.Descriptor(\n  name=\'TensorProto\',\n  full_name=\'tensorboardX.TensorProto\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'dtype\', full_name=\'tensorboardX.TensorProto.dtype\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'tensor_shape\', full_name=\'tensorboardX.TensorProto.tensor_shape\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'version_number\', full_name=\'tensorboardX.TensorProto.version_number\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'tensor_content\', full_name=\'tensorboardX.TensorProto.tensor_content\', index=3,\n      number=4, type=12, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(""""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'half_val\', full_name=\'tensorboardX.TensorProto.half_val\', index=4,\n      number=13, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=_b(\'\\020\\001\'), file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'float_val\', full_name=\'tensorboardX.TensorProto.float_val\', index=5,\n      number=5, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=_b(\'\\020\\001\'), file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'double_val\', full_name=\'tensorboardX.TensorProto.double_val\', index=6,\n      number=6, type=1, cpp_type=5, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=_b(\'\\020\\001\'), file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'int_val\', full_name=\'tensorboardX.TensorProto.int_val\', index=7,\n      number=7, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=_b(\'\\020\\001\'), file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'string_val\', full_name=\'tensorboardX.TensorProto.string_val\', index=8,\n      number=8, type=12, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'scomplex_val\', full_name=\'tensorboardX.TensorProto.scomplex_val\', index=9,\n      number=9, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=_b(\'\\020\\001\'), file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'int64_val\', full_name=\'tensorboardX.TensorProto.int64_val\', index=10,\n      number=10, type=3, cpp_type=2, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=_b(\'\\020\\001\'), file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'bool_val\', full_name=\'tensorboardX.TensorProto.bool_val\', index=11,\n      number=11, type=8, cpp_type=7, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=_b(\'\\020\\001\'), file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'dcomplex_val\', full_name=\'tensorboardX.TensorProto.dcomplex_val\', index=12,\n      number=12, type=1, cpp_type=5, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=_b(\'\\020\\001\'), file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'resource_handle_val\', full_name=\'tensorboardX.TensorProto.resource_handle_val\', index=13,\n      number=14, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=163,\n  serialized_end=588,\n)\n\n_TENSORPROTO.fields_by_name[\'dtype\'].enum_type = tensorboardX_dot_proto_dot_types__pb2._DATATYPE\n_TENSORPROTO.fields_by_name[\'tensor_shape\'].message_type = tensorboardX_dot_proto_dot_tensor__shape__pb2._TENSORSHAPEPROTO\n_TENSORPROTO.fields_by_name[\'resource_handle_val\'].message_type = tensorboardX_dot_proto_dot_resource__handle__pb2._RESOURCEHANDLEPROTO\nDESCRIPTOR.message_types_by_name[\'TensorProto\'] = _TENSORPROTO\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nTensorProto = _reflection.GeneratedProtocolMessageType(\'TensorProto\', (_message.Message,), dict(\n  DESCRIPTOR = _TENSORPROTO,\n  __module__ = \'tensorboardX.proto.tensor_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.TensorProto)\n  ))\n_sym_db.RegisterMessage(TensorProto)\n\n\nDESCRIPTOR._options = None\n_TENSORPROTO.fields_by_name[\'half_val\']._options = None\n_TENSORPROTO.fields_by_name[\'float_val\']._options = None\n_TENSORPROTO.fields_by_name[\'double_val\']._options = None\n_TENSORPROTO.fields_by_name[\'int_val\']._options = None\n_TENSORPROTO.fields_by_name[\'scomplex_val\']._options = None\n_TENSORPROTO.fields_by_name[\'int64_val\']._options = None\n_TENSORPROTO.fields_by_name[\'bool_val\']._options = None\n_TENSORPROTO.fields_by_name[\'dcomplex_val\']._options = None\n# @@protoc_insertion_point(module_scope)\n'"
tensorboardX/tensorboardX/proto/tensor_shape_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorboardX/proto/tensor_shape.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorboardX/proto/tensor_shape.proto\',\n  package=\'tensorboardX\',\n  syntax=\'proto3\',\n  serialized_options=_b(\'\\n\\030org.tensorflow.frameworkB\\021TensorShapeProtosP\\001\\370\\001\\001\'),\n  serialized_pb=_b(\'\\n%tensorboardX/proto/tensor_shape.proto\\x12\\x0ctensorboardX\\""|\\n\\x10TensorShapeProto\\x12/\\n\\x03\\x64im\\x18\\x02 \\x03(\\x0b\\x32\\"".tensorboardX.TensorShapeProto.Dim\\x12\\x14\\n\\x0cunknown_rank\\x18\\x03 \\x01(\\x08\\x1a!\\n\\x03\\x44im\\x12\\x0c\\n\\x04size\\x18\\x01 \\x01(\\x03\\x12\\x0c\\n\\x04name\\x18\\x02 \\x01(\\tB2\\n\\x18org.tensorflow.frameworkB\\x11TensorShapeProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n)\n\n\n\n\n_TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(\n  name=\'Dim\',\n  full_name=\'tensorboardX.TensorShapeProto.Dim\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'size\', full_name=\'tensorboardX.TensorShapeProto.Dim.size\', index=0,\n      number=1, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorboardX.TensorShapeProto.Dim.name\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=146,\n  serialized_end=179,\n)\n\n_TENSORSHAPEPROTO = _descriptor.Descriptor(\n  name=\'TensorShapeProto\',\n  full_name=\'tensorboardX.TensorShapeProto\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'dim\', full_name=\'tensorboardX.TensorShapeProto.dim\', index=0,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'unknown_rank\', full_name=\'tensorboardX.TensorShapeProto.unknown_rank\', index=1,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_TENSORSHAPEPROTO_DIM, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=55,\n  serialized_end=179,\n)\n\n_TENSORSHAPEPROTO_DIM.containing_type = _TENSORSHAPEPROTO\n_TENSORSHAPEPROTO.fields_by_name[\'dim\'].message_type = _TENSORSHAPEPROTO_DIM\nDESCRIPTOR.message_types_by_name[\'TensorShapeProto\'] = _TENSORSHAPEPROTO\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nTensorShapeProto = _reflection.GeneratedProtocolMessageType(\'TensorShapeProto\', (_message.Message,), dict(\n\n  Dim = _reflection.GeneratedProtocolMessageType(\'Dim\', (_message.Message,), dict(\n    DESCRIPTOR = _TENSORSHAPEPROTO_DIM,\n    __module__ = \'tensorboardX.proto.tensor_shape_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorboardX.TensorShapeProto.Dim)\n    ))\n  ,\n  DESCRIPTOR = _TENSORSHAPEPROTO,\n  __module__ = \'tensorboardX.proto.tensor_shape_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.TensorShapeProto)\n  ))\n_sym_db.RegisterMessage(TensorShapeProto)\n_sym_db.RegisterMessage(TensorShapeProto.Dim)\n\n\nDESCRIPTOR._options = None\n# @@protoc_insertion_point(module_scope)\n'"
tensorboardX/tensorboardX/proto/types_pb2.py,0,"b""# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorboardX/proto/types.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode('latin1'))\nfrom google.protobuf.internal import enum_type_wrapper\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name='tensorboardX/proto/types.proto',\n  package='tensorboardX',\n  syntax='proto3',\n  serialized_options=_b('\\n\\030org.tensorflow.frameworkB\\013TypesProtosP\\001\\370\\001\\001'),\n  serialized_pb=_b('\\n\\x1etensorboardX/proto/types.proto\\x12\\x0ctensorboardX*\\xc2\\x05\\n\\x08\\x44\\x61taType\\x12\\x0e\\n\\nDT_INVALID\\x10\\x00\\x12\\x0c\\n\\x08\\x44T_FLOAT\\x10\\x01\\x12\\r\\n\\tDT_DOUBLE\\x10\\x02\\x12\\x0c\\n\\x08\\x44T_INT32\\x10\\x03\\x12\\x0c\\n\\x08\\x44T_UINT8\\x10\\x04\\x12\\x0c\\n\\x08\\x44T_INT16\\x10\\x05\\x12\\x0b\\n\\x07\\x44T_INT8\\x10\\x06\\x12\\r\\n\\tDT_STRING\\x10\\x07\\x12\\x10\\n\\x0c\\x44T_COMPLEX64\\x10\\x08\\x12\\x0c\\n\\x08\\x44T_INT64\\x10\\t\\x12\\x0b\\n\\x07\\x44T_BOOL\\x10\\n\\x12\\x0c\\n\\x08\\x44T_QINT8\\x10\\x0b\\x12\\r\\n\\tDT_QUINT8\\x10\\x0c\\x12\\r\\n\\tDT_QINT32\\x10\\r\\x12\\x0f\\n\\x0b\\x44T_BFLOAT16\\x10\\x0e\\x12\\r\\n\\tDT_QINT16\\x10\\x0f\\x12\\x0e\\n\\nDT_QUINT16\\x10\\x10\\x12\\r\\n\\tDT_UINT16\\x10\\x11\\x12\\x11\\n\\rDT_COMPLEX128\\x10\\x12\\x12\\x0b\\n\\x07\\x44T_HALF\\x10\\x13\\x12\\x0f\\n\\x0b\\x44T_RESOURCE\\x10\\x14\\x12\\x10\\n\\x0c\\x44T_FLOAT_REF\\x10\\x65\\x12\\x11\\n\\rDT_DOUBLE_REF\\x10\\x66\\x12\\x10\\n\\x0c\\x44T_INT32_REF\\x10g\\x12\\x10\\n\\x0c\\x44T_UINT8_REF\\x10h\\x12\\x10\\n\\x0c\\x44T_INT16_REF\\x10i\\x12\\x0f\\n\\x0b\\x44T_INT8_REF\\x10j\\x12\\x11\\n\\rDT_STRING_REF\\x10k\\x12\\x14\\n\\x10\\x44T_COMPLEX64_REF\\x10l\\x12\\x10\\n\\x0c\\x44T_INT64_REF\\x10m\\x12\\x0f\\n\\x0b\\x44T_BOOL_REF\\x10n\\x12\\x10\\n\\x0c\\x44T_QINT8_REF\\x10o\\x12\\x11\\n\\rDT_QUINT8_REF\\x10p\\x12\\x11\\n\\rDT_QINT32_REF\\x10q\\x12\\x13\\n\\x0f\\x44T_BFLOAT16_REF\\x10r\\x12\\x11\\n\\rDT_QINT16_REF\\x10s\\x12\\x12\\n\\x0e\\x44T_QUINT16_REF\\x10t\\x12\\x11\\n\\rDT_UINT16_REF\\x10u\\x12\\x15\\n\\x11\\x44T_COMPLEX128_REF\\x10v\\x12\\x0f\\n\\x0b\\x44T_HALF_REF\\x10w\\x12\\x13\\n\\x0f\\x44T_RESOURCE_REF\\x10xB,\\n\\x18org.tensorflow.frameworkB\\x0bTypesProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3')\n)\n\n_DATATYPE = _descriptor.EnumDescriptor(\n  name='DataType',\n  full_name='tensorboardX.DataType',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name='DT_INVALID', index=0, number=0,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_FLOAT', index=1, number=1,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_DOUBLE', index=2, number=2,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_INT32', index=3, number=3,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_UINT8', index=4, number=4,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_INT16', index=5, number=5,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_INT8', index=6, number=6,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_STRING', index=7, number=7,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_COMPLEX64', index=8, number=8,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_INT64', index=9, number=9,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_BOOL', index=10, number=10,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_QINT8', index=11, number=11,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_QUINT8', index=12, number=12,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_QINT32', index=13, number=13,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_BFLOAT16', index=14, number=14,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_QINT16', index=15, number=15,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_QUINT16', index=16, number=16,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_UINT16', index=17, number=17,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_COMPLEX128', index=18, number=18,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_HALF', index=19, number=19,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_RESOURCE', index=20, number=20,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_FLOAT_REF', index=21, number=101,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_DOUBLE_REF', index=22, number=102,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_INT32_REF', index=23, number=103,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_UINT8_REF', index=24, number=104,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_INT16_REF', index=25, number=105,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_INT8_REF', index=26, number=106,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_STRING_REF', index=27, number=107,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_COMPLEX64_REF', index=28, number=108,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_INT64_REF', index=29, number=109,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_BOOL_REF', index=30, number=110,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_QINT8_REF', index=31, number=111,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_QUINT8_REF', index=32, number=112,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_QINT32_REF', index=33, number=113,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_BFLOAT16_REF', index=34, number=114,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_QINT16_REF', index=35, number=115,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_QUINT16_REF', index=36, number=116,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_UINT16_REF', index=37, number=117,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_COMPLEX128_REF', index=38, number=118,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_HALF_REF', index=39, number=119,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name='DT_RESOURCE_REF', index=40, number=120,\n      serialized_options=None,\n      type=None),\n  ],\n  containing_type=None,\n  serialized_options=None,\n  serialized_start=49,\n  serialized_end=755,\n)\n_sym_db.RegisterEnumDescriptor(_DATATYPE)\n\nDataType = enum_type_wrapper.EnumTypeWrapper(_DATATYPE)\nDT_INVALID = 0\nDT_FLOAT = 1\nDT_DOUBLE = 2\nDT_INT32 = 3\nDT_UINT8 = 4\nDT_INT16 = 5\nDT_INT8 = 6\nDT_STRING = 7\nDT_COMPLEX64 = 8\nDT_INT64 = 9\nDT_BOOL = 10\nDT_QINT8 = 11\nDT_QUINT8 = 12\nDT_QINT32 = 13\nDT_BFLOAT16 = 14\nDT_QINT16 = 15\nDT_QUINT16 = 16\nDT_UINT16 = 17\nDT_COMPLEX128 = 18\nDT_HALF = 19\nDT_RESOURCE = 20\nDT_FLOAT_REF = 101\nDT_DOUBLE_REF = 102\nDT_INT32_REF = 103\nDT_UINT8_REF = 104\nDT_INT16_REF = 105\nDT_INT8_REF = 106\nDT_STRING_REF = 107\nDT_COMPLEX64_REF = 108\nDT_INT64_REF = 109\nDT_BOOL_REF = 110\nDT_QINT8_REF = 111\nDT_QUINT8_REF = 112\nDT_QINT32_REF = 113\nDT_BFLOAT16_REF = 114\nDT_QINT16_REF = 115\nDT_QUINT16_REF = 116\nDT_UINT16_REF = 117\nDT_COMPLEX128_REF = 118\nDT_HALF_REF = 119\nDT_RESOURCE_REF = 120\n\n\nDESCRIPTOR.enum_types_by_name['DataType'] = _DATATYPE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\nDESCRIPTOR._options = None\n# @@protoc_insertion_point(module_scope)\n"""
tensorboardX/tensorboardX/proto/versions_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorboardX/proto/versions.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorboardX/proto/versions.proto\',\n  package=\'tensorboardX\',\n  syntax=\'proto3\',\n  serialized_options=_b(\'\\n\\030org.tensorflow.frameworkB\\016VersionsProtosP\\001\\370\\001\\001\'),\n  serialized_pb=_b(\'\\n!tensorboardX/proto/versions.proto\\x12\\x0ctensorboardX\\""K\\n\\nVersionDef\\x12\\x10\\n\\x08producer\\x18\\x01 \\x01(\\x05\\x12\\x14\\n\\x0cmin_consumer\\x18\\x02 \\x01(\\x05\\x12\\x15\\n\\rbad_consumers\\x18\\x03 \\x03(\\x05\\x42/\\n\\x18org.tensorflow.frameworkB\\x0eVersionsProtosP\\x01\\xf8\\x01\\x01\\x62\\x06proto3\')\n)\n\n\n\n\n_VERSIONDEF = _descriptor.Descriptor(\n  name=\'VersionDef\',\n  full_name=\'tensorboardX.VersionDef\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'producer\', full_name=\'tensorboardX.VersionDef.producer\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'min_consumer\', full_name=\'tensorboardX.VersionDef.min_consumer\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'bad_consumers\', full_name=\'tensorboardX.VersionDef.bad_consumers\', index=2,\n      number=3, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=51,\n  serialized_end=126,\n)\n\nDESCRIPTOR.message_types_by_name[\'VersionDef\'] = _VERSIONDEF\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nVersionDef = _reflection.GeneratedProtocolMessageType(\'VersionDef\', (_message.Message,), dict(\n  DESCRIPTOR = _VERSIONDEF,\n  __module__ = \'tensorboardX.proto.versions_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorboardX.VersionDef)\n  ))\n_sym_db.RegisterMessage(VersionDef)\n\n\nDESCRIPTOR._options = None\n# @@protoc_insertion_point(module_scope)\n'"
apex/apex/amp/lists/__init__.py,0,b''
apex/apex/amp/lists/functional_overrides.py,6,"b'\n# TODO: think about the following two. They do weird things.\n# - torch.nn.utils.clip_grad (but it should always be fp32 anyway)\n# - torch.nn.utils.weight_norm\n\n# Notes:\n# F.instance_norm uses batch_norm internally. Which correctly handles\n#   fp16 in/out with fp32 weights. So we shouldn\'t do anything for\n#   either of these.\n# F.normalize calls `input.norm()` internally, so it\'s redundant, but\n#   kept here in case impl. changes.\n# F.cosine_similarity is same: calls `x.norm()` internally.\n\nimport torch.nn.functional\n\nMODULE = torch.nn.functional\n\nFP16_FUNCS = [\n    \'conv1d\',\n    \'conv2d\',\n    \'conv3d\',\n    \'conv_transpose1d\',\n    \'conv_transpose2d\',\n    \'conv_transpose3d\',\n    \'conv_tbc\', # Undocumented / maybe new?\n    \'linear\',\n]\n\nFP32_FUNCS = [\n\n    # Interpolation/Upsampling\n    \'interpolate\',\n\n    # Pointwise\n    \'softplus\',\n    \'softmin\',\n    \'log_softmax\',\n    \'softmax\',\n\n    # Normalization\n    \'layer_norm\',\n    \'group_norm\',\n    \'local_response_norm\',\n    \'normalize\',\n    \'cosine_similarity\',\n\n    # Loss functions\n    # TODO: which of these can be fp16?\n    \'poisson_nll_loss\',\n    \'cosine_embedding_loss\',\n    \'cross_entropy\',\n    \'hinge_embedding_loss\',\n    \'kl_div\',\n    \'l1_loss\',\n    \'mse_loss\',\n    \'margin_ranking_loss\',\n    \'multilabel_margin_loss\',\n    \'multilabel_soft_margin_loss\',\n    \'multi_margin_loss\',\n    \'nll_loss\',\n    \'binary_cross_entropy_with_logits\',\n    \'smooth_l1_loss\',\n    \'soft_margin_loss\',\n    \'triplet_margin_loss\'\n]\n\nBANNED_FUNCS = [\n    (\'binary_cross_entropy\',\n     (""\\namp does not work out-of-the-box with `F.binary_cross_entropy` or `torch.nn.BCELoss.` ""\n      ""It requires that the output of the previous function be already a FloatTensor. \\n\\n""\n      ""Most models have a Sigmoid right before BCELoss. In that case, you can use\\n""\n      ""    torch.nn.BCEWithLogitsLoss\\nto combine Sigmoid+BCELoss into a single layer ""\n      ""that is compatible with amp.\\nAnother option is to add\\n""\n      ""    amp.register_float_function(torch, \'sigmoid\')\\nbefore calling `amp.init()`.\\n""\n      ""If you _really_ know what you are doing, you can disable this warning by passing ""\n      ""allow_banned=True to `amp.init()`.""))\n]\n'"
apex/apex/amp/lists/tensor_overrides.py,3,"b""from .. import compat\nfrom . import torch_overrides\n\nimport importlib\n\nimport torch\n\nif compat.variable_is_tensor() and not compat.tensor_is_variable():\n    MODULE = torch.Tensor\nelse:\n    MODULE = torch.autograd.Variable\n\n\nFP16_FUNCS = [\n    '__matmul__',\n]\n\nFP32_FUNCS = [\n    '__ipow__',\n    '__pow__',\n    '__rpow__',\n\n    # Cast to fp32 before transfer to CPU\n    'cpu',\n]\n\nCASTS = [\n    '__add__',\n    '__div__',\n    '__eq__',\n    '__ge__',\n    '__gt__',\n    '__iadd__',\n    '__idiv__',\n    '__imul__',\n    '__isub__',\n    '__itruediv__',\n    '__le__',\n    '__lt__',\n    '__mul__',\n    '__ne__',\n    '__radd__',\n    '__rdiv__',\n    '__rmul__',\n    '__rsub__',\n    '__rtruediv__',\n    '__sub__',\n    '__truediv__',\n]\n\n# None of these, but here to make code cleaner.\nSEQUENCE_CASTS = []\n\n# We need to grab all the methods from torch_overrides and add them to\n# the Tensor lists as well, as almost all methods are duplicated\n# between `torch` and `torch.Tensor` (and check with `hasattr`,\n# because a few random ones aren't defined on Tensor)\n_self_mod = importlib.import_module(__name__)\nfor attrname in ['FP16_FUNCS', 'FP32_FUNCS', 'CASTS', 'SEQUENCE_CASTS']:\n    lst = getattr(_self_mod, attrname)\n    for fn in getattr(torch_overrides, attrname):\n        if hasattr(MODULE, fn):\n            lst.append(fn)\n"""
apex/apex/amp/lists/torch_overrides.py,1,"b""import torch\n\nfrom .. import utils\n\nMODULE = torch\n\nFP16_FUNCS = [\n    # Low level functions wrapped by torch.nn layers.\n    # The wrapper layers contain the weights which are then passed in as a parameter\n    # to these functions.\n    'conv1d',\n    'conv2d',\n    'conv3d',\n    'conv_transpose1d',\n    'conv_transpose2d',\n    'conv_transpose3d',\n    'conv_tbc',\n    'prelu',\n\n    # BLAS\n    'addmm',\n    'addmv',\n    'addr',\n    'matmul',\n    'mm',\n    'mv',\n]\n\nFP32_FUNCS = [\n    # Pointwise\n    'acos',\n    'asin',\n    'cosh',\n    'erfinv',\n    'exp',\n    'expm1',\n    'log',\n    'log10',\n    'log2',\n    'reciprocal',\n    'rsqrt',\n    'sinh',\n    'tan',\n\n    # Other math\n    'pow',\n\n    # Reduction\n    'cumprod',\n    'cumsum',\n    'dist',\n    'mean',\n    'norm',\n    'prod',\n    'std',\n    'sum',\n    'var',\n\n    # Misc\n    'renorm'\n]\n\n# Before CUDA 9.1, batched matmul was missing fast FP16 kernels. We\n# check the CUDA version -- if at least 9.1, then put the bmm\n# functions on the fp16 list. Otherwise, put them on the fp32 list.\n_bmms = ['addbmm',\n         'baddbmm',\n         'bmm']\nif utils.get_cuda_version() >= (9, 1, 0):\n    FP16_FUNCS.extend(_bmms)\nelse:\n    FP32_FUNCS.extend(_bmms)\n\n# Multi-tensor fns that may need type promotion\nCASTS = [\n    # Multi-tensor math\n    'addcdiv',\n    'addcmul',\n    'atan2',\n    'cross',\n    'bilinear',\n\n    # Element-wise _or_ tensor-wise math\n    'add',\n    'div',\n    'mul',\n\n    # Comparison\n    'eq',\n    'equal',\n    'ge',\n    'gt',\n    'le',\n    'lt',\n    'ne'\n]\n\n# Functions that take sequence arguments. We need to inspect the whole\n# sequence and cast to the widest type.\nSEQUENCE_CASTS = [\n    'cat',\n    'stack'\n]\n"""
apex/examples/simple/distributed/distributed_data_parallel.py,14,"b'import torch\nimport argparse\nimport os\nfrom apex import amp\n# FOR DISTRIBUTED: (can also use torch.nn.parallel.DistributedDataParallel instead)\nfrom apex.parallel import DistributedDataParallel\n\nparser = argparse.ArgumentParser()\n# FOR DISTRIBUTED:  Parse for the local_rank argument, which will be supplied\n# automatically by torch.distributed.launch.\nparser.add_argument(""--local_rank"", default=0, type=int)\nargs = parser.parse_args()\n\n# FOR DISTRIBUTED:  If we are running under torch.distributed.launch,\n# the \'WORLD_SIZE\' environment variable will also be set automatically.\nargs.distributed = False\nif \'WORLD_SIZE\' in os.environ:\n    args.distributed = int(os.environ[\'WORLD_SIZE\']) > 1\n\nif args.distributed:\n    # FOR DISTRIBUTED:  Set the device according to local_rank.\n    torch.cuda.set_device(args.local_rank)\n\n    # FOR DISTRIBUTED:  Initialize the backend.  torch.distributed.launch will provide\n    # environment variables, and requires that you use init_method=`env://`.\n    torch.distributed.init_process_group(backend=\'nccl\',\n                                         init_method=\'env://\')\n\ntorch.backends.cudnn.benchmark = True\n\nN, D_in, D_out = 64, 1024, 16\n\n# Each process receives its own batch of ""fake input data"" and ""fake target data.""\n# The ""training loop"" in each process just uses this fake batch over and over.\n# https://github.com/NVIDIA/apex/tree/master/examples/imagenet provides a more realistic\n# example of distributed data sampling for both training and validation.\nx = torch.randn(N, D_in, device=\'cuda\')\ny = torch.randn(N, D_out, device=\'cuda\')\n\nmodel = torch.nn.Linear(D_in, D_out).cuda()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n\nmodel, optimizer = amp.initialize(model, optimizer, opt_level=""O1"")\n\nif args.distributed:\n    # FOR DISTRIBUTED:  After amp.initialize, wrap the model with\n    # apex.parallel.DistributedDataParallel.\n    model = DistributedDataParallel(model)\n    # torch.nn.parallel.DistributedDataParallel is also fine, with some added args:\n    # model = torch.nn.parallel.DistributedDataParallel(model,\n    #                                                   device_ids=[args.local_rank],\n    #                                                   output_device=args.local_rank)\n\nloss_fn = torch.nn.MSELoss()\n\nfor t in range(500):\n    optimizer.zero_grad()\n    y_pred = model(x)\n    loss = loss_fn(y_pred, y)\n    with amp.scale_loss(loss, optimizer) as scaled_loss:\n        scaled_loss.backward()\n    optimizer.step()\n\nif args.local_rank == 0:\n    print(""final loss = "", loss)\n'"
apex/tests/L0/run_amp/__init__.py,0,b''
apex/tests/L0/run_amp/test_add_param_group.py,9,"b'import unittest\n\nimport functools as ft\nimport itertools as it\n\nfrom apex import amp\nfrom apex.amp import _amp_state\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\n\nfrom utils import common_init, HALF, FLOAT,\\\n    ALWAYS_HALF, ALWAYS_FLOAT, MATCH_INPUT\n\nclass MyModel(torch.nn.Module):\n    def __init__(self, unique):\n        super(MyModel, self).__init__()\n        self.weight0 = Parameter(unique +\n            torch.arange(2, device=\'cuda\', dtype=torch.float32))\n        self.weight1 = Parameter(1. + unique + torch.arange(2, device=\'cuda\', dtype=torch.float16))\n\n    @staticmethod\n    def ops(input, weight0, weight1):\n        return ((input*(weight0.float()))*(weight1.float())).sum()\n\n    def forward(self, input):\n        return self.ops(input, self.weight0, self.weight1)\n\n\n# Abandon all hope, ye who enter here.\n\n\nclass TestAddParamGroup(unittest.TestCase):\n    def setUp(self):\n        self.x = torch.ones((2), device=\'cuda\', dtype=torch.float32)\n        common_init(self)\n\n    def tearDown(self):\n        pass\n\n    def zero_grad(self, models, optimizer, how_to_zero):\n        if how_to_zero == ""none"":\n            for model in models:\n                for param in model.parameters():\n                    param.grad = None\n        elif how_to_zero == ""model"":\n            for model in models:\n                model.zero_grad()\n        elif how_to_zero == ""optimizer"":\n            optimizer.zero_grad()\n\n    def test_add_param_group(self):\n        for opt_level in (""O0"", ""O1"", ""O2"", ""O3""):\n          for zero_before_add in (True, False):\n            for try_accumulation in (True, False):\n              model0 = MyModel(1)\n              model1 = MyModel(2)\n\n              optimizer = torch.optim.SGD([{\'params\' : model0.parameters(), \'lr\' : 0.25}],\n                                          momentum=0.125)\n\n              optimizer.zero_grad()\n              loss = model0(self.x)\n              loss.backward()\n              optimizer.step()\n\n              if zero_before_add:\n                  optimizer.zero_grad()\n              optimizer.add_param_group({\'params\' : model1.parameters(), \'lr\' : 0.5})\n              if not zero_before_add:\n                  optimizer.zero_grad()\n\n              loss = model0(self.x) + model1(self.x)\n              loss.backward(retain_graph=try_accumulation)\n              if try_accumulation:\n                  loss.backward()\n              optimizer.step()\n\n              # Once more to make sure the new params pick up momemtums properly\n              optimizer.zero_grad()\n              loss = model0(self.x) + model1(self.x)\n              loss.backward(retain_graph=try_accumulation)\n              if try_accumulation:\n                  loss.backward()\n              optimizer.step()\n\n              reference_params = [param.data.clone() for param in model0.parameters()] + \\\n                                 [param.data.clone() for param in model1.parameters()]\n\n              for how_to_zero in ""none"", ""model"", ""optimizer"":\n                  model0 = MyModel(1)\n                  model1 = MyModel(2)\n\n                  optimizer = torch.optim.SGD([{\'params\' : model0.parameters(), \'lr\' : 0.25}],\n                                              momentum=0.125)\n\n                  _amp_state.allow_incoming_model_not_fp32 = True\n                  [model0, model1], optimizer = amp.initialize([model0, model1],\n                      optimizer,\n                      opt_level=opt_level,\n                      verbosity=0,\n                      cast_model_type=False)\n                  _amp_state.allow_incoming_model_not_fp32 = False\n\n                  _amp_state.loss_scalers[0]._loss_scale = 4.0\n\n                  self.zero_grad([model0, model1], optimizer, how_to_zero)\n                  loss = model0(self.x)\n                  with amp.scale_loss(loss, optimizer) as scaled_loss:\n                      scaled_loss.backward()\n                  optimizer.step()\n\n                  if zero_before_add:\n                      self.zero_grad([model0, model1], optimizer, how_to_zero)\n                  optimizer.add_param_group({\'params\' : model1.parameters(), \'lr\' : 0.5})\n                  if not zero_before_add:\n                      self.zero_grad([model0, model1], optimizer, how_to_zero)\n\n                  loss = model0(self.x) + model1(self.x)\n                  with amp.scale_loss(loss, optimizer) as scaled_loss:\n                      scaled_loss.backward(retain_graph=try_accumulation)\n                  if try_accumulation:\n                      with amp.scale_loss(loss, optimizer) as scaled_loss:\n                          scaled_loss.backward()\n                  optimizer.step()\n\n                  # Once more to make sure the new params pick up momentums properly\n                  self.zero_grad([model0, model1], optimizer, how_to_zero)\n                  loss = model0(self.x) + model1(self.x)\n                  with amp.scale_loss(loss, optimizer) as scaled_loss:\n                      scaled_loss.backward(retain_graph=try_accumulation)\n                  if try_accumulation:\n                      with amp.scale_loss(loss, optimizer) as scaled_loss:\n                          scaled_loss.backward()\n                  optimizer.step()\n\n                  final_params = [param.data.clone() for param in model0.parameters()] + \\\n                                 [param.data.clone() for param in model1.parameters()]\n\n                  for reference, final in zip(reference_params, final_params):\n                      self.assertTrue(torch.allclose(reference.to(final.dtype), final),\n                                      ""opt_level = {}, how_to_zero = {}, zero_before_add = {}"".format(\n                                      opt_level, how_to_zero, zero_before_add))\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
apex/tests/L0/run_amp/test_basic_casts.py,11,"b""import unittest\n\nimport functools as ft\nimport itertools as it\n\nfrom apex import amp\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom utils import common_init, HALF, FLOAT,\\\n    ALWAYS_HALF, ALWAYS_FLOAT, MATCH_INPUT\n\ndef run_layer_test(test_case, fns, expected, input_shape, test_backward=True):\n    for fn, typ in it.product(fns, expected.keys()):\n        x = torch.randn(input_shape, dtype=typ).requires_grad_()\n        y = fn(x)\n        test_case.assertEqual(y.type(), expected[typ])\n        if test_backward:\n            y.float().sum().backward()\n            test_case.assertEqual(x.grad.type(), MATCH_INPUT[typ])\n\nclass TestBasicCasts(unittest.TestCase):\n    def setUp(self):\n        self.handle = amp.init(enabled=True)\n        common_init(self)\n\n    def tearDown(self):\n        self.handle._deactivate()\n\n    def test_linear_is_half(self):\n        m = nn.Linear(self.h, self.h)\n        f = ft.partial(F.linear, weight=m.weight, bias=m.bias)\n        run_layer_test(self, [m, f], ALWAYS_HALF, (self.b, self.h))\n\n    def test_conv2d_is_half(self):\n        m = nn.Conv2d(self.c, self.c, self.k)\n        f = ft.partial(F.conv2d, weight=m.weight, bias=m.bias)\n        run_layer_test(self, [m, f], ALWAYS_HALF, (self.b, self.c, self.h, self.h))\n\n    def test_softmax_is_float(self):\n        m = nn.Softmax(dim=1)\n        f = ft.partial(F.softmax, dim=1)\n        run_layer_test(self, [m, f], ALWAYS_FLOAT, (self.b, self.h))\n\n    def test_group_norm_is_float(self):\n        m = nn.GroupNorm(num_groups=4, num_channels=self.c)\n        run_layer_test(self, [m], ALWAYS_FLOAT, (self.b, self.c, self.h, self.h))\n\n    def test_mse_loss_is_float(self):\n        shape = (self.b, self.h)\n        target = torch.randn(shape)\n        mod = nn.MSELoss()\n        m = lambda x: mod(x, target)\n        f = ft.partial(F.mse_loss, target=target)\n        run_layer_test(self, [m], ALWAYS_FLOAT, shape)\n\n    def test_relu_is_match(self):\n        run_layer_test(self, [nn.ReLU(), F.relu], MATCH_INPUT, (self.b, self.h))\n\n    def test_batch_norm_is_match(self):\n        m = nn.BatchNorm2d(num_features=self.c)\n        f = ft.partial(F.batch_norm, running_mean=m.running_mean, running_var=m.running_var,\n                       weight=m.weight, bias=m.bias, training=True)\n        run_layer_test(self, [m], MATCH_INPUT, (self.b, self.c, self.h, self.h))\n\n        # Test forward-only for BN inference\n        m.eval()\n        f = ft.partial(F.batch_norm, running_mean=m.running_mean, running_var=m.running_var,\n                       weight=m.weight, bias=m.bias, training=False)\n        run_layer_test(self, [m, f], MATCH_INPUT, (self.b, self.c, self.h, self.h),\n                            test_backward=False)\n\nclass TestBannedMethods(unittest.TestCase):\n    def setUp(self):\n        self.handle = amp.init(enabled=True)\n        common_init(self)\n\n    def tearDown(self):\n        self.handle._deactivate()\n\n    def bce_common(self, assertion):\n        shape = (self.b, self.h)\n        target = torch.rand(shape)\n        mod = nn.BCELoss()\n        m = lambda x: mod(x, target)\n        f = ft.partial(F.binary_cross_entropy, target=target)\n        for fn in [m, f]:\n            x = torch.rand(shape, dtype=torch.half)\n            assertion(fn, x)\n\n    def test_bce_raises_by_default(self):\n        assertion = lambda fn, x: self.assertRaises(NotImplementedError, fn, x)\n        self.bce_common(assertion)\n\n    def test_bce_is_float_with_allow_banned(self):\n        self.handle._deactivate()\n        self.handle = amp.init(enabled=True, allow_banned=True)\n        assertion = lambda fn, x: self.assertEqual(fn(x).type(), FLOAT)\n        self.bce_common(assertion)\n\nclass TestTensorCasts(unittest.TestCase):\n    def setUp(self):\n        self.handle = amp.init(enabled=True)\n        common_init(self)\n\n    def tearDown(self):\n        self.handle._deactivate()\n\n    def test_matmul_method_is_half(self):\n        other = torch.randn(self.h, self.h)\n        lhs = lambda x: x.matmul(other)\n        rhs = lambda x: other.matmul(x)\n        run_layer_test(self, [lhs, rhs], ALWAYS_HALF, (self.h, self.h))\n\n    def test_matmul_op_is_half(self):\n        other = torch.randn(self.h, self.h)\n        lhs = lambda x: x @ other\n        rhs = lambda x: other @ x\n        run_layer_test(self, [lhs, rhs], ALWAYS_HALF, (self.h, self.h))\n\n    def test_pow_method_is_float(self):\n        fn = lambda x: x.pow(2.)\n        run_layer_test(self, [fn], ALWAYS_FLOAT, (self.b, self.h))\n\n    def test_pow_op_is_float(self):\n        fn = lambda x: x ** 2.\n        run_layer_test(self, [fn], ALWAYS_FLOAT, (self.b, self.h))\n\n    def test_cpu_is_float(self):\n        fn = lambda x: x.cpu()\n        always_cpu_float = {torch.float: 'torch.FloatTensor',\n                            torch.half: 'torch.FloatTensor'}\n        run_layer_test(self, [fn], always_cpu_float, (self.b, self.h))\n\n    def test_sum_is_float(self):\n        fn = lambda x: x.sum()\n        run_layer_test(self, [fn], ALWAYS_FLOAT, (self.b, self.h))\n\nclass TestDisabledCasts(unittest.TestCase):\n    def setUp(self):\n        self.handle = amp.init(enabled=False)\n        common_init(self)\n\n    def test_disabled_linear(self):\n        m = nn.Linear(self.h, self.h)\n        f = ft.partial(F.linear, weight=m.weight, bias=m.bias)\n        input_shape = (self.b, self.h)\n\n        for fn in [m, f]:\n            x = torch.randn(input_shape, dtype=torch.float).requires_grad_()\n            y = fn(x)\n            self.assertEqual(y.type(), FLOAT)\n            y.sum().backward()\n            self.assertEqual(x.grad.type(), FLOAT)\n\n            x = torch.randn(input_shape, dtype=torch.half).requires_grad_()\n            self.assertRaises(RuntimeError, fn, x)\n\n    # TODO: maybe more tests on disabled casting?\n\nif __name__ == '__main__':\n    unittest.main()\n"""
apex/tests/L0/run_amp/test_cache.py,22,"b'import unittest\n\nimport functools as ft\nimport itertools as it\n\nfrom apex import amp\nfrom apex.amp import _amp_state\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom utils import common_init, HALF, FLOAT,\\\n    ALWAYS_HALF, ALWAYS_FLOAT, MATCH_INPUT\n\ndef get_reference_grad(i, w, ops):\n    # Creating new tensors ensures, among other things, that the new tensors are not in the cache.\n    # In fact, they are guaranteed not to use the cache because they are not torch.nn.Parameters.\n    fp32_i = i.detach().clone().float()\n    fp32_w = w.detach().clone().float().requires_grad_()\n    loss = ops(fp32_i, fp32_w)\n    loss.backward()\n    return fp32_w.grad\n\nclass WhitelistModule(torch.nn.Module):\n    def __init__(self, dtype):\n        super(WhitelistModule, self).__init__()\n        self.weight = torch.nn.Parameter(torch.arange(8*8, device=\'cuda\', dtype=dtype).view(8,8))\n\n    @staticmethod\n    def ops(input, weight):\n        return (input.mm(weight)).mm(weight).sum()\n\n    def forward(self, input):\n        return self.ops(input, self.weight)\n\n\nclass BlacklistModule(torch.nn.Module):\n    def __init__(self, dtype):\n        super(BlacklistModule, self).__init__()\n        self.weight = torch.nn.Parameter(torch.arange(2*8, device=\'cuda\', dtype=dtype).view(2,8))\n\n    @staticmethod\n    def ops(input, weight):\n        return (input + torch.pow(weight, 2) + torch.pow(weight, 2)).sum()\n\n    def forward(self, input):\n        return self.ops(input, self.weight)\n\n\nclass PromoteModule(torch.nn.Module):\n    def __init__(self, dtype):\n        super(PromoteModule, self).__init__()\n        self.weight = torch.nn.Parameter(torch.arange(2*8, device=\'cuda\', dtype=dtype).view(2,8))\n\n    @staticmethod\n    def ops(input, weight):\n        return ((input*weight)*weight).sum()\n\n    def forward(self, input):\n        return self.ops(input, self.weight)\n\nclass TestCache(unittest.TestCase):\n    def setUp(self):\n        self.x = torch.ones((2, 8), device=\'cuda\', dtype=torch.float32)\n        common_init(self)\n\n    def tearDown(self):\n        pass\n\n    def train_eval_train_test(self, module, t):\n        model = module(t).cuda()\n        optimizer = torch.optim.SGD(model.parameters(), lr=1.0)\n\n        _amp_state.allow_incoming_model_not_fp32 = True\n        model, optimizer = amp.initialize(model, optimizer, opt_level=""O1"", verbosity=0)\n        _amp_state.allow_incoming_model_not_fp32 = False\n        \n        def training_step():\n            for param in model.parameters():\n                param.grad = None\n        \n            loss = model(self.x).sum()\n            _amp_state.loss_scalers[0]._loss_scale = 4.0\n            with amp.scale_loss(loss, optimizer) as scaled_loss:\n                scaled_loss.backward()\n        \n            self.assertEqual(len([p.grad for p in model.parameters() if p.grad is not None]), 1)\n            self.assertEqual(model.weight.grad.type(), model.weight.type())\n        \n            reference_grad = get_reference_grad(self.x, model.weight, model.ops)\n        \n            # Currently there\'s no difference in the allclose calls, so no need for branching,\n            # but I\'m keeping this in case we want different tolerances for fp16 and fp32 checks. \n            if model.weight.grad.type() == ""torch.cuda.HalfTensor"":\n                self.assertTrue(torch.allclose(model.weight.grad.float(), reference_grad))\n            elif model.weight.grad.type() == ""torch.cuda.FloatTensor"":\n                self.assertTrue(torch.allclose(model.weight.grad.float(), reference_grad))\n            else:\n                raise RuntimeError(""model.weight.grad.type = {}"".format(model.weight.grad.type()))\n\n            model.weight.data -= 1.\n        \n        # Simulates first epoch\n        training_step()\n        \n        # Simulates eval\n        with torch.no_grad():\n            loss = model(self.x).sum()\n        \n        # Simulates resuming training after eval\n        training_step()\n\n        _amp_state.handle._deactivate()\n   \n    # I could easily have these as a set of for loops in a single test,\n    # instead of going for granularity.\n    def test_whitelist_module_fp16_weight(self):\n        self.train_eval_train_test(WhitelistModule, torch.float16)\n\n    def test_whitelist_module_fp32_weight(self):\n        self.train_eval_train_test(WhitelistModule, torch.float32)\n\n    def test_blacklist_module_fp16_weight(self):\n        self.train_eval_train_test(BlacklistModule, torch.float16)\n\n    def test_blacklist_module_fp32_weight(self):\n        self.train_eval_train_test(BlacklistModule, torch.float32)\n\n    def test_promote_module_fp16_weight(self):\n        self.train_eval_train_test(PromoteModule, torch.float16)\n\n    def test_promote_module_fp32_weight(self):\n        self.train_eval_train_test(PromoteModule, torch.float32)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
apex/tests/L0/run_amp/test_multi_tensor_axpby.py,11,"b'import unittest\n\nimport functools as ft\nimport itertools as it\n\nfrom apex import amp\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom utils import common_init, HALF, FLOAT,\\\n    ALWAYS_HALF, ALWAYS_FLOAT, MATCH_INPUT\n\ntry:\n  import amp_C\n  from amp_C import multi_tensor_axpby\n  from apex.multi_tensor_apply import MultiTensorApply\n  disabled = False\nexcept ImportError as err:\n  print(""amp_C fused kernels unavailable, disabling TestMultiTensorApply.  ImportError was "", err)\n  disabled = True\n\n\nclass TestMultiTensorAxpby(unittest.TestCase):\n\n    def setUp(self):\n        common_init(self)\n\n        self.a = 2.0\n        self.b = 8.0\n        self.xval = 4.0\n        self.yval = 16.0\n        self.overflow_buf = torch.cuda.IntTensor(1).zero_()\n        self.ref = torch.cuda.FloatTensor([136.0])\n\n    def tearDown(self):\n        pass\n\n    # The tensor creation here is written for convenience, not speed.\n    def axpby(self, sizea, sizeb, applier, repeat_tensors,\n              x_type, y_type, out_type, inplace=False):\n        self.overflow_buf.zero_()\n        t1 = torch.cuda.FloatTensor(sizea).fill_(1.0)\n        t2 = torch.cuda.FloatTensor(sizeb).fill_(1.0)\n\n        y_list = []\n        for i in range(repeat_tensors):\n            y_list += [t1.clone().to(y_type)*self.yval, t2.clone().to(y_type)*self.yval]\n\n        x_list = [x.clone().to(x_type)*(self.xval/self.yval) for x in y_list]\n\n        if inplace:\n            out_list = y_list\n        else:\n            out_list = [out.clone().to(out_type)*3.0 for out in y_list]\n\n        applier(multi_tensor_axpby, self.overflow_buf, [x_list, y_list, out_list], self.a, self.b, -1)\n\n        self.assertTrue(all([torch.allclose(out, self.ref.to(out_type)) for out in out_list]),\n                        msg=""{} {} {} {} {} {} {}"".format(sizea, sizeb, repeat_tensors,\n                        x_type, y_type, out_type, inplace))\n        self.assertTrue(self.overflow_buf.item() == 0,\n                        msg=""{} {} {} {} {} {} {}"".format(sizea, sizeb, repeat_tensors,\n                        x_type, y_type, out_type, inplace))\n\n    # def find_inf(self, sizea, sizeb, applier, repeat_tensors, in_type, out_type, t, ind, val, inplace=False):\n    #     self.overflow_buf.zero_()\n    #     a = torch.cuda.FloatTensor(sizea).fill_(self.scale)\n    #     b = torch.cuda.FloatTensor(sizeb).fill_(self.scale)\n\n    #     out_list = []\n    #     for i in range(repeat_tensors):\n    #         out_list += [a.clone().to(out_type), b.clone().to(out_type)]\n\n    #     if inplace:\n    #         in_list = out_list\n    #     else:\n    #         in_list = [out.clone().to(in_type) for out in out_list]\n\n    #     applier(multi_tensor_scale, self.overflow_buf, [in_list, out_list], 1./self.scale)\n\n    #     self.overflow_buf.zero_()\n    #     in_list[t][ind] = val\n    #     applier(multi_tensor_scale, self.overflow_buf, [in_list, out_list], 1./self.scale)\n    #     self.assertTrue(self.overflow_buf.item())\n\n    @unittest.skipIf(disabled, ""amp_C is unavailable"")\n    def test_fuzz(self):\n        input_size_pairs = (\n            (7777*77, 555*555),\n            (777, 555),\n            (555, 2048*32+1),\n            (2048*32+1, 555),\n            (555, 2048*32),\n            (2048*32, 555),\n            (33333, 555),\n            (555, 33333))\n        appliers = (\n            MultiTensorApply(2048*32),\n            MultiTensorApply(333),\n            MultiTensorApply(33333))\n        repeat_tensors = (\n            1,\n            55)\n\n        for sizea, sizeb in input_size_pairs:\n          for applier in appliers:\n            for repeat in repeat_tensors:\n              for x_type in (torch.float32, torch.float16):\n                for y_type in (torch.float32, torch.float16):\n                  for out_type in (torch.float32, torch.float16):\n                    for inplace in (True, False):\n                      if inplace is True and (y_type is not out_type):\n                        continue\n                      else:\n                        self.axpby(sizea, sizeb, applier, repeat,\n                                   x_type, y_type, out_type, inplace=inplace)\n                      # self.find_inf(sizea, sizeb, applier, repeat, in_type, out_type,\n                      #               0, 0, float(\'nan\'), inplace=inplace)\n                      # self.find_inf(sizea, sizeb, applier, repeat, in_type, out_type,\n                      #               2*repeat-1, sizeb-1, float(\'inf\'), inplace=inplace)\n                      # self.find_inf(sizea, sizeb, applier, repeat, in_type, out_type,\n                      #              2*(repeat//2), sizea//2, float(\'inf\'), inplace=inplace)\n\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
apex/tests/L0/run_amp/test_multi_tensor_l2norm.py,9,"b'import unittest\n\nimport functools as ft\nimport itertools as it\n\nfrom apex import amp\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom utils import common_init, HALF, FLOAT,\\\n    ALWAYS_HALF, ALWAYS_FLOAT, MATCH_INPUT\n\ntry:\n  import amp_C\n  from amp_C import multi_tensor_l2norm\n  from apex.multi_tensor_apply import MultiTensorApply\n  disabled = False\nexcept ImportError as err:\n  print(""amp_C fused kernels unavailable, disabling TestMultiTensorApply.  ImportError was "", err)\n  disabled = True\n\n\nclass TestMultiTensorL2Norm(unittest.TestCase):\n\n    def setUp(self):\n        common_init(self)\n        self.val = 4.0\n        self.overflow_buf = torch.cuda.IntTensor(1).zero_()\n\n    def tearDown(self):\n        pass\n\n    # The tensor creation here is written for convenience, not speed.\n    def l2norm(self, sizea, sizeb, applier, repeat_tensors, in_type, per_tensor):\n        self.overflow_buf.zero_()\n        a = torch.cuda.FloatTensor(sizea).fill_(self.val)\n        b = torch.cuda.FloatTensor(sizeb).fill_(self.val)\n\n        in_list = []\n        for i in range(repeat_tensors):\n            in_list += [a.clone().to(in_type), b.clone().to(in_type)]\n\n        if per_tensor:\n            norm, norm_per_tensor = applier(multi_tensor_l2norm, self.overflow_buf, [in_list], True)\n            normab = torch.cat((a.norm().view(1), b.norm().view(1)))\n            norm_per_tensor = norm_per_tensor.view(-1, 2)\n        else:\n            norm, _ = applier(multi_tensor_l2norm, self.overflow_buf, [in_list], True)\n\n        reference = torch.cuda.FloatTensor((sizea + sizeb)*repeat_tensors).fill_(self.val).norm()\n\n        self.assertTrue(torch.allclose(norm, reference))\n        if per_tensor:\n          self.assertTrue(torch.allclose(norm_per_tensor, normab))\n        self.assertTrue(self.overflow_buf.item() == 0)\n\n    @unittest.skipIf(disabled, ""amp_C is unavailable"")\n    def test_fuzz(self):\n        input_size_pairs = (\n            (7777*77, 555*555),\n            (777, 555),\n            (555, 2048*32+1),\n            (2048*32+1, 555),\n            (555, 2048*32),\n            (2048*32, 555),\n            (33333, 555),\n            (555, 33333))\n        appliers = (\n            MultiTensorApply(2048*32), \n            MultiTensorApply(333),\n            MultiTensorApply(33333))\n        repeat_tensors = (\n            1,\n            55)\n\n        for sizea, sizeb in input_size_pairs:\n          for applier in appliers:\n            for repeat in repeat_tensors:\n              for in_type in (torch.float32, torch.float16):\n                for per_tensor in (False, True):\n                  self.l2norm(sizea, sizeb, applier, repeat, in_type, per_tensor)\n\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
apex/tests/L0/run_amp/test_multi_tensor_scale.py,10,"b'import unittest\n\nimport functools as ft\nimport itertools as it\n\nfrom apex import amp\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom utils import common_init, HALF, FLOAT,\\\n    ALWAYS_HALF, ALWAYS_FLOAT, MATCH_INPUT\n\ntry:\n  import amp_C\n  from amp_C import multi_tensor_scale \n  from apex.multi_tensor_apply import MultiTensorApply\n  disabled = False\nexcept ImportError as err:\n  print(""amp_C fused kernels unavailable, disabling TestMultiTensorApply.  ImportError was "", err)\n  disabled = True\n\n\nclass TestMultiTensorScale(unittest.TestCase):\n\n    def setUp(self):\n        common_init(self)\n        self.scale = 4.0\n        self.overflow_buf = torch.cuda.IntTensor(1).zero_()\n        self.ref = torch.cuda.FloatTensor([1.0])\n\n    def tearDown(self):\n        pass\n\n    # The tensor creation here is written for convenience, not speed.\n    def downscale(self, sizea, sizeb, applier, repeat_tensors, in_type, out_type, inplace=False):\n        self.overflow_buf.zero_()\n        a = torch.cuda.FloatTensor(sizea).fill_(self.scale)\n        b = torch.cuda.FloatTensor(sizeb).fill_(self.scale)\n\n        out_list = []\n        for i in range(repeat_tensors):\n            out_list += [a.clone().to(out_type), b.clone().to(out_type)]\n\n        if inplace:\n            in_list = out_list\n        else:\n            in_list = [out.clone().to(in_type) for out in out_list]\n\n        applier(multi_tensor_scale, self.overflow_buf, [in_list, out_list], 1./self.scale)\n\n        self.assertTrue(all([torch.allclose(out, self.ref.to(out_type)) for out in out_list]))\n        self.assertTrue(self.overflow_buf.item() == 0)\n \n    def find_inf(self, sizea, sizeb, applier, repeat_tensors, in_type, out_type, t, ind, val, inplace=False):\n        self.overflow_buf.zero_()\n        a = torch.cuda.FloatTensor(sizea).fill_(self.scale)\n        b = torch.cuda.FloatTensor(sizeb).fill_(self.scale)\n\n        out_list = []\n        for i in range(repeat_tensors):\n            out_list += [a.clone().to(out_type), b.clone().to(out_type)]\n\n        if inplace:\n            in_list = out_list\n        else:\n            in_list = [out.clone().to(in_type) for out in out_list]\n\n        applier(multi_tensor_scale, self.overflow_buf, [in_list, out_list], 1./self.scale)\n\n        self.overflow_buf.zero_()\n        in_list[t][ind] = val\n        applier(multi_tensor_scale, self.overflow_buf, [in_list, out_list], 1./self.scale)\n        self.assertTrue(self.overflow_buf.item())\n\n    # Currently, the fused kernel gives a hard error if you attempt to downscale\n    # into fp16 output, which imo is the desired behavior.  Maybe someday we\n    # will learn otherwise.\n    # @unittest.skipIf(disabled, ""amp_C is unavailable"")\n    # def test_fp16_to_fp16(self):\n    #     self.downscale(self.fp16, self.fp16, self.fp16_ref)\n    # \n    # @unittest.skipIf(disabled, ""amp_C is unavailable"")\n    # def test_fp32_to_fp16(self):\n    #     self.downscale(self.fp32, self.fp16, self.fp16_ref)\n\n    @unittest.skipIf(disabled, ""amp_C is unavailable"")\n    def test_fuzz(self):\n        input_size_pairs = (\n            (7777*77, 555*555),\n            (777, 555),\n            (555, 2048*32+1),\n            (2048*32+1, 555),\n            (555, 2048*32),\n            (2048*32, 555),\n            (33333, 555),\n            (555, 33333))\n        appliers = (\n            MultiTensorApply(2048*32), \n            MultiTensorApply(333),\n            MultiTensorApply(33333))\n        repeat_tensors = (\n            1,\n            55)\n\n        for sizea, sizeb in input_size_pairs:\n          for applier in appliers:\n            for repeat in repeat_tensors:\n              for in_type in (torch.float32, torch.float16):\n                for out_type in (torch.float32, torch.float16):\n                  for inplace in (True, False):\n                    if inplace is True and (out_type is not in_type):\n                      continue\n                    else:\n                      self.downscale(sizea, sizeb, applier, repeat, in_type, out_type, inplace=inplace)\n                      self.find_inf(sizea, sizeb, applier, repeat, in_type, out_type,\n                                    0, 0, float(\'nan\'), inplace=inplace)\n                      self.find_inf(sizea, sizeb, applier, repeat, in_type, out_type,\n                                    2*repeat-1, sizeb-1, float(\'inf\'), inplace=inplace)\n                      self.find_inf(sizea, sizeb, applier, repeat, in_type, out_type,\n                                   2*(repeat//2), sizea//2, float(\'inf\'), inplace=inplace)\n\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
apex/tests/L0/run_amp/test_multiple_models_optimizers_losses.py,34,"b'import unittest\n\nimport functools as ft\nimport itertools as it\n\nfrom apex import amp\nfrom apex.amp import _amp_state\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\n\nfrom utils import common_init, HALF, FLOAT,\\\n    ALWAYS_HALF, ALWAYS_FLOAT, MATCH_INPUT\n\nclass MyModel(torch.nn.Module):\n    def __init__(self, unique):\n        super(MyModel, self).__init__()\n        self.weight0 = Parameter(unique +\n            torch.arange(2, device=\'cuda\', dtype=torch.float32))\n        self.weight1 = Parameter(1. + unique + torch.arange(2, device=\'cuda\', dtype=torch.float16))\n\n    @staticmethod\n    def ops(input, weight0, weight1):\n        return ((input*(weight0.float()))*(weight1.float())).sum()\n\n    def forward(self, input):\n        return self.ops(input, self.weight0, self.weight1)\n\n# Abandon all hope, ye who enter here.\n\n# This is hands down the ugliest code I have ever written, but it succeeds in testing\n# multiple models/optimizers/losses fairly thoroughly.  Many of the different test cases\n# require slightly divergent code in a way that seems near-impossible to genericize into a simple\n# cross product or nested loops.\n\nclass TestMultipleModelsOptimizersLosses(unittest.TestCase):\n    def setUp(self):\n        self.x = torch.ones((2), device=\'cuda\', dtype=torch.float32)\n        common_init(self)\n\n    def tearDown(self):\n        pass\n\n    def test_2models2losses1optimizer(self):\n        model0 = MyModel(1)\n        model1 = MyModel(2)\n\n        optimizer = torch.optim.SGD([{\'params\' : model0.parameters(), \'lr\' : 0.25},\n                                     {\'params\' : model1.parameters(), \'lr\' : 0.5}],\n                                    momentum=0.125)\n\n        reference_grads = []\n        for i in range(2):\n            optimizer.zero_grad()\n            loss0 = model0(self.x)\n            loss1 = model1(self.x)\n            loss0.backward()\n            loss1.backward()\n\n            reference_grads.append([param.grad.data.clone() for param in model0.parameters()] +\n                                   [param.grad.data.clone() for param in model1.parameters()])\n\n            optimizer.step()\n\n        final_params = [param.data.clone() for param in model0.parameters()] + \\\n                       [param.data.clone() for param in model1.parameters()]\n\n        for opt_level in (""O0"", ""O1"", ""O2"", ""O3""):\n          for how_to_zero in (""none"", ""model"", ""optimizer""):\n            for use_multiple_loss_scalers in (True, False):\n              if opt_level == ""O1"" or opt_level == ""O2"":\n                  inject_inf_iters = (-1, 0, 1)\n              else:\n                  inject_inf_iters = (-1,)\n\n              for inject_inf in inject_inf_iters:\n                if inject_inf >= 0:\n                   inject_inf_locs = (""fp16"", ""fp32"")\n                   which_backwards = (0, 1)\n                else:\n                   inject_inf_locs = (""fdsa"",)\n                   which_backwards = (None,)\n\n                for inject_inf_loc in inject_inf_locs:\n                  for which_backward in which_backwards:\n                      if use_multiple_loss_scalers:\n                          num_losses = 2\n                          loss_ids = [0, 1]\n                      else:\n                          num_losses = 1\n                          loss_ids = [0, 0]\n\n                      if inject_inf >= 0:\n                          iters = 3\n                      else:\n                          iters = 2\n\n                      model0 = MyModel(1)\n                      model1 = MyModel(2)\n\n                      models = [model0, model1]\n\n                      optimizer = torch.optim.SGD([{\'params\' : model0.parameters(), \'lr\' : 0.25},\n                                                   {\'params\' : model1.parameters(), \'lr\' : 0.5}],\n                                                  momentum=0.125)\n\n                      _amp_state.allow_incoming_model_not_fp32 = True\n                      [model0, model1], optimizer = amp.initialize(\n                          [model0, model1],\n                          optimizer,\n                          opt_level=opt_level,\n                          verbosity=0,\n                          cast_model_type=False,\n                          num_losses=num_losses)\n                      _amp_state.allow_incoming_model_not_fp32 = False\n\n                      _amp_state.loss_scalers[0]._loss_scale = 4.0\n                      if use_multiple_loss_scalers:\n                          _amp_state.loss_scalers[1]._loss_scale = 16.0\n\n                      unskipped = 0\n                      for i in range(iters):\n                          if how_to_zero == ""none"":\n                              for model in models:\n                                  for param in model.parameters():\n                                      param.grad = None\n                          elif how_to_zero == ""model"":\n                              for model in models:\n                                  model.zero_grad()\n                          else:\n                              optimizer.zero_grad()\n\n                          loss0 = model0(self.x)\n                          loss1 = model1(self.x)\n\n                          with amp.scale_loss(loss0, optimizer, loss_id=loss_ids[0]) as scaled_loss:\n                              scaled_loss.backward()\n                              if i == inject_inf and which_backward == 0:\n                                  if inject_inf_loc == ""fp32"":\n                                      model0.weight0.grad[0] = float(\'inf\')\n                                  elif inject_inf_loc == ""fp16"":\n                                      model0.weight1.grad[0] = float(\'inf\')\n                          with amp.scale_loss(loss1, optimizer, loss_id=loss_ids[1]) as scaled_loss:\n                              scaled_loss.backward()\n                              if i == inject_inf and which_backward == 1:\n                                  if inject_inf_loc == ""fp32"":\n                                      model1.weight0.grad[0] = float(\'inf\')\n                                  elif inject_inf_loc == ""fp16"":\n                                      model1.weight1.grad[0] = float(\'inf\')\n\n                          if i != inject_inf:\n                              for param, reference_grad in zip(amp.master_params(optimizer),\n                                                               reference_grads[unskipped]):\n                                  self.assertTrue(torch.allclose(param.grad.float(), reference_grad.float()))\n                              unskipped += 1\n                          optimizer.step()\n\n                      model_params = [p for p in model0.parameters()] + [p for p in model1.parameters()]\n                      for model, master, reference in zip(\n                              model_params,\n                              amp.master_params(optimizer),\n                              final_params):\n                          self.assertTrue(torch.allclose(model, reference))\n                          self.assertTrue(torch.allclose(model, master.to(model.dtype)))\n\n                      if opt_level == ""O1"":\n                          _amp_state.handle._deactivate()\n\n    def test_3models2losses1optimizer(self):\n\n        model0 = MyModel(1)\n        model1 = MyModel(2)\n        model2 = MyModel(3)\n\n        optimizer = torch.optim.SGD([{\'params\' : model0.parameters(), \'lr\' : 0.25},\n                                     {\'params\' : model1.parameters(), \'lr\' : 0.5},\n                                     {\'params\' : model2.parameters(), \'lr\' : 0.125}],\n                                     momentum=0.125)\n\n        reference_grads = []\n        for i in range(2):\n            optimizer.zero_grad()\n            loss0 = model0(self.x) + model2(self.x)\n            loss1 = model1(self.x) + model2(self.x)\n            loss0.backward()\n            loss1.backward()\n\n            reference_grads.append([param.grad.data.clone() for param in model0.parameters()] +\n                                   [param.grad.data.clone() for param in model1.parameters()] +\n                                   [param.grad.data.clone() for param in model2.parameters()])\n\n            optimizer.step()\n\n\n        final_params = [param.data.clone() for param in model0.parameters()] + \\\n                       [param.data.clone() for param in model1.parameters()] + \\\n                       [param.data.clone() for param in model2.parameters()]\n\n        for opt_level in (""O0"", ""O1"", ""O2"", ""O3""):\n          for how_to_zero in (""none"", ""model"", ""optimizer""):\n            for use_multiple_loss_scalers in (True, False):\n              if opt_level == ""O1"" or opt_level == ""O2"":\n                  inject_inf_iters = (-1, 0, 1)\n              else:\n                  inject_inf_iters = (-1,)\n\n              for inject_inf in inject_inf_iters:\n                if inject_inf >= 0:\n                   inject_inf_locs = (""fp16"", ""fp32"")\n                   which_backwards = (0, 1)\n                else:\n                   inject_inf_locs = (""fdsa"",)\n                   which_backwards = (None,)\n\n                for inject_inf_loc in inject_inf_locs:\n                  for which_backward in which_backwards:\n                    if use_multiple_loss_scalers:\n                        num_losses = 2\n                        loss_ids = [0, 1]\n                    else:\n                        num_losses = 1\n                        loss_ids = [0, 0]\n\n                    if inject_inf >= 0:\n                        iters = 3\n                        if which_backward == 0:\n                            which_models = (0, 2)\n                        elif which_backward == 1:\n                            which_models = (1, 2)\n                    else:\n                        iters = 2\n                        which_models = (None,)\n\n                    for which_model in which_models:\n                        model0 = MyModel(1)\n                        model1 = MyModel(2)\n                        model2 = MyModel(3)\n\n                        models = [model0, model1, model2]\n\n                        optimizer = torch.optim.SGD([{\'params\' : model0.parameters(), \'lr\' : 0.25},\n                                                     {\'params\' : model1.parameters(), \'lr\' : 0.5},\n                                                     {\'params\' : model2.parameters(), \'lr\' : 0.125}],\n                                                     momentum=0.125)\n\n                        _amp_state.allow_incoming_model_not_fp32 = True\n                        [model0, model1, model2], optimizer = amp.initialize(\n                            [model0, model1, model2],\n                            optimizer,\n                            opt_level=opt_level,\n                            verbosity=0,\n                            cast_model_type=False,\n                            num_losses=num_losses)\n                        _amp_state.allow_incoming_model_not_fp32 = False\n\n                        _amp_state.loss_scalers[0]._loss_scale = 4.0\n                        if use_multiple_loss_scalers:\n                            _amp_state.loss_scalers[1]._loss_scale = 16.0\n\n                        unskipped = 0\n                        for i in range(iters):\n                            if how_to_zero == ""none"":\n                                for model in models:\n                                    for param in model.parameters():\n                                        param.grad = None\n                            elif how_to_zero == ""model"":\n                                for model in models:\n                                    model.zero_grad()\n                            else:\n                                optimizer.zero_grad()\n\n                            # print(""opt_level {} i {} inject_inf {} which_backward {} inject_inf_loc {} which_model {} use_multiple_loss_scalers {}"".format(opt_level, i, inject_inf, which_backward, inject_inf_loc, which_model, use_multiple_loss_scalers))\n\n                            loss0 = model0(self.x) + model2(self.x)\n                            loss1 = model1(self.x) + model2(self.x)\n\n                            with amp.scale_loss(loss0, optimizer, loss_id=loss_ids[0]) as scaled_loss:\n                                scaled_loss.backward()\n                                if i == inject_inf and which_backward == 0:\n                                    if which_model == 0:\n                                        inj_model = model0\n                                    elif which_model == 2:\n                                        inj_model = model2\n                                    else:\n                                        raise RuntimeError(which_model + "" invalid for loss 0"")\n                                    if inject_inf_loc == ""fp32"":\n                                        inj_model.weight0.grad[0] = float(\'inf\')\n                                    elif inject_inf_loc == ""fp16"":\n                                        inj_model.weight1.grad[0] = float(\'inf\')\n                            with amp.scale_loss(loss1, optimizer, loss_id=loss_ids[1]) as scaled_loss:\n                                scaled_loss.backward()\n                                if i == inject_inf and which_backward == 1:\n                                    if which_model == 1:\n                                        inj_model = model1\n                                    elif which_model == 2:\n                                        inj_model = model2\n                                    else:\n                                        raise RuntimeError(which_model + "" invalid for loss 1 "")\n                                    if inject_inf_loc == ""fp32"":\n                                        inj_model.weight0.grad[0] = float(\'inf\')\n                                    elif inject_inf_loc == ""fp16"":\n                                        inj_model.weight1.grad[0] = float(\'inf\')\n\n                            if i != inject_inf:\n                                for param, reference_grad in zip(amp.master_params(optimizer),\n                                                                 reference_grads[unskipped]):\n                                    self.assertTrue(torch.allclose(param.grad.float(), reference_grad.float()))\n                                unskipped += 1\n\n                            optimizer.step()\n\n                        model_params = [p for p in model0.parameters()] + \\\n                                       [p for p in model1.parameters()] + \\\n                                       [p for p in model2.parameters()]\n                        for model, master, reference in zip(\n                                model_params,\n                                amp.master_params(optimizer),\n                                final_params):\n                            self.assertTrue(torch.allclose(model, reference))\n                            self.assertTrue(torch.allclose(model, master.to(model.dtype)))\n\n                        if opt_level == ""O1"":\n                            _amp_state.handle._deactivate()\n\n    def test_2models2losses2optimizers(self):\n        model0 = MyModel(1)\n        model1 = MyModel(2)\n\n        optimizer0 = torch.optim.SGD([{\'params\' : model0.parameters(), \'lr\' : 0.25}],\n                                      momentum=0.125)\n        optimizer1 = torch.optim.SGD([{\'params\' : model1.parameters(), \'lr\' : 0.5}],\n                                      momentum=0.25)\n\n        # Don\'t do it like this:  reference_grads = [[]]*5\n        # because then it creates a list of 5 references to the same ""[]"" and appending\n        # to any of them effectively makes you append to all of them, which multiplies\n        # the resulting size of reference_grads by 5x and needless to say makes the test fail.\n        reference_grads = [[], [], [], [], []]\n        final_params = [None, None, None, None, None]\n        for i in range(2):\n            optimizer0.zero_grad()\n            optimizer1.zero_grad()\n            loss0 = model0(self.x)\n            loss1 = model1(self.x)\n            loss0.backward()\n            loss1.backward()\n\n            reference_grads[0].append([param.grad.data.clone() for param in model0.parameters()] +\n                                   [param.grad.data.clone() for param in model1.parameters()])\n\n            optimizer0.step()\n            optimizer1.step()\n\n        final_params[0] = [param.data.clone() for param in model0.parameters()] + \\\n                          [param.data.clone() for param in model1.parameters()]\n\n        def what_got_skipped(which_iter, which_backward):\n            if which_iter == 0 and which_backward == 0:\n                return 1\n            if which_iter == 0 and which_backward == 1:\n                return 2\n            if which_iter == 1 and which_backward == 0:\n                return 3\n            if which_iter == 1 and which_backward == 1:\n                return 4\n            return 0\n\n        for which_iter in (0,1):\n            for which_backward in (0,1):\n                model0 = MyModel(1)\n                model1 = MyModel(2)\n\n                optimizer0 = torch.optim.SGD([{\'params\' : model0.parameters(), \'lr\' : 0.25}],\n                                              momentum=0.125)\n                optimizer1 = torch.optim.SGD([{\'params\' : model1.parameters(), \'lr\' : 0.5}],\n                                              momentum=0.25)\n\n                for i in range(3):\n                    optimizer0.zero_grad()\n                    optimizer1.zero_grad()\n                    loss0 = model0(self.x)\n                    loss1 = model1(self.x)\n                    loss0.backward()\n                    loss1.backward()\n\n                    if i != which_iter:\n                        reference_grads[what_got_skipped(which_iter, which_backward)].append(\n                            [param.grad.data.clone() for param in model0.parameters()] +\n                            [param.grad.data.clone() for param in model1.parameters()])\n\n                    if i == which_iter:\n                        if which_backward == 0:\n                            optimizer1.step()\n                        else:\n                            optimizer0.step()\n                    else:\n                        optimizer0.step()\n                        optimizer1.step()\n\n                final_params[what_got_skipped(which_iter, which_backward)] = \\\n                    [param.data.clone() for param in model0.parameters()] + \\\n                    [param.data.clone() for param in model1.parameters()]\n\n        for opt_level in (""O0"", ""O1"", ""O2"", ""O3""):\n          for how_to_zero in (""none"", ""model"", ""optimizer""):\n            for use_multiple_loss_scalers in (True, False):\n              if opt_level == ""O1"" or opt_level == ""O2"":\n                  inject_inf_iters = (-1, 0, 1)\n              else:\n                  inject_inf_iters = (-1,)\n\n              for inject_inf in inject_inf_iters:\n                if inject_inf >= 0:\n                   inject_inf_locs = (""fp16"", ""fp32"")\n                   which_backwards = (0, 1)\n                else:\n                   inject_inf_locs = (""fdsa"",)\n                   which_backwards = (None,)\n\n                for inject_inf_loc in inject_inf_locs:\n                  for which_backward in which_backwards:\n                      if use_multiple_loss_scalers:\n                          num_losses = 2\n                          loss_ids = [0, 1]\n                      else:\n                          num_losses = 1\n                          loss_ids = [0, 0]\n\n                      if inject_inf >= 0:\n                          iters = 3\n                      else:\n                          iters = 2\n\n                      model0 = MyModel(1)\n                      model1 = MyModel(2)\n\n                      models = [model0, model1]\n\n                      optimizer0 = torch.optim.SGD([{\'params\' : model0.parameters(), \'lr\' : 0.25}],\n                                                    momentum=0.125)\n                      optimizer1 = torch.optim.SGD([{\'params\' : model1.parameters(), \'lr\' : 0.5}],\n                                                    momentum=0.25)\n\n                      _amp_state.allow_incoming_model_not_fp32 = True\n                      [model0, model1], [optimizer0, optimizer1] = amp.initialize(\n                          [model0, model1],\n                          [optimizer0, optimizer1],\n                          opt_level=opt_level,\n                          verbosity=0,\n                          cast_model_type=False,\n                          num_losses=num_losses)\n                      _amp_state.allow_incoming_model_not_fp32 = False\n\n                      _amp_state.loss_scalers[0]._loss_scale = 4.0\n                      if use_multiple_loss_scalers:\n                          _amp_state.loss_scalers[1]._loss_scale = 16.0\n\n                      unskipped = 0\n                      for i in range(iters):\n                          if how_to_zero == ""none"":\n                              for model in models:\n                                  for param in model.parameters():\n                                      param.grad = None\n                          elif how_to_zero == ""model"":\n                              for model in models:\n                                  model.zero_grad()\n                          else:\n                              optimizer0.zero_grad()\n                              optimizer1.zero_grad()\n\n                          loss0 = model0(self.x)\n                          loss1 = model1(self.x)\n\n                          with amp.scale_loss(loss0, optimizer0, loss_id=loss_ids[0]) as scaled_loss:\n                              scaled_loss.backward()\n                              if i == inject_inf and which_backward == 0:\n                                  if inject_inf_loc == ""fp32"":\n                                      model0.weight0.grad[0] = float(\'inf\')\n                                  elif inject_inf_loc == ""fp16"":\n                                      model0.weight1.grad[0] = float(\'inf\')\n                          with amp.scale_loss(loss1, optimizer1, loss_id=loss_ids[1]) as scaled_loss:\n                              scaled_loss.backward()\n                              if i == inject_inf and which_backward == 1:\n                                  if inject_inf_loc == ""fp32"":\n                                      model1.weight0.grad[0] = float(\'inf\')\n                                  elif inject_inf_loc == ""fp16"":\n                                      model1.weight1.grad[0] = float(\'inf\')\n\n                          # print(""opt_level {} i {} inject_inf {} which_backward {} inject_inf_loc {} use_multiple_loss_scalers {}"".format(opt_level, i, inject_inf, which_backward, inject_inf_loc, use_multiple_loss_scalers))\n\n                          if i != inject_inf:\n                              master_params = list(amp.master_params(optimizer0)) + \\\n                                              list(amp.master_params(optimizer1))\n                              for param, reference_grad in zip(master_params,\n                                      reference_grads[what_got_skipped(inject_inf, which_backward)][unskipped]):\n                                  self.assertTrue(torch.allclose(param.grad.float(), reference_grad.float()))\n                              unskipped += 1\n\n                          optimizer0.step()\n                          optimizer1.step()\n\n                      model_params = [p for p in model0.parameters()] + [p for p in model1.parameters()]\n                      master_params = [p for p in amp.master_params(optimizer0)] + \\\n                                      [p for p in amp.master_params(optimizer1)]\n                      for model, master, reference in zip(\n                              model_params,\n                              master_params,\n                              final_params[what_got_skipped(inject_inf, which_backward)]):\n                          self.assertTrue(torch.allclose(model, reference))\n                          self.assertTrue(torch.allclose(model, master.to(model.dtype)))\n\n                      if opt_level == ""O1"":\n                          _amp_state.handle._deactivate()\n\n    def test_3models2losses2optimizers(self):\n        model0 = MyModel(1)\n        model1 = MyModel(2)\n        model2 = MyModel(3)\n\n        optimizer0 = torch.optim.SGD([{\'params\' : model0.parameters(), \'lr\' : 0.25},\n                                      {\'params\' : model1.parameters(), \'lr\' : 1.0}],\n                                     momentum=0.5)\n        optimizer1 = torch.optim.SGD([{\'params\' : model2.parameters(), \'lr\' : 0.5}],\n                                     momentum=0.25)\n\n        # Again, can\'t do this:  reference_grads = [[]]*9\n        reference_grads = [[], [], [], [], [], [], [], [], []]\n        final_params = [None, None, None, None, None, None, None, None, None]\n        for i in range(2):\n            optimizer0.zero_grad()\n            optimizer1.zero_grad()\n            loss0 = model0(self.x) + model1(self.x)\n            loss1 = model2(self.x) + model1(self.x)\n            loss0.backward()\n            loss1.backward()\n\n            reference_grads[0].append([param.grad.data.clone() for param in model0.parameters()] +\n                                   [param.grad.data.clone() for param in model1.parameters()])\n\n            optimizer0.step()\n            optimizer1.step()\n\n        final_params[0] = \\\n            [param.data.clone() for param in model0.parameters()] + \\\n            [param.data.clone() for param in model1.parameters()] + \\\n            [param.data.clone() for param in model2.parameters()]\n\n        def what_got_skipped(which_iter, which_backward, which_model):\n            if which_iter == 0:\n                if which_backward == 0:\n                    if which_model == 0:\n                        return 1\n                    if which_model == 1:\n                        return 2\n                if which_backward == 1:\n                    if which_model == 2:\n                        return 3\n                    if which_model == 1:\n                        return 4\n            if which_iter == 1:\n                if which_backward == 0:\n                    if which_model == 0:\n                        return 5\n                    if which_model == 1:\n                        return 6\n                if which_backward == 1:\n                    if which_model == 2:\n                        return 7\n                    if which_model == 1:\n                        return 8\n            return 0\n\n        for which_iter in (0,1):\n            for which_backward in (0,1):\n                if which_backward == 0:\n                    which_models = (0,1)\n                if which_backward == 1:\n                    which_models = (2,1)\n                for which_model in which_models:\n\n                    model0 = MyModel(1)\n                    model1 = MyModel(2)\n                    model2 = MyModel(3)\n\n                    optimizer0 = torch.optim.SGD([{\'params\' : model0.parameters(), \'lr\' : 0.25},\n                                                  {\'params\' : model1.parameters(), \'lr\' : 1.0}],\n                                                 momentum=0.5)\n                    optimizer1 = torch.optim.SGD([{\'params\' : model2.parameters(), \'lr\' : 0.5}],\n                                                 momentum=0.25)\n\n                    for i in range(3):\n                        optimizer0.zero_grad()\n                        optimizer1.zero_grad()\n                        loss0 = model0(self.x) + model1(self.x)\n                        loss1 = model2(self.x) + model1(self.x)\n                        loss0.backward()\n                        loss1.backward()\n\n                        if i != which_iter:\n                            reference_grads[what_got_skipped(which_iter,\n                                    which_backward, which_model)].append(\n                                [param.grad.data.clone() for param in model0.parameters()] +\n                                [param.grad.data.clone() for param in model1.parameters()])\n\n                        if i == which_iter:\n                            if which_backward == 0:\n                                # if which_model == 0:\n                                    optimizer1.step()\n                                # if which_model == 1:\n                                #     optimizer1.step()\n                            if which_backward == 1:\n                                # if which_model == 2:\n                                #     optimizer0.step()\n                                # if which_model == 1:\n                                    continue\n                        else:\n                            optimizer0.step()\n                            optimizer1.step()\n\n                    final_params[what_got_skipped(which_iter, which_backward, which_model)] = \\\n                        [param.data.clone() for param in model0.parameters()] + \\\n                        [param.data.clone() for param in model1.parameters()] + \\\n                        [param.data.clone() for param in model2.parameters()]\n\n        for opt_level in (""O0"", ""O1"", ""O2"", ""O3""):\n          for how_to_zero in (""none"", ""model"", ""optimizer""):\n            for use_multiple_loss_scalers in (True, False):\n              if opt_level == ""O1"" or opt_level == ""O2"":\n                  inject_inf_iters = (-1, 0, 1)\n              else:\n                  inject_inf_iters = (-1,)\n\n              for inject_inf in inject_inf_iters:\n                if inject_inf >= 0:\n                   inject_inf_locs = (""fp16"", ""fp32"")\n                   which_backwards = (0, 1)\n                else:\n                   inject_inf_locs = (""fdsa"",)\n                   which_backwards = (None,)\n\n                for inject_inf_loc in inject_inf_locs:\n                  for which_backward in which_backwards:\n                    if use_multiple_loss_scalers:\n                        num_losses = 2\n                        loss_ids = [0, 1]\n                    else:\n                        num_losses = 1\n                        loss_ids = [0, 0]\n\n                    if inject_inf >= 0:\n                        iters = 3\n                        if which_backward == 0:\n                            which_models = (0, 1)\n                        elif which_backward == 1:\n                            which_models = (2, 1)\n                    else:\n                        iters = 2\n                        which_models = (None,)\n\n                    for which_model in which_models:\n                        model0 = MyModel(1)\n                        model1 = MyModel(2)\n                        model2 = MyModel(3)\n\n                        models = [model0, model1, model2]\n\n                        optimizer0 = torch.optim.SGD([{\'params\' : model0.parameters(), \'lr\' : 0.25},\n                                                      {\'params\' : model1.parameters(), \'lr\' : 1.0}],\n                                                     momentum=0.5)\n                        optimizer1 = torch.optim.SGD([{\'params\' : model2.parameters(), \'lr\' : 0.5}],\n                                                     momentum=0.25)\n\n                        _amp_state.allow_incoming_model_not_fp32 = True\n                        [model0, model1, model2], [optimizer0, optimizer1] = amp.initialize(\n                            [model0, model1, model2],\n                            [optimizer0, optimizer1],\n                            opt_level=opt_level,\n                            verbosity=0,\n                            cast_model_type=False,\n                            num_losses=num_losses)\n                        _amp_state.allow_incoming_model_not_fp32 = False\n\n                        _amp_state.loss_scalers[0]._loss_scale = 4.0\n                        if use_multiple_loss_scalers:\n                            _amp_state.loss_scalers[1]._loss_scale = 16.0\n\n                        unskipped = 0\n                        for i in range(iters):\n                            if how_to_zero == ""none"":\n                                for model in models:\n                                    for param in model.parameters():\n                                        param.grad = None\n                            elif how_to_zero == ""model"":\n                                for model in models:\n                                    model.zero_grad()\n                            else:\n                                optimizer0.zero_grad()\n                                optimizer1.zero_grad()\n\n                            loss0 = model0(self.x) + model1(self.x)\n                            loss1 = model2(self.x) + model1(self.x)\n\n                            with amp.scale_loss(loss0, optimizer0, loss_id=loss_ids[0]) as scaled_loss:\n                                scaled_loss.backward()\n                                if i == inject_inf and which_backward == 0:\n                                    if which_model == 0:\n                                        inj_model = model0\n                                    elif which_model == 1:\n                                        inj_model = model1\n                                    else:\n                                        raise RuntimeError(which_model + "" invalid for loss 0"")\n                                    if inject_inf_loc == ""fp32"":\n                                        inj_model.weight0.grad[0] = float(\'inf\')\n                                    elif inject_inf_loc == ""fp16"":\n                                        inj_model.weight1.grad[0] = float(\'inf\')\n                            with amp.scale_loss(loss1, [optimizer0, optimizer1], loss_id=loss_ids[1]) as scaled_loss:\n                                scaled_loss.backward()\n                                if i == inject_inf and which_backward == 1:\n                                    if which_model == 2:\n                                        inj_model = model2\n                                    elif which_model == 1:\n                                        inj_model = model1\n                                    else:\n                                        raise RuntimeError(which_model + "" invalid for loss 1 "")\n                                    if inject_inf_loc == ""fp32"":\n                                        inj_model.weight0.grad[0] = float(\'inf\')\n                                    elif inject_inf_loc == ""fp16"":\n                                        inj_model.weight1.grad[0] = float(\'inf\')\n\n                            if i != inject_inf:\n                                master_params = list(amp.master_params(optimizer0)) + \\\n                                                list(amp.master_params(optimizer1))\n                                for param, reference_grad in zip(master_params,\n                                      reference_grads[what_got_skipped(inject_inf,\n                                          which_backward, which_model)][unskipped]):\n                                    self.assertTrue(torch.allclose(param.grad.float(), reference_grad.float()))\n                                unskipped += 1\n\n                            optimizer0.step()\n                            optimizer1.step()\n\n                        model_params = [p for p in model0.parameters()] + \\\n                                       [p for p in model1.parameters()] + \\\n                                       [p for p in model2.parameters()]\n                        master_params = [p for p in amp.master_params(optimizer0)] + \\\n                                        [p for p in amp.master_params(optimizer1)]\n\n                        # print(""opt_level {} i {} inject_inf {} which_backward {} inject_inf_loc {} use_multiple_loss_scalers {} which_model {}"".format(opt_level, i, inject_inf, which_backward, inject_inf_loc, use_multiple_loss_scalers, which_model))\n\n                        for model, master, reference in zip(\n                                model_params,\n                                master_params,\n                                final_params[what_got_skipped(inject_inf, which_backward, which_model)]):\n                            self.assertTrue(torch.allclose(model, reference))\n                            self.assertTrue(torch.allclose(model, master.to(model.dtype)))\n\n                        if opt_level == ""O1"":\n                            _amp_state.handle._deactivate()\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
apex/tests/L0/run_amp/test_promotion.py,13,"b""import unittest\n\nimport itertools as it\n\nfrom apex import amp\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom utils import common_init, HALF, FLOAT, DTYPES\n\nclass TestPromotion(unittest.TestCase):\n    def setUp(self):\n        self.handle = amp.init(enabled=True)\n        common_init(self)\n\n    def tearDown(self):\n        self.handle._deactivate()\n\n    def run_binary_promote_test(self, fns, input_shape, x_inplace=False):\n        type_pairs = it.product(DTYPES, DTYPES)\n        for fn, (xtype, ytype) in it.product(fns, type_pairs):\n            x = torch.randn(input_shape, dtype=xtype).requires_grad_()\n            x_leaf = x\n            if x_inplace:\n                # We need a non-leaf to call in place on\n                x = x.clone()\n            y = torch.randn(input_shape, dtype=ytype)\n            out = fn(x, y)\n            if x_inplace:\n                # In place: always match xtype\n                self.assertEqual(out.type(), x.type())\n            else:\n                # Out of place: match widest type\n                if xtype == torch.float or ytype == torch.float:\n                    self.assertEqual(out.type(), FLOAT)\n                else:\n                    self.assertEqual(out.type(), HALF)\n            out.float().sum().backward()\n            self.assertEqual(x_leaf.grad.dtype, xtype)\n\n    def test_atan2_matches_widest(self):\n        fns = [lambda x, y : torch.atan2(x, y),\n               lambda x, y : x.atan2(y)]\n        self.run_binary_promote_test(fns, (self.b,))\n\n    def test_mul_matches_widest(self):\n        fns = [lambda x, y : torch.mul(x, y),\n               lambda x, y: x.mul(y)]\n        self.run_binary_promote_test(fns, (self.b,))\n\n    def test_cat_matches_widest(self):\n        shape = self.b\n        ys = [torch.randn(shape, dtype=torch.half) for _ in range(5)]\n        x_float = torch.randn(shape)\n        out = torch.cat(ys + [x_float])\n        self.assertEqual(out.type(), FLOAT)\n        x_half = torch.randn(shape, dtype=torch.half)\n        out = torch.cat(ys + [x_half])\n        self.assertEqual(out.type(), HALF)\n\n    def test_inplace_exp_is_error_for_half(self):\n        xs = torch.randn(self.b)\n        xs.exp_()\n        self.assertEqual(xs.type(), FLOAT)\n        xs = torch.randn(self.b, dtype=torch.half)\n        with self.assertRaises(NotImplementedError):\n            xs.exp_()\n\n    def test_inplace_add_matches_self(self):\n        fn = lambda x, y: x.add_(y)\n        self.run_binary_promote_test([fn], (self.b,), x_inplace=True)\n\nif __name__ == '__main__':\n    unittest.main()\n"""
apex/tests/L0/run_amp/test_rnn.py,12,"b""import unittest\n\nfrom apex import amp\nimport random\nimport torch\nfrom torch import nn\n\nfrom utils import common_init, HALF\n\nclass TestRnnCells(unittest.TestCase):\n    def setUp(self):\n        self.handle = amp.init(enabled=True)\n        common_init(self)\n\n    def tearDown(self):\n        self.handle._deactivate()\n\n    def run_cell_test(self, cell, state_tuple=False):\n        shape = (self.b, self.h)\n        for typ in [torch.float, torch.half]:\n            xs = [torch.randn(shape, dtype=typ).requires_grad_()\n                  for _ in range(self.t)]\n            hidden_fn = lambda: torch.zeros(shape, dtype=typ)\n            if state_tuple:\n                hidden = (hidden_fn(), hidden_fn())\n            else:\n                hidden = hidden_fn()\n            outputs = []\n            for i in range(self.t):\n                hidden = cell(xs[i], hidden)\n                if state_tuple:\n                    output = hidden[0]\n                else:\n                    output = hidden\n                outputs.append(output)\n            for y in outputs:\n                self.assertEqual(y.type(), HALF)\n            outputs[-1].float().sum().backward()\n            for i, x in enumerate(xs):\n                self.assertEqual(x.grad.dtype, x.dtype)\n\n    def test_rnn_cell_is_half(self):\n        cell = nn.RNNCell(self.h, self.h)\n        self.run_cell_test(cell)\n\n    def test_gru_cell_is_half(self):\n        cell = nn.GRUCell(self.h, self.h)\n        self.run_cell_test(cell)\n\n    def test_lstm_cell_is_half(self):\n        cell = nn.LSTMCell(self.h, self.h)\n        self.run_cell_test(cell, state_tuple=True)\n\nclass TestRnns(unittest.TestCase):\n    def setUp(self):\n        self.handle = amp.init(enabled=True)\n        common_init(self)\n\n    def tearDown(self):\n        self.handle._deactivate()\n\n    def run_rnn_test(self, rnn, layers, bidir, state_tuple=False):\n        for typ in [torch.float, torch.half]:\n            x = torch.randn((self.t, self.b, self.h), dtype=typ).requires_grad_()\n            hidden_fn = lambda: torch.zeros((layers + (layers * bidir),\n                                             self.b, self.h), dtype=typ)\n            if state_tuple:\n                hidden = (hidden_fn(), hidden_fn())\n            else:\n                hidden = hidden_fn()\n            output, _ = rnn(x, hidden)\n            self.assertEqual(output.type(), HALF)\n            output[-1, :, :].float().sum().backward()\n            self.assertEqual(x.grad.dtype, x.dtype)\n\n    def test_rnn_is_half(self):\n        configs = [(1, False), (2, False), (2, True)]\n        for layers, bidir in configs:\n            rnn = nn.RNN(input_size=self.h, hidden_size=self.h, num_layers=layers,\n                         nonlinearity='relu', bidirectional=bidir)\n            self.run_rnn_test(rnn, layers, bidir)\n\n    def test_gru_is_half(self):\n        configs = [(1, False), (2, False), (2, True)]\n        for layers, bidir in configs:\n            rnn = nn.GRU(input_size=self.h, hidden_size=self.h, num_layers=layers,\n                         bidirectional=bidir)\n            self.run_rnn_test(rnn, layers, bidir)\n\n    def test_lstm_is_half(self):\n        configs = [(1, False), (2, False), (2, True)]\n        for layers, bidir in configs:\n            rnn = nn.LSTM(input_size=self.h, hidden_size=self.h, num_layers=layers,\n                         bidirectional=bidir)\n            self.run_rnn_test(rnn, layers, bidir, state_tuple=True)\n\n    def test_rnn_packed_sequence(self):\n        num_layers = 2\n        rnn = nn.RNN(input_size=self.h, hidden_size=self.h, num_layers=num_layers)\n        for typ in [torch.float, torch.half]:\n            x = torch.randn((self.t, self.b, self.h), dtype=typ).requires_grad_()\n            lens = sorted([random.randint(self.t // 2, self.t) for _ in range(self.b)],\n                          reverse=True)\n            # `pack_padded_sequence` breaks if default tensor type is non-CPU\n            torch.set_default_tensor_type(torch.FloatTensor)\n            lens = torch.tensor(lens, dtype=torch.int64, device=torch.device('cpu'))\n            packed_seq = nn.utils.rnn.pack_padded_sequence(x, lens)\n            torch.set_default_tensor_type(torch.cuda.FloatTensor)\n            hidden = torch.zeros((num_layers, self.b, self.h), dtype=typ)\n            output, _ = rnn(packed_seq, hidden)\n            self.assertEqual(output.data.type(), HALF)\n            output.data.float().sum().backward()\n            self.assertEqual(x.grad.dtype, x.dtype)\n\nif __name__ == '__main__':\n    unittest.main()\n"""
apex/tests/L0/run_amp/utils.py,10,"b""import torch\n\nHALF = 'torch.cuda.HalfTensor'\nFLOAT = 'torch.cuda.FloatTensor'\n\nDTYPES = [torch.half, torch.float]\n\nALWAYS_HALF = {torch.float: HALF,\n               torch.half: HALF}\nALWAYS_FLOAT = {torch.float: FLOAT,\n                torch.half: FLOAT}\nMATCH_INPUT = {torch.float: FLOAT,\n               torch.half: HALF}\n\ndef common_init(test_case):\n    test_case.h = 64\n    test_case.b = 16\n    test_case.c = 16\n    test_case.k = 3\n    test_case.t = 10\n    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n"""
apex/tests/L0/run_fp16util/__init__.py,0,b''
apex/tests/L0/run_fp16util/test_fp16util.py,5,"b'import unittest\n\nimport torch\nimport torch.nn as nn\n\nfrom apex.fp16_utils import FP16Model\n\n\nclass DummyBlock(nn.Module):\n    def __init__(self):\n        super(DummyBlock, self).__init__()\n\n        self.conv = nn.Conv2d(10, 10, 2)\n        self.bn = nn.BatchNorm2d(10, affine=True)\n\n    def forward(self, x):\n        return self.conv(self.bn(x))\n\n\nclass DummyNet(nn.Module):\n    def __init__(self):\n        super(DummyNet, self).__init__()\n\n        self.conv1 = nn.Conv2d(3, 10, 2)\n        self.bn1 = nn.BatchNorm2d(10, affine=False)\n        self.db1 = DummyBlock()\n        self.db2 = DummyBlock()\n\n    def forward(self, x):\n        out = x\n        out = self.conv1(out)\n        out = self.bn1(out)\n        out = self.db1(out)\n        out = self.db2(out)\n        return out\n\n\nclass DummyNetWrapper(nn.Module):\n    def __init__(self):\n        super(DummyNetWrapper, self).__init__()\n\n        self.bn = nn.BatchNorm2d(3, affine=True)\n        self.dn = DummyNet()\n\n    def forward(self, x):\n        return self.dn(self.bn(x))\n\n\nclass TestFP16Model(unittest.TestCase):\n    def setUp(self):\n        self.N = 64\n        self.C_in = 3\n        self.H_in = 16\n        self.W_in = 32\n        self.in_tensor = torch.randn((self.N, self.C_in, self.H_in, self.W_in)).cuda()\n        self.orig_model = DummyNetWrapper().cuda()\n        self.fp16_model = FP16Model(self.orig_model)\n\n    def test_params_and_buffers(self):\n        exempted_modules = [\n            self.fp16_model.network.bn,\n            self.fp16_model.network.dn.db1.bn,\n            self.fp16_model.network.dn.db2.bn,\n        ]\n        for m in self.fp16_model.modules():\n            expected_dtype = torch.float if (m in exempted_modules) else torch.half\n            for p in m.parameters(recurse=False):\n                assert p.dtype == expected_dtype\n            for b in m.buffers(recurse=False):\n                assert b.dtype in (expected_dtype, torch.int64)\n\n    def test_output_is_half(self):\n        out_tensor = self.fp16_model(self.in_tensor)\n        assert out_tensor.dtype == torch.half\n\n'"
apex/tests/L0/run_fused_layer_norm/test_fused_layer_norm.py,5,"b'import unittest\nimport os\nimport random\n\nimport torch\nimport apex\n\n        \nclass TestFusedLayerNorm(unittest.TestCase):\n    def setUp(self):\n        self.module = apex.normalization.FusedLayerNorm(normalized_shape=[32, 64], elementwise_affine=False)\n        self.input_ = torch.randn(16, 32, 64)\n        torch.cuda.manual_seed(42)\n        \n    def forward_cpu(self, input_):\n        self.module.cpu()\n        return self.module(input_.cpu())\n    \n    def forward_cuda(self, input_):\n        self.module.cuda()\n        return self.module(input_.cuda())\n    \n    def test_forward_cuda(self):\n        out_ = self.forward_cuda(self.input_)\n        assert out_.is_cuda == True\n        \n    def test_forward_cpu(self):\n        out_ = self.forward_cpu(self.input_)\n        assert out_.is_cuda == False\n        \n    def test_same_output(self):\n        out_cpu = self.forward_cpu(self.input_)\n        out_cuda = self.forward_cuda(self.input_)\n        torch.testing.assert_allclose(out_cpu, out_cuda.cpu())\n        \n        \nclass TestFusedLayerNormElemWise(TestFusedLayerNorm):\n    def setUp(self):\n        self.module = apex.normalization.FusedLayerNorm(normalized_shape=[32, 64], elementwise_affine=True)\n        self.input_ = torch.randn(16, 32, 64)\n        torch.cuda.manual_seed(42)'"
apex/tests/L0/run_mixed_adam/__init__.py,0,b''
apex/tests/L0/run_mixed_adam/test_fp16_optimizer.py,8,"b""import unittest\nimport torch\nimport apex\n\nclass TestFP16Optimizer(unittest.TestCase):\n    def setUp(self, max_abs_diff=1e-3, max_rel_diff=1, iters=7):\n        self.max_abs_diff = max_abs_diff\n        self.max_rel_diff = max_rel_diff\n        self.iters = iters\n        torch.cuda.manual_seed(13337)\n\n        N, D_in, D_out = 64, 1024, 16\n        self.N = N\n        self.D_in = D_in\n        self.D_out = D_out\n        self.x = torch.randn((N, D_in), dtype=torch.float16, device='cuda')\n        self.ref_model = torch.nn.Linear(D_in, D_out).cuda().half()\n        self.tst_model = torch.nn.Linear(D_in, D_out).cuda().half()\n        for p,q in zip(self.tst_model.parameters(), self.ref_model.parameters()):\n            p.data.copy_(q.data)\n\n    def get_max_diff(self, ref_param, tst_param):\n        max_abs_diff = max_rel_diff = 0\n        for p_ref, p_tst in zip(ref_param, tst_param):\n            max_abs_diff_p = (p_ref - p_tst).abs().max().item()\n            max_rel_diff_p = ((p_ref - p_tst) / p_ref).abs().max().item()\n\n            if max_abs_diff_p > max_abs_diff:  max_abs_diff = max_abs_diff_p\n            if max_rel_diff_p > max_rel_diff:  max_rel_diff = max_rel_diff_p\n\n        return max_abs_diff, max_rel_diff\n\n    def test_fp16_optimizer(self):\n\n        ref_optim = torch.optim.Adam(self.ref_model.parameters())\n        ref_optim = apex.fp16_utils.FP16_Optimizer(ref_optim, verbose=False)\n\n        tst_optim = apex.optimizers.FusedAdam(self.tst_model.parameters())\n        tst_optim = apex.optimizers.FP16_Optimizer(tst_optim)\n\n        for i in range(self.iters):\n            ref_loss = self.ref_model(self.x).sum()\n            ref_optim.backward(ref_loss)\n            ref_optim.step()\n\n            tst_loss = self.tst_model(self.x).sum()\n            tst_optim.backward(tst_loss)\n            tst_optim.step()\n\n            max_abs_diff, max_rel_diff = self.get_max_diff(self.ref_model.parameters(), self.tst_model.parameters())\n            self.assertLessEqual(max_abs_diff, self.max_abs_diff)\n            self.assertLessEqual(max_rel_diff, self.max_rel_diff)\n\n\n    def test_loss_scaling(self):\n\n        ref_optim = torch.optim.Adam(self.ref_model.parameters())\n        ref_optim = apex.fp16_utils.FP16_Optimizer(ref_optim, static_loss_scale=128.0, verbose=False)\n\n        tst_optim = apex.optimizers.FusedAdam(self.tst_model.parameters())\n        tst_optim = apex.optimizers.FP16_Optimizer(tst_optim, static_loss_scale=128.0)\n\n        for i in range(self.iters):\n            ref_loss = self.ref_model(self.x).sum()\n            ref_optim.backward(ref_loss)\n            ref_optim.step()\n\n            tst_loss = self.tst_model(self.x).sum()\n            tst_optim.backward(tst_loss)\n            tst_optim.step()\n\n            max_abs_diff, max_rel_diff = self.get_max_diff(self.ref_model.parameters(), self.tst_model.parameters())\n            self.assertLessEqual(max_abs_diff, self.max_abs_diff)\n            self.assertLessEqual(max_rel_diff, self.max_rel_diff)\n\n    def test_parameter_groups(self):\n\n        ref_groups = [{'params': [self.ref_model.weight]},{'params': [self.ref_model.bias]}]\n        ref_optim = torch.optim.Adam(ref_groups)\n        ref_optim = apex.fp16_utils.FP16_Optimizer(ref_optim, verbose=False)\n\n        tst_groups = [{'params': [self.tst_model.weight]},{'params': [self.tst_model.bias]}]\n        tst_optim = apex.optimizers.FusedAdam(tst_groups)\n        tst_optim = apex.optimizers.FP16_Optimizer(tst_optim)\n\n        for i in range(self.iters):\n            ref_loss = self.ref_model(self.x).sum()\n            ref_optim.backward(ref_loss)\n            ref_optim.step()\n\n            tst_loss = self.tst_model(self.x).sum()\n            tst_optim.backward(tst_loss)\n            tst_optim.step()\n\n            max_abs_diff, max_rel_diff = self.get_max_diff(self.ref_model.parameters(), self.tst_model.parameters())\n            self.assertLessEqual(max_abs_diff, self.max_abs_diff)\n            self.assertLessEqual(max_rel_diff, self.max_rel_diff)\n\n    def test_grad_clip(self):\n        ref_optim = torch.optim.Adam(self.ref_model.parameters())\n        ref_optim = apex.fp16_utils.FP16_Optimizer(ref_optim, verbose=False)\n\n        tst_optim = apex.optimizers.FusedAdam(self.tst_model.parameters(), max_grad_norm=0.01)\n        tst_optim = apex.optimizers.FP16_Optimizer(tst_optim)\n\n        for i in range(self.iters):\n            ref_loss = self.ref_model(self.x).sum()\n            ref_optim.backward(ref_loss)\n            ref_optim.clip_master_grads(0.01)\n            ref_optim.step()\n\n            tst_loss = self.tst_model(self.x).sum()\n            tst_optim.backward(tst_loss)\n            tst_optim.step()\n\n            max_abs_diff, max_rel_diff = self.get_max_diff(self.ref_model.parameters(), self.tst_model.parameters())\n            self.assertLessEqual(max_abs_diff, self.max_abs_diff)\n            self.assertLessEqual(max_rel_diff, self.max_rel_diff)\n\n    @unittest.skip('Not support grad being None')\n    def test_grad_None(self):\n        self.fail()\n\n    @unittest.skip('Not support same weight decay as pytorch')\n    def test_weight_decay(self):\n        self.fail()\n\n    @unittest.skip('Not support empty parameter groups')\n    def test_group_empty(self):\n        self.fail()\n\nif __name__ == '__main__':\n    script_path = os.path.dirname(os.path.realpath(__file__))\n    unittest.main()\n"""
apex/tests/L0/run_mixed_adam/test_mixed_adam.py,16,"b""import unittest\nimport os\nimport random\n\nimport torch\nimport apex\n\nclass TestFusedAdam(unittest.TestCase):\n    def setUp(self, max_abs_diff=1e-3, max_rel_diff=1, iters=7):\n        self.max_abs_diff = max_abs_diff\n        self.max_rel_diff = max_rel_diff\n        self.iters = iters\n        torch.cuda.manual_seed(9876)\n\n    def tearDown(self):\n        pass\n\n    def gen_param_optim(self, tensors, adam_option):\n        ref_param = []\n        tst_param = []\n        for tensor in tensors:\n            ref_param.append(torch.nn.Parameter(tensor.clone()))\n            tst_param.append(torch.nn.Parameter(tensor.clone()))\n\n        ref_optim = torch.optim.Adam(ref_param, **adam_option)\n        tst_optim = apex.optimizers.FusedAdam(tst_param, **adam_option)\n       \n        return (ref_param, tst_param, ref_optim, tst_optim)\n\n    def gen_grad(self, ref_param, tst_param):\n        for p_ref, p_tst in zip(ref_param, tst_param):\n            p_ref.grad = torch.rand_like(p_ref)\n            p_tst.grad = p_ref.grad\n\n    def gen_mixed_grad(self, ref_param, tst_param, scale=1.0):\n        half_grads = []\n        for p_ref, p_tst in zip(ref_param, tst_param):\n            half_grads.append(torch.rand_like(p_ref).half())\n            p_ref.grad = half_grads[-1].float() / scale\n        return half_grads\n\n    def get_max_diff(self, ref_param, tst_param):\n        max_abs_diff = max_rel_diff = 0\n        for p_ref, p_tst in zip(ref_param, tst_param):\n            max_abs_diff_p = (p_ref - p_tst).abs().max().item()\n            max_rel_diff_p = ((p_ref - p_tst) / p_ref).abs().max().item()\n\n            if max_abs_diff_p > max_abs_diff:  max_abs_diff = max_abs_diff_p\n            if max_rel_diff_p > max_rel_diff:  max_rel_diff = max_rel_diff_p\n\n        return max_abs_diff, max_rel_diff\n\n    def gen_single_type_test(self, param_type=torch.float):\n        nelem = 278011\n        adam_option = {'lr':5e-4, 'betas':(0.9, 0.999), 'eps':1e-08,\n            'weight_decay':0, 'amsgrad':False}\n\n        tensor = torch.rand(nelem, dtype=param_type, device='cuda')\n        ref_param, tst_param, ref_optim, tst_optim = \\\n            self.gen_param_optim([tensor], adam_option)\n\n        for i in range(self.iters):\n            self.gen_grad(ref_param, tst_param)\n            ref_optim.step()\n            tst_optim.step()\n            max_abs_diff, max_rel_diff = self.get_max_diff(ref_param, tst_param)\n\n            self.assertLessEqual(max_abs_diff, self.max_abs_diff)\n            self.assertLessEqual(max_rel_diff, self.max_rel_diff)\n\n    def test_double(self):\n        self.gen_single_type_test(param_type=torch.double)\n\n    def test_float(self):\n        self.gen_single_type_test(param_type=torch.float)\n\n    def test_half(self):\n        nelem = 278011\n        adam_option = {'lr':5e-4, 'betas':(0.9, 0.999), 'eps':1e-08,\n            'weight_decay':0, 'amsgrad':False}\n\n        tensor = torch.rand(nelem, dtype=torch.float, device='cuda')\n        ref_param, tst_param, ref_optim, tst_optim = \\\n            self.gen_param_optim([tensor], adam_option)\n\n        for i in range(self.iters):\n            half_grads = self.gen_mixed_grad(ref_param, tst_param)\n            ref_optim.step()\n            tst_optim.step(grads=half_grads)\n            max_abs_diff, max_rel_diff = self.get_max_diff(ref_param, tst_param)\n\n            self.assertLessEqual(max_abs_diff, self.max_abs_diff)\n            self.assertLessEqual(max_rel_diff, self.max_rel_diff)\n\n    def test_multi_params(self):\n        sizes = [[4096, 1024], [4096], [4096, 2048], [32320, 1024], [1]]\n        adam_option = {'lr':5e-4, 'betas':(0.9, 0.999), 'eps':1e-08,\n            'weight_decay':0, 'amsgrad':False}\n\n        tensors = []\n        for size in sizes:\n            tensors.append(torch.rand(size, dtype=torch.float, device='cuda'))\n        ref_param, tst_param, ref_optim, tst_optim = \\\n            self.gen_param_optim(tensors, adam_option)\n\n        for i in range(self.iters):\n            half_grads = self.gen_mixed_grad(ref_param, tst_param)\n            ref_optim.step()\n            tst_optim.step(grads=half_grads)\n            max_abs_diff, max_rel_diff = self.get_max_diff(ref_param, tst_param)\n            self.assertLessEqual(max_abs_diff, self.max_abs_diff)\n            self.assertLessEqual(max_rel_diff, self.max_rel_diff)\n\n    def test_scale(self):\n        nelem = 278011\n        adam_option = {'lr':5e-4, 'betas':(0.9, 0.999), 'eps':1e-08,\n            'weight_decay':0, 'amsgrad':False}\n\n        tensor = torch.rand(nelem, dtype=torch.float, device='cuda')\n        ref_param, tst_param, ref_optim, tst_optim = \\\n            self.gen_param_optim([tensor], adam_option)\n\n        for i in range(self.iters):\n            scale = random.random() * 1000\n            half_grads = self.gen_mixed_grad(ref_param, tst_param, scale)\n            ref_optim.step()\n            tst_optim.step(grads=half_grads, scale=scale)\n            max_abs_diff, max_rel_diff = self.get_max_diff(ref_param, tst_param)\n\n            self.assertLessEqual(max_abs_diff, self.max_abs_diff)\n            self.assertLessEqual(max_rel_diff, self.max_rel_diff)\n\n    def test_fp16_output(self):\n        nelem = 278011\n        adam_option = {'lr':5e-4, 'betas':(0.9, 0.999), 'eps':1e-08,\n            'weight_decay':0, 'amsgrad':False}\n\n        tensor = torch.rand(nelem, dtype=torch.float, device='cuda')\n        ref_param, tst_param, ref_optim, tst_optim = \\\n            self.gen_param_optim([tensor], adam_option)\n\n        fp16_param = torch.nn.Parameter(tensor.clone().half())\n\n        for i in range(self.iters):\n            half_grads = self.gen_mixed_grad(ref_param, tst_param)\n            ref_optim.step()\n            tst_optim.step(grads=half_grads, output_params=[fp16_param])\n\n            max_abs_diff, max_rel_diff = self.get_max_diff(ref_param, tst_param)\n            self.assertLessEqual(max_abs_diff, self.max_abs_diff)\n            self.assertLessEqual(max_rel_diff, self.max_rel_diff)\n\n            max_abs_diff, max_rel_diff = self.get_max_diff(tst_param, \\\n                [fp16_param.float()])\n            self.assertLessEqual(max_abs_diff, self.max_abs_diff)\n            self.assertLessEqual(max_rel_diff, self.max_rel_diff)\n\n    def test_adam_option(self):\n        nelem = 1\n        adam_option = {'lr':0.01, 'betas':(0.6, 0.9), 'eps':3e-06,\n            'weight_decay':0, 'amsgrad':False}\n\n        tensor = torch.rand(nelem, dtype=torch.float, device='cuda')\n        ref_param, tst_param, ref_optim, tst_optim = \\\n            self.gen_param_optim([tensor], adam_option)\n\n        for i in range(self.iters):\n            self.gen_grad(ref_param, tst_param)\n            ref_optim.step()\n            tst_optim.step()\n            max_abs_diff, max_rel_diff = self.get_max_diff(ref_param, tst_param)\n\n            self.assertLessEqual(max_abs_diff, self.max_abs_diff)\n            self.assertLessEqual(max_rel_diff, self.max_rel_diff)\n\n\nif __name__ == '__main__':\n    script_path = os.path.dirname(os.path.realpath(__file__))\n    unittest.main()\n"""
apex/tests/L1/common/compare.py,4,"b'import argparse\nimport torch\n\nparser = argparse.ArgumentParser(description=\'Compare\')\nparser.add_argument(\'--opt-level\', type=str)\nparser.add_argument(\'--keep-batchnorm-fp32\', type=str, default=None)\nparser.add_argument(\'--loss-scale\', type=str, default=None)\nparser.add_argument(\'--fused-adam\', action=\'store_true\')\nparser.add_argument(\'--use_baseline\', action=\'store_true\')\nargs = parser.parse_args()\n\nbase_file = str(args.opt_level) + ""_"" +\\\n            str(args.loss_scale) + ""_"" +\\\n            str(args.keep_batchnorm_fp32) + ""_"" +\\\n            str(args.fused_adam)\n\nfile_e = ""True_"" + base_file\nfile_p = ""False_"" + base_file\nif args.use_baseline:\n    file_b = ""baselines/True_"" + base_file\n\ndict_e = torch.load(file_e)\ndict_p = torch.load(file_p)\nif args.use_baseline:\n    dict_b = torch.load(file_b)\n\ntorch.set_printoptions(precision=10)\n\nprint(file_e)\nprint(file_p)\nif args.use_baseline:\n    print(file_b)\n\n# ugly duplication here...\nif not args.use_baseline:\n    for n, (i_e, i_p) in enumerate(zip(dict_e[""Iteration""], dict_p[""Iteration""])):\n        assert i_e == i_p, ""i_e = {}, i_p = {}"".format(i_e, i_p)\n\n        loss_e = dict_e[""Loss""][n]\n        loss_p = dict_p[""Loss""][n]\n        assert loss_e == loss_p, ""Iteration {}, loss_e = {}, loss_p = {}"".format(i_e, loss_e, loss_p)\n        print(""{:4} {:15.10f} {:15.10f} {:15.10f} {:15.10f}"".format(\n              i_e,\n              loss_e,\n              loss_p,\n              dict_e[""Speed""][n],\n              dict_p[""Speed""][n]))\nelse:\n    for n, (i_e, i_p) in enumerate(zip(dict_e[""Iteration""], dict_p[""Iteration""])):\n        assert i_e == i_p, ""i_e = {}, i_p = {}"".format(i_e, i_p)\n\n        loss_e = dict_e[""Loss""][n]\n        loss_p = dict_p[""Loss""][n]\n        loss_b = dict_b[""Loss""][n]\n        assert loss_e == loss_p, ""Iteration {}, loss_e = {}, loss_p = {}"".format(i_e, loss_e, loss_p)\n        assert loss_e == loss_b, ""Iteration {}, loss_e = {}, loss_b = {}"".format(i_e, loss_e, loss_b)\n        print(""{:4} {:15.10f} {:15.10f} {:15.10f} {:15.10f} {:15.10f} {:15.10f}"".format(\n              i_e,\n              loss_b,\n              loss_e,\n              loss_p,\n              dict_b[""Speed""][n],\n              dict_e[""Speed""][n],\n              dict_p[""Speed""][n]))\n'"
apex/tests/L1/common/main_amp.py,36,"b'import argparse\nimport os\nimport shutil\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.distributed as dist\nimport torch.optim\nimport torch.utils.data\nimport torch.utils.data.distributed\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport torchvision.models as models\n\nimport numpy as np\n\ntry:\n    from apex.parallel import DistributedDataParallel as DDP\n    from apex.fp16_utils import *\n    from apex import amp, optimizers\n    from apex.multi_tensor_apply import multi_tensor_applier\nexcept ImportError:\n    raise ImportError(""Please install apex from https://www.github.com/nvidia/apex to run this example."")\n\nmodel_names = sorted(name for name in models.__dict__\n                     if name.islower() and not name.startswith(""__"")\n                     and callable(models.__dict__[name]))\n\nparser = argparse.ArgumentParser(description=\'PyTorch ImageNet Training\')\nparser.add_argument(\'data\', metavar=\'DIR\',\n                    help=\'path to dataset\')\nparser.add_argument(\'--arch\', \'-a\', metavar=\'ARCH\', default=\'resnet18\',\n                    choices=model_names,\n                    help=\'model architecture: \' +\n                    \' | \'.join(model_names) +\n                    \' (default: resnet18)\')\nparser.add_argument(\'-j\', \'--workers\', default=4, type=int, metavar=\'N\',\n                    help=\'number of data loading workers (default: 4)\')\nparser.add_argument(\'--epochs\', default=90, type=int, metavar=\'N\',\n                    help=\'number of total epochs to run\')\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'-b\', \'--batch-size\', default=256, type=int,\n                    metavar=\'N\', help=\'mini-batch size per process (default: 256)\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=0.1, type=float,\n                    metavar=\'LR\', help=\'Initial learning rate.  Will be scaled by <global batch size>/256: args.lr = args.lr*float(args.batch_size*args.world_size)/256.  A warmup schedule will also be applied over the first 5 epochs.\')\nparser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\',\n                    help=\'momentum\')\nparser.add_argument(\'--weight-decay\', \'--wd\', default=1e-4, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 1e-4)\')\nparser.add_argument(\'--print-freq\', \'-p\', default=10, type=int,\n                    metavar=\'N\', help=\'print frequency (default: 10)\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'-e\', \'--evaluate\', dest=\'evaluate\', action=\'store_true\',\n                    help=\'evaluate model on validation set\')\nparser.add_argument(\'--pretrained\', dest=\'pretrained\', action=\'store_true\',\n                    help=\'use pre-trained model\')\n\nparser.add_argument(\'--prof\', dest=\'prof\', action=\'store_true\',\n                    help=\'Only run 10 iterations for profiling.\')\nparser.add_argument(\'--deterministic\', action=\'store_true\')\n\nparser.add_argument(""--local_rank"", default=0, type=int)\nparser.add_argument(\'--sync_bn\', action=\'store_true\',\n                    help=\'enabling apex sync BN.\')\n\nparser.add_argument(\'--has-ext\', action=\'store_true\')\nparser.add_argument(\'--opt-level\', type=str)\nparser.add_argument(\'--keep-batchnorm-fp32\', type=str, default=None)\nparser.add_argument(\'--loss-scale\', type=str, default=None)\nparser.add_argument(\'--fused-adam\', action=\'store_true\')\n\nparser.add_argument(\'--prints-to-process\', type=int, default=10)\n\ncudnn.benchmark = True\n\ndef fast_collate(batch):\n    imgs = [img[0] for img in batch]\n    targets = torch.tensor([target[1] for target in batch], dtype=torch.int64)\n    w = imgs[0].size[0]\n    h = imgs[0].size[1]\n    tensor = torch.zeros( (len(imgs), 3, h, w), dtype=torch.uint8 )\n    for i, img in enumerate(imgs):\n        nump_array = np.asarray(img, dtype=np.uint8)\n        if(nump_array.ndim < 3):\n            nump_array = np.expand_dims(nump_array, axis=-1)\n        nump_array = np.rollaxis(nump_array, 2)\n\n        tensor[i] += torch.from_numpy(nump_array)\n        \n    return tensor, targets\n\nbest_prec1 = 0\nargs = parser.parse_args()\n\n# Let multi_tensor_applier be the canary in the coalmine\n# that verifies if the backend is what we think it is\nassert multi_tensor_applier.available == args.has_ext \n\nprint(""opt_level = {}"".format(args.opt_level))\nprint(""keep_batchnorm_fp32 = {}"".format(args.keep_batchnorm_fp32), type(args.keep_batchnorm_fp32))\nprint(""loss_scale = {}"".format(args.loss_scale), type(args.loss_scale))\n\n\nprint(""\\nCUDNN VERSION: {}\\n"".format(torch.backends.cudnn.version()))\n\nif args.deterministic:\n    cudnn.benchmark = False\n    cudnn.deterministic = True\n    torch.manual_seed(args.local_rank)\n    torch.set_printoptions(precision=10)\n\ndef main():\n    global best_prec1, args\n\n    args.distributed = False\n    if \'WORLD_SIZE\' in os.environ:\n        args.distributed = int(os.environ[\'WORLD_SIZE\']) > 1\n\n    args.gpu = 0\n    args.world_size = 1\n\n    if args.distributed:\n        args.gpu = args.local_rank % torch.cuda.device_count()\n        torch.cuda.set_device(args.gpu)\n        torch.distributed.init_process_group(backend=\'nccl\',\n                                             init_method=\'env://\')\n        args.world_size = torch.distributed.get_world_size()\n\n    assert torch.backends.cudnn.enabled, ""Amp requires cudnn backend to be enabled.""\n\n    # create model\n    if args.pretrained:\n        print(""=> using pre-trained model \'{}\'"".format(args.arch))\n        model = models.__dict__[args.arch](pretrained=True)\n    else:\n        print(""=> creating model \'{}\'"".format(args.arch))\n        model = models.__dict__[args.arch]()\n\n    if args.sync_bn:\n        import apex\n        print(""using apex synced BN"")\n        model = apex.parallel.convert_syncbn_model(model)\n\n    model = model.cuda()\n\n    # Scale learning rate based on global batch size\n    args.lr = args.lr*float(args.batch_size*args.world_size)/256. \n    if args.fused_adam:\n        optimizer = optimizers.FusedAdam(model.parameters())\n    else:\n        optimizer = torch.optim.SGD(model.parameters(), args.lr,\n                                    momentum=args.momentum,\n                                    weight_decay=args.weight_decay)\n\n    model, optimizer = amp.initialize(\n        model, optimizer,\n        # enabled=False,\n        opt_level=args.opt_level,\n        keep_batchnorm_fp32=args.keep_batchnorm_fp32,\n        loss_scale=args.loss_scale\n        )\n\n    if args.distributed:\n        # By default, apex.parallel.DistributedDataParallel overlaps communication with \n        # computation in the backward pass.\n        # model = DDP(model)\n        # delay_allreduce delays all communication to the end of the backward pass.\n        model = DDP(model, delay_allreduce=True)\n\n    # define loss function (criterion) and optimizer\n    criterion = nn.CrossEntropyLoss().cuda()\n\n    # Optionally resume from a checkpoint\n    if args.resume:\n        # Use a local scope to avoid dangling references\n        def resume():\n            if os.path.isfile(args.resume):\n                print(""=> loading checkpoint \'{}\'"".format(args.resume))\n                checkpoint = torch.load(args.resume, map_location = lambda storage, loc: storage.cuda(args.gpu))\n                args.start_epoch = checkpoint[\'epoch\']\n                best_prec1 = checkpoint[\'best_prec1\']\n                model.load_state_dict(checkpoint[\'state_dict\'])\n                optimizer.load_state_dict(checkpoint[\'optimizer\'])\n                print(""=> loaded checkpoint \'{}\' (epoch {})""\n                      .format(args.resume, checkpoint[\'epoch\']))\n            else:\n                print(""=> no checkpoint found at \'{}\'"".format(args.resume))\n        resume()\n\n    # Data loading code\n    traindir = os.path.join(args.data, \'train\')\n    valdir = os.path.join(args.data, \'val\')\n\n    if(args.arch == ""inception_v3""):\n        crop_size = 299\n        val_size = 320 # I chose this value arbitrarily, we can adjust.\n    else:\n        crop_size = 224\n        val_size = 256\n\n    train_dataset = datasets.ImageFolder(\n        traindir,\n        transforms.Compose([\n            transforms.RandomResizedCrop(crop_size),\n            transforms.RandomHorizontalFlip(),\n            # transforms.ToTensor(), Too slow\n            # normalize,\n        ]))\n    val_dataset = datasets.ImageFolder(valdir, transforms.Compose([\n            transforms.Resize(val_size),\n            transforms.CenterCrop(crop_size),\n        ]))\n\n    train_sampler = None\n    val_sampler = None\n    if args.distributed:\n        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n        val_sampler = torch.utils.data.distributed.DistributedSampler(val_dataset)\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=args.batch_size, shuffle=(train_sampler is None),\n        num_workers=args.workers, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)\n\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset,\n        batch_size=args.batch_size, shuffle=False,\n        num_workers=args.workers, pin_memory=True,\n        sampler=val_sampler,\n        collate_fn=fast_collate)\n\n    if args.evaluate:\n        validate(val_loader, model, criterion)\n        return\n\n    for epoch in range(args.start_epoch, args.epochs):\n        if args.distributed:\n            train_sampler.set_epoch(epoch)\n\n        # train for one epoch\n        train(train_loader, model, criterion, optimizer, epoch)\n        if args.prof:\n            break\n        # evaluate on validation set\n        prec1 = validate(val_loader, model, criterion)\n\n        # remember best prec@1 and save checkpoint\n        if args.local_rank == 0:\n            is_best = prec1 > best_prec1\n            best_prec1 = max(prec1, best_prec1)\n            save_checkpoint({\n                \'epoch\': epoch + 1,\n                \'arch\': args.arch,\n                \'state_dict\': model.state_dict(),\n                \'best_prec1\': best_prec1,\n                \'optimizer\' : optimizer.state_dict(),\n            }, is_best)\n\nclass data_prefetcher():\n    def __init__(self, loader):\n        self.loader = iter(loader)\n        self.stream = torch.cuda.Stream()\n        self.mean = torch.tensor([0.485 * 255, 0.456 * 255, 0.406 * 255]).cuda().view(1,3,1,1)\n        self.std = torch.tensor([0.229 * 255, 0.224 * 255, 0.225 * 255]).cuda().view(1,3,1,1)\n        # With Amp, it isn\'t necessary to manually convert data to half.\n        # if args.fp16:\n        #     self.mean = self.mean.half()\n        #     self.std = self.std.half()\n        self.preload()\n\n    def preload(self):\n        try:\n            self.next_input, self.next_target = next(self.loader)\n        except StopIteration:\n            self.next_input = None\n            self.next_target = None\n            return\n        with torch.cuda.stream(self.stream):\n            self.next_input = self.next_input.cuda(non_blocking=True)\n            self.next_target = self.next_target.cuda(non_blocking=True)\n            # With Amp, it isn\'t necessary to manually convert data to half.\n            # if args.fp16:\n            #     self.next_input = self.next_input.half()\n            # else:\n            self.next_input = self.next_input.float()\n            self.next_input = self.next_input.sub_(self.mean).div_(self.std)\n            \n    def next(self):\n        torch.cuda.current_stream().wait_stream(self.stream)\n        input = self.next_input\n        target = self.next_target\n        self.preload()\n        return input, target\n\n\ndef train(train_loader, model, criterion, optimizer, epoch):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    # switch to train mode\n    model.train()\n    end = time.time()\n\n    run_info_dict = {""Iteration"" : [],\n                     ""Loss"" : [],\n                     ""Speed"" : []}\n\n    prefetcher = data_prefetcher(train_loader)\n    input, target = prefetcher.next()\n    i = -1\n    while input is not None:\n        i += 1\n\n        # No learning rate warmup for this test, to expose bitwise inaccuracies more quickly\n        # adjust_learning_rate(optimizer, epoch, i, len(train_loader))\n\n        if args.prof:\n            if i > 10:\n                break\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        # compute output\n        output = model(input)\n        loss = criterion(output, target)\n\n        # measure accuracy and record loss\n        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n\n        if args.distributed:\n            reduced_loss = reduce_tensor(loss.data)\n            prec1 = reduce_tensor(prec1)\n            prec5 = reduce_tensor(prec5)\n        else:\n            reduced_loss = loss.data\n\n        losses.update(to_python_float(reduced_loss), input.size(0))\n        top1.update(to_python_float(prec1), input.size(0))\n        top5.update(to_python_float(prec5), input.size(0))\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n\n        with amp.scale_loss(loss, optimizer) as scaled_loss:\n            scaled_loss.backward()\n\n        # for param in model.parameters():\n        #     print(param.data.double().sum().item(), param.grad.data.double().sum().item())\n\n        # torch.cuda.synchronize()\n        torch.cuda.nvtx.range_push(""step"")\n        optimizer.step()\n        torch.cuda.nvtx.range_pop()\n\n        torch.cuda.synchronize()\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n\n        end = time.time()\n\n        # If you decide to refactor this test, like examples/imagenet, to sample the loss every\n        # print_freq iterations, make sure to move this prefetching below the accuracy calculation.\n        input, target = prefetcher.next()\n\n        if i % args.print_freq == 0 and i > 1:\n            if args.local_rank == 0:\n                print(\'Epoch: [{0}][{1}/{2}]\\t\'\n                      \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                      \'Speed {3:.3f} ({4:.3f})\\t\'\n                      \'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t\'\n                      \'Loss {loss.val:.10f} ({loss.avg:.4f})\\t\'\n                      \'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t\'\n                      \'Prec@5 {top5.val:.3f} ({top5.avg:.3f})\'.format(\n                       epoch, i, len(train_loader),\n                       args.world_size * args.batch_size / batch_time.val,\n                       args.world_size * args.batch_size / batch_time.avg,\n                       batch_time=batch_time,\n                       data_time=data_time, loss=losses, top1=top1, top5=top5))\n            run_info_dict[""Iteration""].append(i)\n            run_info_dict[""Loss""].append(losses.val)\n            run_info_dict[""Speed""].append(args.world_size * args.batch_size / batch_time.val)\n            if len(run_info_dict[""Loss""]) == args.prints_to_process:\n                if args.local_rank == 0:\n                    torch.save(run_info_dict,\n                               str(args.has_ext) + ""_"" + str(args.opt_level) + ""_"" +\n                               str(args.loss_scale) + ""_"" + str(args.keep_batchnorm_fp32) + ""_"" +\n                               str(args.fused_adam))\n                quit()\n\n\ndef validate(val_loader, model, criterion):\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    # switch to evaluate mode\n    model.eval()\n\n    end = time.time()\n\n    prefetcher = data_prefetcher(val_loader)\n    input, target = prefetcher.next()\n    i = -1\n    while input is not None:\n        i += 1\n\n        # compute output\n        with torch.no_grad():\n            output = model(input)\n            loss = criterion(output, target)\n\n        # measure accuracy and record loss\n        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n\n        if args.distributed:\n            reduced_loss = reduce_tensor(loss.data)\n            prec1 = reduce_tensor(prec1)\n            prec5 = reduce_tensor(prec5)\n        else:\n            reduced_loss = loss.data\n\n        losses.update(to_python_float(reduced_loss), input.size(0))\n        top1.update(to_python_float(prec1), input.size(0))\n        top5.update(to_python_float(prec5), input.size(0))\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if args.local_rank == 0 and i % args.print_freq == 0:\n            print(\'Test: [{0}/{1}]\\t\'\n                  \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                  \'Speed {2:.3f} ({3:.3f})\\t\'\n                  \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\n                  \'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t\'\n                  \'Prec@5 {top5.val:.3f} ({top5.avg:.3f})\'.format(\n                   i, len(val_loader),\n                   args.world_size * args.batch_size / batch_time.val,\n                   args.world_size * args.batch_size / batch_time.avg,\n                   batch_time=batch_time, loss=losses,\n                   top1=top1, top5=top5))\n\n        input, target = prefetcher.next()\n\n    print(\' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}\'\n          .format(top1=top1, top5=top5))\n\n    return top1.avg\n\n\ndef save_checkpoint(state, is_best, filename=\'checkpoint.pth.tar\'):\n    torch.save(state, filename)\n    if is_best:\n        shutil.copyfile(filename, \'model_best.pth.tar\')\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef adjust_learning_rate(optimizer, epoch, step, len_epoch):\n    """"""LR schedule that should yield 76% converged accuracy with batch size 256""""""\n    factor = epoch // 30\n\n    if epoch >= 80:\n        factor = factor + 1\n\n    lr = args.lr*(0.1**factor)\n\n    """"""Warmup""""""\n    if epoch < 5:\n        lr = lr*float(1 + step + epoch*len_epoch)/(5.*len_epoch)\n\n    # if(args.local_rank == 0):\n    #     print(""epoch = {}, step = {}, lr = {}"".format(epoch, step, lr))\n\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n\n\ndef accuracy(output, target, topk=(1,)):\n    """"""Computes the precision@k for the specified values of k""""""\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res\n\n\ndef reduce_tensor(tensor):\n    rt = tensor.clone()\n    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n    rt /= args.world_size\n    return rt\n\nif __name__ == \'__main__\':\n    main()\n'"
apex/tests/distributed/DDP/ddp_race_condition_test.py,19,"b'import torch\nimport torch.distributed as dist\nfrom torch.nn import Parameter\nfrom torch.nn import Module\nfrom apex.parallel import DistributedDataParallel as DDP\nimport argparse\nimport os\n\n\nparser = argparse.ArgumentParser(description=\'allreduce hook example\')\nparser.add_argument(""--local_rank"", default=0, type=int)\nargs = parser.parse_args()\n\nargs.distributed = False\nif \'WORLD_SIZE\' in os.environ:\n    args.distributed = int(os.environ[\'WORLD_SIZE\']) > 1\n\nif args.distributed:\n    args.gpu = args.local_rank % torch.cuda.device_count()\n    torch.cuda.set_device(args.gpu)\n    torch.distributed.init_process_group(backend=\'nccl\',\n                                         init_method=\'env://\')\n    args.world_size = torch.distributed.get_world_size()\n\ntorch.set_printoptions(precision=10)\ntorch.manual_seed(args.local_rank)\n\nclass Model(Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.a = Parameter(torch.cuda.FloatTensor(4096*4096).fill_(1.0))\n        self.b = Parameter(torch.cuda.FloatTensor(4096*4096).fill_(2.0))\n    def forward(self, input):\n        return (input*self.a)*self.b\n\nmodel = Model()\n# model = DDP(model, message_size=1, gradient_predivide_factor=8.0)\nmodel = DDP(model, delay_allreduce=True)\n# model = DDP(model, message_size=1, allreduce_trigger_params=[model.b])\n\nx = torch.cuda.FloatTensor(4096*4096)\n\npassed = True\ntorch.cuda.cudart().cudaProfilerStart()\nfor i in range(10):\n    x.fill_(i + args.local_rank) # fill x with new values every iteration for sanity\n    model.zero_grad()\n    out = model(x)\n    loss = out.sum()\n    # torch.cuda.nvtx.range_push(""backward"")\n    loss.backward()\n    # torch.cuda.nvtx.range_pop()\n    \n    # torch.cuda.nvtx.range_push(""synchronize() + info"")\n    # torch.cuda.synchronize()\n    print(""i = {}"".format(i))\n    def info(name, param, val):\n        expected = val*4096*4096*(2.*i+1)/2.\n        actual = param.grad.data.sum().item()\n        print(name+"": grad.data_ptr() = {}, expected sum {}, got {}"".format(\n              param.grad.data_ptr(), expected, actual))\n        return (expected == actual)\n    if not info(""model.a"", model.module.a, 2.):  passed = False\n    if not info(""model.b"", model.module.b, 1.):  passed = False\n    # torch.cuda.nvtx.range_pop()\ntorch.cuda.cudart().cudaProfilerStop()\n\nprint(""passed = "", passed)\n'"
apex/tests/distributed/amp_master_params/amp_master_params.py,17,"b'import torch\nimport argparse\nimport os\nfrom apex import amp\n# FOR DISTRIBUTED: (can also use torch.nn.parallel.DistributedDataParallel instead)\nfrom apex.parallel import DistributedDataParallel\n\nparser = argparse.ArgumentParser()\n# FOR DISTRIBUTED:  Parse for the local_rank argument, which will be supplied\n# automatically by torch.distributed.launch.\nparser.add_argument(""--local_rank"", default=0, type=int)\nargs = parser.parse_args()\n\n# FOR DISTRIBUTED:  If we are running under torch.distributed.launch,\n# the \'WORLD_SIZE\' environment variable will also be set automatically.\nargs.distributed = False\nif \'WORLD_SIZE\' in os.environ:\n    args.distributed = int(os.environ[\'WORLD_SIZE\']) > 1\n\nif args.distributed:\n    # FOR DISTRIBUTED:  Set the device according to local_rank.\n    torch.cuda.set_device(args.local_rank)\n\n    # FOR DISTRIBUTED:  Initialize the backend.  torch.distributed.launch will provide\n    # environment variables, and requires that you use init_method=`env://`.\n    torch.distributed.init_process_group(backend=\'nccl\',\n                                         init_method=\'env://\')\n\n    torch.manual_seed(torch.distributed.get_rank())\n\ntorch.backends.cudnn.benchmark = True\n\nN, D_in, D_out = 64, 1024, 16\n\n# Each process receives its own batch of ""fake input data"" and ""fake target data.""\n# The ""training loop"" in each process just uses this fake batch over and over.\n# https://github.com/NVIDIA/apex/tree/master/examples/imagenet provides a more realistic\n# example of distributed data sampling for both training and validation.\nx = torch.randn(N, D_in, device=\'cuda\')\ny = torch.randn(N, D_out, device=\'cuda\')\n\nmodel = torch.nn.Linear(D_in, D_out).cuda()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n\nmodel, optimizer = amp.initialize(model, optimizer, opt_level=""O2"")\n\nif args.distributed:\n    # FOR DISTRIBUTED:  After amp.initialize, wrap the model with\n    # apex.parallel.DistributedDataParallel.\n    model = DistributedDataParallel(model)\n    # torch.nn.parallel.DistributedDataParallel is also fine, with some added args:\n    # model = torch.nn.parallel.DistributedDataParallel(model,\n    #                                                   device_ids=[args.local_rank],\n    #                                                   output_device=args.local_rank)\n\nloss_fn = torch.nn.MSELoss()\n\nfor t in range(500):\n    optimizer.zero_grad()\n    y_pred = model(x)\n    loss = loss_fn(y_pred, y)\n    with amp.scale_loss(loss, optimizer) as scaled_loss:\n        scaled_loss.backward()\n    optimizer.step()\n\nif args.local_rank == 0:\n    print(""final loss = "", loss)\n\ntorch.save(list(model.parameters()), ""rank{}model.pth"".format(torch.distributed.get_rank()))\ntorch.save(list(amp.master_params(optimizer)), ""rank{}master.pth"".format(torch.distributed.get_rank()))\n'"
apex/tests/distributed/amp_master_params/compare.py,8,"b'import torch\n\nmodel_params_rank0 = torch.load(""rank0model.pth"",\n                           map_location = lambda storage, loc: storage.cuda(0))\nmodel_params_rank1 = torch.load(""rank1model.pth"",\n                                 map_location = lambda storage, loc: storage.cuda(0))\nmaster_params_rank0 = torch.load(""rank0master.pth"",\n                                 map_location = lambda storage, loc: storage.cuda(0))\nmaster_params_rank1 = torch.load(""rank1master.pth"",\n                                 map_location = lambda storage, loc: storage.cuda(0))\n\nfor model_rank0, model_rank1, master_rank0, master_rank1 in zip(\n        model_params_rank0,\n        model_params_rank1,\n        master_params_rank0,\n        master_params_rank1):\n    assert torch.allclose(model_rank0, model_rank1), ""Model param mismatch""\n    assert torch.allclose(master_rank0, master_rank1), ""Master param mismatch""\n    # Some debugging/investigation assistance code:\n    # maxval, maxind = torch.max(((torch.abs(model_rank0).float())/torch.abs(master_rank0)).view(-1), 0)\n    # offending_val_half = model_rank0.view(-1)[maxind.item()]\n    # offending_val_float = master_rank0.view(-1)[maxind.item()]\n    # print(maxval.item(), maxind.item(), offending_val_half.item(), offending_val_float.item(),\n    #       offending_val_float.half().item())\n    # rtol needs to be > 2^-11 because of denormals...\n    assert torch.allclose(model_rank0, master_rank0.half(), rtol=.005), ""Model-master mismatch""\n\nprint(""OK:  Model and master params match across ranks."")\n'"
apex/tests/distributed/synced_batchnorm/single_gpu_unit_test.py,8,"b'import torch\nimport numpy as np\nimport apex\nif True:\n    print(""using setup tools"")\n    import syncbn\nelse:\n    print(""using jit"")\n    from torch.utils.cpp_extension import load\n    syncbn = load(name=\'syncbn\', sources=[\'../../csrc/syncbn.cpp\', \'../../csrc/welford.cu\'])\n\ndef compare(desc, inp1, inp2, error):\n    a = inp1.clone().detach().cpu().numpy()\n    b = inp2.clone().detach().cpu().numpy()\n    close = np.allclose(a,b, error, error)\n    if not close:\n        print(desc, close)\n        z = a - b\n        index = (np.abs(z) >= error + error * np.abs(b)).nonzero()\n        print(""dif    : "", z[index])\n        print(""inp1   : "", a[index])\n        print(""inp2   : "", b[index])\n    return close\n\nfeature_size = 10\nspace_size = 16\nbatch_size = 5\n\n\nerror = 1e-5\n\nnp.random.seed(1)\ndtype = np.float32\ninp = (np.random.randn(batch_size, feature_size, space_size, space_size)).astype(dtype)\ngrad = (np.random.randn(batch_size, feature_size, space_size, space_size)).astype(dtype)\nweight = (np.random.randn(feature_size)).astype(dtype)\nbias = (np.random.randn(feature_size)).astype(dtype)\n\ntype_tensor = torch.cuda.FloatTensor\nref_tensor = torch.cuda.DoubleTensor\n\ninp_t = type_tensor(inp)\nweight_t = type_tensor(weight)\nbias_t = type_tensor(bias)\n\ninp_r = ref_tensor(inp.transpose(1, 0, 2, 3).reshape(feature_size, -1))\ninp2_r = ref_tensor(inp)\nweight_r = ref_tensor(weight).view(-1, 1, 1)\nbias_r = ref_tensor(bias).view(-1, 1, 1)\n\ngrad_output_t = type_tensor(grad)\n\nm = inp_r.mean(1)\nb_v = inp_r.var(1, unbiased=False)\nunb_v = inp_r.var(1, unbiased=True)\n\neps = 1e-5\n\n#mean, var, var_biased = syncbn.welford_mean_var(inp_t)\nmean, var_biased = syncbn.welford_mean_var(inp_t)\ninv_std = 1.0 / torch.sqrt(var_biased + eps)\n\nbn = torch.nn.BatchNorm2d(feature_size).cuda()\nbn.momentum = 1.0\nbn.weight.data = weight_t.clone()\nbn.bias.data = bias_t.clone()\ninp_bn = inp_t.clone().requires_grad_()\ngrad_bn = grad_output_t.clone().detach()\nout_bn = bn(inp_bn)\nout_bn.backward(grad_bn)\n\nsbn = apex.parallel.SyncBatchNorm(feature_size).cuda()\nsbn.momentum = 1.0\nsbn.weight.data = weight_t.clone()\nsbn.bias.data = bias_t.clone()\ninp_sbn = inp_t.clone().requires_grad_()\ngrad_sbn = grad_output_t.clone().detach()\nout_sbn = sbn(inp_sbn)\nout_sbn.backward(grad_sbn)\n\nsbn_c_last = apex.parallel.SyncBatchNorm(feature_size, channel_last=True).cuda()\nsbn_c_last.momentum = 1.0\nsbn_c_last.weight.data = weight_t.clone()\nsbn_c_last.bias.data = bias_t.clone()\ninp_sbn_c_last = inp_t.clone().transpose(-1, 1).contiguous().requires_grad_()\ngrad_sbn_c_last = grad_output_t.clone().transpose(-1, 1).contiguous().detach()\nout_sbn_c_last = sbn_c_last(inp_sbn_c_last)\nout_sbn_c_last.backward(grad_sbn_c_last)\n\nsbn_result = True\nsbn_result_c_last = True\nbn_result = True\n\nsbn_result = compare(""comparing mean: "", mean, m, error) and sbn_result\n#sbn_result = compare(""comparing variance: "", var, unb_v, error) and sbn_result\nsbn_result = compare(""comparing biased variance: "", var_biased, b_v, error) and sbn_result\n\n\nout = syncbn.batchnorm_forward(inp_t, mean, inv_std, weight_t, bias_t)\nout_r = weight_r * (inp2_r - m.view(-1, 1, 1)) * torch.rsqrt(b_v.view(-1,1,1) + eps) + bias_r\n\nsbn_result = compare(""comparing output: "", out, out_r, error) and sbn_result\ncompare(""comparing bn output: "", out_bn, out_r, error)\n\ngrad_output_t = type_tensor(grad)\n\ngrad_output_r = ref_tensor(grad.transpose(1, 0, 2, 3).reshape(feature_size, -1))\ngrad_output2_r = ref_tensor(grad)\n\ngrad_bias_r = grad_output_r.sum(1)\ngrad_weight_r = ((inp2_r - m.view(-1, 1, 1)) * torch.rsqrt(b_v.view(-1,1,1) + eps) * grad_output2_r).transpose(1,0).contiguous().view(feature_size, -1).sum(1)\n\nmean_dy_r = grad_output_r.mean(1)\nmean_dy_xmu_r = ((inp2_r - m.view(-1, 1, 1)) * grad_output2_r).transpose(1,0).contiguous().view(feature_size, -1).mean(1)\n\ngrad_input_r = (grad_output2_r - mean_dy_r.view(-1, 1, 1) - (inp2_r - m.view(-1, 1, 1)) / (b_v.view(-1,1,1) + eps) * mean_dy_xmu_r.view(-1, 1, 1) ) * torch.rsqrt(b_v.view(-1,1,1) + eps) * weight_r.view(-1,1,1)\n\nmean_dy, mean_dy_xmu, grad_weight, grad_bias = syncbn.reduce_bn(grad_output_t, inp_t, mean, inv_std, weight_t)\ngrad_input = syncbn.batchnorm_backward(grad_output_t, inp_t, mean, inv_std, weight_t, mean_dy, mean_dy_xmu)\nsbn_result = compare(""comparing bias grad: "", grad_bias, grad_bias_r, error) and sbn_result\nsbn_result = compare(""comparing weight grad: "", grad_weight, grad_weight_r, error) and sbn_result\nsbn_result = compare(""comparing mean_dy grad: "", mean_dy, mean_dy_r, error) and sbn_result\nsbn_result = compare(""comparing mean_dy_xmu grad: "", mean_dy_xmu, mean_dy_xmu_r, error) and sbn_result\nsbn_result = compare(""comparing input grad: "", grad_input, grad_input_r, error) and sbn_result\ncompare(""comparing bn input grad: "", inp_bn.grad, grad_input_r, error)\nsbn_result = compare(""comparing sbn input grad: "", inp_sbn.grad, grad_input_r, error) and sbn_result\n\ncompare(""comparing bn/sbn output: "", out_bn, out_sbn, error)\nsbn_result = compare(""comparing running_mean: "", bn.running_mean.data, sbn.running_mean.data, error) and sbn_result\nsbn_result = compare(""comparing running_variance: "", bn.running_var.data, sbn.running_var.data, error) and sbn_result\ncompare(""comparing grad_input: "", inp_bn.grad, inp_sbn.grad, error)\ncompare(""comparing grad_bias: "", bn.bias.grad, sbn.bias.grad, error)\ncompare(""comparing grad_bias bn to ref: "", bn.bias.grad, grad_bias_r, error)\nsbn_result = compare(""comparing grad_bias sbn to ref: "", sbn.bias.grad, grad_bias_r, error) and sbn_result\ncompare(""comparing grad_weight: "", bn.weight.grad, sbn.weight.grad, error)\ncompare(""comparing grad_weight bn to ref: "", bn.weight.grad, grad_weight_r, error)\nsbn_result = compare(""comparing grad_weight sbn to ref: "", sbn.weight.grad, grad_weight_r, error) and sbn_result\n\ncompare(""comparing channel last bn/sbn output: "", out_bn, out_sbn_c_last.transpose(-1, 1).contiguous(), error)\nsbn_result_c_last = compare(""comparing channel last running_mean: "", bn.running_mean.data, sbn_c_last.running_mean.data, error) and sbn_result_c_last\nsbn_result_c_last = compare(""comparing channel last running_variance: "", bn.running_var.data, sbn_c_last.running_var.data, error) and sbn_result_c_last\ncompare(""comparing channel last grad_input: "", inp_bn.grad, inp_sbn_c_last.grad.transpose(-1, 1).contiguous(), error)\ncompare(""comparing channel last grad_bias: "", bn.bias.grad, sbn_c_last.bias.grad, error)\nsbn_result_c_last = compare(""comparing channel last grad_bias sbn to ref: "", sbn_c_last.bias.grad, grad_bias_r, error) and sbn_result_c_last\ncompare(""comparing channel last grad_weight: "", bn.weight.grad, sbn_c_last.weight.grad, error)\nsbn_result_c_last = compare(""comparing channel last grad_weight sbn to ref: "", sbn_c_last.weight.grad, grad_weight_r, error) and sbn_result_c_last\n\nif sbn_result:\n    print(""====SBN single gpu passed tests"")\nelse:\n    print(""*SBN single gpu failed*"")\n\nif sbn_result_c_last:\n    print(""====SBN channel last single gpu passed tests"")\nelse:\n    print(""*SBN channel last single gpu failed*"")\n'"
apex/tests/distributed/synced_batchnorm/test_groups.py,13,"b'import torch\nimport numpy as np\nimport apex\nimport syncbn\nimport os\nimport argparse\nimport torch.optim as optim\n\ndef compare(desc, inp1, inp2, error):\n    a = inp1.clone().detach().cpu().numpy()\n    b = inp2.clone().detach().cpu().numpy()\n    close = np.allclose(a,b, error, error)\n    if not close:\n        print(desc, close)\n        z = a - b\n        index = (np.abs(z) >= error + error * np.abs(b)).nonzero()\n        print(""dif    : "", z[index])\n        print(""inp1   : "", a[index])\n        print(""inp2   : "", b[index])\n    return close\n\nfeature_size = 10\nspace_size = 40\nbatch_size = 32\n\n\nfrom apex.parallel import DistributedDataParallel as DDP\nparser = argparse.ArgumentParser()\nparser.add_argument(""--local_rank"", default=0, type=int)\nparser.add_argument(""--fp16"", action=\'store_true\', default=False)\nparser.add_argument(""--fp64"", action=\'store_true\', default=False)\nparser.add_argument(""--group_size"", default=0, type=int)\nargs = parser.parse_args()\n\ntry:\n    args.world_size = int(os.environ[\'WORLD_SIZE\'])\nexcept:\n    print(""This is a multi-gpu test. To run it please use \'python -m torch.distributed.launch --nproc_per_node=<num gpus> test_groups.py <more options>\'"")\n    exit(1)\n\ntorch.cuda.set_device(args.local_rank)\ntorch.distributed.init_process_group(backend=\'nccl\', init_method=\'env://\')\n\nstart = (args.local_rank%args.group_size) * batch_size//args.group_size\nfinish = (args.local_rank%args.group_size + 1) * batch_size//args.group_size\n\nerror = 1e-5\ndtype = np.float32\nif args.fp16:\n    error = 1e-3\n    dtype = np.float16\nelif args.fp64:\n    error = 1e-8\n    dtype = np.float64\n\n\nnp.random.seed(18 + args.local_rank//args.group_size)\n\ninp = np.random.randn(batch_size, feature_size, space_size, space_size).astype(dtype)\ngrad = np.random.randn(batch_size, feature_size, space_size, space_size).astype(dtype)\nweight = np.random.randn(feature_size).astype(dtype)\nbias = np.random.randn(feature_size).astype(dtype)\n\n\ntype_tensor = torch.cuda.FloatTensor\nif args.fp16:\n    type_tensor = torch.cuda.HalfTensor\nif args.fp64:\n    type_tensor = torch.cuda.DoubleTensor\n\nref_tensor = torch.cuda.DoubleTensor\n\ninp_t = type_tensor(inp)\nweight_t = type_tensor(weight)\nbias_t = type_tensor(bias)\n\ninp_r = ref_tensor(inp.transpose(1, 0, 2, 3).reshape(feature_size, -1))\ninp2_r = ref_tensor(inp)\nweight_r = ref_tensor(weight).view(-1, 1, 1)\nbias_r = ref_tensor(bias).view(-1, 1, 1)\n\ngrad_output_t = type_tensor(grad)\n\nm = inp_r.mean(1)\nb_v = inp_r.var(1, unbiased=False)\nunb_v = inp_r.var(1, unbiased=True)\n\neps = 1e-5\n\nmean, var_biased = syncbn.welford_mean_var(inp_t)\ninv_std = 1.0 / torch.sqrt(var_biased + eps)\n\nbn = torch.nn.BatchNorm2d(feature_size).cuda()\nbn.momentum = 1.0\nbn.weight.data = weight_t.clone()\nbn.bias.data = bias_t.clone()\nif args.fp16:\n    bn.half()\nif args.fp64:\n    bn.double()\nbn = DDP(bn)\ninp_bn = inp_t.clone().requires_grad_()\ngrad_bn = grad_output_t.clone().detach()\nout_bn = bn(inp_bn)\nout_bn.backward(grad_bn)\n# compensating the averaging over processes done by DDP\n# in order to produce mathematically equivalent result\n# https://github.com/NVIDIA/apex/issues/134#issuecomment-458307368\nfor param in bn.parameters():\n    param.grad = param.grad / args.group_size\nbn_opt = optim.SGD(bn.parameters(), lr=1.0)\n\nsbn = apex.parallel.SyncBatchNorm(feature_size, process_group=apex.parallel.create_syncbn_process_group(args.group_size)).cuda()\nsbn.momentum = 1.0\nsbn.weight.data = weight_t.clone()\nsbn.bias.data = bias_t.clone()\nif args.fp16:\n    sbn.half()\nif args.fp64:\n    sbn.double()\nsbn = DDP(sbn)\nsbn_opt = optim.SGD(sbn.parameters(), lr=1.0)\ninp_sbn = inp_t.clone().requires_grad_()\ngrad_sbn = grad_output_t.clone().detach()\nout_sbn = sbn(inp_sbn[start:finish])\nout_sbn.backward(grad_sbn[start:finish])\n\nsbn_result = True\nbn_result = True\n\nif args.local_rank == 0:\n    sbn_result = compare(""comparing mean: "", mean, m, error) and sbn_result\n    sbn_result = compare(""comparing biased variance: "", var_biased, b_v, error) and sbn_result\n\nout = syncbn.batchnorm_forward(inp_t, mean, inv_std, weight_t, bias_t)\nout_r = weight_r * (inp2_r - m.view(-1, 1, 1)) * torch.rsqrt(b_v.view(-1,1,1) + eps) + bias_r\n\nif args.local_rank == 0:\n    sbn_result = compare(""comparing output: "", out, out_r, error) and sbn_result\n    compare(""comparing bn output: "", out_bn, out_r, error)\n\ngrad_output_t = type_tensor(grad)\n\ngrad_output_r = ref_tensor(grad.transpose(1, 0, 2, 3).reshape(feature_size, -1))\ngrad_output2_r = ref_tensor(grad)\n\ngrad_bias_r = grad_output_r.sum(1)\ngrad_weight_r = ((inp2_r - m.view(-1, 1, 1)) * torch.rsqrt(b_v.view(-1,1,1) + eps) * grad_output2_r).transpose(1,0).contiguous().view(feature_size, -1).sum(1)\n\nmean_dy_r = grad_output_r.mean(1)\nmean_dy_xmu_r = ((inp2_r - m.view(-1, 1, 1)) * grad_output2_r).transpose(1,0).contiguous().view(feature_size, -1).mean(1)\n\ngrad_input_r = (grad_output2_r - mean_dy_r.view(-1, 1, 1) - (inp2_r - m.view(-1, 1, 1)) / (b_v.view(-1,1,1) + eps) * mean_dy_xmu_r.view(-1, 1, 1) ) * torch.rsqrt(b_v.view(-1,1,1) + eps) * weight_r.view(-1,1,1)\n\nmean_dy, mean_dy_xmu, grad_weight, grad_bias = syncbn.reduce_bn(grad_output_t, inp_t, mean, inv_std, weight_t)\ngrad_input = syncbn.batchnorm_backward(grad_output_t, inp_t, mean, inv_std, weight_t, mean_dy, mean_dy_xmu)\n\nif args.local_rank == 0:\n    sbn_result = compare(""comparing bias grad: "", grad_bias, grad_bias_r, error) and sbn_result\n    sbn_result = compare(""comparing weight grad: "", grad_weight, grad_weight_r, error) and sbn_result\n    sbn_result = compare(""comparing mean_dy grad: "", mean_dy, mean_dy_r, error) and sbn_result\n    sbn_result = compare(""comparing mean_dy_xmu grad: "", mean_dy_xmu, mean_dy_xmu_r, error) and sbn_result\n    sbn_result = compare(""comparing input grad: "", grad_input, grad_input_r, error) and sbn_result\n    compare(""comparing bn input grad: "", inp_bn.grad, grad_input_r, error)\n\nif args.local_rank == 0:\n    sbn_result = compare(""comparing running_mean: "", bn.module.running_mean.data, sbn.module.running_mean.data, error) and sbn_result\n    sbn_result = compare(""comparing running_variance: "", bn.module.running_var.data, sbn.module.running_var.data, error) and sbn_result\n\n# execute by both\ncompare(""comparing layers output: "", out_bn[start:finish], out_sbn, error) and sbn_result\ncompare(""comparing layers grad_input: "", inp_bn.grad[start:finish], inp_sbn.grad[start:finish], error) and sbn_result\n\nbn_opt.step()\nsbn_opt.step()\n\nif args.local_rank == 0:\n    compare(""comparing bn vs sbn bias: "", bn.module.bias, sbn.module.bias, error)\n    compare(""comparing bn vs sbn weight: "", bn.module.weight, sbn.module.weight, error)\n\n\nif sbn_result:\n    print(""====SBN group test passed"")\nelse:\n    print(""*SBN group test failed*"")\n'"
apex/tests/distributed/synced_batchnorm/two_gpu_unit_test.py,12,"b'import torch\nimport numpy as np\nimport apex\nimport syncbn\nimport os\nimport argparse\nimport torch.optim as optim\n\ndef compare(desc, inp1, inp2, error):\n    a = inp1.clone().detach().cpu().numpy()\n    b = inp2.clone().detach().cpu().numpy()\n    close = np.allclose(a,b, error, error)\n    if not close:\n        print(desc, close)\n        z = a - b\n        index = (np.abs(z) >= error + error * np.abs(b)).nonzero()\n        print(""dif    : "", z[index])\n        print(""inp1   : "", a[index])\n        print(""inp2   : "", b[index])\n    return close\n\nfeature_size = 10\nspace_size = 40\nbatch_size = 32\n\n\nfrom apex.parallel import DistributedDataParallel as DDP\nparser = argparse.ArgumentParser()\nparser.add_argument(""--local_rank"", default=0, type=int)\nparser.add_argument(""--fp16"", action=\'store_true\', default=False)\nparser.add_argument(""--fp64"", action=\'store_true\', default=False)\nargs = parser.parse_args()\nargs.world_size = int(os.environ[\'WORLD_SIZE\'])\ntorch.cuda.set_device(args.local_rank)\ntorch.distributed.init_process_group(backend=\'nccl\', init_method=\'env://\')\nstart = args.local_rank * batch_size//args.world_size\nfinish = (args.local_rank + 1) * batch_size//args.world_size\n\nerror = 1e-5\ndtype = np.float32\nif args.fp16:\n    error = 1e-3\n    dtype = np.float16\nelif args.fp64:\n    error = 1e-8\n    dtype = np.float64\n\nnp.random.seed(18)\ninp = np.random.randn(batch_size, feature_size, space_size, space_size).astype(dtype)\ngrad = np.random.randn(batch_size, feature_size, space_size, space_size).astype(dtype)\nweight = np.random.randn(feature_size).astype(dtype)\nbias = np.random.randn(feature_size).astype(dtype)\n\n\ntype_tensor = torch.cuda.FloatTensor\nif args.fp16:\n    type_tensor = torch.cuda.HalfTensor\nif args.fp64:\n    type_tensor = torch.cuda.DoubleTensor\n\nref_tensor = torch.cuda.DoubleTensor\n\ninp_t = type_tensor(inp)\nweight_t = type_tensor(weight)\nbias_t = type_tensor(bias)\n\ninp_r = ref_tensor(inp.transpose(1, 0, 2, 3).reshape(feature_size, -1))\ninp2_r = ref_tensor(inp)\nweight_r = ref_tensor(weight).view(-1, 1, 1)\nbias_r = ref_tensor(bias).view(-1, 1, 1)\n\ngrad_output_t = type_tensor(grad)\n\nm = inp_r.mean(1)\nb_v = inp_r.var(1, unbiased=False)\nunb_v = inp_r.var(1, unbiased=True)\n\neps = 1e-5\n\nmean, var_biased = syncbn.welford_mean_var(inp_t)\ninv_std = 1.0 / torch.sqrt(var_biased + eps)\n\nbn = torch.nn.BatchNorm2d(feature_size).cuda()\nbn.momentum = 1.0\nbn.weight.data = weight_t.clone()\nbn.bias.data = bias_t.clone()\nif args.fp16:\n    bn.half()\nif args.fp64:\n    bn.double()\ninp_bn = inp_t.clone().requires_grad_()\ngrad_bn = grad_output_t.clone().detach()\nout_bn = bn(inp_bn)\nout_bn.backward(grad_bn)\n# compensating the averaging over processes done by DDP\n# in order to produce mathematically equivalent result\n# https://github.com/NVIDIA/apex/issues/134#issuecomment-458307368\nfor param in bn.parameters():\n    param.grad = param.grad / args.world_size\nbn_opt = optim.SGD(bn.parameters(), lr=1.0)\n\nsbn = apex.parallel.SyncBatchNorm(feature_size).cuda()\nsbn.momentum = 1.0\nsbn.weight.data = weight_t.clone()\nsbn.bias.data = bias_t.clone()\nif args.fp16:\n    sbn.half()\nif args.fp64:\n    sbn.double()\nsbn = DDP(sbn)\nsbn_opt = optim.SGD(sbn.parameters(), lr=1.0)\ninp_sbn = inp_t.clone().requires_grad_()\ngrad_sbn = grad_output_t.clone().detach()\nout_sbn = sbn(inp_sbn[start:finish])\nout_sbn.backward(grad_sbn[start:finish])\n\nsbn_result = True\nbn_result = True\n\nif args.local_rank == 0:\n    sbn_result = compare(""comparing mean: "", mean, m, error) and sbn_result\n    sbn_result = compare(""comparing biased variance: "", var_biased, b_v, error) and sbn_result\n\nout = syncbn.batchnorm_forward(inp_t, mean, inv_std, weight_t, bias_t)\nout_r = weight_r * (inp2_r - m.view(-1, 1, 1)) * torch.rsqrt(b_v.view(-1,1,1) + eps) + bias_r\n\nif args.local_rank == 0:\n    sbn_result = compare(""comparing output: "", out, out_r, error) and sbn_result\n    compare(""comparing bn output: "", out_bn, out_r, error)\n\ngrad_output_t = type_tensor(grad)\n\ngrad_output_r = ref_tensor(grad.transpose(1, 0, 2, 3).reshape(feature_size, -1))\ngrad_output2_r = ref_tensor(grad)\n\ngrad_bias_r = grad_output_r.sum(1)\ngrad_weight_r = ((inp2_r - m.view(-1, 1, 1)) * torch.rsqrt(b_v.view(-1,1,1) + eps) * grad_output2_r).transpose(1,0).contiguous().view(feature_size, -1).sum(1)\n\nmean_dy_r = grad_output_r.mean(1)\nmean_dy_xmu_r = ((inp2_r - m.view(-1, 1, 1)) * grad_output2_r).transpose(1,0).contiguous().view(feature_size, -1).mean(1)\n\ngrad_input_r = (grad_output2_r - mean_dy_r.view(-1, 1, 1) - (inp2_r - m.view(-1, 1, 1)) / (b_v.view(-1,1,1) + eps) * mean_dy_xmu_r.view(-1, 1, 1) ) * torch.rsqrt(b_v.view(-1,1,1) + eps) * weight_r.view(-1,1,1)\n\nmean_dy, mean_dy_xmu, grad_weight, grad_bias = syncbn.reduce_bn(grad_output_t, inp_t, mean, inv_std, weight_t)\ngrad_input = syncbn.batchnorm_backward(grad_output_t, inp_t, mean, inv_std, weight_t, mean_dy, mean_dy_xmu)\nif args.local_rank == 0:\n    sbn_result = compare(""comparing bias grad: "", grad_bias, grad_bias_r, error) and sbn_result\n    sbn_result = compare(""comparing weight grad: "", grad_weight, grad_weight_r, error) and sbn_result\n    sbn_result = compare(""comparing mean_dy grad: "", mean_dy, mean_dy_r, error) and sbn_result\n    sbn_result = compare(""comparing mean_dy_xmu grad: "", mean_dy_xmu, mean_dy_xmu_r, error) and sbn_result\n    sbn_result = compare(""comparing input grad: "", grad_input, grad_input_r, error) and sbn_result\n    compare(""comparing bn input grad: "", inp_bn.grad, grad_input_r, error)\n\nif args.local_rank == 0:\n    sbn_result = compare(""comparing running_mean: "", bn.running_mean.data, sbn.module.running_mean.data, error) and sbn_result\n    sbn_result = compare(""comparing running_variance: "", bn.running_var.data, sbn.module.running_var.data, error) and sbn_result\n\n# execute by both\ncompare(""comparing layers output: "", out_bn[start:finish], out_sbn, error) and sbn_result\ncompare(""comparing layers grad_input: "", inp_bn.grad[start:finish], inp_sbn.grad[start:finish], error) and sbn_result\n\nbn_opt.step()\nsbn_opt.step()\n\nif args.local_rank == 0:\n    compare(""comparing bn vs sbn bias: "", bn.bias, sbn.module.bias, error)\n    compare(""comparing bn vs sbn weight: "", bn.weight, sbn.module.weight, error)\n\n\nif sbn_result:\n    print(""====SBN two gpu passed tests"")\nelse:\n    print(""*SBN two gpu failed*"")\n'"
tensorboardX/examples/chainer/extension_logger/net.py,0,"b'#!/usr/bin/env python\n\nfrom __future__ import print_function\n\nimport numpy\n\nimport chainer\nfrom chainer import cuda\nimport chainer.functions as F\nimport chainer.links as L\n\n\ndef add_noise(h, sigma=0.2):\n    xp = cuda.get_array_module(h.data)\n    if chainer.config.train:\n        return h + sigma * xp.random.randn(*h.shape)\n    else:\n        return h\n\n\nclass Generator(chainer.Chain):\n\n    def __init__(self, n_hidden, bottom_width=4, ch=512, wscale=0.02):\n        super(Generator, self).__init__()\n        self.n_hidden = n_hidden\n        self.ch = ch\n        self.bottom_width = bottom_width\n\n        with self.init_scope():\n            w = chainer.initializers.Normal(wscale)\n            self.l0 = L.Linear(self.n_hidden, bottom_width * bottom_width * ch,\n                               initialW=w)\n            self.dc1 = L.Deconvolution2D(ch, ch // 2, 4, 2, 1, initialW=w)\n            self.dc2 = L.Deconvolution2D(ch // 2, ch // 4, 4, 2, 1, initialW=w)\n            self.dc3 = L.Deconvolution2D(ch // 4, ch // 8, 4, 2, 1, initialW=w)\n            self.dc4 = L.Deconvolution2D(ch // 8, 3, 3, 1, 1, initialW=w)\n            self.bn0 = L.BatchNormalization(bottom_width * bottom_width * ch)\n            self.bn1 = L.BatchNormalization(ch // 2)\n            self.bn2 = L.BatchNormalization(ch // 4)\n            self.bn3 = L.BatchNormalization(ch // 8)\n\n    def make_hidden(self, batchsize):\n        return numpy.random.uniform(-1, 1, (batchsize, self.n_hidden, 1, 1))\\\n            .astype(numpy.float32)\n\n    def __call__(self, z):\n        h = F.reshape(F.relu(self.bn0(self.l0(z))),\n                      (len(z), self.ch, self.bottom_width, self.bottom_width))\n        h = F.relu(self.bn1(self.dc1(h)))\n        h = F.relu(self.bn2(self.dc2(h)))\n        h = F.relu(self.bn3(self.dc3(h)))\n        x = F.sigmoid(self.dc4(h))\n        return x\n\n\nclass Discriminator(chainer.Chain):\n\n    def __init__(self, bottom_width=4, ch=512, wscale=0.02):\n        w = chainer.initializers.Normal(wscale)\n        super(Discriminator, self).__init__()\n        with self.init_scope():\n            self.c0_0 = L.Convolution2D(3, ch // 8, 3, 1, 1, initialW=w)\n            self.c0_1 = L.Convolution2D(ch // 8, ch // 4, 4, 2, 1, initialW=w)\n            self.c1_0 = L.Convolution2D(ch // 4, ch // 4, 3, 1, 1, initialW=w)\n            self.c1_1 = L.Convolution2D(ch // 4, ch // 2, 4, 2, 1, initialW=w)\n            self.c2_0 = L.Convolution2D(ch // 2, ch // 2, 3, 1, 1, initialW=w)\n            self.c2_1 = L.Convolution2D(ch // 2, ch // 1, 4, 2, 1, initialW=w)\n            self.c3_0 = L.Convolution2D(ch // 1, ch // 1, 3, 1, 1, initialW=w)\n            self.l4 = L.Linear(bottom_width * bottom_width * ch, 1, initialW=w)\n            self.bn0_1 = L.BatchNormalization(ch // 4, use_gamma=False)\n            self.bn1_0 = L.BatchNormalization(ch // 4, use_gamma=False)\n            self.bn1_1 = L.BatchNormalization(ch // 2, use_gamma=False)\n            self.bn2_0 = L.BatchNormalization(ch // 2, use_gamma=False)\n            self.bn2_1 = L.BatchNormalization(ch // 1, use_gamma=False)\n            self.bn3_0 = L.BatchNormalization(ch // 1, use_gamma=False)\n\n    def __call__(self, x):\n        h = add_noise(x)\n        h = F.leaky_relu(add_noise(self.c0_0(h)))\n        h = F.leaky_relu(add_noise(self.bn0_1(self.c0_1(h))))\n        h = F.leaky_relu(add_noise(self.bn1_0(self.c1_0(h))))\n        h = F.leaky_relu(add_noise(self.bn1_1(self.c1_1(h))))\n        h = F.leaky_relu(add_noise(self.bn2_0(self.c2_0(h))))\n        h = F.leaky_relu(add_noise(self.bn2_1(self.c2_1(h))))\n        h = F.leaky_relu(add_noise(self.bn3_0(self.c3_0(h))))\n        return self.l4(h)\n'"
tensorboardX/examples/chainer/extension_logger/train_dcgan.py,0,"b""#!/usr/bin/env python\n\nfrom __future__ import print_function\nimport argparse\nimport os\n\nimport chainer\nfrom chainer import training\nfrom chainer.training import extensions\n\nfrom net import Discriminator\nfrom net import Generator\nfrom updater import DCGANUpdater\nfrom visualize import out_generated_image\nfrom tensorboardX import SummaryWriter\nfrom writetensorboard import LogTensorboard\n\n\ndef main():\n    parser = argparse.ArgumentParser(description='Chainer example: DCGAN')\n    parser.add_argument('--batchsize', '-b', type=int, default=50,\n                        help='Number of images in each mini-batch')\n    parser.add_argument('--epoch', '-e', type=int, default=1000,\n                        help='Number of sweeps over the dataset to train')\n    parser.add_argument('--gpu', '-g', type=int, default=-1,\n                        help='GPU ID (negative value indicates CPU)')\n    parser.add_argument('--dataset', '-i', default='',\n                        help='Directory of image files.  Default is cifar-10.')\n    parser.add_argument('--out', '-o', default='result',\n                        help='Directory to output the result')\n    parser.add_argument('--resume', '-r', default='',\n                        help='Resume the training from snapshot')\n    parser.add_argument('--n_hidden', '-n', type=int, default=100,\n                        help='Number of hidden units (z)')\n    parser.add_argument('--seed', type=int, default=0,\n                        help='Random seed of z at visualization stage')\n    parser.add_argument('--snapshot_interval', type=int, default=1000,\n                        help='Interval of snapshot')\n    parser.add_argument('--display_interval', type=int, default=100,\n                        help='Interval of displaying log to console')\n    args = parser.parse_args()\n\n    print('GPU: {}'.format(args.gpu))\n    print('# Minibatch-size: {}'.format(args.batchsize))\n    print('# n_hidden: {}'.format(args.n_hidden))\n    print('# epoch: {}'.format(args.epoch))\n    print('')\n    writer = SummaryWriter()\n    # Set up a neural network to train\n    gen = Generator(n_hidden=args.n_hidden)\n    dis = Discriminator()\n\n    if args.gpu >= 0:\n        # Make a specified GPU current\n        chainer.cuda.get_device_from_id(args.gpu).use()\n        gen.to_gpu()  # Copy the model to the GPU\n        dis.to_gpu()\n\n    # Setup an optimizer\n    def make_optimizer(model, alpha=0.0002, beta1=0.5):\n        optimizer = chainer.optimizers.Adam(alpha=alpha, beta1=beta1)\n        optimizer.setup(model)\n        optimizer.add_hook(chainer.optimizer.WeightDecay(0.0001), 'hook_dec')\n        return optimizer\n    opt_gen = make_optimizer(gen)\n    opt_dis = make_optimizer(dis)\n\n    if args.dataset == '':\n        # Load the CIFAR10 dataset if args.dataset is not specified\n        train, _ = chainer.datasets.get_cifar10(withlabel=False, scale=255.)\n    else:\n        all_files = os.listdir(args.dataset)\n        image_files = [f for f in all_files if ('png' in f or 'jpg' in f)]\n        print('{} contains {} image files'\n              .format(args.dataset, len(image_files)))\n        train = chainer.datasets\\\n            .ImageDataset(paths=image_files, root=args.dataset)\n\n    train_iter = chainer.iterators.SerialIterator(train, args.batchsize)\n\n    # Set up a trainer\n    updater = DCGANUpdater(\n        models=(gen, dis),\n        iterator=train_iter,\n        optimizer={\n            'gen': opt_gen, 'dis': opt_dis},\n        device=args.gpu)\n    trainer = training.Trainer(updater, (args.epoch, 'epoch'), out=args.out)\n\n    snapshot_interval = (args.snapshot_interval, 'iteration')\n    display_interval = (args.display_interval, 'iteration')\n    trainer.extend(\n        extensions.snapshot(filename='snapshot_iter_{.updater.iteration}.npz'),\n        trigger=snapshot_interval)\n    trainer.extend(extensions.snapshot_object(\n        gen, 'gen_iter_{.updater.iteration}.npz'), trigger=snapshot_interval)\n    trainer.extend(extensions.snapshot_object(\n        dis, 'dis_iter_{.updater.iteration}.npz'), trigger=snapshot_interval)\n    trainer.extend(extensions.LogReport(trigger=display_interval))\n    trainer.extend(LogTensorboard(trigger=display_interval, logger=writer))\n    trainer.extend(extensions.PrintReport([\n        'epoch', 'iteration', 'gen/loss', 'dis/loss',\n    ]), trigger=display_interval)\n    trainer.extend(extensions.ProgressBar(update_interval=10))\n    trainer.extend(\n        out_generated_image(\n            gen, dis,\n            10, 10, args.seed, args.out, writer),\n        trigger=snapshot_interval)\n\n    if args.resume:\n        # Resume from a snapshot\n        chainer.serializers.load_npz(args.resume, trainer)\n\n    # Run the training\n    trainer.run()\n\n\nif __name__ == '__main__':\n    main()\n"""
tensorboardX/examples/chainer/extension_logger/updater.py,0,"b""#!/usr/bin/env python\n\nfrom __future__ import print_function\n\nimport chainer\nimport chainer.functions as F\nfrom chainer import Variable\n\n\nclass DCGANUpdater(chainer.training.StandardUpdater):\n\n    def __init__(self, *args, **kwargs):\n        self.gen, self.dis = kwargs.pop('models')\n        super(DCGANUpdater, self).__init__(*args, **kwargs)\n\n    def loss_dis(self, dis, y_fake, y_real):\n        batchsize = len(y_fake)\n        L1 = F.sum(F.softplus(-y_real)) / batchsize\n        L2 = F.sum(F.softplus(y_fake)) / batchsize\n        loss = L1 + L2\n        chainer.report({'loss': loss}, dis)\n        return loss\n\n    def loss_gen(self, gen, y_fake):\n        batchsize = len(y_fake)\n        loss = F.sum(F.softplus(-y_fake)) / batchsize\n        chainer.report({'loss': loss}, gen)\n        return loss\n\n    def update_core(self):\n        gen_optimizer = self.get_optimizer('gen')\n        dis_optimizer = self.get_optimizer('dis')\n\n        batch = self.get_iterator('main').next()\n        x_real = Variable(self.converter(batch, self.device)) / 255.\n        xp = chainer.cuda.get_array_module(x_real.data)\n\n        gen, dis = self.gen, self.dis\n        batchsize = len(batch)\n\n        y_real = dis(x_real)\n\n        z = Variable(xp.asarray(gen.make_hidden(batchsize)))\n        x_fake = gen(z)\n        y_fake = dis(x_fake)\n\n        dis_optimizer.update(self.loss_dis, dis, y_fake, y_real)\n        gen_optimizer.update(self.loss_gen, gen, y_fake)\n"""
tensorboardX/examples/chainer/extension_logger/visualize.py,0,"b""#!/usr/bin/env python\n\nimport os\n\nimport numpy as np\nfrom PIL import Image\n\nimport chainer\nimport chainer.cuda\nfrom chainer import Variable\n\n\ndef out_generated_image(gen, dis, rows, cols, seed, dst, writer):\n    @chainer.training.make_extension()\n    def make_image(trainer):\n        np.random.seed(seed)\n        n_images = rows * cols\n        xp = gen.xp\n        z = Variable(xp.asarray(gen.make_hidden(n_images)))\n        with chainer.using_config('train', False):\n            x = gen(z)\n        writer.add_image('img', x, trainer.updater.iteration)\n\n    return make_image\n"""
tensorboardX/examples/chainer/extension_logger/writetensorboard.py,0,"b'import json\nimport os\nimport shutil\nimport tempfile\n\nimport six\nfrom chainer import reporter\nfrom chainer import serializer as serializer_module\nfrom chainer.training import extension\nfrom chainer.training import trigger as trigger_module\n\n\nclass LogTensorboard(extension.Extension):\n\n    """"""Trainer extension to output the accumulated results to a log file.\n\n    This extension accumulates the observations of the trainer to\n    :class:`~chainer.DictSummary` at a regular interval specified by a supplied\n    trigger, and writes them into a log file in JSON format.\n\n    There are two triggers to handle this extension. One is the trigger to\n    invoke this extension, which is used to handle the timing of accumulating\n    the results. It is set to ``1, \'iteration\'`` by default. The other is the\n    trigger to determine when to emit the result. When this trigger returns\n    True, this extension appends the summary of accumulated values to the list\n    of past summaries, and writes the list to the log file. Then, this\n    extension makes a new fresh summary object which is used until the next\n    time that the trigger fires.\n\n    It also adds some entries to each result dictionary.\n\n    - ``\'epoch\'`` and ``\'iteration\'`` are the epoch and iteration counts at the\n      output, respectively.\n    - ``\'elapsed_time\'`` is the elapsed time in seconds since the training\n      begins. The value is taken from :attr:`Trainer.elapsed_time`.\n\n    Args:\n        keys (iterable of strs): Keys of values to accumulate. If this is None,\n            all the values are accumulated and output to the log file.\n        trigger: Trigger that decides when to aggregate the result and output\n            the values. This is distinct from the trigger of this extension\n            itself. If it is a tuple in the form ``<int>, \'epoch\'`` or\n            ``<int>, \'iteration\'``, it is passed to :class:`IntervalTrigger`.\n        postprocess: Callback to postprocess the result dictionaries. Each\n            result dictionary is passed to this callback on the output. This\n            callback can modify the result dictionaries, which are used to\n            output to the log file.\n        log_name (str): Name of the log file under the output directory. It can\n            be a format string: the last result dictionary is passed for the\n            formatting. For example, users can use \'{iteration}\' to separate\n            the log files for different iterations. If the log name is None, it\n            does not output the log to any file.\n\n    """"""\n\n    def __init__(self, keys=None, trigger=(1, \'epoch\'), postprocess=None,\n                 log_name=\'log\', logger=None):\n        self._keys = keys\n        self._trigger = trigger_module.get_trigger(trigger)\n        self._postprocess = postprocess\n        self._log_name = log_name\n        self._log = []\n        self._logger = logger\n        self._init_summary()\n\n    def __call__(self, trainer):\n        # accumulate the observations\n        keys = self._keys\n        observation = trainer.observation\n        summary = self._summary\n\n        if keys is None:\n            summary.add(observation)\n        else:\n            summary.add({k: observation[k] for k in keys if k in observation})\n        for k, v in observation.items():\n            #self._logger.add_scalar(k, chainer.cuda.to_cpu(observation[k].data), trainer.updater.iteration)\n            self._logger.add_scalar(\n                k, observation[k], trainer.updater.iteration)\n        if self._trigger(trainer):\n            # output the result\n            stats = self._summary.compute_mean()\n            stats_cpu = {}\n            for name, value in six.iteritems(stats):\n                stats_cpu[name] = float(value)  # copy to CPU\n\n            updater = trainer.updater\n            stats_cpu[\'epoch\'] = updater.epoch\n            stats_cpu[\'iteration\'] = updater.iteration\n            stats_cpu[\'elapsed_time\'] = trainer.elapsed_time\n\n            if self._postprocess is not None:\n                self._postprocess(stats_cpu)\n\n            self._log.append(stats_cpu)\n\n            # write to the log file\n            if self._log_name is not None:\n                log_name = self._log_name.format(**stats_cpu)\n                fd, path = tempfile.mkstemp(prefix=log_name, dir=trainer.out)\n                with os.fdopen(fd, \'w\') as f:\n                    json.dump(self._log, f, indent=4)\n\n                new_path = os.path.join(trainer.out, log_name)\n                shutil.move(path, new_path)\n\n            # reset the summary for the next output\n            self._init_summary()\n\n    @property\n    def log(self):\n        """"""The current list of observation dictionaries.""""""\n        return self._log\n\n    def serialize(self, serializer):\n        if hasattr(self._trigger, \'serialize\'):\n            self._trigger.serialize(serializer[\'_trigger\'])\n\n        # Note that this serialization may lose some information of small\n        # numerical differences.\n        if isinstance(serializer, serializer_module.Serializer):\n            log = json.dumps(self._log)\n            serializer(\'_log\', log)\n        else:\n            log = serializer(\'_log\', \'\')\n            self._log = json.loads(log)\n\n    def _init_summary(self):\n        self._summary = reporter.DictSummary()\n'"
tensorboardX/examples/chainer/plain_logger/data.py,0,"b""import gzip\nimport os\n\nimport numpy as np\nimport six\nfrom six.moves.urllib import request\n\nparent = 'http://yann.lecun.com/exdb/mnist'\ntrain_images = 'train-images-idx3-ubyte.gz'\ntrain_labels = 'train-labels-idx1-ubyte.gz'\ntest_images = 't10k-images-idx3-ubyte.gz'\ntest_labels = 't10k-labels-idx1-ubyte.gz'\nnum_train = 60000\nnum_test = 10000\ndim = 784\n\n\ndef load_mnist(images, labels, num):\n    data = np.zeros(num * dim, dtype=np.uint8).reshape((num, dim))\n    target = np.zeros(num, dtype=np.uint8).reshape((num, ))\n\n    with gzip.open(images, 'rb') as f_images,\\\n            gzip.open(labels, 'rb') as f_labels:\n        f_images.read(16)\n        f_labels.read(8)\n        for i in six.moves.range(num):\n            target[i] = ord(f_labels.read(1))\n            for j in six.moves.range(dim):\n                data[i, j] = ord(f_images.read(1))\n\n    return data, target\n\n\ndef download_mnist_data():\n    print('Downloading {:s}...'.format(train_images))\n    request.urlretrieve('{:s}/{:s}'.format(parent, train_images), train_images)\n    print('Done')\n    print('Downloading {:s}...'.format(train_labels))\n    request.urlretrieve('{:s}/{:s}'.format(parent, train_labels), train_labels)\n    print('Done')\n    print('Downloading {:s}...'.format(test_images))\n    request.urlretrieve('{:s}/{:s}'.format(parent, test_images), test_images)\n    print('Done')\n    print('Downloading {:s}...'.format(test_labels))\n    request.urlretrieve('{:s}/{:s}'.format(parent, test_labels), test_labels)\n    print('Done')\n\n    print('Converting training data...')\n    data_train, target_train = load_mnist(train_images, train_labels,\n                                          num_train)\n    print('Done')\n    print('Converting test data...')\n    data_test, target_test = load_mnist(test_images, test_labels, num_test)\n    mnist = {'data': np.append(data_train, data_test, axis=0),\n             'target': np.append(target_train, target_test, axis=0)}\n    print('Done')\n    print('Save output...')\n    with open('mnist.pkl', 'wb') as output:\n        six.moves.cPickle.dump(mnist, output, -1)\n    print('Done')\n    print('Convert completed')\n\n\ndef load_mnist_data():\n    if not os.path.exists('mnist.pkl'):\n        download_mnist_data()\n    with open('mnist.pkl', 'rb') as mnist_pickle:\n        mnist = six.moves.cPickle.load(mnist_pickle)\n    return mnist\n"""
tensorboardX/examples/chainer/plain_logger/net.py,0,"b'import six\n\nimport chainer\nimport chainer.functions as F\nfrom chainer.functions.loss.vae import gaussian_kl_divergence\nimport chainer.links as L\n\n\nclass VAE(chainer.Chain):\n    """"""Variational AutoEncoder""""""\n\n    def __init__(self, n_in, n_latent, n_h):\n        super(VAE, self).__init__()\n        with self.init_scope():\n            # encoder\n            self.le1 = L.Linear(n_in, n_h)\n            self.le2_mu = L.Linear(n_h, n_latent)\n            self.le2_ln_var = L.Linear(n_h, n_latent)\n            # decoder\n            self.ld1 = L.Linear(n_latent, n_h)\n            self.ld2 = L.Linear(n_h, n_in)\n\n    def __call__(self, x, sigmoid=True):\n        """"""AutoEncoder""""""\n        return self.decode(self.encode(x)[0], sigmoid)\n\n    def encode(self, x):\n        h1 = F.tanh(self.le1(x))\n        mu = self.le2_mu(h1)\n        ln_var = self.le2_ln_var(h1)  # log(sigma**2)\n        return mu, ln_var\n\n    def decode(self, z, sigmoid=True):\n        h1 = F.tanh(self.ld1(z))\n        h2 = self.ld2(h1)\n        if sigmoid:\n            return F.sigmoid(h2)\n        else:\n            return h2\n\n    def get_loss_func(self, C=1.0, k=1):\n        """"""Get loss function of VAE.\n\n        The loss value is equal to ELBO (Evidence Lower Bound)\n        multiplied by -1.\n\n        Args:\n            C (int): Usually this is 1.0. Can be changed to control the\n                second term of ELBO bound, which works as regularization.\n            k (int): Number of Monte Carlo samples used in encoded vector.\n        """"""\n        def lf(x):\n            mu, ln_var = self.encode(x)\n            batchsize = len(mu.data)\n            # reconstruction loss\n            rec_loss = 0\n            for l in six.moves.range(k):\n                z = F.gaussian(mu, ln_var)\n                rec_loss += F.bernoulli_nll(x, self.decode(z, sigmoid=False)) \\\n                    / (k * batchsize)\n            self.rec_loss = rec_loss\n            self.loss = self.rec_loss + \\\n                C * gaussian_kl_divergence(mu, ln_var) / batchsize\n            return self.loss\n        return lf\n'"
tensorboardX/examples/chainer/plain_logger/train_vae.py,0,"b'#!/usr/bin/env python\n""""""Chainer example: train a VAE on MNIST\n""""""\nfrom __future__ import print_function\nimport argparse\n\nimport matplotlib\n# Disable interactive backend\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport six\n\nimport chainer\nfrom chainer import computational_graph\nfrom chainer import cuda\nfrom chainer import optimizers\nfrom chainer import serializers\nfrom tensorboardX import SummaryWriter\nimport data\nimport net\n\nwriter = SummaryWriter()\n\nparser = argparse.ArgumentParser(description=\'Chainer example: MNIST\')\nparser.add_argument(\'--initmodel\', \'-m\', default=\'\',\n                    help=\'Initialize the model from given file\')\nparser.add_argument(\'--resume\', \'-r\', default=\'\',\n                    help=\'Resume the optimization from snapshot\')\nparser.add_argument(\'--gpu\', \'-g\', default=-1, type=int,\n                    help=\'GPU ID (negative value indicates CPU)\')\nparser.add_argument(\'--epoch\', \'-e\', default=100, type=int,\n                    help=\'number of epochs to learn\')\nparser.add_argument(\'--dimz\', \'-z\', default=20, type=int,\n                    help=\'dimention of encoded vector\')\nparser.add_argument(\'--batchsize\', \'-b\', type=int, default=100,\n                    help=\'learning minibatch size\')\nparser.add_argument(\'--test\', action=\'store_true\',\n                    help=\'Use tiny datasets for quick tests\')\nargs = parser.parse_args()\n\nbatchsize = args.batchsize\nn_epoch = args.epoch\nn_latent = args.dimz\n\nwriter.add_text(\'config\', str(args))\n\nprint(\'GPU: {}\'.format(args.gpu))\nprint(\'# dim z: {}\'.format(args.dimz))\nprint(\'# Minibatch-size: {}\'.format(args.batchsize))\nprint(\'# epoch: {}\'.format(args.epoch))\nprint(\'\')\n\n# Prepare dataset\nprint(\'load MNIST dataset\')\nmnist = data.load_mnist_data()\nmnist[\'data\'] = mnist[\'data\'].astype(np.float32)\nmnist[\'data\'] /= 255\nmnist[\'target\'] = mnist[\'target\'].astype(np.int32)\n\nif args.test:\n    mnist[\'data\'] = mnist[\'data\'][0:100]\n    mnist[\'target\'] = mnist[\'target\'][0:100]\n    N = 30\nelse:\n    N = 60000\n\nx_train, x_test = np.split(mnist[\'data\'],   [N])\ny_train, y_test = np.split(mnist[\'target\'], [N])\nN_test = y_test.size\n\n# Prepare VAE model, defined in net.py\nmodel = net.VAE(784, n_latent, 500)\nif args.gpu >= 0:\n    cuda.get_device_from_id(args.gpu).use()\n    model.to_gpu()\nxp = np if args.gpu < 0 else cuda.cupy\n\n# Setup optimizer\noptimizer = optimizers.Adam()\noptimizer.setup(model)\n\n# Init/Resume\nif args.initmodel:\n    print(\'Load model from\', args.initmodel)\n    serializers.load_npz(args.initmodel, model)\nif args.resume:\n    print(\'Load optimizer state from\', args.resume)\n    serializers.load_npz(args.resume, optimizer)\n\n# Learning loop\nfor epoch in six.moves.range(1, n_epoch + 1):\n    print(\'epoch\', epoch)\n\n    # training\n    perm = np.random.permutation(N)\n    sum_loss = 0       # total loss\n    sum_rec_loss = 0   # reconstruction loss\n    for i in six.moves.range(0, N, batchsize):\n        x = chainer.Variable(xp.asarray(x_train[perm[i:i + batchsize]]))\n        optimizer.update(model.get_loss_func(), x)\n        if epoch == 1 and i == 0:\n            with open(\'graph.dot\', \'w\') as o:\n                g = computational_graph.build_computational_graph(\n                    (model.loss, ))\n                o.write(g.dump())\n            print(\'graph generated\')\n        writer.add_scalar(\'train/loss\', model.loss, epoch * N + i)\n        writer.add_scalar(\'train/rec_loss\', model.rec_loss, epoch * N + i)\n        sum_loss += float(model.loss.data) * len(x.data)\n        sum_rec_loss += float(model.rec_loss.data) * len(x.data)\n\n    print(\'train mean loss={}, mean reconstruction loss={}\'\n          .format(sum_loss / N, sum_rec_loss / N))\n\n    # evaluation\n    sum_loss = 0\n    sum_rec_loss = 0\n    with chainer.no_backprop_mode():\n        for i in six.moves.range(0, N_test, batchsize):\n            x = chainer.Variable(xp.asarray(x_test[i:i + batchsize]))\n            loss_func = model.get_loss_func(k=10)\n            loss_func(x)\n            sum_loss += float(model.loss.data) * len(x.data)\n            sum_rec_loss += float(model.rec_loss.data) * len(x.data)\n            writer.add_scalar(\'test/loss\', model.loss, epoch * N_test + i)\n            writer.add_scalar(\'test/rec_loss\', model.rec_loss,\n                              epoch * N_test + i)\n            writer.add_image(\'reconstructed\', model(\n                x).reshape(-1, 1, 28, 28), epoch * N_test + i)\n            writer.add_image(\'input\', x.reshape(-1, 1, 28, 28),\n                             epoch * N_test + i)\n            del model.loss\n    print(\'test  mean loss={}, mean reconstruction loss={}\'\n          .format(sum_loss / N_test, sum_rec_loss / N_test))\n\n\n# Save the model and the optimizer\nprint(\'save the model\')\nserializers.save_npz(\'mlp.model\', model)\nprint(\'save the optimizer\')\nserializers.save_npz(\'mlp.state\', optimizer)\n\nmodel.to_cpu()\n\n\n# original images and reconstructed images\ndef save_images(x, filename):\n    fig, ax = plt.subplots(3, 3, figsize=(9, 9), dpi=100)\n    for ai, xi in zip(ax.flatten(), x):\n        ai.imshow(xi.reshape(28, 28))\n    fig.savefig(filename)\n\n\ntrain_ind = [1, 3, 5, 10, 2, 0, 13, 15, 17]\nx = chainer.Variable(np.asarray(x_train[train_ind]))\nwith chainer.no_backprop_mode():\n    x1 = model(x)\nsave_images(x.data, \'train\')\nsave_images(x1.data, \'train_reconstructed\')\n\ntest_ind = [3, 2, 1, 18, 4, 8, 11, 17, 61]\nx = chainer.Variable(np.asarray(x_test[test_ind]))\nwith chainer.no_backprop_mode():\n    x1 = model(x)\nsave_images(x.data, \'test\')\nsave_images(x1.data, \'test_reconstructed\')\n\n\n# draw images from randomly sampled z\nz = chainer.Variable(np.random.normal(0, 1, (9, n_latent)).astype(np.float32))\nx = model.decode(z)\nsave_images(x.data, \'sampled\')\n'"
