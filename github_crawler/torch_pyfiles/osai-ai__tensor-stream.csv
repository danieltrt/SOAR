file_path,api_count,code
setup.py,6,"b'import os\nimport io\nimport re\nimport platform\nfrom setuptools import setup, find_packages, Extension\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\nimport torch\nfrom packaging import version\n\ndef read(*names, **kwargs):\n    with io.open(os.path.join(os.path.dirname(__file__), *names),\n                 encoding=kwargs.get(""encoding"", ""utf8"")) as fp:\n        return fp.read()\n\n\ndef find_version(*file_paths):\n    version_file = read(*file_paths)\n    version_match = re.search(r""^__version__ = [\'\\""]([^\'\\""]*)[\'\\""]"", version_file, re.M)\n    if version_match:\n        return version_match.group(1)\n    raise RuntimeError(""Unable to find version string."")\n\n\nreadme = read(\'README.md\')\n\nVERSION = find_version(\'tensor_stream\', \'__init__.py\')\n\ninclude_path = torch.utils.cpp_extension.include_paths(cuda=True)\ninclude_path += [""include/""]\ninclude_path += [""include/Wrappers/""]\nffmpeg_path = """"\nnvtx_path = """"\nif platform.system() == \'Windows\':\n    if not os.getenv(\'FFMPEG_PATH\'):\n        raise RuntimeError(""Please set FFmpeg root folder path to FFMPEG_PATH variable."")\n\n    ffmpeg_path = os.getenv(\'FFMPEG_PATH\')\n    include_path += [ffmpeg_path + ""/include""]\n\n    if not os.getenv(\'NVTOOLSEXT_PATH\'):\n        raise RuntimeError(""Please set NVToolsExt root folder path to NVTOOLSEXT_PATH variable."")\n\n    nvtx_path = os.getenv(\'NVTOOLSEXT_PATH\')\n    include_path += [nvtx_path + ""/include""]\n\n\nlibrary_path = torch.utils.cpp_extension.library_paths(cuda=True)\nif platform.system() == \'Windows\':\n    if ffmpeg_path:\n        library_path += [ffmpeg_path + ""/bin""]\n    else:\n        raise RuntimeError(""Please set FFmpeg root folder path to FFMPEG_PATH variable."")\n\n    if nvtx_path:\n        library_path += [nvtx_path + ""/lib/x64""]\n    else:\n        raise RuntimeError(""Please set NVToolsExt root folder path to NVTOOLSEXT_PATH variable."")\n\n\nlibrary = [""cudart""]\nlibrary += [""cuda""]\nlibrary += [""cudadevrt""]\nlibrary += [""cudart_static""]\nlibrary += [""avcodec""]\nlibrary += [""avdevice""]\nlibrary += [""avfilter""]\nlibrary += [""avformat""]\nlibrary += [""avutil""]\nlibrary += [""swresample""]\nlibrary += [""swscale""]\nif platform.system() == \'Windows\':\n    if version.parse(torch.__version__) <= version.parse(""1.1.0""):\n        library += [""caffe2""]\n        library += [""caffe2_gpu""]\n    library += [""_C""]\n    library += [""c10""]\n    library += [""c10_cuda""]\n\nif version.parse(torch.__version__) >= version.parse(""1.5.0""):\n    library += [""torch_cpu""]\n    library += [""torch_cuda""]\n\nif version.parse(torch.__version__) >= version.parse(""1.5.0"") or platform.system() == \'Windows\':\n    library += [""torch""]\n    library += [""torch_python""]\n\nif platform.system() == \'Windows\':\n    library += [""nvToolsExt64_1""]\nelse:\n    library += [""nvToolsExt""]\n\napp_src_path = []\napp_src_path += [""src/Decoder.cpp""]\napp_src_path += [""src/Common.cpp""]\napp_src_path += [""src/ColorConversion.cu""]\napp_src_path += [""src/Resize.cu""]\napp_src_path += [""src/Crop.cu""]\napp_src_path += [""src/Parser.cpp""]\napp_src_path += [""src/VideoProcessor.cpp""]\napp_src_path += [""src/Wrappers/WrapperPython.cpp""]\n\nsetup(\n    name=\'tensor_stream\',\n    version=VERSION,\n    author=\'Bykadorov Roman\',\n    description=\'Stream video reader\',\n    long_description=readme,\n    long_description_content_type=\'text/markdown\',\n    ext_modules=[\n        Extension(\n            name=\'TensorStream\',\n            sources=app_src_path,\n            include_dirs=include_path,\n            library_dirs=library_path,\n            libraries=library,\n            extra_compile_args=[\'-g\'],\n            language=\'c++\')\n    ],\n    cmdclass={\n        \'build_ext\': BuildExtension.with_options(use_ninja=False)\n    },\n    packages=find_packages(),\n    zip_safe=True,\n    python_requires=\'>=3.6\',\n    classifiers=[\n        \'Development Status :: 4 - Beta\',\n        \'Intended Audience :: Developers\',\n        \'Operating System :: OS Independent\',\n        \'Programming Language :: Python\',\n        \'Programming Language :: Python :: 3\',\n        \'Programming Language :: Python :: 3.6\',\n    ],\n    install_requires=[\'numpy\'],\n)\n'"
python_examples/different_streams.py,0,"b'import time\nimport argparse\nfrom threading import Thread\n\nfrom tensor_stream import TensorStreamConverter, FourCC, LogsLevel, LogsType\n\n\ndef parse_arguments():\n    parser = argparse.ArgumentParser(add_help=False,\n                                     description=""Example with two consumers"")\n    parser.add_argument(""-i1"", ""--input1"",\n                        default=""rtmp://37.228.119.44:1935/vod/big_buck_bunny.mp4"",\n                        help=""Path to bitstream: RTMP, local file"")\n    parser.add_argument(""-i2"", ""--input2"",\n                        default=""rtmp://37.228.119.44:1935/vod/big_buck_bunny.mp4"",\n                        help=""Path to bitstream: RTMP, local file"")\n    parser.add_argument(""-o1"", ""--output1"",\n                        help=""Name of output raw stream"", default="""")\n    parser.add_argument(""-o2"", ""--output2"",\n                        help=""Name of output raw stream"", default="""")\n    parser.add_argument(""-v1"", ""--verbose1"", default=""NONE"",\n                        choices=[""LOW"", ""MEDIUM"", ""HIGH""],\n                        help=""Set output level from library (default: NONE)"")\n    parser.add_argument(""-v2"", ""--verbose2"", default=""NONE"",\n                        choices=[""LOW"", ""MEDIUM"", ""HIGH""],\n                        help=""Set output level from library (default: NONE)"")\n    parser.add_argument(""-n1"", ""--number1"",\n                        help=""Number of frame to parse (default: 50)"",\n                        type=int, default=50)\n    parser.add_argument(""-n2"", ""--number2"",\n                        help=""Number of frame to parse (default: 50)"",\n                        type=int, default=50)\n    parser.add_argument(""--cuda_device1"",\n                        help=""Set GPU for processing (default: 0)"",\n                        type=int, default=0)\n    parser.add_argument(""--cuda_device2"",\n                        help=""Set GPU for processing (default: 0)"",\n                        type=int, default=0)\n    return parser.parse_args()\n\n\ndef consumer1(reader, n_frames):\n    for i in range(n_frames):\n        parameters = {\'pixel_format\': FourCC.RGB24}\n        tensor = reader.read(**parameters, name=""consumer1"")\n        if args.output1:\n            reader.dump(tensor, args.output1, **parameters)\n\n    print()\n    print(""consumer1 shape:"", tensor.shape)\n    print(""consumer1 dtype:"", tensor.dtype, end=\'\\n\\n\')\n    reader.stop()\n\n\ndef consumer2(reader, n_frames):\n    for i in range(n_frames):\n        parameters = {\'pixel_format\': FourCC.BGR24,\n                      \'width\': 720,\n                      \'height\': 480}\n        tensor, index = reader.read(**parameters,\n                                    name=""consumer2"",\n                                    return_index=True)\n        if args.output2:\n            reader.dump(tensor, args.output2, **parameters)\n\n        if index % int(reader.fps) == 0:\n            print(""consumer2 frame index"", index)\n\n    reader.stop()\n    time.sleep(1.0)  # prevent simultaneous print\n    print(""consumer2 shape:"", tensor.shape)\n    print(""consumer2 dtype:"", tensor.dtype)\n    print(""consumer2 last frame index:"", index)\n\n\nif __name__ == ""__main__"":\n    args = parse_arguments()\n\n    reader1 = TensorStreamConverter(args.input1,\n                                    cuda_device=args.cuda_device1)\n    reader1.enable_logs(LogsLevel[args.verbose1], LogsType.CONSOLE)\n    reader1.initialize(repeat_number=20)\n\n    reader2 = TensorStreamConverter(args.input2,\n                                    cuda_device=args.cuda_device2)\n    reader2.enable_logs(LogsLevel[args.verbose2], LogsType.CONSOLE)\n    reader2.initialize(repeat_number=20)\n\n    reader1.start()\n    reader2.start()\n\n    thread1 = Thread(target=consumer1, args=(reader1, args.number1))\n    thread2 = Thread(target=consumer2, args=(reader2, args.number2))\n\n    thread1.start()\n    thread2.start()\n\n    thread1.join()\n    thread2.join()\n'"
python_examples/many_consumers.py,0,"b'import time\nimport argparse\nfrom threading import Thread\n\nfrom tensor_stream import TensorStreamConverter, FourCC, FrameRate\n\n\ndef parse_arguments():\n    parser = argparse.ArgumentParser(add_help=False,\n                                     description=""Example with two consumers"")\n    parser.add_argument(""-i"", ""--input"",\n                        default=""rtmp://37.228.119.44:1935/vod/big_buck_bunny.mp4"",\n                        help=""Path to bitstream: RTMP, local file"")\n    parser.add_argument(""-n"", ""--number"",\n                        help=""Number of frame to parse (default: 100)"",\n                        type=int, default=100)\n    parser.add_argument(""--framerate_mode"", default=""NATIVE"",\n                        choices=[""NATIVE"", ""FAST"", ""BLOCKING""],\n                        help=""Stream reading mode"")\n    return parser.parse_args()\n\n\ndef consumer1(reader, n_frames):\n    try:\n        for i in range(n_frames):\n            tensor = reader.read(name=""consumer1"",\n                                 pixel_format=FourCC.RGB24,\n                                 width=540,\n                                 height=304)\n\n        print()\n        print(""consumer1 shape:"", tensor.shape)\n        print(""consumer1 dtype:"", tensor.dtype, end=\'\\n\\n\')\n\n    except RuntimeError as e:\n        print(f""Bad things happened: {e}"")\n\n\ndef consumer2(reader, n_frames):\n    try:\n        for i in range(n_frames):\n            tensor, index = reader.read(name=""consumer2"",\n                                        pixel_format=FourCC.BGR24,\n                                        return_index=True)\n\n            if index % int(reader.fps) == 0:\n                print(""consumer2 frame index"", index)\n  \n        time.sleep(1.0)  # prevent simultaneous print\n        print(""consumer2 shape:"", tensor.shape)\n        print(""consumer2 dtype:"", tensor.dtype)\n        print(""consumer2 last frame index:"", index)\n\n    except RuntimeError as e:\n        print(f""Bad things happened: {e}"")\n\n\nif __name__ == ""__main__"":\n    args = parse_arguments()\n\n    reader = TensorStreamConverter(args.input, framerate_mode=FrameRate[args.framerate_mode])\n    reader.initialize(repeat_number=20)\n\n    reader.start()\n\n    thread1 = Thread(target=consumer1, args=(reader, args.number))\n    thread2 = Thread(target=consumer2, args=(reader, args.number))\n\n    thread1.start()\n    thread2.start()\n\n    thread1.join()\n    thread2.join()\n\n    reader.stop()\n'"
python_examples/simple.py,0,"b'from tensor_stream import TensorStreamConverter\nfrom tensor_stream import LogsLevel, LogsType, FourCC, Planes, FrameRate, ResizeType\n\nimport argparse\nimport os\n\ndef string_bool(s):\n    if s not in {\'False\', \'True\'}:\n        raise ValueError(\'Not a valid boolean string\')\n    return s == \'True\'\n\ndef crop_coords(s):\n    try:\n        x1, y1, x2, y2 = map(int, s.split(\',\'))\n        return x1, y1, x2, y2\n    except:\n        raise argparse.ArgumentTypeError(""Coordinates must be x1,y1,x2,y2"")\n\ndef parse_arguments():\n    parser = argparse.ArgumentParser(add_help=False,\n                                     description=""Simple usage example"")\n    parser.add_argument(\'--help\', action=\'help\')\n    parser.add_argument(""-i"", ""--input"",\n                        default=""rtmp://37.228.119.44:1935/vod/big_buck_bunny.mp4"",\n                        help=""Path to bitstream: RTMP, local file"")\n    parser.add_argument(""-o"", ""--output"",\n                        help=""Name of output raw stream"", default="""")\n    parser.add_argument(""-w"", ""--width"",\n                        help=""Output width (default: input bitstream width)"",\n                        type=int, default=0)\n    parser.add_argument(""-h"", ""--height"",\n                        help=""Output height (default: input bitstream height)"",\n                        type=int, default=0)\n    parser.add_argument(""-fc"", ""--fourcc"", default=""RGB24"",\n                        choices=[""RGB24"",""BGR24"", ""Y800"", ""NV12"", ""UYVY"", ""YUV444"", ""HSV""],\n                        help=""Decoded stream\' FourCC (default: RGB24)"")\n    parser.add_argument(""-v"", ""--verbose"", default=""LOW"",\n                        choices=[""LOW"", ""MEDIUM"", ""HIGH""],\n                        help=""Set output level from library (default: LOW)"")\n    parser.add_argument(""-vd"", ""--verbose_destination"", default=""CONSOLE"",\n                        choices=[""CONSOLE"", ""FILE""],\n                        help=""Set destination of logs (default: CONSOLE)"")\n    parser.add_argument(""-n"", ""--number"",\n                        help=""Number of frame to parse (default: unlimited)"",\n                        type=int, default=0)\n    parser.add_argument(""-bs"", ""--buffer_size"",\n                        help=""Size of internal buffer stores processed frames (default: 5)"",\n                        type=int, default=5)\n    parser.add_argument(""--normalize"",\n                        help=""Set if output pixel values should be normalized. Option takes True or False arguments. \\\n                              If not set TensorStream will define value automatically"",\n                        type=string_bool)\n    parser.add_argument(""--nvtx"",\n                        help=""Enable NVTX logs"",\n                        action=\'store_true\')\n    parser.add_argument(""--cuda_device"",\n                        help=""Set GPU for processing (default: 0)"",\n                        type=int, default=0)\n    parser.add_argument(""--planes"", default=""MERGED"",\n                        choices=[""PLANAR"", ""MERGED""],\n                        help=""Possible planes order in RGB format"")\n    parser.add_argument(""--resize_type"", default=""NEAREST"",\n                        choices=[""NEAREST"", ""BILINEAR"", ""BICUBIC"", ""AREA""],\n                        help=""Algorithm used to do resize"")\n    parser.add_argument(""--framerate_mode"", default=""NATIVE"",\n                        choices=[""NATIVE"", ""FAST"", ""BLOCKING""],\n                        help=""Stream reading mode"")\n    parser.add_argument(""--skip_analyze"",\n                        help=""Skip bitstream frames reordering / loss analyze stage"",\n                        action=\'store_true\')\n    parser.add_argument(""--timeout"",\n                        help=""Set timeout in seconds for input frame reading (default: None, means disabled)"",\n                        type=float, default=None)\n    parser.add_argument(""--crop"", \n                        help=""set crop, left top corner and right bottom corner (default: disabled)"",\n                        type=crop_coords, default=(0,0,0,0))\n\n    return parser.parse_args()\n\n\nif __name__ == \'__main__\':\n    args = parse_arguments()\n\n    reader = TensorStreamConverter(args.input,\n                                   max_consumers=5,\n                                   cuda_device=args.cuda_device,\n                                   buffer_size=args.buffer_size,\n                                   framerate_mode=FrameRate[args.framerate_mode],\n                                   timeout=args.timeout)\n    # To log initialize stage, logs should be defined before initialize call\n    reader.enable_logs(LogsLevel[args.verbose], LogsType[args.verbose_destination])\n\n    if args.nvtx:\n        reader.enable_nvtx()\n    \n    reader.initialize(repeat_number=20)\n\n    if args.skip_analyze:\n        reader.skip_analyze()\n\n    reader.start()\n\n    if args.output:\n        if os.path.exists(args.output + "".yuv""):\n            os.remove(args.output + "".yuv"")\n\n    print(f""Normalize {args.crop}"")\n    tensor = None\n    try:\n        while True:\n            parameters = {\'pixel_format\': FourCC[args.fourcc],\n                          \'width\': args.width,\n                          \'height\': args.height,\n                          \'crop_coords\' : args.crop,\n                          \'normalization\': args.normalize,\n                          \'planes_pos\': Planes[args.planes],\n                          \'resize_type\': ResizeType[args.resize_type]}\n\n            tensor, index = reader.read(**parameters, return_index=True)\n\n            if args.number:\n                if index > args.number:\n                    break\n\n            if args.output:\n                reader.dump(tensor, args.output, **parameters)\n    except RuntimeError as e:\n        print(f""Bad things happened: {e}"")\n    finally:\n        print(""Frame size: "", reader.frame_size)\n        print(""FPS: "", reader.fps)\n        if tensor is not None:\n            print(""Tensor shape:"", tensor.shape)\n            print(""Tensor dtype:"", tensor.dtype)\n            print(""Tensor device:"", tensor.device)\n        reader.stop()\n'"
tensor_stream/__init__.py,0,"b""from tensor_stream.tensor_stream import (\n    TensorStreamConverter,\n    StatusLevel,\n    LogsLevel,\n    LogsType,\n    FourCC,\n    Planes,\n    ResizeType,\n    FrameRate,\n    FrameParameters\n)\n\n__version__ = '0.4.0'\n"""
tensor_stream/tensor_stream.py,1,"b'import torch\nimport TensorStream\nimport threading\nimport logging\nfrom enum import Enum\n\n\n## @defgroup pythonAPI Python API\n# @brief The list of TensorStream components can be used via Python interface\n# @details Here are all the classes, enums, functions described which can be used via Python to do RTMP/local stream converting to Pytorch Tensor with additional post-processing conversions\n# @{\n\n## Class with list of possible error statuses can be returned from TensorStream extension\n# @warning These statuses are used only in Python wrapper that communicates with TensorStream C++ extension\nclass StatusLevel(Enum):\n    ## No errors\n    OK = 0\n    ## Need to call %TensorStream API one more time\n    REPEAT = 1\n    ## Some issue in %TensorStream component occured\n    ERROR = 2\n\n\n## Class with list of modes for logs output\n# @details Used in @ref TensorStreamConverter.enable_logs() function\nclass LogsLevel(Enum):\n    ## No logs are needed\n    NONE = 0\n    ## Print the indexes of processed frames\n    LOW = 1\n    ## Print also frame processing duration\n    MEDIUM = 2\n    ## Print also the detailed information about functions in callstack\n    HIGH = 3\n\n\n## Class with list of places the log file has to be written to\n# @details Used in @ref TensorStreamConverter.enable_logs() function\nclass LogsType(Enum):\n    ## Print all logs to file\n    FILE = 1\n    ## Print all logs to console\n    CONSOLE = 2\n\n\n## Class with supported frame output color formats\n# @details Used in @ref TensorStreamConverter.read() function\nclass FourCC(Enum):\n    ## Monochrome format, 8 bit for pixel\n    Y800 = 0\n    ## RGB format, 24 bit for pixel, color plane order: R, G, B\n    RGB24 = 1\n    ## RGB format, 24 bit for pixel, color plane order: B, G, R\n    BGR24 = 2\n    ## YUV semi-planar format, 12 bit for pixel\n    NV12 = 3\n    ## YUV merged format, 16 bit for pixel\n    UYVY = 4\n    ## YUV merged format, 24 bit for pixel\n    YUV444 = 5\n    ## HSV format, 24 bit for pixel\n    HSV = 6\n\n\n## Algorithm used to do resize\n# @details Resize algorithms are applied to NV12 so b2b with another frameworks isn\'t guaranteed\nclass ResizeType(Enum):\n    ## Simple algorithm without any interpolation\n    NEAREST = 0\n    ## Algorithm that does simple linear interpolation\n    BILINEAR = 1\n    ## Algorithm that does spline bicubic interpolation\n    BICUBIC = 2\n    ## Algorithm that does INTER_AREA OpenCV interpolation\n    AREA = 3\n\n\n## Possible planes order in RGB format\nclass Planes(Enum):\n    ## Color components R, G, B are stored in memory separately like RRRRR, GGGGG, BBBBB\n    PLANAR = 0\n    ## Color components R, G, B are stored in memory one by one like RGBRGBRGB\n    MERGED = 1\n\n\n## Enum with possible stream reading modes\nclass FrameRate(Enum):\n    ## Read at native stream frame rate\n    NATIVE = 0\n    ## Read at fixed stream frame rate\n    NATIVE_SIMPLE = 1\n    ## Read frames as fast as possible\n    FAST = 2\n    ## Read frame by frame without skipping (only local files)\n    BLOCKING = 3\n\n\n## Class that stores frame parameters\nclass FrameParameters:\n    ## Constructor of FrameParameters class\n    # @param[in] width Specify the width of decoded frame\n    # @param[in] height Specify the height of decoded frame\n    # @param[in] crop_coords Left top and right bottom coordinates of crop\n    # @param[in] resize_type Algorithm used to do resize, see @ref ResizeType for supported values\n    # @param[in] pixel_format Output FourCC of frame stored in tensor, see @ref FourCC for supported values\n    # @param[in] planes_pos Possible planes order in RGB format, see @ref Planes for supported values\n    # @param[in] normalization Should final colors be normalized or not\n    def __init__(self,\n                 width=0,\n                 height=0,\n                 crop_coords=(0, 0, 0, 0),\n                 resize_type=ResizeType.NEAREST,\n                 pixel_format=FourCC.RGB24,\n                 planes_pos=Planes.MERGED,\n                 normalization=None):\n        parameters = TensorStream.FrameParameters()\n        color_options = TensorStream.ColorOptions(TensorStream.FourCC(pixel_format.value))\n        if normalization is not None:\n            color_options.normalization = normalization\n        color_options.planesPos = TensorStream.Planes(planes_pos.value)\n\n        resize_options = TensorStream.ResizeOptions()\n        resize_options.width = width\n        resize_options.height = height\n        resize_options.resizeType = TensorStream.ResizeType(resize_type.value)\n\n        crop_options = TensorStream.CropOptions()\n        crop_options.leftTopCorner = crop_coords[0:2]\n        crop_options.rightBottomCorner = crop_coords[2:4]\n\n        parameters.color = color_options\n        parameters.resize = resize_options\n        parameters.crop = crop_options\n        self.parameters = parameters\n\n    def __repr__(self):\n        string = (f""FrameParameters(\\n""\n                  f""    width={self.parameters.resize.height},\\n""\n                  f""    height={self.parameters.resize.width},\\n""\n                  f""    crop_left_top={self.parameters.crop.leftTopCorner},\\n""\n                  f""    crop_right_bottom={self.parameters.crop.rightBottomCorner},\\n""\n                  f""    resize_type={self.parameters.resize.resizeType},\\n""\n                  f""    pixel_format={self.parameters.color.dstFourCC},\\n""\n                  f""    planes_pos={self.parameters.color.planesPos},\\n""\n                  f""    normalization={self.parameters.color.normalization}\\n""\n                  "")"")\n        return string\n\n\n## Class which allow start decoding process and get Pytorch tensors with post-processed frame data\nclass TensorStreamConverter:\n    ## Constructor of TensorStreamConverter class\n    # @param[in] stream_url Path to stream should be decoded\n    # @param[in] max_consumers Allowed number of simultaneously working consumers\n    # @param[in] cuda_device GPU used for execution\n    # @param[in] buffer_size Set how many processed frames can be stored in internal buffer\n    # @warning Size of buffer should be less or equal to DPB\n    # @param[in] timeout How many seconds to wait for the new frame\n    def __init__(self,\n                 stream_url,\n                 max_consumers=5,\n                 cuda_device=torch.cuda.current_device(),\n                 buffer_size=5,\n                 framerate_mode=FrameRate.NATIVE,\n                 timeout=None):\n        self.log = logging.getLogger(__name__)\n        self.log.info(""Create TensorStream"")\n        self.tensor_stream = TensorStream.TensorStream()\n        self.thread = None\n        ## Amount of frames per second obtained from input bitstream, set by @ref initialize() function\n        self.fps = None\n        ## Size (width and height) of frames in input bitstream, set by @ref initialize() function\n        self.frame_size = None\n\n        self.max_consumers = max_consumers\n        self.cuda_device = cuda_device\n        self.buffer_size = buffer_size\n        self.stream_url = stream_url\n        self.framerate_mode = TensorStream.FrameRateMode(framerate_mode.value)\n        self.set_timeout(timeout=timeout)\n\n    ## Initialization of C++ extension\n    # @param[in] repeat_number Set how many times try to initialize pipeline in case of any issues\n    # @warning if initialization attempts exceeded @ref repeat_number, RuntimeError is being thrown\n    def initialize(self, repeat_number=1):\n        self.log.info(""Initialize TensorStream"")\n        status = StatusLevel.REPEAT.value\n        repeat = repeat_number\n        while status != StatusLevel.OK.value and repeat > 0:\n            status = self.tensor_stream.init(self.stream_url,\n                                             self.max_consumers,\n                                             self.cuda_device,\n                                             self.buffer_size,\n                                             self.framerate_mode)\n            if status != StatusLevel.OK.value:\n                self.stop()\n                repeat = repeat - 1\n\n        if repeat == 0:\n            raise RuntimeError(""Can\'t initialize TensorStream"")\n        else:\n            params = self.tensor_stream.getPars()\n            self.fps = params[\'framerate_num\'] / params[\'framerate_den\']\n            self.frame_size = (params[\'width\'], params[\'height\'])\n\n    ## Enable logs from TensorStream C++ extension\n    # @param[in] level Specify output level of logs, see @ref LogsLevel for supported values\n    # @param[in] log_type Specify where the logs should be printed, see @ref LogsType for supported values\n    def enable_logs(self, level, log_type):\n        if level != LogsLevel.NONE:\n            if log_type == LogsType.FILE:\n                self.tensor_stream.enableLogs(level.value)\n            else:\n                self.tensor_stream.enableLogs(-level.value)\n\n    ## Enable NVTX from TensorStream C++ extension\n    def enable_nvtx(self):\n        self.tensor_stream.enableNVTX()\n\n    ## Pass timeout for reading input frame\n    # @param[in] timeout How many seconds to wait for the new frame\n    def set_timeout(self, timeout):\n        if timeout is None:\n            self.tensor_stream.setTimeout(-1)\n        else:\n            ms_timeout = int(timeout * 1000)\n            self.tensor_stream.setTimeout(ms_timeout)\n\n    ## Skip bitstream frames reordering / loss analyze stage\n    def skip_analyze(self):\n        self.tensor_stream.skipAnalyze()\n\n    ## Read the next decoded frame, should be invoked only after @ref start() call\n    # @param[in] name The unique ID of consumer. Needed mostly in case of several consumers work in different threads\n    # @param[in] width Specify the width of decoded frame\n    # @param[in] height Specify the height of decoded frame\n    # @param[in] crop_coords Left top and right bottom coordinates of crop\n    # @param[in] resize_type Algorithm used to do resize, see @ref ResizeType for supported values\n    # @param[in] pixel_format Output FourCC of frame stored in tensor, see @ref FourCC for supported values\n    # @param[in] planes_pos Possible planes order in RGB format, see @ref Planes for supported values\n    # @param[in] normalization Should final colors be normalized or not\n    # @param[in] delay Specify which frame should be read from decoded buffer. Can take values in range [-buffer_size, 0]\n    # @param[in] return_index Specify whether need return index of decoded frame or not\n\n    # @return Decoded frame in CUDA memory wrapped to Pytorch tensor and index of decoded frame if @ref return_index option set\n    def read(self,\n             name=""default"",\n             width=0,\n             height=0,\n             resize_type=ResizeType.NEAREST,\n             crop_coords=(0,0,0,0),\n             pixel_format=FourCC.RGB24,\n             planes_pos=Planes.MERGED,\n             normalization=None,\n             delay=0,\n             return_index=False):\n\n        frame_parameters = FrameParameters(\n            width=width,\n            height=height,\n            crop_coords=crop_coords,\n            resize_type=resize_type,\n            pixel_format=pixel_format,\n            planes_pos=planes_pos,\n            normalization=normalization\n        )\n        result = self.param_read(frame_parameters,\n                                 name=name,\n                                 delay=delay,\n                                 return_index=return_index)\n        return result\n\n    ## Read the next decoded frame, should be invoked only after @ref start() call\n    # @param[in] name The unique ID of consumer. Needed mostly in case of several consumers work in different threads\n    # @param[in] frame_parameters Frame parameters\n    # @param[in] delay Specify which frame should be read from decoded buffer. Can take values in range [-buffer_size, 0]\n    # @param[in] return_index Specify whether need return index of decoded frame or not\n\n    # @return Decoded frame in CUDA memory wrapped to Pytorch tensor and index of decoded frame if @ref return_index option set\n    def param_read(self,\n                   frame_parameters: FrameParameters,\n                   name=""default"",\n                   delay=0,\n                   return_index=False):\n        tensor, index = self.tensor_stream.get(name, delay, frame_parameters.parameters)\n        if return_index:\n            return tensor, index\n        else:\n            return tensor\n\n    ## Dump the tensor to hard driver\n    # @param[in] tensor Tensor which should be dumped\n    # @param[in] name The name of file with dumps\n    # @param[in] width Specify the width of decoded frame\n    # @param[in] height Specify the height of decoded frame\n    # @param[in] crop_coords Left top and right bottom coordinates of crop\n    # @param[in] resize_type Algorithm used to do resize, see @ref ResizeType for supported values\n    # @param[in] pixel_format Output FourCC of frame stored in tensor, see @ref FourCC for supported values\n    # @param[in] planes_pos Possible planes order in RGB format, see @ref Planes for supported values\n    # @param[in] normalization Should final colors be normalized or not\n    def dump(self,\n             tensor,\n             name=""default"",\n             width=0,\n             height=0,\n             crop_coords=(0,0,0,0),\n             resize_type=ResizeType.NEAREST,\n             pixel_format=FourCC.RGB24,\n             planes_pos=Planes.MERGED,\n             normalization=None):\n        frame_parameters = FrameParameters(\n            width=width,\n            height=height,\n            crop_coords=crop_coords,\n            resize_type=resize_type,\n            pixel_format=pixel_format,\n            planes_pos=planes_pos,\n            normalization=normalization\n        )\n        self.tensor_stream.dump(tensor, name, frame_parameters.parameters)\n\n    def _start(self):\n        self.tensor_stream.start()\n\n    ## Start processing with parameters set via @ref initialize() function\n    # This functions is being executed in separate thread\n    def start(self):\n        self.thread = threading.Thread(target=self._start)\n        self.thread.start()\n\n    ## Close TensorStream session\n    # @param[in] level Value from @ref CloseLevel\n    def stop(self):\n        self.log.info(""Stop TensorStream"")\n        self.tensor_stream.close()\n        if self.thread is not None:\n            self.thread.join()\n\n## @}'"
python_examples/fast_neural_style/download_saved_models.py,0,"b'import urllib.request\nimport zipfile\n\n\nif __name__ == ""__main__"":\n    url = ""https://www.dropbox.com/s/lrvwfehqdcxoza8/saved_models.zip?dl=1""\n\n    u = urllib.request.urlopen(url)\n    data = u.read()\n    u.close()\n\n    with open(\'saved_models.zip\', ""wb"") as f:\n        f.write(data)\n\n    zip_ref = zipfile.ZipFile(\'saved_models.zip\', \'r\')\n    zip_ref.extractall(\'.\')\n    zip_ref.close()\n'"
python_examples/fast_neural_style/ffmpeg_video_writer.py,0,"b'import logging\nimport subprocess as sp\n\n\nclass FFmpegVideoWriter:\n    def __init__(self,\n                 url,\n                 out_size,\n                 out_fps=30,\n                 bitrate=2000,\n                 codec=""h264_nvenc"",\n                 preset=""medium"",\n                 loglevel=""info"",\n                 keyframe_freq=2):\n        self.logger = logging.getLogger(__name__)\n\n        self.url = url\n        self.out_fps = out_fps\n        self.out_size = out_size\n        self.bitrate = bitrate\n        self.codec = codec\n        self.preset = preset\n        self.loglevel = loglevel\n        self.keyframe_freq = keyframe_freq\n\n        self.in_size = None\n        self.pipe = None\n        self.stopped = True\n        self.logger.info(""Create VideoWriter"")\n\n    def _start(self):\n        command = [\n            \'ffmpeg\',\n            \'-y\',\n            \'-loglevel\', self.loglevel,\n            # input\n            \'-r:v\', str(self.out_fps),\n            \'-f\', \'rawvideo\', \'-vcodec\', \'rawvideo\',\n            \'-s:v\', \'%dx%d\'%tuple(self.in_size), \'-pix_fmt\', \'rgb24\',\n            \'-i\', \'-\', \'-an\',\n            # output\n            \'-vcodec\', self.codec,\n            \'-s:v\', \'%dx%d\'%tuple(self.out_size),\n            \'-r:v\', str(self.out_fps),\n            \'-b:v\', \'%dk\' % self.bitrate,\n            \'-preset\', self.preset,\n            \'-g\', str(int(self.keyframe_freq * self.out_fps)),\n            \'-f\', \'mpegts\', self.url\n        ]\n        self.logger.info(f""Start VideoWriter\\nFFmpeg command: {command}"")\n        self.pipe = sp.Popen(\n            command,\n            stdin=sp.PIPE\n        )\n        self.stopped = False\n\n    def write(self, frame):\n        if self.stopped:\n            self.in_size = tuple(frame.shape[:2][::-1])\n            self._start()\n\n        self.pipe.stdin.write(frame.tobytes())\n\n    def stop(self):\n        self.logger.info(""Stop VideoWriter"")\n        if self.pipe is not None:\n            self.pipe.stdin.close()\n            self.pipe.kill()\n            self.pipe.wait()\n'"
python_examples/fast_neural_style/neural_style.py,4,"b'import re\nimport torch\nimport argparse\nimport numpy as np\n\nfrom transfromer_net import TransformerNet\nfrom ffmpeg_video_writer import FFmpegVideoWriter\n\nfrom tensor_stream import TensorStreamConverter, Planes, FourCC\n\n\ndef parse_arguments():\n    parser = argparse.ArgumentParser(add_help=False,\n                                     description=""Real-time video style transfer example"")\n    parser.add_argument(\'--help\', action=\'help\')\n    parser.add_argument(""-m"", ""--model"",\n                        default=""saved_models/mosaic.pth"",\n                        help=""Path to model weight"")\n    parser.add_argument(""-i"", ""--input"",\n                        default=""rtmp://37.228.119.44:1935/vod/big_buck_bunny.mp4"",\n                        help=""Input stream (RTMP) or local video file"")\n    parser.add_argument(""-o"", ""--output"",\n                        default=""video.mp4"",\n                        help=""Output stream or video file"")\n    parser.add_argument(""--concat_orig"", action=\'store_true\',\n                        help=""Concatenate original frames to output video"")\n    parser.add_argument(""-w"", ""--width"",\n                        help=""Output width (default: input width)"",\n                        type=int, default=0)\n    parser.add_argument(""-h"", ""--height"",\n                        help=""Output height (default: input height)"",\n                        type=int, default=0)\n    parser.add_argument(""-t"", ""--time"",\n                        help=""Seconds to record, (default: unlimited)"",\n                        type=int, default=0)\n    parser.add_argument(\'-c\', \'--codec\', default=""h264_nvenc"",\n                        help=\'Encoder codec for output video\', type=str)\n    parser.add_argument(\'-p\', \'--preset\', default=""slow"",\n                        help=\'Preset for output video\', type=str)\n    parser.add_argument(\'-b\', \'--bitrate\', default=5000,\n                        help=\'Bitrate (kb/s) for output video\', type=int)\n    return parser.parse_args()\n\n\ndef load_model(model_path, device=\'cuda\'):\n    model = TransformerNet()\n    state_dict = torch.load(model_path)\n\n    # remove saved deprecated running_* keys in InstanceNorm from the checkpoint\n    for k in list(state_dict.keys()):\n        if re.search(r\'in\\d+\\.running_(mean|var)$\', k):\n            del state_dict[k]\n\n    model.load_state_dict(state_dict)\n    model = model.to(device)\n    model.eval()\n    return model\n\n\ndef tensor_to_image(tensor):\n    image = tensor[0].to(torch.uint8)\n    image = image.permute(1, 2, 0)\n    image = image.cpu().numpy()\n    return image\n\n\nif __name__ == ""__main__"":\n    args = parse_arguments()\n\n    style_model = load_model(args.model, device=\'cuda\')\n\n    reader = TensorStreamConverter(args.input)\n    reader.initialize(repeat_number=20)\n    print(f""Input video frame size: {reader.frame_size}, fps: {reader.fps}"")\n\n    width = args.width if args.width else reader.frame_size[0]\n    height = args.height if args.height else reader.frame_size[1]\n    print(f""Model input image width: {width}, height: {height}"")\n\n    writer = FFmpegVideoWriter(args.output,\n                               out_size=(width * 2 if args.concat_orig else width,\n                                         height),\n                               out_fps=reader.fps,\n                               bitrate=args.bitrate,\n                               codec=args.codec,\n                               preset=args.preset)\n\n    reader.start()\n\n    try:\n        while True:\n            tensor, index = reader.read(pixel_format=FourCC.RGB24,\n                                        return_index=True,\n                                        width=width,\n                                        height=height,\n                                        planes_pos=Planes.PLANAR,\n                                        normalization=True)\n            tensor = tensor.unsqueeze(0)\n            with torch.no_grad():\n                output = style_model(tensor)\n                output = torch.clamp(output, 0, 255)\n\n            style_frame = tensor_to_image(output)\n            if args.concat_orig:\n                orig_frame = tensor_to_image(tensor)\n                write_frame = np.concatenate((orig_frame, style_frame), axis=1)\n            else:\n                write_frame = style_frame\n            writer.write(write_frame)\n\n            if args.time:\n                if index > args.time * reader.fps:\n                    break\n\n    except RuntimeError as e:\n        print(f""Bad things happened: {e}"")\n    finally:\n        reader.stop()\n        writer.stop()\n'"
python_examples/fast_neural_style/transfromer_net.py,19,"b'import torch\n\n\nclass TransformerNet(torch.nn.Module):\n    def __init__(self):\n        super(TransformerNet, self).__init__()\n        # Initial convolution layers\n        self.conv1 = ConvLayer(3, 32, kernel_size=9, stride=1)\n        self.in1 = torch.nn.InstanceNorm2d(32, affine=True)\n        self.conv2 = ConvLayer(32, 64, kernel_size=3, stride=2)\n        self.in2 = torch.nn.InstanceNorm2d(64, affine=True)\n        self.conv3 = ConvLayer(64, 128, kernel_size=3, stride=2)\n        self.in3 = torch.nn.InstanceNorm2d(128, affine=True)\n        # Residual layers\n        self.res1 = ResidualBlock(128)\n        self.res2 = ResidualBlock(128)\n        self.res3 = ResidualBlock(128)\n        self.res4 = ResidualBlock(128)\n        self.res5 = ResidualBlock(128)\n        # Upsampling Layers\n        self.deconv1 = UpsampleConvLayer(128, 64, kernel_size=3, stride=1, upsample=2)\n        self.in4 = torch.nn.InstanceNorm2d(64, affine=True)\n        self.deconv2 = UpsampleConvLayer(64, 32, kernel_size=3, stride=1, upsample=2)\n        self.in5 = torch.nn.InstanceNorm2d(32, affine=True)\n        self.deconv3 = ConvLayer(32, 3, kernel_size=9, stride=1)\n        # Non-linearities\n        self.relu = torch.nn.ReLU()\n\n    def forward(self, X):\n        y = self.relu(self.in1(self.conv1(X)))\n        y = self.relu(self.in2(self.conv2(y)))\n        y = self.relu(self.in3(self.conv3(y)))\n        y = self.res1(y)\n        y = self.res2(y)\n        y = self.res3(y)\n        y = self.res4(y)\n        y = self.res5(y)\n        y = self.relu(self.in4(self.deconv1(y)))\n        y = self.relu(self.in5(self.deconv2(y)))\n        y = self.deconv3(y)\n        return y\n\n\nclass ConvLayer(torch.nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride):\n        super(ConvLayer, self).__init__()\n        reflection_padding = kernel_size // 2\n        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n\n    def forward(self, x):\n        out = self.reflection_pad(x)\n        out = self.conv2d(out)\n        return out\n\n\nclass ResidualBlock(torch.nn.Module):\n    """"""ResidualBlock\n    introduced in: https://arxiv.org/abs/1512.03385\n    recommended architecture: http://torch.ch/blog/2016/02/04/resnets.html\n    """"""\n\n    def __init__(self, channels):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n        self.in1 = torch.nn.InstanceNorm2d(channels, affine=True)\n        self.conv2 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n        self.in2 = torch.nn.InstanceNorm2d(channels, affine=True)\n        self.relu = torch.nn.ReLU()\n\n    def forward(self, x):\n        residual = x\n        out = self.relu(self.in1(self.conv1(x)))\n        out = self.in2(self.conv2(out))\n        out = out + residual\n        return out\n\n\nclass UpsampleConvLayer(torch.nn.Module):\n    """"""UpsampleConvLayer\n    Upsamples the input and then does a convolution. This method gives better results\n    compared to ConvTranspose2d.\n    ref: http://distill.pub/2016/deconv-checkerboard/\n    """"""\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n        super(UpsampleConvLayer, self).__init__()\n        self.upsample = upsample\n        reflection_padding = kernel_size // 2\n        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n\n    def forward(self, x):\n        x_in = x\n        if self.upsample:\n            x_in = torch.nn.functional.interpolate(x_in, mode=\'nearest\', scale_factor=self.upsample)\n        out = self.reflection_pad(x_in)\n        out = self.conv2d(out)\n        return out\n'"
tests/python_tests/CommonTests.py,1,"b'import torch\nfrom tensor_stream import TensorStreamConverter, LogsLevel, LogsType\nimport time\nimport unittest\nimport os\n\n\nclass TestTensorStream(unittest.TestCase):\n    path = os.path.dirname(os.path.abspath(__file__)) \\\n           + ""/../../tests/resources/billiard_1920x1080_420_100.h264""\n\n    def setUp(self):\n        print (f""In method {self._testMethodName}"")\n\n    def test_constructor(self):\n        max_consumers = 5\n        cuda_device = 0\n        buffer_size = 10\n        reader = TensorStreamConverter(self.path,\n                                       max_consumers=max_consumers,\n                                       cuda_device=cuda_device,\n                                       buffer_size=buffer_size)\n        self.assertEqual(reader.max_consumers, max_consumers)\n        self.assertEqual(reader.cuda_device, cuda_device)\n        self.assertEqual(reader.buffer_size, buffer_size)\n        self.assertEqual(reader.stream_url, self.path)\n\n    def test_constructor_default(self):\n        reader = TensorStreamConverter(self.path)\n        self.assertEqual(reader.max_consumers, 5)\n        self.assertEqual(reader.cuda_device, torch.cuda.current_device())\n        self.assertEqual(reader.buffer_size, 5)\n        self.assertEqual(reader.stream_url, self.path)\n\n    def test_initialize_wrong_path(self):\n        reader = TensorStreamConverter(""wrong.h264"",\n                                       max_consumers=5,\n                                       cuda_device=0,\n                                       buffer_size=10)\n        with self.assertRaises(RuntimeError):\n            reader.initialize(repeat_number=5)\n\n    def test_initialize_correct_path(self):\n        reader = TensorStreamConverter(self.path,\n                                       max_consumers=5,\n                                       cuda_device=0,\n                                       buffer_size=10)\n        reader.initialize()\n        self.assertEqual(reader.frame_size, (1920, 1080))\n        self.assertEqual(reader.fps, 25)\n\n    def test_stop_without_init(self):\n        reader = TensorStreamConverter(self.path)\n        reader.stop()\n\n    def test_logs_enabling(self):\n        reader = TensorStreamConverter(self.path)\n        reader.enable_logs(LogsLevel.LOW, LogsType.CONSOLE)\n        reader.enable_nvtx()\n\n    def test_start_close(self):\n        reader = TensorStreamConverter(self.path)\n        reader.initialize()\n        reader.start()\n        time.sleep(1.0)\n        reader.stop()\n\n    def test_close_start(self):\n        reader = TensorStreamConverter(self.path)\n        reader.initialize()\n        reader.stop()\n        #won\'t work but at least no crush\n        reader.start()\n\n    def test_start_read_close(self):\n        reader = TensorStreamConverter(self.path)\n        reader.initialize()\n        reader.start()\n        time.sleep(1.0)\n        tensor = reader.read()\n        self.assertEqual(tensor.shape[0], 1080)\n        self.assertEqual(tensor.shape[1], 1920)\n        self.assertEqual(tensor.shape[2], 3)\n        reader.stop()\n\n    def test_return_index(self):\n        reader = TensorStreamConverter(self.path)\n        reader.initialize()\n        reader.start()\n        time.sleep(1.0)\n        tensor, index = reader.read(return_index=True)\n        self.assertTrue(index > 0 and index < 100)\n        reader.stop()\n\n    def test_normalization(self):\n        reader = TensorStreamConverter(self.path)\n        reader.initialize()\n        reader.start()\n        time.sleep(1.0)\n        tensor = reader.read(normalization=True)\n        value = tensor[0][0][0].item()\n        self.assertEqual(type(value), float)\n        reader.stop()\n\n    def test_read_without_init(self):\n        reader = TensorStreamConverter(self.path)\n        reader.start()\n        time.sleep(1.0)\n        with self.assertRaises(RuntimeError):\n            tensor, index = reader.read(return_index=True)\n\n        reader.stop()\n\n    def test_check_dump_size(self):\n        reader = TensorStreamConverter(self.path)\n        reader.initialize()\n        reader.start()\n        time.sleep(1.0)\n        expected_width = 1920\n        expected_height = 1080\n        expected_channels = 3\n        tensor, index = reader.read(return_index=True)\n        self.assertEqual(tensor.shape[0], expected_height)\n        self.assertEqual(tensor.shape[1], expected_width)\n        self.assertEqual(tensor.shape[2], expected_channels)\n        # need to find dumped file and compare expected and real sizes\n        reader.dump(tensor)\n\n        dump_size = os.stat(\'default.yuv\')\n        os.remove(""default.yuv"")\n        self.assertEqual(dump_size.st_size, expected_width * expected_height * expected_channels)\n        reader.stop()\n\n    def test_read_without_init_start(self):\n        reader = TensorStreamConverter(self.path)\n        time.sleep(1.0)\n        with self.assertRaises(RuntimeError):\n            tensor, index = reader.read(return_index=True)\n\n        reader.stop()\n\n    def test_dump_name(self):\n        reader = TensorStreamConverter(self.path)\n        reader.initialize()\n        reader.start()\n        time.sleep(1.0)\n        tensor = reader.read()\n        # need to find dumped file and compare expected and real sizes\n        reader.dump(tensor, name=""dump"")\n        self.assertTrue(os.path.isfile(""dump.yuv""))\n        os.remove(""dump.yuv"")\n        reader.stop()\n\n    def test_multiple_init(self):\n        reader = TensorStreamConverter(self.path)\n        number_close_init = 10\n        while number_close_init > 0:\n            reader.initialize()\n            reader.stop()\n            number_close_init -= 1\n\n    def test_read_after_stop(self):\n        reader = TensorStreamConverter(self.path)\n        reader.initialize()\n        reader.start()\n        time.sleep(1.0)\n        reader.stop()\n        with self.assertRaises(RuntimeError):\n            tensor = reader.read()\n\n    def test_frame_number(self):\n        reader = TensorStreamConverter(self.path)\n        reader.initialize()\n        reader.start()\n        time.sleep(1.0)\n        frame_num = i = 10\n        while i > 0:\n            tensor = reader.read()\n            reader.dump(tensor)\n            i -= 1\n\n        dump_size = os.stat(\'default.yuv\')\n        print(f""SIZE {dump_size}"")\n        os.remove(""default.yuv"")\n        expected_width = 1920\n        expected_height = 1080\n        expected_channels = 3\n        expected_size = expected_width * expected_height * expected_channels * frame_num\n        self.assertEqual(dump_size.st_size,\n                         expected_size)\n        reader.stop()\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
