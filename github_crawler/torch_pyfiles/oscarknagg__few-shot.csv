file_path,api_count,code
config.py,0,"b""import os\n\n\nPATH = os.path.dirname(os.path.realpath(__file__))\n\nDATA_PATH = None\n\nEPSILON = 1e-8\n\nif DATA_PATH is None:\n    raise Exception('Configure your data folder location in config.py before continuing!')\n"""
scratch.py,0,b''
experiments/maml.py,6,"b'""""""\nReproduce Model-agnostic Meta-learning results (supervised only) of Finn et al\n""""""\nfrom torch.utils.data import DataLoader\nfrom torch import nn\nimport argparse\n\nfrom few_shot.datasets import OmniglotDataset, MiniImageNet\nfrom few_shot.core import NShotTaskSampler, create_nshot_task_label, EvaluateFewShot\nfrom few_shot.maml import meta_gradient_step\nfrom few_shot.models import FewShotClassifier\nfrom few_shot.train import fit\nfrom few_shot.callbacks import *\nfrom few_shot.utils import setup_dirs\nfrom config import PATH\n\n\nsetup_dirs()\nassert torch.cuda.is_available()\ndevice = torch.device(\'cuda\')\ntorch.backends.cudnn.benchmark = True\n\n\n##############\n# Parameters #\n##############\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--dataset\')\nparser.add_argument(\'--n\', default=1, type=int)\nparser.add_argument(\'--k\', default=5, type=int)\nparser.add_argument(\'--q\', default=1, type=int)  # Number of examples per class to calculate meta gradients with\nparser.add_argument(\'--inner-train-steps\', default=1, type=int)\nparser.add_argument(\'--inner-val-steps\', default=3, type=int)\nparser.add_argument(\'--inner-lr\', default=0.4, type=float)\nparser.add_argument(\'--meta-lr\', default=0.001, type=float)\nparser.add_argument(\'--meta-batch-size\', default=32, type=int)\nparser.add_argument(\'--order\', default=1, type=int)\nparser.add_argument(\'--epochs\', default=50, type=int)\nparser.add_argument(\'--epoch-len\', default=100, type=int)\nparser.add_argument(\'--eval-batches\', default=20, type=int)\n\nargs = parser.parse_args()\n\nif args.dataset == \'omniglot\':\n    dataset_class = OmniglotDataset\n    fc_layer_size = 64\n    num_input_channels = 1\nelif args.dataset == \'miniImageNet\':\n    dataset_class = MiniImageNet\n    fc_layer_size = 1600\n    num_input_channels = 3\nelse:\n    raise(ValueError(\'Unsupported dataset\'))\n\nparam_str = f\'{args.dataset}_order={args.order}_n={args.n}_k={args.k}_metabatch={args.meta_batch_size}_\' \\\n            f\'train_steps={args.inner_train_steps}_val_steps={args.inner_val_steps}\'\nprint(param_str)\n\n\n###################\n# Create datasets #\n###################\nbackground = dataset_class(\'background\')\nbackground_taskloader = DataLoader(\n    background,\n    batch_sampler=NShotTaskSampler(background, args.epoch_len, n=args.n, k=args.k, q=args.q,\n                                   num_tasks=args.meta_batch_size),\n    num_workers=8\n)\nevaluation = dataset_class(\'evaluation\')\nevaluation_taskloader = DataLoader(\n    evaluation,\n    batch_sampler=NShotTaskSampler(evaluation, args.eval_batches, n=args.n, k=args.k, q=args.q,\n                                   num_tasks=args.meta_batch_size),\n    num_workers=8\n)\n\n\n############\n# Training #\n############\nprint(f\'Training MAML on {args.dataset}...\')\nmeta_model = FewShotClassifier(num_input_channels, args.k, fc_layer_size).to(device, dtype=torch.double)\nmeta_optimiser = torch.optim.Adam(meta_model.parameters(), lr=args.meta_lr)\nloss_fn = nn.CrossEntropyLoss().to(device)\n\n\ndef prepare_meta_batch(n, k, q, meta_batch_size):\n    def prepare_meta_batch_(batch):\n        x, y = batch\n        # Reshape to `meta_batch_size` number of tasks. Each task contains\n        # n*k support samples to train the fast model on and q*k query samples to\n        # evaluate the fast model on and generate meta-gradients\n        x = x.reshape(meta_batch_size, n*k + q*k, num_input_channels, x.shape[-2], x.shape[-1])\n        # Move to device\n        x = x.double().to(device)\n        # Create label\n        y = create_nshot_task_label(k, q).cuda().repeat(meta_batch_size)\n        return x, y\n\n    return prepare_meta_batch_\n\n\ncallbacks = [\n    EvaluateFewShot(\n        eval_fn=meta_gradient_step,\n        num_tasks=args.eval_batches,\n        n_shot=args.n,\n        k_way=args.k,\n        q_queries=args.q,\n        taskloader=evaluation_taskloader,\n        prepare_batch=prepare_meta_batch(args.n, args.k, args.q, args.meta_batch_size),\n        # MAML kwargs\n        inner_train_steps=args.inner_val_steps,\n        inner_lr=args.inner_lr,\n        device=device,\n        order=args.order,\n    ),\n    ModelCheckpoint(\n        filepath=PATH + f\'/models/maml/{param_str}.pth\',\n        monitor=f\'val_{args.n}-shot_{args.k}-way_acc\'\n    ),\n    ReduceLROnPlateau(patience=10, factor=0.5, monitor=f\'val_loss\'),\n    CSVLogger(PATH + f\'/logs/maml/{param_str}.csv\'),\n]\n\n\nfit(\n    meta_model,\n    meta_optimiser,\n    loss_fn,\n    epochs=args.epochs,\n    dataloader=background_taskloader,\n    prepare_batch=prepare_meta_batch(args.n, args.k, args.q, args.meta_batch_size),\n    callbacks=callbacks,\n    metrics=[\'categorical_accuracy\'],\n    fit_function=meta_gradient_step,\n    fit_function_kwargs={\'n_shot\': args.n, \'k_way\': args.k, \'q_queries\': args.q,\n                         \'train\': True,\n                         \'order\': args.order, \'device\': device, \'inner_train_steps\': args.inner_train_steps,\n                         \'inner_lr\': args.inner_lr},\n)\n'"
experiments/matching_nets.py,7,"b'""""""\nReproduce Matching Network results of Vinyals et al\n""""""\nimport argparse\nfrom torch.utils.data import DataLoader\nfrom torch.optim import Adam\n\nfrom few_shot.datasets import OmniglotDataset, MiniImageNet\nfrom few_shot.core import NShotTaskSampler, prepare_nshot_task, EvaluateFewShot\nfrom few_shot.matching import matching_net_episode\nfrom few_shot.train import fit\nfrom few_shot.callbacks import *\nfrom few_shot.utils import setup_dirs\nfrom config import PATH\n\n\nsetup_dirs()\nassert torch.cuda.is_available()\ndevice = torch.device(\'cuda\')\ntorch.backends.cudnn.benchmark = True\n\n\n##############\n# Parameters #\n##############\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--dataset\')\nparser.add_argument(\'--fce\', type=lambda x: x.lower()[0] == \'t\')  # Quick hack to extract boolean\nparser.add_argument(\'--distance\', default=\'cosine\')\nparser.add_argument(\'--n-train\', default=1, type=int)\nparser.add_argument(\'--n-test\', default=1, type=int)\nparser.add_argument(\'--k-train\', default=5, type=int)\nparser.add_argument(\'--k-test\', default=5, type=int)\nparser.add_argument(\'--q-train\', default=15, type=int)\nparser.add_argument(\'--q-test\', default=1, type=int)\nparser.add_argument(\'--lstm-layers\', default=1, type=int)\nparser.add_argument(\'--unrolling-steps\', default=2, type=int)\nargs = parser.parse_args()\n\nevaluation_episodes = 1000\nepisodes_per_epoch = 100\n\nif args.dataset == \'omniglot\':\n    n_epochs = 100\n    dataset_class = OmniglotDataset\n    num_input_channels = 1\n    lstm_input_size = 64\nelif args.dataset == \'miniImageNet\':\n    n_epochs = 200\n    dataset_class = MiniImageNet\n    num_input_channels = 3\n    lstm_input_size = 1600\nelse:\n    raise(ValueError, \'Unsupported dataset\')\n\nparam_str = f\'{args.dataset}_n={args.n_train}_k={args.k_train}_q={args.q_train}_\' \\\n            f\'nv={args.n_test}_kv={args.k_test}_qv={args.q_test}_\'\\\n            f\'dist={args.distance}_fce={args.fce}\'\n\n\n#########\n# Model #\n#########\nfrom few_shot.models import MatchingNetwork\nmodel = MatchingNetwork(args.n_train, args.k_train, args.q_train, args.fce, num_input_channels,\n                        lstm_layers=args.lstm_layers,\n                        lstm_input_size=lstm_input_size,\n                        unrolling_steps=args.unrolling_steps,\n                        device=device)\nmodel.to(device, dtype=torch.double)\n\n\n###################\n# Create datasets #\n###################\nbackground = dataset_class(\'background\')\nbackground_taskloader = DataLoader(\n    background,\n    batch_sampler=NShotTaskSampler(background, episodes_per_epoch, args.n_train, args.k_train, args.q_train),\n    num_workers=4\n)\nevaluation = dataset_class(\'evaluation\')\nevaluation_taskloader = DataLoader(\n    evaluation,\n    batch_sampler=NShotTaskSampler(evaluation, episodes_per_epoch, args.n_test, args.k_test, args.q_test),\n    num_workers=4\n)\n\n\n############\n# Training #\n############\nprint(f\'Training Matching Network on {args.dataset}...\')\noptimiser = Adam(model.parameters(), lr=1e-3)\nloss_fn = torch.nn.NLLLoss().cuda()\n\n\ncallbacks = [\n    EvaluateFewShot(\n        eval_fn=matching_net_episode,\n        num_tasks=evaluation_episodes,\n        n_shot=args.n_test,\n        k_way=args.k_test,\n        q_queries=args.q_test,\n        taskloader=evaluation_taskloader,\n        prepare_batch=prepare_nshot_task(args.n_test, args.k_test, args.q_test),\n        fce=args.fce,\n        distance=args.distance\n    ),\n    ModelCheckpoint(\n        filepath=PATH + f\'/models/matching_nets/{param_str}.pth\',\n        monitor=f\'val_{args.n_test}-shot_{args.k_test}-way_acc\',\n        # monitor=f\'val_loss\',\n    ),\n    ReduceLROnPlateau(patience=20, factor=0.5, monitor=f\'val_{args.n_test}-shot_{args.k_test}-way_acc\'),\n    CSVLogger(PATH + f\'/logs/matching_nets/{param_str}.csv\'),\n]\n\nfit(\n    model,\n    optimiser,\n    loss_fn,\n    epochs=n_epochs,\n    dataloader=background_taskloader,\n    prepare_batch=prepare_nshot_task(args.n_train, args.k_train, args.q_train),\n    callbacks=callbacks,\n    metrics=[\'categorical_accuracy\'],\n    fit_function=matching_net_episode,\n    fit_function_kwargs={\'n_shot\': args.n_train, \'k_way\': args.k_train, \'q_queries\': args.q_train, \'train\': True,\n                         \'fce\': args.fce, \'distance\': args.distance}\n)\n'"
experiments/proto_nets.py,7,"b'""""""\nReproduce Omniglot results of Snell et al Prototypical networks.\n""""""\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\nimport argparse\n\nfrom few_shot.datasets import OmniglotDataset, MiniImageNet\nfrom few_shot.models import get_few_shot_encoder\nfrom few_shot.core import NShotTaskSampler, EvaluateFewShot, prepare_nshot_task\nfrom few_shot.proto import proto_net_episode\nfrom few_shot.train import fit\nfrom few_shot.callbacks import *\nfrom few_shot.utils import setup_dirs\nfrom config import PATH\n\n\nsetup_dirs()\nassert torch.cuda.is_available()\ndevice = torch.device(\'cuda\')\ntorch.backends.cudnn.benchmark = True\n\n\n##############\n# Parameters #\n##############\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--dataset\')\nparser.add_argument(\'--distance\', default=\'l2\')\nparser.add_argument(\'--n-train\', default=1, type=int)\nparser.add_argument(\'--n-test\', default=1, type=int)\nparser.add_argument(\'--k-train\', default=60, type=int)\nparser.add_argument(\'--k-test\', default=5, type=int)\nparser.add_argument(\'--q-train\', default=5, type=int)\nparser.add_argument(\'--q-test\', default=1, type=int)\nargs = parser.parse_args()\n\nevaluation_episodes = 1000\nepisodes_per_epoch = 100\n\nif args.dataset == \'omniglot\':\n    n_epochs = 40\n    dataset_class = OmniglotDataset\n    num_input_channels = 1\n    drop_lr_every = 20\nelif args.dataset == \'miniImageNet\':\n    n_epochs = 80\n    dataset_class = MiniImageNet\n    num_input_channels = 3\n    drop_lr_every = 40\nelse:\n    raise(ValueError, \'Unsupported dataset\')\n\nparam_str = f\'{args.dataset}_nt={args.n_train}_kt={args.k_train}_qt={args.q_train}_\' \\\n            f\'nv={args.n_test}_kv={args.k_test}_qv={args.q_test}\'\n\nprint(param_str)\n\n###################\n# Create datasets #\n###################\nbackground = dataset_class(\'background\')\nbackground_taskloader = DataLoader(\n    background,\n    batch_sampler=NShotTaskSampler(background, episodes_per_epoch, args.n_train, args.k_train, args.q_train),\n    num_workers=4\n)\nevaluation = dataset_class(\'evaluation\')\nevaluation_taskloader = DataLoader(\n    evaluation,\n    batch_sampler=NShotTaskSampler(evaluation, episodes_per_epoch, args.n_test, args.k_test, args.q_test),\n    num_workers=4\n)\n\n\n#########\n# Model #\n#########\nmodel = get_few_shot_encoder(num_input_channels)\nmodel.to(device, dtype=torch.double)\n\n\n############\n# Training #\n############\nprint(f\'Training Prototypical network on {args.dataset}...\')\noptimiser = Adam(model.parameters(), lr=1e-3)\nloss_fn = torch.nn.NLLLoss().cuda()\n\n\ndef lr_schedule(epoch, lr):\n    # Drop lr every 2000 episodes\n    if epoch % drop_lr_every == 0:\n        return lr / 2\n    else:\n        return lr\n\n\ncallbacks = [\n    EvaluateFewShot(\n        eval_fn=proto_net_episode,\n        num_tasks=evaluation_episodes,\n        n_shot=args.n_test,\n        k_way=args.k_test,\n        q_queries=args.q_test,\n        taskloader=evaluation_taskloader,\n        prepare_batch=prepare_nshot_task(args.n_test, args.k_test, args.q_test),\n        distance=args.distance\n    ),\n    ModelCheckpoint(\n        filepath=PATH + f\'/models/proto_nets/{param_str}.pth\',\n        monitor=f\'val_{args.n_test}-shot_{args.k_test}-way_acc\'\n    ),\n    LearningRateScheduler(schedule=lr_schedule),\n    CSVLogger(PATH + f\'/logs/proto_nets/{param_str}.csv\'),\n]\n\nfit(\n    model,\n    optimiser,\n    loss_fn,\n    epochs=n_epochs,\n    dataloader=background_taskloader,\n    prepare_batch=prepare_nshot_task(args.n_train, args.k_train, args.q_train),\n    callbacks=callbacks,\n    metrics=[\'categorical_accuracy\'],\n    fit_function=proto_net_episode,\n    fit_function_kwargs={\'n_shot\': args.n_train, \'k_way\': args.k_train, \'q_queries\': args.q_train, \'train\': True,\n                         \'distance\': args.distance},\n)\n'"
few_shot/__init__.py,0,b''
few_shot/callbacks.py,3,"b'""""""\nPorts of Callback classes from the Keras library.\n""""""\nfrom tqdm import tqdm\nimport numpy as np\nimport torch\nfrom collections import OrderedDict, Iterable\nimport warnings\nimport os\nimport csv\nimport io\n\nfrom few_shot.eval import evaluate\n\n\nclass CallbackList(object):\n    """"""Container abstracting a list of callbacks.\n\n    # Arguments\n        callbacks: List of `Callback` instances.\n    """"""\n    def __init__(self, callbacks):\n        self.callbacks = [c for c in callbacks]\n\n    def set_params(self, params):\n        for callback in self.callbacks:\n            callback.set_params(params)\n\n    def set_model(self, model):\n        for callback in self.callbacks:\n            callback.set_model(model)\n\n    def on_epoch_begin(self, epoch, logs=None):\n        """"""Called at the start of an epoch.\n        # Arguments\n            epoch: integer, index of epoch.\n            logs: dictionary of logs.\n        """"""\n        logs = logs or {}\n        for callback in self.callbacks:\n            callback.on_epoch_begin(epoch, logs)\n\n    def on_epoch_end(self, epoch, logs=None):\n        """"""Called at the end of an epoch.\n        # Arguments\n            epoch: integer, index of epoch.\n            logs: dictionary of logs.\n        """"""\n        logs = logs or {}\n        for callback in self.callbacks:\n            callback.on_epoch_end(epoch, logs)\n\n    def on_batch_begin(self, batch, logs=None):\n        """"""Called right before processing a batch.\n        # Arguments\n            batch: integer, index of batch within the current epoch.\n            logs: dictionary of logs.\n        """"""\n        logs = logs or {}\n        for callback in self.callbacks:\n            callback.on_batch_begin(batch, logs)\n\n    def on_batch_end(self, batch, logs=None):\n        """"""Called at the end of a batch.\n        # Arguments\n            batch: integer, index of batch within the current epoch.\n            logs: dictionary of logs.\n        """"""\n        logs = logs or {}\n        for callback in self.callbacks:\n            callback.on_batch_end(batch, logs)\n\n    def on_train_begin(self, logs=None):\n        """"""Called at the beginning of training.\n        # Arguments\n            logs: dictionary of logs.\n        """"""\n        logs = logs or {}\n        for callback in self.callbacks:\n            callback.on_train_begin(logs)\n\n    def on_train_end(self, logs=None):\n        """"""Called at the end of training.\n        # Arguments\n            logs: dictionary of logs.\n        """"""\n        logs = logs or {}\n        for callback in self.callbacks:\n            callback.on_train_end(logs)\n\n\nclass Callback(object):\n    def __init__(self):\n        self.model = None\n\n    def set_params(self, params):\n        self.params = params\n\n    def set_model(self, model):\n        self.model = model\n\n    def on_epoch_begin(self, epoch, logs=None):\n        pass\n\n    def on_epoch_end(self, epoch, logs=None):\n        pass\n\n    def on_batch_begin(self, batch, logs=None):\n        pass\n\n    def on_batch_end(self, batch, logs=None):\n        pass\n\n    def on_train_begin(self, logs=None):\n        pass\n\n    def on_train_end(self, logs=None):\n        pass\n\n\nclass DefaultCallback(Callback):\n    """"""Records metrics over epochs by averaging over each batch.\n\n    NB The metrics are calculated with a moving model\n    """"""\n    def on_epoch_begin(self, batch, logs=None):\n        self.seen = 0\n        self.totals = {}\n        self.metrics = [\'loss\'] + self.params[\'metrics\']\n\n    def on_batch_end(self, batch, logs=None):\n        logs = logs or {}\n        batch_size = logs.get(\'size\', 1) or 1\n        self.seen += batch_size\n\n        for k, v in logs.items():\n            if k in self.totals:\n                self.totals[k] += v * batch_size\n            else:\n                self.totals[k] = v * batch_size\n\n    def on_epoch_end(self, epoch, logs=None):\n        if logs is not None:\n            for k in self.metrics:\n                if k in self.totals:\n                    # Make value available to next callbacks.\n                    logs[k] = self.totals[k] / self.seen\n\n\nclass ProgressBarLogger(Callback):\n    """"""TQDM progress bar that displays the running average of loss and other metrics.""""""\n    def __init__(self):\n        super(ProgressBarLogger, self).__init__()\n\n    def on_train_begin(self, logs=None):\n        self.num_batches = self.params[\'num_batches\']\n        self.verbose = self.params[\'verbose\']\n        self.metrics = [\'loss\'] + self.params[\'metrics\']\n\n    def on_epoch_begin(self, epoch, logs=None):\n        self.target = self.num_batches\n        self.pbar = tqdm(total=self.target, desc=\'Epoch {}\'.format(epoch))\n        self.seen = 0\n\n    def on_batch_begin(self, batch, logs=None):\n        self.log_values = {}\n\n    def on_batch_end(self, batch, logs=None):\n        logs = logs or {}\n        self.seen += 1\n\n        for k in self.metrics:\n            if k in logs:\n                self.log_values[k] = logs[k]\n\n        # Skip progbar update for the last batch;\n        # will be handled by on_epoch_end.\n        if self.verbose and self.seen < self.target:\n            self.pbar.update(1)\n            self.pbar.set_postfix(self.log_values)\n\n    def on_epoch_end(self, epoch, logs=None):\n        # Update log values\n        self.log_values = {}\n        for k in self.metrics:\n            if k in logs:\n                self.log_values[k] = logs[k]\n\n        if self.verbose:\n            self.pbar.update(1)\n            self.pbar.set_postfix(self.log_values)\n\n        self.pbar.close()\n\n\nclass CSVLogger(Callback):\n    """"""Callback that streams epoch results to a csv file.\n    Supports all values that can be represented as a string,\n    including 1D iterables such as np.ndarray.\n\n    # Arguments\n        filename: filename of the csv file, e.g. \'run/log.csv\'.\n        separator: string used to separate elements in the csv file.\n        append: True: append if file exists (useful for continuing\n            training). False: overwrite existing file,\n    """"""\n\n    def __init__(self, filename, separator=\',\', append=False):\n        self.sep = separator\n        self.filename = filename\n        self.append = append\n        self.writer = None\n        self.keys = None\n        self.append_header = True\n        self.file_flags = \'\'\n        self._open_args = {\'newline\': \'\\n\'}\n        super(CSVLogger, self).__init__()\n\n    def on_train_begin(self, logs=None):\n        if self.append:\n            if os.path.exists(self.filename):\n                with open(self.filename, \'r\' + self.file_flags) as f:\n                    self.append_header = not bool(len(f.readline()))\n            mode = \'a\'\n        else:\n            mode = \'w\'\n\n        self.csv_file = io.open(self.filename,\n                                mode + self.file_flags,\n                                **self._open_args)\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n\n        def handle_value(k):\n            is_zero_dim_ndarray = isinstance(k, np.ndarray) and k.ndim == 0\n            if isinstance(k, str):\n                return k\n            elif isinstance(k, Iterable) and not is_zero_dim_ndarray:\n                return \'""[%s]""\' % (\', \'.join(map(str, k)))\n            else:\n                return k\n\n        if self.keys is None:\n            self.keys = sorted(logs.keys())\n\n        if not self.writer:\n            class CustomDialect(csv.excel):\n                delimiter = self.sep\n            fieldnames = [\'epoch\'] + self.keys\n            self.writer = csv.DictWriter(self.csv_file,\n                                         fieldnames=fieldnames,\n                                         dialect=CustomDialect)\n            if self.append_header:\n                self.writer.writeheader()\n\n        row_dict = OrderedDict({\'epoch\': epoch})\n        row_dict.update((key, handle_value(logs[key])) for key in self.keys)\n        self.writer.writerow(row_dict)\n        self.csv_file.flush()\n\n    def on_train_end(self, logs=None):\n        self.csv_file.close()\n        self.writer = None\n\n\nclass EvaluateMetrics(Callback):\n    """"""Evaluates metrics on a dataset after every epoch.\n\n    # Argments\n        dataloader: torch.DataLoader of the dataset on which the model will be evaluated\n        prefix: Prefix to prepend to the names of the metrics when they is logged. Defaults to \'val_\' but can be changed\n        if the model is to be evaluated on many datasets separately.\n        suffix: Suffix to append to the names of the metrics when they is logged.\n    """"""\n    def __init__(self, dataloader, prefix=\'val_\', suffix=\'\'):\n        super(EvaluateMetrics, self).__init__()\n        self.dataloader = dataloader\n        self.prefix = prefix\n        self.suffix = suffix\n\n    def on_train_begin(self, logs=None):\n        self.metrics = self.params[\'metrics\']\n        self.prepare_batch = self.params[\'prepare_batch\']\n        self.loss_fn = self.params[\'loss_fn\']\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        logs.update(\n            evaluate(self.model, self.dataloader, self.prepare_batch, self.metrics, self.loss_fn, self.prefix, self.suffix)\n        )\n\n\nclass ReduceLROnPlateau(Callback):\n    """"""Reduce learning rate when a metric has stopped improving.\n\n    Models often benefit from reducing the learning rate by a factor\n    of 2-10 once learning stagnates. This callback monitors a\n    quantity and if no improvement is seen for a \'patience\' number\n    of epochs, the learning rate is reduced.\n\n    # Arguments\n        monitor: quantity to be monitored.\n        factor: factor by which the learning rate will\n            be reduced. new_lr = lr * factor\n        patience: number of epochs with no improvement\n            after which learning rate will be reduced.\n        verbose: int. 0: quiet, 1: update messages.\n        mode: one of {auto, min, max}. In `min` mode,\n            lr will be reduced when the quantity\n            monitored has stopped decreasing; in `max`\n            mode it will be reduced when the quantity\n            monitored has stopped increasing; in `auto`\n            mode, the direction is automatically inferred\n            from the name of the monitored quantity.\n        min_delta: threshold for measuring the new optimum,\n            to only focus on significant changes.\n        cooldown: number of epochs to wait before resuming\n            normal operation after lr has been reduced.\n        min_lr: lower bound on the learning rate.\n    """"""\n\n    def __init__(self, monitor=\'val_loss\', factor=0.1, patience=10,\n                 verbose=0, mode=\'auto\', min_delta=1e-4, cooldown=0, min_lr=0,\n                 **kwargs):\n        super(ReduceLROnPlateau, self).__init__()\n\n        self.monitor = monitor\n        if factor >= 1.0:\n            raise ValueError(\'ReduceLROnPlateau does not support a factor >= 1.0.\')\n        self.factor = factor\n        self.min_lr = min_lr\n        self.min_delta = min_delta\n        self.patience = patience\n        self.verbose = verbose\n        self.cooldown = cooldown\n        self.cooldown_counter = 0  # Cooldown counter.\n        self.wait = 0\n        self.best = 0\n        if mode not in [\'auto\', \'min\', \'max\']:\n            raise ValueError(\'Mode must be one of (auto, min, max).\')\n        self.mode = mode\n        self.monitor_op = None\n\n        self._reset()\n\n    def _reset(self):\n        """"""Resets wait counter and cooldown counter.\n        """"""\n        if (self.mode == \'min\' or\n                (self.mode == \'auto\' and \'acc\' not in self.monitor)):\n            self.monitor_op = lambda a, b: np.less(a, b - self.min_delta)\n            self.best = np.Inf\n        else:\n            self.monitor_op = lambda a, b: np.greater(a, b + self.min_delta)\n            self.best = -np.Inf\n        self.cooldown_counter = 0\n        self.wait = 0\n\n    def on_train_begin(self, logs=None):\n        self.optimiser = self.params[\'optimiser\']\n        self.min_lrs = [self.min_lr] * len(self.optimiser.param_groups)\n        self._reset()\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        if len(self.optimiser.param_groups) == 1:\n            logs[\'lr\'] = self.optimiser.param_groups[0][\'lr\']\n        else:\n            for i, param_group in enumerate(self.optimiser.param_groups):\n                logs[\'lr_{}\'.format(i)] = param_group[\'lr\']\n\n        current = logs.get(self.monitor)\n\n        if self.in_cooldown():\n            self.cooldown_counter -= 1\n            self.wait = 0\n\n        if self.monitor_op(current, self.best):\n            self.best = current\n            self.wait = 0\n        elif not self.in_cooldown():\n            self.wait += 1\n            if self.wait >= self.patience:\n                self._reduce_lr(epoch)\n                self.cooldown_counter = self.cooldown\n                self.wait = 0\n\n    def _reduce_lr(self, epoch):\n        for i, param_group in enumerate(self.optimiser.param_groups):\n            old_lr = float(param_group[\'lr\'])\n            new_lr = max(old_lr * self.factor, self.min_lrs[i])\n            if old_lr - new_lr > self.min_delta:\n                param_group[\'lr\'] = new_lr\n                if self.verbose:\n                    print(\'Epoch {:5d}: reducing learning rate\'\n                          \' of group {} to {:.4e}.\'.format(epoch, i, new_lr))\n\n    def in_cooldown(self):\n        return self.cooldown_counter > 0\n\n\nclass ModelCheckpoint(Callback):\n    """"""Save the model after every epoch.\n\n    `filepath` can contain named formatting options, which will be filled the value of `epoch` and keys in `logs`\n    (passed in `on_epoch_end`).\n\n    For example: if `filepath` is `weights.{epoch:02d}-{val_loss:.2f}.hdf5`, then the model checkpoints will be saved\n    with the epoch number and the validation loss in the filename.\n\n    # Arguments\n        filepath: string, path to save the model file.\n        monitor: quantity to monitor.\n        verbose: verbosity mode, 0 or 1.\n        save_best_only: if `save_best_only=True`,\n            the latest best model according to\n            the quantity monitored will not be overwritten.\n        mode: one of {auto, min, max}.\n            If `save_best_only=True`, the decision\n            to overwrite the current save file is made\n            based on either the maximization or the\n            minimization of the monitored quantity. For `val_acc`,\n            this should be `max`, for `val_loss` this should\n            be `min`, etc. In `auto` mode, the direction is\n            automatically inferred from the name of the monitored quantity.\n        save_weights_only: if True, then only the model\'s weights will be\n            saved (`model.save_weights(filepath)`), else the full model\n            is saved (`model.save(filepath)`).\n        period: Interval (number of epochs) between checkpoints.\n    """"""\n\n    def __init__(self, filepath, monitor=\'val_loss\', verbose=0, save_best_only=False, mode=\'auto\', period=1):\n        super(ModelCheckpoint, self).__init__()\n        self.monitor = monitor\n        self.verbose = verbose\n        self.filepath = filepath\n        self.save_best_only = save_best_only\n        self.period = period\n        self.epochs_since_last_save = 0\n\n        if mode not in [\'auto\', \'min\', \'max\']:\n            raise ValueError(\'Mode must be one of (auto, min, max).\')\n\n        if mode == \'min\':\n            self.monitor_op = np.less\n            self.best = np.Inf\n        elif mode == \'max\':\n            self.monitor_op = np.greater\n            self.best = -np.Inf\n        else:\n            if \'acc\' in self.monitor or self.monitor.startswith(\'fmeasure\'):\n                self.monitor_op = np.greater\n                self.best = -np.Inf\n            else:\n                self.monitor_op = np.less\n\n        self.best = np.Inf\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        self.epochs_since_last_save += 1\n        if self.epochs_since_last_save >= self.period:\n            self.epochs_since_last_save = 0\n            filepath = self.filepath.format(epoch=epoch + 1, **logs)\n            if self.save_best_only:\n                current = logs.get(self.monitor)\n                if current is None:\n                    warnings.warn(\'Can save best model only with %s available, \'\n                                  \'skipping.\' % (self.monitor), RuntimeWarning)\n                else:\n                    if self.monitor_op(current, self.best):\n                        if self.verbose > 0:\n                            print(\'\\nEpoch %05d: %s improved from %0.5f to %0.5f,\'\n                                  \' saving model to %s\'\n                                  % (epoch + 1, self.monitor, self.best,\n                                     current, filepath))\n                        self.best = current\n                        torch.save(self.model.state_dict(), filepath)\n                    else:\n                        if self.verbose > 0:\n                            print(\'\\nEpoch %05d: %s did not improve from %0.5f\' %\n                                  (epoch + 1, self.monitor, self.best))\n            else:\n                if self.verbose > 0:\n                    print(\'\\nEpoch %05d: saving model to %s\' % (epoch + 1, filepath))\n                torch.save(self.model.state_dict(), filepath)\n\n\nclass LearningRateScheduler(Callback):\n    """"""Learning rate scheduler.\n    # Arguments\n        schedule: a function that takes an epoch index as input\n            (integer, indexed from 0) and current learning rate\n            and returns a new learning rate as output (float).\n        verbose: int. 0: quiet, 1: update messages.\n    """"""\n\n    def __init__(self, schedule, verbose=0):\n        super(LearningRateScheduler, self).__init__()\n        self.schedule = schedule\n        self.verbose = verbose\n\n    def on_train_begin(self, logs=None):\n        self.optimiser = self.params[\'optimiser\']\n\n    def on_epoch_begin(self, epoch, logs=None):\n        lrs = [self.schedule(epoch, param_group[\'lr\']) for param_group in self.optimiser.param_groups]\n\n        if not all(isinstance(lr, (float, np.float32, np.float64)) for lr in lrs):\n            raise ValueError(\'The output of the ""schedule"" function \'\n                             \'should be float.\')\n        self.set_lr(epoch, lrs)\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        if len(self.optimiser.param_groups) == 1:\n            logs[\'lr\'] = self.optimiser.param_groups[0][\'lr\']\n        else:\n            for i, param_group in enumerate(self.optimiser.param_groups):\n                logs[\'lr_{}\'.format(i)] = param_group[\'lr\']\n\n    def set_lr(self, epoch, lrs):\n        for i, param_group in enumerate(self.optimiser.param_groups):\n            new_lr = lrs[i]\n            param_group[\'lr\'] = new_lr\n            if self.verbose:\n                print(\'Epoch {:5d}: setting learning rate\'\n                      \' of group {} to {:.4e}.\'.format(epoch, i, new_lr))\n'"
few_shot/core.py,7,"b'from torch.utils.data import Sampler\nfrom typing import List, Iterable, Callable, Tuple\nimport numpy as np\nimport torch\n\nfrom few_shot.metrics import categorical_accuracy\nfrom few_shot.callbacks import Callback\n\n\nclass NShotTaskSampler(Sampler):\n    def __init__(self,\n                 dataset: torch.utils.data.Dataset,\n                 episodes_per_epoch: int = None,\n                 n: int = None,\n                 k: int = None,\n                 q: int = None,\n                 num_tasks: int = 1,\n                 fixed_tasks: List[Iterable[int]] = None):\n        """"""PyTorch Sampler subclass that generates batches of n-shot, k-way, q-query tasks.\n\n        Each n-shot task contains a ""support set"" of `k` sets of `n` samples and a ""query set"" of `k` sets\n        of `q` samples. The support set and the query set are all grouped into one Tensor such that the first n * k\n        samples are from the support set while the remaining q * k samples are from the query set.\n\n        The support and query sets are sampled such that they are disjoint i.e. do not contain overlapping samples.\n\n        # Arguments\n            dataset: Instance of torch.utils.data.Dataset from which to draw samples\n            episodes_per_epoch: Arbitrary number of batches of n-shot tasks to generate in one epoch\n            n_shot: int. Number of samples for each class in the n-shot classification tasks.\n            k_way: int. Number of classes in the n-shot classification tasks.\n            q_queries: int. Number query samples for each class in the n-shot classification tasks.\n            num_tasks: Number of n-shot tasks to group into a single batch\n            fixed_tasks: If this argument is specified this Sampler will always generate tasks from\n                the specified classes\n        """"""\n        super(NShotTaskSampler, self).__init__(dataset)\n        self.episodes_per_epoch = episodes_per_epoch\n        self.dataset = dataset\n        if num_tasks < 1:\n            raise ValueError(\'num_tasks must be > 1.\')\n\n        self.num_tasks = num_tasks\n        # TODO: Raise errors if initialise badly\n        self.k = k\n        self.n = n\n        self.q = q\n        self.fixed_tasks = fixed_tasks\n\n        self.i_task = 0\n\n    def __len__(self):\n        return self.episodes_per_epoch\n\n    def __iter__(self):\n        for _ in range(self.episodes_per_epoch):\n            batch = []\n\n            for task in range(self.num_tasks):\n                if self.fixed_tasks is None:\n                    # Get random classes\n                    episode_classes = np.random.choice(self.dataset.df[\'class_id\'].unique(), size=self.k, replace=False)\n                else:\n                    # Loop through classes in fixed_tasks\n                    episode_classes = self.fixed_tasks[self.i_task % len(self.fixed_tasks)]\n                    self.i_task += 1\n\n                df = self.dataset.df[self.dataset.df[\'class_id\'].isin(episode_classes)]\n\n                support_k = {k: None for k in episode_classes}\n                for k in episode_classes:\n                    # Select support examples\n                    support = df[df[\'class_id\'] == k].sample(self.n)\n                    support_k[k] = support\n\n                    for i, s in support.iterrows():\n                        batch.append(s[\'id\'])\n\n                for k in episode_classes:\n                    query = df[(df[\'class_id\'] == k) & (~df[\'id\'].isin(support_k[k][\'id\']))].sample(self.q)\n                    for i, q in query.iterrows():\n                        batch.append(q[\'id\'])\n\n            yield np.stack(batch)\n\n\nclass EvaluateFewShot(Callback):\n    """"""Evaluate a network on  an n-shot, k-way classification tasks after every epoch.\n\n    # Arguments\n        eval_fn: Callable to perform few-shot classification. Examples include `proto_net_episode`,\n            `matching_net_episode` and `meta_gradient_step` (MAML).\n        num_tasks: int. Number of n-shot classification tasks to evaluate the model with.\n        n_shot: int. Number of samples for each class in the n-shot classification tasks.\n        k_way: int. Number of classes in the n-shot classification tasks.\n        q_queries: int. Number query samples for each class in the n-shot classification tasks.\n        task_loader: Instance of NShotWrapper class\n        prepare_batch: function. The preprocessing function to apply to samples from the dataset.\n        prefix: str. Prefix to identify dataset.\n    """"""\n\n    def __init__(self,\n                 eval_fn: Callable,\n                 num_tasks: int,\n                 n_shot: int,\n                 k_way: int,\n                 q_queries: int,\n                 taskloader: torch.utils.data.DataLoader,\n                 prepare_batch: Callable,\n                 prefix: str = \'val_\',\n                 **kwargs):\n        super(EvaluateFewShot, self).__init__()\n        self.eval_fn = eval_fn\n        self.num_tasks = num_tasks\n        self.n_shot = n_shot\n        self.k_way = k_way\n        self.q_queries = q_queries\n        self.taskloader = taskloader\n        self.prepare_batch = prepare_batch\n        self.prefix = prefix\n        self.kwargs = kwargs\n        self.metric_name = f\'{self.prefix}{self.n_shot}-shot_{self.k_way}-way_acc\'\n\n    def on_train_begin(self, logs=None):\n        self.loss_fn = self.params[\'loss_fn\']\n        self.optimiser = self.params[\'optimiser\']\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        seen = 0\n        totals = {\'loss\': 0, self.metric_name: 0}\n        for batch_index, batch in enumerate(self.taskloader):\n            x, y = self.prepare_batch(batch)\n\n            loss, y_pred = self.eval_fn(\n                self.model,\n                self.optimiser,\n                self.loss_fn,\n                x,\n                y,\n                n_shot=self.n_shot,\n                k_way=self.k_way,\n                q_queries=self.q_queries,\n                train=False,\n                **self.kwargs\n            )\n\n            seen += y_pred.shape[0]\n\n            totals[\'loss\'] += loss.item() * y_pred.shape[0]\n            totals[self.metric_name] += categorical_accuracy(y, y_pred) * y_pred.shape[0]\n\n        logs[self.prefix + \'loss\'] = totals[\'loss\'] / seen\n        logs[self.metric_name] = totals[self.metric_name] / seen\n\n\ndef prepare_nshot_task(n: int, k: int, q: int) -> Callable:\n    """"""Typical n-shot task preprocessing.\n\n    # Arguments\n        n: Number of samples for each class in the n-shot classification task\n        k: Number of classes in the n-shot classification task\n        q: Number of query samples for each class in the n-shot classification task\n\n    # Returns\n        prepare_nshot_task_: A Callable that processes a few shot tasks with specified n, k and q\n    """"""\n    def prepare_nshot_task_(batch: Tuple[torch.Tensor, torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:\n        """"""Create 0-k label and move to GPU.\n\n        TODO: Move to arbitrary device\n        """"""\n        x, y = batch\n        x = x.double().cuda()\n        # Create dummy 0-(num_classes - 1) label\n        y = create_nshot_task_label(k, q).cuda()\n        return x, y\n\n    return prepare_nshot_task_\n\n\ndef create_nshot_task_label(k: int, q: int) -> torch.Tensor:\n    """"""Creates an n-shot task label.\n\n    Label has the structure:\n        [0]*q + [1]*q + ... + [k-1]*q\n\n    # TODO: Test this\n\n    # Arguments\n        k: Number of classes in the n-shot classification task\n        q: Number of query samples for each class in the n-shot classification task\n\n    # Returns\n        y: Label vector for n-shot task of shape [q * k, ]\n    """"""\n    y = torch.arange(0, k, 1 / q).long()\n    return y\n'"
few_shot/datasets.py,2,"b'from torch.utils.data import Dataset\nimport torch\nfrom PIL import Image\nfrom torchvision import transforms\nfrom skimage import io\nfrom tqdm import tqdm\nimport pandas as pd\nimport numpy as np\nimport os\n\nfrom config import DATA_PATH\n\n\nclass OmniglotDataset(Dataset):\n    def __init__(self, subset):\n        """"""Dataset class representing Omniglot dataset\n\n        # Arguments:\n            subset: Whether the dataset represents the background or evaluation set\n        """"""\n        if subset not in (\'background\', \'evaluation\'):\n            raise(ValueError, \'subset must be one of (background, evaluation)\')\n        self.subset = subset\n\n        self.df = pd.DataFrame(self.index_subset(self.subset))\n\n        # Index of dataframe has direct correspondence to item in dataset\n        self.df = self.df.assign(id=self.df.index.values)\n\n        # Convert arbitrary class names of dataset to ordered 0-(num_speakers - 1) integers\n        self.unique_characters = sorted(self.df[\'class_name\'].unique())\n        self.class_name_to_id = {self.unique_characters[i]: i for i in range(self.num_classes())}\n        self.df = self.df.assign(class_id=self.df[\'class_name\'].apply(lambda c: self.class_name_to_id[c]))\n\n        # Create dicts\n        self.datasetid_to_filepath = self.df.to_dict()[\'filepath\']\n        self.datasetid_to_class_id = self.df.to_dict()[\'class_id\']\n\n    def __getitem__(self, item):\n        instance = io.imread(self.datasetid_to_filepath[item])\n        # Reindex to channels first format as supported by pytorch\n        instance = instance[np.newaxis, :, :]\n\n        # Normalise to 0-1\n        instance = (instance - instance.min()) / (instance.max() - instance.min())\n\n        label = self.datasetid_to_class_id[item]\n\n        return torch.from_numpy(instance), label\n\n    def __len__(self):\n        return len(self.df)\n\n    def num_classes(self):\n        return len(self.df[\'class_name\'].unique())\n\n    @staticmethod\n    def index_subset(subset):\n        """"""Index a subset by looping through all of its files and recording relevant information.\n\n        # Arguments\n            subset: Name of the subset\n\n        # Returns\n            A list of dicts containing information about all the image files in a particular subset of the\n            Omniglot dataset dataset\n        """"""\n        images = []\n        print(\'Indexing {}...\'.format(subset))\n        # Quick first pass to find total for tqdm bar\n        subset_len = 0\n        for root, folders, files in os.walk(DATA_PATH + \'/Omniglot/images_{}/\'.format(subset)):\n            subset_len += len([f for f in files if f.endswith(\'.png\')])\n\n        progress_bar = tqdm(total=subset_len)\n        for root, folders, files in os.walk(DATA_PATH + \'/Omniglot/images_{}/\'.format(subset)):\n            if len(files) == 0:\n                continue\n\n            alphabet = root.split(\'/\')[-2]\n            class_name = \'{}.{}\'.format(alphabet, root.split(\'/\')[-1])\n\n            for f in files:\n                progress_bar.update(1)\n                images.append({\n                    \'subset\': subset,\n                    \'alphabet\': alphabet,\n                    \'class_name\': class_name,\n                    \'filepath\': os.path.join(root, f)\n                })\n\n        progress_bar.close()\n        return images\n\n\nclass MiniImageNet(Dataset):\n    def __init__(self, subset):\n        """"""Dataset class representing miniImageNet dataset\n\n        # Arguments:\n            subset: Whether the dataset represents the background or evaluation set\n        """"""\n        if subset not in (\'background\', \'evaluation\'):\n            raise(ValueError, \'subset must be one of (background, evaluation)\')\n        self.subset = subset\n\n        self.df = pd.DataFrame(self.index_subset(self.subset))\n\n        # Index of dataframe has direct correspondence to item in dataset\n        self.df = self.df.assign(id=self.df.index.values)\n\n        # Convert arbitrary class names of dataset to ordered 0-(num_speakers - 1) integers\n        self.unique_characters = sorted(self.df[\'class_name\'].unique())\n        self.class_name_to_id = {self.unique_characters[i]: i for i in range(self.num_classes())}\n        self.df = self.df.assign(class_id=self.df[\'class_name\'].apply(lambda c: self.class_name_to_id[c]))\n\n        # Create dicts\n        self.datasetid_to_filepath = self.df.to_dict()[\'filepath\']\n        self.datasetid_to_class_id = self.df.to_dict()[\'class_id\']\n\n        # Setup transforms\n        self.transform = transforms.Compose([\n            transforms.CenterCrop(224),\n            transforms.Resize(84),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n        ])\n\n    def __getitem__(self, item):\n        instance = Image.open(self.datasetid_to_filepath[item])\n        instance = self.transform(instance)\n        label = self.datasetid_to_class_id[item]\n        return instance, label\n\n    def __len__(self):\n        return len(self.df)\n\n    def num_classes(self):\n        return len(self.df[\'class_name\'].unique())\n\n    @staticmethod\n    def index_subset(subset):\n        """"""Index a subset by looping through all of its files and recording relevant information.\n\n        # Arguments\n            subset: Name of the subset\n\n        # Returns\n            A list of dicts containing information about all the image files in a particular subset of the\n            miniImageNet dataset\n        """"""\n        images = []\n        print(\'Indexing {}...\'.format(subset))\n        # Quick first pass to find total for tqdm bar\n        subset_len = 0\n        for root, folders, files in os.walk(DATA_PATH + \'/miniImageNet/images_{}/\'.format(subset)):\n            subset_len += len([f for f in files if f.endswith(\'.png\')])\n\n        progress_bar = tqdm(total=subset_len)\n        for root, folders, files in os.walk(DATA_PATH + \'/miniImageNet/images_{}/\'.format(subset)):\n            if len(files) == 0:\n                continue\n\n            class_name = root.split(\'/\')[-1]\n\n            for f in files:\n                progress_bar.update(1)\n                images.append({\n                    \'subset\': subset,\n                    \'class_name\': class_name,\n                    \'filepath\': os.path.join(root, f)\n                })\n\n        progress_bar.close()\n        return images\n\n\nclass DummyDataset(Dataset):\n    def __init__(self, samples_per_class=10, n_classes=10, n_features=1):\n        """"""Dummy dataset for debugging/testing purposes\n\n        A sample from the DummyDataset has (n_features + 1) features. The first feature is the index of the sample\n        in the data and the remaining features are the class index.\n\n        # Arguments\n            samples_per_class: Number of samples per class in the dataset\n            n_classes: Number of distinct classes in the dataset\n            n_features: Number of extra features each sample should have.\n        """"""\n        self.samples_per_class = samples_per_class\n        self.n_classes = n_classes\n        self.n_features = n_features\n\n        # Create a dataframe to be consistent with other Datasets\n        self.df = pd.DataFrame({\n            \'class_id\': [i % self.n_classes for i in range(len(self))]\n        })\n        self.df = self.df.assign(id=self.df.index.values)\n\n    def __len__(self):\n        return self.samples_per_class * self.n_classes\n\n    def __getitem__(self, item):\n        class_id = item % self.n_classes\n        return np.array([item] + [class_id]*self.n_features, dtype=np.float), float(class_id)\n'"
few_shot/eval.py,4,"b'import torch\nfrom torch.nn import Module\nfrom torch.utils.data import DataLoader\nfrom typing import Callable, List, Union\n\nfrom few_shot.metrics import NAMED_METRICS\n\n\ndef evaluate(model: Module, dataloader: DataLoader, prepare_batch: Callable, metrics: List[Union[str, Callable]],\n             loss_fn: Callable = None, prefix: str = \'val_\', suffix: str = \'\'):\n    """"""Evaluate a model on one or more metrics on a particular dataset\n\n    # Arguments\n        model: Model to evaluate\n        dataloader: Instance of torch.utils.data.DataLoader representing the dataset\n        prepare_batch: Callable to perform any desired preprocessing\n        metrics: List of metrics to evaluate the model with. Metrics must either be a named metric (see `metrics.py`) or\n            a Callable that takes predictions and ground truth labels and returns a scalar value\n        loss_fn: Loss function to calculate over the dataset\n        prefix: Prefix to prepend to the name of each metric - used to identify the dataset. Defaults to \'val_\' as\n            it is typical to evaluate on a held-out validation dataset\n        suffix: Suffix to append to the name of each metric.\n    """"""\n    logs = {}\n    seen = 0\n    totals = {m: 0 for m in metrics}\n    if loss_fn is not None:\n        totals[\'loss\'] = 0\n    model.eval()\n    with torch.no_grad():\n        for batch in dataloader:\n            x, y = prepare_batch(batch)\n            y_pred = model(x)\n\n            seen += x.shape[0]\n\n            if loss_fn is not None:\n                totals[\'loss\'] += loss_fn(y_pred, y).item() * x.shape[0]\n\n            for m in metrics:\n                if isinstance(m, str):\n                    v = NAMED_METRICS[m](y, y_pred)\n                else:\n                    # Assume metric is a callable function\n                    v = m(y, y_pred)\n\n                totals[m] += v * x.shape[0]\n\n    for m in [\'loss\'] + metrics:\n        logs[prefix + m + suffix] = totals[m] / seen\n\n    return logs\n'"
few_shot/maml.py,12,"b'import torch\nfrom collections import OrderedDict\nfrom torch.optim import Optimizer\nfrom torch.nn import Module\nfrom typing import Dict, List, Callable, Union\n\nfrom few_shot.core import create_nshot_task_label\n\n\ndef replace_grad(parameter_gradients, parameter_name):\n    def replace_grad_(module):\n        return parameter_gradients[parameter_name]\n\n    return replace_grad_\n\n\ndef meta_gradient_step(model: Module,\n                       optimiser: Optimizer,\n                       loss_fn: Callable,\n                       x: torch.Tensor,\n                       y: torch.Tensor,\n                       n_shot: int,\n                       k_way: int,\n                       q_queries: int,\n                       order: int,\n                       inner_train_steps: int,\n                       inner_lr: float,\n                       train: bool,\n                       device: Union[str, torch.device]):\n    """"""\n    Perform a gradient step on a meta-learner.\n\n    # Arguments\n        model: Base model of the meta-learner being trained\n        optimiser: Optimiser to calculate gradient step from loss\n        loss_fn: Loss function to calculate between predictions and outputs\n        x: Input samples for all few shot tasks\n        y: Input labels of all few shot tasks\n        n_shot: Number of examples per class in the support set of each task\n        k_way: Number of classes in the few shot classification task of each task\n        q_queries: Number of examples per class in the query set of each task. The query set is used to calculate\n            meta-gradients after applying the update to\n        order: Whether to use 1st order MAML (update meta-learner weights with gradients of the updated weights on the\n            query set) or 2nd order MAML (use 2nd order updates by differentiating through the gradients of the updated\n            weights on the query with respect to the original weights).\n        inner_train_steps: Number of gradient steps to fit the fast weights during each inner update\n        inner_lr: Learning rate used to update the fast weights on the inner update\n        train: Whether to update the meta-learner weights at the end of the episode.\n        device: Device on which to run computation\n    """"""\n    data_shape = x.shape[2:]\n    create_graph = (True if order == 2 else False) and train\n\n    task_gradients = []\n    task_losses = []\n    task_predictions = []\n    for meta_batch in x:\n        # By construction x is a 5D tensor of shape: (meta_batch_size, n*k + q*k, channels, width, height)\n        # Hence when we iterate over the first  dimension we are iterating through the meta batches\n        x_task_train = meta_batch[:n_shot * k_way]\n        x_task_val = meta_batch[n_shot * k_way:]\n\n        # Create a fast model using the current meta model weights\n        fast_weights = OrderedDict(model.named_parameters())\n\n        # Train the model for `inner_train_steps` iterations\n        for inner_batch in range(inner_train_steps):\n            # Perform update of model weights\n            y = create_nshot_task_label(k_way, n_shot).to(device)\n            logits = model.functional_forward(x_task_train, fast_weights)\n            loss = loss_fn(logits, y)\n            gradients = torch.autograd.grad(loss, fast_weights.values(), create_graph=create_graph)\n\n            # Update weights manually\n            fast_weights = OrderedDict(\n                (name, param - inner_lr * grad)\n                for ((name, param), grad) in zip(fast_weights.items(), gradients)\n            )\n\n        # Do a pass of the model on the validation data from the current task\n        y = create_nshot_task_label(k_way, q_queries).to(device)\n        logits = model.functional_forward(x_task_val, fast_weights)\n        loss = loss_fn(logits, y)\n        loss.backward(retain_graph=True)\n\n        # Get post-update accuracies\n        y_pred = logits.softmax(dim=1)\n        task_predictions.append(y_pred)\n\n        # Accumulate losses and gradients\n        task_losses.append(loss)\n        gradients = torch.autograd.grad(loss, fast_weights.values(), create_graph=create_graph)\n        named_grads = {name: g for ((name, _), g) in zip(fast_weights.items(), gradients)}\n        task_gradients.append(named_grads)\n\n    if order == 1:\n        if train:\n            sum_task_gradients = {k: torch.stack([grad[k] for grad in task_gradients]).mean(dim=0)\n                                  for k in task_gradients[0].keys()}\n            hooks = []\n            for name, param in model.named_parameters():\n                hooks.append(\n                    param.register_hook(replace_grad(sum_task_gradients, name))\n                )\n\n            model.train()\n            optimiser.zero_grad()\n            # Dummy pass in order to create `loss` variable\n            # Replace dummy gradients with mean task gradients using hooks\n            logits = model(torch.zeros((k_way, ) + data_shape).to(device, dtype=torch.double))\n            loss = loss_fn(logits, create_nshot_task_label(k_way, 1).to(device))\n            loss.backward()\n            optimiser.step()\n\n            for h in hooks:\n                h.remove()\n\n        return torch.stack(task_losses).mean(), torch.cat(task_predictions)\n\n    elif order == 2:\n        model.train()\n        optimiser.zero_grad()\n        meta_batch_loss = torch.stack(task_losses).mean()\n\n        if train:\n            meta_batch_loss.backward()\n            optimiser.step()\n\n        return meta_batch_loss, torch.cat(task_predictions)\n    else:\n        raise ValueError(\'Order must be either 1 or 2.\')\n'"
few_shot/matching.py,10,"b'import torch\nfrom torch.nn.utils import clip_grad_norm_\nfrom torch.optim import Optimizer\nfrom torch.nn import Module\nfrom torch.nn.modules.loss import _Loss as Loss\n\nfrom config import EPSILON\nfrom few_shot.core import create_nshot_task_label\nfrom few_shot.utils import pairwise_distances\n\n\ndef matching_net_episode(model: Module,\n                         optimiser: Optimizer,\n                         loss_fn: Loss,\n                         x: torch.Tensor,\n                         y: torch.Tensor,\n                         n_shot: int,\n                         k_way: int,\n                         q_queries: int,\n                         distance: str,\n                         fce: bool,\n                         train: bool):\n    """"""Performs a single training episode for a Matching Network.\n\n    # Arguments\n        model: Matching Network to be trained.\n        optimiser: Optimiser to calculate gradient step from loss\n        loss_fn: Loss function to calculate between predictions and outputs\n        x: Input samples of few shot classification task\n        y: Input labels of few shot classification task\n        n_shot: Number of examples per class in the support set\n        k_way: Number of classes in the few shot classification task\n        q_queries: Number of examples per class in the query set\n        distance: Distance metric to use when calculating distance between support and query set samples\n        fce: Whether or not to us fully conditional embeddings\n        train: Whether (True) or not (False) to perform a parameter update\n\n    # Returns\n        loss: Loss of the Matching Network on this task\n        y_pred: Predicted class probabilities for the query set on this task\n    """"""\n    if train:\n        # Zero gradients\n        model.train()\n        optimiser.zero_grad()\n    else:\n        model.eval()\n\n    # Embed all samples\n    embeddings = model.encoder(x)\n\n    # Samples are ordered by the NShotWrapper class as follows:\n    # k lots of n support samples from a particular class\n    # k lots of q query samples from those classes\n    support = embeddings[:n_shot * k_way]\n    queries = embeddings[n_shot * k_way:]\n\n    # Optionally apply full context embeddings\n    if fce:\n        # LSTM requires input of shape (seq_len, batch, input_size). `support` is of\n        # shape (k_way * n_shot, embedding_dim) and we want the LSTM to treat the\n        # support set as a sequence so add a single dimension to transform support set\n        # to the shape (k_way * n_shot, 1, embedding_dim) and then remove the batch dimension\n        # afterwards\n\n        # Calculate the fully conditional embedding, g, for support set samples as described\n        # in appendix A.2 of the paper. g takes the form of a bidirectional LSTM with a\n        # skip connection from inputs to outputs\n        support, _, _ = model.g(support.unsqueeze(1))\n        support = support.squeeze(1)\n\n        # Calculate the fully conditional embedding, f, for the query set samples as described\n        # in appendix A.1 of the paper.\n        queries = model.f(support, queries)\n\n    # Efficiently calculate distance between all queries and all prototypes\n    # Output should have shape (q_queries * k_way, k_way) = (num_queries, k_way)\n    distances = pairwise_distances(queries, support, distance)\n\n    # Calculate ""attention"" as softmax over support-query distances\n    attention = (-distances).softmax(dim=1)\n\n    # Calculate predictions as in equation (1) from Matching Networks\n    # y_hat = \\sum_{i=1}^{k} a(x_hat, x_i) y_i\n    y_pred = matching_net_predictions(attention, n_shot, k_way, q_queries)\n\n    # Calculated loss with negative log likelihood\n    # Clip predictions for numerical stability\n    clipped_y_pred = y_pred.clamp(EPSILON, 1 - EPSILON)\n    loss = loss_fn(clipped_y_pred.log(), y)\n\n    if train:\n        # Backpropagate gradients\n        loss.backward()\n        # I found training to be quite unstable so I clip the norm\n        # of the gradient to be at most 1\n        clip_grad_norm_(model.parameters(), 1)\n        # Take gradient step\n        optimiser.step()\n\n    return loss, y_pred\n\n\ndef matching_net_predictions(attention: torch.Tensor, n: int, k: int, q: int) -> torch.Tensor:\n    """"""Calculates Matching Network predictions based on equation (1) of the paper.\n\n    The predictions are the weighted sum of the labels of the support set where the\n    weights are the ""attentions"" (i.e. softmax over query-support distances) pointing\n    from the query set samples to the support set samples.\n\n    # Arguments\n        attention: torch.Tensor containing softmax over query-support distances.\n            Should be of shape (q * k, k * n)\n        n: Number of support set samples per class, n-shot\n        k: Number of classes in the episode, k-way\n        q: Number of query samples per-class\n\n    # Returns\n        y_pred: Predicted class probabilities\n    """"""\n    if attention.shape != (q * k, k * n):\n        raise(ValueError(f\'Expecting attention Tensor to have shape (q * k, k * n) = ({q * k, k * n})\'))\n\n    # Create one hot label vector for the support set\n    y_onehot = torch.zeros(k * n, k)\n\n    # Unsqueeze to force y to be of shape (K*n, 1) as this\n    # is needed for .scatter()\n    y = create_nshot_task_label(k, n).unsqueeze(-1)\n    y_onehot = y_onehot.scatter(1, y, 1)\n\n    y_pred = torch.mm(attention, y_onehot.cuda().double())\n\n    return y_pred'"
few_shot/metrics.py,1,"b'import torch\n\n\ndef categorical_accuracy(y, y_pred):\n    """"""Calculates categorical accuracy.\n\n    # Arguments:\n        y_pred: Prediction probabilities or logits of shape [batch_size, num_categories]\n        y: Ground truth categories. Must have shape [batch_size,]\n    """"""\n    return torch.eq(y_pred.argmax(dim=-1), y).sum().item() / y_pred.shape[0]\n\n\nNAMED_METRICS = {\n    \'categorical_accuracy\': categorical_accuracy\n}\n'"
few_shot/models.py,11,"b'from torch import nn\nimport numpy as np\nimport torch.nn.functional as F\nimport torch\nfrom typing import Dict\n\n\n##########\n# Layers #\n##########\nclass Flatten(nn.Module):\n    """"""Converts N-dimensional Tensor of shape [batch_size, d1, d2, ..., dn] to 2-dimensional Tensor\n    of shape [batch_size, d1*d2*...*dn].\n\n    # Arguments\n        input: Input tensor\n    """"""\n    def forward(self, input):\n        return input.view(input.size(0), -1)\n\n\nclass GlobalMaxPool1d(nn.Module):\n    """"""Performs global max pooling over the entire length of a batched 1D tensor\n\n    # Arguments\n        input: Input tensor\n    """"""\n    def forward(self, input):\n        return nn.functional.max_pool1d(input, kernel_size=input.size()[2:]).view(-1, input.size(1))\n\n\nclass GlobalAvgPool2d(nn.Module):\n    """"""Performs global average pooling over the entire height and width of a batched 2D tensor\n\n    # Arguments\n        input: Input tensor\n    """"""\n    def forward(self, input):\n        return nn.functional.avg_pool2d(input, kernel_size=input.size()[2:]).view(-1, input.size(1))\n\n\ndef conv_block(in_channels: int, out_channels: int) -> nn.Module:\n    """"""Returns a Module that performs 3x3 convolution, ReLu activation, 2x2 max pooling.\n\n    # Arguments\n        in_channels:\n        out_channels:\n    """"""\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2, stride=2)\n    )\n\n\ndef functional_conv_block(x: torch.Tensor, weights: torch.Tensor, biases: torch.Tensor,\n                          bn_weights, bn_biases) -> torch.Tensor:\n    """"""Performs 3x3 convolution, ReLu activation, 2x2 max pooling in a functional fashion.\n\n    # Arguments:\n        x: Input Tensor for the conv block\n        weights: Weights for the convolutional block\n        biases: Biases for the convolutional block\n        bn_weights:\n        bn_biases:\n    """"""\n    x = F.conv2d(x, weights, biases, padding=1)\n    x = F.batch_norm(x, running_mean=None, running_var=None, weight=bn_weights, bias=bn_biases, training=True)\n    x = F.relu(x)\n    x = F.max_pool2d(x, kernel_size=2, stride=2)\n    return x\n\n\n##########\n# Models #\n##########\ndef get_few_shot_encoder(num_input_channels=1) -> nn.Module:\n    """"""Creates a few shot encoder as used in Matching and Prototypical Networks\n\n    # Arguments:\n        num_input_channels: Number of color channels the model expects input data to contain. Omniglot = 1,\n            miniImageNet = 3\n    """"""\n    return nn.Sequential(\n        conv_block(num_input_channels, 64),\n        conv_block(64, 64),\n        conv_block(64, 64),\n        conv_block(64, 64),\n        Flatten(),\n    )\n\n\nclass FewShotClassifier(nn.Module):\n    def __init__(self, num_input_channels: int, k_way: int, final_layer_size: int = 64):\n        """"""Creates a few shot classifier as used in MAML.\n\n        This network should be identical to the one created by `get_few_shot_encoder` but with a\n        classification layer on top.\n\n        # Arguments:\n            num_input_channels: Number of color channels the model expects input data to contain. Omniglot = 1,\n                miniImageNet = 3\n            k_way: Number of classes the model will discriminate between\n            final_layer_size: 64 for Omniglot, 1600 for miniImageNet\n        """"""\n        super(FewShotClassifier, self).__init__()\n        self.conv1 = conv_block(num_input_channels, 64)\n        self.conv2 = conv_block(64, 64)\n        self.conv3 = conv_block(64, 64)\n        self.conv4 = conv_block(64, 64)\n\n        self.logits = nn.Linear(final_layer_size, k_way)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n\n        x = x.view(x.size(0), -1)\n\n        return self.logits(x)\n\n    def functional_forward(self, x, weights):\n        """"""Applies the same forward pass using PyTorch functional operators using a specified set of weights.""""""\n\n        for block in [1, 2, 3, 4]:\n            x = functional_conv_block(x, weights[f\'conv{block}.0.weight\'], weights[f\'conv{block}.0.bias\'],\n                                      weights.get(f\'conv{block}.1.weight\'), weights.get(f\'conv{block}.1.bias\'))\n\n        x = x.view(x.size(0), -1)\n\n        x = F.linear(x, weights[\'logits.weight\'], weights[\'logits.bias\'])\n\n        return x\n\n\nclass MatchingNetwork(nn.Module):\n    def __init__(self, n: int, k: int, q: int, fce: bool, num_input_channels: int,\n                 lstm_layers: int, lstm_input_size: int, unrolling_steps: int, device: torch.device):\n        """"""Creates a Matching Network as described in Vinyals et al.\n\n        # Arguments:\n            n: Number of examples per class in the support set\n            k: Number of classes in the few shot classification task\n            q: Number of examples per class in the query set\n            fce: Whether or not to us fully conditional embeddings\n            num_input_channels: Number of color channels the model expects input data to contain. Omniglot = 1,\n                miniImageNet = 3\n            lstm_layers: Number of LSTM layers in the bidrectional LSTM g that embeds the support set (fce = True)\n            lstm_input_size: Input size for the bidirectional and Attention LSTM. This is determined by the embedding\n                dimension of the few shot encoder which is in turn determined by the size of the input data. Hence we\n                have Omniglot -> 64, miniImageNet -> 1600.\n            unrolling_steps: Number of unrolling steps to run the Attention LSTM\n            device: Device on which to run computation\n        """"""\n        super(MatchingNetwork, self).__init__()\n        self.n = n\n        self.k = k\n        self.q = q\n        self.fce = fce\n        self.num_input_channels = num_input_channels\n        self.encoder = get_few_shot_encoder(self.num_input_channels)\n        if self.fce:\n            self.g = BidrectionalLSTM(lstm_input_size, lstm_layers).to(device, dtype=torch.double)\n            self.f = AttentionLSTM(lstm_input_size, unrolling_steps=unrolling_steps).to(device, dtype=torch.double)\n\n    def forward(self, inputs):\n        pass\n\n\nclass BidrectionalLSTM(nn.Module):\n    def __init__(self, size: int, layers: int):\n        """"""Bidirectional LSTM used to generate fully conditional embeddings (FCE) of the support set as described\n        in the Matching Networks paper.\n\n        # Arguments\n            size: Size of input and hidden layers. These are constrained to be the same in order to implement the skip\n                connection described in Appendix A.2\n            layers: Number of LSTM layers\n        """"""\n        super(BidrectionalLSTM, self).__init__()\n        self.num_layers = layers\n        self.batch_size = 1\n        # Force input size and hidden size to be the same in order to implement\n        # the skip connection as described in Appendix A.1 and A.2 of Matching Networks\n        self.lstm = nn.LSTM(input_size=size,\n                            num_layers=layers,\n                            hidden_size=size,\n                            bidirectional=True)\n\n    def forward(self, inputs):\n        # Give None as initial state and Pytorch LSTM creates initial hidden states\n        output, (hn, cn) = self.lstm(inputs, None)\n\n        forward_output = output[:, :, :self.lstm.hidden_size]\n        backward_output = output[:, :, self.lstm.hidden_size:]\n\n        # g(x_i, S) = h_forward_i + h_backward_i + g\'(x_i) as written in Appendix A.2\n        # AKA A skip connection between inputs and outputs is used\n        output = forward_output + backward_output + inputs\n        return output, hn, cn\n\n\nclass AttentionLSTM(nn.Module):\n    def __init__(self, size: int, unrolling_steps: int):\n        """"""Attentional LSTM used to generate fully conditional embeddings (FCE) of the query set as described\n        in the Matching Networks paper.\n\n        # Arguments\n            size: Size of input and hidden layers. These are constrained to be the same in order to implement the skip\n                connection described in Appendix A.2\n            unrolling_steps: Number of steps of attention over the support set to compute. Analogous to number of\n                layers in a regular LSTM\n        """"""\n        super(AttentionLSTM, self).__init__()\n        self.unrolling_steps = unrolling_steps\n        self.lstm_cell = nn.LSTMCell(input_size=size,\n                                     hidden_size=size)\n\n    def forward(self, support, queries):\n        # Get embedding dimension, d\n        if support.shape[-1] != queries.shape[-1]:\n            raise(ValueError(""Support and query set have different embedding dimension!""))\n\n        batch_size = queries.shape[0]\n        embedding_dim = queries.shape[1]\n\n        h_hat = torch.zeros_like(queries).cuda().double()\n        c = torch.zeros(batch_size, embedding_dim).cuda().double()\n\n        for k in range(self.unrolling_steps):\n            # Calculate hidden state cf. equation (4) of appendix A.2\n            h = h_hat + queries\n\n            # Calculate softmax attentions between hidden states and support set embeddings\n            # cf. equation (6) of appendix A.2\n            attentions = torch.mm(h, support.t())\n            attentions = attentions.softmax(dim=1)\n\n            # Calculate readouts from support set embeddings cf. equation (5)\n            readout = torch.mm(attentions, support)\n\n            # Run LSTM cell cf. equation (3)\n            # h_hat, c = self.lstm_cell(queries, (torch.cat([h, readout], dim=1), c))\n            h_hat, c = self.lstm_cell(queries, (h + readout, c))\n\n        h = h_hat + queries\n\n        return h\n'"
few_shot/proto.py,6,"b'import torch\nfrom torch.optim import Optimizer\nfrom torch.nn import Module\nfrom typing import Callable\n\nfrom few_shot.utils import pairwise_distances\n\n\ndef proto_net_episode(model: Module,\n                      optimiser: Optimizer,\n                      loss_fn: Callable,\n                      x: torch.Tensor,\n                      y: torch.Tensor,\n                      n_shot: int,\n                      k_way: int,\n                      q_queries: int,\n                      distance: str,\n                      train: bool):\n    """"""Performs a single training episode for a Prototypical Network.\n\n    # Arguments\n        model: Prototypical Network to be trained.\n        optimiser: Optimiser to calculate gradient step\n        loss_fn: Loss function to calculate between predictions and outputs. Should be cross-entropy\n        x: Input samples of few shot classification task\n        y: Input labels of few shot classification task\n        n_shot: Number of examples per class in the support set\n        k_way: Number of classes in the few shot classification task\n        q_queries: Number of examples per class in the query set\n        distance: Distance metric to use when calculating distance between class prototypes and queries\n        train: Whether (True) or not (False) to perform a parameter update\n\n    # Returns\n        loss: Loss of the Prototypical Network on this task\n        y_pred: Predicted class probabilities for the query set on this task\n    """"""\n    if train:\n        # Zero gradients\n        model.train()\n        optimiser.zero_grad()\n    else:\n        model.eval()\n\n    # Embed all samples\n    embeddings = model(x)\n\n    # Samples are ordered by the NShotWrapper class as follows:\n    # k lots of n support samples from a particular class\n    # k lots of q query samples from those classes\n    support = embeddings[:n_shot*k_way]\n    queries = embeddings[n_shot*k_way:]\n    prototypes = compute_prototypes(support, k_way, n_shot)\n\n    # Calculate squared distances between all queries and all prototypes\n    # Output should have shape (q_queries * k_way, k_way) = (num_queries, k_way)\n    distances = pairwise_distances(queries, prototypes, distance)\n\n    # Calculate log p_{phi} (y = k | x)\n    log_p_y = (-distances).log_softmax(dim=1)\n    loss = loss_fn(log_p_y, y)\n\n    # Prediction probabilities are softmax over distances\n    y_pred = (-distances).softmax(dim=1)\n\n    if train:\n        # Take gradient step\n        loss.backward()\n        optimiser.step()\n    else:\n        pass\n\n    return loss, y_pred\n\n\ndef compute_prototypes(support: torch.Tensor, k: int, n: int) -> torch.Tensor:\n    """"""Compute class prototypes from support samples.\n\n    # Arguments\n        support: torch.Tensor. Tensor of shape (n * k, d) where d is the embedding\n            dimension.\n        k: int. ""k-way"" i.e. number of classes in the classification task\n        n: int. ""n-shot"" of the classification task\n\n    # Returns\n        class_prototypes: Prototypes aka mean embeddings for each class\n    """"""\n    # Reshape so the first dimension indexes by class then take the mean\n    # along that dimension to generate the ""prototypes"" for each class\n    class_prototypes = support.reshape(k, n, -1).mean(dim=1)\n    return class_prototypes'"
few_shot/train.py,6,"b'""""""\nThe `fit` function in this file implements a slightly modified version\nof the Keras `model.fit()` API.\n""""""\nimport torch\nfrom torch.optim import Optimizer\nfrom torch.nn import Module\nfrom torch.utils.data import DataLoader\nfrom typing import Callable, List, Union\n\nfrom few_shot.callbacks import DefaultCallback, ProgressBarLogger, CallbackList, Callback\nfrom few_shot.metrics import NAMED_METRICS\n\n\ndef gradient_step(model: Module, optimiser: Optimizer, loss_fn: Callable, x: torch.Tensor, y: torch.Tensor, **kwargs):\n    """"""Takes a single gradient step.\n\n    # Arguments\n        model: Model to be fitted\n        optimiser: Optimiser to calculate gradient step from loss\n        loss_fn: Loss function to calculate between predictions and outputs\n        x: Input samples\n        y: Input targets\n    """"""\n    model.train()\n    optimiser.zero_grad()\n    y_pred = model(x)\n    loss = loss_fn(y_pred, y)\n    loss.backward()\n    optimiser.step()\n\n    return loss, y_pred\n\n\ndef batch_metrics(model: Module, y_pred: torch.Tensor, y: torch.Tensor, metrics: List[Union[str, Callable]],\n                  batch_logs: dict):\n    """"""Calculates metrics for the current training batch\n\n    # Arguments\n        model: Model being fit\n        y_pred: predictions for a particular batch\n        y: labels for a particular batch\n        batch_logs: Dictionary of logs for the current batch\n    """"""\n    model.eval()\n    for m in metrics:\n        if isinstance(m, str):\n            batch_logs[m] = NAMED_METRICS[m](y, y_pred)\n        else:\n            # Assume metric is a callable function\n            batch_logs = m(y, y_pred)\n\n    return batch_logs\n\n\ndef fit(model: Module, optimiser: Optimizer, loss_fn: Callable, epochs: int, dataloader: DataLoader,\n        prepare_batch: Callable, metrics: List[Union[str, Callable]] = None, callbacks: List[Callback] = None,\n        verbose: bool =True, fit_function: Callable = gradient_step, fit_function_kwargs: dict = {}):\n    """"""Function to abstract away training loop.\n\n    The benefit of this function is that allows training scripts to be much more readable and allows for easy re-use of\n    common training functionality provided they are written as a subclass of voicemap.Callback (following the\n    Keras API).\n\n    # Arguments\n        model: Model to be fitted.\n        optimiser: Optimiser to calculate gradient step from loss\n        loss_fn: Loss function to calculate between predictions and outputs\n        epochs: Number of epochs of fitting to be performed\n        dataloader: `torch.DataLoader` instance to fit the model to\n        prepare_batch: Callable to perform any desired preprocessing\n        metrics: Optional list of metrics to evaluate the model with\n        callbacks: Additional functionality to incorporate into training such as logging metrics to csv, model\n            checkpointing, learning rate scheduling etc... See voicemap.callbacks for more.\n        verbose: All print output is muted if this argument is `False`\n        fit_function: Function for calculating gradients. Leave as default for simple supervised training on labelled\n            batches. For more complex training procedures (meta-learning etc...) you will need to write your own\n            fit_function\n        fit_function_kwargs: Keyword arguments to pass to `fit_function`\n    """"""\n    # Determine number of samples:\n    num_batches = len(dataloader)\n    batch_size = dataloader.batch_size\n\n    callbacks = CallbackList([DefaultCallback(), ] + (callbacks or []) + [ProgressBarLogger(), ])\n    callbacks.set_model(model)\n    callbacks.set_params({\n        \'num_batches\': num_batches,\n        \'batch_size\': batch_size,\n        \'verbose\': verbose,\n        \'metrics\': (metrics or []),\n        \'prepare_batch\': prepare_batch,\n        \'loss_fn\': loss_fn,\n        \'optimiser\': optimiser\n    })\n\n    if verbose:\n        print(\'Begin training...\')\n\n    callbacks.on_train_begin()\n\n    for epoch in range(1, epochs+1):\n        callbacks.on_epoch_begin(epoch)\n\n        epoch_logs = {}\n        for batch_index, batch in enumerate(dataloader):\n            batch_logs = dict(batch=batch_index, size=(batch_size or 1))\n\n            callbacks.on_batch_begin(batch_index, batch_logs)\n\n            x, y = prepare_batch(batch)\n\n            loss, y_pred = fit_function(model, optimiser, loss_fn, x, y, **fit_function_kwargs)\n            batch_logs[\'loss\'] = loss.item()\n\n            # Loops through all metrics\n            batch_logs = batch_metrics(model, y_pred, y, metrics, batch_logs)\n\n            callbacks.on_batch_end(batch_index, batch_logs)\n\n        # Run on epoch end\n        callbacks.on_epoch_end(epoch, epoch_logs)\n\n    # Run on train end\n    if verbose:\n        print(\'Finished.\')\n\n    callbacks.on_train_end()\n'"
few_shot/utils.py,11,"b'import torch\nimport os\nimport shutil\nfrom typing import Tuple, List\n\nfrom config import EPSILON, PATH\n\n\ndef mkdir(dir):\n    """"""Create a directory, ignoring exceptions\n\n    # Arguments:\n        dir: Path of directory to create\n    """"""\n    try:\n        os.mkdir(dir)\n    except:\n        pass\n\n\ndef rmdir(dir):\n    """"""Recursively remove a directory and contents, ignoring exceptions\n\n   # Arguments:\n       dir: Path of directory to recursively remove\n   """"""\n    try:\n        shutil.rmtree(dir)\n    except:\n        pass\n\n\ndef setup_dirs():\n    """"""Creates directories for this project.""""""\n    mkdir(PATH + \'/logs/\')\n    mkdir(PATH + \'/logs/proto_nets\')\n    mkdir(PATH + \'/logs/matching_nets\')\n    mkdir(PATH + \'/logs/maml\')\n    mkdir(PATH + \'/models/\')\n    mkdir(PATH + \'/models/proto_nets\')\n    mkdir(PATH + \'/models/matching_nets\')\n    mkdir(PATH + \'/models/maml\')\n\n\ndef pairwise_distances(x: torch.Tensor,\n                       y: torch.Tensor,\n                       matching_fn: str) -> torch.Tensor:\n    """"""Efficiently calculate pairwise distances (or other similarity scores) between\n    two sets of samples.\n\n    # Arguments\n        x: Query samples. A tensor of shape (n_x, d) where d is the embedding dimension\n        y: Class prototypes. A tensor of shape (n_y, d) where d is the embedding dimension\n        matching_fn: Distance metric/similarity score to compute between samples\n    """"""\n    n_x = x.shape[0]\n    n_y = y.shape[0]\n\n    if matching_fn == \'l2\':\n        distances = (\n                x.unsqueeze(1).expand(n_x, n_y, -1) -\n                y.unsqueeze(0).expand(n_x, n_y, -1)\n        ).pow(2).sum(dim=2)\n        return distances\n    elif matching_fn == \'cosine\':\n        normalised_x = x / (x.pow(2).sum(dim=1, keepdim=True).sqrt() + EPSILON)\n        normalised_y = y / (y.pow(2).sum(dim=1, keepdim=True).sqrt() + EPSILON)\n\n        expanded_x = normalised_x.unsqueeze(1).expand(n_x, n_y, -1)\n        expanded_y = normalised_y.unsqueeze(0).expand(n_x, n_y, -1)\n\n        cosine_similarities = (expanded_x * expanded_y).sum(dim=2)\n        return 1 - cosine_similarities\n    elif matching_fn == \'dot\':\n        expanded_x = x.unsqueeze(1).expand(n_x, n_y, -1)\n        expanded_y = y.unsqueeze(0).expand(n_x, n_y, -1)\n\n        return -(expanded_x * expanded_y).sum(dim=2)\n    else:\n        raise(ValueError(\'Unsupported similarity function\'))\n\n\ndef copy_weights(from_model: torch.nn.Module, to_model: torch.nn.Module):\n    """"""Copies the weights from one model to another model.\n\n    # Arguments:\n        from_model: Model from which to source weights\n        to_model: Model which will receive weights\n    """"""\n    if not from_model.__class__ == to_model.__class__:\n        raise(ValueError(""Models don\'t have the same architecture!""))\n\n    for m_from, m_to in zip(from_model.modules(), to_model.modules()):\n        is_linear = isinstance(m_to, torch.nn.Linear)\n        is_conv = isinstance(m_to, torch.nn.Conv2d)\n        is_bn = isinstance(m_to, torch.nn.BatchNorm2d)\n        if is_linear or is_conv or is_bn:\n            m_to.weight.data = m_from.weight.data.clone()\n            if m_to.bias is not None:\n                m_to.bias.data = m_from.bias.data.clone()\n\n\ndef autograd_graph(tensor: torch.Tensor) -> Tuple[\n            List[torch.autograd.Function],\n            List[Tuple[torch.autograd.Function, torch.autograd.Function]]\n        ]:\n    """"""Recursively retrieves the autograd graph for a particular tensor.\n\n    # Arguments\n        tensor: The Tensor to retrieve the autograd graph for\n\n    # Returns\n        nodes: List of torch.autograd.Functions that are the nodes of the autograd graph\n        edges: List of (Function, Function) tuples that are the edges between the nodes of the autograd graph\n    """"""\n    nodes, edges = list(), list()\n\n    def _add_nodes(tensor):\n        if tensor not in nodes:\n            nodes.append(tensor)\n\n            if hasattr(tensor, \'next_functions\'):\n                for f in tensor.next_functions:\n                    if f[0] is not None:\n                        edges.append((f[0], tensor))\n                        _add_nodes(f[0])\n\n            if hasattr(tensor, \'saved_tensors\'):\n                for t in tensor.saved_tensors:\n                    edges.append((t, tensor))\n                    _add_nodes(t)\n\n    _add_nodes(tensor.grad_fn)\n\n    return nodes, edges\n'"
scripts/__init__.py,0,b''
scripts/prepare_mini_imagenet.py,0,"b'""""""\nRun this script to prepare the miniImageNet dataset.\n\nThis script uses the 100 classes of 600 images each used in the Matching Networks paper. The exact images used are\ngiven in data/mini_imagenet.txt which is downloaded from the link provided in the paper (https://goo.gl/e3orz6).\n\n1. Download files from https://drive.google.com/file/d/0B3Irx3uQNoBMQ1FlNXJsZUdYWEE/view and place in\n    data/miniImageNet/images\n2. Run the script\n""""""\nfrom tqdm import tqdm as tqdm\nimport numpy as np\nimport shutil\nimport os\n\nfrom config import DATA_PATH\nfrom few_shot.utils import mkdir, rmdir\n\n\n# Clean up folders\nrmdir(DATA_PATH + \'/miniImageNet/images_background\')\nrmdir(DATA_PATH + \'/miniImageNet/images_evaluation\')\nmkdir(DATA_PATH + \'/miniImageNet/images_background\')\nmkdir(DATA_PATH + \'/miniImageNet/images_evaluation\')\n\n# Find class identities\nclasses = []\nfor root, _, files in os.walk(DATA_PATH + \'/miniImageNet/images/\'):\n    for f in files:\n        if f.endswith(\'.jpg\'):\n            classes.append(f[:-12])\n\nclasses = list(set(classes))\n\n# Train/test split\nnp.random.seed(0)\nnp.random.shuffle(classes)\nbackground_classes, evaluation_classes = classes[:80], classes[80:]\n\n# Create class folders\nfor c in background_classes:\n    mkdir(DATA_PATH + f\'/miniImageNet/images_background/{c}/\')\n\nfor c in evaluation_classes:\n    mkdir(DATA_PATH + f\'/miniImageNet/images_evaluation/{c}/\')\n\n# Move images to correct location\nfor root, _, files in os.walk(DATA_PATH + \'/miniImageNet/images\'):\n    for f in tqdm(files, total=600*100):\n        if f.endswith(\'.jpg\'):\n            class_name = f[:-12]\n            image_name = f[-12:]\n            # Send to correct folder\n            subset_folder = \'images_evaluation\' if class_name in evaluation_classes else \'images_background\'\n            src = f\'{root}/{f}\'\n            dst = DATA_PATH + f\'/miniImageNet/{subset_folder}/{class_name}/{image_name}\'\n            shutil.copy(src, dst)\n'"
scripts/prepare_omniglot.py,0,"b'""""""\nRun this script to prepare the Omniglot dataset from the raw Omniglot dataset that is found at\nhttps://github.com/brendenlake/omniglot/tree/master/python.\n\nThis script prepares an enriched version of Omniglot the same as is used in the Matching Networks and Prototypical\nNetworks papers.\n\n1. Augment classes with rotations in multiples of 90 degrees.\n2. Downsize images to 28x28\n3. Uses background and evaluation sets present in the raw dataset\n""""""\nfrom skimage import io\nfrom skimage import transform\nimport zipfile\nimport shutil\nimport os\n\nfrom config import DATA_PATH\nfrom few_shot.utils import mkdir, rmdir\n\n\n# Parameters\ndataset_zip_files = [\'images_background.zip\', \'images_evaluation.zip\']\nraw_omniglot_location = DATA_PATH + \'/Omniglot_Raw/\'\nprepared_omniglot_location = DATA_PATH + \'/Omniglot/\'\noutput_shape = (28, 28)\n\n\ndef handle_characters(alphabet_folder, character_folder, rotate):\n    for root, _, character_images in os.walk(character_folder):\n        character_name = root.split(\'/\')[-1]\n        mkdir(f\'{alphabet_folder}.{rotate}/{character_name}\')\n        for img_path in character_images:\n            # print(root+\'/\'+img_path)\n            img = io.imread(root+\'/\'+img_path)\n            img = transform.rotate(img, angle=rotate)\n            img = transform.resize(img, output_shape, anti_aliasing=True)\n            img = (img - img.min()) / (img.max() - img.min())\n            # print(img.min(), img.max())\n            # print(f\'{alphabet_folder}.{rotate}/{character_name}/{img_path}\')\n            io.imsave(f\'{alphabet_folder}.{rotate}/{character_name}/{img_path}\', img)\n            # return\n\n\ndef handle_alphabet(folder):\n    print(\'{}...\'.format(folder.split(\'/\')[-1]))\n    for rotate in [0, 90, 180, 270]:\n        # Create new folders for each augmented alphabet\n        mkdir(f\'{folder}.{rotate}\')\n        for root, character_folders, _ in os.walk(folder):\n            for character_folder in character_folders:\n                # For each character folder in an alphabet rotate and resize all of the images and save\n                # to the new folder\n                handle_characters(folder, root + \'/\' + character_folder, rotate)\n                # return\n\n    # Delete original alphabet\n    rmdir(folder)\n\n\n# Clean up previous extraction\nrmdir(prepared_omniglot_location)\nmkdir(prepared_omniglot_location)\n\n# Unzip dataset\nfor root, _, files in os.walk(raw_omniglot_location):\n    for f in files:\n        if f in dataset_zip_files:\n            print(\'Unzipping {}...\'.format(f))\n            zip_ref = zipfile.ZipFile(root + f, \'r\')\n            zip_ref.extractall(prepared_omniglot_location)\n            zip_ref.close()\n\nprint(\'Processing background set...\')\nfor root, alphabets, _ in os.walk(prepared_omniglot_location + \'images_background/\'):\n    for alphabet in sorted(alphabets):\n        handle_alphabet(root + alphabet)\n\nprint(\'Processing evaluation set...\')\nfor root, alphabets, _ in os.walk(prepared_omniglot_location + \'images_evaluation/\'):\n    for alphabet in sorted(alphabets):\n        handle_alphabet(root + alphabet)\n'"
tests/__init__.py,0,b''
tests/test_few_shot.py,3,"b""import unittest\n\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom few_shot.core import create_nshot_task_label, NShotTaskSampler\nfrom few_shot.datasets import DummyDataset\n\n\nclass TestNShotLabel(unittest.TestCase):\n    def test_label(self):\n        n = 1\n        k = 5\n        q = 1\n\n        y = create_nshot_task_label(k, q)\n\n\nclass TestNShotSampler(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.dataset = DummyDataset(samples_per_class=1000, n_classes=20)\n\n    def test_n_shot_sampler(self):\n        n, k, q = 2, 4, 3\n        n_shot_taskloader = DataLoader(self.dataset,\n                                       batch_sampler=NShotTaskSampler(self.dataset, 100, n, k, q))\n\n        # Load a single n-shot task and check it's properties\n        for x, y in n_shot_taskloader:\n            support = x[:n*k]\n            queries = x[n*k:]\n            support_labels = y[:n*k]\n            query_labels = y[n*k:]\n\n            # Check ordering of support labels is correct\n            for i in range(0, n * k, n):\n                support_set_labels_correct = torch.all(support_labels[i:i + n] == support_labels[i])\n                self.assertTrue(\n                    support_set_labels_correct,\n                    'Classes of support set samples should be arranged like: '\n                    '[class_1]*n + [class_2]*n + ... + [class_k]*n'\n                )\n\n            # Check ordering of query labels is correct\n            for i in range(0, q * k, q):\n                support_set_labels_correct = torch.all(query_labels[i:i + q] == query_labels[i])\n                self.assertTrue(\n                    support_set_labels_correct,\n                    'Classes of query set samples should be arranged like: '\n                    '[class_1]*q + [class_2]*q + ... + [class_k]*q'\n                )\n\n            # Check labels are consistent across query and support\n            for i in range(k):\n                self.assertEqual(\n                    support_labels[i*n],\n                    query_labels[i*q],\n                    'Classes of query and support set should be consistent.'\n                )\n\n            # Check no overlap of IDs between support and query.\n            # By construction the first feature in the DummyDataset is the\n            # id of the sample in the dataset so we can use this to test\n            # for overlap betwen query and suppport samples\n            self.assertEqual(\n                len(set(support[:, 0].numpy()).intersection(set(queries[:, 0].numpy()))),\n                0,\n                'There should be no overlap between support and query set samples.'\n            )\n\n            break"""
tests/test_maml.py,10,"b'import unittest\nimport torch\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nfrom typing import List, Tuple\n\nfrom few_shot.datasets import DummyDataset\nfrom few_shot.core import NShotTaskSampler, create_nshot_task_label\nfrom few_shot.maml import meta_gradient_step\nfrom few_shot.utils import autograd_graph\n\n\nclass DummyModel(torch.nn.Module):\n    """"""Dummy 1 layer (0 hidden layer) model for testing purposes""""""\n    def __init__(self, k: int):\n        super(DummyModel, self).__init__()\n        self.out = torch.nn.Linear(2, k, bias=False)\n\n    def forward(self, x):\n        x = self.out(x)\n        return x\n\n    def functional_forward(self, x, weights):\n        x = F.linear(x, weights[\'out.weight\'])\n        return x\n\n\nclass TestMAML(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.n = 1\n        cls.k = 5\n        cls.q = 1\n\n        cls.meta_batch_size = 1\n\n        cls.dummy = DummyDataset()\n        cls.dummy_tasks = DataLoader(\n            cls.dummy, batch_sampler=NShotTaskSampler(cls.dummy, cls.meta_batch_size, n=cls.n, k=cls.k, q=cls.q, num_tasks=1),\n        )\n\n        cls.model = DummyModel(cls.k).double()\n        cls.opt = torch.optim.Adam(cls.model.parameters(), lr=0.001)\n\n    def _get_maml_graph(self, order: int, inner_train_steps: int) -> Tuple[\n            List[torch.autograd.Function],\n            List[Tuple[torch.autograd.Function, torch.autograd.Function]]\n        ]:\n        """"""Gets the autograd graph for a single iteration of MAML.\n\n        # Arguments:\n            order: Whether to use 1st order MAML (update meta-learner weights with gradients of the updated weights on the\n            query set) or 2nd order MAML (use 2nd order updates by differentiating through the gradients of the updated\n            weights on the query with respect to the original weights).\n            inner_train_steps: Number of gradient steps to fit the fast weights during each inner update\n\n        # Returns\n            nodes: List of torch.autograd.Functions that are the nodes of the autograd graph\n            edges: List of (Function, Function) tuples that are the edges between the nodes of the autograd graph\n        """"""\n        x, _ = self.dummy_tasks.__iter__().__next__()\n        x = x.double().reshape(self.meta_batch_size, self.n * self.k + self.q * self.k, x.shape[-1])\n        y = create_nshot_task_label(self.k, self.q).repeat(self.meta_batch_size)\n\n        loss, y_pred = meta_gradient_step(self.model, self.opt, torch.nn.CrossEntropyLoss(), x, y, self.n, self.k,\n                                          self.q, order=order, inner_train_steps=inner_train_steps, inner_lr=0.1,\n                                          train=True, device=\'cpu\')\n\n        nodes, edges = autograd_graph(loss)\n\n        return nodes, edges\n\n    def _count_named_nodes(self, nodes: List[torch.autograd.Function], name: str) -> int:\n        """"""Counts the number of autograd graph nodes that are named in a particular way\n\n        # Arguments\n            nodes: List of autograd Functions\n            name: Name of Function to look for\n\n        # Returns\n            count: Number of s in `nodes` that match `name`\n        """"""\n        count = 0\n\n        for n in nodes:\n            if n.__class__.__name__ == name:\n                count += 1\n\n        return count\n\n    def test_first_order(self):\n        """"""Test the 1st order MAML is only perform a first order update by inspecting the autograd graph.""""""\n        # Run a single meta batch using 1st order MAML\n        nodes_1_1, _ = self._get_maml_graph(order=1, inner_train_steps=1)\n\n        self.assertEqual(\n            self._count_named_nodes(nodes_1_1, \'NllLossBackwardBackward\'),\n            0,\n            \'The autograd graph of 1st order MAML should not contain any double backwards operations\'\n        )\n\n    def test_second_order(self):\n        """"""Test that 2nd order MAML is genuinely performing a second order update by inspecting the autograd graph.""""""\n        # Run a single meta batch using 1st order MAML\n        nodes_1_1, _ = self._get_maml_graph(order=1, inner_train_steps=1)\n\n        # Run a single meta batch using 2nd order MAML\n        nodes_2_1, _ = self._get_maml_graph(order=2, inner_train_steps=1)\n\n        # Run a single meta batch using 2nd order MAML\n        nodes_2_2, _ = self._get_maml_graph(order=2, inner_train_steps=2)\n\n        self.assertGreater(\n            len(nodes_2_1),\n            len(nodes_1_1),\n            \'2nd order MAML should produce a larger autograd graph than 1st order MAMl\'\n        )\n\n        self.assertEqual(\n            self._count_named_nodes(nodes_2_1, \'NllLossBackwardBackward\'),\n            1,\n            \'2nd order MAML with 1 inner step should produce an autograd graph with a single double backwards \'\n            \'operation corresponding to taking the gradient of the gradient due to the first inner step.\'\n        )\n\n        self.assertEqual(\n            self._count_named_nodes(nodes_2_2, \'NllLossBackwardBackward\'),\n            2,\n            \'2nd order MAML with 2 inner steps should produce an autograd graph with two double backwards \'\n            \'operations corresponding to taking the second differential of the gradients of the 1st and 2nd inner step.\'\n        )\n'"
tests/test_matching.py,5,"b'import unittest\n\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom few_shot.core import NShotTaskSampler\nfrom few_shot.datasets import DummyDataset\nfrom few_shot.matching import matching_net_predictions\nfrom few_shot.utils import pairwise_distances\n\n\nclass TestMatchingNets(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.dataset = DummyDataset(samples_per_class=1000, n_classes=20)\n\n    def _test_n_k_q_combination(self, n, k, q):\n        n_shot_taskloader = DataLoader(self.dataset,\n                                       batch_sampler=NShotTaskSampler(self.dataset, 100, n, k, q))\n\n        # Load a single n-shot, k-way task\n        for batch in n_shot_taskloader:\n            x, y = batch\n            break\n\n        # Take just dummy label features and a little bit of noise\n        # So distances are never 0\n        support = x[:n * k, 1:]\n        queries = x[n * k:, 1:]\n        support += torch.rand_like(support)\n        queries += torch.rand_like(queries)\n\n        distances = pairwise_distances(queries, support, \'cosine\')\n\n        # Calculate ""attention"" as softmax over distances\n        attention = (-distances).softmax(dim=1).cuda()\n\n        y_pred = matching_net_predictions(attention, n, k, q)\n\n        self.assertEqual(\n            y_pred.shape,\n            (q * k, k),\n            \'Matching Network predictions must have shape (q * k, k).\'\n        )\n\n        y_pred_sum = y_pred.sum(dim=1)\n        self.assertTrue(\n            torch.all(\n                torch.isclose(y_pred_sum, torch.ones_like(y_pred_sum).double())\n            ),\n            \'Matching Network predictions probabilities must sum to 1 for each \'\n            \'query sample.\'\n        )\n\n    def test_matching_net_predictions(self):\n        test_combinations = [\n            (1, 5, 5),\n            (5, 5, 5),\n            (1, 20, 5),\n            (5, 20, 5)\n        ]\n\n        for n, k, q in test_combinations:\n            self._test_n_k_q_combination(n, k, q)'"
tests/test_proto.py,1,"b""import unittest\n\nfrom torch.utils.data import DataLoader\n\nfrom few_shot.core import NShotTaskSampler\nfrom few_shot.datasets import DummyDataset, OmniglotDataset, MiniImageNet\nfrom few_shot.models import get_few_shot_encoder\nfrom few_shot.proto import compute_prototypes\n\n\nclass TestProtoNets(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.dataset = DummyDataset(samples_per_class=1000, n_classes=20)\n\n    def _test_n_k_q_combination(self, n, k, q):\n        n_shot_taskloader = DataLoader(self.dataset,\n                                       batch_sampler=NShotTaskSampler(self.dataset, 100, n, k, q))\n\n        # Load a single n-shot, k-way task\n        for batch in n_shot_taskloader:\n            x, y = batch\n            break\n\n        support = x[:n * k]\n        support_labels = y[:n * k]\n        prototypes = compute_prototypes(support, k, n)\n\n        # By construction the second feature of samples from the\n        # DummyDataset is equal to the label.\n        # As class prototypes are constructed from the means of the support\n        # set items of a particular class the value of the second feature\n        # of the class prototypes should be equal to the label of that class.\n        for i in range(k):\n            self.assertEqual(\n                support_labels[i * n],\n                prototypes[i, 1],\n                'Prototypes computed incorrectly!'\n            )\n\n    def test_compute_prototypes(self):\n        test_combinations = [\n            (1, 5, 5),\n            (5, 5, 5),\n            (1, 20, 5),\n            (5, 20, 5)\n        ]\n\n        for n, k, q in test_combinations:\n            self._test_n_k_q_combination(n, k, q)\n\n    def test_create_model(self):\n        # Check output of encoder has shape specified in paper\n        encoder = get_few_shot_encoder(num_input_channels=1).float()\n        omniglot = OmniglotDataset('background')\n        self.assertEqual(\n            encoder(omniglot[0][0].unsqueeze(0).float()).shape[1],\n            64,\n            'Encoder network should produce 64 dimensional embeddings on Omniglot dataset.'\n        )\n\n        encoder = get_few_shot_encoder(num_input_channels=3).float()\n        omniglot = MiniImageNet('background')\n        self.assertEqual(\n            encoder(omniglot[0][0].unsqueeze(0).float()).shape[1],\n            1600,\n            'Encoder network should produce 1600 dimensional embeddings on miniImageNet dataset.'\n        )"""
tests/test_utils.py,29,"b'import unittest\nimport numpy as np\nimport torch\nfrom torch.nn.modules.distance import CosineSimilarity, PairwiseDistance\n\nfrom few_shot.utils import *\nfrom config import PATH\n\n\nclass TestDistance(unittest.TestCase):\n    def test_query_support_distances(self):\n        # Create some dummy data with easily verifiable distances\n        q = 1  # 1 query per class\n        k = 3  # 3 way classification\n        d = 2  # embedding dimension of two\n\n        query = torch.zeros([q * k, d], dtype=torch.double)\n        query[0] = torch.Tensor([0, 0])\n        query[1] = torch.Tensor([0, 1])\n        query[2] = torch.Tensor([1, 0])\n        support = torch.zeros([k, d], dtype=torch.double)\n        support[0] = torch.Tensor([1, 1])\n        support[1] = torch.Tensor([1, 2])\n        support[2] = torch.Tensor([2, 2])\n\n        distances = pairwise_distances(query, support, \'l2\')\n        self.assertEqual(\n            distances.shape, (q * k, k),\n            \'Output should have shape (q * k, k).\'\n        )\n\n        # Calculate squared distances by iterating through all query-support pairs\n        for i, q_ in enumerate(query):\n            for j, s_ in enumerate(support):\n                self.assertEqual(\n                    (q_ - s_).pow(2).sum(),\n                    distances[i, j].item(),\n                    \'The jth column of the ith row should be the squared distance between the \'\n                    \'ith query sample and the kth query sample\'\n                )\n\n        # Create some dummy data with easily verifiable distances\n        q = 1  # 1 query per class\n        k = 3  # 3 way classification\n        d = 2  # embedding dimension of two\n        query = torch.zeros([q * k, d], dtype=torch.double)\n        query[0] = torch.Tensor([1, 0])\n        query[1] = torch.Tensor([0, 1])\n        query[2] = torch.Tensor([1, 1])\n        support = torch.zeros([k, d], dtype=torch.double)\n        support[0] = torch.Tensor([1, 1])\n        support[1] = torch.Tensor([-1, -1])\n        support[2] = torch.Tensor([0, 2])\n\n        distances = pairwise_distances(query, support, \'cosine\')\n\n        # Calculate distances by iterating through all query-support pairs\n        for i, q_ in enumerate(query):\n            for j, s_ in enumerate(support):\n                self.assertTrue(\n                    torch.isclose(1-CosineSimilarity(dim=0)(q_, s_), distances[i, j], atol=2e-8),\n                    \'The jth column of the ith row should be the squared distance between the \'\n                    \'ith query sample and the kth query sample\'\n                )\n\n    def test_no_nans_on_zero_vectors(self):\n        """"""Cosine distance calculation involves a divide-through by vector magnitude which\n        can divide by zeros to occur.\n        """"""\n        # Create some dummy data with easily verifiable distances\n        q = 1  # 1 query per class\n        k = 3  # 3 way classification\n        d = 2  # embedding dimension of two\n        query = torch.zeros([q * k, d], dtype=torch.double)\n        query[0] = torch.Tensor([0, 0])  # First query sample is all zeros\n        query[1] = torch.Tensor([0, 1])\n        query[2] = torch.Tensor([1, 1])\n        support = torch.zeros([k, d], dtype=torch.double)\n        support[0] = torch.Tensor([1, 1])\n        support[1] = torch.Tensor([-1, -1])\n        support[2] = torch.Tensor([0, 0])  # Third support sample is all zeros\n\n        distances = pairwise_distances(query, support, \'cosine\')\n\n        self.assertTrue(torch.isnan(distances).sum() == 0, \'Cosine distances between 0-vectors should not be nan\')\n\n\nclass TestAutogradGraphRetrieval(unittest.TestCase):\n    def test_retrieval(self):\n        """"""Create a simple autograd graph and check that the output is what is expected""""""\n        x = torch.ones(2, 2, requires_grad=True)\n        y = x + 2\n        # The operation on the next line will create two edges because the y variable is\n        # y variable is used twice.\n        z = y * y\n        out = z.mean()\n\n        nodes, edges = autograd_graph(out)\n\n        # This is quite a brittle test as it will break if the names of the autograd Functions change\n        # TODO: Less brittle test\n\n        expected_nodes = [\n            \'MeanBackward1\',\n            \'MulBackward0\',\n            \'AddBackward0\',\n            \'AccumulateGrad\',\n        ]\n\n        self.assertEqual(\n            set(expected_nodes),\n            set(n.__class__.__name__ for n in nodes),\n            \'autograd_graph() must return all nodes in the autograd graph.\'\n        )\n\n        # Check for the existence of the expected edges\n        expected_edges = [\n            (\'MulBackward0\', \'MeanBackward1\'),  # z = y * y, out = z.mean()\n            (\'AddBackward0\', \'MulBackward0\'),    # y = x + 2, z = y * y\n            (\'AccumulateGrad\', \'AddBackward0\'),   # x = torch.ones(2, 2, requires_grad=True), y = x + 2\n        ]\n        for e in edges:\n            self.assertIn(\n                (e[0].__class__.__name__, e[1].__class__.__name__),\n                expected_edges,\n                \'autograd_graph() must return all edges in the autograd graph.\'\n            )\n\n        # Check for two edges between the AddBackward node and the ThMulBackward node\n        num_y_squared_edges = 0\n        for e in edges:\n            if e[0].__class__.__name__ == \'AddBackward0\' and e[1].__class__.__name__ == \'MulBackward0\':\n                num_y_squared_edges += 1\n\n        self.assertEqual(\n            num_y_squared_edges,\n            2,\n            \'autograd_graph() must return multiple edges between nodes if they exist.\'\n        )\n'"
