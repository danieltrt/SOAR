file_path,api_count,code
jupyter_notebook_config.py,0,"b""import os\nc.NotebookApp.ip = '*'\nc.NotebookApp.port = int(os.getenv('PORT', 8888))\nc.NotebookApp.open_browser = False\nc.MultiKernelManager.default_kernel_name = 'python3'\n"""
setup.py,0,"b'from setuptools import find_packages, setup\n\n__author__ = ""Agis Oikonomou""\n\nsetup(name=\'quicknlp\',\n      version=\'0.1.0\',\n      license=\'MIT\',\n      description=\'Pytorch Deep Learning NLP library based on fastai \',\n      author=\'Agis Oikonomou\',\n      url=\'https://github.com/outcastofmusic/quick-nlp\',\n      packages=find_packages(where=\'src\'),\n      package_dir={\'\': \'src\'},\n      python_requires="">=3.6"",\n      tests_require=[\'pytest\', \'pytest-mock\'],\n      )\n'"
src/__init__.py,0,b''
src/quicknlp/__init__.py,0,"b'from .data import *\nfrom .metrics import token_accuracy\nfrom .utils import print_batch, print_dialogue_batch, print_features, print_dialogue_features\n'"
src/quicknlp/callbacks.py,0,b'from fastai.sgdr import Callback\n\n\nclass CVAELossCallback(Callback):\n   pass'
src/quicknlp/metrics.py,3,"b'import torch\nfrom fastai.core import to_np\nimport numpy as np\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\n\ndef token_accuracy(preds, targs):\n    preds = torch.max(preds, dim=-1)[1]\n    return (preds[:-1] == targs.data).float().mean()\n\n\ndef perplexity(preds, targs):\n    return torch.exp(-preds.mean())\n\n\ndef bleu_score(preds, targs, stoi=None):\n    sf = SmoothingFunction().method1\n    preds = torch.max(preds, dim=-1)[1][:-1]\n    bleus = np.zeros(targs.size(1))\n    for res in zip(to_np(targs, preds)):\n        if len(res[1]) > 2:\n            bleu = sentence_bleu([res[1]], res[2], smoothing_function=sf, weights=(1 / 3., 1 / 3., 1 / 3.))\n        elif len(res[1]) == 2:\n            bleu = sentence_bleu([res[1]], res[2], smoothing_function=sf, weights=(0.5, 0.5))\n        else:\n            bleu = sentence_bleu([res[1]], res[2], smoothing_function=sf, weights=(1.0,))\n        bleus.append(bleu)\n    return\n'"
src/quicknlp/stepper.py,2,"b""import torch\nimport torch.nn as nn\n\nfrom fastai.model import Stepper, update_fp32_grads, IS_TORCH_04, trainable_params_, torch_item, copy_fp32_to_model\n\n\nclass S2SStepper(Stepper):\n    def __init__(self, m, opt, crit, clip=0, reg_fn=None, fp16=False, loss_scale=1, teacher_forcing_cycle=None,\n                 max_kld_step: int = None, teacher_forcing_static_prob: float = None):\n        super().__init__(m=m, opt=opt, crit=crit, clip=clip, reg_fn=reg_fn, fp16=fp16,\n                         loss_scale=loss_scale)\n        self.teacher_forcing_epochs = teacher_forcing_cycle  # if None to disable teacher forcing schedule\n        self.max_kld_step = max_kld_step  # optionally used in CVAE\n        self.static_teacher_forcing_probability = teacher_forcing_static_prob  # if a float use a static teacher forcing probability\n\n    def step(self, xs, y, epoch):\n        xtra = []\n        # teacher forcing setup\n        if self.static_teacher_forcing_probability is not None:\n            self.m.pr_force = self.static_teacher_forcing_probability\n        elif self.teacher_forcing_epochs is None:\n            self.m.pr_force = 1.\n        elif 0 <= epoch < self.teacher_forcing_epochs:\n            self.m.pr_force = (self.teacher_forcing_epochs - epoch) / self.teacher_forcing_epochs\n        else:\n            self.m.pr_force = 0.\n        output = self.m(*xs)\n        if isinstance(output, tuple): output, *xtra = output\n        if self.fp16:\n            self.m.zero_grad()\n        else:\n            self.opt.zero_grad()\n        loss = raw_loss = self.crit(output, y, step=epoch, max_kld_step=self.max_kld_step)\n        if self.loss_scale != 1: assert (self.fp16); loss = loss * self.loss_scale\n        if self.reg_fn: loss = self.reg_fn(output, xtra, raw_loss)\n        loss.backward()\n        if self.fp16: update_fp32_grads(self.fp32_params, self.m)\n        if self.loss_scale != 1:\n            for param in self.fp32_params: param.grad.data.div_(self.loss_scale)\n        if self.clip:  # Gradient clipping\n            if IS_TORCH_04:\n                nn.utils.clip_grad_norm_(trainable_params_(self.m), self.clip)\n            else:\n                nn.utils.clip_grad_norm(trainable_params_(self.m), self.clip)\n        if 'wd' in self.opt.param_groups and self.opt.param_groups['wd'] != 0:\n            # Weight decay out of the loss. After the gradient computation but before the step.\n            for group in self.opt.param_groups:\n                lr, wd = group['lr'], group['wd']\n                for p in group['params']:\n                    if p.grad is not None: p.data = p.data.add(-wd * lr, p.data)\n        self.opt.step()\n        if self.fp16:\n            copy_fp32_to_model(self.m, self.fp32_params)\n            torch.cuda.synchronize()\n        return torch_item(raw_loss.data)\n\n    def evaluate(self, xs, y):\n        preds = self.m(*xs)\n        if isinstance(preds, tuple): preds = preds[0]\n        return preds, self.crit(preds, y)\n"""
src/quicknlp/utils.py,4,"b'import json\nfrom functools import partial\nfrom inspect import signature\nfrom operator import itemgetter\nfrom pathlib import Path\nfrom typing import Any, Callable, List, Optional, Sequence, Tuple, Union\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom fastai.core import to_np\nfrom fastai.learner import Learner, ModelData\nfrom nltk.translate.bleu_score import SmoothingFunction\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom tqdm import tqdm\n\nfrom quicknlp.data.model_helpers import BatchBeamTokens\n\nStates = Union[List[Union[Tuple[torch.Tensor, torch.Tensor], torch.Tensor]], torch.Tensor]\n\nHParam = Union[List[int], int]\n\n\nclass RandomUniform:\n\n    def __init__(self, numbers=1000000):\n        self.numbers = numbers\n        self.array = np.random.rand(numbers)\n        self.count = 0\n\n    def __call__(self, *args, **kwargs):\n        if self.count >= self.array.size:\n            self.count = 0\n            self.array = np.random.rand(self.numbers)\n        rand = self.array[self.count]\n        self.count += 1\n        return rand\n\n\ndef concat_layer_bidir_state(states: States, bidir):\n    if isinstance(states, (list, tuple)) and bidir: # lstm case\n        return (states[0].transpose(1, 0).contiguous().view(1, -1, 2 * states[0].size(-1)),\n                states[1].transpose(1, 0).contiguous().view(1, -1, 2 * states[1].size(-1)))\n    elif bidir: # gru case\n        return states.transpose(1, 0).contiguous().view(1, -1, 2 * states[0].size(-1))\n    else:\n        return states\n\n\ndef concat_bidir_state(states: States, bidir: bool, cell_type: str, nlayers: int) -> States:\n    if isinstance(states, list):\n        state = []\n        for index in range(len(states)):\n            state.append(concat_layer_bidir_state(states[index], bidir=bidir))\n    else:\n        state = concat_layer_bidir_state(states, bidir=bidir)\n    return state\n\n\ndef print_dialogue_features(modeldata: ModelData, num_batches: int, num_sentences: int):\n    inputs, responses, targets = [], [], []\n    for *x, y in iter(modeldata.trn_dl):\n        inputs.append(to_np(x[0]))\n        responses.append(to_np(x[1]))\n        targets.append(to_np(y))\n    for batch_num, (input, response, target) in enumerate(zip(inputs, responses, targets)):\n        input = np.transpose(input, [1, 2, 0])  # transpose number of utterances to beams [sl, bs, nb]\n        inputs_str = modeldata.itos(input, ""text"")\n        inputs_str = [""\\n"".join(conv) for conv in inputs_str]\n        targets_str = modeldata.itos(target, ""text"")\n        response_str = modeldata.itos(response, ""text"")\n        for index, (inp, resp, targ) in enumerate(zip(inputs_str, response_str, targets_str)):\n            print(\n                f\'BATCH: {batch_num} SAMPLE : {index}\\nINPUT:\\n{"""".join(inp)}, {len(inp.split())}\\nRESPONSE:\\n{"""".join(resp)}, {len(resp[0].split())}\\nTARGET:\\n{ """".join(targ)}, {len(targ[0].split())}\\n\\n\')\n            if 0 < num_sentences <= index - 1:\n                break\n        if 0 < num_batches <= batch_num - 1:\n            break\n\n\ndef print_features(modeldata: ModelData, num_batches=1, num_sentences=-1):\n    inputs, responses, targets = [], [], []\n    for *x, y in iter(modeldata.trn_dl):\n        inputs.append(to_np(x[0]))\n        responses.append(to_np(x[1]))\n        targets.append(to_np(y))\n    for batch_num, (input, target, response) in enumerate(zip(inputs, targets, responses)):\n        inputs_str: BatchBeamTokens = modeldata.itos(input, modeldata.trn_dl.source_names[0])\n        response_str: BatchBeamTokens = modeldata.itos(response, modeldata.trn_dl.source_names[1])\n        targets_str: BatchBeamTokens = modeldata.itos(target, modeldata.trn_dl.target_names[0])\n        for index, (inp, targ, resp) in enumerate(zip(inputs_str, targets_str, response_str)):\n            print(\n                f\'batch: {batch_num} sample : {index}\\ninput: {"" "".join(inp)}\\ntarget: { "" "".join(targ)}\\nresponse: {"" "".join(resp)}\\n\\n\')\n            if 0 < num_sentences <= index - 1:\n                break\n        if 0 < num_batches <= batch_num - 1:\n            break\n\n\ndef print_batch(learner: Learner, modeldata: ModelData, input_field, output_field, num_batches=1, num_sentences=-1,\n                is_test=False, num_beams=1, weights=None, smoothing_function=None):\n    predictions, targets, inputs = learner.predict_with_targs_and_inputs(is_test=is_test, num_beams=num_beams)\n    weights = (1 / 3., 1 / 3., 1 / 3.) if weights is None else weights\n    smoothing_function = SmoothingFunction().method1 if smoothing_function is None else smoothing_function\n    blue_scores = []\n    for batch_num, (input, target, prediction) in enumerate(zip(inputs, targets, predictions)):\n        inputs_str: BatchBeamTokens = modeldata.itos(input, input_field)\n        predictions_str: BatchBeamTokens = modeldata.itos(prediction, output_field)\n        targets_str: BatchBeamTokens = modeldata.itos(target, output_field)\n        for index, (inp, targ, pred) in enumerate(zip(inputs_str, targets_str, predictions_str)):\n            blue_score = sentence_bleu([targ], pred, smoothing_function=smoothing_function, weights=weights)\n            print(\n                f\'batch: {batch_num} sample : {index}\\ninput: {"" "".join(inp)}\\ntarget: { "" "".join(targ)}\\nprediction: {"" "".join(pred)}\\nbleu: {blue_score}\\n\\n\')\n            blue_scores.append(blue_score)\n            if 0 < num_sentences <= index - 1:\n                break\n        if 0 < num_batches <= batch_num - 1:\n            break\n    print(f\'mean bleu score: {np.mean(blue_scores)}\')\n\n\ndef print_dialogue_batch(learner: Learner, modeldata: ModelData, input_field, output_field, num_batches=1,\n                         num_sentences=-1, is_test=False,\n                         num_beams=1, smoothing_function=None, weights=None):\n    weights = (1 / 3., 1 / 3., 1 / 3.) if weights is None else weights\n    smoothing_function = SmoothingFunction().method1 if smoothing_function is None else smoothing_function\n    predictions, targets, inputs = learner.predict_with_targs_and_inputs(is_test=is_test, num_beams=num_beams)\n    blue_scores = []\n    for batch_num, (input, target, prediction) in enumerate(zip(inputs, targets, predictions)):\n        input = np.transpose(input, [1, 2, 0])  # transpose number of utterances to beams [sl, bs, nb]\n        inputs_str: BatchBeamTokens = modeldata.itos(input, input_field)\n        inputs_str: List[str] = [""\\n"".join(conv) for conv in inputs_str]\n        predictions_str: BatchBeamTokens = modeldata.itos(prediction, output_field)\n        targets_str: BatchBeamTokens = modeldata.itos(target, output_field)\n        for index, (inp, targ, pred) in enumerate(zip(inputs_str, targets_str, predictions_str)):\n            if targ[0].split() == pred[0].split()[1:]:\n                blue_score = 1\n            else:\n                blue_score = sentence_bleu([targ[0].split()], pred[0].split()[1:],\n                                           smoothing_function=smoothing_function,\n                                           weights=weights\n                                           )\n            print(\n                f\'BATCH: {batch_num} SAMPLE : {index}\\nINPUT:\\n{"""".join(inp)}\\nTARGET:\\n{ """".join(targ)}\\nPREDICTON:\\n{"""".join(pred)}\\nblue: {blue_score}\\n\\n\')\n            blue_scores.append(blue_score)\n            if 0 < num_sentences <= index - 1:\n                break\n        if 0 < num_batches <= batch_num - 1:\n            break\n    print(f\'bleu score: mean: {np.mean(blue_scores)}, std: {np.std(blue_scores)}\')\n\n\ndef get_trainable_parameters(model: nn.Module, grad=False) -> List[str]:\n    if grad:\n        return [name for name, param in model.named_parameters() if\n                param.grad is not None and param.requires_grad is True]\n    else:\n        return [name for name, param in model.named_parameters() if param.requires_grad is True]\n\n\ndef get_list(value: Union[List[Any], Any], multiplier: int = 1) -> List[Any]:\n    if isinstance(value, list):\n        assert len(value) == multiplier, f""{value} is not the correct size {multiplier}""\n    else:\n        value = [value] * multiplier\n    return value\n\n\nArray = Union[np.ndarray, torch.Tensor, int, float]\n\n\ndef assert_dims(value: Sequence[Array], dims: List[Optional[int]]) -> Sequence[Array]:\n    """"""Given a nested sequence, with possibly torch or nympy tensors inside, assert it agrees with the\n        dims provided\n\n    Args:\n        value (Sequence[Array]): A sequence of sequences with potentially arrays inside\n        dims (List[Optional[int]]: A list with the expected dims. None is used if the dim size can be anything\n\n    Raises:\n        AssertionError if the value does not comply with the dims provided\n    """"""\n    if isinstance(value, list):\n        if dims[0] is not None:\n            assert len(value) == dims[0], f\'{value} does not match {dims}\'\n            for row in value:\n                assert_dims(row, dims[1:])\n    # support for collections with a shape variable, e.g. torch.Tensor, np.ndarray, Variable\n    elif hasattr(value, ""shape""):\n        shape = value.shape\n        assert len(shape) == len(dims), f\'{shape} does not match {dims}\'\n        for actual_dim, expected_dim in zip(shape, dims):\n            if expected_dim is not None:\n                if isinstance(expected_dim, tuple):\n                    assert actual_dim in expected_dim, f\'{shape} does not match {dims}\'\n                else:\n                    assert actual_dim == expected_dim, f\'{shape} does not match {dims}\'\n    return value\n\n\ndef get_kwarg(kwargs, name, default_value=None, remove=True):\n    """"""Returns the value for the parameter if it exists in the kwargs otherwise the default value provided""""""\n    if remove:\n        value = kwargs.pop(name) if name in kwargs else default_value\n    else:\n        value = kwargs.get(name, default_value)\n    return value\n\n\ndef call_with_signature(callable_fn: Callable, *args, **kwargs):\n    new_kwargs = {}\n    sig = signature(callable_fn)\n    for param in sig.parameters.values():\n        if param.name in kwargs:\n            new_kwargs[param.name] = kwargs[param.name]\n    return callable_fn(*args, **new_kwargs)\n\n\ndef get_pairs_from_dialogues(path_dir, utterance_key, sort_key, role_key, text_key, response_role):\n    for file_index, file in enumerate(path_dir.glob(""*.json"")):\n        with file.open(\'r\', encoding=\'utf-8\') as fh:\n            dialogues = json.load(fh)\n        for dialogue in tqdm(dialogues, desc=f\'processed file {file}\'):\n            if isinstance(sort_key, str):\n                key = itemgetter(sort_key)\n            elif callable(sort_key):\n                key = sort_key\n            else:\n                raise ValueError(""Invalid sort_key provided"")\n            conversation = sorted(dialogue[utterance_key], key=key)\n            text = """"\n            for utterance in conversation:\n                conv_role = ""__"" + utterance[role_key] + ""__""\n                text_with_role = conv_role + "" "" + utterance[text_key]\n                if text != """" and utterance[role_key] == response_role:\n                    yield dict(context=text, response=text_with_role)\n                text += "" "" + text_with_role\n\n\ndef save_pairs_to_tsv(pairs, filename):\n    filename = Path(filename)\n    assert filename.name.endswith("".tsv"")\n    filename.parent.mkdir(exist_ok=True, parents=True)\n    with filename.open(\'w\', encoding=\'utf-8\') as fh:\n        for pair in pairs:\n            fh.write(""{}\\t{}\\n"".format(pair[\'context\'], pair[\'response\']))\n\n\ndef convert_dialogues_to_pairs(path_dir, output_dir, utterance_key, sort_key, role_key, text_key, response_role,\n                               train_path=None, validation_path=None, test_path=None):\n    path_dir = Path(path_dir)\n    iter_func = partial(get_pairs_from_dialogues, utterance_key=utterance_key, sort_key=sort_key,\n                        role_key=role_key, text_key=text_key, response_role=response_role)\n\n    def convert_data(folder):\n        if folder is not None:\n            input_path = path_dir / folder\n            save_pairs_to_tsv(iter_func(input_path), output_dir / folder / ""dialogues.tsv"")\n\n    convert_data(train_path)\n    convert_data(validation_path)\n    convert_data(test_path)\n'"
src/tests/conftest.py,0,"b'from pathlib import Path\n\nimport pandas as pd\nimport pytest\nfrom torchtext.data import Field\n\nfrom quicknlp import HierarchicalModelData\nfrom quicknlp.data import TabularDatasetFromFiles\nfrom quicknlp.data.datasets import HierarchicalDatasetFromDataFrame\nfrom quicknlp.data.s2s_model_data_loader import S2SModelData\nfrom quicknlp.data.torchtext_data_loaders import S2SDataLoader\n\nTRAIN_DATA = \\\n    """"""""hello"",""bonjour"",""Guten Tag""\n    ""goodbye"",""au\'revoir"",""Auf Wieder Sehen""\n    ""I like to read"","" J\'aim lire"",""Ich liebe lesen""\n    ""I am hungry"","" J\'ai faim"","" Ich will essen""\n    """"""\n\nHIERARCHICAL_TRAIN_DATA = \\\n    """"""""chat_id"",""index"",""text"",""role""\n    ""chat_1"",""0"",""hello"",""role1""\n    ""chat_1"",""1"",""hello"",""role2""\n    ""chat_1"",""2"",""I need help"",""role1""\n    ""chat_1"",""3"",""How can I help you"",""role2""\n    ""chat_2"",""0"",""hello, my account is locked"",""role1""\n    ""chat_2"",""1"",""have you tried turning it off and back on again"",""role2""\n    ""chat_2"",""2"",""This is the first thing to try"",""role2""\n    ""chat_2"",""3"",""this never works"",""role1""\n    ""chat_2"",""4"",""sure it does"",""role2""\n    ""chat_2"",""4"",""no it doesn\'t."",""role1""\n    ""chat_3"",""0"",""yo"",""role1""\n    ""chat_3"",""1"",""what\'s up"",""role2""\n    ""chat_4"",""0"",""hey"",""role1""\n    """"""\n\n\n@pytest.fixture()\ndef hierarchical_data(tmpdir):\n    htr = ""\\n"".join(i.lstrip() for i in HIERARCHICAL_TRAIN_DATA.splitlines())\n    data_dir = tmpdir.mkdir(""data"")\n    train = data_dir.mkdir(""train"")\n    train_data = train.join(""data.csv"")\n    with train_data.open(""w"") as fh:\n        for _ in range(1):\n            fh.write(htr)\n    valid = data_dir.mkdir(""valid"")\n    valid_data = valid.join(""data.csv"")\n    with valid_data.open(""w"") as fh:\n        fh.write(htr)\n    test = data_dir.mkdir(""test"")\n    test_data = test.join(""data.csv"")\n    with test_data.open(""w"") as fh:\n        fh.write(htr)\n    return Path(str(data_dir)), str(train.basename), str(valid.basename), str(test.basename)\n\n\n@pytest.fixture()\ndef hierarchical_dataset(hierarchical_data):\n    path, train, valid, test = hierarchical_data\n    df = pd.read_csv(path / train / ""data.csv"", header=None)\n    df.columns = [""chat_id"", ""timestamp"", ""text"", ""role""]\n    field = Field(init_token=""__init__"", eos_token=""__eos__"", lower=True)\n    return HierarchicalDatasetFromDataFrame(df=df, text_field=field, batch_col=""chat_id"",\n                                            sort_col=""timestamp"",\n                                            text_col=""text"", role_col=""role""), field\n\n\n@pytest.fixture()\ndef s2smodel_data(tmpdir):\n    td = ""\\n"".join(i.lstrip() for i in TRAIN_DATA.splitlines())\n    data_dir = tmpdir.mkdir(""data"")\n    train = data_dir.mkdir(""train"")\n    train_data = train.join(""data.csv"")\n    with train_data.open(""w"") as fh:\n        for _ in range(100):\n            fh.write(td)\n    valid = data_dir.mkdir(""valid"")\n    valid_data = valid.join(""data.csv"")\n    with valid_data.open(""w"") as fh:\n        fh.write(td)\n    test = data_dir.mkdir(""test"")\n    test_data = test.join(""data.csv"")\n    with test_data.open(""w"") as fh:\n        fh.write(td)\n    return Path(str(data_dir)), str(train.basename), str(valid.basename), str(test.basename)\n\n\n@pytest.fixture()\ndef s2smodel_loader(s2smodel_data):\n    path, train, valid, test = s2smodel_data\n    fields = [\n        (""english"", Field(init_token=""__init__"", eos_token=""__eos__"", lower=True)),\n        (""french"", Field(init_token=""__init__"", eos_token=""__eos__"", lower=True)),\n        (""german"", Field(init_token=""__init__"", eos_token=""__eos__"", lower=True))\n    ]\n    ds = TabularDatasetFromFiles(path=train, fields=fields)\n\n    for name, field in fields:\n        field.build_vocab(ds)\n    bs = 2\n    ml = S2SDataLoader(dataset=ds, batch_size=bs, source_names=[""english"", ""french""], target_names=[""french""])\n    return ml\n\n\n@pytest.fixture\ndef s2smodel(s2smodel_data):\n    fields = [\n        (""english"", Field(init_token=""__init__"", eos_token=""__eos__"", lower=True)),\n        (""french"", Field(init_token=""__init__"", eos_token=""__eos__"", lower=True)),\n        (""german"", Field(init_token=""__init__"", eos_token=""__eos__"", lower=True))\n    ]\n    path, train, valid, test = s2smodel_data\n    return S2SModelData.from_text_files(path=path, source_names=[""english"", ""german""],\n                                        target_names=[""german""],\n                                        train=train, validation=valid, test=None, fields=fields, bs=2)\n\n\n@pytest.fixture\ndef hredmodel(hierarchical_data):\n    field = Field(init_token=""__init__"", eos_token=""__eos__"", lower=True)\n    path, train, valid, test = hierarchical_data\n    cols = {""text_col"": ""text"", ""sort_col"": ""index"", ""batch_col"": ""chat_id"", ""role_col"": ""role""}\n    return HierarchicalModelData.from_text_files(path=path, text_field=field,\n                                                 train=train,\n                                                 validation=valid,\n                                                 test=None,\n                                                 bs=2,\n                                                 target_names=None,\n                                                 file_format=""csv"",\n                                                 **cols\n                                                 )\n'"
src/tests/test_attention.py,0,"b'import numpy as np\nimport pytest\nfrom fastai.core import T, V, to_gpu\n\nfrom quicknlp.modules.attention import MLPAttention, MultiHeadAttention, SDPAttention\nfrom quicknlp.utils import assert_dims\n\nparams = [(300, 300), (300, 600)]\n\nids = [""same sized"", ""different sized""]\n\n\n@pytest.fixture(params=params, ids=ids)\ndef attention_setup(request):\n    sl, bs = 3, 2\n    edq, edk = request.param\n\n    # query would be the hidden state of the decoder\n    keys = to_gpu(V(T(np.random.rand(sl, bs, edk))))\n    query = to_gpu(V(T(np.random.rand(bs, edq))))\n    return keys, query\n\n\ndef test_MPLPAttention(attention_setup):\n    keys, query = attention_setup\n    ed = keys.size(2)\n    bs = query.size(0)\n    in_features = keys.size(2) + query.size(1)\n    attention = to_gpu(MLPAttention(n_in=in_features, nhid=200))\n    result = attention(query=query, keys=keys, values=keys)\n    assert (bs, ed) == result.shape\n\n\ndef test_SDPAttention(attention_setup):\n    keys, query = attention_setup\n    bs = query.size(0)\n    ed = keys.size(2)\n    eq = query.size(1)\n    attention = to_gpu(SDPAttention(n_in=ed))\n    if ed != eq:\n        with pytest.raises(RuntimeError):\n            result = attention(query=query, keys=keys, values=keys)\n    else:\n        result = attention(query=query, keys=keys, values=keys)\n        assert (bs, ed) == result.shape\n\n\n@pytest.fixture()\ndef self_attention_setup(attention_setup):\n    keys, query = attention_setup\n    query = query.unsqueeze(0).repeat(7, 1, 1)\n    return keys, query\n\n\ndef test_MultiHeadAttention(self_attention_setup):\n    keys, query = self_attention_setup\n    slk, bs, ek = keys.size()\n    slq, bs, eq = query.size()\n    num_heads = 4\n    nhid = 10\n    attention = to_gpu(\n        MultiHeadAttention(num_heads=num_heads, nhid=nhid, keys_dim=ek, query_dim=eq, values_dim=ek, dropout=0.3))\n\n    result = attention(query=V(query), keys=V(keys), values=V(keys))\n    assert_dims(result, [slq, bs, num_heads * nhid])\n\n\ndef test_MultiHeadAttention_with_mask(self_attention_setup):\n    keys, query = self_attention_setup\n    slk, bs, ek = keys.size()\n    slq, bs, eq = query.size()\n    num_heads = 4\n    nhid = 10\n    attention = to_gpu(\n        MultiHeadAttention(num_heads=num_heads, nhid=nhid, keys_dim=ek, query_dim=eq, values_dim=ek, dropout=0.3))\n    mask = T(np.tril(np.ones((bs, num_heads, slq, slk)))).float()\n    result = attention(query=V(query), keys=V(keys), values=V(keys), mask=mask)\n    assert_dims(result, [slq, bs, num_heads * nhid])\n'"
src/tests/test_attention_projection.py,0,"b'import numpy as np\nimport pytest\nfrom fastai.core import T, V, to_gpu, to_np\n\nfrom quicknlp.modules import AttentionProjection\nfrom quicknlp.utils import assert_dims\n\nparams = [(300, 300)]\n\nids = [""same sized""]\n\n\n@pytest.fixture(params=params, ids=ids)\ndef attention_projection_setup(request):\n    sl, bs = 3, 2\n    edq, edk = request.param\n\n    encoder_outputs = V(T(np.random.rand(sl, bs, edk)))\n    # query would be the hidden state of the decoder\n    decoder_output = V(T(np.random.rand(bs, edq)))\n    params = {""output_size"": 10,\n              ""input_size"": edk,\n              ""dropout"": 0.2,\n              ""att_nhid"": 13\n              }\n    return encoder_outputs, decoder_output, params\n\n\ndef test_attention_projection(attention_projection_setup):\n    encoder_outputs, decoder_output, params = attention_projection_setup\n    module = to_gpu(AttentionProjection(**params))\n    # When I reset the module\n    module.reset(keys=encoder_outputs)\n    # the attention output will be a zeros array with shape equal to the input\n    assert to_np(module.get_attention_output(decoder_output)).sum() == 0\n    assert module.get_attention_output(decoder_output) is not module._attention_output\n    # when when I pass an input for the the decoder output\n    results = module(decoder_output)\n    assert_dims(results, [1, 2, params[\'output_size\']])\n    # the new attention_output is calculated from he attention module and is no longer zero\n    assert to_np(module.get_attention_output(decoder_output)).sum() != 0\n    assert module.get_attention_output(decoder_output) is module._attention_output\n    assert_dims(module._attention_output, [2, params[\'input_size\']])\n'"
src/tests/test_cell.py,0,"b'import pytest\nimport torch as tr\nfrom fastai.core import V, Variable, to_gpu\n\nfrom quicknlp.modules.cell import Cell\n\n\n@pytest.mark.parametrize(\'cell_type, hidden_type\',\n                          [(""lstm"", tuple),\n                           (""gru"", Variable)\n                           ])\ndef test_cell(cell_type, hidden_type):\n    sl, bs, input_size, output_size = 8, 10, 12, 14\n    cell = Cell(cell_type, input_size, output_size, dropout=0.0, wdrop=0.0)\n    cell = to_gpu(cell)\n    inputs = V(tr.rand(sl, bs, input_size))\n    hidden = cell.hidden_state(bs)\n    outputs, hidden = cell(inputs, hidden)\n    assert (sl, bs, output_size) == outputs.shape\n    assert isinstance(hidden, hidden_type)\n'"
src/tests/test_cvae.py,1,"b'import pytest\nfrom fastai.core import V, to_gpu\nfrom torch.optim import Adam\n\nfrom quicknlp.data.learners import get_cvae_loss\nfrom quicknlp.data.model_helpers import CVAEModel\nfrom quicknlp.models import CVAE\nfrom quicknlp.utils import get_trainable_parameters\n\nparams = [(True), (False)]\nids = [""bidir"", ""unidir""]\nmodel_type = [""simple"", ""attention""]\n\n\n@pytest.fixture(params=model_type)\ndef model_type(request):\n    return request.param\n\n\n@pytest.fixture(params=params, ids=ids)\ndef model(hredmodel, request):\n    emb_size = 300\n    nh = 1024\n    ntoken = hredmodel.nt\n    model = CVAE(ntoken=ntoken, nhid=nh, nlayers=2, emb_sz=emb_size, pad_token=hredmodel.pad_idx,\n                 eos_token=hredmodel.eos_idx, latent_dim=100, bow_nhid=400, bidir=request.param)\n    model = to_gpu(model)\n    return model\n\n\n@pytest.mark.parametrize(""tchebycheff, sigmoid"", [\n    (True, False),\n    (False, False),\n    (False, True),\n])\ndef test_cvae_training_parameters(model, hredmodel, tchebycheff, sigmoid):\n    *xs, y = next(iter(hredmodel.trn_dl))\n    xs = V(xs)\n    y = V(y)\n    optimizer = Adam(model.parameters())\n    output = model(*xs)\n    optimizer.zero_grad()\n    cvae_loss = get_cvae_loss(pad_idx=hredmodel.pad_idx, tchebycheff=tchebycheff, sigmoid=sigmoid)\n    loss = cvae_loss(input=output[0], target=y)\n    loss.backward()\n    model_parameters = get_trainable_parameters(model)\n    grad_flow_parameters = get_trainable_parameters(model, grad=True)\n    assert set(model_parameters) == set(grad_flow_parameters)\n\n\ndef test_cvae_encoder_decoder_model(model):\n    enc_dec_model = CVAEModel(model)\n    groups = enc_dec_model.get_layer_groups()\n    num_groups = 10\n    if model.share_embedding_layer:\n        num_groups -= 1\n    if model.tie_decoder:\n        num_groups -= 1\n    assert len(groups) == num_groups\n'"
src/tests/test_datasets.py,0,"b'import pandas as pd\nfrom torchtext.data import Field\n\nfrom quicknlp.data import TabularDatasetFromFiles, TabularDatasetFromDataFrame\n\n\ndef test_TabularDatasetFromFiles(s2smodel_data):\n    path, train, valid, test = s2smodel_data\n    fields = [\n        (""english"", Field(init_token=""__init__"", eos_token=""__eos__"", lower=True)),\n        (""french"", Field(init_token=""__init__"", eos_token=""__eos__"", lower=True)),\n        (""german"", Field(init_token=""__init__"", eos_token=""__eos__"", lower=True))\n    ]\n    ds = TabularDatasetFromFiles(path=path / train, fields=fields)\n    assert ds is not None\n    assert 400 == len(ds)\n    assert {""english"", ""french"", ""german""} == ds.fields.keys()\n    for example in ds:\n        example_vars = vars(example)\n        assert ""english"" in example_vars\n        assert ""french"" in example_vars\n        assert ""german"" in example_vars\n\n\ndef test_TabularDatasetFromDataFrame(s2smodel_data):\n    path, train, valid, test = s2smodel_data\n    df = pd.read_csv(path / train / ""data.csv"", header=None)\n    df.columns = [""english"", ""french"", ""german""]\n    df[\'random_column\'] = ""N/A""\n    fields = [\n        (""english"", Field(init_token=""__init__"", eos_token=""__eos__"", lower=True)),\n        (""french"", Field(init_token=""__init__"", eos_token=""__eos__"", lower=True)),\n        (""german"", Field(init_token=""__init__"", eos_token=""__eos__"", lower=True))\n    ]\n    ds = TabularDatasetFromDataFrame(df=df, fields=fields)\n    assert ds is not None\n    assert 400 == len(ds)\n    assert ds.fields.keys() == {""english"", ""french"", ""german""}\n    for example in ds:\n        example_vars = vars(example)\n        assert ""english"" in example_vars\n        assert ""french"" in example_vars\n        assert ""german"" in example_vars\n'"
src/tests/test_decoder.py,0,"b'from types import SimpleNamespace\n\nimport numpy as np\nimport pytest\nfrom fastai.core import T, V, to_gpu, to_np\nfrom numpy.testing import assert_allclose\n\nfrom quicknlp.modules import AttentionDecoder, AttentionProjection, Projection, RNNLayers, TransformerDecoderLayers\nfrom quicknlp.modules.basic_decoder import Decoder, TransformerDecoder, reshape_parent_indices, select_hidden_by_index\nfrom quicknlp.modules.embeddings import DropoutEmbeddings, TransformerEmbeddings\nfrom quicknlp.utils import assert_dims\n\nparams_to_try = [\n    SimpleNamespace(ntokens=4, emb_size=20, nhid=32, nlayers=1, max_tokens=10, batch_size=2, num_beams=0,\n                    attention=False),\n    SimpleNamespace(ntokens=4, emb_size=20, nhid=32, nlayers=1, max_tokens=10, batch_size=2, num_beams=1,\n                    attention=False),\n    SimpleNamespace(ntokens=4, emb_size=20, nhid=32, nlayers=1, max_tokens=10, batch_size=4, num_beams=3,\n                    attention=False),\n    SimpleNamespace(ntokens=4, emb_size=20, nhid=32, nlayers=1, max_tokens=10, batch_size=4, num_beams=0,\n                    attention=True, att_hid=5),\n    SimpleNamespace(ntokens=4, emb_size=20, nhid=32, nlayers=1, max_tokens=10, batch_size=4, num_beams=1,\n                    attention=True, att_hid=5),\n    SimpleNamespace(ntokens=4, emb_size=20, nhid=32, nlayers=1, max_tokens=10, batch_size=4, num_beams=3,\n                    attention=True, att_hid=5),\n    SimpleNamespace(ntokens=4, emb_size=20, nhid=32, nlayers=3, max_tokens=10, batch_size=4, num_beams=0,\n                    attention=True, att_hid=5),\n    SimpleNamespace(ntokens=4, emb_size=20, nhid=32, nlayers=3, max_tokens=10, batch_size=4, num_beams=1,\n                    attention=True, att_hid=5),\n    SimpleNamespace(ntokens=4, emb_size=20, nhid=32, nlayers=3, max_tokens=10, batch_size=4, num_beams=3,\n                    attention=True, att_hid=5)\n]\n\nids = [""teacher_forcing"", ""greedy"", ""beam_search"", ""attention_tf"", ""attention_greedy"", ""attention_beam_search"",\n       ""attention_tf_three_layers"", ""attention_greedy_three_layers"", ""attention_beam_search_three_layers""]\n\n\n@pytest.fixture(scope=""session"", params=params_to_try, ids=ids)\ndef decoder_params(request):\n    return request.param\n\n\n@pytest.fixture(scope=""session"")\ndef rnn_decoder(decoder_params):\n    decoder_embedding_layer = DropoutEmbeddings(ntokens=decoder_params.ntokens,\n                                                emb_size=decoder_params.emb_size,\n                                                )\n\n    if decoder_params.attention:\n        # attention decoder must have double the input_size to accommodate for the attention concat\n        decoder_rnn = RNNLayers(input_size=decoder_params.emb_size * 2, output_size=decoder_params.emb_size,\n                                nhid=decoder_params.nhid, bidir=False,\n                                nlayers=decoder_params.nlayers, cell_type=""gru"")\n        projection_layer = AttentionProjection(output_size=decoder_params.ntokens,\n                                               input_size=decoder_params.emb_size,\n                                               att_nhid=decoder_params.att_hid,\n                                               tie_encoder=None,\n                                               dropout=0.0)\n        decoder = AttentionDecoder(decoder_layer=decoder_rnn, embedding_layer=decoder_embedding_layer,\n                                   projection_layer=projection_layer,\n                                   pad_token=1, eos_token=2,\n                                   max_tokens=decoder_params.max_tokens)\n\n    else:\n\n        decoder_rnn = RNNLayers(input_size=decoder_params.emb_size, output_size=decoder_params.emb_size,\n                                nhid=decoder_params.nhid, bidir=False,\n                                nlayers=decoder_params.nlayers, cell_type=""gru"")\n        projection_layer = Projection(output_size=decoder_params.ntokens, input_size=decoder_params.emb_size,\n                                      dropout=0.0,\n                                      tie_encoder=None\n                                      )\n        decoder = Decoder(\n            decoder_layer=decoder_rnn,\n            projection_layer=projection_layer,\n            embedding_layer=decoder_embedding_layer,\n            pad_token=0,\n            eos_token=1,\n            max_tokens=decoder_params.max_tokens,\n        )\n    decoder = to_gpu(decoder)\n    decoder.reset(decoder_params.batch_size)\n    return decoder, decoder_params\n\n\ndef test_select_hidden_by_index():\n    bs, num_beams = 2, 3\n    # when I pass inputs to the select_hidden_by_index function with bs=2, num_beams = 3\n    inputs = np.array([2, 3, 4, 10, 11, 12]).reshape(1, 6, 1)  # [ndir, bs, hd]\n    tr_inputs = [V(T(inputs))]\n    # and  indices for every batch [bs, ndims]\n    indices = np.array([[0, 0, 1], [2, 2, 2]])\n    tr_indices = V(T(indices))\n    tr_indices = reshape_parent_indices(tr_indices.view(-1), bs=bs, num_beams=num_beams)\n    results = select_hidden_by_index(tr_inputs, tr_indices.view(-1))\n    # then I get the expected seletec hidden\n    expected = np.array([2, 2, 3, 12, 12, 12])\n    assert_allclose(actual=to_np(results[0]).ravel(), desired=expected)\n\n\n@pytest.fixture()\ndef decoder_inputs(decoder_params):\n    batch_size = decoder_params.batch_size\n    inputs = np.zeros(batch_size, dtype=np.int).reshape(1, batch_size)\n    enc_inputs = np.random.rand(1, decoder_params.batch_size, decoder_params.emb_size)\n    vin = V(T(inputs))\n    ven = V(T(enc_inputs))\n    return vin, ven\n\n\ndef test_rnn_decoder(rnn_decoder, decoder_inputs):\n    dec_ins, keys = decoder_inputs\n    decoder, params = rnn_decoder\n    decoder.reset(params.batch_size)\n    hidden = decoder.hidden\n    decoder.projection_layer.keys = keys\n    outputs = decoder(dec_ins, hidden=hidden, num_beams=params.num_beams)\n    if params.num_beams > 0:\n        assert_dims(outputs,\n                    [None, params.num_beams * params.batch_size, (params.nhid, params.ntokens)])\n        # actual beam outputs can be found in beam_outputs\n        assert decoder.beam_outputs is not None\n        assert_dims(decoder.beam_outputs, [None, params.batch_size, params.num_beams])\n        # the sl can go up to max_tokens + 1(for the extra 0 token at the end)\n        assert 0 < decoder.beam_outputs.shape[0] <= params.max_tokens + 1\n    else:\n        assert_dims(outputs, [None, params.batch_size, (params.nhid, params.ntokens)])\n        assert decoder.beam_outputs is None\n\n\n@pytest.fixture()\ndef decoder_inputs_transformer():\n    batch_size = 2\n    emb_size = 12\n    nlayers = 8\n    sl = 3\n    inputs = np.zeros(batch_size, dtype=np.int).reshape(1, batch_size)\n    enc_inputs = np.random.rand(nlayers, sl, batch_size, emb_size)\n    vin = V(T(inputs))\n    ven = V(T(enc_inputs))\n    return batch_size, emb_size, nlayers, sl, vin, ven\n\n\n@pytest.mark.parametrize(""num_beams"", [0, 1, 2], ids=[""teacher_forcing"", ""greedy"", ""beam_search""])\ndef test_transformer_decoder(num_beams, decoder_inputs_transformer):\n    batch_size, emb_size, nlayers, sl, vin, ven = decoder_inputs_transformer\n    ntokens, nhid, max_tokens = 10, 2, 20\n    embedding = TransformerEmbeddings(ntokens=ntokens, emb_size=emb_size,\n                                      dropout=0.0,\n                                      pad_token=1)\n\n    encoder = TransformerDecoderLayers(nlayers=nlayers, input_size=emb_size,\n                                       num_heads=2, nhid=emb_size)\n    projection_layer = Projection(output_size=ntokens,\n                                  input_size=emb_size, tie_encoder=None, dropout=0.0)\n    decoder = TransformerDecoder(decoder_layer=encoder, projection_layer=projection_layer, pad_token=1, eos_token=2,\n                                 max_tokens=max_tokens,\n                                 embedding_layer=embedding)\n    decoder = to_gpu(decoder)\n    outputs = decoder(vin, ven, num_beams=num_beams)\n    if num_beams > 0:\n        assert_dims(outputs,\n                    [None, num_beams * batch_size, (emb_size, ntokens)])\n        # actual beam outputs can be found in beam_outputs\n        assert decoder.beam_outputs is not None\n        assert_dims(decoder.beam_outputs, [None, batch_size, num_beams])\n        # the sl can go up to max_tokens + 1(for the extra 0 token at the end)\n        assert 0 < decoder.beam_outputs.shape[0] <= max_tokens + 1\n    else:\n        assert_dims(outputs, [None, batch_size, (emb_size, ntokens)])\n        assert decoder.beam_outputs is None\n'"
src/tests/test_hierarchical_data_loader.py,0,"b'from quicknlp.data.torchtext_data_loaders import HierarchicalDataLoader\n\n\ndef test_hierarchical_data_loader(hierarchical_dataset):\n    # When I crate a Hierarchical Model loader\n    ds, field = hierarchical_dataset\n    field.build_vocab(ds)\n    dl = HierarchicalDataLoader(ds, batch_size=2, target_names=[""__role2__""])\n\n    # Then I expect every batch to have a context and a response and targets\n    for batch in dl:\n        assert len(batch) == 3\n        # and I expect the features to be 3D for context [conv_length,sequence_length, bs]\n        assert len(batch[0].shape) == 3\n        # and I expect the features to be 2D [sequence_length, bs] for response and targets\n        assert len(batch[1].shape) == 2\n        assert len(batch[2].shape) == 2\n'"
src/tests/test_hierarichal_datasets.py,0,"b'import pandas as pd\nfrom torchtext.data import Field\n\nfrom quicknlp.data.datasets import HierarchicalDatasetFromDataFrame\n\n\ndef test_tabular_dataset_from_dataframe(hierarchical_data):\n    path, train, valid, test = hierarchical_data\n    df = pd.read_csv(path / train / ""data.csv"", header=None)\n    df.columns = [""chat_id"", ""timestamp"", ""text"", ""role""]\n    field = Field(pad_token=""__pad__"", init_token=""__init__"", eos_token=""__eos__"", lower=True)\n    # When I create a hierarchical Dataset\n    ds = HierarchicalDatasetFromDataFrame(df=df, text_field=field, batch_col=""chat_id"",\n                                          sort_col=""timestamp"",\n                                          text_col=""text"", role_col=""role"")\n    assert ds is not None\n    assert 3 == len(ds)\n    # Then every batch is a conversation\n    for example in ds:\n        example_vars = vars(example)\n        assert ""text"" in example_vars\n        # and every utterance starts with the role\n        assert example.text[0].startswith(""__role1__"")\n        assert ""sl"" in example_vars\n        assert ""roles"" in example_vars\n        # and the example.sl has the sequence length of every utterance in the conversation\n        assert len(example.text) == sum(example.sl)\n'"
src/tests/test_hred.py,1,"b'import pytest\nfrom fastai.core import V, to_gpu\nfrom torch.optim import Adam\n\nfrom quicknlp.data.learners import decoder_loss\nfrom quicknlp.data.model_helpers import HREDModel\nfrom quicknlp.models import HRED\nfrom quicknlp.utils import get_trainable_parameters\n\nparams = [\n    (True, 1, True, False, False, ""gru""),\n    (True, 2, True, False, False, ""gru""),\n    (False, 1, False, False, False, ""lstm""),\n    (True, 1, True, False, False, ""lstm""),\n    (True, 1, True, True, False, ""gru""),\n    (True, 3, True, True, False, ""lstm""),\n    (True, 1, True, True, True, ""gru""),\n    (True, 3, True, True, True, ""lstm""),\n]\n\n\n@pytest.fixture(params=params)\ndef model(hredmodel, request):\n    bidir, nlayers, share_embedding_layer, tie_decoder, session_constraint, cell_type = request.param\n    emb_size = 300\n    nh = 1024\n    ntoken = hredmodel.nt\n    model = HRED(ntoken=ntoken, nhid=nh, nlayers=nlayers, emb_sz=emb_size, pad_token=hredmodel.pad_idx,\n                 share_embedding_layer=share_embedding_layer, tie_decoder=tie_decoder,\n                 session_constraint=session_constraint, cell_type=cell_type,\n                 eos_token=hredmodel.eos_idx, bidir=bidir)\n    model = to_gpu(model)\n    return model\n\n\ndef test_hred_training_parameters(model, hredmodel):\n    *xs, y = next(iter(hredmodel.trn_dl))\n    xs = V(xs)\n    y = V(y)\n    optimizer = Adam(model.parameters())\n    output = model(*xs)\n    optimizer.zero_grad()\n    loss = decoder_loss(input=output[0], target=y, pad_idx=hredmodel.pad_idx)\n    loss.backward()\n    model_parameters = get_trainable_parameters(model)\n    grad_flow_parameters = get_trainable_parameters(model, grad=True)\n    assert set(model_parameters) == set(grad_flow_parameters)\n\n\ndef test_hred_encoder_decoder_model(model):\n    enc_dec_model = HREDModel(model)\n    groups = enc_dec_model.get_layer_groups()\n    num_groups = 7\n    if model.share_embedding_layer:\n        num_groups -= 1\n    if model.tie_decoder:\n        num_groups -= 1\n    assert len(groups) == num_groups\n'"
src/tests/test_iterator.py,0,"b'import pytest\n\nfrom quicknlp.data.iterators import HierarchicalIterator\nfrom quicknlp.utils import assert_dims\n\n\n@pytest.fixture()\ndef hiterator(hierarchical_dataset):\n    ds, field = hierarchical_dataset\n    field.build_vocab(ds)\n    return HierarchicalIterator(ds, batch_size=2, sort_key=lambda x: len(x.roles)), field\n\n\ndef test_hierarchical_iterator_padding(hiterator):\n    max_sl = 15\n    max_conv = 12\n    iterator, field = hiterator\n    padded, lens, padded_roles = iterator.pad(iterator.dataset[0], max_sl=max_sl, max_conv=max_conv, field=field)\n    assert max_conv == len(padded)\n    assert max_conv == len(padded_roles)\n    for row in padded:\n        assert max_sl == len(row)\n\n\ndef test_hierarchical_iterator_padding_filter(hiterator):\n    max_sl = 15\n    max_conv = 12\n    iterator, field = hiterator\n    target_role = ""__role1__""\n    padded, lens, padded_roles = iterator.pad(iterator.dataset[0], max_sl=max_sl, max_conv=max_conv, field=field,\n                                              target_roles=target_role)\n\n    for row, role in zip(padded, padded_roles):\n        if role != target_role:\n            assert all([i == field.pad_token for i in row])\n\n\ndef test_hierarchical_iterator_process_minibatch(hiterator):\n    iterator, field = hiterator\n    bs = 2\n    minibatch = iterator.dataset[:bs]\n    cl = max([len(ex.roles) for ex in minibatch])\n    sl = max([sl for ex in minibatch for sl in ex.sl])\n    x0, x1, y = iterator.process_minibatch(minibatch)\n    assert_dims(x0, [cl - 1, sl, bs])\n    assert_dims(x1, [cl - 1, sl, bs])\n    assert_dims(y, [cl - 1, sl - 1, bs])\n\n\ndef test_hierarchical_iterator(hiterator):\n    # When I crate a Hierarchical Data loader\n    iterator, field = hiterator\n    # Then I expect every batch to have a source and target\n    dliter = iter(iterator)\n    batch = next(dliter)\n    # and I expect the features to be 3D [conv_length, sequence_length, batch_size]\n    assert len(batch.context.shape) == 3\n    # responses and targets have shape [sequence_length, batch_size]\n    assert len(batch.response.shape) == 2\n    assert len(batch.targets.shape) == 2\n\n    assert_dims(batch.context, [None, None, batch.batch_size])\n    assert_dims(batch.response, [None, batch.batch_size])\n    assert_dims(batch.targets, [None, batch.batch_size])\n    assert (batch.response[1:] == batch.targets).all()\n'"
src/tests/test_rnn_encoder.py,0,"b'import numpy as np\nfrom fastai.core import T, V, to_gpu, to_np\n\nfrom quicknlp.modules import RNNLayers\nfrom quicknlp.modules.basic_encoder import Encoder\nfrom quicknlp.modules.embeddings import DropoutEmbeddings\nfrom quicknlp.utils import assert_dims\n\n\ndef test_BiRNNEncoder():\n    ntoken = 4\n    emb_sz = 2\n    nhid = 6\n    nlayers = 2\n    # Given a birnnencoder\n\n    embedding = DropoutEmbeddings(ntokens=ntoken, emb_size=emb_sz, pad_token=0,\n                                  dropouti=0.0, dropoute=0.0)\n    rnn_layers = RNNLayers(input_size=emb_sz,\n                           nhid=nhid,\n                           nlayers=nlayers,\n                           output_size=emb_sz,\n                           dropouth=0.0,\n                           wdrop=0.0,\n                           )\n    encoder = Encoder(embedding_layer=embedding, encoder_layer=rnn_layers)\n\n    encoder = to_gpu(encoder)\n    assert encoder is not None\n\n    weight = encoder.embedding_layer.weight\n    assert (4, 2) == weight.shape\n    sl = 2\n    bs = 3\n    np.random.seed(0)\n    inputs = np.random.randint(0, ntoken, sl * bs).reshape(sl, bs)\n    vin = V(T(inputs))\n    # Then the initial output states should be zero\n    encoder.reset(bs)\n    initial_hidden = encoder.encoder_layer.hidden\n    h = []\n    c = []\n    for layer in initial_hidden:\n        h.append(layer[0].data.cpu().numpy())\n        c.append(layer[1].data.cpu().numpy())\n        assert h[-1].sum() == 0\n        assert c[-1].sum() == 0\n    embeddings = encoder.embedding_layer(vin)\n    assert (2, 3, emb_sz) == embeddings.shape\n\n    # Then the the new states are different from before\n    outputs = encoder(vin)\n    assert_dims(outputs, [nlayers, sl, bs, (nhid, encoder.output_size)])\n    initial_hidden = encoder.encoder_layer.hidden\n    h1 = []\n    c1 = []\n    for hl, cl, layer in zip(h, c, initial_hidden):\n        h1.append(to_np(layer[0]))\n        c1.append(to_np(layer[0]))\n        assert ~np.allclose(hl, h1[-1])\n        assert ~np.allclose(cl, c1[-1])\n\n    # Then the the new states are different from before\n    outputs = encoder(vin)\n    assert_dims(outputs, [nlayers, sl, bs, (nhid, encoder.output_size)])\n    initial_hidden = encoder.encoder_layer.hidden\n\n    for hl, cl, layer in zip(h1, c1, initial_hidden):\n        h_new = to_np(layer[0])\n        c_new = to_np(layer[0])\n        assert ~np.allclose(hl, h_new)\n        assert ~np.allclose(cl, c_new)\n'"
src/tests/test_sentence_data.py,1,"b'import pytest\nfrom fastai.core import to_np\nfrom torch.optim import Adam\nfrom torchtext.data import Field\n\nfrom quicknlp.data import TabularDatasetFromFiles\nfrom quicknlp.data.s2s_model_data_loader import S2SModelData\nfrom quicknlp.data.torchtext_data_loaders import S2SDataLoader\nfrom quicknlp.utils import assert_dims\n\nHAVE_TEST = [True, False]\nHAVE_TEST_IDS = [""TestData"", ""NoTestData""]\n\n\ndef test_S2SModelLoader(s2smodel_data):\n    path, train, valid, test = s2smodel_data\n    fields = [\n        (""english"", Field(init_token=""__init__"", eos_token=""__eos__"", lower=True)),\n        (""french"", Field(init_token=""__init__"", eos_token=""__eos__"", lower=True)),\n        (""german"", Field(init_token=""__init__"", eos_token=""__eos__"", lower=True))\n    ]\n    ds = TabularDatasetFromFiles(path=path / train, fields=fields)\n    for name, field in fields:\n        field.build_vocab(ds)\n    bs = 2\n    ml = S2SDataLoader(dataset=ds, batch_size=bs, source_names=[""english"", ""french""], target_names=[""french""])\n    assert len(ml) == 200\n    index = 0\n    for index, (*X, Y) in enumerate(ml):\n        assert_dims(X, [2, None, (1, bs)])\n        assert_dims(Y, [None, (1, bs)])\n\n        assert X[1].shape[0] == Y.shape[0] + 1\n\n    assert len(ml) == index + 1\n\n\n@pytest.fixture(params=HAVE_TEST, ids=HAVE_TEST_IDS)\ndef generalmodel(s2smodel_data, request):\n    fields = [\n        (""english"", Field(init_token=""__init__"", eos_token=""__eos__"", lower=True)),\n        (""french"", Field(init_token=""__init__"", eos_token=""__eos__"", lower=True)),\n        (""german"", Field(init_token=""__init__"", eos_token=""__eos__"", lower=True))\n    ]\n    path, train, valid, test = s2smodel_data\n    test = test if request.param else None\n    data = S2SModelData.from_text_files(path=path, source_names=[""english"", ""french"", ""german""],\n                                        target_names=[""german""],\n                                        train=train, validation=valid, test=test, fields=fields, bs=2)\n    return data\n\n\ndef test_S2SModelData_from_file(generalmodel):\n    assert generalmodel is not None\n    # number of batches\n    assert 200 == len(generalmodel.trn_dl)\n    train_iter = iter(generalmodel.trn_dl)\n    batch = next(train_iter)\n    assert isinstance(batch, list)\n    # shape should be equal to sl, bs\n    # The elements in the batch equal the sum of source_names and target_names (in this case 4)\n    # the first three being the sources (inputs to the encoder, and the last the target_names (input to the decoder)\n    assert_dims(batch, [4, None, 2])\n\n    sentences = to_np(batch[0])\n    batch_sentences = generalmodel.itos(sentences, ""english"")\n    for beam_sentence in batch_sentences:\n        for sentence in beam_sentence:\n            assert sentence in {""goodbye"", ""hello"", ""i like to read"", ""i am hungry""}\n\n\ndef test_S2SModelData_learner(s2smodel):\n    max_tokens, bs = 4, 2\n    s2slearner = s2smodel.get_model(opt_fn=Adam, max_tokens=max_tokens)\n    assert s2slearner is not None\n    # got predictions targets\n    results = s2slearner.predict_with_targs()\n    assert 2 == len(results)\n    predict_results = s2slearner.predict()\n    # got a list of predictions for every batch\n    assert len(s2smodel.val_dl) == len(predict_results)\n    # predict_results has shape: sl, bs\n    assert len(predict_results) == 2\n    assert predict_results[0].shape[0] <= max_tokens + 1\n    assert predict_results[0].shape[1] == bs\n    for result_batch in predict_results:\n        text_results = s2smodel.itos(result_batch, ""german"")\n        assert len(text_results) == result_batch.shape[1]\n\n\ndef test_S2SModelData_learner_summary(s2smodel):\n    s2slearner = s2smodel.get_model(opt_fn=Adam, max_tokens=4)\n    s2slearner.summary()\n'"
src/tests/test_seq2seq.py,1,"b'import pytest\nfrom fastai.core import V, to_gpu\nfrom torch.optim import Adam\n\nfrom quicknlp.data.model_helpers import S2SModel\nfrom quicknlp.data.learners import decoder_loss\nfrom quicknlp.models import Seq2Seq\nfrom quicknlp.models.seq2seq_attention import Seq2SeqAttention\nfrom quicknlp.utils import get_trainable_parameters\n\nparams = [(True), (False)]\nids = [""bidir"", ""unidir""]\n\nmodel_type = [""simple"", ""attention""]\n\n\n@pytest.fixture(params=model_type)\ndef model_type(request):\n    return request.param\n\n\n@pytest.fixture(params=params, ids=ids)\ndef model(s2smodel, model_type, request):\n    emb_size = 300\n    nh = 1024\n    tnh = 512\n    ntoken = [s2smodel.nt[name] for name in s2smodel.trn_dl.source_names]\n    if model_type == ""attention"":\n        model = Seq2SeqAttention(ntoken=ntoken, nhid=nh, nlayers=2, emb_sz=emb_size, pad_token=s2smodel.pad_idx,\n                                 eos_token=s2smodel.eos_idx, bidir=request.param, att_nhid=tnh)\n    else:\n        model = Seq2Seq(ntoken=ntoken, nhid=nh, nlayers=2, emb_sz=emb_size, pad_token=s2smodel.pad_idx,\n                        eos_token=s2smodel.eos_idx, bidir=request.param)\n    model = to_gpu(model)\n    return model\n\n\ndef test_seq2seq_training_parameters(model, s2smodel):\n    *xs, y = next(iter(s2smodel.trn_dl))\n    xs = V(xs)\n    y = V(y)\n    optimizer = Adam(model.parameters())\n    output = model(*xs)\n    optimizer.zero_grad()\n    loss = decoder_loss(input=output[0], target=y, pad_idx=s2smodel.pad_idx)\n    loss.backward()\n    model_parameters = get_trainable_parameters(model)\n    grad_flow_parameters = get_trainable_parameters(model, grad=True)\n    assert set(model_parameters) == set(grad_flow_parameters)\n\n\ndef test_seq2seq_encoder_decoder_model(model):\n    enc_dec_model = S2SModel(model)\n    groups = enc_dec_model.get_layer_groups()\n    assert len(groups) == 2\n'"
src/tests/test_spacy_tokenizer.py,0,"b'import subprocess\n\nimport pytest\nfrom quicknlp.data import SpacyTokenizer\n\n\n@pytest.fixture(scope=""session"")\ndef spacy_en():\n    try:\n        import spacy\n        spacy.load(""en"")\n    except IOError as e:\n        subprocess.run([""python"", ""-m"", ""spacy"", ""download"", ""en""])\n\n\n@pytest.fixture()\ndef tokenizer(spacy_en):\n    return SpacyTokenizer()\n\n\ndef test_spacy_tokenizer(tokenizer):\n    sentence = ""You guys, you guys! Chef is going away. \\n""\n\n    expected_results = [""You"", ""guys"", "","", ""you"", ""guys"", ""!"", ""Chef"", ""is"", ""going"", ""away"", ""."", ""\\n""]\n    results = tokenizer(sentence)\n    assert len(results) == 12\n    assert results == expected_results\n\n\ndef test_spacy_tokenizer_regex_patterns(spacy_en):\n    tokenizer = SpacyTokenizer(regex_cases=[r\'__name_\\w+__\'])\n    sentence = ""You guys, you guys! __name_john_doe__ Chef is going away. \\n""\n    expected_results = [""You"", ""guys"", "","", ""you"", ""guys"", ""!"", ""__name_john_doe__"", ""Chef"", ""is"", ""going"", ""away"", ""."",\n                        ""\\n""]\n    results = tokenizer(sentence)\n    assert len(results) == 13\n    assert results == expected_results\n\n\n# def test_spacy_tokenizer_reverse(tokenizer):\n#     tokenizer.reverse = True\n#     sentence = ""You guys, you guys! Chef is going away. \\n""\n#\n#     expected_results = [""You"", ""guys"", "","", ""you"", ""guys"", ""!"", ""Chef"", ""is"", ""going"", ""away"", ""."", ""\\n""][::-1]\n#     results = tokenizer(sentence)\n#     assert len(results) == 12\n#     assert results == expected_results\n\n\ndef test_spacy_tokenizer_sentences(tokenizer):\n    sentence = ""You guys, you guys! Chef is going away. \\nGoing away? For how long?\\n""\n\n    expected_results = [\n        [""You"", ""guys"", "","", ""you"", ""guys"", ""!""],\n        [""Chef"", ""is"", ""going"", ""away"", ""."", ""\\n""],\n        [""Going"", ""away"", ""?""],\n        [""For"", ""how"", ""long"", ""?"", ""\\n""]\n    ]\n    results = tokenizer(sentence, sentence=True)\n    assert len(results) == 4\n    for index in range(len(results)):\n        assert results[index] == expected_results[index]\n'"
src/tests/test_transformer.py,1,"b'from fastai.core import V, to_gpu\nfrom torch.optim import Adam\n\nfrom quicknlp.data.learners import decoder_loss\nfrom quicknlp.models.transformer import Transformer\nfrom quicknlp.utils import get_trainable_parameters\n\n\n\ndef test_model(s2smodel):\n    ntoken = [s2smodel.nt[name] for name in s2smodel.trn_dl.source_names]\n    model = Transformer(ntoken=ntoken, max_tokens=5, eos_token=s2smodel.eos_idx)\n    model = to_gpu(model)\n    *xs, y = next(iter(s2smodel.trn_dl))\n    xs = V(xs)\n    y = V(y)\n    optimizer = Adam(model.parameters())\n    output = model(*xs)\n    optimizer.zero_grad()\n    loss = decoder_loss(input=output[0], target=y, pad_idx=s2smodel.pad_idx)\n    loss.backward()\n    model_parameters = get_trainable_parameters(model)\n    grad_flow_parameters = get_trainable_parameters(model, grad=True)\n    assert set(model_parameters) == set(grad_flow_parameters)\n'"
src/tests/test_transformer_modules.py,0,"b'import torch as tr\nfrom fastai.core import T, V, to_gpu\n\nfrom quicknlp.modules.transformer import AttentionLayer, TransformerDecoderLayers, TransformerEncoderLayers, \\\n    TransformerLayer, TransformerLayerDecoder\nfrom quicknlp.utils import assert_dims\n\n\ndef test_attention_layer():\n    sl = 2\n    bs = 2\n    in_features = 32\n    tr.random.manual_seed(0)\n    inputs = to_gpu(V(tr.randn([sl, bs, in_features])))\n    layer = to_gpu(AttentionLayer(input_size=32, num_heads=4, dropout=0.0))\n    outputs1 = layer(inputs, inputs, inputs, mask=True)\n    assert_dims(outputs1, [sl, bs, in_features])\n\n    outputs2 = layer(inputs[:1], inputs[:1], inputs[:1])\n    assert_dims(outputs2, [1, bs, in_features])\n    assert ((outputs1[0] - outputs2[0]).abs() < 1E-6).all()\n\n    outputs = layer(inputs, inputs, inputs, mask=False)\n    assert_dims(outputs, [sl, bs, in_features])\n    assert (outputs[0] != outputs1[0]).all()\n    assert (outputs[0] != outputs2[0]).all()\n\n\ndef test_transfomer_layer():\n    sl = 10\n    bs = 2\n    in_features = 32\n    inputs = tr.randn([sl, bs, in_features])\n    inputs = to_gpu(V(T(inputs)))\n    transfomer = to_gpu(TransformerLayer(input_size=in_features, num_heads=8, nhid=64))\n    outputs = transfomer(inputs)\n    assert_dims(outputs, [sl, bs, in_features])\n\n\ndef test_transfomer_layer_decoder():\n    sl = 10\n    bs = 2\n    in_features = 32\n    tr.random.manual_seed(0)\n    encoder_inputs = tr.randn([sl, bs, in_features])\n    decoder_inputs = tr.randn([sl, bs, in_features])\n    encoder_inputs = to_gpu(V(T(encoder_inputs)))\n    decoder_inputs = to_gpu(V(T(decoder_inputs)))\n    transformer = to_gpu(TransformerLayerDecoder(input_size=in_features, num_heads=8, nhid=64, dropout=0))\n    outputs = transformer(encoder_inputs, decoder_inputs)\n    assert_dims(outputs, [sl, bs, in_features])\n    outputs1 = transformer(encoder_inputs, decoder_inputs[:1])\n    assert_dims(outputs1, [1, bs, in_features])\n    assert ((outputs[0] - outputs1[0]).abs() < 1E-6).all()\n\n\ndef test_transformer_encoder():\n    sl = 10\n    bs = 2\n    in_features = 300\n    num_layers = 5\n    inputs = tr.randn([sl, bs, in_features])\n    inputs = to_gpu(V(T(inputs)))\n    transformer = to_gpu(TransformerEncoderLayers(input_size=in_features, num_heads=8, nhid=512, num_layers=num_layers))\n    layer_outputs = transformer(inputs)\n    assert_dims(layer_outputs, [num_layers, sl, bs, in_features])\n\n\ndef test_transformer_decoder_layers():\n    sl = 10\n    bs = 2\n    in_features = 32\n    num_layers = 5\n    inputs = tr.randn([sl, bs, in_features])\n    encoder_inputs = to_gpu(V(T(tr.randn([num_layers, sl, bs, in_features]))))\n    inputs = to_gpu(V(T(inputs)))\n    transformer = to_gpu(\n        TransformerDecoderLayers(input_size=in_features, num_heads=8, nhid=512, nlayers=num_layers, dropout=0.0))\n    assert transformer.hidden is None\n    layer_outputs = transformer(inputs, encoder_inputs)\n    assert_dims(layer_outputs, [num_layers, sl, bs, in_features])\n    assert transformer.hidden is None\n    # Passing through tht decoderlayers only one output I should be getting the same output\n    layer_outputs2 = transformer(inputs[:1], encoder_inputs)\n    assert_dims(layer_outputs2, [num_layers, 1, bs, in_features])\n    for layer1, layer2 in zip(layer_outputs, layer_outputs2):\n        assert ((layer1[0] - layer2[0]).abs() < 1E-6).all()\n'"
src/tests/test_utils.py,8,"b'import numpy as np\nimport pytest\nimport torch\nfrom fastai.core import V\n\nfrom quicknlp.modules.cell import Cell\nfrom quicknlp.utils import assert_dims, concat_bidir_state\n\n\n@pytest.mark.parametrize(\'mapping, dims\',\n                         [\n                             ([[[2, 3, 4]]], [1, 1, 3]),\n                             ([[[torch.zeros(10, 2, 3)]]], [1, 1, 1, 10, None, None]),\n                             (torch.zeros(10, 2, 3, 4), [None, 2, None, 4]),\n                             ([torch.zeros(10, 2, 3, 4) for _ in range(3)], [3, None, 2, None, 4]),\n                             ([torch.zeros(10, 2, 1 + i % 2, 4) for i in range(3)], [3, None, 2, (1, 2), 4]),\n                             ([np.zeros((10, 2, 3, 4)) for _ in range(5)], [5, None, 2, None, 4]),\n                             ([V(torch.zeros(10, 2, 1 + i % 2, 4)) for i in range(3)], [3, None, 2, (1, 2), 4])\n                         ]\n                         )\ndef test_assert_dims(mapping, dims):\n    mapping_2 = assert_dims(mapping, dims)\n    assert mapping_2 is mapping\n\n\n@pytest.mark.parametrize(\'mapping, dims\',\n                         [\n                             ([[[2, 3, 4]]], [1, 1, 1, 3]),\n                             (torch.zeros(10, 2, 3, 4), [1, 2, None, 4]),\n                             ([torch.zeros(10, 2, 3, 4) for _ in range(5)], [3, None, 2, None, 4]),\n                             ([np.zeros((10, 2, 3, 4)) for _ in range(5)], [3, None, 2, None, 4]),\n                             ([V(torch.zeros(10, 3, 4)) for i in range(3)], [3, None, 2, 4])\n                         ]\n                         )\ndef test_assert_fail_dims(mapping, dims):\n    with pytest.raises(AssertionError):\n        assert_dims(mapping, dims)\n\n\n@pytest.mark.parametrize(\'cell_type, input_size, output_size, bidir\',\n                         [\n                             (""gru"", 256, 256, True),\n                             (""gru"", 256, 256, False),\n                             (""lstm"", 256, 256, False),\n                             (""lstm"", 256, 256, True),\n                         ]\n                         )\ndef test_concat_bidirs(cell_type, input_size, output_size, bidir):\n    cell = Cell(cell_type=cell_type, input_size=input_size, output_size=output_size, bidir=bidir)\n    cell.reset(bs=32)\n    output = concat_bidir_state(cell.hidden, bidir=bidir, cell_type=cell_type, nlayers=1)\n    cell2 = Cell(cell_type=cell_type, input_size=input_size, output_size=output_size * 2 if bidir else output_size,\n                 bidir=False)\n    cell2.reset(bs=32)\n    dec_state = cell2.hidden\n    for layer_in, layer_out in zip(output, dec_state):\n        if isinstance(layer_in, (tuple, list)):\n            for h1, h2 in zip(layer_in, layer_out):\n                assert h1.size() == h2.size()\n        else:\n            assert layer_in.size() == layer_out.size()\n'"
src/quicknlp/data/__init__.py,0,"b'from .data_loaders import DialogueDataLoader\nfrom .datasets import DialogueDataset, HierarchicalDatasetFromDataFrame, HierarchicalDatasetFromFiles, \\\n    TabularDatasetFromDataFrame, TabularDatasetFromFiles, DialDataset, HREDDataset, HREDConstraintsDataset\nfrom .dialogue_analysis import DialogueAnalysis\nfrom .dialogue_model_data_loader import CVAEModelData, HREDModelData, HREDAttentionModelData\nfrom .hierarchical_model_data_loader import HierarchicalModelData\nfrom .s2s_model_data_loader import S2SAttentionModelData, S2SModelData, TransformerModelData\nfrom .sampler import DialogueRandomSampler, DialogueSampler\nfrom .spacy_tokenizer import SpacyTokenizer\n'"
src/quicknlp/data/data_loaders.py,0,"b""import numpy as np\nfrom fastai.dataloader import DataLoader\n\n\nclass DialogueDataLoader(DataLoader):\n\n    def pad2d(self, x, max_cl, max_sl):\n\n        paddings = [(max_cl - x.shape[0], 0), (max_sl - x.shape[1], 0)] if self.pre_pad else [(0, max_cl - x.shape[0]),\n                                                                                              (0, max_sl - x.shape[1])]\n        return np.pad(x, paddings, mode='constant', constant_values=self.pad_idx)\n\n    def pad1d(self, x, max_sl):\n        paddings = [(max_sl - x.size, 0)] if self.pre_pad else [(0, max_sl - x.size)]\n        return np.pad(x, paddings, mode='constant', constant_values=self.pad_idx)\n\n    def get_batch(self, indices):\n        max_cl, max_sl = np.asarray([self.dataset[i][0].shape for i in indices]).max(axis=0)\n        x_batch = np.stack([self.pad2d(self.dataset[i][0], max_cl, max_sl) for i in indices], axis=0)\n\n        max_sl = max([self.dataset[i][-2].size for i in indices])\n        y_batch = np.stack([self.pad1d(self.dataset[i][-2], max_sl) for i in indices], axis=0)\n        y_target = np.stack([self.pad1d(self.dataset[i][-1], max_sl) for i in indices], axis=0)\n        res = [x_batch, y_batch, y_target]\n        if self.transpose:\n            res[0], res[1] = np.transpose(res[0], [1, 2, 0]), res[1].T\n        if self.transpose_y:\n            res[2] = res[2].T\n        if len(self.dataset[0]) > 3:\n            constraints = np.stack([self.dataset[i][1] for i in indices], axis=0)\n            if self.transpose:\n                constraints = constraints.T\n            res = res[0], constraints, res[1], res[2]\n\n        return res\n"""
src/quicknlp/data/datasets.py,2,"b'import json\nimport os\nimport pickle\nfrom glob import glob\nfrom operator import itemgetter\nfrom pathlib import Path\nfrom typing import Iterator, List, Optional, Tuple, Union\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom fastai.core import A\nfrom torchtext.data import Dataset, Example, Field\nfrom tqdm import tqdm\n\nNamedField = Tuple[str, Field]\n\n\nclass TabularDatasetFromFiles(Dataset):\n    """"""This class allows the loading of multiple column data from a tabular format (e.g. csv, tsv, json, Similar to torchtext\n    TabularDataset class. The difference is it can work through a directory of multiple files instead of only\n    a single file\n    """"""\n\n    def get_examples_from_file(self, path: str, fields: List[NamedField], format: str, encoding: str = \'utf-8\',\n                               skip_header: bool = True) -> Tuple[List[Example], List[NamedField]]:\n        if format.lower() in [""csv"", ""tsv""]:\n            sep = "","" if format.lower() == ""csv"" else ""\\t""\n            data = pd.read_csv(os.path.expanduser(path), encoding=encoding, header=0 if skip_header else None,\n                               sep=sep)\n        elif format.lower() == ""json"":\n            data = pd.read_json(os.path.expanduser(path), encoding=encoding)\n        examples = []\n        for _, row in data.iterrows():\n            examples.append(Example.fromlist(row.values.tolist(), fields))\n        return examples, fields\n\n    def __init__(self, path: str, fields: List[NamedField], encoding: str = \'utf-8\', skip_header: bool = False,\n                 **kwargs):\n        paths = glob(f\'{path}/*.*\') if os.path.isdir(path) else [path]\n        examples = []\n        for path_ in paths:\n            examples_from_file, fields = self.get_examples_from_file(path_, fields,\n                                                                     format=os.path.splitext(path_)[-1][1:],\n                                                                     skip_header=skip_header,\n                                                                     encoding=encoding)\n            examples.extend(examples_from_file)\n\n        super().__init__(examples, fields, **kwargs)\n\n\nclass TabularDatasetFromDataFrame(Dataset):\n\n    @classmethod\n    def columns(cls, fields: List[NamedField]) -> List[str]:\n        return [i[0] for i in fields]\n\n    def __init__(self, df, fields, **kwargs):\n        df = df.loc[:, self.columns(fields)]\n        examples = []\n        for index, row in df.iterrows():\n            example = Example.fromlist(row.tolist(), fields)\n            examples.append(example)\n\n        super().__init__(examples, fields, **kwargs)\n\n    @classmethod\n    def splits(cls, train_df: Optional[pd.DataFrame] = None, val_df: Optional[pd.DataFrame] = None,\n               test_df: Optional[pd.DataFrame] = None, **kwargs) -> Tuple[\'TabularDatasetFromDataFrame\', ...]:\n        train_data = None if train_df is None else cls(train_df, **kwargs)\n        val_data = None if val_df is None else cls(val_df, **kwargs)\n        test_data = None if test_df is None else cls(test_df, **kwargs)\n\n        return tuple(d for d in (train_data, val_data, test_data) if d is not None)\n\n\ndef df_to_dialogue_examples(df: pd.DataFrame, *, fields: List[Tuple[str, Field]], batch_col: str,\n                            role_col: str, text_col: str, sort_col: str, max_sl=1000) -> Iterator[Example]:\n    """"""convert df to dialogue examples""""""\n    df = [df] if not isinstance(df, list) else df\n    tokenize = fields[0][1].tokenize\n    for file_index, _df in enumerate(df):\n        for chat_id, conversation in tqdm(_df.groupby(batch_col), desc=f""processed file {file_index}/{len(df)}""):\n            if conversation[role_col].nunique() > 1:\n                conversation = conversation.sort_values(by=sort_col)\n                conversation_tokens = ""__"" + conversation[role_col] + ""__""\n                text_with_roles = (conversation_tokens + "" "" + conversation[text_col]).astype(str)\n                text_with_roles_length = text_with_roles.apply(lambda x: len(tokenize(x)))\n                text = """".join(text_with_roles.str.cat(sep="" ""))\n                roles = """".join(conversation_tokens.str.cat(sep="" ""))\n                example = Example.fromlist([text.strip(), roles.strip()], fields)\n                example.sl = text_with_roles_length.tolist()\n                # sanity check if the sl is much larger than expected ignore\n                if max(example.sl) < max_sl:\n                    yield example\n\n\ndef json_to_dialogue_examples(path_dir: Path, *, fields: List[Tuple[str, Field]], utterance_key: str, role_key: str,\n                              text_key: str, sort_key: str, max_sl: int = 1000,\n                              target_roles: Optional[List[str]] = None) -> \\\n        Iterator[Example]:\n    """"""Load dialogues from json files\n    a json file should have a List of Dicts, see examples:\n     [{batch_col:chat_id, utterance_col:[{text_col:message, role_col:role, sort_col:timestamp}]}]\n\n    """"""\n    for file_index, file in enumerate(path_dir.glob(""*.json"")):\n        with file.open(\'r\', encoding=\'utf-8\') as fh:\n            dialogues = json.load(fh)\n        for dialogue in tqdm(dialogues, desc=f\'processed file {file}\'):\n            if isinstance(sort_key, str):\n                key = itemgetter(sort_key)\n            elif callable(sort_key):\n                key = sort_key\n            else:\n                raise ValueError(""Invalid sort_key provided"")\n            conversation = sorted(dialogue[utterance_key], key=key)\n            text = """"\n            roles = """"\n            lengths = []\n            tokenize = fields[0][1].tokenize\n            for utterance in conversation:\n                ut = utterance[text_key]\n                ut = "" "".join(ut) if isinstance(ut, list) else ut\n                conv_role = ""__"" + utterance[role_key] + ""__""\n                text_with_role = conv_role + "" "" + ut\n                if text.strip() != """":\n                    if target_roles is None or utterance[role_key] in target_roles:\n                        example = Example.fromlist([text.strip(), roles.strip(), text_with_role], fields)\n                        example.sl = [i for i in lengths]\n                        # sanity check if the sl is much larger than expected ignore\n                        assert len(lengths) == len(roles.split())\n                        if max(example.sl) < max_sl:\n                            yield example\n                text += "" "" + text_with_role\n                roles += "" "" + conv_role\n                lengths.append(len(tokenize(text_with_role)))\n\n\nclass HierarchicalDatasetFromDataFrame(Dataset):\n\n    def __init__(self, df: Union[pd.DataFrame, List[pd.DataFrame]], text_field: Field, batch_col: str,\n                 text_col: str, role_col: str, sort_col: str, path: Optional[str] = None, max_sl: int = 1000,\n                 reset: bool = False, **kwargs):\n        """"""\n\n        Args:\n            df (Union[pd.DataFrame, List[pd.DataFrame]]: A dataframe or a list of dataframes with the data\n            text_field (Field): a torchtext.data.Field object that will process the tokenizations\n            batch_col (str): The name of the column in the data df that will be used to group the conversations, e.g. chat_id\n            text_col (str): The name of the column in the data containing the text data, e.g. body\n            role_col (str): The name of the column in the data containing the role/name of the person speaking, e.g. role\n            sort_col (str): The name of the column in the data that will be used to sort the data of every group, e.g. timestamp\n            reset (bool): If true and example pickles exist delete them\n            **kwargs:\n        """"""\n        fields = [(""text"", text_field), (""roles"", text_field)]\n        iterator = df_to_dialogue_examples(df, fields=fields, batch_col=batch_col, role_col=role_col,\n                                           sort_col=sort_col, text_col=text_col, max_sl=max_sl)\n        if path is not None:\n            path = Path(path)\n            examples_pickle = path / ""examples.pickle""\n            if examples_pickle.exists() and not reset:\n                with examples_pickle.open(""rb"") as fh:\n                    examples = pickle.load(fh)\n            else:\n                with examples_pickle.open(\'wb\') as fh:\n                    examples = [i for i in iterator]\n                    pickle.dump(examples, fh)\n        else:\n            examples = [i for i in iterator]\n        super().__init__(examples=examples, fields=fields, **kwargs)\n\n    @classmethod\n    def splits(cls, path: Optional[str] = None, train_df: Optional[pd.DataFrame] = None,\n               val_df: Optional[pd.DataFrame] = None, test_df: Optional[pd.DataFrame] = None,\n               max_sl: int = 1000, **kwargs) -> Tuple[\'HierarchicalDatasetFromDataFrame\', ...]:\n        train_data = None if train_df is None else cls(path=path, df=train_df, max_sl=max_sl, **kwargs)\n        val_data = None if val_df is None else cls(path=path, df=val_df, **kwargs)\n        test_data = None if test_df is None else cls(path=path, df=test_df, **kwargs)\n\n        return tuple(d for d in (train_data, val_data, test_data) if d is not None)\n\n\ndef load_dfs(paths: str, file_format: str, encoding: Optional[str] = None) -> List[pd.DataFrame]:\n    if file_format in [""csv"", ""tsv""]:\n        sep = {""csv"": "","", ""tsv"": ""\\t""}[file_format]\n        return [pd.read_csv(path, sep=sep, encoding=encoding) for path in paths if path.endswith(file_format)]\n    elif file_format == ""json"":\n        return [pd.read_json(path, encoding=encoding) for path in paths if path.endswith(file_format)]\n\n\nclass HierarchicalDatasetFromFiles(HierarchicalDatasetFromDataFrame):\n    def __init__(self, path, file_format, text_field: Field, batch_col: str, text_col: str, role_col: str,\n                 sort_col: Optional[str] = None, encoding: Optional[str] = None, max_sl: int = 1000, **kwargs):\n        paths = glob(f\'{path}/*.*\') if os.path.isdir(path) else [path]\n        dfs = load_dfs(paths, file_format=file_format, encoding=encoding)\n        super().__init__(path=path, df=dfs, text_field=text_field, batch_col=batch_col, text_col=text_col,\n                         role_col=role_col, sort_col=sort_col, max_sl=max_sl, **kwargs)\n\n    @classmethod\n    def splits(cls, path: str, train_path: Optional[str] = None, val_path: Optional[str] = None,\n               test_path: Optional[str] = None, max_sl: int = 1000, **kwargs) -> Tuple[\n        \'HierarchicalDatasetFromFiles\', ...]:\n        train_data = None if train_path is None else cls(path=os.path.join(path, train_path), max_sl=max_sl, **kwargs)\n        val_data = None if val_path is None else cls(path=os.path.join(path, val_path), max_sl=max_sl, **kwargs)\n        test_data = None if test_path is None else cls(path=os.path.join(path, test_path), max_sl=max_sl, **kwargs)\n\n        return tuple(d for d in (train_data, val_data, test_data) if d is not None)\n\n\nclass DialogueDataset(Dataset):\n\n    def __init__(self, path: Union[Path, str], text_field: Field, utterance_key: str,\n                 text_key: str, role_key: str, sort_key: str, max_sl: int = 1000, reset=False, target_roles=None,\n                 **kwargs):\n        """"""\n\n        Args:\n            path (Path,str): the path to a directory with json files to load\n            text_field (Field): a torchtext Field object that will tokenize the data\n            utterance_key (str): The name of the key in the data that will be contain the utterances (e.g. utterances)\n            text_key (str): The name of the key in the json containing the text data\n            role_key (str): The name of the key in the json containing the role/name of the person speaking\n            sort_key (str): The name of the key in the json that will be used to sort the data of every group\n            reset (bool): If true and example pickles exist delete them\n            target_roles (Optional[List[str]]): Optionally the roles that will be targets\n\n            **kwargs:\n\n            An example json could be like this:\n\n            [   {utterance_col:[\n                    {text_col:body, sort_col:time1, role_col:user1},\n                    {text_col:body, sort_col:time2, role_col:user2}]\n                 },\n                {utterance_col:[\n                    {text_col:body, sort_col:time1, role_col:user1},\n                    {text_col:body, sort_col:time2, role_col:user2}]\n                 }\n            ]\n        """"""\n        path = Path(path) if isinstance(path, str) else path\n        fields = [(""text"", text_field), (""roles"", text_field), (""response"", text_field)]\n        iterator = json_to_dialogue_examples(path_dir=path, fields=fields, utterance_key=utterance_key,\n                                             role_key=role_key, text_key=text_key, sort_key=sort_key, max_sl=max_sl,\n                                             target_roles=target_roles\n                                             )\n        if path is not None:\n            examples_pickle = path / ""examples.pickle""\n            if examples_pickle.exists() and not reset:\n                with examples_pickle.open(""rb"") as fh:\n                    examples = pickle.load(fh)\n            else:\n                with examples_pickle.open(\'wb\') as fh:\n                    examples = [i for i in iterator]\n                    pickle.dump(examples, fh)\n        else:\n            examples = [i for i in iterator]\n        super().__init__(examples=examples, fields=fields, **kwargs)\n\n    @classmethod\n    def splits(cls, path: str, train_path: Optional[str] = None, val_path: Optional[str] = None,\n               test_path: Optional[str] = None, max_sl: int = 1000, **kwargs) -> Tuple[\n        \'DialogueDataset\', ...]:\n        path = Path(path)\n        train_data = None if train_path is None else cls(path=path / train_path, max_sl=max_sl, **kwargs)\n        val_data = None if val_path is None else cls(path=path / val_path, max_sl=max_sl, **kwargs)\n        test_data = None if test_path is None else cls(path=path / test_path, max_sl=max_sl, **kwargs)\n\n        return tuple(d for d in (train_data, val_data, test_data) if d is not None)\n\n\nclass ContextResponseDataset(Dataset):\n    def __init__(self, context: List[int], response: List[int], label: Optional[int] = None, backwards=False,\n                 sos: Optional[int] = None,\n                 eos: Optional[int] = None):\n        self.c, self.r, self.l, self.backwards, self.sos, self.eos = context, response, label, backwards, sos, eos\n\n    def __getitem__(self, idx):\n        x = self.c[idx]\n        y = self.r[idx]\n        label = None if self.label is None else self.l[idx]\n        if self.backwards: x = list(reversed(x))\n        if self.eos is not None:\n            x = x + [self.eos]\n            y = y + [self.eos]\n        if self.sos is not None:\n            x = [self.sos] + x\n            y = [self.sos] + y\n        if label is None:\n            return np.array(x), np.array(y)\n        else:\n            return np.array(x), np.array(y), label\n\n    def __len__(self):\n        return len(self.x)\n\n\nclass DialDataset(Dataset):\n    def __init__(self, context: List[List[int]], response: List[int], pad: int, label: Optional[int] = None,\n                 backwards=False,\n                 sos: Optional[int] = None,\n                 eos: Optional[int] = None,\n                 ):\n        self.c, self.r, self.l, self.backwards, self.sos, self.eos, self.pad = context, response, label, backwards, sos, eos, pad\n\n    def __getitem__(self, idx):\n        x = self.c[idx]\n        y = self.r[idx]\n        if self.backwards: x = [list(reversed(i)) for i in x]\n        if self.eos is not None:\n            x = [i + [self.eos] for i in x]\n            y = y + [self.eos]\n        if self.sos is not None:\n            x = [[self.sos] + i for i in x]\n            y = [self.sos] + y\n\n        max_sl = max([len(i) for i in x])\n        x_padded = np.zeros((len(x), max_sl), dtype=np.int64)\n        for i, row in enumerate(x):\n            x_padded[i, :len(row)] = row\n        return x_padded, np.array(y)\n\n    def __len__(self):\n        return len(self.c)\n\n\nclass HREDDataset(torch.utils.data.Dataset):\n    def __init__(self, x, y):\n        self.x, self.y = x, y\n\n    def __getitem__(self, idx):\n        return A(np.atleast_2d(self.x[idx]), self.y[idx],\n                 np.hstack((self.y[idx][1:], np.asarray([2]))))\n\n    def __len__(self):\n        return len(self.x)\n\n\nclass HREDConstraintsDataset(torch.utils.data.Dataset):\n    def __init__(self, x, c, y):\n        self.x, self.c, self.y = x, c, y\n\n    def __getitem__(self, idx):\n        return A(np.atleast_2d(self.x[idx]), self.c[idx], self.y[idx],\n                 np.hstack((self.y[idx][1:], np.asarray([2]))))\n\n    def __len__(self):\n        return len(self.x)\n'"
src/quicknlp/data/dialogue_analysis.py,0,"b'import pickle\n\nimport numpy as np\nimport pandas as pd\nimport spacy\nimport tqdm\n\n\nclass DialogueAnalysis:\n    processed_col = ""spacy_processed""\n    sentence_length_col = ""sentence_length""\n    entities_col = ""ents_col""\n\n    def __init__(self, data, text_col, chat_id_col, role_col, sort_col=None, language=""en"", lower=True):\n        self.data = pd.DataFrame(data)\n        if sort_col is not None:\n            self.data.sort_values(by=sort_col, inplace=True, ascending=True)\n        self.chat_id_col = chat_id_col\n        self.role_col = role_col\n        self.text_col = text_col\n        self.lower = lower\n        self._text = None\n        self.nlp = spacy.load(language)\n\n    def process_data(self):\n        texts = self.data.loc[:, self.text_col].tolist()\n        self.data[self.processed_col] = [doc for doc in tqdm.tqdm(self.nlp.pipe(texts, batch_size=100, n_threads=20),\n                                                                  desc=""processing data"", total=len(texts))]\n\n    def __len__(self):\n        return self.data.shape[0]\n\n    @property\n    def conv_length(self):\n        return self.data.groupby(self.chat_id_col).count()[self.text_col]\n\n    @property\n    def sentence_length(self):\n        if self.processed_col not in self.data.columns:\n            self.process_data()\n        if self.sentence_length_col not in self.data:\n            self.data[self.sentence_length_col] = self.data.loc[:, self.processed_col].apply(len)\n        return self.data[[self.chat_id_col, self.role_col, self.sentence_length_col]]\n\n    @property\n    def text(self):\n        if self.processed_col not in self.data.columns:\n            self.process_data()\n        if self._text is None:\n            if self.lower:\n                self._text = pd.Series([token.text.lower() for doc in self.data[self.processed_col] for token in doc])\n            else:\n                self._text = pd.Series([token.text for doc in self.data[self.processed_col] for token in doc])\n        return self._text\n\n    @property\n    def vocab(self):\n        return pd.Categorical(self.text).describe()\n\n    @property\n    def entities(self):\n        if self.processed_col not in self.data.columns:\n            self.process_data()\n        if self.entities_col not in self.data:\n            self.data[self.entities_col] = self.data[self.processed_col].apply(\n                lambda x: [(e.text, e.start_char, e.end_char, e.label_) for e in x.ents] if len(x.ents) > 0 else np.nan)\n        return self.data[[self.chat_id_col, self.role_col, self.entities_col]]\n\n    def save(self, filename: str):\n        pickle.dump(self.data, open(filename + "".pickle"", \'wb\'))\n        self.nlp.to_disk(filename + "".bin"")\n\n    @classmethod\n    def load(cls, filename: str, **kwargs):\n        data = pickle.load(open(filename + "".pickle"", \'rb\'))\n        analysis_object = cls(data=data, **kwargs)\n        analysis_object.nlp.from_disk(filename + "".bin"")\n        return analysis_object\n\n    def __repr__(self):\n        result = f\'Num utterances: {len(self)} \'\n        result += f\'Num dialogues: {self.data[self.chat_id_col].nunique()}\\n\'\n        result += f\'percentiles of dialogue lengths: min: {self.conv_length.min()} 85%: {np.ceil(self.conv_length.quantile(0.85)):.0f} 90%: {np.ceil(self.conv_length.quantile(0.90)):.0f} 99%: {np.ceil(self.conv_length.quantile(0.99)):.0f}, max: {self.conv_length.max()}\\n\'\n        sl = self.sentence_length[self.sentence_length_col]\n        result += f\'percentiles of utterance lengths: min: {sl.min()} 85%: {np.ceil(sl.quantile(0.85)):.0f} 90%: {np.ceil(sl.quantile(0.90)):.0f} 99%: {np.ceil(sl.quantile(0.99)):.0f}, max: {sl.max()}\\n\'\n        result += f\'Vocab size: {self.vocab[""counts""].size}\'\n        return result\n'"
src/quicknlp/data/dialogue_model_data_loader.py,0,"b'from functools import partial\nfrom typing import Callable, List, Optional, Union\n\nfrom fastai.core import to_gpu\nfrom fastai.dataset import ModelData\nfrom torch import optim\nfrom torchtext.data import Dataset, Field\n\nfrom quicknlp.data.torchtext_data_loaders import DialogueTTDataLoader\nfrom quicknlp.models import CVAE, HRED\nfrom quicknlp.models.hred_attention import HREDAttention\nfrom .datasets import DialogueDataset\nfrom .learners import EncoderDecoderLearner, get_cvae_loss\nfrom .model_helpers import CVAEModel, HREDModel, PrintingMixin, HREDAttentionModel\n\n\nclass HREDModelData(ModelData, PrintingMixin):\n    """"""\n    This class provides the entry point for dealing with supported NLP Dialogue Tasks, i.e. tasks where each sample involves\n    sequences of sentences e.g. dialogues etc.\n    1. Use one of the factory constructors (from dataframes, from text files) to obtain an instance of the class\n    2. use the get_model method to return an instance of one of the provided models\n    3. Use stoi, itos functions to quickly convert between tokens and sentences\n\n    """"""\n\n    def __init__(self, path: str, text_field: Field, target_names: List[str], trn_ds: Dataset, val_ds: Dataset,\n                 test_ds: Dataset, bs: int, max_context_size: int = 130000,\n                 backwards: bool = False, **kwargs):\n        """""" Constructor for the class. An important thing that happens here is\n        that the field\'s ""build_vocab"" method is invoked, which builds the vocabulary\n        for this NLP model.\n\n        Also, three instances of a HierarchicalIterator are constructed; one each\n        for training data (self.trn_dl), validation data (self.val_dl), and the\n        testing data (self.test_dl)\n\n        Args:\n            path (str): the path to save the data\n            text_field (Field): The field object to use to manage the vocabulary\n            trn_ds (Dataset): a pytorch Dataset with the training data\n            val_ds (Dataset): a pytorch Dataset with the validation data\n            test_ds (Dataset: a pytorch Dataset with the test data\n            bs (int): the batch_size\n            sort_key (Union[Callable,str]): if sort_key == ""sl"" sort by length of largest sequence in a dialogue,\n                or if sort_key == \'cl"" sort by  conversation length. Alternative sort_key can be a function to sort\n                the examples based on some property of the examples (""roles"", ""sl"", ""text\').\n            max_context_size (Optional[int]: The maximums size of allowed context tensors (bs x cl xsl)\n                These will be filtered out so as not to run out of gpu memory\n            backwards (bool): Reverse the order of the text or not (not implemented yet)\n            **kwargs: Other arguments to be passed to the BucketIterator and the fields build_vocab function\n        """"""\n\n        self.bs = bs\n        if not hasattr(text_field, \'vocab\'):\n            text_field.build_vocab(trn_ds, **kwargs)\n        self.nt = len(text_field.vocab)\n        self.pad_idx = text_field.vocab.stoi[text_field.pad_token]\n        self.eos_idx = text_field.vocab.stoi[text_field.eos_token]\n\n        trn_dl, val_dl, test_dl = [DialogueTTDataLoader(ds, bs, target_names=target_names,\n                                                        max_context_size=max_context_size, backwards=backwards)\n                                   if ds is not None else None\n                                   for ds in (trn_ds, val_ds, test_ds)]\n        super().__init__(path=path, trn_dl=trn_dl, val_dl=val_dl, test_dl=test_dl)\n        self.fields = trn_ds.fields\n\n    @property\n    def sz(self):\n        return self.bs\n\n    @classmethod\n    def from_json_files(cls, path: str, text_field: Field, train: str, validation: str,\n                        text_key: str, utterance_key: str, role_key: str, sort_key_json: Union[Callable, str, str],\n                        test: Optional[str] = None, target_names: Optional[List[str]] = None, bs: Optional[int] = 64,\n                        max_sl: int = 1000, reset: bool = False, **kwargs) -> \'DialogueModelData\':\n        """"""Method used to instantiate a DialogueModelData object that can be used for a supported NLP Task from files\n\n        Args:\n            target_names (Optional[List[str]]): A list of targets to add to the model targets (default is all)\n            path (str): the absolute path in which temporary model data will be saved\n            text_field (Field): A Field to manage the vocab for all the dialogues\n                if multiple fields should use the same vocab, the same field should be passed to them\n            path (str): the absolute path in which temporary model data will be saved\n            train (str):  The path to the training data\n            validation (str):  The path to the test data\n            test (Optional[str]): The path to the test data\n            text_key (str): The name of the column with the text data\n            utterance_key (str): The name of the key with the hierarchical groups, e.g. conversation ids\n            sort_key_json (str): A key to sort the utterances of every dialogue, e.g. timestamps\n            role_key (str): A key with the role of the person saying every text\n            bs (Optional[int]): the batch size\n            max_sl (Int): The maximum sequence length allowed when creating examples dialogues with larger sl will be filtered out\n            reset (bool): If true and example pickles exist delete them\n            **kwargs:\n\n        Returns:\n            a HierarchicalModelData instance, which provides datasets for training, validation, testing\n\n        Note:\n            see also the fastai.nlp.LanguageModelData class which inspired this class\n\n        """"""\n        datasets = DialogueDataset.splits(path=path, train_path=train, val_path=validation,\n                                          test_path=test, text_field=text_field,\n                                          text_key=text_key,\n                                          utterance_key=utterance_key,\n                                          role_key=role_key,\n                                          sort_key=sort_key_json,\n                                          max_sl=max_sl,\n                                          reset=reset,\n                                          )\n        trn_ds = datasets[0]\n        val_ds = datasets[1]\n        test_ds = datasets[2] if len(datasets) == 3 else None\n        return cls(path=path, text_field=text_field, target_names=target_names,\n                   trn_ds=trn_ds, val_ds=val_ds, test_ds=test_ds, bs=bs, **kwargs)\n\n    def to_model(self, m, opt_fn):\n        model = HREDModel(to_gpu(m))\n        return EncoderDecoderLearner(self, model, opt_fn=opt_fn)\n\n    def get_model(self, opt_fn=None, emb_sz=300, nhid=512, nlayers=2, max_tokens=100, **kwargs):\n        if opt_fn is None:\n            opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n        m = HRED(\n            ntoken=self.nt,\n            emb_sz=emb_sz,\n            nhid=nhid,\n            nlayers=nlayers,\n            pad_token=self.pad_idx,\n            eos_token=self.eos_idx,\n            max_tokens=max_tokens,\n            **kwargs\n        )\n        return self.to_model(m, opt_fn)\n\n\nclass HREDAttentionModelData(HREDModelData):\n\n    def to_model(self, m, opt_fn):\n        model = HREDAttentionModel(to_gpu(m))\n        learner = EncoderDecoderLearner(self, model, opt_fn=opt_fn)\n        return learner\n\n    def get_model(self, opt_fn=None, emb_sz=300, nhid=512, nlayers=2, att_nhid=512, max_tokens=100, **kwargs):\n        if opt_fn is None:\n            opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n        m = HREDAttention(\n            ntoken=self.nt,\n            emb_sz=emb_sz,\n            nhid=nhid,\n            nlayers=nlayers,\n            pad_token=self.pad_idx,\n            eos_token=self.eos_idx,\n            max_tokens=max_tokens,\n            att_nhid=att_nhid,\n            **kwargs\n        )\n        return self.to_model(m, opt_fn)\n\n\nclass CVAEModelData(HREDModelData):\n\n    def to_model(self, m, opt_fn):\n        model = CVAEModel(to_gpu(m))\n        learner = EncoderDecoderLearner(self, model, opt_fn=opt_fn)\n        learner.crit = get_cvae_loss(pad_idx=learner.data.pad_idx)\n        return learner\n\n    def get_model(self, opt_fn=None, emb_sz=300, nhid=512, nlayers=2, max_tokens=100, latent_dim=100, bow_nhid=400,\n                  **kwargs):\n        if opt_fn is None:\n            opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n        m = CVAE(\n            ntoken=self.nt,\n            emb_sz=emb_sz,\n            nhid=nhid,\n            nlayers=nlayers,\n            pad_token=self.pad_idx,\n            eos_token=self.eos_idx,\n            max_tokens=max_tokens,\n            latent_dim=latent_dim,\n            bow_nhid=bow_nhid,\n            **kwargs\n        )\n        return self.to_model(m, opt_fn)\n'"
src/quicknlp/data/hierarchical_model_data_loader.py,0,"b'from functools import partial\nfrom typing import Callable, List, Optional, Union\n\nimport pandas as pd\nfrom fastai.core import to_gpu\nfrom fastai.dataset import ModelData\nfrom torch import optim\nfrom torchtext.data import Dataset, Field\n\nfrom quicknlp.data.torchtext_data_loaders import HierarchicalDataLoader\nfrom quicknlp.models import HRED\nfrom .datasets import HierarchicalDatasetFromDataFrame, HierarchicalDatasetFromFiles\nfrom .learners import EncoderDecoderLearner\nfrom .model_helpers import HREDModel, PrintingMixin\n\n\nclass HierarchicalModelData(ModelData, PrintingMixin):\n    """"""\n    This class provides the entry point for dealing with supported NLP Hierarchical Tasks, i.e. tasks where each sample involves\n    sequences of sentences e.g. dialogues etc.\n    1. Use one of the factory constructors (from dataframes, from text files) to obtain an instance of the class\n    2. use the get_model method to return an instance of one of the provided models\n    3. Use stoi, itos functions to quickly convert between tokens and sentences\n\n    """"""\n\n    def __init__(self, path: str, text_field: Field, target_names: List[str], trn_ds: Dataset, val_ds: Dataset,\n                 test_ds: Dataset, bs: int, sort_key: Union[Callable, str] = ""sl"", max_context_size: int = 130000,\n                 backwards: bool = False, **kwargs):\n        """""" Constructor for the class. An important thing that happens here is\n        that the field\'s ""build_vocab"" method is invoked, which builds the vocabulary\n        for this NLP model.\n\n        Also, three instances of a HierarchicalIterator are constructed; one each\n        for training data (self.trn_dl), validation data (self.val_dl), and the\n        testing data (self.test_dl)\n\n        Args:\n            path (str): the path to save the data\n            text_field (Field): The field object to use to manage the vocabulary\n            trn_ds (Dataset): a pytorch Dataset with the training data\n            val_ds (Dataset): a pytorch Dataset with the validation data\n            test_ds (Dataset: a pytorch Dataset with the test data\n            bs (int): the batch_size\n            sort_key (Union[Callable,str]): if sort_key == ""sl"" sort by length of largest sequence in a dialogue,\n                or if sort_key == \'cl"" sort by  conversation length. Alternative sort_key can be a function to sort\n                the examples based on some property of the examples (""roles"", ""sl"", ""text\').\n            max_context_size (Optional[int]: The maximums size of allowed context tensors (bs x cl xsl)\n                These will be filtered out so as not to run out of gpu memory\n            backwards (bool): Reverse the order of the text or not (not implemented yet)\n            **kwargs: Other arguments to be passed to the BucketIterator and the fields build_vocab function\n        """"""\n\n        self.bs = bs\n        if not hasattr(text_field, \'vocab\'):\n            text_field.build_vocab(trn_ds, **kwargs)\n        self.nt = len(text_field.vocab)\n        self.pad_idx = text_field.vocab.stoi[text_field.pad_token]\n        self.eos_idx = text_field.vocab.stoi[text_field.eos_token]\n\n        trn_dl, val_dl, test_dl = [HierarchicalDataLoader(ds, bs, target_names=target_names, sort_key=sort_key,\n                                                          max_context_size=max_context_size, backwards=backwards)\n                                   if ds is not None else None\n                                   for ds in (trn_ds, val_ds, test_ds)]\n        super().__init__(path=path, trn_dl=trn_dl, val_dl=val_dl, test_dl=test_dl)\n        self.fields = trn_ds.fields\n\n    @property\n    def sz(self):\n        return self.bs\n\n    @classmethod\n    def from_dataframes(cls, path: str, text_field: Field, train_df: pd.DataFrame, val_df: pd.DataFrame,\n                        text_col: str, batch_col: str, role_col: str, sort_col: str,\n                        test_df: Optional[pd.DataFrame] = None, target_names: Optional[List[str]] = None, bs: int = 64,\n                        sort_key: Optional[Callable] = None, max_sl: int = 1000, reset: bool = False,\n                        **kwargs) -> \'HierarchicalModelData\':\n        """"""Method used to instantiate a HierarchicalModelData object that can be used for a supported NLP Task from dataframes\n\n        Args:\n            target_names (Optional[List[str]]): A list of targets to add to the model targets (default is all)\n            path (str): the absolute path in which temporary model data will be saved\n            text_field (Field): A Field to manage the vocab for all the dialogues\n                if multiple fields should use the same vocab, the same field should be passed to them\n            train_df (pd.DataFrame):  a pandas DataFrame with the training Data\n            val_df (str): a pandas DataFrame with the validation Data\n            test_df (Optional[str]):a pandas DataFrame with the test Data\n            bs (Optional[int]): the batch size\n            text_col (str): The name of the column with the text data\n            batch_col (str): The name of the column with the hierarchical groups, e.g. conversation ids\n            sort_col (str): A column to sort the text for every batch_col, e.g. timestamps\n            role_col (str): A column with the role of the person saying every text\n            sort_key (Optional[Callable]): A function to sort the examples in batch size based on a field\n            max_sl (Int): The maximum sequence length allowed when creating examples dialogues with larger sl will be filtered out\n            reset (bool): If true and example pickles exist delete them\n            **kwargs:\n\n        Returns:\n            a HierarchicalModelData instance, which provides datasets for training, validation, testing\n\n        Note:\n            see also the fastai.nlp.LanguageModelData class which inspired this class\n\n        """"""\n\n        datasets = HierarchicalDatasetFromDataFrame.splits(text_field=text_field,\n                                                           train_df=train_df,\n                                                           val_df=val_df,\n                                                           test_df=test_df,\n                                                           batch_col=batch_col,\n                                                           text_col=text_col,\n                                                           role_col=role_col,\n                                                           sort_col=sort_col,\n                                                           max_sl=max_sl,\n                                                           reset=reset\n                                                           )\n\n        train_ds = datasets[0]\n        val_ds = datasets[1]\n        test_ds = datasets[2] if len(datasets) == 3 else None\n        return cls(path=path, text_field=text_field, trn_ds=train_ds, val_ds=val_ds, test_ds=test_ds,\n                   target_names=target_names, bs=bs, sort_key=sort_key, **kwargs)\n\n    @classmethod\n    def from_text_files(cls, path: str, text_field: Field, train: str, validation: str,\n                        text_col: str, batch_col: str, sort_col: str, role_col: str,\n                        file_format: str,\n                        test: Optional[str] = None, target_names: Optional[List[str]] = None, bs: Optional[int] = 64,\n                        sort_key: Union[Callable, str] = ""sl"", max_sl: int = 1000, reset: bool = False,\n                        **kwargs) -> \'HierarchicalModelData\':\n        """"""Method used to instantiate a HierarchicalModelData object that can be used for a supported NLP Task from files\n\n        Args:\n            target_names (Optional[List[str]]): A list of targets to add to the model targets (default is all)\n            path (str): the absolute path in which temporary model data will be saved\n            text_field (Field): A Field to manage the vocab for all the dialogues\n                if multiple fields should use the same vocab, the same field should be passed to them\n            path (str): the absolute path in which temporary model data will be saved\n            train (str):  The path to the training data\n            validation (str):  The path to the test data\n            test (Optional[str]): The path to the test data\n            text_col (str): The name of the column with the text data\n            batch_col (str): The name of the column with the hierarchical groups, e.g. conversation ids\n            sort_col (str): A column to sort the text for every batch_col, e.g. timestamps\n            role_col (str): A column with the role of the person saying every text\n            file_format (str): The format of the file e.g. csv, json, tsv\n            bs (Optional[int]): the batch size\n            sort_key (Union[Callable,str]): A function to sort the examples in batch size based on a field or\n                sl for sorting by sequence length, or cl for sorting by conversation length\n            max_sl (Int): The maximum sequence length allowed when creating examples dialogues with larger sl will be filtered out\n            reset (bool): If true and example pickles exist delete them\n            **kwargs:\n\n        Returns:\n            a HierarchicalModelData instance, which provides datasets for training, validation, testing\n\n        Note:\n            see also the fastai.nlp.LanguageModelData class which inspired this class\n\n        """"""\n        datasets = HierarchicalDatasetFromFiles.splits(path=path, train_path=train, val_path=validation,\n                                                       test_path=test, text_field=text_field,\n                                                       text_col=text_col,\n                                                       batch_col=batch_col,\n                                                       role_col=role_col,\n                                                       sort_col=sort_col,\n                                                       file_format=file_format,\n                                                       max_sl=max_sl,\n                                                       reset=reset\n                                                       )\n        trn_ds = datasets[0]\n        val_ds = datasets[1]\n        test_ds = datasets[2] if len(datasets) == 3 else None\n        return cls(path=path, text_field=text_field, target_names=target_names,\n                   trn_ds=trn_ds, val_ds=val_ds, test_ds=test_ds, bs=bs, sort_key=sort_key, **kwargs)\n\n    def to_model(self, m, opt_fn):\n        model = HREDModel(to_gpu(m))\n        return EncoderDecoderLearner(self, model, opt_fn=opt_fn)\n\n    def get_model(self, opt_fn=None, emb_sz=300, nhid=512, nlayers=2, max_tokens=100, **kwargs):\n        if opt_fn is None:\n            opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n        m = HRED(\n            ntoken=self.nt,\n            emb_sz=emb_sz,\n            nhid=nhid,\n            nlayers=nlayers,\n            pad_token=self.pad_idx,\n            eos_token=self.eos_idx,\n            max_tokens=max_tokens,\n            **kwargs\n        )\n        return self.to_model(m, opt_fn)\n'"
src/quicknlp/data/iterators.py,1,"b'import random\nfrom doctest import Example\nfrom typing import Iterator as Iter\nfrom typing import List, Optional, Tuple\n\nimport numpy as np\nimport torch.cuda as cuda\nfrom torch import LongTensor\nfrom torchtext.data import Batch, BucketIterator, Field, Iterator, batch\n\nfrom quicknlp.utils import assert_dims\n\nConversations = List[List[str]]\nRoles = List[str]\nLengths = List[int]\nLT = LongTensor\n\n\nclass HierarchicalIterator(BucketIterator):\n    def __init__(self, dataset, batch_size, sort_key, target_roles=None, max_context_size=130000, backwards=False,\n                 **kwargs):\n        self.target_roles = target_roles\n        self.text_field = dataset.fields[\'text\']\n        self.max_context_size = max_context_size\n        self.backwards = backwards\n        device = None if cuda.is_available() else -1\n        super().__init__(dataset=dataset, batch_size=batch_size, sort_key=sort_key, device=device, **kwargs)\n\n    def process_minibatch(self, minibatch: List[Example]) -> Tuple[LT, LT, LT]:\n        max_sl = max([max(ex.sl) for ex in minibatch])\n        max_conv = max([len(ex.roles) for ex in minibatch])\n        padded_examples, padded_targets, padded_lengths, padded_roles = [], [], [], []\n        for example in minibatch:\n            examples, lens, roles = self.pad(example, max_sl=max_sl, max_conv=max_conv, field=self.text_field)\n            padded_examples.extend(examples)\n            padded_lengths.extend(lens)\n            padded_roles.append(roles)\n            # if self.target_roles is not None we will pad the roles we do not want to train on\n            # this allows for learning only the responses we are interested in\n            targets, *_ = self.pad(example, max_sl=max_sl, max_conv=max_conv, field=self.text_field,\n                                   target_roles=self.target_roles)\n            padded_targets.extend(targets)\n        self.text_field.include_lengths = False\n\n        data = self.text_field.numericalize(padded_examples, device=self.device, train=self.train)\n        batch_size = len(minibatch)\n        assert_dims(data, [max_sl, max_conv * batch_size])\n        data = data.view(max_sl, batch_size, max_conv).transpose(2, 0).transpose(2, 1).contiguous()\n        source = data[:-1]  # we remove the extra padding  sentence added here\n        targets = self.text_field.numericalize(padded_targets, device=self.device, train=self.train)\n        targets = targets.view(max_sl, batch_size, max_conv).transpose(2, 0).transpose(2, 1).contiguous()\n        # shapes will be max_conv -1 , max_sl, batch_size\n        assert_dims(source, [max_conv - 1, max_sl, batch_size])\n        assert_dims(targets, [max_conv, max_sl, batch_size])\n        return source, targets[1:], targets[1:, 1:]\n\n    def __iter__(self) -> Iter[Batch]:\n        """"""Same iterator almost as bucket iterator""""""\n        while True:\n            self.init_epoch()\n            for idx, minibatch in enumerate(self.batches):\n                # fast-forward if loaded from state\n                if self._iterations_this_epoch > idx:\n                    continue\n                self.iterations += 1\n                self._iterations_this_epoch += 1\n                if self.sort_within_batch:\n                    if self.sort:\n                        minibatch.reverse()\n                    else:\n                        minibatch.sort(key=self.sort_key, reverse=True)\n\n                context, response, targets = self.process_minibatch(minibatch)\n                for index in range(context.shape[0]):\n                    # do not yield if the target is just padding (does not provide anything to training)\n                    num_empty_targets = targets[index] == self.text_field.vocab.stoi[self.text_field.pad_token]\n                    if num_empty_targets.all():\n                        continue\n                    # skip examples with contexts that won\'t fit in gpu memory\n                    if np.prod(context[:index + 1].shape) > self.max_context_size:\n                        continue\n                    yield Batch.fromvars(dataset=self.dataset, batch_size=len(minibatch),\n                                         train=self.train,\n                                         context=context[:index + 1],\n                                         response=response[index],\n                                         targets=targets[index]\n                                         )\n            if not self.repeat:\n                raise StopIteration\n\n    def pad(self, example: Example, max_sl: int, max_conv: int, field: Field, target_roles: Optional[Roles] = None) -> \\\n            Tuple[Conversations, Lengths, Roles]:\n        """"""Pad a hierarchical example to the max sequence length and max conv length provided. Optionally if\n           target_roles parameter is provided every sentence whose role, found from example.roles,\n           is not matching the target_roles will be padded completely.\n        """"""\n        indices = [0] + np.cumsum(example.sl).tolist()\n        minibatch = self.get_minibatch_text(example, indices, backwards=self.backwards)\n        field.fix_length = max_sl\n        field.include_lengths = True\n        padded, lens = field.pad(minibatch=minibatch)\n        padded_roles = list(example.roles)\n        padded_sentence = [field.pad_token for _ in range(max_sl)]\n        if target_roles is not None:\n            padded = [p if r in target_roles else padded_sentence for p, r in zip(padded, padded_roles)]\n        for _ in range(max_conv - len(padded)):\n            padded.append(padded_sentence)\n            lens.append(0)\n            padded_roles.append(field.pad_token)\n        return padded, lens, padded_roles\n\n    def get_minibatch_text(self, example: Example, indices: List[int], backwards: bool = False) -> List[List[str]]:\n        minibatch = [example.text[indices[index]:indices[index + 1]] for index in range(len(indices) - 1)]\n        if backwards:\n            minibatch = [i[::-1] for i in minibatch]\n        return minibatch\n\n\nclass DialogueIterator(Iterator):\n    def __init__(self, dataset, batch_size, sort_key_inner, sort_key_outer, sort_key, target_roles=None,\n                 max_context_size=130000, backwards=False,\n                 **kwargs):\n        self.target_roles = target_roles\n        self.text_field = dataset.fields[\'text\']\n        self.max_context_size = max_context_size\n        self.backwards = backwards\n        device = None if cuda.is_available() else -1\n        self.sort_key_inner = sort_key_inner  # inner should be utterance sizes\n        self.sort_key_outer = sort_key_outer  # outer should be dialogue sizes\n        super().__init__(dataset=dataset, batch_size=batch_size, sort_key=sort_key, device=device, **kwargs)\n\n    def create_batches(self):\n        if self.sort:\n            self.batches = batch(self.data(), self.batch_size,\n                                 self.batch_size_fn)\n        else:\n            self.batches = self.dialogue_pool(self.data(), self.batch_size,\n                                              self.sort_key_inner,\n                                              self.sort_key_outer, self.batch_size_fn,\n                                              random_shuffler=self.random_shuffler)\n\n    def dialogue_pool(self, data, batch_size, key_inner, key_outer, batch_size_fn=lambda new, count, sofar: count,\n                      random_shuffler=None):\n        """"""Sort within buckets, then batch, then shuffle batches.\n\n        Partitions data into chunks of size 100*batch_size, sorts examples within\n        each chunk using sort_key, then batch these examples and shuffle the\n        batches.\n        """"""\n        if random_shuffler is None:\n            random_shuffler = random.shuffle\n        for p in batch(data, batch_size * 100, batch_size_fn):\n            p_batch = batch(sorted(sorted(p, key=key_inner), key=key_outer), batch_size, batch_size_fn)\n            for b in random_shuffler(list(p_batch)):\n                yield b\n\n    def pad(self, example: Example, max_sl: int, max_conv: int, field: Field, target_roles: Optional[Roles] = None) -> \\\n            Tuple[Conversations, Lengths, Roles]:\n        """"""Pad a hierarchical example to the max sequence length and max conv length provided. Optionally if\n           target_roles parameter is provided every sentence whose role, found from example.roles,\n           is not matching the target_roles will be padded completely.\n        """"""\n        indices = [0] + np.cumsum(example.sl).tolist()\n        minibatch = self.get_minibatch_text(example, indices, backwards=self.backwards)\n        field.fix_length = max_sl\n        field.include_lengths = True\n        padded, lens = field.pad(minibatch=minibatch)\n        padded_roles = list(example.roles)\n        padded_sentence = [field.pad_token for _ in range(max_sl)]\n        if target_roles is not None:\n            padded = [p if r in target_roles else padded_sentence for p, r in zip(padded, padded_roles)]\n        for _ in range(max_conv - len(padded)):\n            padded.append(padded_sentence)\n            lens.append(0)\n            padded_roles.append(field.pad_token)\n        return padded, lens, padded_roles\n\n    def get_minibatch_text(self, example: Example, indices: List[int], backwards: bool = False) -> List[List[str]]:\n        minibatch = [example.text[indices[index]:indices[index + 1]] for index in range(len(indices) - 1)]\n        if backwards:\n            minibatch = [i[::-1] for i in minibatch]\n        return minibatch\n\n    def process_minibatch(self, minibatch: List[Example]) -> Tuple[LT, LT, LT]:\n        max_sl = max([max(ex.sl) for ex in minibatch])\n        max_conv = max([len(ex.roles) for ex in minibatch])\n        padded_examples, targets, padded_lengths, padded_roles = [], [], [], []\n        for example in minibatch:\n            examples, lens, roles = self.pad(example, max_sl=max_sl, max_conv=max_conv, field=self.text_field)\n            padded_examples.extend(examples)\n            padded_lengths.extend(lens)\n            padded_roles.append(roles)\n            targets.append(example.response)\n        self.text_field.include_lengths = False\n\n        data = self.text_field.numericalize(padded_examples, device=self.device, train=self.train)\n        batch_size = len(minibatch)\n        assert_dims(data, [max_sl, max_conv * batch_size])\n        data = data.view(max_sl, batch_size, max_conv).transpose(2, 0).transpose(2, 1).contiguous()\n        self.text_field.fix_length = None\n        padded_targets = self.text_field.pad(targets)\n        targets = self.text_field.numericalize(padded_targets, device=self.device,\n                                               train=self.train)  # [max_sl, batch_size]\n        assert_dims(data, [max_conv, max_sl, batch_size])\n        assert_dims(targets, [None, batch_size])\n        return data, targets, targets[1:]\n\n    def __iter__(self) -> Iter[Batch]:\n        """"""Same iterator almost as bucket iterator""""""\n        while True:\n            self.init_epoch()\n            for idx, minibatch in enumerate(self.batches):\n                # fast-forward if loaded from state\n                if self._iterations_this_epoch > idx:\n                    continue\n                self.iterations += 1\n                self._iterations_this_epoch += 1\n                if self.sort_within_batch:\n                    if self.sort:\n                        minibatch.reverse()\n                    else:\n                        minibatch.sort(key=self.sort_key, reverse=True)\n\n                context, response, targets = self.process_minibatch(minibatch)\n                yield Batch.fromvars(dataset=self.dataset, batch_size=len(minibatch),\n                                     train=self.train,\n                                     context=context, response=response, targets=targets)\n            if not self.repeat:\n                raise StopIteration\n'"
src/quicknlp/data/learners.py,8,"b'from functools import partial\n\nimport torch\nfrom fastai.core import T, V, to_gpu\nfrom fastai.learner import Learner\nfrom fastai.torch_imports import load_model, save_model\nfrom torch.nn import functional as F\n\nfrom quicknlp.data.model_helpers import CVAEModel, predict_with_seq2seq\nfrom quicknlp.stepper import S2SStepper\n\n\ndef decoder_loss(input, target, pad_idx, predict_first_token=False, **kwargs):\n    sl_in, bs_in, vocab = input.size()\n    sl, bs = target.size()\n    # if the input size is smaller than the target fill it up with zeros (i.e. unks)\n    if sl > sl_in:\n        input = F.pad(input, (0, sl - sl_in, 0, 0, 0, 0), value=0)\n    input = input[:sl]\n    if predict_first_token:\n        input = input[:1]\n        target = target[:1]\n    return F.cross_entropy(input=input.view(-1, vocab),\n                           target=target.view(-1),\n                           ignore_index=pad_idx)\n\n\ndef decoder_loss_label_smoothing(input, target, pad_idx, confidence=0.9, **kwargs):\n    sl_in, bs_in, vocab = input.size()\n    sl, bs = target.size()\n    # if the input size is smaller than the target fill it up with zeros (i.e. unks)\n    if sl > sl_in:\n        input = F.pad(input, (0, sl - sl_in, 0, 0, 0, 0), value=0.)\n    input = input[:sl]\n    smoothing_pdf = (1. - confidence) / (vocab - 2)  # we subtract the confidence token and the pad token from the vocab\n    targets = torch.zeros_like(input).scatter(2, target.unsqueeze(-1), confidence - smoothing_pdf)\n    targets = (targets + smoothing_pdf)\n    targets.div_(targets.sum(dim=-1).unsqueeze_(-1))\n    targets[..., pad_idx] = 0.  # padding targets are set to 0\n    input = F.log_softmax(input,dim=-1)\n    # we sum over the vocab length and get the mean\n    return F.kl_div(input, targets, size_average=False,reduce=False).sum(dim=-1).mean()\n\n\ndef gaussian_kld(recog_mu, recog_logvar, prior_mu, prior_logvar):\n    kld = -0.5 * torch.sum(\n        1 + recog_logvar - prior_logvar - (prior_mu - recog_mu).pow(2).div(torch.exp(prior_logvar))\n        - torch.exp(recog_logvar).div(torch.exp(prior_logvar)))\n    return kld\n\n\ndef tchebycheff_objective(losses, weights=1., optimal_point=0., norm=2):\n    return losses.sub(optimal_point).pow(norm).mul(weights).sum().pow(1 / norm)\n\n\ndef get_cvae_loss(pad_idx, tchebycheff=False, sigmoid=False, tbc_weights=None, tbc_optimal_point=None, tbc_norm=2):\n    STEP = 0\n    optimal_point = 0. if tbc_optimal_point is None else tbc_optimal_point\n    weights = T([2., 1., 100.]) if tbc_weights is None else tbc_weights\n\n    def cvae_loss(input, target, step=0, max_kld_step=None, **kwargs):\n        predictions, recog_mu, recog_log_var, prior_mu, prior_log_var, bow_logits = input\n        sl, bs, vocab = predictions.size()\n        # dims are sq-1 times bs times vocab\n        dec_input = predictions[:target.size(0)].view(-1, vocab).contiguous()\n        slt = target.size(0)\n        bow_targets = bow_logits.unsqueeze_(0).repeat(slt, 1, 1)\n        target = target.view(-1).contiguous()\n        bow_loss = F.cross_entropy(input=bow_targets.view(-1, vocab), target=target, ignore_index=pad_idx,\n                                   reduce=False).view(-1, bs)\n        bow_loss = bow_loss.mean()\n        # targets are sq-1 times bs (one label for every word)\n        kld_loss = gaussian_kld(recog_mu, recog_log_var, prior_mu, prior_log_var)\n        decoder_loss = F.cross_entropy(input=dec_input,\n                                       target=target,\n                                       ignore_index=pad_idx,\n                                       )\n        kld_weight = 1.0 if max_kld_step is None else min((step + 1) / max_kld_step, 1)\n        nonlocal STEP\n        if step > STEP:\n            if step == 0: STEP = 0\n            print(f""\\nlosses: decoder {decoder_loss}, bow: {bow_loss}, kld x weight: {kld_loss} x {kld_weight}"")\n            STEP += 1\n        return decoder_loss + bow_loss + kld_loss * kld_weight\n\n    def cvae_loss_tchebycheff(input, target, step=0, **kwargs):\n        predictions, recog_mu, recog_log_var, prior_mu, prior_log_var, bow_logits = input\n        sl, bs, vocab = predictions.size()\n        # dims are sq-1 times bs times vocab\n        dec_input = predictions[:target.size(0)].view(-1, vocab).contiguous()\n        slt = target.size(0)\n        bow_targets = bow_logits.unsqueeze_(0).repeat(slt, 1, 1)\n        target = target.view(-1).contiguous()\n        bow_loss = F.cross_entropy(input=bow_targets.view(-1, vocab), target=target, ignore_index=pad_idx,\n                                   reduce=False).view(-1, bs)\n        bow_loss = bow_loss.mean()\n        # targets are sq-1 times bs (one label for every word)\n        kld_loss = gaussian_kld(recog_mu, recog_log_var, prior_mu, prior_log_var)\n        decoder_loss = F.cross_entropy(input=dec_input,\n                                       target=target,\n                                       ignore_index=pad_idx,\n                                       )\n        # kld_weight = 1.0 if max_kld_step is None else min((step + 1) / max_kld_step, 1)\n        nonlocal STEP\n        if step > STEP:\n            print(f""\\nlosses: decoder {decoder_loss}, bow: {bow_loss}, kld x weight: {kld_loss}"")\n            STEP += 1\n        losses = torch.cat([decoder_loss.view(1), bow_loss.view(1), kld_loss.view(1)])\n        loss = tchebycheff_objective(losses, weights=weights, norm=tbc_norm, optimal_point=optimal_point)\n        return loss\n\n    def cvae_loss_sigmoid(input, target, step=0, max_kld_step=None, **kwargs):\n        predictions, recog_mu, recog_log_var, prior_mu, prior_log_var, bow_logits = input\n        vocab = predictions.size(-1)\n        # dims are sq-1 times bs times vocab\n        dec_input = predictions[:target.size(0)].view(-1, vocab).contiguous()\n        bow_targets = torch.zeros_like(bow_logits).scatter(1, target.transpose(1, 0), 1)\n        # mask pad token\n        weights = to_gpu(V(torch.ones(bow_logits.size(-1)).unsqueeze_(0)))\n        weights[0, pad_idx] = 0\n        bow_loss = F.binary_cross_entropy_with_logits(bow_logits, bow_targets, weight=weights)\n\n        # targets are sq-1 times bs (one label for every word)\n        kld_loss = gaussian_kld(recog_mu, recog_log_var, prior_mu, prior_log_var)\n        target = target.view(-1).contiguous()\n        decoder_loss = F.cross_entropy(input=dec_input,\n                                       target=target,\n                                       ignore_index=pad_idx,\n                                       )\n        kld_weight = 1.0 if max_kld_step is None else min((step + 1) / max_kld_step, 1)\n        nonlocal STEP\n        if step > STEP:\n            if step == 0: STEP = 0\n            print(f""losses: decoder {decoder_loss}, bow: {bow_loss}, kld x weight: {kld_loss} x {kld_weight}"")\n            STEP += 1\n\n        return decoder_loss + bow_loss + kld_loss * kld_weight\n\n    if tchebycheff:\n        return cvae_loss_tchebycheff\n    elif sigmoid:\n        return cvae_loss_sigmoid\n    else:\n        return cvae_loss\n\n\nclass EncoderDecoderLearner(Learner):\n\n    def s2sloss(self, input, target, label_smoothing_confidence=None, pad_idx=1, **kwargs):\n        if label_smoothing_confidence is None:\n            return decoder_loss(input=input, target=target, pad_idx=pad_idx, **kwargs)\n        else:\n            return decoder_loss_label_smoothing(input=input, target=target, confidence=label_smoothing_confidence,\n                                                pad_idx=pad_idx,\n                                                **kwargs\n                                                )\n\n    def __init__(self, data, models, label_smoothing_confidence=None, predict_first_token=False, **kwargs):\n        tchebycheff_loss = kwargs.pop(""tchebycheff"", False)\n        super().__init__(data, models, **kwargs)\n        if isinstance(models, CVAEModel):\n            self.crit = get_cvae_loss(pad_idx=1, tchebycheff=tchebycheff_loss,\n                                      sigmoid=kwargs.get(""sigmoid"", False),\n                                      )\n        else:\n            self.crit = partial(self.s2sloss, label_smoothing_confidence=label_smoothing_confidence,\n                                predict_first_token=predict_first_token)\n        self.fit_gen = partial(self.fit_gen, stepper=S2SStepper)\n\n    def save_encoder(self, name):\n        save_model(self.model[0], self.get_model_path(name))\n\n    def load_encoder(self, name):\n        load_model(self.model[0], self.get_model_path(name))\n\n    def predict_with_targs(self, is_test=False):\n        return self.predict_with_targs_and_inputs(is_test=is_test)[:2]\n\n    def predict_with_targs_and_inputs(self, is_test=False, num_beams=1):\n        dl = self.data.test_dl if is_test else self.data.val_dl\n        return predict_with_seq2seq(self.model, dl, num_beams=num_beams)\n\n    def predict_array(self, arr):\n        raise NotImplementedError\n\n    def summary(self):\n        print(self.model)\n\n    def predict(self, is_test=False):\n        dl = self.data.test_dl if is_test else self.data.val_dl\n        pr, *_ = predict_with_seq2seq(self.model, dl)\n        return pr\n'"
src/quicknlp/data/model_helpers.py,0,"b'from typing import Dict, List, Union\n\nimport numpy as np\nimport pandas as pd\nfrom fastai.core import BasicModel, VV, to_np\nfrom torchtext.data import Field\n\nBeamTokens = List[str]\n\n\ndef get_beam_strings(tokens: np.ndarray, field: Field) -> BeamTokens:\n    beams = []\n    for row in tokens:\n        words = [field.vocab.itos[i] for i in row]\n        if field.eos_token in words:\n            words = words[:words.index(field.eos_token)]\n        elif field.pad_token in words:\n            words = words[:words.index(field.pad_token)]\n        beams.append("" "".join(words[1:]))\n    return beams\n\n\nBatchBeamTokens = List[BeamTokens]\n\n\nclass PrintingMixin:\n    fields: Dict[str, Field]\n\n    def itos(self, tokens: Union[List[np.ndarray], np.ndarray], field_name: str) -> BatchBeamTokens:\n        field = self.fields[field_name]\n        # token batch has dims [sl, bs, nb]\n        token_batch = np.expand_dims(tokens, axis=-1) if tokens.ndim == 2 else tokens\n        batch = []\n        # bb is one batch dims: [bs ,nb, sl]\n        for bb in token_batch.transpose(1, 2, 0):\n            # one row is one beam for the batch\n            beams: List[str] = get_beam_strings(bb, field)\n            batch.append(beams)\n        # Batch list of beam list of\n        return batch\n\n    def stoi(self, sentences: List[str], field_name: str) -> np.ndarray:\n        results = []\n        for sentence in sentences:\n            sentence = self.fields[field_name].preprocess(sentence)\n            sentence = self.fields[field_name].tokenize(sentence)\n            tokens = [self.fields[field_name].vocab.stoi(i) for i in sentence]\n            results.append(tokens)\n        return np.asarray(results)\n\n\ndef check_columns_in_df(df: pd.DataFrame, columns: List[str]) -> bool:\n    if df is not None:\n        return (df.columns.union(columns) == df.columns).all()\n    else:\n        return True\n\n\ndef predict_with_seq2seq(m, dl, num_beams=1):\n    m.eval()\n    if hasattr(m, \'reset\'):\n        m.reset()\n    inputs, predictions, targets = [], [], []\n    for *x, y in iter(dl):\n        inputs.append(to_np(x[0]))\n        targets.append(to_np(y))\n        prediction, *_ = m(*VV(x), num_beams=num_beams)\n        predictions.append(to_np(prediction))\n    return predictions, targets, inputs\n\n\nclass S2SModel(BasicModel):\n    def get_layer_groups(self, do_fc=False):\n        return [self.model.encoder, self.model.decoder]\n\n\nclass HREDModel(BasicModel):\n    def get_layer_groups(self, do_fc=False):\n        layers = [self.model.encoder.embedding_layer, self.model.encoder.query_encoder_layer,\n                  self.model.encoder.session_encoder_layer, self.model.decoder_state_linear]\n        if self.model.decoder.embedding_layer.encoder is not self.model.encoder.embedding_layer.encoder:\n            layers += [self.model.decoder.embedding_layer]\n        layers += [self.model.decoder.decoder_layer]\n        if len(self.model.decoder.projection_layer.layers) > 2:\n            layers += [self.model.decoder.projection_layer.layers[:2]]\n        if self.model.decoder.projection_layer.layers[\n            -1].weight is not self.model.decoder.embedding_layer.encoder.weight:\n            layers += [self.model.decoder.projection_layer.layers[-1]]\n        return layers\n\n\nclass HREDAttentionModel(BasicModel):\n    def get_layer_groups(self, do_fc=False):\n        return [self.model]\n\n\nclass CVAEModel(BasicModel):\n    def get_layer_groups(self, do_fc=False):\n        layers = [self.model.encoder.embedding_layer, self.model.encoder.query_encoder_layer,\n                  self.model.encoder.session_encoder_layer, self.model.prior_network, self.model.recognition_network,\n                  self.model.bow_network, self.model.decoder_state_linear]\n        if self.model.decoder.embedding_layer.encoder is not self.model.encoder.embedding_layer.encoder:\n            layers += [self.model.decoder.embedding_layer]\n        layers += [self.model.decoder.decoder_layer]\n        if len(self.model.decoder.projection_layer.layers) > 2:\n            layers += [self.model.decoder.projection_layer.layers[:2]]\n        if self.model.decoder.projection_layer.layers[\n            -1].weight is not self.model.decoder.embedding_layer.encoder.weight:\n            layers += [self.model.decoder.projection_layer.layers[-1]]\n        return layers\n'"
src/quicknlp/data/s2s_model_data_loader.py,0,"b'from functools import partial\nfrom typing import Callable, List, Optional\n\nimport pandas as pd\nfrom fastai.core import to_gpu\nfrom fastai.dataset import ModelData\nfrom torch import optim\nfrom torchtext.data import Dataset, Field\n\nfrom quicknlp.data.torchtext_data_loaders import S2SDataLoader\nfrom quicknlp.models import Seq2Seq, Seq2SeqAttention, Transformer\nfrom .datasets import NamedField, TabularDatasetFromDataFrame, TabularDatasetFromFiles\nfrom .learners import EncoderDecoderLearner\nfrom .model_helpers import PrintingMixin, S2SModel, check_columns_in_df\n\n\nclass S2SModelData(ModelData, PrintingMixin):\n    """"""\n    This class provides the entrypoing for dealing with supported NLP S2S Tasks, i.e. tasks where each sample involves\n    multiple sentences, e.g. Translation, Q/A etc.\n    1. Use one of the factory constructors (from dataframes, from text files) to obtain an instance of the class\n    2. use the get_model method to return an instance of one of the provided models\n    3. Use stoi, itos functions to quickly convert between tokens and sentences\n\n    """"""\n\n    def __init__(self, path: str, fields: List[NamedField], source_names: List[str], target_names: List[str],\n                 trn_ds: Dataset, val_ds: Dataset, test_ds: Dataset, bs: int,\n                 sort_key: Optional[Callable] = None,\n                 **kwargs):\n        """""" Constructor for the class. An important thing that happens here is\n        that the field\'s ""build_vocab"" method is invoked, which builds the vocabulary\n        for this NLP model.\n\n        Also, three instances of a BucketIterator are constructed; one each\n        for training data (self.trn_dl), validation data (self.val_dl), and the\n        testing data (self.test_dl)\n\n        Args:\n            path (str): the path to save the data\n            fields (List[NamedField]): a list of the named fields that each example will use\n            trn_ds (Dataset): a pytorch Dataset with the training data\n            val_ds (Dataset): a pytorch Dataset with the validation data\n            test_ds (Dataset: a pytorch Dataset with the test data\n            bs (int): the batch_size\n            sort_key (Optional[Callable]): A function to sort the data in the batches. I should provide the name of a\n                field to use. If None the name of the first field in fields will be used to sort the batch.\n            backwards (bool): Reverse the order of the text or not (not implemented yet)\n            **kwargs: Other arguments to be passed to the BucketIterator and the fields build_vocab function\n        """"""\n\n        self.bs = bs\n        self.nt = dict()\n        for index, (name, field) in enumerate(fields):\n            if not hasattr(field, \'vocab\'):\n                field.build_vocab(trn_ds, **kwargs)\n            self.nt[name] = len(field.vocab)\n        self.pad_idx = fields[0][1].vocab.stoi[fields[0][1].pad_token]\n        self.eos_idx = fields[0][1].vocab.stoi[fields[0][1].eos_token]\n\n        trn_dl, val_dl, test_dl = [S2SDataLoader(ds, bs, source_names=source_names,\n                                                 target_names=target_names, sort_key=sort_key,\n                                                 )\n                                   if ds is not None else None\n                                   for ds in (trn_ds, val_ds, test_ds)]\n        super(S2SModelData, self).__init__(path=path, trn_dl=trn_dl, val_dl=val_dl, test_dl=test_dl)\n        self.fields = trn_ds.fields\n\n    @property\n    def sz(self):\n        return self.bs\n\n    @classmethod\n    def from_dataframes(cls, path: str, fields: List[NamedField], source_names: List[str], target_names: List[str],\n                        train_df: pd.DataFrame, val_df: pd.DataFrame,\n                        test_df: Optional[pd.DataFrame] = None, bs: int = 64, sort_key: Optional[Callable] = None,\n                        **kwargs) -> \'S2SModelData\':\n        """"""Method used to instantiate a S2SModelData object that can be used for a supported NLP Task from dataframes\n\n        Args:\n            path (str): the absolute path in which temporary model data will be saved\n            fields (List[NamedField]): A list of Tuple[Name,Field] for every field to be used in the data\n                if multiple fields should use the same vocab, the same field should be passed to them\n            source_names (List[str]): A list of field names to be used from the fields for the encoder inputs\n            target_names (List[str]): A list of field names to be used from the fields for the decoder inputs\n            train_df (pd.DataFrame):  a pandas DataFrame with the training Data\n            val_df (str): a pandas DataFrame with the validation Data\n            test_df (Optional[str]):a pandas DataFrame with the test Data\n            bs (Optional[int]): the batch size\n            sort_key (Optional[Callable]): A function to sort the examples in batch size based on a field\n            **kwargs:\n\n        Returns:\n            a S2SModel Data instance, which provides datasets for training, validation, testing\n\n        Note:\n            see also the fastai.nlp.LanguageModelData class which inspired this class\n\n        """"""\n        columns = TabularDatasetFromDataFrame.columns(fields)\n        assert check_columns_in_df(train_df, columns)\n        assert check_columns_in_df(val_df, columns)\n        assert check_columns_in_df(test_df, columns)\n        datasets = TabularDatasetFromDataFrame.splits(fields=fields,\n                                                      train_df=train_df,\n                                                      val_df=val_df,\n                                                      test_df=test_df)\n\n        train_ds = datasets[0]\n        val_ds = datasets[1]\n        test_ds = datasets[2] if len(datasets) == 3 else None\n        return cls(path=path, fields=fields, trn_ds=train_ds, val_ds=val_ds, test_ds=test_ds, source_names=source_names,\n                   target_names=target_names, bs=bs, sort_key=sort_key, **kwargs)\n\n    @classmethod\n    def from_text_files(cls, path: str, fields: List[NamedField], source_names: List[str], target_names: List[str],\n                        train: str, validation: str,\n                        test: Optional[str] = None, bs: Optional[int] = 64, sort_key: Optional[Callable] = None,\n                        **kwargs) -> \'S2SModelData\':\n        """"""Method used to instantiate a S2SModelData object that cna be used for a supported NLP Task from files\n\n        Args:\n            path (str): the absolute path in which temporary model data will be saved\n            fields (List[NamedField]): A list of Tuple[Name,Field] for every field to be used in the data\n                if multiple fields should use the same vocab, the same field should be passed to them\n            source_names (List[str]): A list of field names to be used from the fields for the encoder inputs\n            target_names (List[str]): A list of field names to bused from the fields for the decoder inputs\n            train (str):  The path to the training data\n            validation (str):  The path to the test data\n            test (Optional[str]): The path to the test data\n            bs (Optional[int]): the batch size\n            sort_key (Optional[Callable]): A function to sort the examples in batch size based on a field\n            **kwargs:\n\n        Returns:\n            a S2SModel Data instance, which provides datasets for training, validation, testing\n\n        Note:\n            see also the fastai.nlp.LanguageModelData class which inspired this class\n\n        """"""\n        assert isinstance(fields, list) and isinstance(fields[0], tuple) and isinstance(fields[0][1], Field)\n        datasets = TabularDatasetFromFiles.splits(path=path, train=train, validation=validation,\n                                                  test=test,\n                                                  fields=fields)\n        trn_ds = datasets[0]\n        val_ds = datasets[1]\n        test_ds = datasets[2] if len(datasets) == 3 else None\n        return cls(path=path, fields=fields, source_names=source_names, target_names=target_names,\n                   trn_ds=trn_ds, val_ds=val_ds, test_ds=test_ds, bs=bs, sort_key=sort_key, **kwargs)\n\n    def to_model(self, m, opt_fn):\n        model = S2SModel(to_gpu(m))\n        return EncoderDecoderLearner(self, model, opt_fn=opt_fn)\n\n    def get_model(self, opt_fn=None, emb_sz=300, nhid=512, nlayers=2, max_tokens=100, **kwargs):\n        if opt_fn is None:\n            opt_fn = partial(optim.Adam, betas=(0.7, 0.99))\n        m = Seq2Seq(\n            ntoken=[self.nt[name] for name in self.trn_dl.source_names],\n            emb_sz=emb_sz,\n            nhid=nhid,\n            nlayers=nlayers,\n            pad_token=self.pad_idx,\n            eos_token=self.eos_idx,\n            max_tokens=max_tokens,\n            **kwargs\n        )\n        return self.to_model(m, opt_fn)\n\n\nclass S2SAttentionModelData(S2SModelData):\n    def get_model(self, opt_fn=None, emb_sz=300, nhid=512, nlayers=2, max_tokens=100, att_nhid=512,\n                  **kwargs):\n        if opt_fn is None:\n            opt_fn = partial(optim.Adam, betas=(0.7, 0.99))\n        m = Seq2SeqAttention(\n            ntoken=[self.nt[name] for name in self.trn_dl.source_names],\n            emb_sz=emb_sz,\n            nhid=nhid,\n            nlayers=nlayers,\n            pad_token=self.pad_idx,\n            eos_token=self.eos_idx,\n            max_tokens=max_tokens,\n            att_nhid=att_nhid,\n            **kwargs\n        )\n        return self.to_model(m, opt_fn)\n\n\nclass TransformerModelData(S2SModelData):\n    def get_model(self, opt_fn=None, emb_sz=300, nlayers=2, max_tokens=100, nhid=512, num_heads=1, **kwargs):\n        if opt_fn is None:\n            opt_fn = partial(optim.Adam, betas=(0.7, 0.99))\n        m = Transformer(\n            ntoken=[self.nt[name] for name in self.trn_dl.source_names],\n            emb_size=emb_sz,\n            nlayers=nlayers,\n            pad_token=self.pad_idx,\n            eos_token=self.eos_idx,\n            max_tokens=max_tokens,\n            nhid=nhid,\n            num_head=num_heads,\n            **kwargs\n        )\n        return self.to_model(m, opt_fn)\n'"
src/quicknlp/data/sampler.py,1,"b'import numpy as np\nfrom torch.utils.data.sampler import Sampler\n\n\nclass DialogueSampler(Sampler):\n    """"""Returns an iterator that traverse dialogue samples in order. Samples are ordered in descending order\n       from longest conversation with largest utterances to shortest conversation with shortest utterances\n    """"""\n\n    def __init__(self, data_source):\n        super().__init__(data_source=data_source)\n        self.data_source = data_source\n\n    def sort(self, indeces, reverse=True):\n        sorted_by_sentence_length = sorted(indeces, key=lambda x: self.data_source[x][0].shape[1], reverse=reverse)\n        sorted_by_conv_length = sorted(sorted_by_sentence_length, key=lambda x: self.data_source[x][0].shape[0],\n                                       reverse=reverse)\n        return sorted_by_conv_length\n\n    def __len__(self): return len(self.data_source)\n\n    def __iter__(self):\n        idxs = np.arange(len(self.data_source))\n        idxs = self.sort(idxs)\n        return iter(idxs)\n\n\nclass DialogueRandomSampler(DialogueSampler):\n    """"""Returns an iterator that traverses dialogues samples in semi random order. Data are split randomly in batches\n        of size bs * 50 and are sorted within by dialogue length and utterance length in descending order.\n    """"""\n    BATCH_WINDOW = 50\n\n    def __init__(self, data_source, bs):\n        super().__init__(data_source=data_source)\n        self.bs = bs\n\n    def __iter__(self):\n        idxs = np.random.permutation(len(self.data_source))\n        sz = self.bs * self.BATCH_WINDOW\n        ck_idx = [idxs[i:i + sz] for i in range(0, len(idxs), sz)]\n        sort_idx = np.concatenate([self.sort(s) for s in ck_idx])\n        sz = self.bs\n        ck_idx = [sort_idx[i:i + sz] for i in range(0, len(sort_idx), sz)]\n        max_ck = np.argmax([ck[0] for ck in ck_idx])  # find the chunk with the largest key,\n        ck_idx[0], ck_idx[max_ck] = ck_idx[max_ck], ck_idx[0]  # then make sure it goes first.\n        sort_idx = np.concatenate(np.random.permutation(ck_idx[1:]))\n        sort_idx = np.concatenate((ck_idx[0], sort_idx))\n        return iter(sort_idx)\n'"
src/quicknlp/data/spacy_tokenizer.py,0,"b'__author__ = ""Agis Oikonomou""\n\nimport re\n\nimport spacy\nfrom spacy.symbols import ORTH\n\n\nclass SpacyTokenizer:\n    """"""\n    Spacy tokenizer can tokenizes a sentence using\n    """"""\n\n    def __init__(self, language=""en"", special_cases=None, regex_cases=None):\n        self.nlp = spacy.load(language)\n        self.nlp.tokenizer.add_special_case(\'<eos>\', [{ORTH: \'<eos>\'}])\n        self.nlp.tokenizer.add_special_case(\'<bos>\', [{ORTH: \'<bos>\'}])\n        self.nlp.tokenizer.add_special_case(\'<sos>\', [{ORTH: \'<sos>\'}])\n        self.nlp.tokenizer.add_special_case(\'<unk>\', [{ORTH: \'<unk>\'}])\n        self.nlp.tokenizer.add_special_case(\'<pad>\', [{ORTH: \'<pad>\'}])\n        special_cases = [] if special_cases is None else special_cases\n        for case in special_cases:\n            self.nlp.tokenizer.add_special_case(case, [{ORTH: case}])\n        self.regex_cases = [] if regex_cases is None else [re.compile(i, flags=re.IGNORECASE) for i in regex_cases]\n\n    def __call__(self, x, sentence=False):\n        if sentence:\n            return [[word.text for word in self.replace_regex(sentence)] for sentence in self.nlp(x).sents]\n        else:\n            return [tok.text for tok in self.replace_regex(self.nlp.tokenizer(x))]\n\n    def replace_regex(self, doc):\n        for pattern in self.regex_cases:\n            doc = self.replace_regex_pattern(doc, pattern)\n        return doc\n\n    def replace_regex_pattern(self, doc, pattern):\n        indexes = [m.span() for m in pattern.finditer(doc.text)]\n        for start, end in indexes:\n            doc.merge(start_idx=start, end_idx=end)\n        return doc\n'"
src/quicknlp/data/torchtext_data_loaders.py,0,"b'from typing import List, Optional, Callable, Union\n\nfrom torch import cuda as cuda\nfrom torchtext.data import Dataset, BucketIterator\n\nfrom quicknlp.data.iterators import HierarchicalIterator, DialogueIterator\n\n\nclass S2SDataLoader:\n    """"""Instance of ModelLoader. It is an iterator that buckets the data in batches of similar sizes based on\n       a sort_key and iterates through the batches.\n\n    """"""\n\n    def __init__(self, dataset: Dataset, batch_size: int, source_names: List[str], target_names: List[str],\n                 sort_key: Optional[Callable] = None, **kwargs):\n        self.dataset = dataset\n        self.source_names = source_names\n        self.target_names = target_names\n        # sort by the first field if no sort key is given\n        if sort_key is None:\n            def sort_key(x):\n                return getattr(x, self.source_names[0])\n        device = None if cuda.is_available() else -1\n        self.dl = BucketIterator(dataset, batch_size=batch_size, sort_key=sort_key, device=device, **kwargs)\n        self.bs = batch_size\n        self.iter = 0\n\n    def __iter__(self):\n        self.iter = 0\n        for batch in self.dl:\n            if self.iter >= len(self):\n                raise StopIteration\n            source = [getattr(batch, name) for name in self.source_names]\n            # target should start from the second token for S2S\n            target = [getattr(batch, name)[1:] for name in self.target_names]\n            yield source + target\n            self.iter += 1\n\n    def __len__(self):\n        """"""number of batches to go through all the data""""""\n        return len(self.dl)\n\n\nclass HierarchicalDataLoader:\n    """"""Loads Hierarchical data into batches, including source and target""""""\n\n    def __init__(self, dataset: Dataset, batch_size: int, target_names: Optional[List[str]] = None,\n                 sort_key: Union[Callable, str] = ""sl"", max_context_size: int = 130000, backwards=False,\n                 **kwargs):\n        self.dataset = dataset\n        target_names = [target_names] if isinstance(target_names, str) else target_names\n        # sort by the first field if no sort key is given\n        if sort_key == ""cl"":\n            def sort_key(x):\n                """"""sort examples by largest conversation length length in example""""""\n                return len(x.roles)\n        elif sort_key == \'sl\':\n            def sort_key(x):\n                """"""sort examples by largest utterance  length in example""""""\n                return max(x.sl)\n        else:\n            assert callable(sort_key), ""sort_key provided is not a function""\n        self.dl = HierarchicalIterator(dataset, batch_size=batch_size, sort_key=sort_key, target_roles=target_names,\n                                       max_context_size=max_context_size, **kwargs)\n        self.bs = batch_size\n        self.iter = 0\n\n    def __iter__(self):\n        self.iter = 0\n        for batch in self.dl:\n            if self.iter >= len(self):\n                raise StopIteration\n            yield [batch.context, batch.response, batch.targets]\n            self.iter += 1\n\n    def __len__(self):\n        """"""number of batches to go through all the data""""""\n        return len(self.dl)\n\n\nclass DialogueTTDataLoader:\n    """"""Loads Hierarchical data into batches, including source and target""""""\n\n    def __init__(self, dataset: Dataset, batch_size: int, target_names: Optional[List[str]] = None,\n                 max_context_size: int = 130000, backwards=False,\n                 **kwargs):\n        self.dataset = dataset\n        target_names = [target_names] if isinstance(target_names, str) else target_names\n\n        def sort_key_inner(x):\n            """"""sort key inner should be utterance size""""""\n            return max(x.sl)\n\n        def sort_key_outer(x):\n            """"""sort key inner should be dialogues size""""""\n            return len(x.roles)\n\n        sort_key = sort_key_inner\n        self.dl = DialogueIterator(dataset, batch_size=batch_size, sort_key=sort_key, sort_key_inner=sort_key_inner,\n                                   sort_key_outer=sort_key_outer, target_roles=target_names,\n                                   max_context_size=max_context_size, **kwargs)\n        self.bs = batch_size\n        self.iter = 0\n\n    def __iter__(self):\n        self.iter = 0\n        for batch in self.dl:\n            if self.iter >= len(self):\n                raise StopIteration\n            yield [batch.context, batch.response, batch.targets]\n            self.iter += 1\n\n    def __len__(self):\n        """"""number of batches to go through all the data""""""\n        return len(self.dl)\n'"
src/quicknlp/data/vocab.py,0,"b'from collections import Counter\nfrom typing import List\n\nfrom nltk import FreqDist\n\nTokens = List[str]\n\n\nclass Vocab:\n\n    def __init__(self, tokens: List[Tokens], special_symbols: List[str] = None):\n        special_symbols = [] if special_symbols is None else special_symbols\n        special_symbols = special_symbols + [""<eot>"", ""<response>"", ""<eos>"", ""<unk>"", ""<pad>"", ""<bos>""]\n        self.vocab = FreqDist()\n        self.cdf = 0.\n        for sample in tokens:\n            for token in sample:\n                if token not in special_symbols:\n                    self.vocab[token] += 1\n\n        print(f""total samples in vocab: {self.vocab.N()}, total tokens in vocab: {self.vocab.B()}"")\n        self.itos = []\n        self.stoi = {}\n\n    def fit(self, num_tokens=15000):\n        cdf = 0.\n        for cdf in self.vocab._cumulative_frequencies([i[0] for i in self.vocab.most_common(num_tokens)]):\n            pass\n        self.cdf = cdf / self.vocab.N()\n        print(f""cdf of the {num_tokens} most common tokens in vocab {self.cdf}"")\n        self.itos = [""<unk>"", ""<pad>"", ""<eos>"", ""<bos>""] + [tup[0] for tup in self.vocab.most_common(num_tokens)]\n        self.stoi = Counter({key: index for index, key in enumerate(self.itos)})\n'"
src/quicknlp/models/__init__.py,0,b'from .hred import HRED\nfrom .cvae import CVAE\nfrom .seq2seq import Seq2Seq\nfrom .seq2seq_attention import Seq2SeqAttention\nfrom .transformer import Transformer\n'
src/quicknlp/models/cvae.py,8,"b'from typing import List, Union\n\nimport torch\nimport torch.nn as nn\nfrom fastai.core import V, to_gpu\n\nfrom quicknlp.utils import assert_dims, concat_bidir_state\nfrom .hred import HRED\n\nHParam = Union[List[int], int]\n\n\nclass CVAE(HRED):\n    """"""Basic CVAE model see:\n    T. Zhao, R. Zhao, and M. Eskenazi, \xe2\x80\x9cLearning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders,\xe2\x80\x9d arXiv.org, vol. cs.CL. 31-Mar-2017.\n    github: https://github.com/snakeztc/NeuralDialog-CVAE\n    arxiv: https://arxiv.org/abs/1703.10960\n    """"""\n\n    def __init__(self, ntoken: int, emb_sz: HParam, nhid: HParam, nlayers: HParam, pad_token: int,\n                 eos_token: int, latent_dim: int, bow_nhid: int, max_tokens: int = 50,\n                 share_embedding_layer: bool = False,\n                 tie_decoder: bool = True, cell_type=""gru"",\n                 bidir: bool = False, **kwargs):\n        """"""\n\n        Args:\n            ntoken (int): Number of tokens for the encoder and the decoder\n            emb_sz (Union[List[int],int]): Embedding size for the encoder and decoder embeddings\n            nhid (Union[List[int],int]): Number of hidden dims for the encoder (first two values) and the decoder\n            nlayers (Union[List[int],int]): Number of layers for the encoder and the decoder\n            pad_token (int): The  index of the token used for padding\n            eos_token (int): The index of the token used for eos\n            latent_dim (int): The dim of the latent variable\n            bow_nhid (int): The dim of the bow training network dimensions\n            max_tokens (int): The maximum number of steps the decoder iterates before stopping\n            share_embedding_layer (bool): if True the decoder shares its input and output embeddings\n            tie_decoder (bool): if True the encoder and the decoder share their embeddings\n            bidir (bool): if True use a bidirectional encoder\n            **kwargs: Extra embeddings that will be passed to the encoder and the decoder\n        """"""\n        assert cell_type == ""gru"", ""lstm not supported""\n        super().__init__(ntoken=ntoken, emb_sz=emb_sz, nhid=nhid, nlayers=nlayers, pad_token=pad_token,\n                         eos_token=eos_token, max_tokens=max_tokens, share_embedding_layer=share_embedding_layer,\n                         tie_decoder=tie_decoder, bidir=bidir, cell_type=cell_type, **kwargs)\n        self.latent_dim = latent_dim\n        self.recognition_network = nn.Linear(\n            in_features=self.encoder.output_size + self.encoder.query_encoder.output_size,\n            out_features=latent_dim * 2)\n        self.prior_network = nn.Sequential(\n            nn.Linear(in_features=self.encoder.output_size, out_features=latent_dim),\n            nn.Tanh(),\n            nn.Linear(in_features=latent_dim, out_features=latent_dim * 2)\n        )\n        self.bow_network = nn.Sequential(nn.Linear(in_features=latent_dim + self.encoder.output_size,\n                                                   out_features=bow_nhid),\n                                         nn.Tanh(),\n                                         nn.Dropout(p=kwargs.get(\'dropout_b\', 0.2)),\n                                         nn.Linear(in_features=bow_nhid, out_features=self.decoder.output_size)\n                                         )\n        self.decoder_state_linear = nn.Linear(in_features=self.encoder.output_size + latent_dim,\n                                              out_features=self.decoder.layers[0].output_size)\n\n    def reparameterize(self, mu, logvar):\n        if self.training:\n            std = torch.exp(0.5 * logvar)\n            eps = to_gpu(V(torch.randn(self.latent_dim)))\n            return mu + eps * std\n        else:\n            return mu\n\n    def forward(self, *inputs, num_beams=0):\n        with torch.set_grad_enabled(self.training):\n            encoder_inputs, decoder_inputs = assert_dims(inputs, [2, None, None])  # dims: [sl, bs] for encoder and decoder\n            # reset the states for the new batch\n            num_utterances, max_sl, bs = encoder_inputs.size()\n            self.reset_encoders(bs)\n            outputs, session = self.encoder(encoder_inputs)\n            self.encoder.query_encoder.reset(bs)\n            decoder_outputs = self.encoder.query_encoder(decoder_inputs)\n            decoder_out = concat_bidir_state(self.encoder.query_encoder_layer.get_last_hidden_state(),\n                                             cell_type=self.cell_type, nlayers=1,\n                                             bidir=self.encoder.bidir\n                                             )\n            x = torch.cat([session, decoder_out], dim=-1)\n            prior_log_var, prior_mu, recog_log_var, recog_mu, session = self.variational_encoding(session, x)\n            bow_logits = self.bow_network(session).squeeze(0) if num_beams == 0 else None\n\n            state, constraints = self.encoder_hidden_state_projection(session)\n            outputs_dec, predictions = self.decoding(decoder_inputs, num_beams, state)\n            if num_beams == 0:\n                return [predictions, recog_mu, recog_log_var, prior_mu, prior_log_var, bow_logits], [*outputs, *outputs_dec]\n            else:\n                return predictions, [*outputs, *outputs_dec]\n\n    def variational_encoding(self, session, x):\n        recog_mu_log_var = self.recognition_network(x)\n        recog_mu, recog_log_var = torch.split(recog_mu_log_var, self.latent_dim, dim=-1)\n        prior_mu_log_var = self.prior_network(session)\n        prior_mu, prior_log_var = torch.split(prior_mu_log_var, self.latent_dim, dim=-1)\n        if self.training:\n            latent_sample = self.reparameterize(recog_mu, recog_log_var)\n        else:\n            latent_sample = self.reparameterize(prior_mu, prior_log_var)\n        session = torch.cat([session, latent_sample], dim=-1)\n        return prior_log_var, prior_mu, recog_log_var, recog_mu, session\n'"
src/quicknlp/models/hred.py,2,"b'import torch\nfrom typing import List, Union\n\nimport torch.nn as nn\n\nfrom quicknlp.modules import Decoder, DropoutEmbeddings, Projection, RNNLayers\nfrom quicknlp.modules.hred_encoder import HREDEncoder\nfrom quicknlp.utils import assert_dims, get_kwarg, get_list\n\nHParam = Union[List[int], int]\n\n\nclass HRED(nn.Module):\n    """"""Basic HRED model\n    paper: A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues. Iulian Vlad Serban et al. 2016a.\n    github: https://github.com/julianser/hed-dlg-truncated\n    arxiv: http://arxiv.org/abs/1605.06069\n    """"""\n\n    BPTT_MAX_UTTERANCES = 1000\n\n    def __init__(self, ntoken: int, emb_sz: HParam, nhid: HParam, nlayers: HParam, pad_token: int,\n                 eos_token: int, max_tokens: int = 50, share_embedding_layer: bool = False, tie_decoder: bool = True,\n                 bidir: bool = False, session_constraint: bool = False, cell_type=""gru"", **kwargs):\n        """"""\n\n        Args:\n            ntoken (int): Number of tokens for the encoder and the decoder\n            emb_sz (Union[List[int],int]): Embedding size for the encoder and decoder embeddings\n            nhid (Union[List[int],int]): Number of hidden dims for the encoder (first two values) and the decoder\n            nlayers (Union[List[int],int]): Number of layers for the encoder and the decoder\n            pad_token (int): The  index of the token used for padding\n            eos_token (int): The index of the token used for eos\n            max_tokens (int): The maximum number of steps the decoder iterates before stopping\n            share_embedding_layer (bool): if True the decoder shares its input and output embeddings\n            tie_decoder (bool): if True the encoder and the decoder share their embeddings\n            bidir (bool): if True use a bidirectional encoder\n            session_constraint (bool) If true the session will be concated as a constraint to the decoder input\n            **kwargs: Extra embeddings that will be passed to the encoder and the decoder\n        """"""\n        super().__init__()\n        # allow for the same or different parameters between encoder and decoder\n        ntoken, emb_sz, nhid, nlayers = get_list(ntoken), get_list(emb_sz, 2), get_list(nhid, 3), get_list(nlayers, 3)\n        dropoutd = get_kwarg(kwargs, name=""dropout_d"", default_value=0.5)  # output dropout\n        dropoute = get_kwarg(kwargs, name=""dropout_e"", default_value=0.1)  # encoder embedding dropout\n        dropoute = get_list(dropoute, 2)\n        dropouti = get_kwarg(kwargs, name=""dropout_i"", default_value=0.65)  # input dropout\n        dropouti = get_list(dropouti, 2)\n        dropouth = get_kwarg(kwargs, name=""dropout_h"", default_value=0.3)  # RNN output layers dropout\n        dropouth = get_list(dropouth, 3)\n        wdrop = get_kwarg(kwargs, name=""wdrop"", default_value=0.5)  # RNN weights dropout\n        wdrop = get_list(wdrop, 3)\n\n        train_init = kwargs.pop(""train_init"", False)  # Have trainable initial states to the RNNs\n        dropoutinit = get_kwarg(kwargs, name=""dropout_init"", default_value=0.1)  # RNN initial states dropout\n        dropoutinit = get_list(dropoutinit, 3)\n        self.cell_type = cell_type\n        self.nt = ntoken[-1]\n        self.pr_force = 1.0\n        self.share_embedding_layer = share_embedding_layer\n        self.tie_decoder = tie_decoder\n\n        self.encoder = HREDEncoder(\n            ntoken=ntoken[0],\n            emb_sz=emb_sz[0],\n            nhid=nhid[:2],\n            nlayers=nlayers[0],\n            bidir=bidir,\n            cell_type=cell_type,\n            dropout_e=dropoute[:2],\n            dropout_i=dropouti[:2],\n            wdrop=wdrop[:2],\n            train_init=train_init,\n            dropoutinit=dropoutinit[:2]\n\n        )\n        if share_embedding_layer:\n            decoder_embedding_layer = self.encoder.embedding_layer\n        else:\n            decoder_embedding_layer = DropoutEmbeddings(ntokens=ntoken[0],\n                                                        emb_size=emb_sz[1],\n                                                        dropoute=dropoute[1],\n                                                        dropouti=dropouti[1]\n                                                        )\n\n        input_size_decoder = kwargs.get(""input_size_decoder"", emb_sz[1])\n        input_size_decoder = input_size_decoder + self.encoder.output_size if session_constraint else input_size_decoder\n        decoder_rnn = RNNLayers(input_size=input_size_decoder,\n                                output_size=kwargs.get(""output_size_decoder"", emb_sz[1]),\n                                nhid=nhid[2], bidir=False, dropouth=dropouth[2],\n                                wdrop=wdrop[2], nlayers=nlayers[2], cell_type=self.cell_type,\n                                train_init=train_init,\n                                dropoutinit=dropoutinit[2]\n                                )\n        self.session_constraint = session_constraint\n        # allow for changing sizes of decoder output\n        input_size = decoder_rnn.output_size\n        nhid = emb_sz[1] if input_size != emb_sz[1] else None\n        projection_layer = Projection(output_size=ntoken[0], input_size=input_size, nhid=nhid, dropout=dropoutd,\n                                      tie_encoder=decoder_embedding_layer if tie_decoder else None\n                                      )\n        self.decoder = Decoder(\n            decoder_layer=decoder_rnn,\n            projection_layer=projection_layer,\n            embedding_layer=decoder_embedding_layer,\n            pad_token=pad_token,\n            eos_token=eos_token,\n            max_tokens=max_tokens,\n        )\n        self.decoder_state_linear = nn.Linear(in_features=self.encoder.output_size,\n                                              out_features=self.decoder.layers[0].output_size)\n\n    def forward(self, *inputs, num_beams=0):\n        with torch.set_grad_enabled(self.training):\n            encoder_inputs, decoder_inputs = assert_dims(inputs, [2, None, None])  # dims: [sl, bs] for encoder and decoder\n            num_utterances, max_sl, bs = encoder_inputs.size()\n            # reset the states for the new batch\n            self.reset_encoders(bs)\n            outputs, last_output = self.encoder(encoder_inputs)\n            state, constraints = self.encoder_hidden_state_projection(last_output)\n            outputs_dec, predictions = self.decoding(decoder_inputs, num_beams, state, constraints=constraints)\n        return predictions, [*outputs, *outputs_dec]\n\n    def encoder_hidden_state_projection(self, last_output):\n        state = self.decoder.hidden\n        # if there are multiple layers we set the state to the first layer and ignore all others\n        # get the session_output of the last layer and the last step\n        if self.cell_type == ""gru"":\n            # Tanh seems to deteriorate performance so not used as a nonlinear (is this a pytorch bug?)\n            state[0] = self.decoder_state_linear(last_output)  # .tanh()\n            constraints = last_output if self.session_constraint else None  # dims  [1, bs, ed]\n        else:\n            # Tanh seems to deteriorate performance so not used as a nonlinear (is this a pytorch bug?)\n            state[0] = self.decoder_state_linear(last_output[0]), self.decoder_state_linear(last_output[1])\n            constraints = last_output[0] if self.session_constraint else None  # dims  [1, bs, ed]\n        return state, constraints\n\n    def reset_encoders(self, bs):\n        self.encoder.reset(bs)\n        self.decoder.reset(bs)\n\n    def decoding(self, decoder_inputs, num_beams, state, constraints=None):\n        if self.training:\n            self.decoder.pr_force = self.pr_force\n            nb = 1 if self.pr_force < 1 else 0\n        else:\n            nb = num_beams\n        outputs_dec = self.decoder(decoder_inputs, hidden=state, num_beams=nb, constraints=constraints)\n        predictions = outputs_dec[:decoder_inputs.size(0)] if num_beams == 0 else self.decoder.beam_outputs\n        return outputs_dec, predictions\n'"
src/quicknlp/models/hred_attention.py,2,"b'from typing import List, Union\n\nimport torch\nimport torch.nn as nn\nfrom fastai.lm_rnn import repackage_var\n\nfrom quicknlp.modules import DropoutEmbeddings, Encoder, RNNLayers, AttentionProjection, \\\n    AttentionDecoder\nfrom quicknlp.utils import assert_dims, get_kwarg, get_list\n\nHParam = Union[List[int], int]\n\n\nclass HREDAttention(nn.Module):\n    """"""Basic HRED model\n    paper: A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues. Iulian Vlad Serban et al. 2016a.\n    github: https://github.com/julianser/hed-dlg-truncated\n    arxiv: http://arxiv.org/abs/1605.06069\n    """"""\n\n    BPTT_MAX_UTTERANCES = 1000\n\n    def __init__(self, ntoken: int, emb_sz: HParam, nhid: HParam, nlayers: HParam, att_nhid: int, pad_token: int,\n                 eos_token: int, max_tokens: int = 50, share_embedding_layer: bool = False, tie_decoder: bool = True,\n                 bidir: bool = False, **kwargs):\n        """"""\n\n        Args:\n            ntoken (int): Number of tokens for the encoder and the decoder\n            emb_sz (Union[List[int],int]): Embedding size for the encoder and decoder embeddings\n            nhid (Union[List[int],int]): Number of hidden dims for the encoder (first two values) and the decoder\n            nlayers (Union[List[int],int]): Number of layers for the encoder and the decoder\n            att_nhid (int): Number of hidden dims for the attention Module\n            pad_token (int): The  index of the token used for padding\n            eos_token (int): The index of the token used for eos\n            max_tokens (int): The maximum number of steps the decoder iterates before stopping\n            share_embedding_layer (bool): if True the decoder shares its input and output embeddings\n            tie_decoder (bool): if True the encoder and the decoder share their embeddings\n            bidir (bool): if True use a bidirectional encoder\n            **kwargs: Extra embeddings that will be passed to the encoder and the decoder\n        """"""\n        super().__init__()\n        # allow for the same or different parameters between encoder and decoder\n        ntoken, emb_sz, nhid, nlayers = get_list(ntoken), get_list(emb_sz, 2), get_list(nhid, 3), get_list(nlayers, 3)\n        dropoutd = get_kwarg(kwargs, name=""dropoutd"", default_value=0.5)  # output dropout\n        dropoute = get_kwarg(kwargs, name=""dropout_e"", default_value=0.1)  # encoder embedding dropout\n        dropoute = get_list(dropoute, 2)\n        dropouti = get_kwarg(kwargs, name=""dropout_i"", default_value=0.65)  # input dropout\n        dropouti = get_list(dropouti, 2)\n        dropouth = get_kwarg(kwargs, name=""dropout_h"", default_value=0.3)  # RNN output layers dropout\n        dropouth = get_list(dropouth, 3)\n        wdrop = get_kwarg(kwargs, name=""wdrop"", default_value=0.5)  # RNN weights dropout\n        wdrop = get_list(wdrop, 3)\n        self.cell_type = ""gru""\n        self.nt = ntoken[-1]\n        self.pr_force = 1.0\n        self.nlayers = nlayers\n\n        encoder_embedding_layer = DropoutEmbeddings(ntokens=ntoken[0],\n                                                    emb_size=emb_sz[0],\n                                                    dropoute=dropoute[0],\n                                                    dropouti=dropouti[0]\n                                                    )\n\n        encoder_rnn = RNNLayers(input_size=emb_sz[0],\n                                output_size=kwargs.get(""output_size_encoder"", emb_sz[0]),\n                                nhid=nhid[0], bidir=bidir,\n                                dropouth=dropouth[0],\n                                wdrop=wdrop[0],\n                                nlayers=nlayers[0],\n                                cell_type=self.cell_type,\n                                )\n        self.query_encoder = Encoder(\n            embedding_layer=encoder_embedding_layer,\n            encoder_layer=encoder_rnn\n\n        )\n        self.session_encoder = RNNLayers(input_size=encoder_rnn.output_size, nhid=nhid[1],\n                                         output_size=kwargs.get(""output_size"", emb_sz[0]), nlayers=1,\n                                         bidir=False, cell_type=self.cell_type,\n                                         wdrop=wdrop[1], dropouth=dropouth[1],\n                                         )\n\n        if share_embedding_layer:\n            decoder_embedding_layer = encoder_embedding_layer\n        else:\n            decoder_embedding_layer = DropoutEmbeddings(ntokens=ntoken[-1],\n                                                        emb_size=emb_sz[-1],\n                                                        dropoute=dropoute[1],\n                                                        dropouti=dropouti[1]\n                                                        )\n\n        decoder_rnn = RNNLayers(input_size=kwargs.get(""input_size"", emb_sz[-1] * 2),\n                                output_size=kwargs.get(""output_size"", emb_sz[-1]),\n                                nhid=nhid[-1], bidir=False, dropouth=dropouth[2],\n                                wdrop=wdrop[2], nlayers=nlayers[-1], cell_type=self.cell_type)\n\n        projection_layer = AttentionProjection(output_size=ntoken[-1],\n                                               input_size=emb_sz[-1],\n                                               dropout=dropoutd,\n                                               att_nhid=att_nhid,\n                                               att_type=""SDP"",\n                                               tie_encoder=decoder_embedding_layer if tie_decoder else None\n                                               )\n        self.decoder = AttentionDecoder(\n            decoder_layer=decoder_rnn,\n            projection_layer=projection_layer,\n            embedding_layer=decoder_embedding_layer,\n            pad_token=pad_token,\n            eos_token=eos_token,\n            max_tokens=max_tokens,\n        )\n\n    def forward(self, *inputs, num_beams=0):\n        encoder_inputs, decoder_inputs = assert_dims(inputs, [2, None, None])  # dims: [sl, bs] for encoder and decoder\n        # reset the states for the new batch\n        bs = encoder_inputs.size(2)\n        self.session_encoder.reset(bs)\n        self.decoder.reset(bs)\n        query_encoder_outputs = []\n        outputs = []\n        num_utterances, max_sl, *_ = encoder_inputs.size()\n        for index, context in enumerate(encoder_inputs):\n            self.query_encoder.reset(bs)\n            outputs = self.query_encoder(context)  # context has size [sl, bs]\n            # BPTT if the dialogue is too long repackage the first half of the outputs to decrease\n            # the gradient backpropagation and fit it into memory\n            # to test before adding back\n            out = repackage_var(outputs[-1][\n                                    -1]) if max_sl * num_utterances > self.BPTT_MAX_UTTERANCES and index <= num_utterances // 2 else \\\n                outputs[-1][-1]\n            query_encoder_outputs.append(out)  # get the last sl output of the query_encoder\n        query_encoder_outputs = torch.stack(query_encoder_outputs, dim=0)  # [cl, bs, nhid]\n        session_outputs = self.session_encoder(query_encoder_outputs)\n        self.decoder.projection_layer.reset(keys=session_outputs[-1])\n        if self.training:\n            self.decoder.pr_force = self.pr_force\n            nb = 1 if self.pr_force < 1 else 0\n        else:\n            nb = num_beams\n        state = self.decoder.hidden\n        outputs_dec = self.decoder(decoder_inputs,hidden=state ,num_beams=nb)\n        predictions = outputs_dec[-1][:decoder_inputs.size(0)] if num_beams == 0 else self.decoder.beam_outputs\n        return predictions, [*outputs, *outputs_dec]\n'"
src/quicknlp/models/hred_constrained.py,2,"b'from typing import List, Union\n\nimport torch\n\nfrom quicknlp.modules import DropoutEmbeddings\nfrom quicknlp.utils import get_kwarg, get_list\nfrom .hred import HRED\n\nHParam = Union[List[int], int]\n\n\nclass HREDConstrained(HRED):\n    def __init__(self, ntoken: int, emb_sz: HParam, nhid: HParam, nlayers: HParam, pad_token: int,\n                 eos_token: int, num_constraints: int, constraints_sz: int, max_tokens: int = 50,\n                 share_embedding_layer: bool = False,\n                 tie_decoder: bool = True,\n                 bidir: bool = False, **kwargs):\n        """"""\n\n        Args:\n            ntoken (int): Number of tokens for the encoder and the decoder\n            emb_sz (Union[List[int],int]): Embedding size for the encoder and decoder embeddings\n            nhid (Union[List[int],int]): Number of hidden dims for the encoder (first two values) and the decoder\n            nlayers (Union[List[int],int]): Number of layers for the encoder and the decoder\n            pad_token (int): The  index of the token used for padding\n            eos_token (int): The index of the token used for eos\n            latent_dim (int): The dim of the latent variable\n            max_tokens (int): The maximum number of steps the decoder iterates before stopping\n            share_embedding_layer (bool): if True the decoder shares its input and output embeddings\n            tie_decoder (bool): if True the encoder and the decoder share their embeddings\n            bidir (bool): if True use a bidirectional encoder\n            **kwargs: Extra embeddings that will be passed to the encoder and the decoder\n        """"""\n\n        super().__init__(ntoken=ntoken, emb_sz=emb_sz, nhid=nhid, nlayers=nlayers, pad_token=pad_token,\n                         eos_token=eos_token, max_tokens=max_tokens, share_embedding_layer=share_embedding_layer,\n                         tie_decoder=tie_decoder, bidir=bidir, input_size_decoder=emb_sz + constraints_sz\n                         )\n\n        dropoute = get_kwarg(kwargs, name=""dropout_e"", default_value=0.1)  # encoder embedding dropout\n        dropoute = get_list(dropoute, 2)\n        dropouti = get_kwarg(kwargs, name=""dropout_i"", default_value=0.65)  # input dropout\n        dropouti = get_list(dropouti, 2)\n        self.constraint_embeddings = DropoutEmbeddings(ntokens=num_constraints,\n                                                       emb_size=constraints_sz,\n                                                       dropoute=dropoute[-1],\n                                                       dropouti=dropouti[-1],\n                                                       )\n\n    def forward(self, *inputs, num_beams=0):\n        with torch.set_grad_enabled(self.training):\n            encoder_inputs, constraints, decoder_inputs = inputs  # dims: [sl, bs] for encoder and decoder\n            # reset the states for the new batch\n            num_utterances, max_sl, bs = encoder_inputs.size()\n            self.reset_encoders(bs)\n            outputs, last_output = self.encoder(encoder_inputs)\n            state, constraints = self.encoder_hidden_state_projection(last_output)\n            # get as a  constraint the second token of the targets\n            constraints = self.constraint_embeddings(constraints)  # dims [bs, ed]\n            constraints = torch.cat([last_output, constraints], dim=-1) if self.session_constraint else constraints\n\n            outputs_dec, predictions = self.decoding(decoder_inputs, num_beams, state, constraints=constraints)\n            return predictions, [*outputs, *outputs_dec]\n'"
src/quicknlp/models/seq2seq.py,2,"b'import torch\n\nimport torch.nn as nn\n\nfrom quicknlp.modules import Projection, RNNLayers, Decoder, Encoder\nfrom quicknlp.modules.embeddings import DropoutEmbeddings\nfrom quicknlp.utils import HParam, assert_dims, concat_bidir_state, get_kwarg, get_list\n\n\nclass Seq2Seq(nn.Module):\n    """"""Basic Seq2Seq model""""""\n\n    def __init__(self, ntoken: HParam, emb_sz: HParam, nhid: HParam, nlayers: HParam, pad_token: int,\n                 eos_token: int, max_tokens: int = 50, share_embedding_layer: bool = False, tie_decoder: bool = True,\n                 bidir: bool = False, **kwargs):\n        """"""\n\n        Args:\n            ntoken (Union[List[int],int]): Number of tokens for the encoder and the decoder\n            emb_sz (Union[List[int],int]): Embedding size for the encoder and decoder embeddings\n            nhid (Union[List[int],int]): Number of hidden dims for the encoder and the decoder\n            nlayers (Union[List[int],int]): Number of layers for the encoder and the decoder\n            pad_token (int): The  index of the token used for padding\n            eos_token (int): The index of the token used for eos\n            max_tokens (int): The maximum number of steps the decoder iterates before stopping\n            share_embedding_layer (bool): if True the decoder shares its input and output embeddings\n            tie_decoder (bool): if True the encoder and the decoder share their embeddings\n            bidir (bool): if True use a bidirectional encoder\n            **kwargs: Extra embeddings that will be passed to the encoder and the decoder\n        """"""\n        super().__init__()\n        # allow for the same or different parameters between encoder and decoder\n        ntoken, emb_sz, nhid, nlayers = get_list(ntoken, 2), get_list(emb_sz, 2), get_list(nhid, 2), get_list(nlayers,\n                                                                                                              2)\n        dropoutd = get_kwarg(kwargs, name=""dropout_d"", default_value=0.5)  # output dropout\n        dropoute = get_kwarg(kwargs, name=""dropout_e"", default_value=0.1)  # encoder embedding dropout\n        dropoute = get_list(dropoute, 2)\n        dropouti = get_kwarg(kwargs, name=""dropout_i"", default_value=0.65)  # input dropout\n        dropouti = get_list(dropouti, 2)\n        dropouth = get_kwarg(kwargs, name=""dropout_h"", default_value=0.3)  # RNN output layers dropout\n        dropouth = get_list(dropouth, 2)\n        wdrop = get_kwarg(kwargs, name=""wdrop"", default_value=0.5)  # RNN weights dropout\n        wdrop = get_list(wdrop, 2)\n        self.cell_type = get_kwarg(kwargs, name=""cell_type"", default_value=""lstm"")\n        encoder_embedding_layer = DropoutEmbeddings(ntokens=ntoken[0],\n                                                    emb_size=emb_sz[0],\n                                                    dropoute=dropoute[0],\n                                                    dropouti=dropouti[0]\n                                                    )\n        self.bidir = bidir\n        self.nlayers = nlayers[0]\n        self.nt = ntoken[-1]  # number of possible tokens\n        self.pr_force = 1.0  # teacher forcing probability\n\n        encoder_rnn = RNNLayers(input_size=emb_sz[0],\n                                output_size=kwargs.get(""out_dim"", emb_sz[0]),\n                                nhid=nhid[0], bidir=bidir,\n                                dropouth=dropouth[0],\n                                wdrop=wdrop[0],\n                                nlayers=nlayers[0],\n                                cell_type=self.cell_type,\n                                )\n        self.encoder = Encoder(\n            embedding_layer=encoder_embedding_layer,\n            encoder_layer=encoder_rnn\n        )\n\n        if share_embedding_layer:\n            decoder_embedding_layer = encoder_embedding_layer\n        else:\n            decoder_embedding_layer = DropoutEmbeddings(ntokens=ntoken[-1],\n                                                        emb_size=emb_sz[-1],\n                                                        dropoute=dropoute[1],\n                                                        dropouti=dropouti[1]\n                                                        )\n\n        decoder_rnn = RNNLayers(input_size=kwargs.get(""input_size"", emb_sz[-1]),\n                                output_size=kwargs.get(""output_size"", emb_sz[-1]),\n                                nhid=nhid[-1], bidir=False, dropouth=dropouth[1],\n                                wdrop=wdrop[1], nlayers=nlayers[-1], cell_type=self.cell_type)\n\n        projection_layer = Projection(output_size=ntoken[-1], input_size=emb_sz[-1], dropout=dropoutd,\n                                      tie_encoder=decoder_embedding_layer if tie_decoder else None\n                                      )\n        self.decoder = Decoder(\n            decoder_layer=decoder_rnn,\n            projection_layer=projection_layer,\n            embedding_layer=decoder_embedding_layer,\n            pad_token=pad_token,\n            eos_token=eos_token,\n            max_tokens=max_tokens,\n        )\n\n    def forward(self, *inputs, num_beams=0):\n        with torch.set_grad_enabled(self.training):\n            encoder_inputs, decoder_inputs = assert_dims(inputs, [2, None, None])  # dims: [sl, bs] for encoder and decoder\n            # reset the states for the new batch\n            bs = encoder_inputs.size(1)\n            self.encoder.reset(bs)\n            self.decoder.reset(bs)\n            outputs = self.encoder(encoder_inputs)\n            state = concat_bidir_state(self.encoder.encoder_layer.hidden, cell_type=self.cell_type, nlayers=self.nlayers,\n                                       bidir=self.bidir\n                                       )\n            if self.training:\n                self.decoder.pr_force = self.pr_force\n                nb = 1 if self.pr_force < 1 else 0\n            else:\n                nb = num_beams\n            outputs_dec = self.decoder(decoder_inputs, hidden=state, num_beams=nb)\n            predictions = outputs_dec[:decoder_inputs.size(0)] if num_beams == 0 else self.decoder.beam_outputs\n        return predictions, [*outputs, *outputs_dec]\n'"
src/quicknlp/models/seq2seq_attention.py,2,"b'import torch\n\nimport torch.nn as nn\n\nfrom quicknlp.modules import AttentionDecoder, AttentionProjection, Encoder, RNNLayers\nfrom quicknlp.modules.embeddings import DropoutEmbeddings\nfrom quicknlp.utils import HParam, assert_dims, get_kwarg\nfrom .seq2seq import get_list\n\n\nclass Seq2SeqAttention(nn.Module):\n\n    def __init__(self, ntoken: HParam, emb_sz: HParam, nhid: HParam, nlayers: HParam, att_nhid: int, pad_token: int,\n                 eos_token: int, max_tokens: int = 50, share_embedding_layer: bool = False, tie_decoder: bool = True,\n                 bidir: bool = False, **kwargs):\n        """"""\n\n        Args:\n            ntoken (Union[List[int],int]): Number of tokens for the encoder and the decoder\n            emb_sz (Union[List[int],int]): Embedding size for the encoder and decoder embeddings\n            nhid (Union[List[int],int]): Number of hidden dims for the encoder and the decoder\n            nlayers (Union[List[int],int]): Number of layers for the encoder and the decoder\n            att_nhid (int): Number of hidden dims for the attention Module\n            pad_token (int): The  index of the token used for padding\n            eos_token (int): The index of the token used for eos\n            max_tokens (int): The maximum number of steps the decoder iterates before stopping\n            share_embedding_layer (bool): if True the decoder shares its input and output embeddings\n            tie_decoder (bool): if True the encoder and the decoder share their embeddings\n            bidir (bool): if True use a bidirectional encoder\n            **kwargs: Extra embeddings that will be passed to the encoder and the decoder\n        """"""\n        super().__init__()\n        # allow for the same or different parameters between encoder and decoder\n        ntoken, emb_sz, nhid, nlayers = get_list(ntoken, 2), get_list(emb_sz, 2), \\\n                                        get_list(nhid, 2), get_list(nlayers, 2)\n        dropoutd = get_kwarg(kwargs, name=""dropoutd"", default_value=0.5)  # output dropout\n        dropoute = get_kwarg(kwargs, name=""dropout_e"", default_value=0.1)  # encoder embedding dropout\n        dropoute = get_list(dropoute, 2)\n        dropouti = get_kwarg(kwargs, name=""dropout_i"", default_value=0.65)  # input dropout\n        dropouti = get_list(dropouti, 2)\n        dropouth = get_kwarg(kwargs, name=""dropout_h"", default_value=0.3)  # RNN output layers dropout\n        dropouth = get_list(dropouth, 2)\n        wdrop = get_kwarg(kwargs, name=""wdrop"", default_value=0.5)  # RNN weights dropout\n        wdrop = get_list(wdrop, 2)\n        cell_type = get_kwarg(kwargs, name=""cell_type"", default_value=""lstm"")\n\n        self.nlayers = nlayers\n        self.nhid = nhid\n        self.emb_sz = emb_sz\n        self.pr_force = 1.0\n\n        encoder_embedding_layer = DropoutEmbeddings(ntokens=ntoken[0],\n                                                    emb_size=emb_sz[0],\n                                                    dropoute=dropoute[0],\n                                                    dropouti=dropouti[0]\n                                                    )\n\n        encoder_rnn = RNNLayers(input_size=emb_sz[0],\n                                output_size=kwargs.get(""output_size"", emb_sz[0]),\n                                nhid=nhid[0], bidir=bidir,\n                                dropouth=dropouth[0],\n                                wdrop=wdrop[0],\n                                nlayers=nlayers[0],\n                                cell_type=cell_type,\n                                )\n        self.encoder = Encoder(\n            embedding_layer=encoder_embedding_layer,\n            encoder_layer=encoder_rnn\n        )\n\n        if share_embedding_layer:\n            decoder_embedding_layer = encoder_embedding_layer\n        else:\n            decoder_embedding_layer = DropoutEmbeddings(ntokens=ntoken[-1],\n                                                        emb_size=emb_sz[-1],\n                                                        dropoute=dropoute[1],\n                                                        dropouti=dropouti[1]\n                                                        )\n\n        decoder_rnn = RNNLayers(input_size=kwargs.get(""input_size"", emb_sz[-1] * 2),\n                                output_size=kwargs.get(""output_size"", emb_sz[-1]),\n                                nhid=nhid[-1], bidir=False, dropouth=dropouth[1],\n                                wdrop=wdrop[1], nlayers=nlayers[-1], cell_type=cell_type)\n\n        projection_layer = AttentionProjection(output_size=ntoken[-1],\n                                               input_size=emb_sz[-1],\n                                               dropout=dropoutd,\n                                               att_nhid=att_nhid,\n                                               tie_encoder=decoder_embedding_layer if tie_decoder else None\n                                               )\n        self.decoder = AttentionDecoder(\n            decoder_layer=decoder_rnn,\n            projection_layer=projection_layer,\n            embedding_layer=decoder_embedding_layer,\n            pad_token=pad_token,\n            eos_token=eos_token,\n            max_tokens=max_tokens,\n        )\n\n    def forward(self, *inputs, num_beams=0):\n        with torch.set_grad_enabled(self.training):\n            encoder_inputs, decoder_inputs = inputs\n            # reset the states for the new batch\n            bs = encoder_inputs.size(1)\n            self.encoder.reset(bs)\n            self.decoder.reset(bs)\n            outputs = self.encoder(encoder_inputs)\n            # as initial state we use the initial decoder state (zeros)\n            state = self.decoder.hidden\n            assert_dims(outputs, [self.nlayers[0], None, bs, (self.nhid[0], self.emb_sz[0])])\n            # pass the encoder outputs as keys to the attention projection_layer\n            self.decoder.projection_layer.reset(keys=outputs[-1])\n            if self.training:\n                self.decoder.pr_force = self.pr_force\n                nb = 1 if self.pr_force < 1 else 0\n            else:\n                nb = num_beams\n            outputs_dec = self.decoder(decoder_inputs, hidden=state, num_beams=nb)\n            predictions = outputs_dec[:decoder_inputs.size(0)] if num_beams == 0 else self.decoder.beam_outputs\n        return predictions, [*outputs, *outputs_dec]\n'"
src/quicknlp/models/transformer.py,2,"b'import torch\n\nimport torch.nn as nn\n\nfrom quicknlp.modules import Encoder, Projection, TransformerDecoder, TransformerDecoderLayers, TransformerEmbeddings, \\\n    TransformerEncoderLayers\nfrom quicknlp.utils import assert_dims, get_kwarg, get_list\n\n\nclass Transformer(nn.Module):\n    """"""Transformer model based on https://arxiv.org/abs/1706.03762\n        code implementation heavily inspired by http://nlp.seas.harvard.edu/2018/04/03/attention.html\n\n    """"""\n\n    def __init__(self, ntoken, emb_size=512, nlayers=6, pad_token=None, eos_token=None, max_tokens=200,\n                 share_embedding_layer=False, tie_decoder=True, **kwargs):\n        super().__init__()\n\n        ntoken = get_list(ntoken, 2)\n        self.nlayers = nlayers\n        dropout = get_kwarg(kwargs, name=""dropout"", default_value=0.1)\n        num_heads = get_kwarg(kwargs, name=""num_heads"", default_value=8)\n        nhid = get_kwarg(kwargs, name=""nhid"", default_value=2048)\n\n        encoder_embedding_layer = TransformerEmbeddings(ntokens=ntoken[0], emb_size=emb_size, dropout=dropout,\n                                                        pad_token=pad_token)\n        encoder_layer = TransformerEncoderLayers(num_layers=nlayers, input_size=emb_size, num_heads=num_heads,\n                                                 nhid=nhid)\n        self.encoder = Encoder(embedding_layer=encoder_embedding_layer, encoder_layer=encoder_layer)\n\n        if share_embedding_layer:\n            decoder_embedding_layer = encoder_embedding_layer\n        else:\n            decoder_embedding_layer = TransformerEmbeddings(ntokens=ntoken[-1], emb_size=emb_size, dropout=dropout,\n                                                            pad_token=pad_token)\n\n        decoder_layer = TransformerDecoderLayers(nlayers=nlayers, input_size=emb_size, num_heads=num_heads, nhid=nhid)\n        projection_layer = Projection(output_size=ntoken[-1], input_size=emb_size, dropout=dropout,\n                                      tie_encoder=decoder_embedding_layer if tie_decoder else None\n                                      )\n        self.decoder = TransformerDecoder(\n            decoder_layer=decoder_layer,\n            projection_layer=projection_layer,\n            embedding_layer=decoder_embedding_layer,\n            pad_token=pad_token,\n            eos_token=eos_token,\n            max_tokens=max_tokens,\n        )\n        self.nt = ntoken[-1]\n        # xavier uniform initialization\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p)\n\n    def forward(self, *inputs, num_beams=0):\n        with torch.set_grad_enabled(self.training):\n            encoder_inputs, decoder_inputs = assert_dims(inputs, [2, None, None])  # dims: [sl, bs] for encoder and decoder\n            encoder_outputs = self.encoder(encoder_inputs)\n            decoder_outputs = self.decoder(decoder_inputs, encoder_outputs, num_beams=num_beams)\n            predictions = decoder_outputs[:decoder_inputs.size(0)] if num_beams == 0 else self.decoder.beam_outputs\n        return predictions, decoder_outputs\n'"
src/quicknlp/modules/__init__.py,0,"b'from .attention_decoder import AttentionDecoder\nfrom .basic_decoder import Decoder, TransformerDecoder\nfrom .basic_encoder import Encoder\nfrom .embeddings import DropoutEmbeddings, TransformerEmbeddings\nfrom .projection import AttentionProjection, Projection\nfrom .rnn_encoder import RNNLayers\nfrom .transformer import TransformerDecoderLayers, TransformerEncoderLayers\n'"
src/quicknlp/modules/attention.py,2,"b'import numpy as np\nimport torch as tr\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fastai.rnn_reg import LockedDropout\n\nfrom quicknlp.utils import assert_dims\n\n\nclass MLPAttention(nn.Module):\n    """"""Multilayer Perceptron Attention Bandandau et al. 2015""""""\n\n    def __init__(self, n_in, nhid, p=0.0):\n        """"""\n\n        Args:\n            n_in (int):  The input dims of the first linear layer. It should equal\n                    the sum of the keys and query dims\n            nhid (int): The dimension of the internal prediction.\n        """"""\n        super().__init__()\n        self.dropout = LockedDropout(p) if p > 0.0 else None\n        self.linear1 = nn.Linear(in_features=n_in, out_features=nhid, bias=False)\n        self.linear2 = nn.Linear(in_features=nhid, out_features=1, bias=False)\n\n    def forward(self, query, keys, values):\n        # Query dim [bs, dimQ]\n        # keys dim [sl, bs, dimK]\n        # values dim [sl, bs, dimV]\n        inputs = tr.cat([query.unsqueeze(0).repeat(keys.size(0), 1, 1), keys], dim=-1)\n        scores = self.linear2(F.tanh((self.linear1(inputs))))  # [sl,bs, 1]\n        scores = F.softmax(scores, dim=0)  # [sl,bs, 1]\n        if self.dropout is not None:\n            scores = self.dropout(scores)\n        return (scores * values).sum(dim=0)  # [bs, dimV]\n\n\nclass SDPAttention(nn.Module):\n    """"""Scaled Dot Product Attention Vaswani et al. 2017""""""\n\n    def __init__(self, n_in, p=0.0):\n        super().__init__()\n        self.dropout = LockedDropout(p) if p > 0.0 else None\n        self.scale = np.sqrt(n_in)\n\n    def forward(self, query, keys, values):\n        # Query dim [bs, dimQ]\n        # keys dim [sl, bs, dimK]\n        # values dim [sl, bs, dimV]\n        dot = (query * keys).sum(dim=-1) / self.scale\n        # dot = (query @ keys) / self.scale\n        weights = F.softmax(dot, dim=0).unsqueeze(-1)\n        if self.dropout is not None:\n            weights = self.dropout(weights)\n        return (weights * values).sum(0)\n\n\nclass MultiHeadAttention(nn.Module):\n\n    def __init__(self, num_heads, nhid, keys_dim, query_dim, values_dim, dropout=0.0, out_dim=None):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout) if dropout > 0.0 else None\n        self.num_heads = num_heads\n        self.nhid = nhid\n        self.linear_out_dim = self.nhid * num_heads\n        self.out_dim = self.linear_out_dim if out_dim is None else out_dim\n        self.keys_linear = nn.Linear(in_features=keys_dim, out_features=self.linear_out_dim, bias=False)\n        self.query_linear = nn.Linear(in_features=query_dim, out_features=self.linear_out_dim, bias=False)\n        self.values_linear = nn.Linear(in_features=values_dim, out_features=self.linear_out_dim, bias=False)\n        self.scale = np.sqrt(self.nhid)\n        self.linear = nn.Linear(in_features=self.linear_out_dim, out_features=self.out_dim, bias=False)\n\n    def forward(self, query, keys, values, mask=None):\n        # Query dim [sl, bs, dimQ]\n        # keys dim [slQ, bs, dimK]\n        # values dim [sl, bs, dimV]\n        sl, bs, dimK = keys.size()\n        slq = query.size(0)\n        # [slQ, bs, dimH *NH] - > [bs, NH, slQ, dimH]\n        query_projection = self.query_linear(query).view(slq, bs, self.num_heads, self.nhid).permute(1, 2, 0, 3)\n        # [sl, bs, dimH *NH] -> [bs, NH, dimH, sl]\n        keys_projection = self.keys_linear(keys).view(sl, bs, self.num_heads, self.nhid).permute(1, 2, 3, 0)\n        # [sl, bs, dimH *NH] -> [bs, NH, sl, dimH]\n        values_projection = self.values_linear(values).view(sl, bs, self.num_heads, self.nhid).permute(1, 2, 0, 3)\n\n        # [bs, NH, slQ, dimH] x [bs, NH, dimH, sl] =  [bs, NH, slQ, sl]\n        scores = query_projection @ keys_projection\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e20)\n        weights = F.softmax(scores, dim=-1)\n        if self.dropout is not None:\n            weights = self.dropout(weights)\n\n        #  [bs, NH, slQ, sl] x  [bs, NH, sl, dimH] =  [bs, NH, slQ, dimH] -> [slQ, bs, NH * dimH]\n        attention = (weights @ values_projection).permute(2, 0, 1, 3).contiguous().view(slq, bs, self.num_heads * self.nhid)\n        output = self.linear(attention)\n        return assert_dims(output, [slq, bs, self.out_dim])\n'"
src/quicknlp/modules/attention_decoder.py,2,"b'import warnings\n\nimport torch\n\nfrom quicknlp.utils import assert_dims\nfrom .basic_decoder import Decoder\n\n\nclass AttentionDecoder(Decoder):\n\n    def _train_forward(self, inputs, hidden=None, constraints=None):\n        sl, bs = inputs.size()\n        emb = self.embedding_layer(inputs)\n        final_outputs = []\n        for step in emb:\n            step = torch.cat([step, self.projection_layer.get_attention_output(step)], dim=-1).unsqueeze_(0)\n            step = assert_dims(step, [1, bs, self.emb_size * 2])\n            outputs = self._rnn_step(step, hidden=hidden)\n            rnn_out = assert_dims(outputs[-1], [1, bs, self.emb_size])\n            final_outputs.append(self.projection_layer(rnn_out[0]))\n        outputs = torch.cat(final_outputs, dim=0)\n        return outputs\n\n    def _beam_forward(self, inputs, hidden, num_beams, constraints=None):\n        # ensure keys exist for all beams\n        if self.projection_layer.keys is not None and num_beams > 0:\n            self.projection_layer.keys = self.projection_layer.keys.repeat(1, num_beams, 1)\n        return super()._beam_forward(inputs, hidden=hidden, num_beams=num_beams)\n\n    def _rnn_step(self, output, hidden):\n        new_hidden, outputs = [], []\n        for layer_index, (rnn, drop) in enumerate(zip(self.decoder_layer.layers, self.decoder_layer.dropouths)):\n            with warnings.catch_warnings():\n                warnings.simplefilter(""ignore"")\n                output, new_h = rnn(output, hidden[layer_index])\n            new_hidden.append(new_h)\n            if layer_index != self.nlayers - 1:  # add dropout between every rnn layer but not after last rnn layer\n                output = drop(output)\n            outputs.append(output)\n        self.decoder_layer.hidden = new_hidden\n        return outputs\n'"
src/quicknlp/modules/basic_decoder.py,32,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fastai.core import V, to_gpu\n\nfrom quicknlp.utils import assert_dims, RandomUniform\n\n\ndef repeat_cell_state(hidden, num_beams):\n    results = []\n    for row in hidden:\n        if isinstance(row, (list, tuple)):\n            state = (row[0].repeat(1, num_beams, 1), row[1].repeat(1, num_beams, 1))\n        else:\n            state = row.repeat(1, num_beams, 1)\n        results.append(state)\n    return results\n\n\ndef reshape_parent_indices(indices, bs, num_beams):\n    parent_indices = V((torch.arange(end=bs) * num_beams).unsqueeze_(1).repeat(1, num_beams).view(-1).long())\n    return indices + parent_indices\n\n\ndef select_hidden_by_index(hidden, indices):\n    if hidden is None:\n        return hidden\n    results = []\n    for row in hidden:\n        if isinstance(row, (list, tuple)):\n            state = (torch.index_select(row[0], 1, indices), torch.index_select(row[1], 1, indices))\n        else:\n            state = torch.index_select(row, 1, indices)\n        results.append(state)\n    return results\n\n\nclass Decoder(nn.Module):\n    MAX_STEPS_ALLOWED = 320\n\n    def __init__(self, decoder_layer, projection_layer, max_tokens, eos_token, pad_token,\n                 embedding_layer: torch.nn.Module):\n        super().__init__()\n        self.decoder_layer = decoder_layer\n        self.nlayers = decoder_layer.nlayers\n        self.projection_layer = projection_layer\n        self.bs = 1\n        self.max_iterations = max_tokens\n        self.eos_token = eos_token\n        self.pad_token = pad_token\n        self.beam_outputs = None\n        self.embedding_layer = embedding_layer\n        self.emb_size = embedding_layer.emb_size\n        self.pr_force = 0.0\n        self.random = RandomUniform()\n\n    def reset(self, bs):\n        self.decoder_layer.reset(bs)\n\n    def forward(self, inputs, hidden=None, num_beams=0, constraints=None):\n        self.bs = inputs.size(1)\n        if num_beams == 0:  # zero beams, a.k.a. teacher forcing\n            return self._train_forward(inputs, hidden, constraints)\n        elif num_beams == 1:  # one beam  a.k.a. greedy search\n            return self._greedy_forward(inputs, hidden, constraints)\n        elif num_beams > 1:  # multiple beams a.k.a topk search\n            return self._beam_forward(inputs, hidden, num_beams, constraints)\n\n    def _beam_forward(self, inputs, hidden, num_beams, constraints=None):\n        return self._topk_forward(inputs, hidden, num_beams, constraints)\n\n    def _train_forward(self, inputs, hidden=None, constraints=None):\n        inputs = self.embedding_layer(inputs)\n        if constraints is not None:\n            # constraint should have dim [1, bs, hd]\n            # and inputs should be [sl,bs,hd]\n            inputs = torch.cat([inputs, constraints.repeat(inputs.size(0), 1, 1)], dim=-1)\n        # outputs are the outputs of every layer\n        outputs = self.decoder_layer(inputs, hidden)\n        # we project only the output of the last layer\n        outputs = self.projection_layer(outputs[-1]) if self.projection_layer is not None else outputs[-1]\n        return outputs\n\n    def _greedy_forward(self, inputs, hidden=None, constraints=None):\n        dec_inputs = inputs\n        max_iterations = min(dec_inputs.size(0), self.MAX_STEPS_ALLOWED) if self.training else self.max_iterations\n        inputs = V(inputs[:1].data)  # inputs should be only first token initially [1,bs]\n        sl, bs = inputs.size()\n        finished = to_gpu(torch.zeros(bs).byte())\n        iteration = 0\n        self.beam_outputs = inputs.clone()\n        final_outputs = []\n        while not finished.all() and iteration < max_iterations:\n            # output should be List[[sl, bs, layer_dim], ...] sl should be one\n            if 0 < iteration and self.training and 0. < self.random() < self.pr_force:\n                inputs = dec_inputs[iteration].unsqueeze(0)\n            output = self.forward(inputs, hidden=hidden, num_beams=0, constraints=constraints)\n            hidden = self.decoder_layer.hidden\n            final_outputs.append(output)  # dim should be [sl=1, bs, nt]\n            #  inputs are the indices  dims [1,bs] # repackage the var to avoid grad backwards\n            inputs = assert_dims(V(output.data.max(dim=-1)[1]), [1, bs])\n            iteration += 1\n            self.beam_outputs = assert_dims(torch.cat([self.beam_outputs, inputs], dim=0), [iteration + 1, bs])\n            new_finished = inputs.data == self.eos_token\n            finished = finished | new_finished\n            # stop if the output is to big to fit in memory\n\n        self.beam_outputs = self.beam_outputs.view(-1, bs, 1)\n        # outputs should be [sl, bs, nt]\n        outputs = torch.cat(final_outputs, dim=0)\n        return outputs\n\n    def _topk_forward(self, inputs, hidden, num_beams, constraints=None):\n        sl, bs = inputs.size()\n        # initial logprobs should be zero (pr of <sos> token in the start is 1)\n        logprobs = torch.zeros_like(inputs[:1]).view(1, bs, 1).float()  # shape will be [sl, bs, 1]\n        inputs = inputs[:1].repeat(1, num_beams)  # inputs should be only first token initially [1,bs x num_beams]\n        finished = to_gpu(torch.zeros(bs * num_beams).byte())\n        iteration = 0\n        final_outputs = []\n        self.beam_outputs = inputs.clone()\n        hidden = repeat_cell_state(hidden, num_beams)\n        while not finished.all() and iteration < self.max_iterations:\n            # output should be List[[sl, bs * num_beams, layer_dim], ...] sl should be one\n            output = self.forward(inputs, hidden=hidden, num_beams=0, constraints=constraints)\n            hidden = self.decoder_layer.hidden\n            final_outputs.append(output)\n\n            # we take the output of the last layer with dims [1, bs, output_dim]\n            # and get the indices of th top k for every bs\n            new_logprobs = F.log_softmax(output, dim=-1)  # [1, bs x num_beams, nt]\n            num_tokens = new_logprobs.size(2)\n            new_logprobs = new_logprobs.view(1, bs, num_beams, num_tokens) + logprobs.unsqueeze(-1)  # [1, bs, nb, nt]\n            # mask logprogs accordingly\n            new_logprobs = self.mask_logprobs(bs, finished, iteration, logprobs, new_logprobs, num_beams, num_tokens)\n\n            # TODO implement stochastic beam search\n            # get the top logprobs and their indices\n            logprobs, beams = torch.topk(new_logprobs, k=num_beams, dim=-1)  # [1, bs, num_beams]\n            parents = beams / num_tokens\n            inputs = beams % num_tokens\n            parent_indices = reshape_parent_indices(parents.view(-1), bs=bs, num_beams=num_beams)\n            self.decoder_layer.hidden = select_hidden_by_index(self.decoder_layer.hidden, indices=parent_indices)\n            finished = torch.index_select(finished, 0, parent_indices.data)\n            inputs = inputs.view(1, -1).contiguous()\n\n            self.beam_outputs = torch.index_select(self.beam_outputs, dim=1, index=parent_indices)\n            self.beam_outputs = torch.cat([self.beam_outputs, inputs], dim=0)\n            new_finished = (inputs.data == self.eos_token).view(-1)\n            finished = finished | new_finished\n            iteration += 1\n\n        self.beam_outputs = self.beam_outputs.view(-1, bs, num_beams)\n        # ensure the outputs is the output of the last layer [sl,bs, nt]\n        outputs = torch.cat(final_outputs, dim=0)\n        return outputs\n\n    def mask_logprobs(self, bs, finished, iteration, logprobs, new_logprobs, num_beams, num_tokens):\n        if iteration == 0:\n            # only the first beam is considered in the first step, otherwise we would get the same result for every beam\n            new_logprobs = new_logprobs[..., 0, :]\n        else:\n            # we have to cater for finished beams as well\n            # create a mask [1, bs x nb, nt] with - inf everywhere\n            mask = torch.zeros_like(new_logprobs).fill_(-1e32).view(1, bs * num_beams, num_tokens)\n            f = V(finished.unsqueeze(0))\n            # set the pad_token position to the last logprob for the finished ones\n            mask[..., self.pad_token] = logprobs.view(1, bs * num_beams)\n            # mask shape = [1, bs * nb (that are finished), nt]\n            mask = mask.masked_select(f.unsqueeze(-1)).view(1, -1, num_tokens)\n            # replace the rows of the finished ones with the mask\n            new_logprobs.masked_scatter_(f.view(1, bs, num_beams, 1), mask)\n            # flatten all beams with the tokens\n            new_logprobs = new_logprobs.view(1, bs, -1)\n        return new_logprobs\n\n    @property\n    def hidden(self):\n        return self.decoder_layer.hidden\n\n    @hidden.setter\n    def hidden(self, value):\n        self.decoder_layer.hidden = value\n\n    @property\n    def layers(self):\n        return self.decoder_layer.layers\n\n    @property\n    def output_size(self):\n        return self.projection_layer.output_size if self.projection_layer is not None else self.decoder_layer.output_size\n\n\nclass TransformerDecoder(Decoder):\n\n    def __init__(self, decoder_layer, projection_layer, max_tokens, eos_token, pad_token,\n                 embedding_layer: torch.nn.Module):\n        super().__init__(decoder_layer=decoder_layer, projection_layer=projection_layer, max_tokens=max_tokens,\n                         eos_token=eos_token, pad_token=pad_token, embedding_layer=embedding_layer)\n\n    def _train_forward(self, inputs, hidden=None, constraints=None):\n        inputs = self.embedding_layer(inputs)\n        # outputs are the outputs of every layer\n        outputs = self.decoder_layer(inputs, hidden)\n        # we project only the output of the last layer\n        outputs = self.projection_layer(outputs[-1]) if self.projection_layer is not None else outputs[-1]\n        return outputs\n\n    def _greedy_forward(self, inputs, hidden=None, constraints=None):\n        inputs = inputs[:1]  # inputs should be only first token initially [1,bs]\n        sl, bs = inputs.size()\n        finished = to_gpu(torch.zeros(bs).byte())\n        iteration = 0\n        self.beam_outputs = inputs.clone().cpu()\n        final_outputs = []\n        while not finished.all() and iteration < self.max_iterations:\n            # output should be List[[sl, bs, layer_dim], ...] sl should be one\n            # step_inputs should be [1, bs]\n            output = self.forward(inputs, hidden=hidden, num_beams=0)\n            final_outputs.append(output[-1:])\n            iteration += 1\n            step_inputs = assert_dims(V(output[-1:].data.max(dim=-1)[1]), [1, bs])\n            self.beam_outputs = assert_dims(torch.cat([self.beam_outputs, step_inputs.cpu()], dim=0),\n                                            [iteration + 1, bs])\n            new_finished = step_inputs.data == self.eos_token\n            inputs = torch.cat([inputs, step_inputs], dim=0)\n            assert_dims(inputs, [iteration + 1, bs])\n            finished = finished | new_finished\n\n        self.beam_outputs = self.beam_outputs.view(-1, bs, 1)\n        outputs = torch.cat(final_outputs, dim=0)\n        return outputs\n\n    def _topk_forward(self, inputs, hidden, num_beams, constraints=None):\n        sl, bs = inputs.size()\n        # initial logprobs should be zero (pr of <sos> token in the start is 1)\n        logprobs = torch.zeros_like(inputs[:1]).view(1, bs, 1).float()  # shape will be [sl, bs, 1]\n        inputs = inputs[:1].repeat(1,\n                                   num_beams)  # inputs should be only first token initially [1,bs x num_beams]\n        finished = to_gpu(torch.zeros(bs * num_beams).byte())\n        iteration = 0\n        self.beam_outputs = inputs.clone().cpu()\n        hidden = repeat_cell_state(hidden, num_beams)\n        final_outputs = []\n        while not finished.all() and iteration < self.max_iterations:\n            # output should be List[[sl, bs * num_beams, layer_dim], ...] sl should be one\n            output = self.forward(inputs, hidden=hidden, num_beams=0)\n            step_prediction = output[-1:]  # [sl, bs* num_beams , ntokens]\n            final_outputs.append(step_prediction.cpu())\n            # we take the output of the last layer with dims [1, bs, output_dim]\n            # and get the indices of th top k for every bs\n            new_logprobs = F.log_softmax(step_prediction, dim=-1)  # [1, bs x num_beams, nt]\n            num_tokens = new_logprobs.size(2)\n            new_logprobs = new_logprobs.view(1, bs, num_beams, num_tokens) + logprobs.unsqueeze(-1)  # [1, bs, nb, nt]\n            # mask logprobs if they are finished or it's the first iteration\n            new_logprobs = self.mask_logprobs(bs, finished, iteration, logprobs, new_logprobs, num_beams, num_tokens)\n\n            # TODO take into account sequence_length for getting the top logprobs and their indices\n            logprobs, beams = torch.topk(new_logprobs, k=num_beams, dim=-1)  # [1, bs, num_beams]\n            parents = beams / num_tokens\n            step_inputs = beams % num_tokens\n            parent_indices = reshape_parent_indices(parents.view(-1), bs=bs, num_beams=num_beams)\n            finished = torch.index_select(finished, 0, parent_indices.data)\n            step_inputs = step_inputs.view(1, -1).contiguous()\n\n            new_finished = (step_inputs.data == self.eos_token).view(-1)\n            inputs = torch.index_select(inputs, dim=1, index=parent_indices)\n            inputs = torch.cat([inputs, step_inputs], dim=0)\n            finished = finished | new_finished\n            iteration += 1\n            self.beam_outputs = torch.index_select(self.beam_outputs, dim=1, index=parent_indices.cpu())\n            self.beam_outputs = torch.cat([self.beam_outputs, step_inputs.cpu()], dim=0)\n\n        # ensure the outputs is the output of the last layer [sl,bs, nt]\n        outputs = torch.cat(final_outputs, dim=0)\n        self.beam_outputs = self.beam_outputs.view(-1, bs, num_beams)\n        return outputs\n"""
src/quicknlp/modules/basic_encoder.py,0,"b'from torch import nn as nn\n\n\nclass Encoder(nn.Module):\n\n    def __init__(self, embedding_layer, encoder_layer):\n        super().__init__()\n        self.embedding_layer = embedding_layer\n        self.encoder_layer = encoder_layer\n\n    def forward(self, input_tensor, state=None):\n        ed = self.embedding_layer(input_tensor)  # dims [sl,bs,ed]\n        return self.encoder_layer(ed, state)\n\n    def reset(self, bs):\n        self.encoder_layer.reset(bs)\n\n    @property\n    def hidden(self):\n        return self.encoder_layer.hidden\n\n    @hidden.setter\n    def hidden(self, value):\n        self.encoder_layer.hidden = value\n\n    @property\n    def layers(self):\n        return self.encoder_layer.layers\n\n    @property\n    def output_size(self):\n        return self.encoder_layer.output_size\n'"
src/quicknlp/modules/cell.py,10,"b'import math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fastai.core import to_gpu\nfrom fastai.rnn_reg import WeightDrop\nfrom torch.nn import Parameter\n\n\nclass Cell(nn.Module):\n    """"""GRU or LSTM cell with withdrop. Can also be bidirectional and have trainable initial state""""""\n\n    def __init__(self, cell_type, input_size, output_size, dropout=0.0, wdrop=0.0, dropoutinit=0.0, bidir=False,\n                 train_init=False):\n        super().__init__()\n        self.cell_type = cell_type.lower()\n        self.bidir = bidir\n        self.input_size = input_size\n        self.output_size = output_size\n        self.dropoutinit = dropoutinit\n        if self.cell_type == ""lstm"":\n            self.cell = nn.LSTM(input_size, output_size, num_layers=1, bidirectional=bidir, dropout=dropout)\n        elif self.cell_type == ""gru"":\n            self.cell = nn.GRU(input_size, output_size, num_layers=1, bidirectional=bidir, dropout=dropout)\n        else:\n            raise NotImplementedError(f""cell: {cell_type} not supported"")\n        if wdrop:\n            self.cell = WeightDrop(self.cell, wdrop)\n        self.train_init = train_init\n        self.init_state = None\n        self.init_cell_state = None\n        if self.train_init:\n            ndir = 2 if bidir else 1\n            self.init_state = Parameter(torch.Tensor(ndir, 1, self.output_size))\n            stdv = 1. / math.sqrt(self.init_state.size(1))\n            self.init_state.data.uniform_(-stdv, stdv)\n            if self.cell_type == ""lstm"":\n                ndir = 2 if bidir else 1\n                self.init_cell_state = Parameter(torch.Tensor(ndir, 1, self.output_size))\n                stdv = 1. / math.sqrt(self.init_state.size(1))\n                self.init_cell_state.data.uniform_(-stdv, stdv)\n        self.reset(bs=1)\n\n    def forward(self, inputs, hidden):\n        """"""\n        LSTM Inputs: input, (h_0, c_0)\n                    - **input** (seq_len, batch, input_size): tensor containing the features\n                      of the input sequence.\n                      The input can also be a packed variable length sequence.\n                      See :func:`torch.nn.utils.rnn.pack_padded_sequence` for details.\n                    - **h_0** (num_layers \\* num_directions, batch, hidden_size): tensor\n                      containing the initial hidden state for each element in the batch.\n                    - **c_0** (num_layers \\* num_directions, batch, hidden_size): tensor\n                      containing the initial cell state for each element in the batch.\n\n                      If (h_0, c_0) is not provided, both **h_0** and **c_0** default to zero.\n            Outputs: output, (h_n, c_n)\n                - **output** (seq_len, batch, hidden_size * num_directions): tensor\n                  containing the output features `(h_t)` from the last layer of the RNN,\n                  for each t. If a :class:`torch.nn.utils.rnn.PackedSequence` has been\n                  given as the input, the output will also be a packed sequence.\n                - **h_n** (num_layers * num_directions, batch, hidden_size): tensor\n                  containing the hidden state for t=seq_len\n                - **c_n** (num_layers * num_directions, batch, hidden_size): tensor\n                  containing the cell state for t=seq_len\n\n        GRU: Inputs: input, h_0\n                    - **input** (seq_len, batch, input_size): tensor containing the features\n                      of the input sequence. The input can also be a packed variable length\n                      sequence. See :func:`torch.nn.utils.rnn.pack_padded_sequence`\n                      for details.\n                    - **h_0** (num_layers * num_directions, batch, hidden_size): tensor\n                      containing the initial hidden state for each element in the batch.\n                      Defaults to zero if not provided.\n            Outputs: output, h_n\n                - **output** (seq_len, batch, hidden_size * num_directions): tensor\n                  containing the output features h_t from the last layer of the RNN,\n                  for each t. If a :class:`torch.nn.utils.rnn.PackedSequence` has been\n                  given as the input, the output will also be a packed sequence.\n                - **h_n** (num_layers * num_directions, batch, hidden_size): tensor\n                  containing the hidden state for t=seq_len\n\n\n        """"""\n        return self.cell(inputs, hidden)\n\n    def one_hidden(self, bs=1, cell_state=False):\n        ndir = 2 if self.bidir else 1\n        if not self.train_init:\n            init_state = to_gpu(torch.zeros(ndir, bs, self.output_size))\n        elif cell_state:\n            init_state = F.dropout(self.init_cell_state, p=self.dropoutinit, training=self.training)\n            init_state.repeat(1, bs, 1)\n        else:\n            init_state = F.dropout(self.init_state, p=self.dropoutinit, training=self.training)\n            return init_state.repeat(1, bs, 1)\n        return init_state\n\n    def hidden_state(self, bs):\n        if self.cell_type == ""gru"":\n            return self.one_hidden(bs)\n        else:\n            return self.one_hidden(bs, cell_state=False), self.one_hidden(bs, cell_state=True)\n\n    def reset(self, bs=1):\n        self.hidden = self.hidden_state(bs=bs)\n\n    def get_hidden_state(self):\n        return self.hidden[0] if self.cell_type == ""lstm"" else self.hidden\n'"
src/quicknlp/modules/embeddings.py,6,"b'import math\n\nimport torch\nimport torch.nn as nn\nfrom fastai.core import V\nfrom fastai.rnn_reg import EmbeddingDropout, LockedDropout\n\n\nclass NormEmbeddings(nn.Module):\n    ""Normalized embedding see http://nlp.seas.harvard.edu/2018/04/03/attention.html""\n\n    def __init__(self, emb_size, tokens, padding_idx=None):\n        super().__init__()\n        self.embedding = nn.Embedding(tokens, emb_size, padding_idx=padding_idx)\n        self.in_features = emb_size\n\n    def forward(self, x):\n        return self.embedding(x) * math.sqrt(self.in_features)\n\n    @property\n    def weight(self):\n        return self.embedding.weight\n\n\nclass PositionalEncoding(nn.Module):\n    ""Sinusoid Positional embedding see http://nlp.seas.harvard.edu/2018/04/03/attention.html""\n\n    def __init__(self, input_size, dropout, max_len=5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        # Compute the positional encodings once in log space.\n        pe = torch.zeros(max_len, input_size)\n        position = torch.arange(0, max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, input_size, 2) *\n                             -(math.log(10000.0) / input_size))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        # we register pe as part of the state but not as a parameter\n        self.register_buffer(\'pe\', pe)\n\n    def forward(self, x):\n        x = x + V(self.pe[:, :x.size(1)])\n        return self.dropout(x)\n\n\nclass DropoutEmbeddings(nn.Module):\n    initrange = 0.1\n\n    def __init__(self, ntokens, emb_size, dropoute=0.1, dropouti=0.65, pad_token=None):\n        """""" Default Constructor for the DropoutEmbeddings class\n\n        Args:\n            ntokens (int): number of vocabulary (or tokens) in the source dataset\n            emb_size (int): the embedding size to use to encode each token\n            pad_token (int): the int value used for padding text.\n            dropoute (float): dropout to apply to the embedding layer. zeros out tokens\n            dropouti (float): dropout to apply to the input layer. zeros out features\n        """"""\n        super().__init__()\n        self.encoder = nn.Embedding(ntokens, emb_size, padding_idx=pad_token)\n        self.encoder_with_dropout = EmbeddingDropout(self.encoder)\n        self.encoder.weight.data.uniform_(-self.initrange, self.initrange)\n        self.dropout_embedding = dropoute\n        self.dropout_input = LockedDropout(dropouti)\n        self.emb_size = emb_size\n\n    def forward(self, input_tensor):\n        emb = self.encoder_with_dropout(input_tensor, dropout=self.dropout_embedding if self.training else 0)\n        return self.dropout_input(emb)\n\n    @property\n    def weight(self):\n        return self.encoder.weight\n\n\nclass TransformerEmbeddings(nn.Module):\n\n    def __init__(self, ntokens, emb_size, dropout, pad_token=None, max_len=5000):\n        super(TransformerEmbeddings, self).__init__()\n        self.layers = nn.Sequential(NormEmbeddings(emb_size=emb_size, tokens=ntokens, padding_idx=pad_token),\n                                    PositionalEncoding(input_size=emb_size, dropout=dropout, max_len=max_len))\n        self.emb_size = emb_size\n\n    def forward(self, input_tensor):\n        return self.layers(input_tensor)\n\n    @property\n    def weight(self):\n        return self.layers[0].weight\n'"
src/quicknlp/modules/hred_encoder.py,2,"b'import torch\nimport torch.nn as nn\n\nfrom quicknlp.modules import DropoutEmbeddings, RNNLayers\nfrom quicknlp.utils import HParam, concat_bidir_state, get_kwarg, get_list\nfrom .basic_encoder import Encoder\n\n\nclass HREDEncoder(nn.Module):\n\n    def __init__(self, ntoken: int, emb_sz: int, nhid: HParam, nlayers: int,\n                 bidir: bool = False, cell_type=""gru"", **kwargs):\n        super().__init__()\n        # allow for the same or different parameters between encoder and decoder\n\n        nhid = get_list(nhid, 2)\n        dropoute = get_kwarg(kwargs, name=""dropout_e"", default_value=0.1)  # encoder embedding dropout\n        dropoute = get_list(dropoute, 2)\n        dropouti = get_kwarg(kwargs, name=""dropout_i"", default_value=0.65)  # input dropout\n        dropouti = get_list(dropouti, 2)\n        dropouth = get_kwarg(kwargs, name=""dropout_h"", default_value=0.3)  # RNN output layers dropout\n        dropouth = get_list(dropouth, 2)\n        wdrop = get_kwarg(kwargs, name=""wdrop"", default_value=0.5)  # RNN weights dropout\n        wdrop = get_list(wdrop, 2)\n        train_init = get_kwarg(kwargs, name=""train_init"", default_value=False)\n        dropoutinit = get_kwarg(kwargs, name=""dropout_init"", default_value=0.1)  # RNN initial states dropout\n        dropoutinit = get_list(dropoutinit, 2)\n\n        self.cell_type = cell_type\n        self.nt = ntoken\n        self.bidir = bidir\n\n        encoder_embedding_layer = DropoutEmbeddings(ntokens=ntoken,\n                                                    emb_size=emb_sz,\n                                                    dropoute=dropoute[0],\n                                                    dropouti=dropouti[0]\n                                                    )\n\n        encoder_rnn = RNNLayers(input_size=emb_sz,\n                                output_size=kwargs.get(""output_size_encoder"", emb_sz),\n                                nhid=nhid[0], bidir=bidir,\n                                dropouth=dropouth[0],\n                                wdrop=wdrop[0],\n                                nlayers=nlayers,\n                                cell_type=self.cell_type,\n                                train_init=train_init,\n                                dropoutinit=dropoutinit[0]\n                                )\n        self.query_encoder = Encoder(\n            embedding_layer=encoder_embedding_layer,\n            encoder_layer=encoder_rnn\n\n        )\n        self.se_enc = RNNLayers(\n            cell_type=self.cell_type,\n            input_size=encoder_rnn.output_size,\n            output_size=nhid[1],\n            nhid=nhid[1],\n            nlayers=1,\n            dropouth=dropouth[1],\n            wdrop=wdrop[1],\n            train_init=train_init,\n            dropoutinit=dropoutinit[1]\n        )\n\n    def forward(self, inputs):\n        query_encoder_outputs = self.query_level_encoding(inputs)\n        outputs = self.se_enc(query_encoder_outputs)\n        last_output = self.se_enc.hidden[-1]\n        return outputs, last_output\n\n    def reset(self, bs):\n        self.query_encoder.reset(bs)\n        self.se_enc.reset(bs)\n\n    def query_level_encoding(self, encoder_inputs):\n        query_encoder_outputs = []\n        for index, context in enumerate(encoder_inputs):\n            self.query_encoder.reset(bs=encoder_inputs.size(2))\n            state = self.query_encoder.hidden\n            outputs = self.query_encoder(context, state)  # context has size [sl, bs]\n            out = concat_bidir_state(self.query_encoder.encoder_layer.get_last_hidden_state(),\n                                     cell_type=self.cell_type, nlayers=1,\n                                     bidir=self.query_encoder.encoder_layer.bidir\n                                     )\n            query_encoder_outputs.append(out)  # get the last sl output of the query_encoder\n            # BPTT if the dialogue is too long repackage the first half of the outputs to decrease\n            # the gradient backpropagation and fit it into memory\n            # out = repackage_var(outputs[-1][\n            #                        -1]) if max_sl * num_utterances > self.BPTT_MAX_UTTERANCES and index <= num_utterances // 2 else \\\n            #    outputs[-1][-1]\n        query_encoder_outputs = torch.cat(query_encoder_outputs, dim=0)  # [cl, bs, nhid]\n        return query_encoder_outputs\n\n    @property\n    def embedding_layer(self):\n        return self.query_encoder.embedding_layer\n\n    @property\n    def output_size(self):\n        return self.se_enc.output_size\n\n    @property\n    def query_encoder_layer(self):\n        return self.query_encoder.encoder_layer\n\n    @property\n    def session_encoder_layer(self):\n        return self.se_enc\n'"
src/quicknlp/modules/projection.py,2,"b'from collections import OrderedDict\n\nimport torch\nfrom fastai.rnn_reg import LockedDropout\nfrom torch import nn as nn\n\nfrom quicknlp.utils import assert_dims\nfrom .attention import MLPAttention, SDPAttention\n\n\nclass Projection(nn.Module):\n    initrange = 0.1\n\n    def __init__(self, output_size: int, input_size: int, dropout: float, nhid: int = None, tie_encoder=None):\n        super().__init__()\n        layers = OrderedDict()\n        self.dropout = LockedDropout(dropout)\n        if nhid is not None:\n            linear1 = nn.Linear(input_size, nhid)\n            linear1.weight.data.uniform_(-self.initrange, self.initrange)\n            layers[""projection1""] = linear1\n            dropout1 = nn.Dropout(dropout)\n            layers[""dropout""] = dropout1\n        else:\n            nhid = input_size\n        linear2 = nn.Linear(nhid, output_size, bias=False)\n        if tie_encoder:\n            assert linear2.weight.shape == tie_encoder.weight.shape, ""tied encoder {} does not match projection {}"".format(\n                tie_encoder.weight.shape,\n                linear2.weight.shape\n            )\n            linear2.weight = tie_encoder.weight\n        layers[""projection2""] = linear2\n        self.layers = nn.Sequential(layers)\n        self.output_size = output_size\n\n    def forward(self, projection_input):\n        # input should be sl, bs, input_dim\n\n        output = self.dropout(projection_input)\n        decoded = output.view(output.size(0) * output.size(1), output.size(2))\n        decoded = self.layers(decoded)\n        return decoded.view(-1, projection_input.size(1), decoded.size(1))\n\n\nclass AttentionProjection(nn.Module):\n\n    def __init__(self, output_size, input_size, dropout, att_nhid, att_type=""MLP"", tie_encoder=None):\n        super().__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.keys = None\n        self._attention_output = None\n        self.attention = MLPAttention(n_in=input_size * 2, nhid=att_nhid) if att_type == ""MLP"" else SDPAttention(\n            n_in=input_size)\n        self.projection1 = Projection(output_size=input_size, input_size=input_size * 2, dropout=dropout)\n        self.projection2 = Projection(output_size=output_size, input_size=input_size, dropout=dropout,\n                                      tie_encoder=tie_encoder)\n\n    def forward(self, input):\n        assert_dims(input, [None, self.input_size])\n        self._attention_output = self.attention(query=input, keys=self.keys, values=self.keys)\n        output = torch.cat([input, self._attention_output], dim=-1).unsqueeze_(0)\n        assert_dims(output, [1, None, self.input_size * 2])\n        output = assert_dims(self.projection1(output), [1, None, self.input_size])\n        projection = self.projection2(output)\n        return assert_dims(projection, [1, None, self.output_size])\n\n    def get_attention_output(self, raw_output):\n        if self._attention_output is None:\n            return torch.zeros_like(raw_output)\n        else:\n            return self._attention_output\n\n    def reset(self, keys):\n        self._attention_output = None\n        self.keys = keys\n'"
src/quicknlp/modules/rnn_encoder.py,1,"b'import warnings\n\nimport torch.nn as nn\nfrom fastai.lm_rnn import LockedDropout\n\nfrom .cell import Cell\n\n\ndef get_layer_dims(layer_index, total_layers, input_size, output_size, nhid, bidir):\n    ndir = 2 if bidir else 1\n    input_size = input_size if layer_index == 0 else nhid\n    output_size = (nhid if layer_index != total_layers - 1 else output_size) // ndir\n    return input_size, output_size\n\n\nclass RNNLayers(nn.Module):\n    """"""\n    Wrote this class to allow for a multilayered RNN encoder. It is based the fastai RNN_Encoder class\n    """"""\n\n    def __init__(self, input_size, output_size, nhid, nlayers, dropouth=0.3, wdrop=0.5, bidir=False, cell_type=""lstm"",\n                 train_init=False, dropoutinit=0.1, **kwargs):\n        """""" Default Constructor for the RNNLayers class\n\n        Args:\n            input_size (int): the dimension of the input vectors\n            output_size (int) the dimension of the output vectors\n            nhid (int): number of hidden activation per layer\n            nlayers (int): number of layers to use in the architecture\n            dropouth (float): dropout to apply to the activations going from one  layer to another\n            wdrop (float): dropout used for a LSTM\'s internal (or hidden) recurrent weights.\n            bidir (bool): If true the cell will be bidirectional\n            train_init (bool): If true the initial states will be trainable\n            dropoutinit (float): The dropout to use in the initial states if trainable\n            cell_type (str): Type of cell (default is LSTM)\n        """"""\n        super().__init__()\n        layers = []\n        for layer_index in range(nlayers):\n            inp_size, out_size = get_layer_dims(layer_index=layer_index, total_layers=nlayers,\n                                                input_size=input_size,\n                                                output_size=output_size,\n                                                nhid=nhid, bidir=bidir)\n            layers.append(\n                Cell(cell_type=cell_type, input_size=inp_size, output_size=out_size,\n                     bidir=bidir, wdrop=wdrop, train_init=train_init, dropoutinit=dropoutinit)\n            )\n\n        self.layers = nn.ModuleList(layers)\n        self.input_size, self.output_size, self.nhid, self.nlayers = input_size, output_size, nhid, nlayers\n        self.cell_type, self.bidir = cell_type, bidir\n        self.dropouths = nn.ModuleList([LockedDropout(dropouth) for l in range(nlayers)])\n        self.hidden, self.weights = None, None\n        self.reset(1)\n\n    def forward(self, input_tensor, hidden=None):\n        """""" Invoked during the forward propagation of the RNN_Encoder module.\n        Args:\n            input_tensor (Tensor): input of shape [sentence_length, batch_size, hidden_dim]\n            hidden (List[Tensor]: state  of the encoder\n\n        Returns:\n            (Tuple[List[Tensor], List[Tensor]]):\n            raw_outputs: list of tensors evaluated from each RNN layer without using dropouth,\n            outputs: list of tensors evaluated from each RNN layer using dropouth,\n            The outputs should have dims [sl,bs,layer_dims]\n        """"""\n        # we reset at very batch as they are not sequential (like a languagemodel)\n        output = input_tensor  # sl, bs, ed\n        self.hidden = self.hidden if hidden is None else hidden\n        new_hidden, outputs = [], []\n        for layer_index, (rnn, drop) in enumerate(zip(self.layers, self.dropouths)):\n            with warnings.catch_warnings():\n                warnings.simplefilter(""ignore"")\n                output, new_h = rnn(output, self.hidden[layer_index])\n            new_hidden.append(new_h)\n            if layer_index != self.nlayers - 1:\n                output = drop(output)\n            outputs.append(output)\n\n        self.hidden = new_hidden\n        return outputs\n\n    def reset_hidden(self, bs):\n        self.hidden = [self.layers[l].hidden_state(bs) for l in range(self.nlayers)]\n\n    def reset(self, bs):\n        self.reset_hidden(bs)\n\n    def hidden_shape(self, bs):\n        if isinstance(self.layers[0].hidden_state(1), tuple):\n            return [self.layers[l].hidden_state(bs)[0].shape for l in range(self.nlayers)]\n        else:\n            return [self.layers[l].hidden_state(bs).shape for l in range(self.nlayers)]\n\n    def get_last_hidden_state(self):\n        return self.hidden[-1][0] if self.cell_type == ""lstm"" else self.hidden[-1]\n'"
src/quicknlp/modules/transformer.py,1,"b'import torch.nn as nn\nfrom fastai.core import T, np\n\nfrom quicknlp.utils import assert_dims, get_list\nfrom .attention import MultiHeadAttention\n\n\nclass PositionFeedForward(nn.Module):\n\n    def __init__(self, input_size, out_dim, nhid, dropout=0.):\n        super().__init__()\n        self.input_size = input_size\n        self.output_size = out_dim\n        self.nhid = nhid\n        self.pff = nn.Sequential(nn.Linear(in_features=self.input_size, out_features=self.nhid),\n                                 nn.ReLU(),\n                                 nn.Dropout(dropout),\n                                 nn.Linear(in_features=self.nhid, out_features=self.output_size)\n                                 )\n\n    def forward(self, inputs):\n        return self.pff(inputs)\n\n\nclass SubLayer(nn.Module):\n\n    def __init__(self, input_size, dropout):\n        super().__init__()\n        self.input_size = input_size,\n        self.layer_norm = nn.LayerNorm(input_size)\n        self.dropout = nn.Dropout(dropout, inplace=True)\n\n    def forward(self, input_tensor, sublayer):\n        return self.layer_norm(input_tensor.add(self.dropout(sublayer(input_tensor))))\n\n\nclass AttentionLayer(nn.Module):\n    def __init__(self, input_size, num_heads, dropout):\n        super().__init__()\n        self.input_size = input_size\n        self.nhid = input_size // num_heads\n        self.num_heads = num_heads\n        self.attention = MultiHeadAttention(num_heads=num_heads, nhid=self.nhid, out_dim=self.input_size,\n                                            keys_dim=self.input_size, values_dim=self.input_size,\n                                            query_dim=self.input_size,\n                                            dropout=dropout)\n\n    def causal_mask(self, bs, sl):\n        return T(np.tril(np.ones((bs, self.num_heads, sl, sl)))).float()\n\n    def forward(self, input_tensor, keys_vector, values_vector, mask=False):\n        sl, bs, _ = keys_vector.size()\n        mask = self.causal_mask(bs=bs, sl=sl) if mask else None\n        # outputs [sl, bs, input_size]\n        outputs = self.attention(query=input_tensor, keys=keys_vector, values=values_vector, mask=mask)\n        return outputs\n\n\nclass TransformerLayer(nn.Module):\n    def __init__(self, input_size, num_heads, nhid=2048, dropout=0.1):\n        super().__init__()\n        self.input_size = input_size\n        self.nhid = input_size // num_heads\n        self.attention = AttentionLayer(input_size=input_size, num_heads=num_heads, dropout=dropout)\n        self.linear = PositionFeedForward(input_size=input_size, out_dim=input_size, nhid=nhid, dropout=dropout)\n        self.sublayers = nn.ModuleList([SubLayer(input_size=input_size, dropout=dropout),\n                                        SubLayer(input_size=input_size, dropout=dropout)])\n\n    def forward(self, input_tensor):\n        attention_output = self.sublayers[0](input_tensor, lambda x: self.attention(x, x, x))\n        ff_output = self.sublayers[1](attention_output, self.linear)\n        return ff_output\n\n\nclass TransformerEncoderLayers(nn.Module):\n\n    def __init__(self, num_layers, input_size, num_heads, nhid, dropout=0.1):\n        super().__init__()\n        nhid = get_list(nhid, num_layers)\n        num_heads = get_list(num_heads, num_layers)\n\n        self.layers = nn.ModuleList(\n            [TransformerLayer(input_size=input_size, nhid=nhid[i], dropout=dropout, num_heads=num_heads[i]) for i in\n             range(num_layers)])\n\n    def forward(self, *input_tensors):\n        output_tensors = []\n        inputs, *_ = input_tensors\n        for layer in self.layers:\n            inputs = layer(inputs)\n            output_tensors.append(inputs)\n\n        return output_tensors\n\n\nclass TransformerLayerDecoder(TransformerLayer):\n\n    def __init__(self, input_size, num_heads, nhid, dropout=0.1):\n        super().__init__(input_size=input_size, num_heads=num_heads, nhid=nhid, dropout=dropout)\n        self.decoder_attention = AttentionLayer(input_size=input_size, num_heads=num_heads, dropout=dropout)\n        self.sublayers.append(SubLayer(input_size=input_size, dropout=dropout))\n\n    def forward(self, *inputs):\n        encoder_input, decoder_input = assert_dims(inputs, [2, None, None, self.input_size])\n        att_output = self.sublayers[0](decoder_input, lambda x: self.attention(x, x, x, mask=True))\n        dec_att_output = self.sublayers[1](att_output,\n                                           lambda x: self.decoder_attention(x, encoder_input, encoder_input))\n        return self.sublayers[2](dec_att_output, self.linear)\n\n\nclass TransformerDecoderLayers(nn.Module):\n    def __init__(self, nlayers, input_size, num_heads, nhid, dropout=0.1):\n        super().__init__()\n        self.nlayers = nlayers\n        nhid = get_list(nhid, nlayers)\n        num_heads = get_list(num_heads, nlayers)\n        self.hidden = None\n        self.input_size = input_size\n        self.layers = nn.ModuleList(\n            [TransformerLayerDecoder(input_size=input_size, nhid=nhid[i],\n                                     dropout=dropout, num_heads=num_heads[i]) for i in range(nlayers)])\n\n    def forward(self, decoder_inputs, encoder_inputs):\n        output_tensors = []\n        sl, bs, input_size = decoder_inputs.size()\n        dec_inputs = assert_dims(decoder_inputs, [sl, bs, self.input_size])\n        # nlayers, sl, bs, input_size\n        encoder_inputs = assert_dims(encoder_inputs, [self.nlayers, None, bs, self.input_size])\n        for enc_inputs, layer in zip(encoder_inputs, self.layers):\n            dec_inputs = layer(enc_inputs, dec_inputs)\n            output_tensors.append(dec_inputs)\n        assert_dims(output_tensors, [self.nlayers, sl, bs, self.input_size])\n        return output_tensors\n'"
