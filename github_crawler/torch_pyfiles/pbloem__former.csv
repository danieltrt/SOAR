file_path,api_count,code
setup.py,0,"b""from setuptools import setup\n\nsetup(name='former',\n      version='0.1',\n      description='Educational implementation of self attention',\n      url='http://www.peterbloem.nl/blog/transformers',\n      author='Peter Bloem',\n      author_email='former@peterbloem.nl',\n      license='MIT',\n      packages=['former'],\n      install_requires=[\n            'torch',\n            'tb-nightly',\n            'tqdm',\n            'numpy',\n            'torchtext'\n      ],\n      zip_safe=False)"""
experiments/_context.py,0,"b""import os\nimport sys\n\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../former')))\n\nimport former"""
experiments/classify.py,7,"b'from _context import former\nfrom former import util\n\nfrom util import d, here\n\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\nfrom torchtext import data, datasets, vocab\n\nimport numpy as np\n\nfrom argparse import ArgumentParser\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport random, tqdm, sys, math, gzip\n\n# Used for converting between nats and bits\nLOG2E = math.log2(math.e)\nTEXT = data.Field(lower=True, include_lengths=True, batch_first=True)\nLABEL = data.Field(sequential=False)\nNUM_CLS = 2\n\ndef go(arg):\n    """"""\n    Creates and trains a basic transformer for the IMDB sentiment classification task.\n    """"""\n    tbw = SummaryWriter(log_dir=arg.tb_dir) # Tensorboard logging\n\n    # load the IMDB data\n    if arg.final:\n        train, test = datasets.IMDB.splits(TEXT, LABEL)\n\n        TEXT.build_vocab(train, max_size=arg.vocab_size - 2)\n        LABEL.build_vocab(train)\n\n        train_iter, test_iter = data.BucketIterator.splits((train, test), batch_size=arg.batch_size, device=util.d())\n    else:\n        tdata, _ = datasets.IMDB.splits(TEXT, LABEL)\n        train, test = tdata.split(split_ratio=0.8)\n\n        TEXT.build_vocab(train, max_size=arg.vocab_size - 2) # - 2 to make space for <unk> and <pad>\n        LABEL.build_vocab(train)\n\n        train_iter, test_iter = data.BucketIterator.splits((train, test), batch_size=arg.batch_size, device=util.d())\n\n    print(f\'- nr. of training examples {len(train_iter)}\')\n    print(f\'- nr. of {""test"" if arg.final else ""validation""} examples {len(test_iter)}\')\n\n    if arg.max_length < 0:\n        mx = max([input.text[0].size(1) for input in train_iter])\n        mx = mx * 2\n        print(f\'- maximum sequence length: {mx}\')\n    else:\n        mx = arg.max_length\n\n    # create the model\n    model = former.CTransformer(emb=arg.embedding_size, heads=arg.num_heads, depth=arg.depth, seq_length=mx, num_tokens=arg.vocab_size, num_classes=NUM_CLS, max_pool=arg.max_pool)\n    if torch.cuda.is_available():\n        model.cuda()\n\n    opt = torch.optim.Adam(lr=arg.lr, params=model.parameters())\n    sch = torch.optim.lr_scheduler.LambdaLR(opt, lambda i: min(i / (arg.lr_warmup / arg.batch_size), 1.0))\n\n    # training loop\n    seen = 0\n    for e in range(arg.num_epochs):\n\n        print(f\'\\n epoch {e}\')\n        model.train(True)\n\n        for batch in tqdm.tqdm(train_iter):\n\n            opt.zero_grad()\n\n            input = batch.text[0]\n            label = batch.label - 1\n\n            if input.size(1) > mx:\n                input = input[:, :mx]\n            out = model(input)\n            loss = F.nll_loss(out, label)\n\n            loss.backward()\n\n            # clip gradients\n            # - If the total gradient vector has a length > 1, we clip it back down to 1.\n            if arg.gradient_clipping > 0.0:\n                nn.utils.clip_grad_norm_(model.parameters(), arg.gradient_clipping)\n\n            opt.step()\n            sch.step()\n\n            seen += input.size(0)\n            tbw.add_scalar(\'classification/train-loss\', float(loss.item()), seen)\n\n        with torch.no_grad():\n\n            model.train(False)\n            tot, cor= 0.0, 0.0\n\n            for batch in test_iter:\n\n                input = batch.text[0]\n                label = batch.label - 1\n\n                if input.size(1) > mx:\n                    input = input[:, :mx]\n                out = model(input).argmax(dim=1)\n\n                tot += float(input.size(0))\n                cor += float((label == out).sum().item())\n\n            acc = cor / tot\n            print(f\'-- {""test"" if arg.final else ""validation""} accuracy {acc:.3}\')\n            tbw.add_scalar(\'classification/test-loss\', float(loss.item()), e)\n\n\nif __name__ == ""__main__"":\n\n    parser = ArgumentParser()\n\n    parser.add_argument(""-e"", ""--num-epochs"",\n                        dest=""num_epochs"",\n                        help=""Number of epochs."",\n                        default=80, type=int)\n\n    parser.add_argument(""-b"", ""--batch-size"",\n                        dest=""batch_size"",\n                        help=""The batch size."",\n                        default=4, type=int)\n\n    parser.add_argument(""-l"", ""--learn-rate"",\n                        dest=""lr"",\n                        help=""Learning rate"",\n                        default=0.0001, type=float)\n\n    parser.add_argument(""-T"", ""--tb_dir"", dest=""tb_dir"",\n                        help=""Tensorboard logging directory"",\n                        default=\'./runs\')\n\n    parser.add_argument(""-f"", ""--final"", dest=""final"",\n                        help=""Whether to run on the real test set (if not included, the validation set is used)."",\n                        action=""store_true"")\n\n    parser.add_argument(""--max-pool"", dest=""max_pool"",\n                        help=""Use max pooling in the final classification layer."",\n                        action=""store_true"")\n\n    parser.add_argument(""-E"", ""--embedding"", dest=""embedding_size"",\n                        help=""Size of the character embeddings."",\n                        default=128, type=int)\n\n    parser.add_argument(""-V"", ""--vocab-size"", dest=""vocab_size"",\n                        help=""Number of words in the vocabulary."",\n                        default=50_000, type=int)\n\n    parser.add_argument(""-M"", ""--max"", dest=""max_length"",\n                        help=""Max sequence length. Longer sequences are clipped (-1 for no limit)."",\n                        default=512, type=int)\n\n    parser.add_argument(""-H"", ""--heads"", dest=""num_heads"",\n                        help=""Number of attention heads."",\n                        default=8, type=int)\n\n    parser.add_argument(""-d"", ""--depth"", dest=""depth"",\n                        help=""Depth of the network (nr. of self-attention layers)"",\n                        default=6, type=int)\n\n    parser.add_argument(""-r"", ""--random-seed"",\n                        dest=""seed"",\n                        help=""RNG seed. Negative for random"",\n                        default=1, type=int)\n\n    parser.add_argument(""--lr-warmup"",\n                        dest=""lr_warmup"",\n                        help=""Learning rate warmup."",\n                        default=10_000, type=int)\n\n    parser.add_argument(""--gradient-clipping"",\n                        dest=""gradient_clipping"",\n                        help=""Gradient clipping."",\n                        default=1.0, type=float)\n\n    options = parser.parse_args()\n\n    print(\'OPTIONS \', options)\n\n    go(options)\n'"
experiments/generate.py,24,"b'from _context import former\nfrom former import util, GTransformer\n\nfrom util import d, here\n\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport torch.distributions as dist\n\nimport numpy as np\n\nfrom argparse import ArgumentParser\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport random, tqdm, sys, math, gzip\n\n# NB, the enwik8 data contains tokens from 9 to 240, but well round up to the nearest\n# power of two.\nNUM_TOKENS = 256\n# Used for converting between nats and bits\nLOG2E = math.log2(math.e)\n\ndef sample(lnprobs, temperature=1.0):\n    """"""\n    Sample an element from a categorical distribution\n    :param lnprobs: Outcome log-probabilities\n    :param temperature: Sampling temperature. 1.0 follows the given distribution,\n        0.0 returns the maximum probability element.\n    :return: The index of the sampled element.\n    """"""\n\n    if temperature == 0.0:\n        return lnprobs.argmax()\n\n    p = F.softmax(lnprobs / temperature, dim=0)\n    cd = dist.Categorical(p)\n\n    return cd.sample()\n\ndef enwik8(path, n_train=int(90e6), n_valid=int(5e6), n_test=int(5e6)):\n    """"""\n    Load the enwik8 dataset from the Hutter challenge.\n\n    Adapted from https://github.com/openai/blocksparse/blob/master/examples/transformer/enwik8.py\n    :param path:\n    :param n_train:\n    :param n_valid:\n    :param n_test:\n    :return:\n    """"""\n    with gzip.open(path) if path.endswith(\'.gz\') else open(path) as file:\n        X = np.fromstring(file.read(n_train + n_valid + n_test), dtype=np.uint8)\n        trX, vaX, teX = np.split(X, [n_train, n_train + n_valid])\n        return torch.from_numpy(trX), torch.from_numpy(vaX), torch.from_numpy(teX)\n\ndef go(arg):\n\n    if arg.seed < 0:\n        seed = random.randint(0, 1000000)\n        print(\'random seed: \', seed)\n    else:\n        torch.manual_seed(arg.seed)\n\n    tbw = SummaryWriter(log_dir=arg.tb_dir) # Tensorboard logging\n\n    # load the data (validation unless arg.final is true, then test)\n    arg.data = here(\'data/enwik8.gz\') if arg.data is None else arg.data\n\n    data_train, data_val, data_test = enwik8(arg.data)\n    data_train, data_test = (torch.cat([data_train, data_val], dim=0), data_test) \\\n                            if arg.final else (data_train, data_val)\n\n    # create the model\n    model = GTransformer(emb=arg.embedding_size, heads=arg.num_heads, depth=arg.depth, seq_length=arg.context, num_tokens=NUM_TOKENS, wide=arg.wide)\n    if torch.cuda.is_available():\n        model.cuda()\n\n\n    opt = torch.optim.Adam(lr=arg.lr, params=model.parameters())\n    # linear learning rate warmup\n    sch = torch.optim.lr_scheduler.LambdaLR(opt, lambda i: min(i / (arg.lr_warmup / arg.batch_size), 1.0))\n\n    # training loop\n    # - note: we don\'t loop over the data, instead we sample a batch of random subsequences each time.\n    for i in tqdm.trange(arg.num_batches):\n\n        opt.zero_grad()\n\n        # sample a batch of random subsequences\n        starts = torch.randint(size=(arg.batch_size, ), low=0, high=data_train.size(0) - arg.context - 1)\n        seqs_source = [data_train[start  :start+arg.context  ] for start in starts]\n        seqs_target = [data_train[start+1:start+arg.context+1] for start in starts]\n        source = torch.cat([s[None, :] for s in seqs_source ], dim=0).to(torch.long)\n        target = torch.cat([s[None, :] for s in seqs_target ], dim=0).to(torch.long)\n        # - target is the same sequence as source, except one character ahead\n\n        if torch.cuda.is_available():\n            source, target = source.cuda(), target.cuda()\n        source, target = Variable(source), Variable(target)\n\n        output = model(source)\n\n        loss = F.nll_loss(output.transpose(2, 1), target, reduction=\'mean\')\n        tbw.add_scalar(\'transformer/train-loss\', float(loss.item()) * LOG2E, i * arg.batch_size)\n\n        loss.backward()\n\n        # clip gradients\n        # - If the total gradient vector has a length > 1, we clip it back down to 1.\n        if arg.gradient_clipping > 0.0:\n            nn.utils.clip_grad_norm_(model.parameters(), arg.gradient_clipping)\n\n        opt.step()\n        sch.step()\n\n        # - validate every {arg.test_every} steps. First we compute the\n        #   compression on the validation (or a subset)\n        #   then we generate some random text to monitor progress\n        if i != 0 and (i % arg.test_every == 0 or i == arg.num_batches - 1):\n\n            upto = data_test.size(0) if i == arg.num_batches - 1 else arg.test_subset\n            data_sub = data_test[:upto]\n\n            with torch.no_grad():\n                bits, tot = 0.0, 0\n                batch = [] # buffer, every time it fills up, we run it through the model\n\n                for current in range(data_sub.size(0)):\n\n                    fr = max(0, current - arg.context)\n                    to = current + 1\n\n                    context = data_sub[fr:to].to(torch.long)\n                    if context.size(0) < arg.context + 1:\n                        pad = torch.zeros(size=(arg.context + 1 - context.size(0),), dtype=torch.long)\n                        context = torch.cat([pad, context], dim=0)\n\n                        assert context.size(0) == arg.context + 1\n\n                    if torch.cuda.is_available():\n                        context = context.cuda()\n\n                    batch.append(context[None, :])\n\n                    if len(batch) == arg.test_batchsize or current == data_sub.size(0) - 1:\n\n                        # batch is full, run it through the model\n                        b = len(batch)\n\n                        all = torch.cat(batch, dim=0)\n                        source = all[:, :-1] # input\n                        target = all[:, -1]  # target values\n\n                        output = model(source)\n\n                        lnprobs = output[torch.arange(b, device=d()), -1, target]\n                        log2probs = lnprobs * LOG2E # convert from nats to bits\n\n                        bits += - log2probs.sum()\n                        batch = [] # empty buffer\n\n                bits_per_byte = bits / data_sub.size(0)\n\n                # print validation performance. 1 bit per byte is (currently) state of the art.\n                print(f\'epoch{i}: {bits_per_byte:.4} bits per byte\')\n                tbw.add_scalar(f\'transformer/eval-loss\', bits_per_byte, i * arg.batch_size)\n\n                # generate some random text\n                GENSIZE = 600\n                TEMP = 0.5\n                seedfr = random.randint(0, data_test.size(0) - arg.context)\n                input = data_test[seedfr:seedfr + arg.context].to(torch.long)\n\n                if torch.cuda.is_available():\n                    input = input.cuda()\n\n                input = Variable(input)\n\n                print(\'[\', end=\'\', flush=True)\n                for c in input:\n                    print(str(chr(c)), end=\'\', flush=True)\n                print(\']\', end=\'\', flush=True)\n\n                for _ in range(GENSIZE):\n                    output = model(input[None, :])\n                    c = sample(output[0, -1, :], TEMP)\n                    print(str(chr(max(32, c))), end=\'\', flush=True)\n\n                    input = torch.cat([input[1:], c[None]], dim=0)\n\n                print()\n\nif __name__ == ""__main__"":\n\n    ## Parse the command line options\n    parser = ArgumentParser()\n\n    parser.add_argument(""-N"", ""--num-batches"",\n                        dest=""num_batches"",\n                        help=""Number of batches to train on. Each batch contains randomly sampled subsequences of the data."",\n                        default=1_000_000, type=int)\n\n    parser.add_argument(""-b"", ""--batch-size"",\n                        dest=""batch_size"",\n                        help=""The batch size."",\n                        default=32, type=int)\n\n    parser.add_argument(""-D"", ""--data"", dest=""data"",\n                        help=""Data file. Will be read as a string of 8-bit characters."",\n                        default=None)\n\n    parser.add_argument(""-l"", ""--learn-rate"",\n                        dest=""lr"",\n                        help=""Learning rate"",\n                        default=0.0001, type=float)\n\n    parser.add_argument(""-T"", ""--tb_dir"", dest=""tb_dir"",\n                        help=""Tensorboard logging directory"",\n                        default=\'./runs\')\n\n    parser.add_argument(""-f"", ""--final"", dest=""final"",\n                        help=""Whether to run on the real test set (if not included, the validation set is used)."",\n                        action=""store_true"")\n\n    parser.add_argument(""-E"", ""--embedding"", dest=""embedding_size"",\n                        help=""Size of the character embeddings."",\n                        default=128, type=int)\n\n    parser.add_argument(""-H"", ""--heads"", dest=""num_heads"",\n                        help=""Number of attention heads."",\n                        default=8, type=int)\n\n    parser.add_argument(""-C"", ""--context"", dest=""context"",\n                        help=""Length of the sequences extracted from the corpus (and the context used during inference)."",\n                        default=256, type=int)\n\n    parser.add_argument(""-d"", ""--depth"", dest=""depth"",\n                        help=""Depth of the network (nr of self-attention layers)"",\n                        default=12, type=int)\n\n    parser.add_argument(""-r"", ""--random-seed"",\n                        dest=""seed"",\n                        help=""RNG seed. Negative for random"",\n                        default=1, type=int)\n\n    parser.add_argument(""--test-every"",\n                        dest=""test_every"",\n                        help=""How many batches between tests."",\n                        default=1500, type=int)\n\n    parser.add_argument(""--test-subset"",\n                        dest=""test_subset"",\n                        help=""A subset for the validation tests."",\n                        default=100000, type=int)\n\n    parser.add_argument(""--test-batchsize"",\n                        dest=""test_batchsize"",\n                        help=""Batch size for computing the validation loss. This can be a bit bigger than the training batch size."",\n                        default=64, type=int)\n\n    parser.add_argument(""--gradient-clipping"",\n                        dest=""gradient_clipping"",\n                        help=""Gradient clipping."",\n                        default=1.0, type=float)\n\n    parser.add_argument(""--lr-warmup"",\n                        dest=""lr_warmup"",\n                        help=""Learning rate warmup."",\n                        default=5000, type=int)\n\n    parser.add_argument(""--wide"", dest=""wide"",\n                        help=""Use wide self attention instead of narrow self attention."",\n                        action=""store_true"")\n\n    options = parser.parse_args()\n\n    print(\'OPTIONS \', options)\n\n    go(options)\n'"
former/__init__.py,0,"b'from .modules import SelfAttentionWide, SelfAttentionWide, TransformerBlock\n\nfrom .transformers import GTransformer, CTransformer\n\n'"
former/modules.py,5,"b'from former import util\nfrom util import mask_\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nimport random, math\n\nclass SelfAttentionWide(nn.Module):\n    def __init__(self, emb, heads=8, mask=False):\n        """"""\n\n        :param emb:\n        :param heads:\n        :param mask:\n        """"""\n\n        super().__init__()\n\n        self.emb = emb\n        self.heads = heads\n        self.mask = mask\n\n        self.tokeys = nn.Linear(emb, emb * heads, bias=False)\n        self.toqueries = nn.Linear(emb, emb * heads, bias=False)\n        self.tovalues = nn.Linear(emb, emb * heads, bias=False)\n\n        self.unifyheads = nn.Linear(heads * emb, emb)\n\n    def forward(self, x):\n\n        b, t, e = x.size()\n        h = self.heads\n        assert e == self.emb, f\'Input embedding dim ({e}) should match layer embedding dim ({self.emb})\'\n\n        keys    = self.tokeys(x)   .view(b, t, h, e)\n        queries = self.toqueries(x).view(b, t, h, e)\n        values  = self.tovalues(x) .view(b, t, h, e)\n\n        # compute scaled dot-product self-attention\n\n        # - fold heads into the batch dimension\n        keys = keys.transpose(1, 2).contiguous().view(b * h, t, e)\n        queries = queries.transpose(1, 2).contiguous().view(b * h, t, e)\n        values = values.transpose(1, 2).contiguous().view(b * h, t, e)\n\n        queries = queries / (e ** (1/4))\n        keys    = keys / (e ** (1/4))\n        # - Instead of dividing the dot products by sqrt(e), we scale the keys and values.\n        #   This should be more memory efficient\n\n        # - get dot product of queries and keys, and scale\n        dot = torch.bmm(queries, keys.transpose(1, 2))\n\n        assert dot.size() == (b*h, t, t)\n\n        if self.mask: # mask out the upper half of the dot matrix, excluding the diagonal\n            mask_(dot, maskval=float(\'-inf\'), mask_diagonal=False)\n\n        dot = F.softmax(dot, dim=2)\n        # - dot now has row-wise self-attention probabilities\n\n        # apply the self attention to the values\n        out = torch.bmm(dot, values).view(b, h, t, e)\n\n        # swap h, t back, unify heads\n        out = out.transpose(1, 2).contiguous().view(b, t, h * e)\n\n        return self.unifyheads(out)\n\nclass SelfAttentionNarrow(nn.Module):\n\n    def __init__(self, emb, heads=8, mask=False):\n        """"""\n\n        :param emb:\n        :param heads:\n        :param mask:\n        """"""\n\n        super().__init__()\n\n        assert emb % heads == 0, f\'Embedding dimension ({emb}) should be divisible by nr. of heads ({heads})\'\n\n        self.emb = emb\n        self.heads = heads\n        self.mask = mask\n\n        s = emb // heads\n        # - We will break the embedding into `heads` chunks and feed each to a different attention head\n\n        self.tokeys    = nn.Linear(s, s, bias=False)\n        self.toqueries = nn.Linear(s, s, bias=False)\n        self.tovalues  = nn.Linear(s, s, bias=False)\n\n        self.unifyheads = nn.Linear(heads * s, emb)\n\n    def forward(self, x):\n\n        b, t, e = x.size()\n        h = self.heads\n        assert e == self.emb, f\'Input embedding dim ({e}) should match layer embedding dim ({self.emb})\'\n\n        s = e // h\n        x = x.view(b, t, h, s)\n\n        keys    = self.tokeys(x)\n        queries = self.toqueries(x)\n        values  = self.tovalues(x)\n\n        assert keys.size() == (b, t, h, s)\n        assert queries.size() == (b, t, h, s)\n        assert values.size() == (b, t, h, s)\n\n        # Compute scaled dot-product self-attention\n\n        # - fold heads into the batch dimension\n        keys = keys.transpose(1, 2).contiguous().view(b * h, t, s)\n        queries = queries.transpose(1, 2).contiguous().view(b * h, t, s)\n        values = values.transpose(1, 2).contiguous().view(b * h, t, s)\n\n        queries = queries / (e ** (1/4))\n        keys    = keys / (e ** (1/4))\n        # - Instead of dividing the dot products by sqrt(e), we scale the keys and values.\n        #   This should be more memory efficient\n\n        # - get dot product of queries and keys, and scale\n        dot = torch.bmm(queries, keys.transpose(1, 2))\n\n        assert dot.size() == (b*h, t, t)\n\n        if self.mask: # mask out the upper half of the dot matrix, excluding the diagonal\n            mask_(dot, maskval=float(\'-inf\'), mask_diagonal=False)\n\n        dot = F.softmax(dot, dim=2)\n        # - dot now has row-wise self-attention probabilities\n\n        # apply the self attention to the values\n        out = torch.bmm(dot, values).view(b, h, t, s)\n\n        # swap h, t back, unify heads\n        out = out.transpose(1, 2).contiguous().view(b, t, s * h)\n\n        return self.unifyheads(out)\n\nclass TransformerBlock(nn.Module):\n\n    def __init__(self, emb, heads, mask, seq_length, ff_hidden_mult=4, dropout=0.0, wide=True):\n        super().__init__()\n\n        self.attention = SelfAttentionWide(emb, heads=heads, mask=mask) if wide \\\n                    else SelfAttentionNarrow(emb, heads=heads, mask=mask)\n        self.mask = mask\n\n        self.norm1 = nn.LayerNorm(emb)\n        self.norm2 = nn.LayerNorm(emb)\n\n        self.ff = nn.Sequential(\n            nn.Linear(emb, ff_hidden_mult * emb),\n            nn.ReLU(),\n            nn.Linear(ff_hidden_mult * emb, emb)\n        )\n\n        self.do = nn.Dropout(dropout)\n\n    def forward(self, x):\n\n        attended = self.attention(x)\n\n        x = self.norm1(attended + x)\n\n        x = self.do(x)\n\n        fedforward = self.ff(x)\n\n        x = self.norm2(fedforward + x)\n\n        x = self.do(x)\n\n        return x'"
former/transformers.py,3,"b'import torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom .modules import TransformerBlock\n\nfrom .util import d\n\nclass GTransformer(nn.Module):\n    """"""\n    Transformer for generating text (character by character).\n    """"""\n\n    def __init__(self, emb, heads, depth, seq_length, num_tokens, wide=False):\n        super().__init__()\n\n        self.num_tokens = num_tokens\n        self.token_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=num_tokens)\n        self.pos_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=seq_length)\n\n        tblocks = []\n        for i in range(depth):\n            tblocks.append(\n                TransformerBlock(emb=emb, heads=heads, seq_length=seq_length, mask=True, wide=wide))\n\n        self.tblocks = nn.Sequential(*tblocks)\n\n        self.toprobs = nn.Linear(emb, num_tokens)\n\n    def forward(self, x):\n        """"""\n        :param x: A (batch, sequence length) integer tensor of token indices.\n        :return: predicted log-probability vectors for each token based on the preceding tokens.\n        """"""\n        tokens = self.token_embedding(x)\n        b, t, e = tokens.size()\n\n        positions = self.pos_embedding(torch.arange(t, device=d()))[None, :, :].expand(b, t, e)\n        x = tokens + positions\n\n        x = self.tblocks(x)\n\n        x = self.toprobs(x.view(b*t, e)).view(b, t, self.num_tokens)\n\n        return F.log_softmax(x, dim=2)\n\nclass CTransformer(nn.Module):\n    """"""\n    Transformer for classifying sequences\n    """"""\n\n    def __init__(self, emb, heads, depth, seq_length, num_tokens, num_classes, max_pool=True, dropout=0.0, wide=False):\n        """"""\n        :param emb: Embedding dimension\n        :param heads: nr. of attention heads\n        :param depth: Number of transformer blocks\n        :param seq_length: Expected maximum sequence length\n        :param num_tokens: Number of tokens (usually words) in the vocabulary\n        :param num_classes: Number of classes.\n        :param max_pool: If true, use global max pooling in the last layer. If false, use global\n                         average pooling.\n        """"""\n        super().__init__()\n\n        self.num_tokens, self.max_pool = num_tokens, max_pool\n\n        self.token_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=num_tokens)\n        self.pos_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=seq_length)\n\n        tblocks = []\n        for i in range(depth):\n            tblocks.append(\n                TransformerBlock(emb=emb, heads=heads, seq_length=seq_length, mask=False, dropout=dropout, wide=wide))\n\n        self.tblocks = nn.Sequential(*tblocks)\n\n        self.toprobs = nn.Linear(emb, num_classes)\n\n        self.do = nn.Dropout(dropout)\n\n    def forward(self, x):\n        """"""\n        :param x: A batch by sequence length integer tensor of token indices.\n        :return: predicted log-probability vectors for each token based on the preceding tokens.\n        """"""\n        tokens = self.token_embedding(x)\n        b, t, e = tokens.size()\n\n        positions = self.pos_embedding(torch.arange(t, device=d()))[None, :, :].expand(b, t, e)\n        x = tokens + positions\n        x = self.do(x)\n\n        x = self.tblocks(x)\n\n        x = x.max(dim=1)[0] if self.max_pool else x.mean(dim=1) # pool over the time dimension\n\n        x = self.toprobs(x)\n\n        return F.log_softmax(x, dim=1)\n\n'"
tests/_context.py,0,"b""import os\nimport sys\n\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../former')))\n\nimport former"""
former/util/__init__.py,0,"b'from .util import mask_, d, here, contains_nan\n'"
former/util/util.py,2,"b'import torch, os\n\ndef mask_(matrices, maskval=0.0, mask_diagonal=True):\n    """"""\n    Masks out all values in the given batch of matrices where i <= j holds,\n    i < j if mask_diagonal is false\n\n    In place operation\n\n    :param tns:\n    :return:\n    """"""\n\n    b, h, w = matrices.size()\n\n    indices = torch.triu_indices(h, w, offset=0 if mask_diagonal else 1)\n    matrices[:, indices[0], indices[1]] = maskval\n\ndef d(tensor=None):\n    """"""\n    Returns a device string either for the best available device,\n    or for the device corresponding to the argument\n    :param tensor:\n    :return:\n    """"""\n    if tensor is None:\n        return \'cuda\' if torch.cuda.is_available() else \'cpu\'\n    return \'cuda\' if tensor.is_cuda else \'cpu\'\n\ndef here(subpath=None):\n    """"""\n    :return: the path in which the package resides (the directory containing the \'former\' dir)\n    """"""\n    if subpath is None:\n        return os.path.abspath(os.path.join(os.path.dirname(__file__), \'../..\'))\n\n    return os.path.abspath(os.path.join(os.path.dirname(__file__), \'../..\', subpath))\n\ndef contains_nan(tensor):\n    return bool((tensor != tensor).sum() > 0)'"
