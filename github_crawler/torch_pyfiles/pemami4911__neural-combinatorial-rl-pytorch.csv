file_path,api_count,code
beam_search.py,2,"b'# beam search implementation in PyTorch.""""""\n#\n#\n#         hyp1#-hyp1---hyp1 -hyp1\n#                 \\             /\n#         hyp2 \\-hyp2 /-hyp2#hyp2\n#                               /      \\\n#         hyp3#-hyp3---hyp3 -hyp3\n#         ========================\n#\n# Takes care of beams, back pointers, and scores.\n\n# Code borrowed from https://github.com/MaximumEntropy/Seq2Seq-PyTorch/blob/master/beam_search.py,\n# who borrowed it from PyTorch OpenNMT example\n# https://github.com/pytorch/examples/blob/master/OpenNMT/onmt/Beam.py\n# :-) \n\nimport torch\n\n\nclass Beam(object):\n    """"""Ordered beam of candidate outputs. Fixed length.""""""\n\n    def __init__(self, size, steps, cuda=False):\n        """"""Initialize params.""""""\n        self.size = size\n        self.done = False\n        self.pad = -1\n        self.steps = steps\n        self.current_step = 0\n        self.tt = torch.cuda if cuda else torch\n\n        # The score for each translation on the beam.\n        self.scores = self.tt.FloatTensor(size).zero_()\n\n        # The backpointers at each time-step.\n        self.prevKs = []\n\n        # The outputs at each time-step.\n        self.nextYs = [self.tt.LongTensor(size).fill_(self.pad)]\n\n        # The attentions (matrix) for each time.\n        self.attn = []\n\n    # Get the outputs for the current timestep.\n    def get_current_state(self):\n        """"""Get state of beam.""""""\n        return self.nextYs[-1]\n\n    # Get the backpointers for the current timestep.\n    def get_current_origin(self):\n        """"""Get the backpointer to the beam at this step.""""""\n        return self.prevKs[-1]\n\n    #  Given prob over words for every last beam `wordLk` and attention\n    #   `attnOut`: Compute and update the beam search.\n    #\n    # Parameters:\n    #\n    #     * `wordLk`- probs of advancing from the last step (K x words)\n    #     * `attnOut`- attention at the last step\n    #\n    # Returns: True if beam search is complete.\n\n    def advance(self, workd_lk):\n        """"""Advance the beam.""""""\n        num_words = workd_lk.size(1)\n\n        # Sum the previous scores.\n        if len(self.prevKs) > 0:\n            beam_lk = workd_lk + self.scores.unsqueeze(1).expand_as(workd_lk)\n        else:\n            beam_lk = workd_lk[0]\n\n        flat_beam_lk = beam_lk.view(-1)\n\n        bestScores, bestScoresId = flat_beam_lk.topk(self.size, 0, True, True)\n        self.scores = bestScores\n\n        # bestScoresId is flattened beam x word array, so calculate which\n        # word and beam each score came from\n        prev_k = bestScoresId / num_words\n        self.prevKs.append(prev_k)\n        self.nextYs.append(bestScoresId - prev_k * num_words)\n\n        self.current_step += 1\n        # End condition is when top-of-beam is EOS.\n        if self.current_step == self.steps:\n            self.done = True\n\n        return self.done\n\n    def sort_best(self):\n        """"""Sort the beam.""""""\n        return torch.sort(self.scores, 0, True)\n\n    # Get the score of the best in the beam.\n    def get_best(self):\n        """"""Get the most likely candidate.""""""\n        scores, ids = self.sort_best()\n        return scores[1], ids[1]\n\n    # Walk back to construct the full hypothesis.\n    #\n    # Parameters.\n    #\n    #     * `k` - the position in the beam to construct.\n    #\n    # Returns.\n    #\n    #     1. The hypothesis\n    #     2. The attention at each time step.\n    def get_hyp(self, k):\n        """"""Get hypotheses.""""""\n        hyp = []\n        # print(len(self.prevKs), len(self.nextYs), len(self.attn))\n        for j in range(len(self.prevKs) - 1, -1, -1):\n            hyp.append(self.nextYs[j + 1][k])\n            k = self.prevKs[j][k]\n\n        return hyp[::-1]\n'"
neural_combinatorial_rl.py,16,"b'import torch\nimport torch.nn as nn\nimport torch.autograd as autograd\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport math\nimport numpy as np\n\nfrom beam_search import Beam\n\n\nclass Encoder(nn.Module):\n    """"""Maps a graph represented as an input sequence\n    to a hidden vector""""""\n    def __init__(self, input_dim, hidden_dim, use_cuda):\n        super(Encoder, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.lstm = nn.LSTM(input_dim, hidden_dim)\n        self.use_cuda = use_cuda\n        self.enc_init_state = self.init_hidden(hidden_dim)\n\n    def forward(self, x, hidden):\n        output, hidden = self.lstm(x, hidden)\n        return output, hidden\n    \n    def init_hidden(self, hidden_dim):\n        """"""Trainable initial hidden state""""""\n        enc_init_hx = Variable(torch.zeros(hidden_dim), requires_grad=False)\n        if self.use_cuda:\n            enc_init_hx = enc_init_hx.cuda()\n\n        #enc_init_hx.data.uniform_(-(1. / math.sqrt(hidden_dim)),\n        #        1. / math.sqrt(hidden_dim))\n\n        enc_init_cx = Variable(torch.zeros(hidden_dim), requires_grad=False)\n        if self.use_cuda:\n            enc_init_cx = enc_init_cx.cuda()\n\n        #enc_init_cx = nn.Parameter(enc_init_cx)\n        #enc_init_cx.data.uniform_(-(1. / math.sqrt(hidden_dim)),\n        #        1. / math.sqrt(hidden_dim))\n        return (enc_init_hx, enc_init_cx)\n\n\nclass Attention(nn.Module):\n    """"""A generic attention module for a decoder in seq2seq""""""\n    def __init__(self, dim, use_tanh=False, C=10, use_cuda=True):\n        super(Attention, self).__init__()\n        self.use_tanh = use_tanh\n        self.project_query = nn.Linear(dim, dim)\n        self.project_ref = nn.Conv1d(dim, dim, 1, 1)\n        self.C = C  # tanh exploration\n        self.tanh = nn.Tanh()\n        \n        v = torch.FloatTensor(dim)\n        if use_cuda:\n            v = v.cuda()  \n        self.v = nn.Parameter(v)\n        self.v.data.uniform_(-(1. / math.sqrt(dim)) , 1. / math.sqrt(dim))\n        \n    def forward(self, query, ref):\n        """"""\n        Args: \n            query: is the hidden state of the decoder at the current\n                time step. batch x dim\n            ref: the set of hidden states from the encoder. \n                sourceL x batch x hidden_dim\n        """"""\n        # ref is now [batch_size x hidden_dim x sourceL]\n        ref = ref.permute(1, 2, 0)\n        q = self.project_query(query).unsqueeze(2)  # batch x dim x 1\n        e = self.project_ref(ref)  # batch_size x hidden_dim x sourceL \n        # expand the query by sourceL\n        # batch x dim x sourceL\n        expanded_q = q.repeat(1, 1, e.size(2)) \n        # batch x 1 x hidden_dim\n        v_view = self.v.unsqueeze(0).expand(\n                expanded_q.size(0), len(self.v)).unsqueeze(1)\n        # [batch_size x 1 x hidden_dim] * [batch_size x hidden_dim x sourceL]\n        u = torch.bmm(v_view, self.tanh(expanded_q + e)).squeeze(1)\n        if self.use_tanh:\n            logits = self.C * self.tanh(u)\n        else:\n            logits = u  \n        return e, logits\n\n\nclass Decoder(nn.Module):\n    def __init__(self, \n            embedding_dim,\n            hidden_dim,\n            max_length,\n            tanh_exploration,\n            terminating_symbol,\n            use_tanh,\n            decode_type,\n            n_glimpses=1,\n            beam_size=0,\n            use_cuda=True):\n        super(Decoder, self).__init__()\n        \n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.n_glimpses = n_glimpses\n        self.max_length = max_length\n        self.terminating_symbol = terminating_symbol \n        self.decode_type = decode_type\n        self.beam_size = beam_size\n        self.use_cuda = use_cuda\n\n        self.input_weights = nn.Linear(embedding_dim, 4 * hidden_dim)\n        self.hidden_weights = nn.Linear(hidden_dim, 4 * hidden_dim)\n\n        self.pointer = Attention(hidden_dim, use_tanh=use_tanh, C=tanh_exploration, use_cuda=self.use_cuda)\n        self.glimpse = Attention(hidden_dim, use_tanh=False, use_cuda=self.use_cuda)\n        self.sm = nn.Softmax()\n\n    def apply_mask_to_logits(self, step, logits, mask, prev_idxs):    \n        if mask is None:\n            mask = torch.zeros(logits.size()).byte()\n            if self.use_cuda:\n                mask = mask.cuda()\n    \n        maskk = mask.clone()\n\n        # to prevent them from being reselected. \n        # Or, allow re-selection and penalize in the objective function\n        if prev_idxs is not None:\n            # set most recently selected idx values to 1\n            maskk[[x for x in range(logits.size(0))],\n                    prev_idxs.data] = 1\n            logits[maskk] = -np.inf\n        return logits, maskk\n\n    def forward(self, decoder_input, embedded_inputs, hidden, context):\n        """"""\n        Args:\n            decoder_input: The initial input to the decoder\n                size is [batch_size x embedding_dim]. Trainable parameter.\n            embedded_inputs: [sourceL x batch_size x embedding_dim]\n            hidden: the prev hidden state, size is [batch_size x hidden_dim]. \n                Initially this is set to (enc_h[-1], enc_c[-1])\n            context: encoder outputs, [sourceL x batch_size x hidden_dim] \n        """"""\n        def recurrence(x, hidden, logit_mask, prev_idxs, step):\n            \n            hx, cx = hidden  # batch_size x hidden_dim\n            \n            gates = self.input_weights(x) + self.hidden_weights(hx)\n            ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n\n            ingate = F.sigmoid(ingate)\n            forgetgate = F.sigmoid(forgetgate)\n            cellgate = F.tanh(cellgate)\n            outgate = F.sigmoid(outgate)\n\n            cy = (forgetgate * cx) + (ingate * cellgate)\n            hy = outgate * F.tanh(cy)  # batch_size x hidden_dim\n            \n            g_l = hy\n            for i in range(self.n_glimpses):\n                ref, logits = self.glimpse(g_l, context)\n                logits, logit_mask = self.apply_mask_to_logits(step, logits, logit_mask, prev_idxs)\n                # [batch_size x h_dim x sourceL] * [batch_size x sourceL x 1] = \n                # [batch_size x h_dim x 1]\n                g_l = torch.bmm(ref, self.sm(logits).unsqueeze(2)).squeeze(2) \n            _, logits = self.pointer(g_l, context)\n            \n            logits, logit_mask = self.apply_mask_to_logits(step, logits, logit_mask, prev_idxs)\n            probs = self.sm(logits)\n            return hy, cy, probs, logit_mask\n    \n        batch_size = context.size(1)\n        outputs = []\n        selections = []\n        steps = range(self.max_length)  # or until terminating symbol ?\n        inps = []\n        idxs = None\n        mask = None\n       \n        if self.decode_type == ""stochastic"":\n            for i in steps:\n                hx, cx, probs, mask = recurrence(decoder_input, hidden, mask, idxs, i)\n                hidden = (hx, cx)\n                # select the next inputs for the decoder [batch_size x hidden_dim]\n                decoder_input, idxs = self.decode_stochastic(\n                    probs,\n                    embedded_inputs,\n                    selections)\n                inps.append(decoder_input) \n                # use outs to point to next object\n                outputs.append(probs)\n                selections.append(idxs)\n            return (outputs, selections), hidden\n        \n        elif self.decode_type == ""beam_search"":\n            \n            # Expand input tensors for beam search\n            decoder_input = Variable(decoder_input.data.repeat(self.beam_size, 1))\n            context = Variable(context.data.repeat(1, self.beam_size, 1))\n            hidden = (Variable(hidden[0].data.repeat(self.beam_size, 1)),\n                    Variable(hidden[1].data.repeat(self.beam_size, 1)))\n            \n            beam = [\n                    Beam(self.beam_size, self.max_length, cuda=self.use_cuda) \n                    for k in range(batch_size)\n            ]\n            \n            for i in steps:\n                hx, cx, probs, mask = recurrence(decoder_input, hidden, mask, idxs, i)\n                hidden = (hx, cx)\n                \n                probs = probs.view(self.beam_size, batch_size, -1\n                        ).transpose(0, 1).contiguous()\n                \n                n_best = 1\n                # select the next inputs for the decoder [batch_size x hidden_dim]\n                decoder_input, idxs, active = self.decode_beam(probs,\n                        embedded_inputs, beam, batch_size, n_best, i)\n               \n                inps.append(decoder_input) \n                # use probs to point to next object\n                if self.beam_size > 1:\n                    outputs.append(probs[:, 0,:])\n                else:\n                    outputs.append(probs.squeeze(0))\n                # Check for indexing\n                selections.append(idxs)\n                 # Should be done decoding\n                if len(active) == 0:\n                    break\n                decoder_input = Variable(decoder_input.data.repeat(self.beam_size, 1))\n\n            return (outputs, selections), hidden\n\n    def decode_stochastic(self, probs, embedded_inputs, selections):\n        """"""\n        Return the next input for the decoder by selecting the \n        input corresponding to the max output\n\n        Args: \n            probs: [batch_size x sourceL]\n            embedded_inputs: [sourceL x batch_size x embedding_dim]\n            selections: list of all of the previously selected indices during decoding\n       Returns:\n            Tensor of size [batch_size x sourceL] containing the embeddings\n            from the inputs corresponding to the [batch_size] indices\n            selected for this iteration of the decoding, as well as the \n            corresponding indicies\n        """"""\n        batch_size = probs.size(0)\n        # idxs is [batch_size]\n        idxs = probs.multinomial().squeeze(1)\n\n        # due to race conditions, might need to resample here\n        for old_idxs in selections:\n            # compare new idxs\n            # elementwise with the previous idxs. If any matches,\n            # then need to resample\n            if old_idxs.eq(idxs).data.any():\n                print(\' [!] resampling due to race condition\')\n                idxs = probs.multinomial().squeeze(1)\n                break\n\n        sels = embedded_inputs[idxs.data, [i for i in range(batch_size)], :] \n        return sels, idxs\n\n    def decode_beam(self, probs, embedded_inputs, beam, batch_size, n_best, step):\n        active = []\n        for b in range(batch_size):\n            if beam[b].done:\n                continue\n\n            if not beam[b].advance(probs.data[b]):\n                active += [b]\n        \n        \n        all_hyp, all_scores = [], []\n        for b in range(batch_size):\n            scores, ks = beam[b].sort_best()\n            all_scores += [scores[:n_best]]\n            hyps = zip(*[beam[b].get_hyp(k) for k in ks[:n_best]])\n            all_hyp += [hyps]\n        \n        all_idxs = Variable(torch.LongTensor([[x for x in hyp] for hyp in all_hyp]).squeeze())\n      \n        if all_idxs.dim() == 2:\n            if all_idxs.size(1) > n_best:\n                idxs = all_idxs[:,-1]\n            else:\n                idxs = all_idxs\n        elif all_idxs.dim() == 3:\n            idxs = all_idxs[:, -1, :]\n        else:\n            if all_idxs.size(0) > 1:\n                idxs = all_idxs[-1]\n            else:\n                idxs = all_idxs\n        \n        if self.use_cuda:\n            idxs = idxs.cuda()\n\n        if idxs.dim() > 1:\n            x = embedded_inputs[idxs.transpose(0,1).contiguous().data, \n                    [x for x in range(batch_size)], :]\n        else:\n            x = embedded_inputs[idxs.data, [x for x in range(batch_size)], :]\n        return x.view(idxs.size(0) * n_best, embedded_inputs.size(2)), idxs, active\n\nclass PointerNetwork(nn.Module):\n    """"""The pointer network, which is the core seq2seq \n    model""""""\n    def __init__(self, \n            embedding_dim,\n            hidden_dim,\n            max_decoding_len,\n            terminating_symbol,\n            n_glimpses,\n            tanh_exploration,\n            use_tanh,\n            beam_size,\n            use_cuda):\n        super(PointerNetwork, self).__init__()\n\n        self.encoder = Encoder(\n                embedding_dim,\n                hidden_dim,\n                use_cuda)\n\n        self.decoder = Decoder(\n                embedding_dim,\n                hidden_dim,\n                max_length=max_decoding_len,\n                tanh_exploration=tanh_exploration,\n                use_tanh=use_tanh,\n                terminating_symbol=terminating_symbol,\n                decode_type=""stochastic"",\n                n_glimpses=n_glimpses,\n                beam_size=beam_size,\n                use_cuda=use_cuda)\n\n        # Trainable initial hidden states\n        dec_in_0 = torch.FloatTensor(embedding_dim)\n        if use_cuda:\n            dec_in_0 = dec_in_0.cuda()\n\n        self.decoder_in_0 = nn.Parameter(dec_in_0)\n        self.decoder_in_0.data.uniform_(-(1. / math.sqrt(embedding_dim)),\n                1. / math.sqrt(embedding_dim))\n            \n    def forward(self, inputs):\n        """""" Propagate inputs through the network\n        Args: \n            inputs: [sourceL x batch_size x embedding_dim]\n        """"""\n        \n        (encoder_hx, encoder_cx) = self.encoder.enc_init_state\n        encoder_hx = encoder_hx.unsqueeze(0).repeat(inputs.size(1), 1).unsqueeze(0)       \n        encoder_cx = encoder_cx.unsqueeze(0).repeat(inputs.size(1), 1).unsqueeze(0)       \n        \n        # encoder forward pass\n        enc_h, (enc_h_t, enc_c_t) = self.encoder(inputs, (encoder_hx, encoder_cx))\n\n        dec_init_state = (enc_h_t[-1], enc_c_t[-1])\n    \n        # repeat decoder_in_0 across batch\n        decoder_input = self.decoder_in_0.unsqueeze(0).repeat(inputs.size(1), 1)\n        \n        (pointer_probs, input_idxs), dec_hidden_t = self.decoder(decoder_input,\n                inputs,\n                dec_init_state,\n                enc_h)\n\n        return pointer_probs, input_idxs\n        \n\nclass CriticNetwork(nn.Module):\n    """"""Useful as a baseline in REINFORCE updates""""""\n    def __init__(self,\n            embedding_dim,\n            hidden_dim,\n            n_process_block_iters,\n            tanh_exploration,\n            use_tanh,\n            use_cuda):\n        super(CriticNetwork, self).__init__()\n        \n        self.hidden_dim = hidden_dim\n        self.n_process_block_iters = n_process_block_iters\n\n        self.encoder = Encoder(\n                embedding_dim,\n                hidden_dim,\n                use_cuda)\n        \n        self.process_block = Attention(hidden_dim,\n                use_tanh=use_tanh, C=tanh_exploration, use_cuda=use_cuda)\n        self.sm = nn.Softmax()\n        self.decoder = nn.Sequential(\n                nn.Linear(hidden_dim, hidden_dim),\n                nn.ReLU(),\n                nn.Linear(hidden_dim, 1)\n        )\n\n    def forward(self, inputs):\n        """"""\n        Args:\n            inputs: [embedding_dim x batch_size x sourceL] of embedded inputs\n        """"""\n         \n        (encoder_hx, encoder_cx) = self.encoder.enc_init_state\n        encoder_hx = encoder_hx.unsqueeze(0).repeat(inputs.size(1), 1).unsqueeze(0)\n        encoder_cx = encoder_cx.unsqueeze(0).repeat(inputs.size(1), 1).unsqueeze(0)       \n        \n        # encoder forward pass\n        enc_outputs, (enc_h_t, enc_c_t) = self.encoder(inputs, (encoder_hx, encoder_cx))\n        \n        # grab the hidden state and process it via the process block \n        process_block_state = enc_h_t[-1]\n        for i in range(self.n_process_block_iters):\n            ref, logits = self.process_block(process_block_state, enc_outputs)\n            process_block_state = torch.bmm(ref, self.sm(logits).unsqueeze(2)).squeeze(2)\n        # produce the final scalar output\n        out = self.decoder(process_block_state)\n        return out\n\nclass NeuralCombOptRL(nn.Module):\n    """"""\n    This module contains the PointerNetwork (actor) and\n    CriticNetwork (critic). It requires\n    an application-specific reward function\n    """"""\n    def __init__(self, \n            input_dim,\n            embedding_dim,\n            hidden_dim,\n            max_decoding_len,\n            terminating_symbol,\n            n_glimpses,\n            n_process_block_iters,\n            tanh_exploration,\n            use_tanh,\n            beam_size,\n            objective_fn,\n            is_train,\n            use_cuda):\n        super(NeuralCombOptRL, self).__init__()\n        self.objective_fn = objective_fn\n        self.input_dim = input_dim\n        self.is_train = is_train\n        self.use_cuda = use_cuda\n\n        \n        self.actor_net = PointerNetwork(\n                embedding_dim,\n                hidden_dim,\n                max_decoding_len,\n                terminating_symbol,\n                n_glimpses,\n                tanh_exploration,\n                use_tanh,\n                beam_size,\n                use_cuda)\n        \n        #self.critic_net = CriticNetwork(\n        #        embedding_dim,\n        #        hidden_dim,\n        #        n_process_block_iters,\n        #        tanh_exploration,\n        #        False,\n        #        use_cuda)\n       \n        embedding_ = torch.FloatTensor(input_dim,\n            embedding_dim)\n        if self.use_cuda: \n            embedding_ = embedding_.cuda()\n        self.embedding = nn.Parameter(embedding_) \n        self.embedding.data.uniform_(-(1. / math.sqrt(embedding_dim)),\n            1. / math.sqrt(embedding_dim))\n\n    def forward(self, inputs):\n        """"""\n        Args:\n            inputs: [batch_size, input_dim, sourceL]\n        """"""\n        batch_size = inputs.size(0)\n        input_dim = inputs.size(1)\n        sourceL = inputs.size(2)\n\n        # repeat embeddings across batch_size\n        # result is [batch_size x input_dim x embedding_dim]\n        embedding = self.embedding.repeat(batch_size, 1, 1)  \n        embedded_inputs = []\n        # result is [batch_size, 1, input_dim, sourceL] \n        ips = inputs.unsqueeze(1)\n        \n        for i in range(sourceL):\n            # [batch_size x 1 x input_dim] * [batch_size x input_dim x embedding_dim]\n            # result is [batch_size, embedding_dim]\n            embedded_inputs.append(torch.bmm(\n                ips[:, :, :, i].float(),\n                embedding).squeeze(1))\n\n        # Result is [sourceL x batch_size x embedding_dim]\n        embedded_inputs = torch.cat(embedded_inputs).view(\n                sourceL,\n                batch_size,\n                embedding.size(2))\n\n        # query the actor net for the input indices \n        # making up the output, and the pointer attn \n        probs_, action_idxs = self.actor_net(embedded_inputs)\n       \n        # Select the actions (inputs pointed to \n        # by the pointer net) and the corresponding\n        # logits\n        # should be size [batch_size x \n        actions = []\n        # inputs is [batch_size, input_dim, sourceL]\n        inputs_ = inputs.transpose(1, 2)\n        # inputs_ is [batch_size, sourceL, input_dim]\n        for action_id in action_idxs:\n            actions.append(inputs_[[x for x in range(batch_size)], action_id.data, :])\n\n        if self.is_train:\n            # probs_ is a list of len sourceL of [batch_size x sourceL]\n            probs = []\n            for prob, action_id in zip(probs_, action_idxs):\n                probs.append(prob[[x for x in range(batch_size)], action_id.data])\n        else:\n            # return the list of len sourceL of [batch_size x sourceL]\n            probs = probs_\n\n        # get the critic value fn estimates for the baseline\n        # [batch_size]\n        #v = self.critic_net(embedded_inputs)\n    \n        # [batch_size]\n        R = self.objective_fn(actions, self.use_cuda)\n        \n        #return R, v, probs, actions, action_idxs\n        return R, probs, actions, action_idxs\n\n'"
plot_attention.py,1,"b'import matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport numpy as np\n\n\ndef plot_attention(in_seq, out_seq, attentions):\n    """""" From http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html""""""\n\n    # Set up figure with colorbar\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(attentions, cmap=\'bone\')\n    fig.colorbar(cax)\n\n    # Set up axes\n    ax.set_xticklabels([\' \'] + [str(x) for x in in_seq], rotation=90)\n    ax.set_yticklabels([\' \'] + [str(x) for x in out_seq])\n\n    # Show label at every tick\n    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n    plt.show()\n'"
sorting_task.py,23,"b'# Generate sorting data and store in .txt\n# Define the reward function \n\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.autograd import Variable\nfrom tqdm import trange, tqdm\nimport os\nimport sys\n\n\ndef reward(sample_solution, USE_CUDA=False):\n    """"""\n    The reward for the sorting task is defined as the\n    length of the longest sorted consecutive subsequence.\n\n    Input sequences must all be the same length.\n\n    Example: \n\n    input       | output\n    ====================\n    [1 4 3 5 2] | [5 1 2 3 4]\n    \n    The output gets a reward of 4/5, or 0.8\n\n    The range is [1/sourceL, 1]\n\n    Args:\n        sample_solution: list of len sourceL of [batch_size]\n        Tensors\n    Returns:\n        [batch_size] containing trajectory rewards\n    """"""\n    batch_size = sample_solution[0].size(0)\n    sourceL = len(sample_solution)\n\n    longest = Variable(torch.ones(batch_size, 1), requires_grad=False)\n    current = Variable(torch.ones(batch_size, 1), requires_grad=False)\n\n    if USE_CUDA:\n        longest = longest.cuda()\n        current = current.cuda()\n\n    for i in range(1, sourceL):\n        # compare solution[i-1] < solution[i] \n        res = torch.lt(sample_solution[i-1], sample_solution[i]) \n        # if res[i,j] == 1, increment length of current sorted subsequence\n        current += res.float()  \n        # else, reset current to 1\n        current[torch.eq(res, 0)] = 1\n        #current[torch.eq(res, 0)] -= 1\n        # if, for any, current > longest, update longest\n        mask = torch.gt(current, longest)\n        longest[mask] = current[mask]\n    return -torch.div(longest, sourceL)\n\ndef create_dataset(\n        train_size,\n        val_size,\n        #test_size,\n        data_dir,\n        data_len,\n        seed=None):\n\n    if seed is not None:\n        torch.manual_seed(seed)\n    \n    train_task = \'sorting-size-{}-len-{}-train.txt\'.format(train_size, data_len)\n    val_task = \'sorting-size-{}-len-{}-val.txt\'.format(val_size, data_len)\n    #test_task = \'sorting-size-{}-len-{}-test.txt\'.format(test_size, data_len)\n    \n    train_fname = os.path.join(data_dir, train_task)\n    val_fname = os.path.join(data_dir, val_task)\n\n    \n    if not os.path.isdir(data_dir):\n        os.mkdir(data_dir)\n    else:\n        if os.path.exists(train_fname) and os.path.exists(val_fname):\n            return train_fname, val_fname\n    \n    train_set = open(os.path.join(data_dir, train_task), \'w\')\n    val_set = open(os.path.join(data_dir, val_task), \'w\') \n    #test_set = open(os.path.join(data_dir, test_task), \'w\')\n    \n    def to_string(tensor):\n        """"""\n        Convert a a torch.LongTensor \n        of size data_len to a string \n        of integers separated by whitespace\n        and ending in a newline character\n        """"""\n        line = \'\'\n        for j in range(data_len-1):\n            line += \'{} \'.format(tensor[j])\n        line += str(tensor[-1]) + \'\\n\'\n        return line\n    \n    print(\'Creating training data set for {}...\'.format(train_task))\n    \n    # Generate a training set of size train_size\n    for i in trange(train_size):\n        x = torch.randperm(data_len)\n        train_set.write(to_string(x))\n\n    print(\'Creating validation data set for {}...\'.format(val_task))\n    \n    for i in trange(val_size):\n        x = torch.randperm(data_len)\n        val_set.write(to_string(x))\n\n#    print(\'Creating test data set for {}...\'.format(test_task))\n#\n#    for i in trange(test_size):\n#        x = torch.randperm(data_len)\n#        test_set.write(to_string(x))\n\n    train_set.close()\n    val_set.close()\n#    test_set.close()\n    return train_fname, val_fname\n\nclass SortingDataset(Dataset):\n\n    def __init__(self, dataset_fname):\n        super(SortingDataset, self).__init__()\n       \n        print(\'Loading training data into memory\')\n        self.data_set = []\n        with open(dataset_fname, \'r\') as dset:\n            lines = dset.readlines()\n            for next_line in tqdm(lines):\n                toks = next_line.split()\n                sample = torch.zeros(1, len(toks)).long()\n                for idx, tok in enumerate(toks):\n                    sample[0, idx] = int(tok)\n                self.data_set.append(sample)\n        \n        self.size = len(self.data_set)\n\n    def __len__(self):\n        return self.size\n\n    def __getitem__(self, idx):\n        return self.data_set[idx]\n\nif __name__ == \'__main__\':\n    if int(sys.argv[1]) == 0:\n        #sample = Variable(torch.Tensor([[3, 2, 1, 4, 5], [2, 3, 5, 1, 4]])) \n        sample = [Variable(torch.Tensor([3,2])), Variable(torch.Tensor([2,3])), Variable(torch.Tensor([1,5])),\n                Variable(torch.Tensor([4, 1])), Variable(torch.Tensor([5, 4]))]\n        answer = torch.Tensor([3/5., 3/5])\n\n        res = reward(sample)\n\n        print(\'Expected answer: {}, Actual answer: {}\'.format(answer, res.data))\n        """"""\n        sample = Variable(torch.Tensor([[1, 2, 3, 4, 5], [5, 4, 3, 2, 1]])) \n        answer = torch.Tensor([1., 1/5])\n\n        res = reward(sample)\n\n        print(\'Expected answer: {}, Actual answer: {}\'.format(answer, res.data))\n        \n        sample = Variable(torch.Tensor([[1, 2, 5, 4, 3], [4, 1, 2, 3, 5]])) \n        answer = torch.Tensor([3/5., 4/5])\n\n        res = reward(sample)\n\n        print(\'Expected answer: {}, Actual answer: {}\'.format(answer, res.data))\n        """"""\n    elif int(sys.argv[1]) == 1:\n        create_sorting_dataset(1000, 100, \'data\', 10, 123)\n    elif int(sys.argv[1]) == 2:\n\n        sorting_data = SortingDataset(\'data\', \'sorting-size-1000-len-10-train.txt\',\n            \'sorting-size-100-len-10-val.txt\')\n        \n        for i in range(len(sorting_data)):\n            print(sorting_data[i])\n'"
trainer.py,15,"b'#!/usr/bin/env python\n\nimport argparse\nimport os\nfrom tqdm import tqdm \n\nimport pprint as pp\nimport numpy as np\n\nimport torch\nprint(torch.__version__)\nimport torch.optim as optim\nimport torch.autograd as autograd\nfrom torch.optim import lr_scheduler\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom tensorboard_logger import configure, log_value\n\nfrom neural_combinatorial_rl import NeuralCombOptRL\nfrom plot_attention import plot_attention\n\n\ndef str2bool(v):\n      return v.lower() in (\'true\', \'1\')\n\nparser = argparse.ArgumentParser(description=""Neural Combinatorial Optimization with RL"")\n\n# Data\nparser.add_argument(\'--task\', default=\'sort_10\', help=""The task to solve, in the form {COP}_{size}, e.g., tsp_20"")\nparser.add_argument(\'--batch_size\', default=128, help=\'\')\nparser.add_argument(\'--train_size\', default=1000000, help=\'\')\nparser.add_argument(\'--val_size\', default=10000, help=\'\')\n# Network\nparser.add_argument(\'--embedding_dim\', default=128, help=\'Dimension of input embedding\')\nparser.add_argument(\'--hidden_dim\', default=128, help=\'Dimension of hidden layers in Enc/Dec\')\nparser.add_argument(\'--n_process_blocks\', default=3, help=\'Number of process block iters to run in the Critic network\')\nparser.add_argument(\'--n_glimpses\', default=2, help=\'No. of glimpses to use in the pointer network\')\nparser.add_argument(\'--use_tanh\', type=str2bool, default=True)\nparser.add_argument(\'--tanh_exploration\', default=10, help=\'Hyperparam controlling exploration in the pointer net by scaling the tanh in the softmax\')\nparser.add_argument(\'--dropout\', default=0., help=\'\')\nparser.add_argument(\'--terminating_symbol\', default=\'<0>\', help=\'\')\nparser.add_argument(\'--beam_size\', default=1, help=\'Beam width for beam search\')\n\n# Training\nparser.add_argument(\'--actor_net_lr\', default=1e-4, help=""Set the learning rate for the actor network"")\nparser.add_argument(\'--critic_net_lr\', default=1e-4, help=""Set the learning rate for the critic network"")\nparser.add_argument(\'--actor_lr_decay_step\', default=5000, help=\'\')\nparser.add_argument(\'--critic_lr_decay_step\', default=5000, help=\'\')\nparser.add_argument(\'--actor_lr_decay_rate\', default=0.96, help=\'\')\nparser.add_argument(\'--critic_lr_decay_rate\', default=0.96, help=\'\')\nparser.add_argument(\'--reward_scale\', default=2, type=float,  help=\'\')\nparser.add_argument(\'--is_train\', type=str2bool, default=True, help=\'\')\nparser.add_argument(\'--n_epochs\', default=1, help=\'\')\nparser.add_argument(\'--random_seed\', default=24601, help=\'\')\nparser.add_argument(\'--max_grad_norm\', default=2.0, help=\'Gradient clipping\')\nparser.add_argument(\'--use_cuda\', type=str2bool, default=True, help=\'\')\nparser.add_argument(\'--critic_beta\', type=float, default=0.9, help=\'Exp mvg average decay\')\n\n# Misc\nparser.add_argument(\'--log_step\', default=50, help=\'Log info every log_step steps\')\nparser.add_argument(\'--log_dir\', type=str, default=\'logs\')\nparser.add_argument(\'--run_name\', type=str, default=\'0\')\nparser.add_argument(\'--output_dir\', type=str, default=\'outputs\')\nparser.add_argument(\'--epoch_start\', type=int, default=0, help=\'Restart at epoch #\')\nparser.add_argument(\'--load_path\', type=str, default=\'\')\nparser.add_argument(\'--disable_tensorboard\', type=str2bool, default=False)\nparser.add_argument(\'--plot_attention\', type=str2bool, default=False)\nparser.add_argument(\'--disable_progress_bar\', type=str2bool, default=False)\n\nargs = vars(parser.parse_args())\n\n# Pretty print the run args\npp.pprint(args)\n\n# Set the random seed\ntorch.manual_seed(int(args[\'random_seed\']))\n\n# Optionally configure tensorboard\nif not args[\'disable_tensorboard\']:\n    configure(os.path.join(args[\'log_dir\'], args[\'task\'], args[\'run_name\']))\n\n# Task specific configuration - generate dataset if needed\ntask = args[\'task\'].split(\'_\')\nCOP = task[0]\nsize = int(task[1])\ndata_dir = \'data/\' + COP\n\nif COP == \'sort\':\n    import sorting_task\n    \n    input_dim = 1\n    reward_fn = sorting_task.reward\n    train_fname, val_fname = sorting_task.create_dataset(\n        int(args[\'train_size\']),\n        int(args[\'val_size\']),\n        data_dir,\n        data_len=size)\n    training_dataset = sorting_task.SortingDataset(train_fname)\n    val_dataset = sorting_task.SortingDataset(val_fname)\nelif COP == \'tsp\':\n    import tsp_task\n\n    input_dim = 2\n    reward_fn = tsp_task.reward\n    val_fname = tsp_task.create_dataset(\n        problem_size=str(size),\n        data_dir=data_dir)\n    training_dataset = tsp_task.TSPDataset(train=True, size=size,\n         num_samples=int(args[\'train_size\']))\n    val_dataset = tsp_task.TSPDataset(train=True, size=size,\n            num_samples=int(args[\'val_size\']))\nelse:\n    print(\'Currently unsupported task!\')\n    exit(1)\n\n# Load the model parameters from a saved state\nif args[\'load_path\'] != \'\':\n    print(\'  [*] Loading model from {}\'.format(args[\'load_path\']))\n\n    model = torch.load(\n        os.path.join(\n            os.getcwd(),\n            args[\'load_path\']\n        ))\n    model.actor_net.decoder.max_length = size\n    model.is_train = args[\'is_train\']\nelse:\n    # Instantiate the Neural Combinatorial Opt with RL module\n    model = NeuralCombOptRL(\n        input_dim,\n        int(args[\'embedding_dim\']),\n        int(args[\'hidden_dim\']),\n        size, # decoder len\n        args[\'terminating_symbol\'],\n        int(args[\'n_glimpses\']),\n        int(args[\'n_process_blocks\']), \n        float(args[\'tanh_exploration\']),\n        args[\'use_tanh\'],\n        int(args[\'beam_size\']),\n        reward_fn,\n        args[\'is_train\'],\n        args[\'use_cuda\'])\n\n\nsave_dir = os.path.join(os.getcwd(),\n           args[\'output_dir\'],\n           args[\'task\'],\n           args[\'run_name\'])    \n\ntry:\n    os.makedirs(save_dir)\nexcept:\n    pass\n\n#critic_mse = torch.nn.MSELoss()\n#critic_optim = optim.Adam(model.critic_net.parameters(), lr=float(args[\'critic_net_lr\']))\nactor_optim = optim.Adam(model.actor_net.parameters(), lr=float(args[\'actor_net_lr\']))\n\nactor_scheduler = lr_scheduler.MultiStepLR(actor_optim,\n        range(int(args[\'actor_lr_decay_step\']), int(args[\'actor_lr_decay_step\']) * 1000,\n            int(args[\'actor_lr_decay_step\'])), gamma=float(args[\'actor_lr_decay_rate\']))\n\n#critic_scheduler = lr_scheduler.MultiStepLR(critic_optim,\n#        range(int(args[\'critic_lr_decay_step\']), int(args[\'critic_lr_decay_step\']) * 1000,\n#            int(args[\'critic_lr_decay_step\'])), gamma=float(args[\'critic_lr_decay_rate\']))\n\ntraining_dataloader = DataLoader(training_dataset, batch_size=int(args[\'batch_size\']),\n    shuffle=True, num_workers=4)\n\nvalidation_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True, num_workers=1)\n\ncritic_exp_mvg_avg = torch.zeros(1)\nbeta = args[\'critic_beta\']\n\nif args[\'use_cuda\']:\n    model = model.cuda()\n    #critic_mse = critic_mse.cuda()\n    critic_exp_mvg_avg = critic_exp_mvg_avg.cuda()\n\nstep = 0\nval_step = 0\n\nif not args[\'is_train\']:\n    args[\'n_epochs\'] = \'1\'\n \n\nepoch = int(args[\'epoch_start\'])\nfor i in range(epoch, epoch + int(args[\'n_epochs\'])):\n    \n    if args[\'is_train\']:\n        # put in train mode!\n        model.train()\n\n        # sample_batch is [batch_size x input_dim x sourceL]\n        for batch_id, sample_batch in enumerate(tqdm(training_dataloader,\n                disable=args[\'disable_progress_bar\'])):\n\n\n            bat = Variable(sample_batch)\n            if args[\'use_cuda\']:\n                bat = bat.cuda()\n\n            R, probs, actions, actions_idxs = model(bat)\n        \n            if batch_id == 0:\n                critic_exp_mvg_avg = R.mean()\n            else:\n                critic_exp_mvg_avg = (critic_exp_mvg_avg * beta) + ((1. - beta) * R.mean())\n\n            advantage = R - critic_exp_mvg_avg\n            \n            logprobs = 0\n            nll = 0\n            for prob in probs: \n                # compute the sum of the log probs\n                # for each tour in the batch\n                logprob = torch.log(prob)\n                nll += -logprob\n                logprobs += logprob\n           \n            # guard against nan\n            nll[(nll != nll).detach()] = 0.\n            # clamp any -inf\'s to 0 to throw away this tour\n            logprobs[(logprobs < -1000).detach()] = 0.\n\n            # multiply each time step by the advanrate\n            reinforce = advantage * logprobs\n            actor_loss = reinforce.mean()\n            \n            actor_optim.zero_grad()\n           \n            actor_loss.backward()\n\n            # clip gradient norms\n            torch.nn.utils.clip_grad_norm(model.actor_net.parameters(),\n                    float(args[\'max_grad_norm\']), norm_type=2)\n\n            actor_optim.step()\n            actor_scheduler.step()\n\n            critic_exp_mvg_avg = critic_exp_mvg_avg.detach()\n\n            #critic_scheduler.step()\n\n            #R = R.detach()\n            #critic_loss = critic_mse(v.squeeze(1), R)\n            #critic_optim.zero_grad()\n            #critic_loss.backward()\n            \n            #torch.nn.utils.clip_grad_norm(model.critic_net.parameters(),\n            #        float(args[\'max_grad_norm\']), norm_type=2)\n\n            #critic_optim.step()\n            \n            step += 1\n            \n            if not args[\'disable_tensorboard\']:\n                log_value(\'avg_reward\', R.mean().data[0], step)\n                log_value(\'actor_loss\', actor_loss.data[0], step)\n                #log_value(\'critic_loss\', critic_loss.data[0], step)\n                log_value(\'critic_exp_mvg_avg\', critic_exp_mvg_avg.data[0], step)\n                log_value(\'nll\', nll.mean().data[0], step)\n\n            if step % int(args[\'log_step\']) == 0:\n                print(\'epoch: {}, train_batch_id: {}, avg_reward: {}\'.format(\n                    i, batch_id, R.mean().data[0]))\n                example_output = []\n                example_input = []\n                for idx, action in enumerate(actions):\n                    if task[0] == \'tsp\':\n                        example_output.append(actions_idxs[idx][0].data[0])\n                    else:\n                        example_output.append(action[0].data[0])  # <-- ?? \n                    example_input.append(sample_batch[0, :, idx][0])\n                #print(\'Example train input: {}\'.format(example_input))\n                print(\'Example train output: {}\'.format(example_output))\n\n    # Use beam search decoding for validation\n    model.actor_net.decoder.decode_type = ""beam_search""\n    \n    print(\'\\n~Validating~\\n\')\n\n    example_input = []\n    example_output = []\n    avg_reward = []\n\n    # put in test mode!\n    model.eval()\n\n    for batch_id, val_batch in enumerate(tqdm(validation_dataloader,\n            disable=args[\'disable_progress_bar\'])):\n        bat = Variable(val_batch)\n\n        if args[\'use_cuda\']:\n            bat = bat.cuda()\n\n        R, probs, actions, action_idxs = model(bat)\n        \n        avg_reward.append(R[0].data[0])\n        val_step += 1.\n\n        if not args[\'disable_tensorboard\']:\n            log_value(\'val_avg_reward\', R[0].data[0], int(val_step))\n\n        if val_step % int(args[\'log_step\']) == 0:\n            example_output = []\n            example_input = []\n            for idx, action in enumerate(actions):\n                if task[0] == \'tsp\':\n                    example_output.append(action_idxs[idx][0].data[0])\n                else:\n                    example_output.append(action[0].data[0])\n                example_input.append(bat[0, :, idx].data[0])\n            print(\'Step: {}\'.format(batch_id))\n            #print(\'Example test input: {}\'.format(example_input))\n            print(\'Example test output: {}\'.format(example_output))\n            print(\'Example test reward: {}\'.format(R[0].data[0]))\n    \n        \n            if args[\'plot_attention\']:\n                probs = torch.cat(probs, 0)\n                plot_attention(example_input,\n                        example_output, probs.data.cpu().numpy())\n    print(\'Validation overall avg_reward: {}\'.format(np.mean(avg_reward)))\n    print(\'Validation overall reward var: {}\'.format(np.var(avg_reward)))\n     \n    if args[\'is_train\']:\n        model.actor_net.decoder.decode_type = ""stochastic""\n         \n        print(\'Saving model...\')\n     \n        torch.save(model, os.path.join(save_dir, \'epoch-{}.pt\'.format(i)))\n\n        # If the task requires generating new data after each epoch, do that here!\n        if COP == \'tsp\':\n            training_dataset = tsp_task.TSPDataset(train=True, size=size,\n                num_samples=int(args[\'train_size\']))\n            training_dataloader = DataLoader(training_dataset, batch_size=int(args[\'batch_size\']),\n                shuffle=True, num_workers=1)\n        if COP == \'sort\':\n            train_fname, _ = sorting_task.create_dataset(\n                int(args[\'train_size\']),\n                int(args[\'val_size\']),\n                data_dir,\n                data_len=size)\n            training_dataset = sorting_task.SortingDataset(train_fname)\n            training_dataloader = DataLoader(training_dataset, batch_size=int(args[\'batch_size\']),\n                    shuffle=True, num_workers=1)\n'"
tsp_task.py,10,"b'# code based in part on\n# http://stackoverflow.com/questions/25010369/wget-curl-large-file-from-google-drive/39225039#39225039\n# and from\n# https://github.com/devsisters/neural-combinatorial-rl-tensorflow/blob/master/data_loader.py\nimport requests\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset\nfrom torch.autograd import Variable\nimport torch\nimport os\nimport numpy as np\nimport re\nimport zipfile\nimport itertools\nfrom collections import namedtuple\n\n\n#######################################\n# Reward Fn\n#######################################\ndef reward(sample_solution, USE_CUDA=False):\n    """"""\n    Args:\n        List of length sourceL of [batch_size] Tensors\n    Returns:\n        Tensor of shape [batch_size] containins rewards\n    """"""\n    batch_size = sample_solution[0].size(0)\n    n = len(sample_solution)\n    tour_len = Variable(torch.zeros([batch_size]))\n    \n    if USE_CUDA:\n        tour_len = tour_len.cuda()\n\n    for i in range(n-1):\n        tour_len += torch.norm(sample_solution[i] - sample_solution[i+1], dim=1)\n    \n    tour_len += torch.norm(sample_solution[n-1] - sample_solution[0], dim=1)\n\n    # For TSP_20 - map to a number between 0 and 1\n    # min_len = 3.5\n    # max_len = 10.\n    # TODO: generalize this for any TSP size\n    #tour_len = -0.1538*tour_len + 1.538 \n    #tour_len[tour_len < 0.] = 0.\n    return tour_len\n\n\n#######################################\n# Functions for downloading dataset\n#######################################\nTSP = namedtuple(\'TSP\', [\'x\', \'y\', \'name\'])\n\nGOOGLE_DRIVE_IDS = {\n    \'tsp5_train.zip\': \'0B2fg8yPGn2TCSW1pNTJMXzFPYTg\',\n    \'tsp10_train.zip\': \'0B2fg8yPGn2TCbHowM0hfOTJCNkU\',\n    \'tsp5-20_train.zip\': \'0B2fg8yPGn2TCTWNxX21jTDBGeXc\',\n    \'tsp50_train.zip\': \'0B2fg8yPGn2TCaVQxSl9ab29QajA\',\n    \'tsp20_test.txt\': \'0B2fg8yPGn2TCdF9TUU5DZVNCNjQ\',\n    \'tsp40_test.txt\': \'0B2fg8yPGn2TCcjFrYk85SGFVNlU\',\n    \'tsp50_test.txt.zip\': \'0B2fg8yPGn2TCUVlCQmQtelpZTTQ\',\n}\n\ndef download_file_from_google_drive(id, destination):\n    URL = ""https://docs.google.com/uc?export=download""\n\n    session = requests.Session()\n\n    response = session.get(URL, params = { \'id\' : id }, stream = True)\n    token = get_confirm_token(response)\n\n    if token:\n        params = { \'id\' : id, \'confirm\' : token }\n        response = session.get(URL, params = params, stream = True)\n\n    save_response_content(response, destination)  \n    return True\n\ndef get_confirm_token(response):\n    for key, value in response.cookies.items():\n        if key.startswith(\'download_warning\'):\n            return value\n    return None\n\ndef save_response_content(response, destination):\n    CHUNK_SIZE = 32768\n\n    with open(destination, ""wb"") as f:\n        for chunk in tqdm(response.iter_content(CHUNK_SIZE)):\n            if chunk: # filter out keep-alive new chunks\n                 f.write(chunk)\n\ndef download_google_drive_file(data_dir, task, min_length, max_length):\n    paths = {}\n    for mode in [\'train\', \'test\']:\n        candidates = []\n        candidates.append(\n            \'{}{}_{}\'.format(task, max_length, mode))\n        candidates.append(\n            \'{}{}-{}_{}\'.format(task, min_length, max_length, mode))\n\n        for key in candidates:\n            print(key)\n            for search_key in GOOGLE_DRIVE_IDS.keys():\n                if search_key.startswith(key):\n                    path = os.path.join(data_dir, search_key)\n                    print(""Download dataset of the paper to {}"".format(path))\n\n                    if not os.path.exists(path):\n                        download_file_from_google_drive(GOOGLE_DRIVE_IDS[search_key], path)\n                    if path.endswith(\'zip\'):\n                        with zipfile.ZipFile(path, \'r\') as z:\n                            z.extractall(data_dir)\n                    paths[mode] = path\n\n    return paths\n\ndef read_paper_dataset(paths, max_length):\n    x, y = [], []\n    for path in paths:\n        print(""Read dataset {} which is used in the paper.."".format(path))\n        length = max(re.findall(\'\\d+\', path))\n        with open(path) as f:\n            for l in tqdm(f):\n                inputs, outputs = l.split(\' output \')\n                x.append(np.array(inputs.split(), dtype=np.float32).reshape([-1, 2]))\n                y.append(np.array(outputs.split(), dtype=np.int32)[:-1]) # skip the last one\n\n    return x, y\n\ndef maybe_generate_and_save(self, except_list=[]):\n    data = {}\n\n    for name, num in self.data_num.items():\n        if name in except_list:\n            print(""Skip creating {} because of given except_list {}"".format(name, except_list))\n            continue\n        path = self.get_path(name)\n\n        print(""Skip creating {} for [{}]"".format(path, self.task))\n        tmp = np.load(path)\n        self.data[name] = TSP(x=tmp[\'x\'], y=tmp[\'y\'], name=name)\n\ndef get_path(self, name):\n    return os.path.join(\n        self.data_dir, ""{}_{}={}.npz"".format(\n            self.task_name, name, self.data_num[name]))\n\ndef read_zip_and_update_data(self, path, name):\n    if path.endswith(\'zip\'):\n        filenames = zipfile.ZipFile(path).namelist()\n        paths = [os.path.join(self.data_dir, filename) for filename in filenames]\n    else:\n        paths = [path]\n\n    x_list, y_list = read_paper_dataset(paths, self.max_length)\n\n    x = np.zeros([len(x_list), self.max_length, 2], dtype=np.float32)\n    y = np.zeros([len(y_list), self.max_length], dtype=np.int32)\n\n    for idx, (nodes, res) in enumerate(tqdm(zip(x_list, y_list))):\n        x[idx,:len(nodes)] = nodes\n        y[idx,:len(res)] = res\n\n    if self.data is None:\n        self.data = {}\n\n    print(""Update [{}] data with {} used in the paper"".format(name, path))\n    self.data[name] = TSP(x=x, y=y, name=name)\n\n\ndef create_dataset(\n    problem_size, \n    data_dir):\n\n    def find_or_return_empty(data_dir, problem_size):\n        #train_fname1 = os.path.join(data_dir, \'tsp{}.txt\'.format(problem_size))\n        val_fname1 = os.path.join(data_dir, \'tsp{}_test.txt\'.format(problem_size))\n        #train_fname2 = os.path.join(data_dir, \'tsp-{}.txt\'.format(problem_size))\n        val_fname2 = os.path.join(data_dir, \'tsp-{}_test.txt\'.format(problem_size))\n        \n        if not os.path.isdir(data_dir):\n            os.mkdir(data_dir)\n        else:\n    #         if os.path.exists(train_fname1) and os.path.exists(val_fname1):\n    #             return train_fname1, val_fname1\n    #         if os.path.exists(train_fname2) and os.path.exists(val_fname2):\n    #             return train_fname2, val_fname2\n    #     return None, None\n\n    # train, val = find_or_return_empty(data_dir, problem_size)\n    # if train is None and val is None:\n    #     download_google_drive_file(data_dir,\n    #         \'tsp\', \'\', problem_size) \n    #     train, val = find_or_return_empty(data_dir, problem_size)\n\n    # return train, val\n            if os.path.exists(val_fname1):\n                return val_fname1\n            if os.path.exists(val_fname2):\n                return val_fname2\n        return None\n\n    val = find_or_return_empty(data_dir, problem_size)\n    if val is None:\n        download_google_drive_file(data_dir, \'tsp\', \'\', problem_size)\n        val = find_or_return_empty(data_dir, problem_size)\n\n    return val\n\n\n#######################################\n# Dataset\n#######################################\nclass TSPDataset(Dataset):\n    \n    def __init__(self, dataset_fname=None, train=False, size=50, num_samples=1000000, random_seed=1111):\n        super(TSPDataset, self).__init__()\n        #start = torch.FloatTensor([[-1], [-1]]) \n        \n        torch.manual_seed(random_seed)\n\n        self.data_set = []\n        if not train:\n            with open(dataset_fname, \'r\') as dset:\n                for l in tqdm(dset):\n                    inputs, outputs = l.split(\' output \')\n                    sample = torch.zeros(1, )\n                    x = np.array(inputs.split(), dtype=np.float32).reshape([-1, 2]).T\n                    #y.append(np.array(outputs.split(), dtype=np.int32)[:-1]) # skip the last one\n                    self.data_set.append(x)\n        else:\n            # randomly sample points uniformly from [0, 1]\n            for l in tqdm(range(num_samples)):\n                x = torch.FloatTensor(2, size).uniform_(0, 1)\n                #x = torch.cat([start, x], 1)\n                self.data_set.append(x)\n\n        self.size = len(self.data_set)\n\n    def __len__(self):\n        return self.size\n\n    def __getitem__(self, idx):\n        return self.data_set[idx]\n    \nif __name__ == \'__main__\':\n    paths = download_google_drive_file(\'data/tsp\', \'tsp\', \'\', \'50\')\n'"
scripts/hyperparam_search.py,0,"b'import subprocess\nimport numpy as np\nimport sys\nfrom math import floor, log10\n\n\nif __name__ == \'__main__\':\n    exp_i = sys.argv[1]\n    #rand_seed = int(sys.argv[2])\n\n    #np.random.seed(rand_seed)\n  \n\n    exps = [4]\n    #num = np.arange(1, 9)\n    num = [2]\n\n    num_trials = 10\n    \n    seeds = [123, 343]\n\n    for i in range(num_trials):\n    #for rs in seeds:\n        """"""\n        for exp in exps:\n            for n in num:\n            \n                lr = n * (1./(10 ** exp))\n                subprocess.call([""./tune_hyper.sh"", str(lr), str(rs), exp_i])\n        """"""\n        lr = np.random.normal(2e-4, 1e-5)\n        subprocess.call([""./tune_hyper.sh"", str(lr), \'4911\', exp_i])\n'"
scripts/plot_reward.py,0,"b""import matplotlib.pyplot as plt\nimport pandas as pd\nimport sys\n\nif __name__ == '__main__':\n    \n    reward_csv = sys.argv[1]\n    data = int(sys.argv[2])\n\n    df = pd.DataFrame.from_csv(reward_csv)\n\n    if data == 0:    \n        plt.figure()\n        plt.plot(df['Step'], df['Value'])\n        plt.title('TSP 50, Average Tour Length (Training)')\n        plt.xlabel('Step')\n        plt.ylabel('Average Tour Length')\n        plt.show()\n    else:\n        # average every 1000\n        vals = []\n        i = 1\n        s = 0\n        for index, row in df.iterrows():\n            if i % 100 == 0:\n                vals.append(s /100.)\n                s = 0\n            s += row['Value']\n            i += 1\n        plt.figure()\n        plt.plot(vals)\n        plt.plot(xrange(10), [5.95 for _ in range(10)])\n        plt.title('TSP 50, Average Tour Length (Validation)')\n        plt.xlabel('Step')\n        plt.ylabel('Average Tour Length')\n        plt.show()\n"""
