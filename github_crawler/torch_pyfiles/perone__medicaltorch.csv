file_path,api_count,code
setup.py,1,"b""from setuptools import setup, find_packages\nfrom codecs import open\nfrom os import path\n\nimport medicaltorch as mt\n\nhere = path.abspath(path.dirname(__file__))\n\nwith open('requirements.txt') as f:\n    requirements = f.read().splitlines()\n\nwith open('test-requirements.txt') as f:\n    test_requirements = f.read().splitlines()\n\nsetup(\n    name='medicaltorch',\n    version=mt.__version__,\n    description='An open-source pytorch medical framework.',\n    url='https://github.com/perone/medicaltorch',\n    author='Christian S. Perone',\n    author_email='christian.perone@gmail.com',\n    classifiers=[\n        'Development Status :: 3 - Alpha',\n        'Intended Audience :: Developers',\n        'Programming Language :: Python :: 3',\n    ],\n    packages=find_packages(exclude=['contrib', 'docs', 'tests']),\n    install_requires=requirements,\n    tests_require=test_requirements,\n    #entry_points={\n        #'console_scripts': [\n        #    'cmdname=medicaltorch.mod:function',\n        #],\n    #},\n)\n"""
examples/gmchallenge_unet.py,4,"b'from collections import defaultdict\nimport time\nimport os\n\nimport numpy as np\n\nfrom tqdm import tqdm\n\nfrom tensorboardX import SummaryWriter\n\nfrom medicaltorch import datasets as mt_datasets\nfrom medicaltorch import models as mt_models\nfrom medicaltorch import transforms as mt_transforms\nfrom medicaltorch import losses as mt_losses\nfrom medicaltorch import metrics as mt_metrics\nfrom medicaltorch import filters as mt_filters\n\nimport torch\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom torch import autograd, optim\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\n\nimport torchvision.utils as vutils\n\ncudnn.benchmark = True\n\n\ndef threshold_predictions(predictions, thr=0.999):\n    thresholded_preds = predictions[:]\n    low_values_indices = thresholded_preds < thr\n    thresholded_preds[low_values_indices] = 0\n    low_values_indices = thresholded_preds >= thr\n    thresholded_preds[low_values_indices] = 1\n    return thresholded_preds\n\n\ndef run_main():\n    train_transform = transforms.Compose([\n        mt_transforms.CenterCrop2D((200, 200)),\n        mt_transforms.ElasticTransform(alpha_range=(28.0, 30.0),\n                                       sigma_range=(3.5, 4.0),\n                                       p=0.3),\n        mt_transforms.RandomAffine(degrees=4.6,\n                                   scale=(0.98, 1.02),\n                                   translate=(0.03, 0.03)),\n        mt_transforms.RandomTensorChannelShift((-0.10, 0.10)),\n        mt_transforms.ToTensor(),\n        mt_transforms.NormalizeInstance(),\n    ])\n\n    val_transform = transforms.Compose([\n        mt_transforms.CenterCrop2D((200, 200)),\n        mt_transforms.ToTensor(),\n        mt_transforms.NormalizeInstance(),\n    ])\n\n    # Here we assume that the SC GM Challenge data is inside the folder\n    # ""../data"" and it was previously resampled.\n    gmdataset_train = mt_datasets.SCGMChallenge2DTrain(root_dir=""../data"",\n                                                       subj_ids=range(1, 9),\n                                                       transform=train_transform,\n                                                       slice_filter_fn=mt_filters.SliceFilter())\n\n    # Here we assume that the SC GM Challenge data is inside the folder\n    # ""../data"" and it was previously resampled.\n    gmdataset_val = mt_datasets.SCGMChallenge2DTrain(root_dir=""../data"",\n                                                     subj_ids=range(9, 11),\n                                                     transform=val_transform)\n\n    train_loader = DataLoader(gmdataset_train, batch_size=16,\n                              shuffle=True, pin_memory=True,\n                              collate_fn=mt_datasets.mt_collate,\n                              num_workers=1)\n\n    val_loader = DataLoader(gmdataset_val, batch_size=16,\n                            shuffle=True, pin_memory=True,\n                            collate_fn=mt_datasets.mt_collate,\n                            num_workers=1)\n\n    model = mt_models.Unet(drop_rate=0.4, bn_momentum=0.1)\n    model.cuda()\n\n    num_epochs = 200\n    initial_lr = 0.001\n\n    optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)\n\n    writer = SummaryWriter(log_dir=""log_exp"")\n    for epoch in tqdm(range(1, num_epochs+1)):\n        start_time = time.time()\n\n        scheduler.step()\n\n        lr = scheduler.get_lr()[0]\n        writer.add_scalar(\'learning_rate\', lr, epoch)\n\n        model.train()\n        train_loss_total = 0.0\n        num_steps = 0\n        for i, batch in enumerate(train_loader):\n            input_samples, gt_samples = batch[""input""], batch[""gt""]\n\n            var_input = input_samples.cuda()\n            var_gt = gt_samples.cuda(non_blocking=True)\n\n            preds = model(var_input)\n\n            loss = mt_losses.dice_loss(preds, var_gt)\n            train_loss_total += loss.item()\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            num_steps += 1\n\n            if epoch % 5 == 0:\n                grid_img = vutils.make_grid(input_samples,\n                                            normalize=True,\n                                            scale_each=True)\n                writer.add_image(\'Input\', grid_img, epoch)\n\n                grid_img = vutils.make_grid(preds.data.cpu(),\n                                            normalize=True,\n                                            scale_each=True)\n                writer.add_image(\'Predictions\', grid_img, epoch)\n\n                grid_img = vutils.make_grid(gt_samples,\n                                            normalize=True,\n                                            scale_each=True)\n                writer.add_image(\'Ground Truth\', grid_img, epoch)\n\n        train_loss_total_avg = train_loss_total / num_steps\n\n        model.eval()\n        val_loss_total = 0.0\n        num_steps = 0\n\n        metric_fns = [mt_metrics.dice_score,\n                      mt_metrics.hausdorff_score,\n                      mt_metrics.precision_score,\n                      mt_metrics.recall_score,\n                      mt_metrics.specificity_score,\n                      mt_metrics.intersection_over_union,\n                      mt_metrics.accuracy_score]\n\n        metric_mgr = mt_metrics.MetricManager(metric_fns)\n\n        for i, batch in enumerate(val_loader):\n            input_samples, gt_samples = batch[""input""], batch[""gt""]\n\n            with torch.no_grad():\n                var_input = input_samples.cuda()\n                var_gt = gt_samples.cuda(async=True)\n\n                preds = model(var_input)\n                loss = mt_losses.dice_loss(preds, var_gt)\n                val_loss_total += loss.item()\n\n            # Metrics computation\n            gt_npy = gt_samples.numpy().astype(np.uint8)\n            gt_npy = gt_npy.squeeze(axis=1)\n\n            preds = preds.data.cpu().numpy()\n            preds = threshold_predictions(preds)\n            preds = preds.astype(np.uint8)\n            preds = preds.squeeze(axis=1)\n\n            metric_mgr(preds, gt_npy)\n\n            num_steps += 1\n\n        metrics_dict = metric_mgr.get_results()\n        metric_mgr.reset()\n\n        writer.add_scalars(\'metrics\', metrics_dict, epoch)\n\n        val_loss_total_avg = val_loss_total / num_steps\n\n        writer.add_scalars(\'losses\', {\n                                \'val_loss\': val_loss_total_avg,\n                                \'train_loss\': train_loss_total_avg\n                            }, epoch)\n\n        end_time = time.time()\n        total_time = end_time - start_time\n        tqdm.write(""Epoch {} took {:.2f} seconds."".format(epoch, total_time))\n\n        writer.add_scalars(\'losses\', {\n                                \'train_loss\': train_loss_total_avg\n                            }, epoch)\n\n\nif __name__ == \'__main__\':\n    run_main()\n'"
medicaltorch/__init__.py,0,"b'__version__ = ""0.2""\n__author__ = ""Christian S. Perone""\n'"
medicaltorch/datasets.py,15,"b'import os\nimport re\nimport collections\n\nfrom medicaltorch import transforms as mt_transforms\n\nfrom tqdm import tqdm\nimport numpy as np\nimport nibabel as nib\n\nfrom torch.utils.data import Dataset\nimport torch\nfrom torch._six import string_classes, int_classes\n\nfrom PIL import Image\n\n\n__numpy_type_map = {\n    \'float64\': torch.DoubleTensor,\n    \'float32\': torch.FloatTensor,\n    \'float16\': torch.HalfTensor,\n    \'int64\': torch.LongTensor,\n    \'int32\': torch.IntTensor,\n    \'int16\': torch.ShortTensor,\n    \'int8\': torch.CharTensor,\n    \'uint8\': torch.ByteTensor,\n}\n\n\nclass SampleMetadata(object):\n    def __init__(self, d=None):\n        self.metadata = {} or d\n\n    def __setitem__(self, key, value):\n        self.metadata[key] = value\n\n    def __getitem__(self, key):\n        return self.metadata[key]\n\n    def __contains__(self, key):\n        return key in self.metadata\n\n    def keys(self):\n        return self.metadata.keys()\n\n\nclass BatchSplit(object):\n    def __init__(self, batch):\n        self.batch = batch\n\n    def __iter__(self):\n        batch_len = len(self.batch[""input""])\n        for i in range(batch_len):\n            single_sample = {k: v[i] for k, v in self.batch.items()}\n            single_sample[\'index\'] = i\n            yield single_sample\n        raise StopIteration\n\n\nclass SegmentationPair2D(object):\n    """"""This class is used to build 2D segmentation datasets. It represents\n    a pair of of two data volumes (the input data and the ground truth data).\n\n    :param input_filename: the input filename (supported by nibabel).\n    :param gt_filename: the ground-truth filename.\n    :param cache: if the data should be cached in memory or not.\n    :param canonical: canonical reordering of the volume axes.\n    """"""\n    def __init__(self, input_filename, gt_filename, cache=True,\n                 canonical=False):\n        self.input_filename = input_filename\n        self.gt_filename = gt_filename\n        self.canonical = canonical\n        self.cache = cache\n\n        self.input_handle = nib.load(self.input_filename)\n\n        # Unlabeled data (inference time)\n        if self.gt_filename is None:\n            self.gt_handle = None\n        else:\n            self.gt_handle = nib.load(self.gt_filename)\n\n        if len(self.input_handle.shape) > 3:\n            raise RuntimeError(""4-dimensional volumes not supported."")\n\n        # Sanity check for dimensions, should be the same\n        input_shape, gt_shape = self.get_pair_shapes()\n\n        if self.gt_handle is not None:\n            if not np.allclose(input_shape, gt_shape):\n                raise RuntimeError(\'Input and ground truth with different dimensions.\')\n\n        if self.canonical:\n            self.input_handle = nib.as_closest_canonical(self.input_handle)\n\n            # Unlabeled data\n            if self.gt_handle is not None:\n                self.gt_handle = nib.as_closest_canonical(self.gt_handle)\n\n    def get_pair_shapes(self):\n        """"""Return the tuple (input, ground truth) representing both the input\n        and ground truth shapes.""""""\n        input_shape = self.input_handle.header.get_data_shape()\n\n        # Handle unlabeled data\n        if self.gt_handle is None:\n            gt_shape = None\n        else:\n            gt_shape = self.gt_handle.header.get_data_shape()\n\n        return input_shape, gt_shape\n\n    def get_pair_data(self):\n        """"""Return the tuble (input, ground truth) with the data content in\n        numpy array.""""""\n        cache_mode = \'fill\' if self.cache else \'unchanged\'\n        input_data = self.input_handle.get_fdata(cache_mode, dtype=np.float32)\n\n        # Handle unlabeled data\n        if self.gt_handle is None:\n            gt_data = None\n        else:\n            gt_data = self.gt_handle.get_fdata(cache_mode, dtype=np.float32)\n\n        return input_data, gt_data\n\n    def get_pair_slice(self, slice_index, slice_axis=2):\n        """"""Return the specified slice from (input, ground truth).\n\n        :param slice_index: the slice number.\n        :param slice_axis: axis to make the slicing.\n        """"""\n        if self.cache:\n            input_dataobj, gt_dataobj = self.get_pair_data()\n        else:\n            # use dataobj to avoid caching\n            input_dataobj = self.input_handle.dataobj\n\n            if self.gt_handle is None:\n                gt_dataobj = None\n            else:\n                gt_dataobj = self.gt_handle.dataobj\n\n        if slice_axis not in [0, 1, 2]:\n            raise RuntimeError(""Invalid axis, must be between 0 and 2."")\n\n        if slice_axis == 2:\n            input_slice = np.asarray(input_dataobj[..., slice_index],\n                                     dtype=np.float32)\n        elif slice_axis == 1:\n            input_slice = np.asarray(input_dataobj[:, slice_index, ...],\n                                     dtype=np.float32)\n        elif slice_axis == 0:\n            input_slice = np.asarray(input_dataobj[slice_index, ...],\n                                     dtype=np.float32)\n\n        # Handle the case for unlabeled data\n        gt_meta_dict = None\n        if self.gt_handle is None:\n            gt_slice = None\n        else:\n            if slice_axis == 2:\n                gt_slice = np.asarray(gt_dataobj[..., slice_index],\n                                      dtype=np.float32)\n            elif slice_axis == 1:\n                gt_slice = np.asarray(gt_dataobj[:, slice_index, ...],\n                                      dtype=np.float32)\n            elif slice_axis == 0:\n                gt_slice = np.asarray(gt_dataobj[slice_index, ...],\n                                      dtype=np.float32)\n\n            gt_meta_dict = SampleMetadata({\n                ""zooms"": self.gt_handle.header.get_zooms()[:2],\n                ""data_shape"": self.gt_handle.header.get_data_shape()[:2],\n            })\n\n        input_meta_dict = SampleMetadata({\n            ""zooms"": self.input_handle.header.get_zooms()[:2],\n            ""data_shape"": self.input_handle.header.get_data_shape()[:2],\n        })\n\n        dreturn = {\n            ""input"": input_slice,\n            ""gt"": gt_slice,\n            ""input_metadata"": input_meta_dict,\n            ""gt_metadata"": gt_meta_dict,\n        }\n\n        return dreturn\n\n\nclass MRI2DSegmentationDataset(Dataset):\n    """"""This is a generic class for 2D (slice-wise) segmentation datasets.\n\n    :param filename_pairs: a list of tuples in the format (input filename,\n                           ground truth filename).\n    :param slice_axis: axis to make the slicing (default axial).\n    :param cache: if the data should be cached in memory or not.\n    :param transform: transformations to apply.\n    """"""\n    def __init__(self, filename_pairs, slice_axis=2, cache=True,\n                 transform=None, slice_filter_fn=None, canonical=False):\n        self.filename_pairs = filename_pairs\n        self.handlers = []\n        self.indexes = []\n        self.transform = transform\n        self.cache = cache\n        self.slice_axis = slice_axis\n        self.slice_filter_fn = slice_filter_fn\n        self.canonical = canonical\n\n        self._load_filenames()\n        self._prepare_indexes()\n\n    def _load_filenames(self):\n        for input_filename, gt_filename in self.filename_pairs:\n            segpair = SegmentationPair2D(input_filename, gt_filename,\n                                         self.cache, self.canonical)\n            self.handlers.append(segpair)\n\n    def _prepare_indexes(self):\n        for segpair in self.handlers:\n            input_data_shape, _ = segpair.get_pair_shapes()\n            for segpair_slice in range(input_data_shape[2]):\n\n                # Check if slice pair should be used or not\n                if self.slice_filter_fn:\n                    slice_pair = segpair.get_pair_slice(segpair_slice,\n                                                        self.slice_axis)\n\n                    filter_fn_ret = self.slice_filter_fn(slice_pair)\n                    if not filter_fn_ret:\n                        continue\n\n                item = (segpair, segpair_slice)\n                self.indexes.append(item)\n\n    def set_transform(self, transform):\n        """"""This method will replace the current transformation for the\n        dataset.\n\n        :param transform: the new transformation\n        """"""\n        self.transform = transform\n\n    def compute_mean_std(self, verbose=False):\n        """"""Compute the mean and standard deviation of the entire dataset.\n\n        :param verbose: if True, it will show a progress bar.\n        :returns: tuple (mean, std dev)\n        """"""\n        sum_intensities = 0.0\n        numel = 0\n\n        with DatasetManager(self,\n                            override_transform=mt_transforms.ToTensor()) as dset:\n            pbar = tqdm(dset, desc=""Mean calculation"", disable=not verbose)\n            for sample in pbar:\n                input_data = sample[\'input\']\n                sum_intensities += input_data.sum()\n                numel += input_data.numel()\n                pbar.set_postfix(mean=""{:.2f}"".format(sum_intensities / numel),\n                                 refresh=False)\n\n            training_mean = sum_intensities / numel\n\n            sum_var = 0.0\n            numel = 0\n\n            pbar = tqdm(dset, desc=""Std Dev calculation"", disable=not verbose)\n            for sample in pbar:\n                input_data = sample[\'input\']\n                sum_var += (input_data - training_mean).pow(2).sum()\n                numel += input_data.numel()\n                pbar.set_postfix(std=""{:.2f}"".format(np.sqrt(sum_var / numel)),\n                                 refresh=False)\n\n        training_std = np.sqrt(sum_var / numel)\n        return training_mean.item(), training_std.item()\n\n    def __len__(self):\n        """"""Return the dataset size.""""""\n        return len(self.indexes)\n\n    def __getitem__(self, index):\n        """"""Return the specific index pair slices (input, ground truth).\n\n        :param index: slice index.\n        """"""\n        segpair, segpair_slice = self.indexes[index]\n        pair_slice = segpair.get_pair_slice(segpair_slice,\n                                            self.slice_axis)\n\n        # Consistency with torchvision, returning PIL Image\n        # Using the ""Float mode"" of PIL, the only mode\n        # supporting unbounded float32 values\n        input_img = Image.fromarray(pair_slice[""input""], mode=\'F\')\n\n        # Handle unlabeled data\n        if pair_slice[""gt""] is None:\n            gt_img = None\n        else:\n            gt_img = Image.fromarray(pair_slice[""gt""], mode=\'F\')\n\n        data_dict = {\n            \'input\': input_img,\n            \'gt\': gt_img,\n            \'input_metadata\': pair_slice[\'input_metadata\'],\n            \'gt_metadata\': pair_slice[\'gt_metadata\'],\n        }\n\n        if self.transform is not None:\n            data_dict = self.transform(data_dict)\n\n        return data_dict\n\n\nclass MRI3DSegmentationDataset(Dataset):\n    """"""This is a generic class for 3D segmentation datasets.\n\n    :param filename_pairs: a list of tuples in the format (input filename,\n                           ground truth filename).\n    :param cache: if the data should be cached in memory or not.\n    :param transform: transformations to apply.\n    """"""\n    def __init__(self, filename_pairs, cache=True,\n                 transform=None, canonical=False):\n        self.filename_pairs = filename_pairs\n        self.handlers = []\n        self.indexes = []\n        self.transform = transform\n        self.cache = cache\n        self.canonical = canonical\n\n        self._load_filenames()\n\n    def _load_filenames(self):\n        for input_filename, gt_filename in self.filename_pairs:\n            segpair = SegmentationPair2D(input_filename, gt_filename,\n                                         self.cache, self.canonical)\n            self.handlers.append(segpair)\n\n    def set_transform(self, transform):\n        """"""This method will replace the current transformation for the\n        dataset.\n\n        :param transform: the new transformation\n        """"""\n        self.transform = transform\n\n    def __len__(self):\n        """"""Return the dataset size.""""""\n        return len(self.handlers)\n\n    def __getitem__(self, index):\n        """"""Return the specific index pair volume (input, ground truth).\n\n        :param index: volume index.\n        """"""\n        input_img, gt_img = self.handlers[index].get_pair_data()\n        data_dict = {\n            \'input\': input_img,\n            \'gt\': gt_img\n        }\n        if self.transform is not None:\n            data_dict = self.transform(data_dict)\n        return data_dict\n\nclass MRI3DSubVolumeSegmentationDataset(MRI3DSegmentationDataset):\n    """"""This is a generic class for 3D segmentation datasets. This class overload\n    MRI3DSegmentationDataset by splitting the initials volumes in several\n    subvolumes. Each subvolumes will be of the sizes of the length parameter.\n\n    This class also implement a padding parameter, which overlap the borders of\n    the different (the borders of the upper-volume aren\'t superposed). For\n    example if you have a length of (32,32,32) and a padding of 16, your final\n    subvolumes will have a total lengths of (64,64,64) with the voxels contained\n    outside the core volume and which are shared with the other subvolumes.\n\n    Be careful, the input\'s dimensions should be compatible with the given\n    lengths and paddings. This class doesn\'t handle missing dimensions.\n\n    :param filename_pairs: a list of tuples in the format (input filename,\n                           ground truth filename).\n    :param cache: if the data should be cached in memory or not.\n    :param transform: transformations to apply.\n    :param length: size of each dimensions of the subvolumes\n    :param padding: size of the overlapping per subvolume and dimensions\n    """"""\n    def __init__(self, filename_pairs, cache=True,\n                 transform=None, canonical=False, length=(64,64,64), padding=0):\n        super().__init__(filename_pairs, cache, transform, canonical)\n        self.length=length\n        self.padding=padding\n        self._prepare_indexes()\n\n    def _prepare_indexes(self):\n        length = self.length\n        padding = self.padding\n\n        for i in range(0, len(self.handlers)):\n            input_img, _ = self.handlers[i].get_pair_data()\n            shape = input_img.shape\n            if (shape[0] - 2 * padding) % length[0] != 0 \\\n                    or (shape[1] - 2 * padding) % length[1] != 0 \\\n                    or (shape[2] - 2 * padding) % length[2] != 0 :\n                raise RuntimeError(\'Input shape of each dimension should be a \\\n                                    multiple of length plus 2 * padding\')\n\n            for x in range(length[0]+padding, shape[0]-padding+1, length[0]):\n                for y in range(length[1]+padding, shape[1]-padding+1, length[1]):\n                    for z in range(length[2]+padding, shape[2]-padding+1, length[2]):\n                        self.indexes.append({\n                            \'x_min\': x-length[0]-padding,\n                            \'x_max\': x+padding,\n                            \'y_min\': y-length[1]-padding,\n                            \'y_max\': y+padding,\n                            \'z_min\': z-length[2]-padding,\n                            \'z_max\': z+padding,\n                            \'handler_index\':i})\n\n    def __len__(self):\n        """"""Return the dataset size. The number of subvolumes.""""""\n        return len(self.indexes)\n\n    def __getitem__(self, index):\n        """"""Return the specific index pair subvolume (input, ground truth).\n\n        :param index: subvolume index.\n        """"""\n        coord = self.indexes[index]\n        input_img, gt_img = self.handlers[coord[\'handler_index\']].get_pair_data()\n        data_dict = {\n            \'input\': input_img,\n            \'gt\': gt_img\n        }\n\n        data_dict[\'input\'] = data_dict[\'input\'][coord[\'x_min\']:coord[\'x_max\'],\n                                coord[\'y_min\']:coord[\'y_max\'],\n                                coord[\'z_min\']:coord[\'z_max\']]\n\n        data_dict[\'gt\'] = data_dict[\'gt\'][coord[\'x_min\']:coord[\'x_max\'],\n                                          coord[\'y_min\']:coord[\'y_max\'],\n                                          coord[\'z_min\']:coord[\'z_max\']]\n\n        if self.transform is not None:\n            data_dict = self.transform(data_dict)\n\n        data_dict[\'input\'] = data_dict[\'input\'][None,:,:,:]\n        data_dict[\'gt\'] = data_dict[\'gt\'][None,:,:,:]\n\n        return data_dict\n\n\nclass DatasetManager(object):\n    def __init__(self, dataset, override_transform=None):\n        self.dataset = dataset\n        self.override_transform = override_transform\n        self._transform_state = None\n\n    def __enter__(self):\n        if self.override_transform:\n            self._transform_state = self.dataset.transform\n            self.dataset.transform = self.override_transform\n        return self.dataset\n\n    def __exit__(self, *args):\n        if self._transform_state:\n            self.dataset.transform = self._transform_state\n\n\nclass SCGMChallenge2DTrain(MRI2DSegmentationDataset):\n    """"""This is the Spinal Cord Gray Matter Challenge dataset.\n\n    :param root_dir: the directory containing the training dataset.\n    :param site_ids: a list of site ids to filter (i.e. [1, 3]).\n    :param subj_ids: the list of subject ids to filter.\n    :param rater_ids: the list of the rater ids to filter.\n    :param transform: the transformations that should be applied.\n    :param cache: if the data should be cached in memory or not.\n    :param slice_axis: axis to make the slicing (default axial).\n\n    .. note:: This dataset assumes that you only have one class in your\n              ground truth mask (w/ 0\'s and 1\'s). It also doesn\'t\n              automatically resample the dataset.\n\n    .. seealso::\n        Prados, F., et al (2017). Spinal cord grey matter\n        segmentation challenge. NeuroImage, 152, 312\xe2\x80\x93329.\n        https://doi.org/10.1016/j.neuroimage.2017.03.010\n\n        Challenge Website:\n        http://cmictig.cs.ucl.ac.uk/spinal-cord-grey-matter-segmentation-challenge\n    """"""\n    NUM_SITES = 4\n    NUM_SUBJECTS = 10\n    NUM_RATERS = 4\n\n    def __init__(self, root_dir, slice_axis=2, site_ids=None,\n                 subj_ids=None, rater_ids=None, cache=True,\n                 transform=None, slice_filter_fn=None,\n                 canonical=False, labeled=True):\n\n        self.labeled = labeled\n        self.root_dir = root_dir\n        self.site_ids = site_ids or range(1, SCGMChallenge2DTrain.NUM_SITES + 1)\n        self.subj_ids = subj_ids or range(1, SCGMChallenge2DTrain.NUM_SUBJECTS + 1)\n        self.rater_ids = rater_ids or range(1, SCGMChallenge2DTrain.NUM_RATERS + 1)\n\n        self.filename_pairs = []\n\n        for site_id in self.site_ids:\n            for subj_id in self.subj_ids:\n                if len(self.rater_ids) > 0:\n                    for rater_id in self.rater_ids:\n                        input_filename = self._build_train_input_filename(site_id, subj_id)\n                        gt_filename = self._build_train_input_filename(site_id, subj_id, rater_id)\n\n                        input_filename = os.path.join(self.root_dir, input_filename)\n                        gt_filename = os.path.join(self.root_dir, gt_filename)\n\n                        if not self.labeled:\n                            gt_filename = None\n\n                        self.filename_pairs.append((input_filename, gt_filename))\n                else:\n                    input_filename = self._build_train_input_filename(site_id, subj_id)\n                    gt_filename = None\n                    input_filename = os.path.join(self.root_dir, input_filename)\n\n                    if not self.labeled:\n                        gt_filename = None\n\n                    self.filename_pairs.append((input_filename, gt_filename))\n\n        super().__init__(self.filename_pairs, slice_axis, cache,\n                         transform, slice_filter_fn, canonical)\n\n    @staticmethod\n    def _build_train_input_filename(site_id, subj_id, rater_id=None):\n        if rater_id is None:\n            return ""site{:d}-sc{:02d}-image.nii.gz"".format(site_id, subj_id)\n        else:\n            return ""site{:d}-sc{:02d}-mask-r{:d}.nii.gz"".format(site_id, subj_id, rater_id)\n\n\nclass SCGMChallenge2DTest(MRI2DSegmentationDataset):\n    """"""This is the Spinal Cord Gray Matter Challenge dataset.\n\n    :param root_dir: the directory containing the test dataset.\n    :param site_ids: a list of site ids to filter (i.e. [1, 3]).\n    :param subj_ids: the list of subject ids to filter.\n    :param transform: the transformations that should be applied.\n    :param cache: if the data should be cached in memory or not.\n    :param slice_axis: axis to make the slicing (default axial).\n\n    .. note:: This dataset assumes that you only have one class in your\n              ground truth mask (w/ 0\'s and 1\'s). It also doesn\'t\n              automatically resample the dataset.\n\n    .. seealso::\n        Prados, F., et al (2017). Spinal cord grey matter\n        segmentation challenge. NeuroImage, 152, 312\xe2\x80\x93329.\n        https://doi.org/10.1016/j.neuroimage.2017.03.010\n\n        Challenge Website:\n        http://cmictig.cs.ucl.ac.uk/spinal-cord-grey-matter-segmentation-challenge\n    """"""\n    NUM_SITES = 4\n    NUM_SUBJECTS = 10\n\n    def __init__(self, root_dir, slice_axis=2, site_ids=None,\n                 subj_ids=None, cache=True,\n                 transform=None, slice_filter_fn=None,\n                 canonical=False):\n\n        self.root_dir = root_dir\n        self.site_ids = site_ids or range(1, SCGMChallenge2DTest.NUM_SITES + 1)\n        self.subj_ids = subj_ids or range(11, 10 + SCGMChallenge2DTest.NUM_SUBJECTS + 1)\n\n        self.filename_pairs = []\n\n        for site_id in self.site_ids:\n            for subj_id in self.subj_ids:\n                input_filename = self._build_train_input_filename(site_id, subj_id)\n                gt_filename = None\n\n                input_filename = os.path.join(self.root_dir, input_filename)\n                if not os.path.exists(input_filename):\n                    raise RuntimeError(""Path \'{}\' doesn\'t exist !"".format(input_filename))\n                self.filename_pairs.append((input_filename, gt_filename))\n\n        super().__init__(self.filename_pairs, slice_axis, cache,\n                         transform, slice_filter_fn, canonical)\n\n    @staticmethod\n    def _build_train_input_filename(site_id, subj_id, rater_id=None):\n        if rater_id is None:\n            return ""site{:d}-sc{:02d}-image.nii.gz"".format(site_id, subj_id)\n        else:\n            return ""site{:d}-sc{:02d}-mask-r{:d}.nii.gz"".format(site_id, subj_id, rater_id)\n\n\n\ndef mt_collate(batch):\n    error_msg = ""batch must contain tensors, numbers, dicts or lists; found {}""\n    elem_type = type(batch[0])\n    if torch.is_tensor(batch[0]):\n        stacked = torch.stack(batch, 0)\n        return stacked\n    elif elem_type.__module__ == \'numpy\' and elem_type.__name__ != \'str_\' \\\n            and elem_type.__name__ != \'string_\':\n        elem = batch[0]\n        if elem_type.__name__ == \'ndarray\':\n            # array of string classes and object\n            if re.search(\'[SaUO]\', elem.dtype.str) is not None:\n                raise TypeError(error_msg.format(elem.dtype))\n            return torch.stack([torch.from_numpy(b) for b in batch], 0)\n        if elem.shape == ():  # scalars\n            py_type = float if elem.dtype.name.startswith(\'float\') else int\n            return __numpy_type_map[elem.dtype.name](list(map(py_type, batch)))\n    elif isinstance(batch[0], int_classes):\n        return torch.LongTensor(batch)\n    elif isinstance(batch[0], float):\n        return torch.DoubleTensor(batch)\n    elif isinstance(batch[0], string_classes):\n        return batch\n    elif isinstance(batch[0], collections.Mapping):\n        return {key: mt_collate([d[key] for d in batch]) for key in batch[0]}\n    elif isinstance(batch[0], collections.Sequence):\n        transposed = zip(*batch)\n        return [mt_collate(samples) for samples in transposed]\n\n    return batch\n'"
medicaltorch/filters.py,0,"b""import numpy as np\n\n\nclass SliceFilter(object):\n    def __init__(self, filter_empty_mask=True,\n                 filter_empty_input=True):\n        self.filter_empty_mask = filter_empty_mask\n        self.filter_empty_input = filter_empty_input\n\n    def __call__(self, sample):\n        input_data, gt_data = sample['input'], sample['gt']\n        \n        if self.filter_empty_mask:\n            if not np.any(gt_data):\n                return False\n        \n        if self.filter_empty_input:\n            if not np.any(input_data):\n                return False\n        \n        return True"""
medicaltorch/losses.py,2,"b'import torch\nfrom torch.nn import Module\n\ndef dice_loss(input, target):\n    """"""Dice loss.\n\n    :param input: The input (predicted)\n    :param target:  The target (ground truth)\n    :returns: the Dice score between 0 and 1.\n    """"""\n    eps = 0.0001\n\n    iflat = input.view(-1)\n    tflat = target.view(-1)\n\n    intersection = (iflat * tflat).sum()\n    union = iflat.sum() + tflat.sum()\n\n    dice = (2.0 * intersection + eps) / (union + eps)\n\n    return - dice\n\n\nclass MaskedDiceLoss(Module):\n    """"""A masked version of the Dice loss.\n\n    :param ignore_value: the value to ignore.\n    """"""\n\n    def __init__(self, ignore_value=-100.0):\n        super().__init__()\n        self.ignore_value = ignore_value\n\n    def forward(self, input, target):\n        eps = 0.0001\n\n        masking = target == self.ignore_value\n        masking = masking.sum(3).sum(2)\n        masking = masking == 0\n        masking = masking.squeeze()\n\n        labeled_target = target.index_select(0, masking.nonzero().squeeze())\n        labeled_input = input.index_select(0, masking.nonzero().squeeze())\n\n        iflat = labeled_input.view(-1)\n        tflat = labeled_target.view(-1)\n\n        intersection = (iflat * tflat).sum()\n        union = iflat.sum() + tflat.sum()\n\n        dice = (2.0 * intersection + eps) / (union + eps)\n\n        return - dice\n\n\nclass ConfidentMSELoss(Module):\n    def __init__(self, threshold=0.96):\n        self.threshold = threshold\n        super().__init__()\n\n    def forward(self, input, target):\n        n = input.size(0)\n        conf_mask = torch.gt(target, self.threshold).float()\n        input_flat = input.view(n, -1)\n        target_flat = target.view(n, -1)\n        conf_mask_flat = conf_mask.view(n, -1)\n        diff = (input_flat - target_flat)**2\n        diff_conf = diff * conf_mask_flat\n        loss = diff_conf.mean()\n        return loss'"
medicaltorch/metrics.py,0,"b'from collections import defaultdict\n\nfrom scipy import spatial\nimport numpy as np\n\n\nclass MetricManager(object):\n    def __init__(self, metric_fns):\n        self.metric_fns = metric_fns\n        self.result_dict = defaultdict(float)\n        self.num_samples = 0 \n    \n    def __call__(self, prediction, ground_truth):\n        self.num_samples += len(prediction)\n        for metric_fn in self.metric_fns:\n            for p, gt in zip(prediction, ground_truth):\n                res = metric_fn(p, gt)\n                dict_key = metric_fn.__name__\n                self.result_dict[dict_key] += res\n            \n    def get_results(self):\n        res_dict = {}\n        for key, val in self.result_dict.items():\n            res_dict[key] = val / self.num_samples\n        return res_dict\n    \n    def reset(self):\n        self.num_samples = 0\n        self.result_dict = defaultdict(float)\n        \n\ndef numeric_score(prediction, groundtruth):\n    """"""Computation of statistical numerical scores:\n\n    * FP = False Positives\n    * FN = False Negatives\n    * TP = True Positives\n    * TN = True Negatives\n\n    return: tuple (FP, FN, TP, TN)\n    """"""\n    FP = np.float(np.sum((prediction == 1) & (groundtruth == 0)))\n    FN = np.float(np.sum((prediction == 0) & (groundtruth == 1)))\n    TP = np.float(np.sum((prediction == 1) & (groundtruth == 1)))\n    TN = np.float(np.sum((prediction == 0) & (groundtruth == 0)))\n    return FP, FN, TP, TN\n\n\ndef dice_score(prediction, groundtruth):\n    pflat = prediction.flatten()\n    gflat = groundtruth.flatten()\n    d = (1 - spatial.distance.dice(pflat, gflat)) * 100.0\n    if np.isnan(d):\n        return 0.0\n    return d\n\n\ndef jaccard_score(prediction, groundtruth):\n    pflat = prediction.flatten()\n    gflat = groundtruth.flatten()\n    return (1 - spatial.distance.jaccard(pflat, gflat)) * 100.0\n\n\ndef hausdorff_score(prediction, groundtruth):\n    return spatial.distance.directed_hausdorff(prediction, groundtruth)[0]\n\n\ndef precision_score(prediction, groundtruth):\n    # PPV\n    FP, FN, TP, TN = numeric_score(prediction, groundtruth)\n    if (TP + FP) <= 0.0:\n        return 0.0\n\n    precision = np.divide(TP, TP + FP)\n    return precision * 100.0\n\n\ndef recall_score(prediction, groundtruth):\n    # TPR, sensitivity\n    FP, FN, TP, TN = numeric_score(prediction, groundtruth)\n    if (TP + FN) <= 0.0:\n        return 0.0\n    TPR = np.divide(TP, TP + FN)\n    return TPR * 100.0\n\n\ndef specificity_score(prediction, groundtruth):\n    FP, FN, TP, TN = numeric_score(prediction, groundtruth)\n    if (TN + FP) <= 0.0:\n        return 0.0\n    TNR = np.divide(TN, TN + FP)\n    return TNR * 100.0\n\n\ndef intersection_over_union(prediction, groundtruth):\n    FP, FN, TP, TN = numeric_score(prediction, groundtruth)\n    if (TP + FP + FN) <= 0.0:\n        return 0.0\n    return TP / (TP + FP + FN) * 100.0\n\n\ndef accuracy_score(prediction, groundtruth):\n    FP, FN, TP, TN = numeric_score(prediction, groundtruth)\n    N = FP + FN + TP + TN\n    accuracy = np.divide(TP + TN, N)\n    return accuracy * 100.0\n'"
medicaltorch/models.py,9,"b'import torch\nimport torch.nn as nn\nfrom torch.nn import Module\nimport torch.nn.functional as F\n\n\nclass NoPoolASPP(Module):\n    """"""\n    .. image:: _static/img/nopool_aspp_arch.png\n        :align: center\n        :scale: 25%\n\n    An ASPP-based model without initial pooling layers.\n\n    :param drop_rate: dropout rate.\n    :param bn_momentum: batch normalization momentum.\n\n    .. seealso::\n        Perone, C. S., et al (2017). Spinal cord gray matter\n        segmentation using deep dilated convolutions.\n        Nature Scientific Reports link:\n        https://www.nature.com/articles/s41598-018-24304-3\n\n    """"""\n    def __init__(self, drop_rate=0.4, bn_momentum=0.1,\n                 base_num_filters=64):\n        super().__init__()\n\n        self.conv1a = nn.Conv2d(1, base_num_filters, kernel_size=3, padding=1)\n        self.conv1a_bn = nn.BatchNorm2d(base_num_filters, momentum=bn_momentum)\n        self.conv1a_drop = nn.Dropout2d(drop_rate)\n        self.conv1b = nn.Conv2d(base_num_filters, base_num_filters, kernel_size=3, padding=1)\n        self.conv1b_bn = nn.BatchNorm2d(base_num_filters, momentum=bn_momentum)\n        self.conv1b_drop = nn.Dropout2d(drop_rate)\n\n        self.conv2a = nn.Conv2d(base_num_filters, base_num_filters, kernel_size=3, padding=2, dilation=2)\n        self.conv2a_bn = nn.BatchNorm2d(base_num_filters, momentum=bn_momentum)\n        self.conv2a_drop = nn.Dropout2d(drop_rate)\n        self.conv2b = nn.Conv2d(base_num_filters, base_num_filters, kernel_size=3, padding=2, dilation=2)\n        self.conv2b_bn = nn.BatchNorm2d(base_num_filters, momentum=bn_momentum)\n        self.conv2b_drop = nn.Dropout2d(drop_rate)\n\n        # Branch 1x1 convolution\n        self.branch1a = nn.Conv2d(base_num_filters, base_num_filters, kernel_size=3, padding=1)\n        self.branch1a_bn = nn.BatchNorm2d(base_num_filters, momentum=bn_momentum)\n        self.branch1a_drop = nn.Dropout2d(drop_rate)\n        self.branch1b = nn.Conv2d(base_num_filters, base_num_filters, kernel_size=1)\n        self.branch1b_bn = nn.BatchNorm2d(base_num_filters, momentum=bn_momentum)\n        self.branch1b_drop = nn.Dropout2d(drop_rate)\n\n        # Branch for 3x3 rate 6\n        self.branch2a = nn.Conv2d(base_num_filters, base_num_filters, kernel_size=3, padding=6, dilation=6)\n        self.branch2a_bn = nn.BatchNorm2d(base_num_filters, momentum=bn_momentum)\n        self.branch2a_drop = nn.Dropout2d(drop_rate)\n        self.branch2b = nn.Conv2d(base_num_filters, base_num_filters, kernel_size=3, padding=6, dilation=6)\n        self.branch2b_bn = nn.BatchNorm2d(base_num_filters, momentum=bn_momentum)\n        self.branch2b_drop = nn.Dropout2d(drop_rate)\n\n        # Branch for 3x3 rate 12\n        self.branch3a = nn.Conv2d(base_num_filters, base_num_filters, kernel_size=3, padding=12, dilation=12)\n        self.branch3a_bn = nn.BatchNorm2d(base_num_filters, momentum=bn_momentum)\n        self.branch3a_drop = nn.Dropout2d(drop_rate)\n        self.branch3b = nn.Conv2d(base_num_filters, base_num_filters, kernel_size=3, padding=12, dilation=12)\n        self.branch3b_bn = nn.BatchNorm2d(base_num_filters, momentum=bn_momentum)\n        self.branch3b_drop = nn.Dropout2d(drop_rate)\n\n        # Branch for 3x3 rate 18\n        self.branch4a = nn.Conv2d(base_num_filters, base_num_filters, kernel_size=3, padding=18, dilation=18)\n        self.branch4a_bn = nn.BatchNorm2d(base_num_filters, momentum=bn_momentum)\n        self.branch4a_drop = nn.Dropout2d(drop_rate)\n        self.branch4b = nn.Conv2d(base_num_filters, base_num_filters, kernel_size=3, padding=18, dilation=18)\n        self.branch4b_bn = nn.BatchNorm2d(base_num_filters, momentum=bn_momentum)\n        self.branch4b_drop = nn.Dropout2d(drop_rate)\n\n        # Branch for 3x3 rate 24\n        self.branch5a = nn.Conv2d(base_num_filters, base_num_filters, kernel_size=3, padding=24, dilation=24)\n        self.branch5a_bn = nn.BatchNorm2d(base_num_filters, momentum=bn_momentum)\n        self.branch5a_drop = nn.Dropout2d(drop_rate)\n        self.branch5b = nn.Conv2d(base_num_filters, base_num_filters, kernel_size=3, padding=24, dilation=24)\n        self.branch5b_bn = nn.BatchNorm2d(base_num_filters, momentum=bn_momentum)\n        self.branch5b_drop = nn.Dropout2d(drop_rate)\n\n        self.concat_drop = nn.Dropout2d(drop_rate)\n        self.concat_bn = nn.BatchNorm2d(6*base_num_filters, momentum=bn_momentum)\n\n        self.amort = nn.Conv2d(6*base_num_filters, base_num_filters*2, kernel_size=1)\n        self.amort_bn = nn.BatchNorm2d(base_num_filters*2, momentum=bn_momentum)\n        self.amort_drop = nn.Dropout2d(drop_rate)\n\n        self.prediction = nn.Conv2d(base_num_filters*2, 1, kernel_size=1)\n\n    def forward(self, x):\n        """"""Model forward pass.\n\n        :param x: input data.\n        """"""\n        x = F.relu(self.conv1a(x))\n        x = self.conv1a_bn(x)\n        x = self.conv1a_drop(x)\n\n        x = F.relu(self.conv1b(x))\n        x = self.conv1b_bn(x)\n        x = self.conv1b_drop(x)\n\n        x = F.relu(self.conv2a(x))\n        x = self.conv2a_bn(x)\n        x = self.conv2a_drop(x)\n        x = F.relu(self.conv2b(x))\n        x = self.conv2b_bn(x)\n        x = self.conv2b_drop(x)\n\n        # Branch 1x1 convolution\n        branch1 = F.relu(self.branch1a(x))\n        branch1 = self.branch1a_bn(branch1)\n        branch1 = self.branch1a_drop(branch1)\n        branch1 = F.relu(self.branch1b(branch1))\n        branch1 = self.branch1b_bn(branch1)\n        branch1 = self.branch1b_drop(branch1)\n\n        # Branch for 3x3 rate 6\n        branch2 = F.relu(self.branch2a(x))\n        branch2 = self.branch2a_bn(branch2)\n        branch2 = self.branch2a_drop(branch2)\n        branch2 = F.relu(self.branch2b(branch2))\n        branch2 = self.branch2b_bn(branch2)\n        branch2 = self.branch2b_drop(branch2)\n\n        # Branch for 3x3 rate 6\n        branch3 = F.relu(self.branch3a(x))\n        branch3 = self.branch3a_bn(branch3)\n        branch3 = self.branch3a_drop(branch3)\n        branch3 = F.relu(self.branch3b(branch3))\n        branch3 = self.branch3b_bn(branch3)\n        branch3 = self.branch3b_drop(branch3)\n\n        # Branch for 3x3 rate 18\n        branch4 = F.relu(self.branch4a(x))\n        branch4 = self.branch4a_bn(branch4)\n        branch4 = self.branch4a_drop(branch4)\n        branch4 = F.relu(self.branch4b(branch4))\n        branch4 = self.branch4b_bn(branch4)\n        branch4 = self.branch4b_drop(branch4)\n\n        # Branch for 3x3 rate 24\n        branch5 = F.relu(self.branch5a(x))\n        branch5 = self.branch5a_bn(branch5)\n        branch5 = self.branch5a_drop(branch5)\n        branch5 = F.relu(self.branch5b(branch5))\n        branch5 = self.branch5b_bn(branch5)\n        branch5 = self.branch5b_drop(branch5)\n\n        # Global Average Pooling\n        global_pool = F.avg_pool2d(x, kernel_size=x.size()[2:])\n        global_pool = global_pool.expand(x.size())\n\n        concatenation = torch.cat([branch1,\n                                   branch2,\n                                   branch3,\n                                   branch4,\n                                   branch5,\n                                   global_pool], dim=1)\n\n        concatenation = self.concat_bn(concatenation)\n        concatenation = self.concat_drop(concatenation)\n\n        amort = F.relu(self.amort(concatenation))\n        amort = self.amort_bn(amort)\n        amort = self.amort_drop(amort)\n\n        predictions = self.prediction(amort)\n        predictions = F.sigmoid(predictions)\n\n        return predictions\n\n\nclass DownConv(Module):\n    def __init__(self, in_feat, out_feat, drop_rate=0.4, bn_momentum=0.1):\n        super(DownConv, self).__init__()\n        self.conv1 = nn.Conv2d(in_feat, out_feat, kernel_size=3, padding=1)\n        self.conv1_bn = nn.BatchNorm2d(out_feat, momentum=bn_momentum)\n        self.conv1_drop = nn.Dropout2d(drop_rate)\n\n        self.conv2 = nn.Conv2d(out_feat, out_feat, kernel_size=3, padding=1)\n        self.conv2_bn = nn.BatchNorm2d(out_feat, momentum=bn_momentum)\n        self.conv2_drop = nn.Dropout2d(drop_rate)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = self.conv1_bn(x)\n        x = self.conv1_drop(x)\n\n        x = F.relu(self.conv2(x))\n        x = self.conv2_bn(x)\n        x = self.conv2_drop(x)\n        return x\n\n\nclass UpConv(Module):\n    def __init__(self, in_feat, out_feat, drop_rate=0.4, bn_momentum=0.1):\n        super(UpConv, self).__init__()\n        self.up1 = nn.functional.interpolate\n        self.downconv = DownConv(in_feat, out_feat, drop_rate, bn_momentum)\n\n    def forward(self, x, y):\n        x = self.up1(x, scale_factor=2, mode=\'bilinear\', align_corners=True)\n        x = torch.cat([x, y], dim=1)\n        x = self.downconv(x)\n        return x\n\n\nclass Unet(Module):\n    """"""A reference U-Net model.\n\n    .. seealso::\n        Ronneberger, O., et al (2015). U-Net: Convolutional\n        Networks for Biomedical Image Segmentation\n        ArXiv link: https://arxiv.org/abs/1505.04597\n    """"""\n    def __init__(self, drop_rate=0.4, bn_momentum=0.1):\n        super(Unet, self).__init__()\n\n        #Downsampling path\n        self.conv1 = DownConv(1, 64, drop_rate, bn_momentum)\n        self.mp1 = nn.MaxPool2d(2)\n\n        self.conv2 = DownConv(64, 128, drop_rate, bn_momentum)\n        self.mp2 = nn.MaxPool2d(2)\n\n        self.conv3 = DownConv(128, 256, drop_rate, bn_momentum)\n        self.mp3 = nn.MaxPool2d(2)\n\n        # Bottom\n        self.conv4 = DownConv(256, 256, drop_rate, bn_momentum)\n\n        # Upsampling path\n        self.up1 = UpConv(512, 256, drop_rate, bn_momentum)\n        self.up2 = UpConv(384, 128, drop_rate, bn_momentum)\n        self.up3 = UpConv(192, 64, drop_rate, bn_momentum)\n\n        self.conv9 = nn.Conv2d(64, 1, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        x1 = self.conv1(x)\n        x2 = self.mp1(x1)\n\n        x3 = self.conv2(x2)\n        x4 = self.mp2(x3)\n\n        x5 = self.conv3(x4)\n        x6 = self.mp3(x5)\n\n        # Bottom\n        x7 = self.conv4(x6)\n\n        # Up-sampling\n        x8 = self.up1(x7, x5)\n        x9 = self.up2(x8, x3)\n        x10 = self.up3(x9, x1)\n\n        x11 = self.conv9(x10)\n        preds = torch.sigmoid(x11)\n\n        return preds\n\n\nclass UNet3D(nn.Module):\n    """"""A reference of 3D U-Net model.\n\n    Implementation origin :\n    https://github.com/shiba24/3d-unet/blob/master/pytorch/model.py\n\n    .. seealso::\n        \xc3\x96zg\xc3\xbcn \xc3\x87i\xc3\xa7ek, Ahmed Abdulkadir, Soeren S. Lienkamp, Thomas Brox\n        and Olaf Ronneberger (2016). 3D U-Net: Learning Dense Volumetric\n        Segmentation from Sparse Annotation\n        ArXiv link: https://arxiv.org/pdf/1606.06650.pdf\n    """"""\n    def __init__(self, in_channel, n_classes):\n        self.in_channel = in_channel\n        self.n_classes = n_classes\n        super(UNet3D, self).__init__()\n        self.ec0 = self.down_conv(self.in_channel, 32, bias=False, batchnorm=False)\n        self.ec1 = self.down_conv(32, 64, bias=False, batchnorm=False)\n        self.ec2 = self.down_conv(64, 64, bias=False, batchnorm=False)\n        self.ec3 = self.down_conv(64, 128, bias=False, batchnorm=False)\n        self.ec4 = self.down_conv(128, 128, bias=False, batchnorm=False)\n        self.ec5 = self.down_conv(128, 256, bias=False, batchnorm=False)\n        self.ec6 = self.down_conv(256, 256, bias=False, batchnorm=False)\n        self.ec7 = self.down_conv(256, 512, bias=False, batchnorm=False)\n\n        self.pool0 = nn.MaxPool3d(2)\n        self.pool1 = nn.MaxPool3d(2)\n        self.pool2 = nn.MaxPool3d(2)\n\n        self.dc9 = self.up_conv(512, 512, kernel_size=2, stride=2, bias=False)\n        self.dc8 = self.down_conv(256 + 512, 256, bias=False)\n        self.dc7 = self.down_conv(256, 256, bias=False)\n        self.dc6 = self.up_conv(256, 256, kernel_size=2, stride=2, bias=False)\n        self.dc5 = self.down_conv(128 + 256, 128, bias=False)\n        self.dc4 = self.down_conv(128, 128, bias=False)\n        self.dc3 = self.up_conv(128, 128, kernel_size=2, stride=2, bias=False)\n        self.dc2 = self.down_conv(64 + 128, 64, bias=False)\n        self.dc1 = self.down_conv(64, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.dc0 = self.down_conv(64, n_classes, kernel_size=1, stride=1, padding=0, bias=False)\n\n\n    def down_conv(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1,\n                bias=True, batchnorm=False):\n        if batchnorm:\n            layer = nn.Sequential(\n                nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias),\n                nn.BatchNorm2d(out_channels),\n                nn.LeakyReLU())\n        else:\n            layer = nn.Sequential(\n                nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias),\n                nn.LeakyReLU())\n        return layer\n\n\n    def up_conv(self, in_channels, out_channels, kernel_size=2, stride=2, padding=0,\n                output_padding=0, bias=True):\n        layer = nn.Sequential(\n            nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride,\n                               padding=padding, output_padding=output_padding, bias=bias),\n            nn.LeakyReLU())\n        return layer\n\n    def forward(self, x):\n        e0 = self.ec0(x)\n        syn0 = self.ec1(e0)\n        e1 = self.pool0(syn0)\n        e2 = self.ec2(e1)\n        syn1 = self.ec3(e2)\n        del e0, e1, e2\n\n        e3 = self.pool1(syn1)\n        e4 = self.ec4(e3)\n        syn2 = self.ec5(e4)\n        del e3, e4\n\n        e5 = self.pool2(syn2)\n        e6 = self.ec6(e5)\n        e7 = self.ec7(e6)\n        del e5, e6\n\n        d9 = torch.cat((self.dc9(e7), syn2), dim=1)\n        del e7, syn2\n\n        d8 = self.dc8(d9)\n        d7 = self.dc7(d8)\n        del d9, d8\n\n        d6 = torch.cat((self.dc6(d7), syn1), dim=1)\n        del d7, syn1\n\n        d5 = self.dc5(d6)\n        d4 = self.dc4(d5)\n        del d6, d5\n\n        d3 = torch.cat((self.dc3(d4), syn0), dim=1)\n        del d4, syn0\n\n        d2 = self.dc2(d3)\n        d1 = self.dc1(d2)\n        del d3, d2\n\n        d0 = self.dc0(d1)\n        return d0\n'"
medicaltorch/transforms.py,0,"b'import skimage\nimport numpy as np\nimport numbers\nimport torchvision.transforms.functional as F\nfrom torchvision import transforms\nfrom PIL import Image\n\nfrom scipy.ndimage.interpolation import map_coordinates\nfrom scipy.ndimage.filters import gaussian_filter\n\n\nclass MTTransform(object):\n\n    def __call__(self, sample):\n        raise NotImplementedError(""You need to implement the transform() method."")\n\n    def undo_transform(self, sample):\n        raise NotImplementedError(""You need to implement the undo_transform() method."")\n\n\nclass UndoCompose(object):\n    def __init__(self, compose):\n        self.transforms = compose.transforms\n\n    def __call__(self):\n        for t in self.transforms:\n            img = t.undo_transform(img)\n        return img\n\n\nclass UndoTransform(object):\n    def __init__(self, transform):\n        self.transform = transform\n\n    def __call__(self, sample):\n        return self.transform.undo_transform(sample)\n\n\nclass ToTensor(MTTransform):\n    """"""Convert a PIL image or numpy array to a PyTorch tensor.""""""\n\n    def __init__(self, labeled=True):\n        self.labeled = labeled\n\n    def __call__(self, sample):\n        rdict = {}\n        input_data = sample[\'input\']\n\n        if isinstance(input_data, list):\n            ret_input = [F.to_tensor(item)\n                         for item in input_data]\n        else:\n            ret_input = F.to_tensor(input_data)\n\n        rdict[\'input\'] = ret_input\n\n        if self.labeled:\n            gt_data = sample[\'gt\']\n            if gt_data is not None:\n                if isinstance(gt_data, list):\n                    ret_gt = [F.to_tensor(item)\n                              for item in gt_data]\n                else:\n                    ret_gt = F.to_tensor(gt_data)\n\n                rdict[\'gt\'] = ret_gt\n        sample.update(rdict)\n        return sample\n\n\nclass ToPIL(MTTransform):\n    def __init__(self, labeled=True):\n        self.labeled = labeled\n\n    def sample_transform(self, sample_data):\n        # Numpy array\n        if not isinstance(sample_data, np.ndarray):\n            input_data_npy = sample_data.numpy()\n        else:\n            input_data_npy = sample_data\n\n        input_data_npy = np.transpose(input_data_npy, (1, 2, 0))\n        input_data_npy = np.squeeze(input_data_npy, axis=2)\n        input_data = Image.fromarray(input_data_npy, mode=\'F\')\n        return input_data\n\n    def __call__(self, sample):\n        rdict = {}\n        input_data = sample[\'input\']\n\n        if isinstance(input_data, list):\n            ret_input = [self.sample_transform(item)\n                         for item in input_data]\n        else:\n            ret_input = self.sample_transform(input_data)\n\n        rdict[\'input\'] = ret_input\n\n        if self.labeled:\n            gt_data = sample[\'gt\']\n\n            if isinstance(gt_data, list):\n                ret_gt = [self.sample_transform(item)\n                          for item in gt_data]\n            else:\n                ret_gt = self.sample_transform(gt_data)\n\n            rdict[\'gt\'] = ret_gt\n\n        sample.update(rdict)\n        return sample\n\n\nclass UnCenterCrop2D(MTTransform):\n    def __init__(self, size, segmentation=True):\n        self.size = size\n        self.segmentation = segmentation\n\n    def __call__(self, sample):\n        input_data, gt_data = sample[\'input\'], sample[\'gt\']\n        input_metadata, gt_metadata = sample[\'input_metadata\'], sample[\'gt_metadata\']\n\n        (fh, fw, w, h) = input_metadata[""__centercrop""]\n        (fh, fw, w, h) = gt_metadata[""__centercrop""]\n\n        return sample\n\n\nclass CenterCrop2D(MTTransform):\n    """"""Make a center crop of a specified size.\n\n    :param segmentation: if it is a segmentation task.\n                         When this is True (default), the crop\n                         will also be applied to the ground truth.\n    """"""\n    def __init__(self, size, labeled=True):\n        self.size = size\n        self.labeled = labeled\n\n    @staticmethod\n    def propagate_params(sample, params):\n        input_metadata = sample[\'input_metadata\']\n        input_metadata[""__centercrop""] = params\n        return input_metadata\n\n    @staticmethod\n    def get_params(sample):\n        input_metadata = sample[\'input_metadata\']\n        return input_metadata[""__centercrop""]\n\n    def __call__(self, sample):\n        rdict = {}\n        input_data = sample[\'input\']\n\n        w, h = input_data.size\n        th, tw = self.size\n        fh = int(round((h - th) / 2.))\n        fw = int(round((w - tw) / 2.))\n\n        params = (fh, fw, w, h)\n        self.propagate_params(sample, params)\n\n        input_data = F.center_crop(input_data, self.size)\n        rdict[\'input\'] = input_data\n\n        if self.labeled:\n            gt_data = sample[\'gt\']\n            gt_metadata = sample[\'gt_metadata\']\n            gt_data = F.center_crop(gt_data, self.size)\n            gt_metadata[""__centercrop""] = (fh, fw, w, h)\n            rdict[\'gt\'] = gt_data\n\n\n        sample.update(rdict)\n        return sample\n\n    def undo_transform(self, sample):\n        rdict = {}\n        input_data = sample[\'input\']\n        fh, fw, w, h = self.get_params(sample)\n        th, tw = self.size\n\n        pad_left = fw\n        pad_right = w - pad_left - tw\n        pad_top = fh\n        pad_bottom = h - pad_top - th\n\n        padding = (pad_left, pad_top, pad_right, pad_bottom)\n        input_data = F.pad(input_data, padding)\n        rdict[\'input\'] = input_data\n\n        sample.update(rdict)\n        return sample\n\n\nclass Normalize(MTTransform):\n    """"""Normalize a tensor image with mean and standard deviation.\n\n    :param mean: mean value.\n    :param std: standard deviation value.\n    """"""\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, sample):\n        input_data = sample[\'input\']\n\n        input_data = F.normalize(input_data, self.mean, self.std)\n\n        rdict = {\n            \'input\': input_data,\n        }\n        sample.update(rdict)\n        return sample\n\n\nclass NormalizeInstance(MTTransform):\n    """"""Normalize a tensor image with mean and standard deviation estimated\n    from the sample itself.\n\n    :param mean: mean value.\n    :param std: standard deviation value.\n    """"""\n    def __call__(self, sample):\n        input_data = sample[\'input\']\n\n        mean, std = input_data.mean(), input_data.std()\n        input_data = F.normalize(input_data, [mean], [std])\n\n        rdict = {\n            \'input\': input_data,\n        }\n        sample.update(rdict)\n        return sample\n\nclass NormalizeInstance3D(MTTransform):\n    """"""Normalize a tensor volume with mean and standard deviation estimated\n    from the sample itself.\n\n    :param mean: mean value.\n    :param std: standard deviation value.\n    """"""\n    def __call__(self, sample):\n        input_data = sample[\'input\']\n\n        mean, std = input_data.mean(), input_data.std()\n\n        if mean != 0 or std != 0:\n            input_data_normalized = F.normalize(input_data,\n                                    [mean for _ in range(0,input_data.shape[0])],\n                                    [std for _ in range(0,input_data.shape[0])])\n\n            rdict = {\n                \'input\': input_data_normalized,\n            }\n            sample.update(rdict)\n        return sample\n\nclass RandomRotation(MTTransform):\n    def __init__(self, degrees, resample=False,\n                 expand=False, center=None,\n                 labeled=True):\n        if isinstance(degrees, numbers.Number):\n            if degrees < 0:\n                raise ValueError(""If degrees is a single number, it must be positive."")\n            self.degrees = (-degrees, degrees)\n        else:\n            if len(degrees) != 2:\n                raise ValueError(""If degrees is a sequence, it must be of len 2."")\n            self.degrees = degrees\n\n        self.resample = resample\n        self.expand = expand\n        self.center = center\n        self.labeled = labeled\n\n    @staticmethod\n    def get_params(degrees):\n        angle = np.random.uniform(degrees[0], degrees[1])\n        return angle\n\n    def __call__(self, sample):\n        rdict = {}\n        input_data = sample[\'input\']\n        angle = self.get_params(self.degrees)\n        input_data = F.rotate(input_data, angle,\n                              self.resample, self.expand,\n                              self.center)\n        rdict[\'input\'] = input_data\n\n        if self.labeled:\n            gt_data = sample[\'gt\']\n            gt_data = F.rotate(gt_data, angle,\n                               self.resample, self.expand,\n                               self.center)\n            rdict[\'gt\'] = gt_data\n\n        sample.update(rdict)\n        return sample\n\nclass RandomRotation3D(MTTransform):\n    """"""Make a rotation of the volume\'s values.\n\n    :param degrees: Maximum rotation\'s degrees.\n    :param axis: Axis of the rotation.\n    """"""\n    def __init__(self, degrees, axis=0, labeled=True):\n        if isinstance(degrees, numbers.Number):\n            if degrees < 0:\n                raise ValueError(""If degrees is a single number, it must be positive."")\n            self.degrees = (-degrees, degrees)\n        else:\n            if len(degrees) != 2:\n                raise ValueError(""If degrees is a sequence, it must be of len 2."")\n            self.degrees = degrees\n        self.labeled = labeled\n        self.axis = axis\n\n    @staticmethod\n    def get_params(degrees):\n        angle = np.random.uniform(degrees[0], degrees[1])\n        return angle\n\n    def __call__(self, sample):\n        rdict = {}\n        input_data = sample[\'input\']\n        if len(sample[\'input\'].shape) != 3:\n            raise ValueError(""Input of RandomRotation3D should be a 3 dimensionnal tensor."")\n\n        angle = self.get_params(self.degrees)\n        input_rotated = np.zeros(input_data.shape, dtype=input_data.dtype)\n        gt_data = sample[\'gt\'] if self.labeled else None\n        gt_rotated = np.zeros(gt_data.shape, dtype=gt_data.dtype) if self.labeled else None\n\n        # TODO: Would be faster with only one vectorial operation\n        # TODO: Use the axis index for factoring this loop\n        for x in range(input_data.shape[self.axis]):\n            if self.axis == 0:\n                input_rotated[x,:,:] = F.rotate(Image.fromarray(input_data[x,:,:], mode=\'F\'), angle)\n                if self.labeled:\n                    gt_rotated[x,:,:] = F.rotate(Image.fromarray(gt_data[x,:,:], mode=\'F\'), angle)\n            if self.axis == 1:\n                input_rotated[:,x,:] = F.rotate(Image.fromarray(input_data[:,x,:], mode=\'F\'), angle)\n                if self.labeled:\n                    gt_rotated[:,x,:] = F.rotate(Image.fromarray(gt_data[:,x,:], mode=\'F\'), angle)\n            if self.axis == 2:\n                input_rotated[:,:,x] = F.rotate(Image.fromarray(input_data[:,:,x], mode=\'F\'), angle)\n                if self.labeled:\n                    gt_rotated[:,:,x] = F.rotate(Image.fromarray(gt_data[:,:,x], mode=\'F\'), angle)\n\n        rdict[\'input\'] = input_rotated\n        if self.labeled : rdict[\'gt\'] = gt_rotated\n        sample.update(rdict)\n\n        return sample\n\nclass RandomReverse3D(MTTransform):\n    """"""Make a symmetric inversion of the different values of each dimensions.\n    (randomized)\n    """"""\n    def __init__(self, labeled=True):\n        self.labeled = labeled\n\n    def __call__(self, sample):\n        rdict = {}\n        input_data = sample[\'input\']\n        gt_data = sample[\'gt\'] if self.labeled else None\n        if np.random.randint(2) == 1:\n            input_data = np.flip(input_data,axis=0).copy()\n            if self.labeled: gt_data = np.flip(gt_data,axis=0).copy()\n        if np.random.randint(2) == 1:\n            input_data = np.flip(input_data,axis=1).copy()\n            if self.labeled: gt_data = np.flip(gt_data,axis=1).copy()\n        if np.random.randint(2) == 1:\n            input_data = np.flip(input_data,axis=2).copy()\n            if self.labeled: gt_data = np.flip(gt_data,axis=2).copy()\n\n        rdict[\'input\'] = input_data\n        if self.labeled : rdict[\'gt\'] = gt_data\n\n        sample.update(rdict)\n        return sample\n\nclass RandomAffine(MTTransform):\n    def __init__(self, degrees, translate=None,\n                 scale=None, shear=None,\n                 resample=False, fillcolor=0,\n                 labeled=True):\n        if isinstance(degrees, numbers.Number):\n            if degrees < 0:\n                raise ValueError(""If degrees is a single number, it must be positive."")\n            self.degrees = (-degrees, degrees)\n        else:\n            assert isinstance(degrees, (tuple, list)) and len(degrees) == 2, \\\n                ""degrees should be a list or tuple and it must be of length 2.""\n            self.degrees = degrees\n\n        if translate is not None:\n            assert isinstance(translate, (tuple, list)) and len(translate) == 2, \\\n                ""translate should be a list or tuple and it must be of length 2.""\n            for t in translate:\n                if not (0.0 <= t <= 1.0):\n                    raise ValueError(""translation values should be between 0 and 1"")\n        self.translate = translate\n\n        if scale is not None:\n            assert isinstance(scale, (tuple, list)) and len(scale) == 2, \\\n                ""scale should be a list or tuple and it must be of length 2.""\n            for s in scale:\n                if s <= 0:\n                    raise ValueError(""scale values should be positive"")\n        self.scale = scale\n\n        if shear is not None:\n            if isinstance(shear, numbers.Number):\n                if shear < 0:\n                    raise ValueError(""If shear is a single number, it must be positive."")\n                self.shear = (-shear, shear)\n            else:\n                assert isinstance(shear, (tuple, list)) and len(shear) == 2, \\\n                    ""shear should be a list or tuple and it must be of length 2.""\n                self.shear = shear\n        else:\n            self.shear = shear\n\n        self.resample = resample\n        self.fillcolor = fillcolor\n        self.labeled = labeled\n\n    @staticmethod\n    def get_params(degrees, translate, scale_ranges, shears, img_size):\n        """"""Get parameters for affine transformation\n        Returns:\n            sequence: params to be passed to the affine transformation\n        """"""\n        angle = np.random.uniform(degrees[0], degrees[1])\n        if translate is not None:\n            max_dx = translate[0] * img_size[0]\n            max_dy = translate[1] * img_size[1]\n            translations = (np.round(np.random.uniform(-max_dx, max_dx)),\n                            np.round(np.random.uniform(-max_dy, max_dy)))\n        else:\n            translations = (0, 0)\n\n        if scale_ranges is not None:\n            scale = np.random.uniform(scale_ranges[0], scale_ranges[1])\n        else:\n            scale = 1.0\n\n        if shears is not None:\n            shear = np.random.uniform(shears[0], shears[1])\n        else:\n            shear = 0.0\n\n        return angle, translations, scale, shear\n\n    def sample_augment(self, input_data, params):\n        input_data = F.affine(input_data, *params, resample=self.resample,\n                              fillcolor=self.fillcolor)\n        return input_data\n\n    def label_augment(self, gt_data, params):\n        gt_data = self.sample_augment(gt_data, params)\n        np_gt_data = np.array(gt_data)\n        np_gt_data[np_gt_data >= 0.5] = 1.0\n        np_gt_data[np_gt_data < 0.5] = 0.0\n        gt_data = Image.fromarray(np_gt_data, mode=\'F\')\n        return gt_data\n\n    def __call__(self, sample):\n        """"""\n            img (PIL Image): Image to be transformed.\n        Returns:\n            PIL Image: Affine transformed image.\n        """"""\n        rdict = {}\n        input_data = sample[\'input\']\n\n        if isinstance(input_data, list):\n            input_data_size = input_data[0].size\n        else:\n            input_data_size = input_data.size\n\n        params = self.get_params(self.degrees, self.translate, self.scale,\n                                 self.shear, input_data_size)\n\n        if isinstance(input_data, list):\n            ret_input = [self.sample_augment(item, params)\n                         for item in input_data]\n        else:\n            ret_input = self.sample_augment(input_data, params)\n\n        rdict[\'input\'] = ret_input\n\n        if self.labeled:\n            gt_data = sample[\'gt\']\n            if isinstance(gt_data, list):\n                ret_gt = [self.label_augment(item, params)\n                          for item in gt_data]\n            else:\n                ret_gt = self.label_augment(gt_data, params)\n\n            rdict[\'gt\'] = ret_gt\n\n        sample.update(rdict)\n        return sample\n\nclass RandomTensorChannelShift(MTTransform):\n    def __init__(self, shift_range):\n        self.shift_range = shift_range\n\n    @staticmethod\n    def get_params(shift_range):\n        sampled_value = np.random.uniform(shift_range[0],\n                                          shift_range[1])\n        return sampled_value\n\n    def sample_augment(self, input_data, params):\n        np_input_data = np.array(input_data)\n        np_input_data += params\n        input_data = Image.fromarray(np_input_data, mode=\'F\')\n        return input_data\n\n    def __call__(self, sample):\n        input_data = sample[\'input\']\n        params = self.get_params(self.shift_range)\n\n        if isinstance(input_data, list):\n            #ret_input = [self.sample_augment(item, params)\n            #             for item in input_data]\n\n            # Augment just the image, not the mask\n            # TODO: fix it later\n            ret_input = []\n            ret_input.append(self.sample_augment(input_data[0], params))\n            ret_input.append(input_data[1])\n        else:\n            ret_input = self.sample_augment(input_data, params)\n\n        rdict = {\n            \'input\': ret_input,\n        }\n\n        sample.update(rdict)\n        return sample\n\n\nclass ElasticTransform(MTTransform):\n    def __init__(self, alpha_range, sigma_range,\n                 p=0.5, labeled=True):\n        self.alpha_range = alpha_range\n        self.sigma_range = sigma_range\n        self.labeled = labeled\n        self.p = p\n\n    @staticmethod\n    def get_params(alpha, sigma):\n        alpha = np.random.uniform(alpha[0], alpha[1])\n        sigma = np.random.uniform(sigma[0], sigma[1])\n        return alpha, sigma\n\n    @staticmethod\n    def elastic_transform(image, alpha, sigma):\n        shape = image.shape\n        dx = gaussian_filter((np.random.rand(*shape) * 2 - 1),\n                             sigma, mode=""constant"", cval=0) * alpha\n        dy = gaussian_filter((np.random.rand(*shape) * 2 - 1),\n                             sigma, mode=""constant"", cval=0) * alpha\n\n        x, y = np.meshgrid(np.arange(shape[0]),\n                           np.arange(shape[1]), indexing=\'ij\')\n        indices = np.reshape(x+dx, (-1, 1)), np.reshape(y+dy, (-1, 1))\n        return map_coordinates(image, indices, order=1).reshape(shape)\n\n    def sample_augment(self, input_data, params):\n        param_alpha, param_sigma = params\n\n        np_input_data = np.array(input_data)\n        np_input_data = self.elastic_transform(np_input_data,\n                                               param_alpha, param_sigma)\n        input_data = Image.fromarray(np_input_data, mode=\'F\')\n        return input_data\n\n    def label_augment(self, gt_data, params):\n        param_alpha, param_sigma = params\n\n        np_gt_data = np.array(gt_data)\n        np_gt_data = self.elastic_transform(np_gt_data,\n                                            param_alpha, param_sigma)\n        np_gt_data[np_gt_data >= 0.5] = 1.0\n        np_gt_data[np_gt_data < 0.5] = 0.0\n        gt_data = Image.fromarray(np_gt_data, mode=\'F\')\n\n        return gt_data\n\n    def __call__(self, sample):\n        rdict = {}\n\n        if np.random.random() < self.p:\n            input_data = sample[\'input\']\n            params = self.get_params(self.alpha_range,\n                                     self.sigma_range)\n\n            if isinstance(input_data, list):\n                ret_input = [self.sample_augment(item, params)\n                             for item in input_data]\n            else:\n                ret_input = self.sample_augment(input_data, params)\n\n            rdict[\'input\'] = ret_input\n\n            if self.labeled:\n                gt_data = sample[\'gt\']\n                if isinstance(gt_data, list):\n                    ret_gt = [self.label_augment(item, params)\n                              for item in gt_data]\n                else:\n                    ret_gt = self.label_augment(gt_data, params)\n\n                rdict[\'gt\'] = ret_gt\n\n        sample.update(rdict)\n        return sample\n\n\n# TODO: Resample should keep state after changing state.\n#       By changing pixel dimensions, we should be\n#       able to return later to the original space.\nclass Resample(MTTransform):\n    def __init__(self, wspace, hspace,\n                 interpolation=Image.BILINEAR,\n                 labeled=True):\n        self.hspace = hspace\n        self.wspace = wspace\n        self.interpolation = interpolation\n        self.labeled = labeled\n\n    def __call__(self, sample):\n        rdict = {}\n        input_data = sample[\'input\']\n        input_metadata = sample[\'input_metadata\']\n\n        # Voxel dimension in mm\n        hzoom, wzoom = input_metadata[""zooms""]\n        hshape, wshape = input_metadata[""data_shape""]\n\n        hfactor = hzoom / self.hspace\n        wfactor = wzoom / self.wspace\n\n        hshape_new = int(hshape * hfactor)\n        wshape_new = int(wshape * wfactor)\n\n        input_data = input_data.resize((wshape_new, hshape_new),\n                                       resample=self.interpolation)\n        rdict[\'input\'] = input_data\n\n        if self.labeled:\n            gt_data = sample[\'gt\']\n            gt_metadata = sample[\'gt_metadata\']\n            gt_data = gt_data.resize((wshape_new, hshape_new),\n                                     resample=self.interpolation)\n            np_gt_data = np.array(gt_data)\n            np_gt_data[np_gt_data >= 0.5] = 1.0\n            np_gt_data[np_gt_data < 0.5] = 0.0\n            gt_data = Image.fromarray(np_gt_data, mode=\'F\')\n            rdict[\'gt\'] = gt_data\n\n        sample.update(rdict)\n        return sample\n\n\nclass AdditiveGaussianNoise(MTTransform):\n    def __init__(self, mean=0.0, std=0.01):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, sample):\n        rdict = {}\n        input_data = sample[\'input\']\n\n        noise = np.random.normal(self.mean, self.std, input_data.size)\n        noise = noise.astype(np.float32)\n\n        np_input_data = np.array(input_data)\n        np_input_data += noise\n        input_data = Image.fromarray(np_input_data, mode=\'F\')\n        rdict[\'input\'] = input_data\n\n        sample.update(rdict)\n        return sample\n\nclass Clahe(MTTransform):\n    def __init__(self, clip_limit=3.0, kernel_size=(8, 8)):\n        # Default values are based upon the following paper:\n        # https://arxiv.org/abs/1804.09400 (3D Consistent Cardiac Segmentation)\n\n        self.clip_limit = clip_limit\n        self.kernel_size = kernel_size\n    \n    def __call__(self, sample):\n        if not isinstance(sample, np.ndarray):\n            raise TypeError(""Input sample must be a numpy array."")\n        input_sample = np.copy(sample)\n        array = skimage.exposure.equalize_adapthist(\n            input_sample,\n            kernel_size=self.kernel_size,\n            clip_limit=self.clip_limit\n        )\n        return array\n\n\nclass HistogramClipping(MTTransform):\n    def __init__(self, min_percentile=5.0, max_percentile=95.0):\n        self.min_percentile = min_percentile\n        self.max_percentile = max_percentile\n\n    def __call__(self, sample):\n        array = np.copy(sample)\n        percentile1 = np.percentile(array, self.min_percentile)\n        percentile2 = np.percentile(array, self.max_percentile)\n        array[array <= percentile1] = percentile1\n        array[array >= percentile2] = percentile2\n        return array\n'"
tests/test_datasets.py,2,"b'import os\nimport pytest\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\n\nfrom medicaltorch import datasets as mt_datasets\nfrom medicaltorch import transforms as mt_transforms\n\nROOT_DIR_GMCHALLENGE = \'./data\'\n\n\nclass TestMRIDataset(object):\n    @pytest.fixture\n    def gmsample_files(self):\n        mri_input_filename = os.path.join(ROOT_DIR_GMCHALLENGE,\n                                          \'site1-sc01-image.nii.gz\')\n        mri_gt_filename = os.path.join(ROOT_DIR_GMCHALLENGE,\n                                       \'site1-sc01-mask-r1.nii.gz\')\n        return mri_input_filename, mri_gt_filename\n\n    def test_pair_loading(self, gmsample_files):\n        mri_input_filename, mri_gt_filename = gmsample_files\n        pair = mt_datasets.SegmentationPair2D(mri_input_filename,\n                                              mri_gt_filename)\n        assert pair is not None\n\n    def test_pair_slicing(self, gmsample_files):\n        mri_input_filename, mri_gt_filename = gmsample_files\n        pair = mt_datasets.SegmentationPair2D(mri_input_filename,\n                                              mri_gt_filename)\n        slice_pair = pair.get_pair_slice(0)\n        input_slice = slice_pair[""input""]\n        gt_slice = slice_pair[""gt""]\n        \n        assert input_slice.shape == (200, 200)\n        assert gt_slice.shape == input_slice.shape\n        assert input_slice[0][0] == pytest.approx(651.5, 0.1)\n\n    def test_dataset(self, gmsample_files):\n        mri_input_filename, mri_gt_filename = gmsample_files\n        filename_mapping = [(mri_input_filename, mri_gt_filename)]\n        dataset = mt_datasets.MRI2DSegmentationDataset(filename_mapping)\n        assert len(dataset) == 3\n\n        first_item = dataset[0]\n        assert isinstance(first_item, dict)\n        assert \'input\' in first_item\n        assert \'gt\' in first_item\n\n        input_slice = first_item[\'input\']\n        assert input_slice.getpixel((0, 0)) == pytest.approx(651.5, 0.1)\n\n    def test_dataset_transform(self, gmsample_files):\n        mri_input_filename, mri_gt_filename = gmsample_files\n        filename_mapping = [(mri_input_filename, mri_gt_filename)]\n        dataset = mt_datasets.MRI2DSegmentationDataset(filename_mapping,\n                                                       transform=mt_transforms.ToTensor())\n        first_item = dataset[0]\n        assert isinstance(first_item[\'input\'], torch.FloatTensor)\n        assert first_item[\'input\'].size() == (1, 200, 200)\n        assert first_item[\'input\'].size() == first_item[\'gt\'].size()\n\n    def test_dataset_loader(self, gmsample_files):\n        mri_input_filename, mri_gt_filename = gmsample_files\n        filename_mapping = [(mri_input_filename, mri_gt_filename)]\n        dataset = mt_datasets.MRI2DSegmentationDataset(filename_mapping,\n                                                       transform=mt_transforms.ToTensor())\n\n        dataloader = DataLoader(dataset, batch_size=4,\n                                shuffle=True, num_workers=1,\n                                collate_fn=mt_datasets.mt_collate)\n\n        minibatch = next(iter(dataloader))\n        assert len(minibatch) == 4\n        assert \'input\' in minibatch\n        assert \'gt\' in minibatch\n        assert \'input_metadata\' in minibatch\n        assert \'gt_metadata\' in minibatch\n\n    def test_gmchallenge_dataset(self):\n        composed_transform = transforms.Compose([\n            mt_transforms.CenterCrop2D((200, 200)),\n            mt_transforms.ToTensor(),\n        ])\n\n        dataset = mt_datasets.SCGMChallenge2D(root_dir=ROOT_DIR_GMCHALLENGE,\n                                              transform=composed_transform)\n        assert len(dataset) == 2204\n\n        dataset = mt_datasets.SCGMChallenge2D(root_dir=ROOT_DIR_GMCHALLENGE,\n                                              rater_ids=[4, ], subj_ids=[1, 2],\n                                              transform=composed_transform)\n        assert len(dataset) == 107\n\n        dataloader = DataLoader(dataset, batch_size=4,\n                                shuffle=True, num_workers=4,\n                                collate_fn=mt_datasets.mt_collate)\n        minibatch = next(iter(dataloader))\n        assert len(minibatch) == 4\n        assert minibatch[\'input\'].size() == (4, 1, 200, 200)\n\n        iterations = 0\n        for minbatch in dataloader:\n            iterations += 1\n        assert iterations == 27\n'"
tests/test_models.py,2,"b'import torch\nfrom torch.autograd import Variable\n\nfrom medicaltorch import models as mt_models\n\n\nclass TestModels(object):\n    def test_aspp(self):\n        model = mt_models.NoPoolASPP()\n        random_data = torch.randn(1, 1, 200, 200)\n        random_var = Variable(random_data)\n        output = model(random_var)\n        assert output.size() == (1, 1, 200, 200)\n'"
docs/source/conf.py,6,"b'# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/stable/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nfrom unittest.mock import MagicMock\n\nsys.path.insert(0, os.path.abspath(\'../..\'))\n\n\n# -- Project information -----------------------------------------------------\n\nproject = \'medicaltorch\'\ncopyright = \'2018, Christian S. Perone\'\nauthor = \'Christian S. Perone\'\n\n# The short X.Y version\nversion = \'0.2\'\n# The full version, including alpha/beta/rc tags\nrelease = \'0.2\'\n\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.viewcode\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\nhtml_show_sourcelink = False\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path .\nexclude_patterns = []\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\nMOCK_MODULES = [\n    # modules to mock\n    \'torch\',\n    \'torchvision\',\n    \'torch.utils\',\n    \'torch.nn\',\n    \'torchvision.transforms\',\n    \'torch.nn.functional\',\n    \'torchvision.transforms.functional\',\n    \'torch.utils.data\',\n    \'torch._six\',\n]\n\nMOCK_CLASSES = [\n    # classes you are inheriting from\n    ""Dataset"",\n    ""Module"",\n]\n\n\nclass Mock(MagicMock):\n    @classmethod\n    def __getattr__(cls, name):\n        if name in MOCK_CLASSES:\n            return object\n        return MagicMock()\n\nfor mod_name in MOCK_MODULES:\n    sys.modules[mod_name] = Mock()\n\n#autodoc_mock_imports = [""torch"", ""torchvision""]\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'alabaster\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\nhtml_theme_options = {\n    \'logo\': \'img/logo_textonly.png\',\n    \'github_user\': \'perone\',\n    \'github_repo\': \'medicaltorch\',\n    \'github_button\': True,\n    \'github_banner\': False,\n    \'github_type\': \'star\',\n    \'logo_name\': False,\n}\n\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\nhtml_sidebars = {\n    \'**\': [\n        \'about.html\',\n        \'navigation.html\',\n        \'relations.html\',\n        \'searchbox.html\',\n        \'donate.html\',\n    ]\n}\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'medicaltorchdoc\'\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'medicaltorch.tex\', \'medicaltorch Documentation\',\n     \'Christian S. Perone\', \'manual\'),\n]\n\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'medicaltorch\', \'medicaltorch Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'medicaltorch\', \'medicaltorch Documentation\',\n     author, \'medicaltorch\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n# -- Extension configuration -------------------------------------------------'"
