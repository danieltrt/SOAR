file_path,api_count,code
setup.py,1,"b""import os\n\nfrom setuptools import setup\n\nBASE_VERSION = '1.3.2'  # update regardless whether you update keras or pytorch or both.\nFRAMEWORK = os.getenv('FRAMEWORK', 'keras')  # keras, pytorch.\n\n# common packages.\nINSTALL_REQUIRES = [\n    'numpy>=1.18.1',\n    'pandas>=0.25.3',\n    'matplotlib>=3.0'\n]\n\nif FRAMEWORK == 'keras':\n    LIB_PACKAGE = ['nbeats_keras']\n    INSTALL_REQUIRES.extend([\n        'keras',\n        'tensorflow==2.0'\n    ])\n\nelif FRAMEWORK == 'pytorch':\n    LIB_PACKAGE = ['nbeats_pytorch']\n    INSTALL_REQUIRES.extend([\n        'torch',\n        'torchvision'\n    ])\nelse:\n    raise ValueError('Unknown framework.')\n\nsetup(\n    name=f'nbeats-{FRAMEWORK}',\n    version=BASE_VERSION,\n    description='N-Beats',\n    author='Philippe Remy (Pytorch), Jean Sebastien Dhr (Keras)',\n    license='MIT',\n    long_description_content_type='text/markdown',\n    long_description=open('README.md').read(),\n    packages=LIB_PACKAGE,\n    install_requires=INSTALL_REQUIRES\n)\n"""
examples/data.py,0,"b'import csv\nfrom os import listdir\n\nimport numpy as np\n\n\ndef dummy_data_generator(backcast_length, forecast_length, signal_type=\'seasonality\', random=False, batch_size=32):\n    def get_x_y():\n        lin_space = np.linspace(-backcast_length, forecast_length, backcast_length + forecast_length)\n        if random:\n            offset = np.random.standard_normal() * 0.1\n        else:\n            offset = 1\n        if signal_type == \'trend\':\n            a = lin_space + offset\n        elif signal_type == \'seasonality\':\n            a = np.cos(2 * np.random.randint(low=1, high=3) * np.pi * lin_space)\n            a += np.cos(2 * np.random.randint(low=2, high=4) * np.pi * lin_space)\n            a += lin_space * offset + np.random.rand() * 0.1\n        elif signal_type == \'cos\':\n            a = np.cos(2 * np.pi * lin_space)\n        else:\n            raise Exception(\'Unknown signal type.\')\n\n        x = a[:backcast_length]\n        y = a[backcast_length:]\n\n        min_x, max_x = np.minimum(np.min(x), 0), np.max(np.abs(x))\n\n        x -= min_x\n        y -= min_x\n\n        x /= max_x\n        y /= max_x\n\n        return x, y\n\n    def gen():\n        while True:\n            xx = []\n            yy = []\n            for i in range(batch_size):\n                x, y = get_x_y()\n                xx.append(x)\n                yy.append(y)\n            yield np.array(xx), np.array(yy)\n\n    return gen()\n\n\ndef get_m4_data(backcast_length, forecast_length, is_training=True):\n    # https://www.mcompetitions.unic.ac.cy/the-dataset/\n\n    if is_training:\n        filename = \'data/m4/train/Daily-train.csv\'\n    else:\n        filename = \'data/m4/val/Daily-test.csv\'\n\n    x = np.array([]).reshape(0, backcast_length)\n    y = np.array([]).reshape(0, forecast_length)\n    x_tl = []\n    headers = True\n    with open(filename, ""r"") as file:\n        reader = csv.reader(file, delimiter=\',\')\n        for line in reader:\n            line = line[1:]\n            if not headers:\n                x_tl.append(line)\n            if headers:\n                headers = False\n    x_tl_tl = np.array(x_tl)\n    for i in range(x_tl_tl.shape[0]):\n        if len(x_tl_tl[i]) < backcast_length + forecast_length:\n            continue\n        time_series = np.array(x_tl_tl[i])\n        time_series = [float(s) for s in time_series if s != \'\']\n        time_series_cleaned = np.array(time_series)\n        if is_training:\n            time_series_cleaned_forlearning_x = np.zeros((1, backcast_length))\n            time_series_cleaned_forlearning_y = np.zeros((1, forecast_length))\n            j = np.random.randint(backcast_length, time_series_cleaned.shape[0] + 1 - forecast_length)\n            time_series_cleaned_forlearning_x[0, :] = time_series_cleaned[j - backcast_length: j]\n            time_series_cleaned_forlearning_y[0, :] = time_series_cleaned[j:j + forecast_length]\n        else:\n            time_series_cleaned_forlearning_x = np.zeros(\n                (time_series_cleaned.shape[0] + 1 - (backcast_length + forecast_length), backcast_length))\n            time_series_cleaned_forlearning_y = np.zeros(\n                (time_series_cleaned.shape[0] + 1 - (backcast_length + forecast_length), forecast_length))\n            for j in range(backcast_length, time_series_cleaned.shape[0] + 1 - forecast_length):\n                time_series_cleaned_forlearning_x[j - backcast_length, :] = time_series_cleaned[j - backcast_length:j]\n                time_series_cleaned_forlearning_y[j - backcast_length, :] = time_series_cleaned[j: j + forecast_length]\n        x = np.vstack((x, time_series_cleaned_forlearning_x))\n        y = np.vstack((y, time_series_cleaned_forlearning_y))\n\n    return x, y\n\n\ndef dummy_data_generator_multivariate(backcast_length, forecast_length, signal_type=\'seasonality\', random=False,\n                                      batch_size=32):\n    def get_x_y():\n        lin_space = np.linspace(-backcast_length, forecast_length, backcast_length + forecast_length)\n        if random:\n            offset = np.random.standard_normal() * 0.1\n        else:\n            offset = 1\n        if signal_type == \'trend\':\n            a = lin_space + offset\n        elif signal_type == \'seasonality\':\n            a = np.cos(2 * np.random.randint(low=1, high=3) * np.pi * lin_space)\n            a += np.cos(2 * np.random.randint(low=2, high=4) * np.pi * lin_space)\n            a += lin_space * offset + np.random.rand() * 0.1\n        elif signal_type == \'cos\':\n            a = np.cos(2 * np.pi * lin_space)\n        else:\n            raise Exception(\'Unknown signal type.\')\n\n        x = a[:backcast_length]\n        y = a[backcast_length:]\n\n        min_x, max_x = np.minimum(np.min(x), 0), np.max(np.abs(x))\n\n        x -= min_x\n        y -= min_x\n\n        x /= max_x\n        y /= max_x\n\n        return x, y\n\n    def gen():\n        while True:\n            xx = []\n            yy = []\n            for i in range(batch_size):\n                x, y = get_x_y()\n                xx.append(x)\n                yy.append(y)\n            yield np.array(xx), np.array(yy)\n\n    return gen()\n\n\ndef get_m4_data_multivariate(backcast_length, forecast_length, is_training=True):\n    # to be downloaded from https://www.mcompetitions.unic.ac.cy/the-dataset/\n\n    filename = \'../examples/data/m4/train/Daily-train.csv\'\n    x_tl = []\n    x_max = []\n    headers = True\n    with open(filename, ""r"") as file:\n        reader = csv.reader(file, delimiter=\',\')\n        for line in reader:\n            line = line[1:]\n            if not headers:\n                x_tl.append(line)\n            if headers:\n                headers = False\n    x_tl_tl = np.array(x_tl)\n    for i in range(x_tl_tl.shape[0]):\n        if len(x_tl_tl[i]) < backcast_length + forecast_length:\n            continue\n        time_series = np.array(x_tl_tl[i])\n        time_series = [float(s) for s in time_series if s != \'\']\n        x_max.append(np.max(time_series))\n    x_max = np.max(x_max)\n\n    if is_training:\n        filename = \'../examples/data/m4/train/Daily-train.csv\'\n    else:\n        filename = \'../examples/data/m4/val/Daily-test.csv\'\n\n    x = np.array([]).reshape(0, backcast_length)\n    y = np.array([]).reshape(0, forecast_length)\n    x_tl = []\n    headers = True\n    with open(filename, ""r"") as file:\n        reader = csv.reader(file, delimiter=\',\')\n        for line in reader:\n            line = line[1:]\n            if not headers:\n                x_tl.append(line)\n            if headers:\n                headers = False\n    x_tl_tl = np.array(x_tl)\n    for i in range(x_tl_tl.shape[0]):\n        if len(x_tl_tl[i]) < backcast_length + forecast_length:\n            continue\n        time_series = np.array(x_tl_tl[i])\n        time_series = [float(s) for s in time_series if s != \'\']\n        time_series = time_series / x_max\n        if is_training:\n            time_series_cleaned_forlearning_x = np.zeros((1, backcast_length))\n            time_series_cleaned_forlearning_y = np.zeros((1, forecast_length))\n            j = np.random.randint(backcast_length, time_series.shape[0] + 1 - forecast_length)\n            time_series_cleaned_forlearning_x[0, :] = time_series[j - backcast_length: j]\n            time_series_cleaned_forlearning_y[0, :] = time_series[j:j + forecast_length]\n        else:\n            time_series_cleaned_forlearning_x = np.zeros(\n                (time_series.shape[0] + 1 - (backcast_length + forecast_length), backcast_length))\n            time_series_cleaned_forlearning_y = np.zeros(\n                (time_series.shape[0] + 1 - (backcast_length + forecast_length), forecast_length))\n            for j in range(backcast_length, time_series.shape[0] + 1 - forecast_length):\n                time_series_cleaned_forlearning_x[j - backcast_length, :] = time_series[j - backcast_length:j]\n                time_series_cleaned_forlearning_y[j - backcast_length, :] = time_series[j: j + forecast_length]\n        x = np.vstack((x, time_series_cleaned_forlearning_x))\n        y = np.vstack((y, time_series_cleaned_forlearning_y))\n\n    return x.reshape(x.shape[0], x.shape[1], 1), None, y.reshape(y.shape[0], y.shape[1], 1)\n\n\ndef process_data(filename):\n    import wfdb\n    ecg_list = listdir(filename)\n    sample_list = [ecg[:-4] for ecg in ecg_list]\n    clean_sample_list = [ecg for ecg in sample_list if\n                         ecg not in [\'102-0\', \'ANNOTA\', \'REC\', \'SHA256SUMS\', \'mitd\', \'x_m\']]\n    all_samples = np.zeros((len(clean_sample_list), 650000, 2))\n    for idx, ecg in enumerate(clean_sample_list):\n        record = wfdb.rdrecord(filename + ecg)\n        all_samples[idx] = record.p_signal\n\n    return all_samples\n\n\ndef get_kcg_data(backcast_length, forecast_length, is_training=True):\n    # to be downloaded from https://physionet.org/content/mitdb/1.0.0/\n    # once downloaded should be put in ../examples/data/kcg/\n\n    dataset = process_data(filename=\'../examples/data/kcg/\')\n    x_max = np.amax(np.abs(dataset[:195, :, :]), axis=(0, 1))\n\n    if is_training:\n        dataset = dataset[:195, :, :]\n    else:\n        dataset = dataset[195:, 30000:30000 + backcast_length + forecast_length + 10, :]\n\n    x = np.array([]).reshape(0, backcast_length, 2)\n    y = np.array([]).reshape(0, forecast_length, 2)\n\n    for i in range(dataset.shape[0]):\n        if (dataset[i].shape[0] < backcast_length + forecast_length):\n            continue\n        time_series = dataset[i]\n        time_series = time_series / x_max\n        if is_training:\n            time_series_cleaned_forlearning_x = np.zeros((1, backcast_length, 2))\n            time_series_cleaned_forlearning_y = np.zeros((1, forecast_length, 2))\n            j = np.random.randint(backcast_length, time_series.shape[0] + 1 - forecast_length)\n            time_series_cleaned_forlearning_x[0] = time_series[j - backcast_length: j, :]\n            time_series_cleaned_forlearning_y[0] = time_series[j:j + forecast_length, :]\n        else:\n            time_series_cleaned_forlearning_x = np.zeros(\n                (time_series.shape[0] + 1 - (backcast_length + forecast_length), backcast_length, 2))\n            time_series_cleaned_forlearning_y = np.zeros(\n                (time_series.shape[0] + 1 - (backcast_length + forecast_length), forecast_length, 2))\n            for j in range(backcast_length, time_series.shape[0] + 1 - forecast_length):\n                time_series_cleaned_forlearning_x[j - backcast_length] = time_series[j - backcast_length:j, :]\n                time_series_cleaned_forlearning_y[j - backcast_length] = time_series[j: j + forecast_length, :]\n\n        x = np.vstack((x, time_series_cleaned_forlearning_x))\n        y = np.vstack((y, time_series_cleaned_forlearning_y))\n\n    return x, None, y\n\n\ndef process_data_price():\n    filename = \'../examples/data/nrj/EPEX_spot_DA_auction_hour_prices_20070720-20170831.csv\'\n\n    x_tl = []\n    headers = True\n    with open(filename, ""r"") as file:\n        reader = csv.reader(file, delimiter=\',\')\n        for line in reader:\n            if not headers:\n                x_tl.append(line)\n            if headers:\n                headers = False\n    x_tl = [float(x_tl[i][1]) for i in range(len(x_tl)) if \'00:00:00\' in x_tl[i][0]]\n    x_tl = np.array(x_tl)\n\n    return x_tl\n\n\ndef process_data_load():\n    filename = \'../examples/data/nrj/20150101-20170830-forecast_load_renewable_gen.csv\'\n\n    x_tl = []\n    headers = True\n    with open(filename, ""r"") as file:\n        reader = csv.reader(file, delimiter=\',\')\n        for line in reader:\n            if not headers:\n                x_tl.append(line)\n            if headers:\n                headers = False\n\n    x_tl = [x_tl[i][1] for i in range(len(x_tl)) if \'00:00:00\' in x_tl[i][0]]\n    x_tl = [float(x_tl[i]) if x_tl[i] != \'\' else 0. for i in range(len(x_tl))]\n    x_tl[x_tl == 0] = np.mean(x_tl)\n    x_tl = np.array(x_tl)\n\n    return x_tl\n\n\ndef process_data_gen():\n    filename = \'../examples/data/nrj/20150101-20170830-gen_per_prod_type.csv\'\n\n    x_tl = []\n    headers = True\n    with open(filename, ""r"") as file:\n        reader = csv.reader(file, delimiter=\',\')\n        for line in reader:\n            if not headers:\n                x_tl.append(line)\n            if headers:\n                headers = False\n\n    x_tl = [x_tl[i][1] for i in range(len(x_tl)) if \'00:00:00\' in x_tl[i][0]]\n    x_tl = [float(x_tl[i]) if x_tl[i] != \'\' else 0. for i in range(len(x_tl))]\n    x_tl[x_tl == 0] = np.mean(x_tl)\n    x_tl = np.array(x_tl)\n\n    return x_tl\n\n\ndef get_x_y_data(backcast_length, forecast_length):\n    x = np.array([]).reshape(0, backcast_length)\n    y = np.array([]).reshape(0, forecast_length)\n\n    time_series = process_data_price()[:-1]\n\n    time_series_cleaned_forlearning_x = np.zeros(\n        (time_series.shape[0] + 1 - (backcast_length + forecast_length), backcast_length))\n    time_series_cleaned_forlearning_y = np.zeros(\n        (time_series.shape[0] + 1 - (backcast_length + forecast_length), forecast_length))\n    for j in range(backcast_length, time_series.shape[0] + 1 - forecast_length):\n        time_series_cleaned_forlearning_x[j - backcast_length, :] = time_series[j - backcast_length:j]\n        time_series_cleaned_forlearning_y[j - backcast_length, :] = time_series[j: j + forecast_length]\n    x = np.vstack((x, time_series_cleaned_forlearning_x))\n    y = np.vstack((y, time_series_cleaned_forlearning_y))\n\n    return x.reshape((x.shape[0], x.shape[1], 1)), y.reshape((y.shape[0], y.shape[1], 1))\n\n\ndef get_exo_var_data(backcast_length, forecast_length):\n    e1 = np.array([]).reshape(0, backcast_length)\n    e2 = np.array([]).reshape(0, backcast_length)\n\n    time_series_1 = process_data_gen()\n    time_series_2 = process_data_load()\n    time_series_cleaned_forlearning_1 = np.zeros(\n        (time_series_1.shape[0] + 1 - (backcast_length + forecast_length), backcast_length))\n    time_series_cleaned_forlearning_2 = np.zeros(\n        (time_series_1.shape[0] + 1 - (backcast_length + forecast_length), backcast_length))\n    for j in range(backcast_length, time_series_1.shape[0] + 1 - forecast_length):\n        time_series_cleaned_forlearning_1[j - backcast_length, :] = time_series_1[j - backcast_length:j]\n        time_series_cleaned_forlearning_2[j - backcast_length, :] = time_series_2[j - backcast_length:j]\n    e1 = np.vstack((e1, time_series_cleaned_forlearning_1))\n    e2 = np.vstack((e2, time_series_cleaned_forlearning_2))\n\n    return e1, e2\n\n\ndef get_nrj_data(backcast_length, forecast_length, is_training=True):\n    x, y = get_x_y_data(backcast_length, forecast_length)\n    e1, e2 = get_exo_var_data(backcast_length, forecast_length)\n\n    x_max = np.amax(np.abs(x[:90 * x.shape[0] // 100, :, :]), axis=(0, 1))\n    e1_max = np.amax(np.abs(e1[:90 * x.shape[0] // 100, :]), axis=(0, 1))\n    e2_max = np.amax(np.abs(e2[:90 * x.shape[0] // 100, :]), axis=(0, 1))\n\n    x = x / x_max\n    y = y / x_max\n    e1 = e1 / e1_max\n    e2 = e2 / e2_max\n\n    e = np.concatenate((e1.reshape((e1.shape[0], e1.shape[1], 1)), e2.reshape((e2.shape[0], e2.shape[1], 1))), axis=-1)\n\n    if is_training:\n        return x[:90 * x.shape[0] // 100], e[:90 * x.shape[0] // 100], y[:90 * x.shape[0] // 100]\n    else:\n        return x[90 * x.shape[0] // 100:], e[90 * x.shape[0] // 100:], y[90 * x.shape[0] // 100:]\n'"
examples/rnn_example.py,0,"b""# https://www.kaggle.com/stytch16/jena-climate-2009-2016\n# https://www.kaggle.com/pankrzysiu/6-3-1-a-temperature-forecasting-problem\n# https://www.tensorflow.org/tutorials/structured_data/time_series\n\n# On the hourly temperature dataset and\n# without much tuning, N-Beats can achieve a 23% reduction in loss compared to the last value benchmark.\n#\n\nimport os\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom keras.callbacks import Callback\nfrom nbeats_keras.model import NBeatsNet\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\n\nzip_path = tf.keras.utils.get_file(\n    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n    fname='jena_climate_2009_2016.csv.zip',\n    extract=True)\ncsv_path, _ = os.path.splitext(zip_path)\ndf = pd.read_csv(csv_path)\nprint(csv_path)\nprint(df.head())\nprint(len(df))\n\n\ndef univariate_data(dataset, start_index, end_index, history_size, target_size):\n    data = []\n    labels = []\n\n    start_index = start_index + history_size\n    if end_index is None:\n        end_index = len(dataset) - target_size\n\n    for i in range(start_index, end_index):\n        indices = range(i - history_size, i)\n        # Reshape data from (history_size,) to (history_size, 1)\n        data.append(np.reshape(dataset[indices], (history_size, 1)))\n        labels.append(dataset[i + target_size])\n    data = np.array(data)\n    labels = np.array(labels)\n    return data.reshape(data.shape[0], data.shape[1], 1), labels.reshape(labels.shape[0], 1, 1)\n\n\nTRAIN_SPLIT = 300000\ntf.random.set_seed(13)\n\nuni_data = df['T (degC)']\nuni_data.index = df['Date Time']\n\nuni_data = uni_data.values\n\n\nclass DataNormalizer:\n\n    def __init__(self, train):\n        self.uni_train_mean = train.mean()\n        self.uni_train_std = train.std()\n\n    def apply(self, x):\n        return (x - self.uni_train_mean) / self.uni_train_std\n\n    def apply_inv(self, x):\n        return x * self.uni_train_std + self.uni_train_mean\n\n\ndn = DataNormalizer(train=uni_data[:TRAIN_SPLIT])\nuni_train_mean = uni_data[:TRAIN_SPLIT].mean()\nuni_train_std = uni_data[:TRAIN_SPLIT].std()\n\nuni_data = dn.apply(uni_data)\n\n# plt.plot(uni_data)\n# plt.show()\n\nunivariate_past_history = 20\nunivariate_future_target = 0\n\nx_train_uni, y_train_uni = univariate_data(uni_data, 0, TRAIN_SPLIT,\n                                           univariate_past_history,\n                                           univariate_future_target)\nx_val_uni, y_val_uni = univariate_data(uni_data, TRAIN_SPLIT, None,\n                                       univariate_past_history,\n                                       univariate_future_target)\n\nprint('x_train_uni.shape=', x_train_uni.shape)\nprint('y_train_uni.shape=', y_train_uni.shape)\nprint('x_val_uni.shape=', x_val_uni.shape)\nprint('y_val_uni.shape=', y_val_uni.shape)\n\nb_val_uni = np.mean(x_val_uni, axis=1)[..., 0]\nprint(np.mean(np.abs(b_val_uni - y_val_uni)))\nprint(np.mean(np.abs(dn.apply_inv(b_val_uni) - dn.apply_inv(y_val_uni))))\n\nb2_val_uni = x_val_uni[:, -1, 0]\nprint(np.mean(np.abs(b2_val_uni - y_val_uni)))\nprint(np.mean(np.abs(dn.apply_inv(b2_val_uni) - dn.apply_inv(y_val_uni))))\n\nm = NBeatsNet(stack_types=(NBeatsNet.GENERIC_BLOCK, NBeatsNet.GENERIC_BLOCK, NBeatsNet.GENERIC_BLOCK),\n              nb_blocks_per_stack=3,\n              forecast_length=1,\n              backcast_length=univariate_past_history,\n              thetas_dim=(15, 15, 15),\n              share_weights_in_stack=False,\n              hidden_layer_units=384)\nm.compile_model(loss='mae', learning_rate=1e-4)\nprint('compile_model()')\nprint('fit()')\n\n\nclass EvaluateModelCallback(Callback):\n\n    def on_epoch_end(self, epoch, logs=None):\n        b3_val_uni = m.predict(x_val_uni)\n        print(f'[{epoch}] b3_val_uni.shape=', b3_val_uni.shape)\n        print(np.mean(np.abs(b3_val_uni - y_val_uni)))\n        print(np.mean(np.abs(dn.apply_inv(b3_val_uni) - dn.apply_inv(y_val_uni))))\n        print('*' * 80)\n\n\nm.fit(x_train_uni, y_train_uni,\n      epochs=20, validation_split=0.1, shuffle=True,\n      callbacks=[EvaluateModelCallback()])\n"""
examples/simple_example.py,0,"b""import numpy as np\n\nfrom nbeats_keras.model import NBeatsNet\n\n\ndef main():\n    # https://keras.io/layers/recurrent/\n    num_samples, time_steps, input_dim, output_dim = 50_000, 10, 1, 1\n\n    # Definition of the model.\n    model = NBeatsNet(backcast_length=time_steps, forecast_length=output_dim,\n                      stack_types=(NBeatsNet.GENERIC_BLOCK, NBeatsNet.GENERIC_BLOCK), nb_blocks_per_stack=2,\n                      thetas_dim=(4, 4), share_weights_in_stack=True, hidden_layer_units=64)\n\n    # Definition of the objective function and the optimizer.\n    model.compile_model(loss='mae', learning_rate=1e-5)\n\n    # Definition of the data. The problem to solve is to find f such as | f(x) - y | -> 0.\n    x = np.random.uniform(size=(num_samples, time_steps, input_dim))\n    y = np.mean(x, axis=1, keepdims=True)\n\n    # Split data into training and testing datasets.\n    c = num_samples // 10\n    x_train, y_train, x_test, y_test = x[c:], y[c:], x[:c], y[:c]\n\n    # Train the model.\n    model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=2, batch_size=128)\n\n    # Save the model for later.\n    model.save('n_beats_model.h5')\n\n    # Predict on the testing set.\n    predictions = model.predict(x_test)\n    print(predictions.shape)\n\n    # Load the model.\n    model2 = NBeatsNet.load('n_beats_model.h5')\n\n    predictions2 = model2.predict(x_test)\n    np.testing.assert_almost_equal(predictions, predictions2)\n\n\nif __name__ == '__main__':\n    main()\n"""
examples/trainer_keras.py,0,"b""import os\nfrom argparse import ArgumentParser\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom data import dummy_data_generator_multivariate, get_m4_data_multivariate, get_nrj_data, get_kcg_data\n\nfrom nbeats_keras.model import NBeatsNet\n\n\ndef get_script_arguments():\n    parser = ArgumentParser()\n    parser.add_argument('--task', choices=['m4', 'kcg', 'nrj', 'dummy'], required=True)\n    parser.add_argument('--test', action='store_true')\n    return parser.parse_args()\n\n\ndef get_metrics(y_true, y_hat):\n    error = np.mean(np.square(y_true - y_hat))\n    smape = np.mean(2 * np.abs(y_true - y_hat) / (np.abs(y_true) + np.abs(y_hat)))\n    return smape, error\n\n\ndef ensure_results_dir():\n    if not os.path.exists('results/test'):\n        os.makedirs('results/test')\n\n\ndef reshape_array(x):\n    assert len(x.shape) == 2, 'input np.array should be in the format: samples, timesteps'\n    if len(x.shape) == 2:\n        nb_samples, nb_timestamps = x.shape\n        return x.reshape((nb_samples, nb_timestamps, 1))\n\n\ndef generate_data(backcast_length, forecast_length):\n    def gen(num_samples):\n        return next(dummy_data_generator_multivariate(backcast_length, forecast_length,\n                                                      signal_type='seasonality', random=True, batch_size=num_samples))\n\n    x_train, y_train = gen(6_000)\n    x_test, y_test = gen(1_000)\n\n    x_train, y_train, x_test, y_test = reshape_array(x_train), reshape_array(y_train), reshape_array(\n        x_test), reshape_array(y_test)\n    return x_train, None, y_train, x_test, None, y_test\n\n\ndef train_model(model: NBeatsNet, task: str, best_perf=np.inf, max_steps=10001, plot_results=100, is_test=False):\n    ensure_results_dir()\n    # if is_test then override max_steps argument\n    if is_test:\n        max_steps = 5\n\n    if task == 'dummy':\n        x_train, e_train, y_train, x_test, e_test, y_test = generate_data(model.backcast_length, model.forecast_length)\n    elif task == 'm4':\n        x_test, e_test, y_test = get_m4_data_multivariate(model.backcast_length, model.forecast_length,\n                                                          is_training=False)\n    elif task == 'kcg':\n        x_test, e_test, y_test = get_kcg_data(model.backcast_length, model.forecast_length, is_training=False)\n    elif task == 'nrj':\n        x_test, e_test, y_test = get_nrj_data(model.backcast_length, model.forecast_length, is_training=False)\n    else:\n        raise ValueError('Invalid task.')\n\n    print('x_test.shape=', x_test.shape)\n\n    x_train, y_train, e_train = None, None, None\n    for step in range(max_steps):\n        if task == 'dummy':\n            x_train, e_train, y_train, x_test, e_test, y_test = generate_data(model.backcast_length,\n                                                                              model.forecast_length)\n        elif task == 'm4':\n            x_train, e_train, y_train = get_m4_data_multivariate(model.backcast_length, model.forecast_length,\n                                                                 is_training=True)\n        elif task == 'kcg':\n            x_train, e_train, y_train = get_kcg_data(model.backcast_length, model.forecast_length, is_training=True)\n        elif task == 'nrj':\n            x_train, e_train, y_train = get_nrj_data(model.backcast_length, model.forecast_length, is_training=True)\n        else:\n            raise ValueError('Invalid task.')\n\n        if model.has_exog():\n            model.train_on_batch([x_train, e_train], y_train)\n        else:\n            model.train_on_batch(x_train, y_train)\n\n        if step % plot_results == 0:\n            print('step=', step)\n            model.save('results/n_beats_model_' + str(step) + '.h5')\n            if model.has_exog():\n                predictions = model.predict([x_train, e_train])\n                validation_predictions = model.predict([x_test, e_test])\n            else:\n                predictions = model.predict(x_train)\n                validation_predictions = model.predict(x_test)\n            smape = get_metrics(y_test, validation_predictions)[0]\n            print('smape=', smape)\n            if smape < best_perf:\n                best_perf = smape\n                model.save('results/n_beats_model_ongoing.h5')\n            for k in range(model.input_dim):\n                plot_keras_model_predictions(model, False, step, x_train[0, :, k], y_train[0, :, k],\n                                             predictions[0, :, k], axis=k)\n                plot_keras_model_predictions(model, True, step, x_test[0, :, k], y_test[0, :, k],\n                                             validation_predictions[0, :, k], axis=k)\n\n    model.save('results/n_beats_model.h5')\n\n    if model.has_exog():\n        predictions = model.predict([x_train, e_train])\n        validation_predictions = model.predict([x_test, e_test])\n    else:\n        predictions = model.predict(x_train)\n        validation_predictions = model.predict(x_test)\n\n    for k in range(model.input_dim):\n        plot_keras_model_predictions(model, False, max_steps, x_train[10, :, k], y_train[10, :, k],\n                                     predictions[10, :, k], axis=k)\n        plot_keras_model_predictions(model, True, max_steps, x_test[10, :, k], y_test[10, :, k],\n                                     validation_predictions[10, :, k], axis=k)\n    print('smape=', get_metrics(y_test, validation_predictions)[0])\n    print('error=', get_metrics(y_test, validation_predictions)[1])\n\n\ndef plot_keras_model_predictions(model, is_test, step, backcast, forecast, prediction, axis):\n    legend = ['backcast', 'forecast', 'predictions of forecast']\n    if is_test:\n        title = 'results/test/' + 'step_' + str(step) + '_axis_' + str(axis) + '.png'\n    else:\n        title = 'results/' + 'step_' + str(step) + '_axis_' + str(axis) + '.png'\n    plt.figure()\n    plt.grid(True)\n    x_y = np.concatenate([backcast, forecast], axis=-1).flatten()\n    plt.plot(list(range(model.backcast_length)), backcast.flatten(), color='b')\n    plt.plot(list(range(len(x_y) - model.forecast_length, len(x_y))), forecast.flatten(), color='g')\n    plt.plot(list(range(len(x_y) - model.forecast_length, len(x_y))), prediction.flatten(), color='r')\n    plt.scatter(range(len(x_y)), x_y.flatten(), color=['b'] * model.backcast_length + ['g'] * model.forecast_length)\n    plt.scatter(list(range(len(x_y) - model.forecast_length, len(x_y))), prediction.flatten(),\n                color=['r'] * model.forecast_length)\n    plt.legend(legend)\n    plt.savefig(title)\n    plt.close()\n\n\ndef main():\n    args = get_script_arguments()\n\n    if args.task in ['m4', 'dummy']:\n        model = NBeatsNet(backcast_length=10, forecast_length=1,\n                          stack_types=(NBeatsNet.GENERIC_BLOCK, NBeatsNet.GENERIC_BLOCK), nb_blocks_per_stack=2,\n                          thetas_dim=(4, 4), share_weights_in_stack=True, hidden_layer_units=128)\n    elif args.task == 'kcg':\n        model = NBeatsNet(input_dim=2, backcast_length=360, forecast_length=10,\n                          stack_types=(NBeatsNet.TREND_BLOCK, NBeatsNet.SEASONALITY_BLOCK), nb_blocks_per_stack=3,\n                          thetas_dim=(4, 8), share_weights_in_stack=False,\n                          hidden_layer_units=256)\n    elif args.task == 'nrj':\n        model = NBeatsNet(input_dim=1, exo_dim=2, backcast_length=10, forecast_length=1,\n                          stack_types=(NBeatsNet.TREND_BLOCK, NBeatsNet.SEASONALITY_BLOCK), nb_blocks_per_stack=2,\n                          thetas_dim=(4, 8), share_weights_in_stack=False, hidden_layer_units=128,\n                          nb_harmonics=10)\n    else:\n        raise ValueError('Unknown task.')\n\n    model.compile_model(loss='mae', learning_rate=1e-5)\n    train_model(model, args.task, is_test=args.test)\n\n\nif __name__ == '__main__':\n    main()\n"""
examples/trainer_pytorch.py,9,"b""import os\nfrom argparse import ArgumentParser\n\nimport matplotlib.pyplot as plt\nimport torch\nfrom data import get_m4_data, dummy_data_generator\nfrom torch import optim\nfrom torch.nn import functional as F\n\nfrom nbeats_pytorch.model import NBeatsNet\n\nCHECKPOINT_NAME = 'nbeats-training-checkpoint.th'\n\n\ndef get_script_arguments():\n    parser = ArgumentParser(description='N-Beats')\n    parser.add_argument('--disable-cuda', action='store_true', help='Disable CUDA')\n    parser.add_argument('--disable-plot', action='store_true', help='Disable interactive plots')\n    parser.add_argument('--task', choices=['m4', 'dummy'], required=True)\n    parser.add_argument('--test', action='store_true')\n    return parser.parse_args()\n\n\ndef split(arr, size):\n    arrays = []\n    while len(arr) > size:\n        slice_ = arr[:size]\n        arrays.append(slice_)\n        arr = arr[size:]\n    arrays.append(arr)\n    return arrays\n\n\ndef batcher(dataset, batch_size, infinite=False):\n    while True:\n        x, y = dataset\n        for x_, y_ in zip(split(x, batch_size), split(y, batch_size)):\n            yield x_, y_\n        if not infinite:\n            break\n\n\ndef main():\n    args = get_script_arguments()\n    device = torch.device('cuda') if not args.disable_cuda and torch.cuda.is_available() else torch.device('cpu')\n    forecast_length = 10\n    backcast_length = 5 * forecast_length\n    batch_size = 4  # greater than 4 for viz\n\n    if args.task == 'm4':\n        data_gen = batcher(get_m4_data(backcast_length, forecast_length), batch_size=batch_size, infinite=True)\n    elif args.task == 'dummy':\n        data_gen = dummy_data_generator(backcast_length, forecast_length,\n                                        signal_type='seasonality', random=True,\n                                        batch_size=batch_size)\n    else:\n        raise Exception('Unknown task.')\n\n    print('--- Model ---')\n    net = NBeatsNet(device=device,\n                    stack_types=[NBeatsNet.TREND_BLOCK, NBeatsNet.SEASONALITY_BLOCK, NBeatsNet.GENERIC_BLOCK],\n                    forecast_length=forecast_length,\n                    thetas_dims=[2, 8, 3],\n                    nb_blocks_per_stack=3,\n                    backcast_length=backcast_length,\n                    hidden_layer_units=1024,\n                    share_weights_in_stack=False,\n                    nb_harmonics=None)\n\n    optimiser = optim.Adam(net.parameters())\n\n    def plot_model(x, target, grad_step):\n        if not args.disable_plot:\n            print('plot()')\n            plot(net, x, target, backcast_length, forecast_length, grad_step)\n\n    max_grad_steps = 10000\n    if args.test:\n        max_grad_steps = 5\n\n    simple_fit(net, optimiser, data_gen, plot_model, device, max_grad_steps)\n\n\ndef simple_fit(net, optimiser, data_generator, on_save_callback, device, max_grad_steps=10000):\n    print('--- Training ---')\n    initial_grad_step = load(net, optimiser)\n    for grad_step, (x, target) in enumerate(data_generator):\n        grad_step += initial_grad_step\n        optimiser.zero_grad()\n        net.train()\n        backcast, forecast = net(torch.tensor(x, dtype=torch.float).to(device))\n        loss = F.mse_loss(forecast, torch.tensor(target, dtype=torch.float).to(device))\n        loss.backward()\n        optimiser.step()\n        print(f'grad_step = {str(grad_step).zfill(6)}, loss = {loss.item():.6f}')\n        if grad_step % 1000 == 0 or (grad_step < 1000 and grad_step % 100 == 0):\n            with torch.no_grad():\n                save(net, optimiser, grad_step)\n                if on_save_callback is not None:\n                    on_save_callback(x, target, grad_step)\n        if grad_step > max_grad_steps:\n            print('Finished.')\n            break\n\n\ndef save(model, optimiser, grad_step):\n    torch.save({\n        'grad_step': grad_step,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimiser.state_dict(),\n    }, CHECKPOINT_NAME)\n\n\ndef load(model, optimiser):\n    if os.path.exists(CHECKPOINT_NAME):\n        checkpoint = torch.load(CHECKPOINT_NAME)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimiser.load_state_dict(checkpoint['optimizer_state_dict'])\n        grad_step = checkpoint['grad_step']\n        print(f'Restored checkpoint from {CHECKPOINT_NAME}.')\n        return grad_step\n    return 0\n\n\ndef plot(net, x, target, backcast_length, forecast_length, grad_step):\n    net.eval()\n    _, f = net(torch.tensor(x, dtype=torch.float))\n    subplots = [221, 222, 223, 224]\n\n    plt.figure(1)\n    plt.subplots_adjust(top=0.88)\n    for i in range(4):\n        ff, xx, yy = f.cpu().numpy()[i], x[i], target[i]\n        plt.subplot(subplots[i])\n        plt.plot(range(0, backcast_length), xx, color='b')\n        plt.plot(range(backcast_length, backcast_length + forecast_length), yy, color='g')\n        plt.plot(range(backcast_length, backcast_length + forecast_length), ff, color='r')\n        # plt.title(f'step #{grad_step} ({i})')\n\n    output = 'n_beats_{}.png'.format(grad_step)\n    plt.savefig(output)\n    plt.clf()\n    print('Saved image to {}.'.format(output))\n\n\nif __name__ == '__main__':\n    main()\n"""
nbeats_keras/__init__.py,0,b''
nbeats_keras/model.py,0,"b'import numpy as np\nfrom keras import backend as K\nfrom keras.layers import Concatenate\nfrom keras.layers import Input, Dense, Lambda, Subtract, Add, Reshape\nfrom keras.models import Model\nfrom keras.optimizers import Adam\n\n\nclass NBeatsNet:\n    GENERIC_BLOCK = \'generic\'\n    TREND_BLOCK = \'trend\'\n    SEASONALITY_BLOCK = \'seasonality\'\n\n    def __init__(self,\n                 input_dim=1,\n                 exo_dim=0,\n                 backcast_length=10,\n                 forecast_length=2,\n                 stack_types=(TREND_BLOCK, SEASONALITY_BLOCK),\n                 nb_blocks_per_stack=3,\n                 thetas_dim=(4, 8),\n                 share_weights_in_stack=False,\n                 hidden_layer_units=256,\n                 nb_harmonics=None\n                 ):\n\n        self.stack_types = stack_types\n        self.nb_blocks_per_stack = nb_blocks_per_stack\n        self.thetas_dim = thetas_dim\n        self.units = hidden_layer_units\n        self.share_weights_in_stack = share_weights_in_stack\n        self.backcast_length = backcast_length\n        self.forecast_length = forecast_length\n        self.input_dim = input_dim\n        self.exo_dim = exo_dim\n        self.input_shape = (self.backcast_length, self.input_dim)\n        self.exo_shape = (self.backcast_length, self.exo_dim)\n        self.output_shape = (self.forecast_length, self.input_dim)\n        self.weights = {}\n        self.nb_harmonics = nb_harmonics\n        assert len(self.stack_types) == len(self.thetas_dim)\n\n        x = Input(shape=self.input_shape, name=\'input_variable\')\n        x_ = {}\n        for k in range(self.input_dim):\n            x_[k] = Lambda(lambda z: z[..., k])(x)\n        e_ = {}\n        if self.has_exog():\n            e = Input(shape=self.exo_shape, name=\'exos_variables\')\n            for k in range(self.exo_dim):\n                e_[k] = Lambda(lambda z: z[..., k])(e)\n        else:\n            e = None\n        y_ = {}\n\n        for stack_id in range(len(self.stack_types)):\n            stack_type = self.stack_types[stack_id]\n            nb_poly = self.thetas_dim[stack_id]\n            for block_id in range(self.nb_blocks_per_stack):\n                backcast, forecast = self.create_block(x_, e_, stack_id, block_id, stack_type, nb_poly)\n                for k in range(self.input_dim):\n                    x_[k] = Subtract()([x_[k], backcast[k]])\n                    if stack_id == 0 and block_id == 0:\n                        y_[k] = forecast[k]\n                    else:\n                        y_[k] = Add()([y_[k], forecast[k]])\n\n        for k in range(self.input_dim):\n            y_[k] = Reshape(target_shape=(self.forecast_length, 1))(y_[k])\n        if self.input_dim > 1:\n            y_ = Concatenate(axis=-1)([y_[ll] for ll in range(self.input_dim)])\n        else:\n            y_ = y_[0]\n\n        if self.has_exog():\n            model = Model([x, e], y_)\n        else:\n            model = Model(x, y_)\n\n        model.summary()\n\n        self.n_beats = model\n\n    def has_exog(self):\n        return self.exo_dim > 0\n\n    @staticmethod\n    def load(filepath, custom_objects=None, compile=True):\n        from tensorflow.keras.models import load_model\n        return load_model(filepath, custom_objects, compile)\n\n    def _r(self, layer_with_weights, stack_id):\n        # mechanism to restore weights when block share the same weights.\n        # only useful when share_weights_in_stack=True.\n        if self.share_weights_in_stack:\n            layer_name = layer_with_weights.name.split(\'/\')[-1]\n            try:\n                reused_weights = self.weights[stack_id][layer_name]\n                return reused_weights\n            except KeyError:\n                pass\n            if stack_id not in self.weights:\n                self.weights[stack_id] = {}\n            self.weights[stack_id][layer_name] = layer_with_weights\n        return layer_with_weights\n\n    def create_block(self, x, e, stack_id, block_id, stack_type, nb_poly):\n\n        # register weights (useful when share_weights_in_stack=True)\n        def reg(layer):\n            return self._r(layer, stack_id)\n\n        # update name (useful when share_weights_in_stack=True)\n        def n(layer_name):\n            return \'/\'.join([str(stack_id), str(block_id), stack_type, layer_name])\n\n        backcast_ = {}\n        forecast_ = {}\n        d1 = reg(Dense(self.units, activation=\'relu\', name=n(\'d1\')))\n        d2 = reg(Dense(self.units, activation=\'relu\', name=n(\'d2\')))\n        d3 = reg(Dense(self.units, activation=\'relu\', name=n(\'d3\')))\n        d4 = reg(Dense(self.units, activation=\'relu\', name=n(\'d4\')))\n        if stack_type == \'generic\':\n            theta_b = reg(Dense(nb_poly, activation=\'linear\', use_bias=False, name=n(\'theta_b\')))\n            theta_f = reg(Dense(nb_poly, activation=\'linear\', use_bias=False, name=n(\'theta_f\')))\n            backcast = reg(Dense(self.backcast_length, activation=\'linear\', name=n(\'backcast\')))\n            forecast = reg(Dense(self.forecast_length, activation=\'linear\', name=n(\'forecast\')))\n        elif stack_type == \'trend\':\n            theta_f = theta_b = reg(Dense(nb_poly, activation=\'linear\', use_bias=False, name=n(\'theta_f_b\')))\n            backcast = Lambda(trend_model, arguments={""is_forecast"": False, ""backcast_length"": self.backcast_length,\n                                                      ""forecast_length"": self.forecast_length})\n            forecast = Lambda(trend_model, arguments={""is_forecast"": True, ""backcast_length"": self.backcast_length,\n                                                      ""forecast_length"": self.forecast_length})\n        else:  # \'seasonality\'\n            if self.nb_harmonics:\n                theta_b = reg(Dense(self.nb_harmonics, activation=\'linear\', use_bias=False, name=n(\'theta_b\')))\n            else:\n                theta_b = reg(Dense(self.forecast_length, activation=\'linear\', use_bias=False, name=n(\'theta_b\')))\n            theta_f = reg(Dense(self.forecast_length, activation=\'linear\', use_bias=False, name=n(\'theta_f\')))\n            backcast = Lambda(seasonality_model,\n                              arguments={""is_forecast"": False, ""backcast_length"": self.backcast_length,\n                                         ""forecast_length"": self.forecast_length})\n            forecast = Lambda(seasonality_model,\n                              arguments={""is_forecast"": True, ""backcast_length"": self.backcast_length,\n                                         ""forecast_length"": self.forecast_length})\n        for k in range(self.input_dim):\n            if self.has_exog():\n                d0 = Concatenate()([x[k]] + [e[ll] for ll in range(self.exo_dim)])\n            else:\n                d0 = x[k]\n            d1_ = d1(d0)\n            d2_ = d2(d1_)\n            d3_ = d3(d2_)\n            d4_ = d4(d3_)\n            theta_f_ = theta_f(d4_)\n            theta_b_ = theta_b(d4_)\n            backcast_[k] = backcast(theta_b_)\n            forecast_[k] = forecast(theta_f_)\n\n        return backcast_, forecast_\n\n    def compile_model(self, loss, learning_rate):\n        optimizer = Adam(lr=learning_rate)\n        self.compile(loss=loss, optimizer=optimizer)\n\n    def __getattr__(self, name):\n        # https://github.com/faif/python-patterns\n        # model.predict() instead of model.n_beats.predict()\n        # same for fit(), train_on_batch()...\n        attr = getattr(self.n_beats, name)\n\n        if not callable(attr):\n            return attr\n\n        def wrapper(*args, **kwargs):\n            return attr(*args, **kwargs)\n\n        return wrapper\n\n\ndef linear_space(backcast_length, forecast_length, fwd_looking=True):\n    ls = K.arange(-float(backcast_length), float(forecast_length), 1) / backcast_length\n    if fwd_looking:\n        ls = ls[backcast_length:]\n    else:\n        ls = ls[:backcast_length]\n    return ls\n\n\ndef seasonality_model(thetas, backcast_length, forecast_length, is_forecast):\n    p = thetas.get_shape().as_list()[-1]\n    p1, p2 = (p // 2, p // 2) if p % 2 == 0 else (p // 2, p // 2 + 1)\n    t = linear_space(backcast_length, forecast_length, fwd_looking=is_forecast)\n    s1 = K.stack([K.cos(2 * np.pi * i * t) for i in range(p1)], axis=0)\n    s2 = K.stack([K.sin(2 * np.pi * i * t) for i in range(p2)], axis=0)\n    if p == 1:\n        s = s2\n    else:\n        s = K.concatenate([s1, s2], axis=0)\n    s = K.cast(s, np.float32)\n    return K.dot(thetas, s)\n\n\ndef trend_model(thetas, backcast_length, forecast_length, is_forecast):\n    p = thetas.shape[-1]\n    t = linear_space(backcast_length, forecast_length, fwd_looking=is_forecast)\n    t = K.transpose(K.stack([t ** i for i in range(p)], axis=0))\n    t = K.cast(t, np.float32)\n    return K.dot(thetas, K.transpose(t))\n'"
nbeats_pytorch/__init__.py,0,b''
nbeats_pytorch/model.py,6,"b""import numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass NBeatsNet(nn.Module):\n    SEASONALITY_BLOCK = 'seasonality'\n    TREND_BLOCK = 'trend'\n    GENERIC_BLOCK = 'generic'\n\n    def __init__(self,\n                 device,\n                 stack_types=(TREND_BLOCK, SEASONALITY_BLOCK),\n                 nb_blocks_per_stack=3,\n                 forecast_length=5,\n                 backcast_length=10,\n                 thetas_dims=(4, 8),\n                 share_weights_in_stack=False,\n                 hidden_layer_units=256,\n                 nb_harmonics=None):\n        super(NBeatsNet, self).__init__()\n        self.forecast_length = forecast_length\n        self.backcast_length = backcast_length\n        self.hidden_layer_units = hidden_layer_units\n        self.nb_blocks_per_stack = nb_blocks_per_stack\n        self.share_weights_in_stack = share_weights_in_stack\n        self.nb_harmonics = nb_harmonics\n        self.stack_types = stack_types\n        self.stacks = []\n        self.thetas_dim = thetas_dims\n        self.parameters = []\n        self.device = device\n        print(f'| N-Beats')\n        for stack_id in range(len(self.stack_types)):\n            self.stacks.append(self.create_stack(stack_id))\n        self.parameters = nn.ParameterList(self.parameters)\n        self.to(self.device)\n\n    def create_stack(self, stack_id):\n        stack_type = self.stack_types[stack_id]\n        print(f'| --  Stack {stack_type.title()} (#{stack_id}) (share_weights_in_stack={self.share_weights_in_stack})')\n        blocks = []\n        for block_id in range(self.nb_blocks_per_stack):\n            block_init = NBeatsNet.select_block(stack_type)\n            if self.share_weights_in_stack and block_id != 0:\n                block = blocks[-1]  # pick up the last one when we share weights.\n            else:\n                block = block_init(self.hidden_layer_units, self.thetas_dim[stack_id],\n                                   self.device, self.backcast_length, self.forecast_length, self.nb_harmonics)\n                self.parameters.extend(block.parameters())\n            print(f'     | -- {block}')\n            blocks.append(block)\n        return blocks\n\n    @staticmethod\n    def select_block(block_type):\n        if block_type == NBeatsNet.SEASONALITY_BLOCK:\n            return SeasonalityBlock\n        elif block_type == NBeatsNet.TREND_BLOCK:\n            return TrendBlock\n        else:\n            return GenericBlock\n\n    def forward(self, backcast):\n        forecast = torch.zeros(size=(backcast.size()[0], self.forecast_length,))  # maybe batch size here.\n        for stack_id in range(len(self.stacks)):\n            for block_id in range(len(self.stacks[stack_id])):\n                b, f = self.stacks[stack_id][block_id](backcast)\n                backcast = backcast.to(self.device) - b\n                forecast = forecast.to(self.device) + f\n        return backcast, forecast\n\n\ndef seasonality_model(thetas, t, device):\n    p = thetas.size()[-1]\n    assert p <= thetas.shape[1], 'thetas_dim is too big.'\n    p1, p2 = (p // 2, p // 2) if p % 2 == 0 else (p // 2, p // 2 + 1)\n    s1 = torch.tensor([np.cos(2 * np.pi * i * t) for i in range(p1)]).float()  # H/2-1\n    s2 = torch.tensor([np.sin(2 * np.pi * i * t) for i in range(p2)]).float()\n    S = torch.cat([s1, s2])\n    return thetas.mm(S.to(device))\n\n\ndef trend_model(thetas, t, device):\n    p = thetas.size()[-1]\n    assert p <= 4, 'thetas_dim is too big.'\n    T = torch.tensor([t ** i for i in range(p)]).float()\n    return thetas.mm(T.to(device))\n\n\ndef linspace(backcast_length, forecast_length):\n    lin_space = np.linspace(-backcast_length, forecast_length, backcast_length + forecast_length)\n    b_ls = lin_space[:backcast_length]\n    f_ls = lin_space[backcast_length:]\n    return b_ls, f_ls\n\n\nclass Block(nn.Module):\n\n    def __init__(self, units, thetas_dim, device, backcast_length=10, forecast_length=5, share_thetas=False,\n                 nb_harmonics=None):\n        super(Block, self).__init__()\n        self.units = units\n        self.thetas_dim = thetas_dim\n        self.backcast_length = backcast_length\n        self.forecast_length = forecast_length\n        self.share_thetas = share_thetas\n        self.fc1 = nn.Linear(backcast_length, units)\n        self.fc2 = nn.Linear(units, units)\n        self.fc3 = nn.Linear(units, units)\n        self.fc4 = nn.Linear(units, units)\n        self.device = device\n        self.backcast_linspace, self.forecast_linspace = linspace(backcast_length, forecast_length)\n        if share_thetas:\n            self.theta_f_fc = self.theta_b_fc = nn.Linear(units, thetas_dim, bias=False)\n        else:\n            self.theta_b_fc = nn.Linear(units, thetas_dim, bias=False)\n            self.theta_f_fc = nn.Linear(units, thetas_dim, bias=False)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x.to(self.device)))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        x = F.relu(self.fc4(x))\n        return x\n\n    def __str__(self):\n        block_type = type(self).__name__\n        return f'{block_type}(units={self.units}, thetas_dim={self.thetas_dim}, ' \\\n               f'backcast_length={self.backcast_length}, forecast_length={self.forecast_length}, ' \\\n               f'share_thetas={self.share_thetas}) at @{id(self)}'\n\n\nclass SeasonalityBlock(Block):\n\n    def __init__(self, units, thetas_dim, device, backcast_length=10, forecast_length=5, nb_harmonics=None):\n        if nb_harmonics:\n            super(SeasonalityBlock, self).__init__(units, nb_harmonics, device, backcast_length,\n                                                   forecast_length, share_thetas=True)\n        else:\n            super(SeasonalityBlock, self).__init__(units, forecast_length, device, backcast_length,\n                                                   forecast_length, share_thetas=True)\n\n    def forward(self, x):\n        x = super(SeasonalityBlock, self).forward(x)\n        backcast = seasonality_model(self.theta_b_fc(x), self.backcast_linspace, self.device)\n        forecast = seasonality_model(self.theta_f_fc(x), self.forecast_linspace, self.device)\n        return backcast, forecast\n\n\nclass TrendBlock(Block):\n\n    def __init__(self, units, thetas_dim, device, backcast_length=10, forecast_length=5, nb_harmonics=None):\n        super(TrendBlock, self).__init__(units, thetas_dim, device, backcast_length,\n                                         forecast_length, share_thetas=True)\n\n    def forward(self, x):\n        x = super(TrendBlock, self).forward(x)\n        backcast = trend_model(self.theta_b_fc(x), self.backcast_linspace, self.device)\n        forecast = trend_model(self.theta_f_fc(x), self.forecast_linspace, self.device)\n        return backcast, forecast\n\n\nclass GenericBlock(Block):\n\n    def __init__(self, units, thetas_dim, device, backcast_length=10, forecast_length=5, nb_harmonics=None):\n        super(GenericBlock, self).__init__(units, thetas_dim, device, backcast_length, forecast_length)\n\n        self.backcast_fc = nn.Linear(thetas_dim, backcast_length)\n        self.forecast_fc = nn.Linear(thetas_dim, forecast_length)\n\n    def forward(self, x):\n        # no constraint for generic arch.\n        x = super(GenericBlock, self).forward(x)\n\n        theta_b = F.relu(self.theta_b_fc(x))\n        theta_f = F.relu(self.theta_f_fc(x))\n\n        backcast = self.backcast_fc(theta_b)  # generic. 3.3.\n        forecast = self.forecast_fc(theta_f)  # generic. 3.3.\n\n        return backcast, forecast\n"""
tests/model_test.py,0,"b""import unittest\n\nfrom nbeats_keras.model import NBeatsNet\n\n\nclass ModelTest(unittest.TestCase):\n    def test_share_weights_count_params(self):\n        m = NBeatsNet(stack_types=(\n            NBeatsNet.TREND_BLOCK,\n            NBeatsNet.SEASONALITY_BLOCK\n        ),\n            nb_blocks_per_stack=3,\n            forecast_length=2,\n            backcast_length=10,\n            thetas_dim=(4, 8),\n            share_weights_in_stack=False,\n            hidden_layer_units=64)\n\n        self.assertEqual(m.count_params(), 80512)\n\n        m2 = NBeatsNet(stack_types=(\n            NBeatsNet.TREND_BLOCK,\n            NBeatsNet.SEASONALITY_BLOCK\n        ),\n            nb_blocks_per_stack=3,\n            forecast_length=2,\n            backcast_length=10,\n            thetas_dim=(4, 8),\n            share_weights_in_stack=True,  # just change it to True.\n            hidden_layer_units=64)\n\n        self.assertEqual(m2.count_params(), (80512 + 128) // 3)  # nb_blocks_per_stack=3\n\n        m3 = NBeatsNet(stack_types=(\n            NBeatsNet.TREND_BLOCK,\n            NBeatsNet.SEASONALITY_BLOCK,\n            NBeatsNet.GENERIC_BLOCK,\n        ),\n            nb_blocks_per_stack=3,\n            forecast_length=2,\n            backcast_length=10,\n            thetas_dim=(4, 8, 4),\n            share_weights_in_stack=True,  # just change it to True.\n            hidden_layer_units=64)\n\n        self.assertEqual(len(m3.weights), len(m3.stack_types))\n\n    def test_thetas_stack_types_same_length(self):\n\n        try:\n            NBeatsNet(stack_types=(\n                NBeatsNet.TREND_BLOCK,\n                NBeatsNet.SEASONALITY_BLOCK,\n                NBeatsNet.GENERIC_BLOCK,\n            ),\n                nb_blocks_per_stack=3,\n                forecast_length=2,\n                backcast_length=10,\n                thetas_dim=(4, 8),\n                share_weights_in_stack=True,  # just change it to True.\n                hidden_layer_units=64)\n            raise Exception('Test fail.')\n        except AssertionError:\n            pass\n"""
