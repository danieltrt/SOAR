file_path,api_count,code
python/CamVid_loader.py,4,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import print_function\n\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport scipy.misc\nimport random\nimport os\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import utils\n\n\nroot_dir   = ""CamVid/""\ntrain_file = os.path.join(root_dir, ""train.csv"")\nval_file   = os.path.join(root_dir, ""val.csv"")\n\nnum_class = 32\nmeans     = np.array([103.939, 116.779, 123.68]) / 255. # mean of three channels in the order of BGR\nh, w      = 720, 960\ntrain_h   = int(h * 2 / 3)  # 480\ntrain_w   = int(w * 2 / 3)  # 640\nval_h     = int(h/32) * 32  # 704\nval_w     = w               # 960\n\n\nclass CamVidDataset(Dataset):\n\n    def __init__(self, csv_file, phase, n_class=num_class, crop=True, flip_rate=0.5):\n        self.data      = pd.read_csv(csv_file)\n        self.means     = means\n        self.n_class   = n_class\n\n        self.flip_rate = flip_rate\n        self.crop      = crop\n        if phase == \'train\':\n            self.new_h = train_h\n            self.new_w = train_w\n        elif phase == \'val\':\n            self.flip_rate = 0.\n            self.crop = False\n            self.new_h = val_h\n            self.new_w = val_w\n\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img_name   = self.data.ix[idx, 0]\n        img        = scipy.misc.imread(img_name, mode=\'RGB\')\n        label_name = self.data.ix[idx, 1]\n        label      = np.load(label_name)\n\n        if self.crop:\n            h, w, _ = img.shape\n            top   = random.randint(0, h - self.new_h)\n            left  = random.randint(0, w - self.new_w)\n            img   = img[top:top + self.new_h, left:left + self.new_w]\n            label = label[top:top + self.new_h, left:left + self.new_w]\n\n        if random.random() < self.flip_rate:\n            img   = np.fliplr(img)\n            label = np.fliplr(label)\n\n        # reduce mean\n        img = img[:, :, ::-1]  # switch to BGR\n        img = np.transpose(img, (2, 0, 1)) / 255.\n        img[0] -= self.means[0]\n        img[1] -= self.means[1]\n        img[2] -= self.means[2]\n\n        # convert to tensor\n        img = torch.from_numpy(img.copy()).float()\n        label = torch.from_numpy(label.copy()).long()\n\n        # create one-hot encoding\n        h, w = label.size()\n        target = torch.zeros(self.n_class, h, w)\n        for c in range(self.n_class):\n            target[c][label == c] = 1\n\n        sample = {\'X\': img, \'Y\': target, \'l\': label}\n\n        return sample\n\n\ndef show_batch(batch):\n    img_batch = batch[\'X\']\n    img_batch[:,0,...].add_(means[0])\n    img_batch[:,1,...].add_(means[1])\n    img_batch[:,2,...].add_(means[2])\n    batch_size = len(img_batch)\n\n    grid = utils.make_grid(img_batch)\n    plt.imshow(grid.numpy()[::-1].transpose((1, 2, 0)))\n\n    plt.title(\'Batch from dataloader\')\n\n\nif __name__ == ""__main__"":\n    train_data = CamVidDataset(csv_file=train_file, phase=\'train\')\n\n    # show a batch\n    batch_size = 4\n    for i in range(batch_size):\n        sample = train_data[i]\n        print(i, sample[\'X\'].size(), sample[\'Y\'].size())\n\n    dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n\n    for i, batch in enumerate(dataloader):\n        print(i, batch[\'X\'].size(), batch[\'Y\'].size())\n    \n        # observe 4th batch\n        if i == 3:\n            plt.figure()\n            show_batch(batch)\n            plt.axis(\'off\')\n            plt.ioff()\n            plt.show()\n            break\n'"
python/CamVid_utils.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import print_function\n\nfrom matplotlib import pyplot as plt\nimport matplotlib.image as mpimg\nimport numpy as np\nimport scipy.misc\nimport random\nimport os\n\n\n#############################\n    # global variables #\n#############################\nroot_dir          = ""CamVid/""\ndata_dir          = os.path.join(root_dir, ""701_StillsRaw_full"")    # train data\nlabel_dir         = os.path.join(root_dir, ""LabeledApproved_full"")  # train label\nlabel_colors_file = os.path.join(root_dir, ""label_colors.txt"")      # color to label\nval_label_file    = os.path.join(root_dir, ""val.csv"")               # validation file\ntrain_label_file  = os.path.join(root_dir, ""train.csv"")             # train file\n\n# create dir for label index\nlabel_idx_dir = os.path.join(root_dir, ""Labeled_idx"")\nif not os.path.exists(label_idx_dir):\n    os.makedirs(label_idx_dir)\n\nlabel2color = {}\ncolor2label = {}\nlabel2index = {}\nindex2label = {}\n\n\ndef divide_train_val(val_rate=0.1, shuffle=True, random_seed=None):\n    data_list   = os.listdir(data_dir)\n    data_len    = len(data_list)\n    val_len     = int(data_len * val_rate)\n\n    if random_seed:\n        random.seed(random_seed)\n\n    if shuffle:\n        data_idx = random.sample(range(data_len), data_len)\n    else:\n        data_idx = list(range(data_len))\n\n    val_idx     = [data_list[i] for i in data_idx[:val_len]]\n    train_idx   = [data_list[i] for i in data_idx[val_len:]]\n\n    # create val.csv\n    v = open(val_label_file, ""w"")\n    v.write(""img,label\\n"")\n    for idx, name in enumerate(val_idx):\n        if \'png\' not in name:\n            continue\n        img_name = os.path.join(data_dir, name)\n        lab_name = os.path.join(label_idx_dir, name)\n        lab_name = lab_name.split(""."")[0] + ""_L.png.npy""\n        v.write(""{},{}\\n"".format(img_name, lab_name))\n\n    # create train.csv\n    t = open(train_label_file, ""w"")\n    t.write(""img,label\\n"")\n    for idx, name in enumerate(train_idx):\n        if \'png\' not in name:\n            continue\n        img_name = os.path.join(data_dir, name)\n        lab_name = os.path.join(label_idx_dir, name)\n        lab_name = lab_name.split(""."")[0] + ""_L.png.npy""\n        t.write(""{},{}\\n"".format(img_name, lab_name))\n\n\ndef parse_label():\n    # change label to class index\n    f = open(label_colors_file, ""r"").read().split(""\\n"")[:-1]  # ignore the last empty line\n    for idx, line in enumerate(f):\n        label = line.split()[-1]\n        color = tuple([int(x) for x in line.split()[:-1]])\n        print(label, color)\n        label2color[label] = color\n        color2label[color] = label\n        label2index[label] = idx\n        index2label[idx]   = label\n        # rgb = np.zeros((255, 255, 3), dtype=np.uint8)\n        # rgb[..., 0] = color[0]\n        # rgb[..., 1] = color[1]\n        # rgb[..., 2] = color[2]\n        # imshow(rgb, title=label)\n    \n    for idx, name in enumerate(os.listdir(label_dir)):\n        filename = os.path.join(label_idx_dir, name)\n        if os.path.exists(filename + \'.npy\'):\n            print(""Skip %s"" % (name))\n            continue\n        print(""Parse %s"" % (name))\n        img = os.path.join(label_dir, name)\n        img = scipy.misc.imread(img, mode=\'RGB\')\n        height, weight, _ = img.shape\n    \n        idx_mat = np.zeros((height, weight))\n        for h in range(height):\n            for w in range(weight):\n                color = tuple(img[h, w])\n                try:\n                    label = color2label[color]\n                    index = label2index[label]\n                    idx_mat[h, w] = index\n                except:\n                    print(""error: img:%s, h:%d, w:%d"" % (name, h, w))\n        idx_mat = idx_mat.astype(np.uint8)\n        np.save(filename, idx_mat)\n        print(""Finish %s"" % (name))\n\n    # test some pixels\' label    \n    img = os.path.join(label_dir, os.listdir(label_dir)[0])\n    img = scipy.misc.imread(img, mode=\'RGB\')   \n    test_cases = [(555, 405), (0, 0), (380, 645), (577, 943)]\n    test_ans   = [\'Car\', \'Building\', \'Truck_Bus\', \'Car\']\n    for idx, t in enumerate(test_cases):\n        color = img[t]\n        assert color2label[tuple(color)] == test_ans[idx]\n\n\n\'\'\'debug function\'\'\'\ndef imshow(img, title=None):\n    try:\n        img = mpimg.imread(img)\n        imgplot = plt.imshow(img)\n    except:\n        plt.imshow(img, interpolation=\'nearest\')\n\n    if title is not None:\n        plt.title(title)\n    \n    plt.show()\n\n\nif __name__ == \'__main__\':\n    divide_train_val(random_seed=1)\n    parse_label()\n'"
python/Cityscapes_loader.py,4,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import print_function\n\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport scipy.misc\nimport random\nimport os\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import utils\n\n\nroot_dir   = ""CityScapes/""\ntrain_file = os.path.join(root_dir, ""train.csv"")\nval_file   = os.path.join(root_dir, ""val.csv"")\n\nnum_class = 20\nmeans     = np.array([103.939, 116.779, 123.68]) / 255. # mean of three channels in the order of BGR\nh, w      = 1024, 2048\ntrain_h   = int(h/2)  # 512\ntrain_w   = int(w/2)  # 1024\nval_h     = h  # 1024\nval_w     = w  # 2048\n\n\nclass CityScapesDataset(Dataset):\n\n    def __init__(self, csv_file, phase, n_class=num_class, crop=False, flip_rate=0.):\n        self.data      = pd.read_csv(csv_file)\n        self.means     = means\n        self.n_class   = n_class\n\n        self.flip_rate = flip_rate\n        self.crop      = crop\n        if phase == \'train\':\n            self.crop = True\n            self.flip_rate = 0.5\n            self.new_h = train_h\n            self.new_w = train_w\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img_name   = self.data.ix[idx, 0]\n        img        = scipy.misc.imread(img_name, mode=\'RGB\')\n        label_name = self.data.ix[idx, 1]\n        label      = np.load(label_name)\n\n        if self.crop:\n            h, w, _ = img.shape\n            top   = random.randint(0, h - self.new_h)\n            left  = random.randint(0, w - self.new_w)\n            img   = img[top:top + self.new_h, left:left + self.new_w]\n            label = label[top:top + self.new_h, left:left + self.new_w]\n\n        if random.random() < self.flip_rate:\n            img   = np.fliplr(img)\n            label = np.fliplr(label)\n\n        # reduce mean\n        img = img[:, :, ::-1]  # switch to BGR\n        img = np.transpose(img, (2, 0, 1)) / 255.\n        img[0] -= self.means[0]\n        img[1] -= self.means[1]\n        img[2] -= self.means[2]\n\n        # convert to tensor\n        img = torch.from_numpy(img.copy()).float()\n        label = torch.from_numpy(label.copy()).long()\n\n        # create one-hot encoding\n        h, w = label.size()\n        target = torch.zeros(self.n_class, h, w)\n        for c in range(self.n_class):\n            target[c][label == c] = 1\n\n        sample = {\'X\': img, \'Y\': target, \'l\': label}\n\n        return sample\n\n\ndef show_batch(batch):\n    img_batch = batch[\'X\']\n    img_batch[:,0,...].add_(means[0])\n    img_batch[:,1,...].add_(means[1])\n    img_batch[:,2,...].add_(means[2])\n    batch_size = len(img_batch)\n\n    grid = utils.make_grid(img_batch)\n    plt.imshow(grid.numpy()[::-1].transpose((1, 2, 0)))\n\n    plt.title(\'Batch from dataloader\')\n\n\nif __name__ == ""__main__"":\n    train_data = CityScapesDataset(csv_file=train_file, phase=\'train\')\n\n    # show a batch\n    batch_size = 4\n    for i in range(batch_size):\n        sample = train_data[i]\n        print(i, sample[\'X\'].size(), sample[\'Y\'].size())\n\n    dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=False, num_workers=4)\n\n    for i, batch in enumerate(dataloader):\n        print(i, batch[\'X\'].size(), batch[\'Y\'].size())\n    \n        # observe 4th batch\n        if i == 3:\n            plt.figure()\n            show_batch(batch)\n            plt.axis(\'off\')\n            plt.ioff()\n            plt.show()\n            break\n'"
python/Cityscapes_utils.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import print_function\n\nfrom collections import namedtuple\nfrom matplotlib import pyplot as plt\nimport matplotlib.image as mpimg\nimport numpy as np\nimport scipy.misc\nimport random\nimport os\n\n\n#############################\n    # global variables #\n#############################\nroot_dir  = ""CityScapes/""\n\nlabel_dir = os.path.join(root_dir, ""gtFine"")\ntrain_dir = os.path.join(label_dir, ""train"")\nval_dir   = os.path.join(label_dir, ""val"")\ntest_dir  = os.path.join(label_dir, ""test"")\n\n# create dir for label index\nlabel_idx_dir = os.path.join(root_dir, ""Labeled_idx"")\ntrain_idx_dir = os.path.join(label_idx_dir, ""train"")\nval_idx_dir   = os.path.join(label_idx_dir, ""val"")\ntest_idx_dir  = os.path.join(label_idx_dir, ""test"")\nfor dir in [train_idx_dir, val_idx_dir, test_idx_dir]:\n    if not os.path.exists(dir):\n        os.makedirs(dir)\n\ntrain_file = os.path.join(root_dir, ""train.csv"")\nval_file   = os.path.join(root_dir, ""val.csv"")\ntest_file  = os.path.join(root_dir, ""test.csv"")\n\ncolor2index = {}\n\nLabel = namedtuple(\'Label\', [\n                   \'name\', \n                   \'id\', \n                   \'trainId\', \n                   \'category\', \n                   \'categoryId\', \n                   \'hasInstances\', \n                   \'ignoreInEval\', \n                   \'color\'])\n\nlabels = [\n    #       name                     id    trainId   category            catId     hasInstances   ignoreInEval   color\n    Label(  \'unlabeled\'            ,  0 ,      255 , \'void\'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  \'ego vehicle\'          ,  1 ,      255 , \'void\'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  \'rectification border\' ,  2 ,      255 , \'void\'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  \'out of roi\'           ,  3 ,      255 , \'void\'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  \'static\'               ,  4 ,      255 , \'void\'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  \'dynamic\'              ,  5 ,      255 , \'void\'            , 0       , False        , True         , (111, 74,  0) ),\n    Label(  \'ground\'               ,  6 ,      255 , \'void\'            , 0       , False        , True         , ( 81,  0, 81) ),\n    Label(  \'road\'                 ,  7 ,        1 , \'flat\'            , 1       , False        , False        , (128, 64,128) ),\n    Label(  \'sidewalk\'             ,  8 ,        2 , \'flat\'            , 1       , False        , False        , (244, 35,232) ),\n    Label(  \'parking\'              ,  9 ,      255 , \'flat\'            , 1       , False        , True         , (250,170,160) ),\n    Label(  \'rail track\'           , 10 ,      255 , \'flat\'            , 1       , False        , True         , (230,150,140) ),\n    Label(  \'building\'             , 11 ,        3 , \'construction\'    , 2       , False        , False        , ( 70, 70, 70) ),\n    Label(  \'wall\'                 , 12 ,        4 , \'construction\'    , 2       , False        , False        , (102,102,156) ),\n    Label(  \'fence\'                , 13 ,        5 , \'construction\'    , 2       , False        , False        , (190,153,153) ),\n    Label(  \'guard rail\'           , 14 ,      255 , \'construction\'    , 2       , False        , True         , (180,165,180) ),\n    Label(  \'bridge\'               , 15 ,      255 , \'construction\'    , 2       , False        , True         , (150,100,100) ),\n    Label(  \'tunnel\'               , 16 ,      255 , \'construction\'    , 2       , False        , True         , (150,120, 90) ),\n    Label(  \'pole\'                 , 17 ,        6 , \'object\'          , 3       , False        , False        , (153,153,153) ),\n    Label(  \'polegroup\'            , 18 ,      255 , \'object\'          , 3       , False        , True         , (153,153,153) ),\n    Label(  \'traffic light\'        , 19 ,        7 , \'object\'          , 3       , False        , False        , (250,170, 30) ),\n    Label(  \'traffic sign\'         , 20 ,        8 , \'object\'          , 3       , False        , False        , (220,220,  0) ),\n    Label(  \'vegetation\'           , 21 ,        9 , \'nature\'          , 4       , False        , False        , (107,142, 35) ),\n    Label(  \'terrain\'              , 22 ,       10 , \'nature\'          , 4       , False        , False        , (152,251,152) ),\n    Label(  \'sky\'                  , 23 ,       11 , \'sky\'             , 5       , False        , False        , ( 70,130,180) ),\n    Label(  \'person\'               , 24 ,       12 , \'human\'           , 6       , True         , False        , (220, 20, 60) ),\n    Label(  \'rider\'                , 25 ,       13 , \'human\'           , 6       , True         , False        , (255,  0,  0) ),\n    Label(  \'car\'                  , 26 ,       14 , \'vehicle\'         , 7       , True         , False        , (  0,  0,142) ),\n    Label(  \'truck\'                , 27 ,       15 , \'vehicle\'         , 7       , True         , False        , (  0,  0, 70) ),\n    Label(  \'bus\'                  , 28 ,       16 , \'vehicle\'         , 7       , True         , False        , (  0, 60,100) ),\n    Label(  \'caravan\'              , 29 ,      255 , \'vehicle\'         , 7       , True         , True         , (  0,  0, 90) ),\n    Label(  \'trailer\'              , 30 ,      255 , \'vehicle\'         , 7       , True         , True         , (  0,  0,110) ),\n    Label(  \'train\'                , 31 ,       17 , \'vehicle\'         , 7       , True         , False        , (  0, 80,100) ),\n    Label(  \'motorcycle\'           , 32 ,       18 , \'vehicle\'         , 7       , True         , False        , (  0,  0,230) ),\n    Label(  \'bicycle\'              , 33 ,       19 , \'vehicle\'         , 7       , True         , False        , (119, 11, 32) ),\n    Label(  \'license plate\'        , -1 ,       -1 , \'vehicle\'         , 7       , False        , True         , (  0,  0,142) ),\n]\n\n\ndef parse_label():\n    # change label to class index\n    color2index[(0,0,0)] = 0  # add an void class \n    for obj in labels:\n        if obj.ignoreInEval:\n            continue\n        idx   = obj.trainId\n        label = obj.name\n        color = obj.color\n        color2index[color] = idx\n\n    # parse train, val, test data    \n    for label_dir, index_dir, csv_file in zip([train_dir, val_dir, test_dir], [train_idx_dir, val_idx_dir, test_idx_dir], [train_file, val_file, test_file]):\n        f = open(csv_file, ""w"")\n        f.write(""img,label\\n"")\n        for city in os.listdir(label_dir):\n            city_dir = os.path.join(label_dir, city)\n            city_idx_dir = os.path.join(index_dir, city)\n            data_dir = city_dir.replace(""gtFine"", ""leftImg8bit"")\n            if not os.path.exists(city_idx_dir):\n                os.makedirs(city_idx_dir)\n            for filename in os.listdir(city_dir):\n                if \'color\' not in filename:\n                    continue\n                lab_name = os.path.join(city_idx_dir, filename)\n                img_name = filename.split(""gtFine"")[0] + ""leftImg8bit.png""\n                img_name = os.path.join(data_dir, img_name)\n                f.write(""{},{}.npy\\n"".format(img_name, lab_name))\n\n                if os.path.exists(lab_name + \'.npy\'):\n                    print(""Skip %s"" % (filename))\n                    continue\n                print(""Parse %s"" % (filename))\n                img = os.path.join(city_dir, filename)\n                img = scipy.misc.imread(img, mode=\'RGB\')\n                height, weight, _ = img.shape\n        \n                idx_mat = np.zeros((height, weight))\n                for h in range(height):\n                    for w in range(weight):\n                        color = tuple(img[h, w])\n                        try:\n                            index = color2index[color]\n                            idx_mat[h, w] = index\n                        except:\n                            # no index, assign to void\n                            idx_mat[h, w] = 19\n                idx_mat = idx_mat.astype(np.uint8)\n                np.save(lab_name, idx_mat)\n                print(""Finish %s"" % (filename))\n\n\n\'\'\'debug function\'\'\'\ndef imshow(img, title=None):\n    try:\n        img = mpimg.imread(img)\n        imgplot = plt.imshow(img)\n    except:\n        plt.imshow(img, interpolation=\'nearest\')\n\n    if title is not None:\n        plt.title(title)\n    \n    plt.show()\n\n\nif __name__ == \'__main__\':\n    parse_label()\n'"
python/fcn.py,14,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import print_function\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models\nfrom torchvision.models.vgg import VGG\n\n\nclass FCN32s(nn.Module):\n\n    def __init__(self, pretrained_net, n_class):\n        super().__init__()\n        self.n_class = n_class\n        self.pretrained_net = pretrained_net\n        self.relu    = nn.ReLU(inplace=True)\n        self.deconv1 = nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n        self.bn1     = nn.BatchNorm2d(512)\n        self.deconv2 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n        self.bn2     = nn.BatchNorm2d(256)\n        self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n        self.bn3     = nn.BatchNorm2d(128)\n        self.deconv4 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n        self.bn4     = nn.BatchNorm2d(64)\n        self.deconv5 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n        self.bn5     = nn.BatchNorm2d(32)\n        self.classifier = nn.Conv2d(32, n_class, kernel_size=1)\n\n    def forward(self, x):\n        output = self.pretrained_net(x)\n        x5 = output[\'x5\']  # size=(N, 512, x.H/32, x.W/32)\n\n        score = self.bn1(self.relu(self.deconv1(x5)))     # size=(N, 512, x.H/16, x.W/16)\n        score = self.bn2(self.relu(self.deconv2(score)))  # size=(N, 256, x.H/8, x.W/8)\n        score = self.bn3(self.relu(self.deconv3(score)))  # size=(N, 128, x.H/4, x.W/4)\n        score = self.bn4(self.relu(self.deconv4(score)))  # size=(N, 64, x.H/2, x.W/2)\n        score = self.bn5(self.relu(self.deconv5(score)))  # size=(N, 32, x.H, x.W)\n        score = self.classifier(score)                    # size=(N, n_class, x.H/1, x.W/1)\n\n        return score  # size=(N, n_class, x.H/1, x.W/1)\n\n\nclass FCN16s(nn.Module):\n\n    def __init__(self, pretrained_net, n_class):\n        super().__init__()\n        self.n_class = n_class\n        self.pretrained_net = pretrained_net\n        self.relu    = nn.ReLU(inplace=True)\n        self.deconv1 = nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n        self.bn1     = nn.BatchNorm2d(512)\n        self.deconv2 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n        self.bn2     = nn.BatchNorm2d(256)\n        self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n        self.bn3     = nn.BatchNorm2d(128)\n        self.deconv4 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n        self.bn4     = nn.BatchNorm2d(64)\n        self.deconv5 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n        self.bn5     = nn.BatchNorm2d(32)\n        self.classifier = nn.Conv2d(32, n_class, kernel_size=1)\n\n    def forward(self, x):\n        output = self.pretrained_net(x)\n        x5 = output[\'x5\']  # size=(N, 512, x.H/32, x.W/32)\n        x4 = output[\'x4\']  # size=(N, 512, x.H/16, x.W/16)\n\n        score = self.relu(self.deconv1(x5))               # size=(N, 512, x.H/16, x.W/16)\n        score = self.bn1(score + x4)                      # element-wise add, size=(N, 512, x.H/16, x.W/16)\n        score = self.bn2(self.relu(self.deconv2(score)))  # size=(N, 256, x.H/8, x.W/8)\n        score = self.bn3(self.relu(self.deconv3(score)))  # size=(N, 128, x.H/4, x.W/4)\n        score = self.bn4(self.relu(self.deconv4(score)))  # size=(N, 64, x.H/2, x.W/2)\n        score = self.bn5(self.relu(self.deconv5(score)))  # size=(N, 32, x.H, x.W)\n        score = self.classifier(score)                    # size=(N, n_class, x.H/1, x.W/1)\n\n        return score  # size=(N, n_class, x.H/1, x.W/1)\n\n\nclass FCN8s(nn.Module):\n\n    def __init__(self, pretrained_net, n_class):\n        super().__init__()\n        self.n_class = n_class\n        self.pretrained_net = pretrained_net\n        self.relu    = nn.ReLU(inplace=True)\n        self.deconv1 = nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n        self.bn1     = nn.BatchNorm2d(512)\n        self.deconv2 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n        self.bn2     = nn.BatchNorm2d(256)\n        self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n        self.bn3     = nn.BatchNorm2d(128)\n        self.deconv4 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n        self.bn4     = nn.BatchNorm2d(64)\n        self.deconv5 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n        self.bn5     = nn.BatchNorm2d(32)\n        self.classifier = nn.Conv2d(32, n_class, kernel_size=1)\n\n    def forward(self, x):\n        output = self.pretrained_net(x)\n        x5 = output[\'x5\']  # size=(N, 512, x.H/32, x.W/32)\n        x4 = output[\'x4\']  # size=(N, 512, x.H/16, x.W/16)\n        x3 = output[\'x3\']  # size=(N, 256, x.H/8,  x.W/8)\n\n        score = self.relu(self.deconv1(x5))               # size=(N, 512, x.H/16, x.W/16)\n        score = self.bn1(score + x4)                      # element-wise add, size=(N, 512, x.H/16, x.W/16)\n        score = self.relu(self.deconv2(score))            # size=(N, 256, x.H/8, x.W/8)\n        score = self.bn2(score + x3)                      # element-wise add, size=(N, 256, x.H/8, x.W/8)\n        score = self.bn3(self.relu(self.deconv3(score)))  # size=(N, 128, x.H/4, x.W/4)\n        score = self.bn4(self.relu(self.deconv4(score)))  # size=(N, 64, x.H/2, x.W/2)\n        score = self.bn5(self.relu(self.deconv5(score)))  # size=(N, 32, x.H, x.W)\n        score = self.classifier(score)                    # size=(N, n_class, x.H/1, x.W/1)\n\n        return score  # size=(N, n_class, x.H/1, x.W/1)\n\n\nclass FCNs(nn.Module):\n\n    def __init__(self, pretrained_net, n_class):\n        super().__init__()\n        self.n_class = n_class\n        self.pretrained_net = pretrained_net\n        self.relu    = nn.ReLU(inplace=True)\n        self.deconv1 = nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n        self.bn1     = nn.BatchNorm2d(512)\n        self.deconv2 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n        self.bn2     = nn.BatchNorm2d(256)\n        self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n        self.bn3     = nn.BatchNorm2d(128)\n        self.deconv4 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n        self.bn4     = nn.BatchNorm2d(64)\n        self.deconv5 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n        self.bn5     = nn.BatchNorm2d(32)\n        self.classifier = nn.Conv2d(32, n_class, kernel_size=1)\n\n    def forward(self, x):\n        output = self.pretrained_net(x)\n        x5 = output[\'x5\']  # size=(N, 512, x.H/32, x.W/32)\n        x4 = output[\'x4\']  # size=(N, 512, x.H/16, x.W/16)\n        x3 = output[\'x3\']  # size=(N, 256, x.H/8,  x.W/8)\n        x2 = output[\'x2\']  # size=(N, 128, x.H/4,  x.W/4)\n        x1 = output[\'x1\']  # size=(N, 64, x.H/2,  x.W/2)\n\n        score = self.bn1(self.relu(self.deconv1(x5)))     # size=(N, 512, x.H/16, x.W/16)\n        score = score + x4                                # element-wise add, size=(N, 512, x.H/16, x.W/16)\n        score = self.bn2(self.relu(self.deconv2(score)))  # size=(N, 256, x.H/8, x.W/8)\n        score = score + x3                                # element-wise add, size=(N, 256, x.H/8, x.W/8)\n        score = self.bn3(self.relu(self.deconv3(score)))  # size=(N, 128, x.H/4, x.W/4)\n        score = score + x2                                # element-wise add, size=(N, 128, x.H/4, x.W/4)\n        score = self.bn4(self.relu(self.deconv4(score)))  # size=(N, 64, x.H/2, x.W/2)\n        score = score + x1                                # element-wise add, size=(N, 64, x.H/2, x.W/2)\n        score = self.bn5(self.relu(self.deconv5(score)))  # size=(N, 32, x.H, x.W)\n        score = self.classifier(score)                    # size=(N, n_class, x.H/1, x.W/1)\n\n        return score  # size=(N, n_class, x.H/1, x.W/1)\n\n\nclass VGGNet(VGG):\n    def __init__(self, pretrained=True, model=\'vgg16\', requires_grad=True, remove_fc=True, show_params=False):\n        super().__init__(make_layers(cfg[model]))\n        self.ranges = ranges[model]\n\n        if pretrained:\n            exec(""self.load_state_dict(models.%s(pretrained=True).state_dict())"" % model)\n\n        if not requires_grad:\n            for param in super().parameters():\n                param.requires_grad = False\n\n        if remove_fc:  # delete redundant fully-connected layer params, can save memory\n            del self.classifier\n\n        if show_params:\n            for name, param in self.named_parameters():\n                print(name, param.size())\n\n    def forward(self, x):\n        output = {}\n\n        # get the output of each maxpooling layer (5 maxpool in VGG net)\n        for idx in range(len(self.ranges)):\n            for layer in range(self.ranges[idx][0], self.ranges[idx][1]):\n                x = self.features[layer](x)\n            output[""x%d""%(idx+1)] = x\n\n        return output\n\n\nranges = {\n    \'vgg11\': ((0, 3), (3, 6),  (6, 11),  (11, 16), (16, 21)),\n    \'vgg13\': ((0, 5), (5, 10), (10, 15), (15, 20), (20, 25)),\n    \'vgg16\': ((0, 5), (5, 10), (10, 17), (17, 24), (24, 31)),\n    \'vgg19\': ((0, 5), (5, 10), (10, 19), (19, 28), (28, 37))\n}\n\n# cropped version from https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\ncfg = {\n    \'vgg11\': [64, \'M\', 128, \'M\', 256, 256, \'M\', 512, 512, \'M\', 512, 512, \'M\'],\n    \'vgg13\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, \'M\', 512, 512, \'M\', 512, 512, \'M\'],\n    \'vgg16\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, \'M\', 512, 512, 512, \'M\', 512, 512, 512, \'M\'],\n    \'vgg19\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, 256, \'M\', 512, 512, 512, 512, \'M\', 512, 512, 512, 512, \'M\'],\n}\n\ndef make_layers(cfg, batch_norm=False):\n    layers = []\n    in_channels = 3\n    for v in cfg:\n        if v == \'M\':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n        else:\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n            else:\n                layers += [conv2d, nn.ReLU(inplace=True)]\n            in_channels = v\n    return nn.Sequential(*layers)\n\n\nif __name__ == ""__main__"":\n    batch_size, n_class, h, w = 10, 20, 160, 160\n\n    # test output size\n    vgg_model = VGGNet(requires_grad=True)\n    input = torch.autograd.Variable(torch.randn(batch_size, 3, 224, 224))\n    output = vgg_model(input)\n    assert output[\'x5\'].size() == torch.Size([batch_size, 512, 7, 7])\n\n    fcn_model = FCN32s(pretrained_net=vgg_model, n_class=n_class)\n    input = torch.autograd.Variable(torch.randn(batch_size, 3, h, w))\n    output = fcn_model(input)\n    assert output.size() == torch.Size([batch_size, n_class, h, w])\n\n    fcn_model = FCN16s(pretrained_net=vgg_model, n_class=n_class)\n    input = torch.autograd.Variable(torch.randn(batch_size, 3, h, w))\n    output = fcn_model(input)\n    assert output.size() == torch.Size([batch_size, n_class, h, w])\n\n    fcn_model = FCN8s(pretrained_net=vgg_model, n_class=n_class)\n    input = torch.autograd.Variable(torch.randn(batch_size, 3, h, w))\n    output = fcn_model(input)\n    assert output.size() == torch.Size([batch_size, n_class, h, w])\n\n    fcn_model = FCNs(pretrained_net=vgg_model, n_class=n_class)\n    input = torch.autograd.Variable(torch.randn(batch_size, 3, h, w))\n    output = fcn_model(input)\n    assert output.size() == torch.Size([batch_size, n_class, h, w])\n\n    print(""Pass size check"")\n\n    # test a random batch, loss should decrease\n    fcn_model = FCNs(pretrained_net=vgg_model, n_class=n_class)\n    criterion = nn.BCELoss()\n    optimizer = optim.SGD(fcn_model.parameters(), lr=1e-3, momentum=0.9)\n    input = torch.autograd.Variable(torch.randn(batch_size, 3, h, w))\n    y = torch.autograd.Variable(torch.randn(batch_size, n_class, h, w), requires_grad=False)\n    for iter in range(10):\n        optimizer.zero_grad()\n        output = fcn_model(input)\n        output = nn.functional.sigmoid(output)\n        loss = criterion(output, y)\n        loss.backward()\n        print(""iter{}, loss {}"".format(iter, loss.data[0]))\n        optimizer.step()\n'"
python/train.py,8,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import print_function\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\n\nfrom fcn import VGGNet, FCN32s, FCN16s, FCN8s, FCNs\nfrom Cityscapes_loader import CityscapesDataset\nfrom CamVid_loader import CamVidDataset\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport time\nimport sys\nimport os\n\n\nn_class    = 20\n\nbatch_size = 6\nepochs     = 500\nlr         = 1e-4\nmomentum   = 0\nw_decay    = 1e-5\nstep_size  = 50\ngamma      = 0.5\nconfigs    = ""FCNs-BCEWithLogits_batch{}_epoch{}_RMSprop_scheduler-step{}-gamma{}_lr{}_momentum{}_w_decay{}"".format(batch_size, epochs, step_size, gamma, lr, momentum, w_decay)\nprint(""Configs:"", configs)\n\nif sys.argv[1] == \'CamVid\':\n    root_dir   = ""CamVid/""\nelse\n    root_dir   = ""CityScapes/""\ntrain_file = os.path.join(root_dir, ""train.csv"")\nval_file   = os.path.join(root_dir, ""val.csv"")\n\n# create dir for model\nmodel_dir = ""models""\nif not os.path.exists(model_dir):\n    os.makedirs(model_dir)\nmodel_path = os.path.join(model_dir, configs)\n\nuse_gpu = torch.cuda.is_available()\nnum_gpu = list(range(torch.cuda.device_count()))\n\nif sys.argv[1] == \'CamVid\':\n    train_data = CamVidDataset(csv_file=train_file, phase=\'train\')\nelse:\n    train_data = CityscapesDataset(csv_file=train_file, phase=\'train\')\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=8)\n\nif sys.argv[1] == \'CamVid\':\n    val_data = CamVidDataset(csv_file=val_file, phase=\'val\', flip_rate=0)\nelse:\n    val_data = CityscapesDataset(csv_file=val_file, phase=\'val\', flip_rate=0)\nval_loader = DataLoader(val_data, batch_size=1, num_workers=8)\n\nvgg_model = VGGNet(requires_grad=True, remove_fc=True)\nfcn_model = FCNs(pretrained_net=vgg_model, n_class=n_class)\n\nif use_gpu:\n    ts = time.time()\n    vgg_model = vgg_model.cuda()\n    fcn_model = fcn_model.cuda()\n    fcn_model = nn.DataParallel(fcn_model, device_ids=num_gpu)\n    print(""Finish cuda loading, time elapsed {}"".format(time.time() - ts))\n\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.RMSprop(fcn_model.parameters(), lr=lr, momentum=momentum, weight_decay=w_decay)\nscheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)  # decay LR by a factor of 0.5 every 30 epochs\n\n# create dir for score\nscore_dir = os.path.join(""scores"", configs)\nif not os.path.exists(score_dir):\n    os.makedirs(score_dir)\nIU_scores    = np.zeros((epochs, n_class))\npixel_scores = np.zeros(epochs)\n\n\ndef train():\n    for epoch in range(epochs):\n        scheduler.step()\n\n        ts = time.time()\n        for iter, batch in enumerate(train_loader):\n            optimizer.zero_grad()\n\n            if use_gpu:\n                inputs = Variable(batch[\'X\'].cuda())\n                labels = Variable(batch[\'Y\'].cuda())\n            else:\n                inputs, labels = Variable(batch[\'X\']), Variable(batch[\'Y\'])\n\n            outputs = fcn_model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            if iter % 10 == 0:\n                print(""epoch{}, iter{}, loss: {}"".format(epoch, iter, loss.data[0]))\n        \n        print(""Finish epoch {}, time elapsed {}"".format(epoch, time.time() - ts))\n        torch.save(fcn_model, model_path)\n\n        val(epoch)\n\n\ndef val(epoch):\n    fcn_model.eval()\n    total_ious = []\n    pixel_accs = []\n    for iter, batch in enumerate(val_loader):\n        if use_gpu:\n            inputs = Variable(batch[\'X\'].cuda())\n        else:\n            inputs = Variable(batch[\'X\'])\n\n        output = fcn_model(inputs)\n        output = output.data.cpu().numpy()\n\n        N, _, h, w = output.shape\n        pred = output.transpose(0, 2, 3, 1).reshape(-1, n_class).argmax(axis=1).reshape(N, h, w)\n\n        target = batch[\'l\'].cpu().numpy().reshape(N, h, w)\n        for p, t in zip(pred, target):\n            total_ious.append(iou(p, t))\n            pixel_accs.append(pixel_acc(p, t))\n\n    # Calculate average IoU\n    total_ious = np.array(total_ious).T  # n_class * val_len\n    ious = np.nanmean(total_ious, axis=1)\n    pixel_accs = np.array(pixel_accs).mean()\n    print(""epoch{}, pix_acc: {}, meanIoU: {}, IoUs: {}"".format(epoch, pixel_accs, np.nanmean(ious), ious))\n    IU_scores[epoch] = ious\n    np.save(os.path.join(score_dir, ""meanIU""), IU_scores)\n    pixel_scores[epoch] = pixel_accs\n    np.save(os.path.join(score_dir, ""meanPixel""), pixel_scores)\n\n\n# borrow functions and modify it from https://github.com/Kaixhin/FCN-semantic-segmentation/blob/master/main.py\n# Calculates class intersections over unions\ndef iou(pred, target):\n    ious = []\n    for cls in range(n_class):\n        pred_inds = pred == cls\n        target_inds = target == cls\n        intersection = pred_inds[target_inds].sum()\n        union = pred_inds.sum() + target_inds.sum() - intersection\n        if union == 0:\n            ious.append(float(\'nan\'))  # if there is no ground truth, do not include in evaluation\n        else:\n            ious.append(float(intersection) / max(union, 1))\n        # print(""cls"", cls, pred_inds.sum(), target_inds.sum(), intersection, float(intersection) / max(union, 1))\n    return ious\n\n\ndef pixel_acc(pred, target):\n    correct = (pred == target).sum()\n    total   = (target == target).sum()\n    return correct / total\n\n\nif __name__ == ""__main__"":\n    val(0)  # show the accuracy before training\n    train()\n'"
