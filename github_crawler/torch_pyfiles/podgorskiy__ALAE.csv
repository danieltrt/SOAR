file_path,api_count,code
align_faces.py,0,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\nimport os\nimport numpy as np\nimport dlib\nfrom PIL import Image\nimport PIL\nimport scipy\nimport scipy.ndimage\n\n# lefteye_x lefteye_y righteye_x righteye_y nose_x nose_y leftmouth_x leftmouth_y rightmouth_x rightmouth_y\n# 69\t111\t108\t111\t88\t136\t72\t152\t105\t152\n# 44\t51\t83\t51\t63\t76\t47\t92\t80\t92\n\nuse_1024 = True\n\n\ndef align(img, parts, dst_dir=\'realign1024x1024\', output_size=1024, transform_size=4096, item_idx=0, enable_padding=True):\n    # Parse landmarks.\n    lm = np.array(parts)\n    lm_chin          = lm[0: 17]  # left-right\n    lm_eyebrow_left = lm[17: 22]  # left-right\n    lm_eyebrow_right = lm[22: 27]  # left-right\n    lm_nose = lm[27: 31]  # top-down\n    lm_nostrils = lm[31: 36]  # top-down\n    lm_eye_left = lm[36: 42]  # left-clockwise\n    lm_eye_right = lm[42: 48]  # left-clockwise\n    lm_mouth_outer = lm[48: 60]  # left-clockwise\n    lm_mouth_inner = lm[60: 68]  # left-clockwise\n\n    # Calculate auxiliary vectors.\n    eye_left = np.mean(lm_eye_left, axis=0)\n    eye_right = np.mean(lm_eye_right, axis=0)\n    eye_avg = (eye_left + eye_right) * 0.5\n    eye_to_eye = eye_right - eye_left\n    mouth_left = lm_mouth_outer[0]\n    mouth_right = lm_mouth_outer[6]\n    mouth_avg = (mouth_left + mouth_right) * 0.5\n    eye_to_mouth = mouth_avg - eye_avg\n\n    # Choose oriented crop rectangle.\n    x = eye_to_eye - np.flipud(eye_to_mouth) * [-1, 1]\n    x /= np.hypot(*x)\n\n    if use_1024:\n        x *= max(np.hypot(*eye_to_eye) * 2.0, np.hypot(*eye_to_mouth) * 1.8)\n    else:\n        x *= (np.hypot(*eye_to_eye) * 1.6410 + np.hypot(*eye_to_mouth) * 1.560) / 2.0\n\n    y = np.flipud(x) * [-1, 1]\n\n    if use_1024:\n        c = eye_avg + eye_to_mouth * 0.1\n        quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])\n    else:\n        c = eye_avg + eye_to_mouth * 0.317\n        quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])\n\n    qsize = np.hypot(*x) * 2\n\n    img = Image.fromarray(img)\n\n    # Shrink.\n    shrink = int(np.floor(qsize / output_size * 0.5))\n    if shrink > 1:\n        rsize = (int(np.rint(float(img.size[0]) / shrink)), int(np.rint(float(img.size[1]) / shrink)))\n        img = img.resize(rsize, PIL.Image.ANTIALIAS)\n        quad /= shrink\n        qsize /= shrink\n\n    # Crop.\n    border = max(int(np.rint(qsize * 0.1)), 3)\n    crop = (int(np.floor(min(quad[:, 0]))), int(np.floor(min(quad[:, 1]))), int(np.ceil(max(quad[:, 0]))),\n            int(np.ceil(max(quad[:, 1]))))\n    crop = (max(crop[0] - border, 0), max(crop[1] - border, 0), min(crop[2] + border, img.size[0]),\n            min(crop[3] + border, img.size[1]))\n    if crop[2] - crop[0] < img.size[0] or crop[3] - crop[1] < img.size[1]:\n        img = img.crop(crop)\n        quad -= crop[0:2]\n\n    # Pad.\n    pad = (int(np.floor(min(quad[:, 0]))), int(np.floor(min(quad[:, 1]))), int(np.ceil(max(quad[:, 0]))),\n           int(np.ceil(max(quad[:, 1]))))\n    pad = (max(-pad[0] + border, 0), max(-pad[1] + border, 0), max(pad[2] - img.size[0] + border, 0),\n           max(pad[3] - img.size[1] + border, 0))\n    if enable_padding and max(pad) > border - 4:\n        pad = np.maximum(pad, int(np.rint(qsize * 0.3)))\n        img = np.pad(np.float32(img), ((pad[1], pad[3]), (pad[0], pad[2]), (0, 0)), \'reflect\')\n        h, w, _ = img.shape\n        y, x, _ = np.ogrid[:h, :w, :1]\n        mask = np.maximum(1.0 - np.minimum(np.float32(x) / pad[0], np.float32(w - 1 - x) / pad[2]),\n                          1.0 - np.minimum(np.float32(y) / pad[1], np.float32(h - 1 - y) / pad[3]))\n        blur = qsize * 0.02\n        img += (scipy.ndimage.gaussian_filter(img, [blur, blur, 0]) - img) * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)\n        img += (np.median(img, axis=(0, 1)) - img) * np.clip(mask, 0.0, 1.0)\n        img = PIL.Image.fromarray(np.uint8(np.clip(np.rint(img), 0, 255)), \'RGB\')\n        quad += pad[:2]\n\n    # Transform.\n    img = img.transform((transform_size, transform_size), PIL.Image.QUAD, (quad + 0.5).flatten(), PIL.Image.BILINEAR)\n    if output_size < transform_size:\n        img = img.resize((output_size, output_size), PIL.Image.ANTIALIAS)\n\n    # Save aligned image.\n    dst_subdir = dst_dir\n    os.makedirs(dst_subdir, exist_ok=True)\n    img.save(os.path.join(dst_subdir, \'%05d.png\' % item_idx))\n\n\npredictor_path = \'shape_predictor_68_face_landmarks.dat\'\n\ndetector = dlib.get_frontal_face_detector()\npredictor = dlib.shape_predictor(predictor_path)\n\nitem_idx = 0\n\nfor filename in os.listdir(\'celebs\'):\n    img = np.asarray(Image.open(\'celebs/\' + filename))\n    if img.shape[2] == 4:\n        img = img[:, :, :3]\n\n    dets = detector(img, 0)\n    print(""Number of faces detected: {}"".format(len(dets)))\n\n    for i, d in enumerate(dets):\n        print(""Detection {}: Left: {} Top: {} Right: {} Bottom: {}"".format(\n        i, d.left(), d.top(), d.right(), d.bottom()))\n\n        shape = predictor(img, d)\n\n        parts = shape.parts()\n\n        parts = [[part.x, part.y] for part in parts]\n\n        if use_1024:\n            align(img, parts, dst_dir=\'dataset_samples/faces/realign1024x1024\', output_size=1024, transform_size=4098, item_idx=item_idx)\n        else:\n            align(img, parts, dst_dir=\'dataset_samples/faces/realign128x128\', output_size=128, transform_size=512, item_idx=item_idx)\n\n        item_idx += 1\n'"
checkpointer.py,2,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport os\nfrom torch import nn\nimport torch\nimport utils\n\n\ndef get_model_dict(x):\n    if x is None:\n        return None\n    if isinstance(x, nn.DataParallel):\n        return x.module.state_dict()\n    else:\n        return x.state_dict()\n\n\ndef load_model(x, state_dict):\n    if isinstance(x, nn.DataParallel):\n        x.module.load_state_dict(state_dict)\n    else:\n        x.load_state_dict(state_dict)\n\n\nclass Checkpointer(object):\n    def __init__(self, cfg, models, auxiliary=None, logger=None, save=True):\n        self.models = models\n        self.auxiliary = auxiliary\n        self.cfg = cfg\n        self.logger = logger\n        self._save = save\n\n    def save(self, _name, **kwargs):\n        if not self._save:\n            return\n        data = dict()\n        data[""models""] = dict()\n        data[""auxiliary""] = dict()\n        for name, model in self.models.items():\n            data[""models""][name] = get_model_dict(model)\n\n        if self.auxiliary is not None:\n            for name, item in self.auxiliary.items():\n                data[""auxiliary""][name] = item.state_dict()\n        data.update(kwargs)\n\n        @utils.async_func\n        def save_data():\n            save_file = os.path.join(self.cfg.OUTPUT_DIR, ""%s.pth"" % _name)\n            self.logger.info(""Saving checkpoint to %s"" % save_file)\n            torch.save(data, save_file)\n            self.tag_last_checkpoint(save_file)\n\n        return save_data()\n\n    def load(self, ignore_last_checkpoint=False, file_name=None):\n        save_file = os.path.join(self.cfg.OUTPUT_DIR, ""last_checkpoint"")\n        try:\n            with open(save_file, ""r"") as last_checkpoint:\n                f = last_checkpoint.read().strip()\n        except IOError:\n            self.logger.info(""No checkpoint found. Initializing model from scratch"")\n            if file_name is None:\n                return {}\n\n        if ignore_last_checkpoint:\n            self.logger.info(""Forced to Initialize model from scratch"")\n            return {}\n        if file_name is not None:\n            f = file_name\n\n        self.logger.info(""Loading checkpoint from {}"".format(f))\n        checkpoint = torch.load(f, map_location=torch.device(""cpu""))\n        for name, model in self.models.items():\n            if name in checkpoint[""models""]:\n                try:\n                    model_dict = checkpoint[""models""].pop(name)\n                    if model_dict is not None:\n                        self.models[name].load_state_dict(model_dict, strict=False)\n                    else:\n                        self.logger.warning(""State dict for model \\""%s\\"" is None "" % name)\n                except RuntimeError as e:\n                    self.logger.warning(\'%s\\nFailed to load: %s\\n%s\' % (\'!\' * 160, name, \'!\' * 160))\n                    self.logger.warning(\'\\nFailed to load: %s\' % str(e))\n            else:\n                self.logger.warning(""No state dict for model: %s"" % name)\n        checkpoint.pop(\'models\')\n        if ""auxiliary"" in checkpoint and self.auxiliary:\n            self.logger.info(""Loading auxiliary from {}"".format(f))\n            for name, item in self.auxiliary.items():\n                try:\n                    if name in checkpoint[""auxiliary""]:\n                        self.auxiliary[name].load_state_dict(checkpoint[""auxiliary""].pop(name))\n                    if ""optimizers"" in checkpoint and name in checkpoint[""optimizers""]:\n                        self.auxiliary[name].load_state_dict(checkpoint[""optimizers""].pop(name))\n                    if name in checkpoint:\n                        self.auxiliary[name].load_state_dict(checkpoint.pop(name))\n                except IndexError:\n                    self.logger.warning(\'%s\\nFailed to load: %s\\n%s\' % (\'!\' * 160, name, \'!\' * 160))\n            checkpoint.pop(\'auxiliary\')\n\n        return checkpoint\n\n    def tag_last_checkpoint(self, last_filename):\n        save_file = os.path.join(self.cfg.OUTPUT_DIR, ""last_checkpoint"")\n        with open(save_file, ""w"") as f:\n            f.write(last_filename)\n'"
custom_adam.py,3,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n# lr_equalization_coef was added for LREQ\n\n# Copyright (c) 2016-     Facebook, Inc            (Adam Paszke)\n# Copyright (c) 2014-     Facebook, Inc            (Soumith Chintala)\n# Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)\n# Copyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)\n# Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)\n# Copyright (c) 2011-2013 NYU                      (Clement Farabet)\n# Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)\n# Copyright (c) 2006      Idiap Research Institute (Samy Bengio)\n# Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)\n\n# https://github.com/pytorch/pytorch/blob/master/LICENSE\n\n\nimport math\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\n\nclass LREQAdam(Optimizer):\n    def __init__(self, params, lr=1e-3, betas=(0.0, 0.99), eps=1e-8,\n                 weight_decay=0):\n        beta_2 = betas[1]\n        if not 0.0 <= lr:\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(""Invalid epsilon value: {}"".format(eps))\n        if not 0.0 == betas[0]:\n            raise ValueError(""Invalid beta parameter at index 0: {}"".format(betas[0]))\n        if not 0.0 <= beta_2 < 1.0:\n            raise ValueError(""Invalid beta parameter at index 1: {}"".format(beta_2))\n        defaults = dict(lr=lr, beta_2=beta_2, eps=eps,\n                        weight_decay=weight_decay)\n        super(LREQAdam, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(LREQAdam, self).__setstate__(state)\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\'Adam does not support sparse gradients, please consider SparseAdam instead\')\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    # state[\'exp_avg\'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p.data)\n\n                # exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                exp_avg_sq = state[\'exp_avg_sq\']\n                beta_2 = group[\'beta_2\']\n\n                state[\'step\'] += 1\n\n                if group[\'weight_decay\'] != 0:\n                    grad.add_(group[\'weight_decay\'], p.data / p.coef)\n\n                # Decay the first and second moment running average coefficient\n                # exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta_2).addcmul_(1 - beta_2, grad, grad)\n                denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\n\n                # bias_correction1 = 1 - beta1 ** state[\'step\'] # 1\n                bias_correction2 = 1 - beta_2 ** state[\'step\']\n                # step_size = group[\'lr\'] * math.sqrt(bias_correction2) / bias_correction1\n                step_size = group[\'lr\'] * math.sqrt(bias_correction2)\n\n                # p.data.addcdiv_(-step_size, exp_avg, denom)\n                if hasattr(p, \'lr_equalization_coef\'):\n                    step_size *= p.lr_equalization_coef\n\n                p.data.addcdiv_(-step_size, grad, denom)\n\n        return loss\n'"
dataloader.py,16,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport dareblopy as db\nimport random\n\nimport numpy as np\nimport torch\nimport torch.tensor\nimport torch.utils\nimport torch.utils.data\nimport time\nimport math\n\ncpu = torch.device(\'cpu\')\n\n\nclass TFRecordsDataset:\n    def __init__(self, cfg, logger, rank=0, world_size=1, buffer_size_mb=200, channels=3, seed=None, train=True, needs_labels=False):\n        self.cfg = cfg\n        self.logger = logger\n        self.rank = rank\n        self.last_data = """"\n        if train:\n            self.part_count = cfg.DATASET.PART_COUNT\n            self.part_size = cfg.DATASET.SIZE // self.part_count\n        else:\n            self.part_count = cfg.DATASET.PART_COUNT_TEST\n            self.part_size = cfg.DATASET.SIZE_TEST // self.part_count\n        self.workers = []\n        self.workers_active = 0\n        self.iterator = None\n        self.filenames = {}\n        self.batch_size = 512\n        self.features = {}\n        self.channels = channels\n        self.seed = seed\n        self.train = train\n        self.needs_labels = needs_labels\n\n        assert self.part_count % world_size == 0\n\n        self.part_count_local = self.part_count // world_size\n\n        if train:\n            path = cfg.DATASET.PATH\n        else:\n            path = cfg.DATASET.PATH_TEST\n\n        for r in range(2, cfg.DATASET.MAX_RESOLUTION_LEVEL + 1):\n            files = []\n            for i in range(self.part_count_local * rank, self.part_count_local * (rank + 1)):\n                file = path % (r, i)\n                files.append(file)\n            self.filenames[r] = files\n\n        self.buffer_size_b = 1024 ** 2 * buffer_size_mb\n\n        self.current_filenames = []\n\n    def reset(self, lod, batch_size):\n        assert lod in self.filenames.keys()\n        self.current_filenames = self.filenames[lod]\n        self.batch_size = batch_size\n\n        img_size = 2 ** lod\n\n        if self.needs_labels:\n            self.features = {\n                # \'shape\': db.FixedLenFeature([3], db.int64),\n                \'data\': db.FixedLenFeature([self.channels, img_size, img_size], db.uint8),\n                \'label\': db.FixedLenFeature([], db.int64)\n            }\n        else:\n            self.features = {\n                # \'shape\': db.FixedLenFeature([3], db.int64),\n                \'data\': db.FixedLenFeature([self.channels, img_size, img_size], db.uint8)\n            }\n\n        buffer_size = self.buffer_size_b // (self.channels * img_size * img_size)\n\n        if self.seed is None:\n            seed = np.uint64(time.time() * 1000)\n        else:\n            seed = self.seed\n            self.logger.info(\'!\' * 80)\n            self.logger.info(\'! Seed is used for to shuffle data in TFRecordsDataset!\')\n            self.logger.info(\'!\' * 80)\n\n        self.iterator = db.ParsedTFRecordsDatasetIterator(self.current_filenames, self.features, self.batch_size, buffer_size, seed=seed)\n\n    def __iter__(self):\n        return self.iterator\n\n    def __len__(self):\n        return self.part_count_local * self.part_size\n\n\ndef make_dataloader(cfg, logger, dataset, GPU_batch_size, local_rank, numpy=False):\n    class BatchCollator(object):\n        def __init__(self, device=torch.device(""cpu"")):\n            self.device = device\n            self.flip = cfg.DATASET.FLIP_IMAGES\n            self.numpy = numpy\n\n        def __call__(self, batch):\n            with torch.no_grad():\n                x, = batch\n                if self.flip:\n                    flips = [(slice(None, None, None), slice(None, None, None), slice(None, None, random.choice([-1, None]))) for _ in range(x.shape[0])]\n                    x = np.array([img[flip] for img, flip in zip(x, flips)])\n                if self.numpy:\n                    return x\n                x = torch.tensor(x, requires_grad=True, device=torch.device(self.device), dtype=torch.float32)\n                return x\n\n    batches = db.data_loader(iter(dataset), BatchCollator(local_rank), len(dataset) // GPU_batch_size)\n\n    return batches\n\n\ndef make_dataloader_y(cfg, logger, dataset, GPU_batch_size, local_rank):\n    class BatchCollator(object):\n        def __init__(self, device=torch.device(""cpu"")):\n            self.device = device\n            self.flip = cfg.DATASET.FLIP_IMAGES\n\n        def __call__(self, batch):\n            with torch.no_grad():\n                x, y = batch\n                if self.flip:\n                    flips = [(slice(None, None, None), slice(None, None, None), slice(None, None, random.choice([-1, None]))) for _ in range(x.shape[0])]\n                    x = np.array([img[flip] for img, flip in zip(x, flips)])\n                x = torch.tensor(x, requires_grad=True, device=torch.device(self.device), dtype=torch.float32)\n                return x, y\n\n    batches = db.data_loader(iter(dataset), BatchCollator(local_rank), len(dataset) // GPU_batch_size)\n\n    return batches\n\n\nclass TFRecordsDatasetImageNet:\n    def __init__(self, cfg, logger, rank=0, world_size=1, buffer_size_mb=200, channels=3, seed=None, train=True, needs_labels=False):\n        self.cfg = cfg\n        self.logger = logger\n        self.rank = rank\n        self.last_data = """"\n        self.part_count = cfg.DATASET.PART_COUNT\n        if train:\n            self.part_size = cfg.DATASET.SIZE // cfg.DATASET.PART_COUNT\n        else:\n            self.part_size = cfg.DATASET.SIZE_TEST // cfg.DATASET.PART_COUNT\n        self.workers = []\n        self.workers_active = 0\n        self.iterator = None\n        self.filenames = {}\n        self.batch_size = 512\n        self.features = {}\n        self.channels = channels\n        self.seed = seed\n        self.train = train\n        self.needs_labels = needs_labels\n\n        assert self.part_count % world_size == 0\n\n        self.part_count_local = cfg.DATASET.PART_COUNT // world_size\n\n        if train:\n            path = cfg.DATASET.PATH\n        else:\n            path = cfg.DATASET.PATH_TEST\n\n        for r in range(2, cfg.DATASET.MAX_RESOLUTION_LEVEL + 1):\n            files = []\n            for i in range(self.part_count_local * rank, self.part_count_local * (rank + 1)):\n                file = path % (r, i)\n                files.append(file)\n            self.filenames[r] = files\n\n        self.buffer_size_b = 1024 ** 2 * buffer_size_mb\n\n        self.current_filenames = []\n\n    def reset(self, lod, batch_size):\n        assert lod in self.filenames.keys()\n        self.current_filenames = self.filenames[lod]\n        self.batch_size = batch_size\n\n        if self.train:\n            img_size = 2 ** lod + 2 ** (lod - 3)\n        else:\n            img_size = 2 ** lod\n\n        if self.needs_labels:\n            self.features = {\n                \'data\': db.FixedLenFeature([3, img_size, img_size], db.uint8),\n                \'label\': db.FixedLenFeature([], db.int64)\n            }\n        else:\n            self.features = {\n                \'data\': db.FixedLenFeature([3, img_size, img_size], db.uint8)\n            }\n\n        buffer_size = self.buffer_size_b // (self.channels * img_size * img_size)\n\n        if self.seed is None:\n            seed = np.uint64(time.time() * 1000)\n        else:\n            seed = self.seed\n            self.logger.info(\'!\' * 80)\n            self.logger.info(\'! Seed is used for to shuffle data in TFRecordsDataset!\')\n            self.logger.info(\'!\' * 80)\n\n        self.iterator = db.ParsedTFRecordsDatasetIterator(self.current_filenames, self.features, self.batch_size, buffer_size, seed=seed)\n\n    def __iter__(self):\n        return self.iterator\n\n    def __len__(self):\n        return self.part_count_local * self.part_size\n\n\ndef make_imagenet_dataloader(cfg, logger, dataset, GPU_batch_size, target_size, local_rank, do_random_crops=True):\n    class BatchCollator(object):\n        def __init__(self, device=torch.device(""cpu"")):\n            self.device = device\n            self.flip = cfg.DATASET.FLIP_IMAGES\n            self.size = target_size\n            p = math.log2(target_size)\n            self.source_size = 2 ** p + 2 ** (p - 3)\n            self.do_random_crops = do_random_crops\n\n        def __call__(self, batch):\n            with torch.no_grad():\n                x, = batch\n\n                if self.do_random_crops:\n                    images = []\n                    for im in x:\n                        deltax = self.source_size - target_size\n                        deltay = self.source_size - target_size\n                        offx = np.random.randint(deltax + 1)\n                        offy = np.random.randint(deltay + 1)\n                        im = im[:, offy:offy + self.size, offx:offx + self.size]\n                        images.append(im)\n                    x = np.stack(images)\n\n                if self.flip:\n                    flips = [(slice(None, None, None), slice(None, None, None), slice(None, None, random.choice([-1, None]))) for _ in range(x.shape[0])]\n                    x = np.array([img[flip] for img, flip in zip(x, flips)])\n                x = torch.tensor(x, requires_grad=True, device=torch.device(self.device), dtype=torch.float32)\n\n                return x\n\n    batches = db.data_loader(iter(dataset), BatchCollator(local_rank), len(dataset) // GPU_batch_size)\n\n    return batches\n\n\ndef make_imagenet_dataloader_y(cfg, logger, dataset, GPU_batch_size, target_size, local_rank, do_random_crops=True):\n    class BatchCollator(object):\n        def __init__(self, device=torch.device(""cpu"")):\n            self.device = device\n            self.flip = cfg.DATASET.FLIP_IMAGES\n            self.size = target_size\n            p = math.log2(target_size)\n            self.source_size = 2 ** p + 2 ** (p - 3)\n            self.do_random_crops = do_random_crops\n\n        def __call__(self, batch):\n            with torch.no_grad():\n                x, y = batch\n\n                if self.do_random_crops:\n                    images = []\n                    for im in x:\n                        deltax = self.source_size - target_size\n                        deltay = self.source_size - target_size\n                        offx = np.random.randint(deltax + 1)\n                        offy = np.random.randint(deltay + 1)\n                        im = im[:, offy:offy+self.size, offx:offx+self.size]\n                        images.append(im)\n                    x = np.stack(images)\n\n                if self.flip:\n                    flips = [(slice(None, None, None), slice(None, None, None), slice(None, None, random.choice([-1, None]))) for _ in range(x.shape[0])]\n                    x = np.array([img[flip] for img, flip in zip(x, flips)])\n                x = torch.tensor(x, requires_grad=True, device=torch.device(self.device), dtype=torch.float32)\n                return x, y\n\n    batches = db.data_loader(iter(dataset), BatchCollator(local_rank), len(dataset) // GPU_batch_size)\n\n    return batches\n'"
defaults.py,0,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom yacs.config import CfgNode as CN\n\n\n_C = CN()\n\n_C.NAME = """"\n_C.PPL_CELEBA_ADJUSTMENT = False\n_C.OUTPUT_DIR = ""results""\n\n_C.DATASET = CN()\n_C.DATASET.PATH = \'celeba/data_fold_%d_lod_%d.pkl\'\n_C.DATASET.PATH_TEST = \'\'\n_C.DATASET.FFHQ_SOURCE = \'/data/datasets/ffhq-dataset/tfrecords/ffhq/ffhq-r%02d.tfrecords\'\n_C.DATASET.PART_COUNT = 1\n_C.DATASET.PART_COUNT_TEST = 1\n_C.DATASET.SIZE = 70000\n_C.DATASET.SIZE_TEST = 10000\n_C.DATASET.FLIP_IMAGES = True\n_C.DATASET.SAMPLES_PATH = \'dataset_samples/faces/realign128x128\'\n\n_C.DATASET.STYLE_MIX_PATH = \'style_mixing/test_images/set_celeba/\'\n\n_C.DATASET.MAX_RESOLUTION_LEVEL = 10\n\n_C.MODEL = CN()\n\n_C.MODEL.LAYER_COUNT = 6\n_C.MODEL.START_CHANNEL_COUNT = 64\n_C.MODEL.MAX_CHANNEL_COUNT = 512\n_C.MODEL.LATENT_SPACE_SIZE = 256\n_C.MODEL.DLATENT_AVG_BETA = 0.995\n_C.MODEL.TRUNCATIOM_PSI = 0.7\n_C.MODEL.TRUNCATIOM_CUTOFF = 8\n_C.MODEL.STYLE_MIXING_PROB = 0.9\n_C.MODEL.MAPPING_LAYERS = 5\n_C.MODEL.CHANNELS = 3\n_C.MODEL.GENERATOR = ""GeneratorDefault""\n_C.MODEL.ENCODER = ""EncoderDefault""\n_C.MODEL.MAPPING_TO_LATENT = ""MappingToLatent""\n_C.MODEL.MAPPING_FROM_LATENT = ""MappingFromLatent""\n_C.MODEL.Z_REGRESSION = False\n\n_C.TRAIN = CN()\n\n_C.TRAIN.EPOCHS_PER_LOD = 15\n\n_C.TRAIN.BASE_LEARNING_RATE = 0.0015\n_C.TRAIN.ADAM_BETA_0 = 0.0\n_C.TRAIN.ADAM_BETA_1 = 0.99\n_C.TRAIN.LEARNING_DECAY_RATE = 0.1\n_C.TRAIN.LEARNING_DECAY_STEPS = []\n_C.TRAIN.TRAIN_EPOCHS = 110\n\n_C.TRAIN.LOD_2_BATCH_8GPU = [512, 256, 128,   64,   32,    32]\n_C.TRAIN.LOD_2_BATCH_4GPU = [512, 256, 128,   64,   32,    16]\n_C.TRAIN.LOD_2_BATCH_2GPU = [256, 256, 128,   64,   32,    16]\n_C.TRAIN.LOD_2_BATCH_1GPU = [128, 128, 128,   64,   32,    16]\n\n\n_C.TRAIN.SNAPSHOT_FREQ = [300, 300, 300, 100, 50, 30, 20, 20, 10]\n\n_C.TRAIN.REPORT_FREQ = [100, 80, 60, 30, 20, 10, 10, 5, 5]\n\n_C.TRAIN.LEARNING_RATES = [0.002]\n\n\ndef get_cfg_defaults():\n    return _C.clone()\n'"
interactive_demo.py,21,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport torch.utils.data\nfrom net import *\nfrom model import Model\nfrom launcher import run\nfrom checkpointer import Checkpointer\nfrom dlutils.pytorch import count_parameters\nfrom defaults import get_cfg_defaults\nimport lreq\n\nfrom PIL import Image\nimport bimpy\n\n\nlreq.use_implicit_lreq.set(True)\n\n\nindices = [0, 1, 2, 3, 4, 10, 11, 17, 19]\n\nlabels = [""gender"",\n          ""smile"",\n          ""attractive"",\n          ""wavy-hair"",\n          ""young"",\n          ""big lips"",\n          ""big nose"",\n          ""chubby"",\n          ""glasses"",\n          ]\n\n\ndef sample(cfg, logger):\n    torch.cuda.set_device(0)\n    model = Model(\n        startf=cfg.MODEL.START_CHANNEL_COUNT,\n        layer_count=cfg.MODEL.LAYER_COUNT,\n        maxf=cfg.MODEL.MAX_CHANNEL_COUNT,\n        latent_size=cfg.MODEL.LATENT_SPACE_SIZE,\n        truncation_psi=cfg.MODEL.TRUNCATIOM_PSI,\n        truncation_cutoff=cfg.MODEL.TRUNCATIOM_CUTOFF,\n        mapping_layers=cfg.MODEL.MAPPING_LAYERS,\n        channels=cfg.MODEL.CHANNELS,\n        generator=cfg.MODEL.GENERATOR,\n        encoder=cfg.MODEL.ENCODER)\n    model.cuda(0)\n    model.eval()\n    model.requires_grad_(False)\n\n    decoder = model.decoder\n    encoder = model.encoder\n    mapping_tl = model.mapping_tl\n    mapping_fl = model.mapping_fl\n    dlatent_avg = model.dlatent_avg\n\n    logger.info(""Trainable parameters generator:"")\n    count_parameters(decoder)\n\n    logger.info(""Trainable parameters discriminator:"")\n    count_parameters(encoder)\n\n    arguments = dict()\n    arguments[""iteration""] = 0\n\n    model_dict = {\n        \'discriminator_s\': encoder,\n        \'generator_s\': decoder,\n        \'mapping_tl_s\': mapping_tl,\n        \'mapping_fl_s\': mapping_fl,\n        \'dlatent_avg\': dlatent_avg\n    }\n\n    checkpointer = Checkpointer(cfg,\n                                model_dict,\n                                {},\n                                logger=logger,\n                                save=False)\n\n    extra_checkpoint_data = checkpointer.load()\n\n    model.eval()\n\n    layer_count = cfg.MODEL.LAYER_COUNT\n\n    def encode(x):\n        Z, _ = model.encode(x, layer_count - 1, 1)\n        Z = Z.repeat(1, model.mapping_fl.num_layers, 1)\n        return Z\n\n    def decode(x):\n        layer_idx = torch.arange(2 * layer_count)[np.newaxis, :, np.newaxis]\n        ones = torch.ones(layer_idx.shape, dtype=torch.float32)\n        coefs = torch.where(layer_idx < model.truncation_cutoff, ones, ones)\n        # x = torch.lerp(model.dlatent_avg.buff.data, x, coefs)\n        return model.decoder(x, layer_count - 1, 1, noise=True)\n\n    path = \'dataset_samples/faces/realign1024x1024\'\n\n    paths = list(os.listdir(path))\n    paths.sort()\n    paths_backup = paths[:]\n    randomize = bimpy.Bool(True)\n    current_file = bimpy.String("""")\n\n    ctx = bimpy.Context()\n\n    attribute_values = [bimpy.Float(0) for i in indices]\n\n    W = [torch.tensor(np.load(""principal_directions/direction_%d.npy"" % i), dtype=torch.float32) for i in indices]\n\n    rnd = np.random.RandomState(5)\n\n    def loadNext():\n        img = np.asarray(Image.open(path + \'/\' + paths[0]))\n        current_file.value = paths[0]\n        paths.pop(0)\n        if len(paths) == 0:\n            paths.extend(paths_backup)\n\n        if img.shape[2] == 4:\n            img = img[:, :, :3]\n        im = img.transpose((2, 0, 1))\n        x = torch.tensor(np.asarray(im, dtype=np.float32), device=\'cpu\', requires_grad=True).cuda() / 127.5 - 1.\n        if x.shape[0] == 4:\n            x = x[:3]\n\n        needed_resolution = model.decoder.layer_to_resolution[-1]\n        while x.shape[2] > needed_resolution:\n            x = F.avg_pool2d(x, 2, 2)\n        if x.shape[2] != needed_resolution:\n            x = F.adaptive_avg_pool2d(x, (needed_resolution, needed_resolution))\n\n        img_src = ((x * 0.5 + 0.5) * 255).type(torch.long).clamp(0, 255).cpu().type(torch.uint8).transpose(0, 2).transpose(0, 1).numpy()\n\n        latents_original = encode(x[None, ...].cuda())\n        latents = latents_original[0, 0].clone()\n        latents -= model.dlatent_avg.buff.data[0]\n\n        for v, w in zip(attribute_values, W):\n            v.value = (latents * w).sum()\n\n        for v, w in zip(attribute_values, W):\n            latents = latents - v.value * w\n\n        return latents, latents_original, img_src\n\n    def loadRandom():\n        latents = rnd.randn(1, cfg.MODEL.LATENT_SPACE_SIZE)\n        lat = torch.tensor(latents).float().cuda()\n        dlat = mapping_fl(lat)\n        layer_idx = torch.arange(2 * layer_count)[np.newaxis, :, np.newaxis]\n        ones = torch.ones(layer_idx.shape, dtype=torch.float32)\n        coefs = torch.where(layer_idx < model.truncation_cutoff, ones, ones)\n        dlat = torch.lerp(model.dlatent_avg.buff.data, dlat, coefs)\n        x = decode(dlat)[0]\n        img_src = ((x * 0.5 + 0.5) * 255).type(torch.long).clamp(0, 255).cpu().type(torch.uint8).transpose(0, 2).transpose(0, 1).numpy()\n        latents_original = dlat\n        latents = latents_original[0, 0].clone()\n        latents -= model.dlatent_avg.buff.data[0]\n\n        for v, w in zip(attribute_values, W):\n            v.value = (latents * w).sum()\n\n        for v, w in zip(attribute_values, W):\n            latents = latents - v.value * w\n\n        return latents, latents_original, img_src\n\n    latents, latents_original, img_src = loadNext()\n\n    ctx.init(1800, 1600, ""Styles"")\n\n    def update_image(w, latents_original):\n        with torch.no_grad():\n            w = w + model.dlatent_avg.buff.data[0]\n            w = w[None, None, ...].repeat(1, model.mapping_fl.num_layers, 1)\n\n            layer_idx = torch.arange(model.mapping_fl.num_layers)[np.newaxis, :, np.newaxis]\n            cur_layers = (7 + 1) * 2\n            mixing_cutoff = cur_layers\n            styles = torch.where(layer_idx < mixing_cutoff, w, latents_original)\n\n            x_rec = decode(styles)\n            resultsample = ((x_rec * 0.5 + 0.5) * 255).type(torch.long).clamp(0, 255)\n            resultsample = resultsample.cpu()[0, :, :, :]\n            return resultsample.type(torch.uint8).transpose(0, 2).transpose(0, 1)\n\n    im_size = 2 ** (cfg.MODEL.LAYER_COUNT + 1)\n    im = update_image(latents, latents_original)\n    print(im.shape)\n    im = bimpy.Image(im)\n\n    display_original = True\n\n    seed = 0\n\n    while not ctx.should_close():\n        with ctx:\n            new_latents = latents + sum([v.value * w for v, w in zip(attribute_values, W)])\n\n            if display_original:\n                im = bimpy.Image(img_src)\n            else:\n                im = bimpy.Image(update_image(new_latents, latents_original))\n\n            bimpy.begin(""Principal directions"")\n            bimpy.columns(2)\n            bimpy.set_column_width(0, im_size + 20)\n            bimpy.image(im)\n            bimpy.next_column()\n\n            for v, label in zip(attribute_values, labels):\n                bimpy.slider_float(label, v, -40.0, 40.0)\n\n            bimpy.checkbox(""Randomize noise"", randomize)\n\n            if randomize.value:\n                seed += 1\n\n            torch.manual_seed(seed)\n\n            if bimpy.button(\'Next\'):\n                latents, latents_original, img_src = loadNext()\n                display_original = True\n            if bimpy.button(\'Display Reconstruction\'):\n                display_original = False\n            if bimpy.button(\'Generate random\'):\n                latents, latents_original, img_src = loadRandom()\n                display_original = False\n\n            if bimpy.input_text(""Current file"", current_file, 64) and os.path.exists(path + \'/\' + current_file.value):\n                paths.insert(0, current_file.value)\n                latents, latents_original, img_src = loadNext()\n\n            bimpy.end()\n\n\nif __name__ == ""__main__"":\n    gpu_count = 1\n    run(sample, get_cfg_defaults(), description=\'ALAE-interactive\', default_config=\'configs/ffhq.yaml\',\n        world_size=gpu_count, write_log=False)\n'"
launcher.py,5,"b'# Copyright 2019 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport os\nimport sys\nimport argparse\nimport logging\nimport torch\nimport torch.multiprocessing as mp\nfrom torch import distributed\nimport inspect\n\n\ndef setup(rank, world_size):\n    os.environ[\'MASTER_ADDR\'] = \'localhost\'\n    os.environ[\'MASTER_PORT\'] = \'12355\'\n    distributed.init_process_group(""nccl"", rank=rank, world_size=world_size)\n\n\ndef cleanup():\n    distributed.destroy_process_group()\n\n\ndef _run(rank, world_size, fn, defaults, write_log, no_cuda, args):\n    if world_size > 1:\n        setup(rank, world_size)\n    if not no_cuda:\n        torch.cuda.set_device(rank)\n\n    cfg = defaults\n    config_file = args.config_file\n    if len(os.path.splitext(config_file)[1]) == 0:\n        config_file += \'.yaml\'\n    if not os.path.exists(config_file) and os.path.exists(os.path.join(\'configs\', config_file)):\n        config_file = os.path.join(\'configs\', config_file)\n    cfg.merge_from_file(config_file)\n    cfg.merge_from_list(args.opts)\n    cfg.freeze()\n\n    logger = logging.getLogger(""logger"")\n    logger.setLevel(logging.DEBUG)\n\n    output_dir = cfg.OUTPUT_DIR\n    os.makedirs(output_dir, exist_ok=True)\n\n    if rank == 0:\n        ch = logging.StreamHandler(stream=sys.stdout)\n        ch.setLevel(logging.DEBUG)\n        formatter = logging.Formatter(""%(asctime)s %(name)s %(levelname)s: %(message)s"")\n        ch.setFormatter(formatter)\n        logger.addHandler(ch)\n\n        if write_log:\n            filepath = os.path.join(output_dir, \'log.txt\')\n            if isinstance(write_log, str):\n                filepath = write_log\n            fh = logging.FileHandler(filepath)\n            fh.setLevel(logging.DEBUG)\n            fh.setFormatter(formatter)\n            logger.addHandler(fh)\n\n    logger.info(args)\n\n    logger.info(""World size: {}"".format(world_size))\n\n    logger.info(""Loaded configuration file {}"".format(config_file))\n    with open(config_file, ""r"") as cf:\n        config_str = ""\\n"" + cf.read()\n        logger.info(config_str)\n    logger.info(""Running with config:\\n{}"".format(cfg))\n\n    if not no_cuda:\n        torch.set_default_tensor_type(\'torch.cuda.FloatTensor\')\n        device = torch.cuda.current_device()\n        print(""Running on "", torch.cuda.get_device_name(device))\n\n    args.distributed = world_size > 1\n    args_to_pass = dict(cfg=cfg, logger=logger, local_rank=rank, world_size=world_size, distributed=args.distributed)\n    signature = inspect.signature(fn)\n    matching_args = {}\n    for key in args_to_pass.keys():\n        if key in signature.parameters.keys():\n            matching_args[key] = args_to_pass[key]\n    fn(**matching_args)\n\n    if world_size > 1:\n        cleanup()\n\n\ndef run(fn, defaults, description=\'\', default_config=\'configs/experiment.yaml\', world_size=1, write_log=True, no_cuda=False):\n    parser = argparse.ArgumentParser(description=description)\n    parser.add_argument(\n        ""-c"", ""--config-file"",\n        default=default_config,\n        metavar=""FILE"",\n        help=""path to config file"",\n        type=str,\n    )\n    parser.add_argument(\n        ""opts"",\n        help=""Modify config options using the command-line"",\n        default=None,\n        nargs=argparse.REMAINDER,\n    )\n\n    import multiprocessing\n    cpu_count = multiprocessing.cpu_count()\n    os.environ[""OMP_NUM_THREADS""] = str(max(1, int(cpu_count / world_size)))\n    del multiprocessing\n\n    args = parser.parse_args()\n\n    if world_size > 1:\n        mp.spawn(_run,\n                 args=(world_size, fn, defaults, write_log, no_cuda, args),\n                 nprocs=world_size,\n                 join=True)\n    else:\n        _run(0, world_size, fn, defaults, write_log, no_cuda, args)\n'"
lod_driver.py,0,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport torch\nimport math\nimport time\nfrom collections import defaultdict\n\n\nclass LODDriver:\n    def __init__(self, cfg, logger, world_size, dataset_size):\n        if world_size == 8:\n            self.lod_2_batch = cfg.TRAIN.LOD_2_BATCH_8GPU\n        if world_size == 4:\n            self.lod_2_batch = cfg.TRAIN.LOD_2_BATCH_4GPU\n        if world_size == 2:\n            self.lod_2_batch = cfg.TRAIN.LOD_2_BATCH_2GPU\n        if world_size == 1:\n            self.lod_2_batch = cfg.TRAIN.LOD_2_BATCH_1GPU\n\n        self.world_size = world_size\n        self.minibatch_base = 16\n        self.cfg = cfg\n        self.dataset_size = dataset_size\n        self.current_epoch = 0\n        self.lod = -1\n        self.in_transition = False\n        self.logger = logger\n        self.iteration = 0\n        self.epoch_end_time = 0\n        self.epoch_start_time = 0\n        self.per_epoch_ptime = 0\n        self.reports = cfg.TRAIN.REPORT_FREQ\n        self.snapshots = cfg.TRAIN.SNAPSHOT_FREQ\n        self.tick_start_nimg_report = 0\n        self.tick_start_nimg_snapshot = 0\n\n    def get_lod_power2(self):\n        return self.lod + 2\n\n    def get_batch_size(self):\n        return self.lod_2_batch[min(self.lod, len(self.lod_2_batch) - 1)]\n\n    def get_dataset_size(self):\n        return self.dataset_size\n\n    def get_per_GPU_batch_size(self):\n        return self.get_batch_size() // self.world_size\n\n    def get_blend_factor(self):\n        if self.cfg.TRAIN.EPOCHS_PER_LOD == 0:\n            return 1\n        blend_factor = float((self.current_epoch % self.cfg.TRAIN.EPOCHS_PER_LOD) * self.dataset_size + self.iteration)\n        blend_factor /= float(self.cfg.TRAIN.EPOCHS_PER_LOD // 2 * self.dataset_size)\n        blend_factor = math.sin(blend_factor * math.pi - 0.5 * math.pi) * 0.5 + 0.5\n\n        if not self.in_transition:\n            blend_factor = 1\n\n        return blend_factor\n\n    def is_time_to_report(self):\n        if self.iteration >= self.tick_start_nimg_report + self.reports[min(self.lod, len(self.reports) - 1)] * 1000:\n            self.tick_start_nimg_report = self.iteration\n            return True\n        return False\n\n    def is_time_to_save(self):\n        if self.iteration >= self.tick_start_nimg_snapshot + self.snapshots[min(self.lod, len(self.snapshots) - 1)] * 1000:\n            self.tick_start_nimg_snapshot = self.iteration\n            return True\n        return False\n\n    def step(self):\n        self.iteration += self.get_batch_size()\n        self.epoch_end_time = time.time()\n        self.per_epoch_ptime = self.epoch_end_time - self.epoch_start_time\n\n    def set_epoch(self, epoch, optimizers):\n        self.current_epoch = epoch\n        self.iteration = 0\n        self.tick_start_nimg_report = 0\n        self.tick_start_nimg_snapshot = 0\n        self.epoch_start_time = time.time()\n\n        if self.cfg.TRAIN.EPOCHS_PER_LOD == 0:\n            self.lod = self.cfg.MODEL.LAYER_COUNT - 1\n            return\n\n        new_lod = min(self.cfg.MODEL.LAYER_COUNT - 1, epoch // self.cfg.TRAIN.EPOCHS_PER_LOD)\n        if new_lod != self.lod:\n            self.lod = new_lod\n            self.logger.info(""#"" * 80)\n            self.logger.info(""# Switching LOD to %d"" % self.lod)\n            self.logger.info(""# Starting transition"")\n            self.logger.info(""#"" * 80)\n            self.in_transition = True\n            for opt in optimizers:\n                opt.state = defaultdict(dict)\n\n        is_in_first_half_of_cycle = (epoch % self.cfg.TRAIN.EPOCHS_PER_LOD) < (self.cfg.TRAIN.EPOCHS_PER_LOD // 2)\n        is_growing = epoch // self.cfg.TRAIN.EPOCHS_PER_LOD == self.lod > 0\n        new_in_transition = is_in_first_half_of_cycle and is_growing\n\n        if new_in_transition != self.in_transition:\n            self.in_transition = new_in_transition\n            self.logger.info(""#"" * 80)\n            self.logger.info(""# Transition ended"")\n            self.logger.info(""#"" * 80)\n'"
losses.py,7,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport torch\nimport torch.nn.functional as F\n\n\n__all__ = [\'kl\', \'reconstruction\', \'discriminator_logistic_simple_gp\',\n           \'discriminator_gradient_penalty\', \'generator_logistic_non_saturating\']\n\n\ndef kl(mu, log_var):\n    return -0.5 * torch.mean(torch.mean(1 + log_var - mu.pow(2) - log_var.exp(), 1))\n\n\ndef reconstruction(recon_x, x, lod=None):\n    return torch.mean((recon_x - x)**2)\n\n\ndef discriminator_logistic_simple_gp(d_result_fake, d_result_real, reals, r1_gamma=10.0):\n    loss = (F.softplus(d_result_fake) + F.softplus(-d_result_real))\n\n    if r1_gamma != 0.0:\n        real_loss = d_result_real.sum()\n        real_grads = torch.autograd.grad(real_loss, reals, create_graph=True, retain_graph=True)[0]\n        r1_penalty = torch.sum(real_grads.pow(2.0), dim=[1, 2, 3])\n        loss = loss + r1_penalty * (r1_gamma * 0.5)\n    return loss.mean()\n\n\ndef discriminator_gradient_penalty(d_result_real, reals, r1_gamma=10.0):\n    real_loss = d_result_real.sum()\n    real_grads = torch.autograd.grad(real_loss, reals, create_graph=True, retain_graph=True)[0]\n    r1_penalty = torch.sum(real_grads.pow(2.0), dim=[1, 2, 3])\n    loss = r1_penalty * (r1_gamma * 0.5)\n    return loss.mean()\n\n\ndef generator_logistic_non_saturating(d_result_fake):\n    return F.softplus(-d_result_fake).mean()\n'"
lreq.py,10,"b'# Copyright 2019 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.nn import init\nfrom torch.nn.parameter import Parameter\nimport numpy as np\n\n\nclass Bool:\n    def __init__(self):\n        self.value = False\n\n    def __bool__(self):\n        return self.value\n    __nonzero__ = __bool__\n\n    def set(self, value):\n        self.value = value\n\n\nuse_implicit_lreq = Bool()\nuse_implicit_lreq.set(True)\n\n\ndef is_sequence(arg):\n    return (not hasattr(arg, ""strip"") and\n            hasattr(arg, ""__getitem__"") or\n            hasattr(arg, ""__iter__""))\n\n\ndef make_tuple(x, n):\n    if is_sequence(x):\n        return x\n    return tuple([x for _ in range(n)])\n\n\nclass Linear(nn.Module):\n    def __init__(self, in_features, out_features, bias=True, gain=np.sqrt(2.0), lrmul=1.0, implicit_lreq=use_implicit_lreq):\n        super(Linear, self).__init__()\n        self.in_features = in_features\n        self.weight = Parameter(torch.Tensor(out_features, in_features))\n        if bias:\n            self.bias = Parameter(torch.Tensor(out_features))\n        else:\n            self.register_parameter(\'bias\', None)\n        self.std = 0\n        self.gain = gain\n        self.lrmul = lrmul\n        self.implicit_lreq = implicit_lreq\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        self.std = self.gain / np.sqrt(self.in_features) * self.lrmul\n        if not self.implicit_lreq:\n            init.normal_(self.weight, mean=0, std=1.0 / self.lrmul)\n        else:\n            init.normal_(self.weight, mean=0, std=self.std / self.lrmul)\n            setattr(self.weight, \'lr_equalization_coef\', self.std)\n            if self.bias is not None:\n                setattr(self.bias, \'lr_equalization_coef\', self.lrmul)\n\n        if self.bias is not None:\n            with torch.no_grad():\n                self.bias.zero_()\n\n    def forward(self, input):\n        if not self.implicit_lreq:\n            bias = self.bias\n            if bias is not None:\n                bias = bias * self.lrmul\n            return F.linear(input, self.weight * self.std, bias)\n        else:\n            return F.linear(input, self.weight, self.bias)\n\n\nclass Conv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, dilation=1,\n                 groups=1, bias=True, gain=np.sqrt(2.0), transpose=False, transform_kernel=False, lrmul=1.0,\n                 implicit_lreq=use_implicit_lreq):\n        super(Conv2d, self).__init__()\n        if in_channels % groups != 0:\n            raise ValueError(\'in_channels must be divisible by groups\')\n        if out_channels % groups != 0:\n            raise ValueError(\'out_channels must be divisible by groups\')\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = make_tuple(kernel_size, 2)\n        self.stride = make_tuple(stride, 2)\n        self.padding = make_tuple(padding, 2)\n        self.output_padding = make_tuple(output_padding, 2)\n        self.dilation = make_tuple(dilation, 2)\n        self.groups = groups\n        self.gain = gain\n        self.lrmul = lrmul\n        self.transpose = transpose\n        self.fan_in = np.prod(self.kernel_size) * in_channels // groups\n        self.transform_kernel = transform_kernel\n        if transpose:\n            self.weight = Parameter(torch.Tensor(in_channels, out_channels // groups, *self.kernel_size))\n        else:\n            self.weight = Parameter(torch.Tensor(out_channels, in_channels // groups, *self.kernel_size))\n        if bias:\n            self.bias = Parameter(torch.Tensor(out_channels))\n        else:\n            self.register_parameter(\'bias\', None)\n        self.std = 0\n        self.implicit_lreq = implicit_lreq\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        self.std = self.gain / np.sqrt(self.fan_in)\n        if not self.implicit_lreq:\n            init.normal_(self.weight, mean=0, std=1.0 / self.lrmul)\n        else:\n            init.normal_(self.weight, mean=0, std=self.std / self.lrmul)\n            setattr(self.weight, \'lr_equalization_coef\', self.std)\n            if self.bias is not None:\n                setattr(self.bias, \'lr_equalization_coef\', self.lrmul)\n\n        if self.bias is not None:\n            with torch.no_grad():\n                self.bias.zero_()\n\n    def forward(self, x):\n        if self.transpose:\n            w = self.weight\n            if self.transform_kernel:\n                w = F.pad(w, (1, 1, 1, 1), mode=\'constant\')\n                w = w[:, :, 1:, 1:] + w[:, :, :-1, 1:] + w[:, :, 1:, :-1] + w[:, :, :-1, :-1]\n            if not self.implicit_lreq:\n                bias = self.bias\n                if bias is not None:\n                    bias = bias * self.lrmul\n                return F.conv_transpose2d(x, w * self.std, bias, stride=self.stride,\n                                          padding=self.padding, output_padding=self.output_padding,\n                                          dilation=self.dilation, groups=self.groups)\n            else:\n                return F.conv_transpose2d(x, w, self.bias, stride=self.stride, padding=self.padding,\n                                          output_padding=self.output_padding, dilation=self.dilation,\n                                          groups=self.groups)\n        else:\n            w = self.weight\n            if self.transform_kernel:\n                w = F.pad(w, (1, 1, 1, 1), mode=\'constant\')\n                w = (w[:, :, 1:, 1:] + w[:, :, :-1, 1:] + w[:, :, 1:, :-1] + w[:, :, :-1, :-1]) * 0.25\n            if not self.implicit_lreq:\n                bias = self.bias\n                if bias is not None:\n                    bias = bias * self.lrmul\n                return F.conv2d(x, w * self.std, bias, stride=self.stride, padding=self.padding,\n                                dilation=self.dilation, groups=self.groups)\n            else:\n                return F.conv2d(x, w, self.bias, stride=self.stride, padding=self.padding,\n                                dilation=self.dilation, groups=self.groups)\n\n\nclass ConvTranspose2d(Conv2d):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, dilation=1,\n                 groups=1, bias=True, gain=np.sqrt(2.0), transform_kernel=False, lrmul=1.0, implicit_lreq=use_implicit_lreq):\n        super(ConvTranspose2d, self).__init__(in_channels=in_channels,\n                                              out_channels=out_channels,\n                                              kernel_size=kernel_size,\n                                              stride=stride,\n                                              padding=padding,\n                                              output_padding=output_padding,\n                                              dilation=dilation,\n                                              groups=groups,\n                                              bias=bias,\n                                              gain=gain,\n                                              transpose=True,\n                                              transform_kernel=transform_kernel,\n                                              lrmul=lrmul,\n                                              implicit_lreq=implicit_lreq)\n\n\nclass SeparableConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, dilation=1,\n                 bias=True, gain=np.sqrt(2.0), transpose=False):\n        super(SeparableConv2d, self).__init__()\n        self.spatial_conv = Conv2d(in_channels, in_channels, kernel_size, stride, padding, output_padding, dilation,\n                                   in_channels, False, 1, transpose)\n        self.channel_conv = Conv2d(in_channels, out_channels, 1, bias, 1, gain=gain)\n\n    def forward(self, x):\n        return self.channel_conv(self.spatial_conv(x))\n\n\nclass SeparableConvTranspose2d(Conv2d):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, dilation=1,\n                 bias=True, gain=np.sqrt(2.0)):\n        super(SeparableConvTranspose2d, self).__init__(in_channels, out_channels, kernel_size, stride, padding,\n                                              output_padding, dilation, bias, gain, True)\n\n'"
model.py,21,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport random\nimport losses\nfrom net import *\nimport numpy as np\n\n\nclass DLatent(nn.Module):\n    def __init__(self, dlatent_size, layer_count):\n        super(DLatent, self).__init__()\n        buffer = torch.zeros(layer_count, dlatent_size, dtype=torch.float32)\n        self.register_buffer(\'buff\', buffer)\n\n\nclass Model(nn.Module):\n    def __init__(self, startf=32, maxf=256, layer_count=3, latent_size=128, mapping_layers=5, dlatent_avg_beta=None,\n                 truncation_psi=None, truncation_cutoff=None, style_mixing_prob=None, channels=3, generator="""",\n                 encoder="""", z_regression=False):\n        super(Model, self).__init__()\n\n        self.layer_count = layer_count\n        self.z_regression = z_regression\n\n        self.mapping_tl = MAPPINGS[""MappingToLatent""](\n            latent_size=latent_size,\n            dlatent_size=latent_size,\n            mapping_fmaps=latent_size,\n            mapping_layers=3)\n\n        self.mapping_fl = MAPPINGS[""MappingFromLatent""](\n            num_layers=2 * layer_count,\n            latent_size=latent_size,\n            dlatent_size=latent_size,\n            mapping_fmaps=latent_size,\n            mapping_layers=mapping_layers)\n\n        self.decoder = GENERATORS[generator](\n            startf=startf,\n            layer_count=layer_count,\n            maxf=maxf,\n            latent_size=latent_size,\n            channels=channels)\n\n        self.encoder = ENCODERS[encoder](\n            startf=startf,\n            layer_count=layer_count,\n            maxf=maxf,\n            latent_size=latent_size,\n            channels=channels)\n\n        self.dlatent_avg = DLatent(latent_size, self.mapping_fl.num_layers)\n        self.latent_size = latent_size\n        self.dlatent_avg_beta = dlatent_avg_beta\n        self.truncation_psi = truncation_psi\n        self.style_mixing_prob = style_mixing_prob\n        self.truncation_cutoff = truncation_cutoff\n\n    def generate(self, lod, blend_factor, z=None, count=32, mixing=True, noise=True, return_styles=False, no_truncation=False):\n        if z is None:\n            z = torch.randn(count, self.latent_size)\n        styles = self.mapping_fl(z)[:, 0]\n        s = styles.view(styles.shape[0], 1, styles.shape[1])\n\n        styles = s.repeat(1, self.mapping_fl.num_layers, 1)\n\n        if self.dlatent_avg_beta is not None:\n            with torch.no_grad():\n                batch_avg = styles.mean(dim=0)\n                self.dlatent_avg.buff.data.lerp_(batch_avg.data, 1.0 - self.dlatent_avg_beta)\n\n        if mixing and self.style_mixing_prob is not None:\n            if random.random() < self.style_mixing_prob:\n                z2 = torch.randn(count, self.latent_size)\n                styles2 = self.mapping_fl(z2)[:, 0]\n                styles2 = styles2.view(styles2.shape[0], 1, styles2.shape[1]).repeat(1, self.mapping_fl.num_layers, 1)\n\n                layer_idx = torch.arange(self.mapping_fl.num_layers)[np.newaxis, :, np.newaxis]\n                cur_layers = (lod + 1) * 2\n                mixing_cutoff = random.randint(1, cur_layers)\n                styles = torch.where(layer_idx < mixing_cutoff, styles, styles2)\n\n        if (self.truncation_psi is not None) and not no_truncation:\n            layer_idx = torch.arange(self.mapping_fl.num_layers)[np.newaxis, :, np.newaxis]\n            ones = torch.ones(layer_idx.shape, dtype=torch.float32)\n            coefs = torch.where(layer_idx < self.truncation_cutoff, self.truncation_psi * ones, ones)\n            styles = torch.lerp(self.dlatent_avg.buff.data, styles, coefs)\n\n        rec = self.decoder.forward(styles, lod, blend_factor, noise)\n        if return_styles:\n            return s, rec\n        else:\n            return rec\n\n    def encode(self, x, lod, blend_factor):\n        Z = self.encoder(x, lod, blend_factor)\n        Z_ = self.mapping_tl(Z)\n        return Z[:, :1], Z_[:, 1, 0]\n\n    def forward(self, x, lod, blend_factor, d_train, ae):\n        if ae:\n            self.encoder.requires_grad_(True)\n\n            z = torch.randn(x.shape[0], self.latent_size)\n            s, rec = self.generate(lod, blend_factor, z=z, mixing=False, noise=True, return_styles=True)\n\n            Z, d_result_real = self.encode(rec, lod, blend_factor)\n\n            assert Z.shape == s.shape\n\n            if self.z_regression:\n                Lae = torch.mean(((Z[:, 0] - z)**2))\n            else:\n                Lae = torch.mean(((Z - s.detach())**2))\n\n            return Lae\n\n        elif d_train:\n            with torch.no_grad():\n                Xp = self.generate(lod, blend_factor, count=x.shape[0], noise=True)\n\n            self.encoder.requires_grad_(True)\n\n            _, d_result_real = self.encode(x, lod, blend_factor)\n\n            _, d_result_fake = self.encode(Xp.detach(), lod, blend_factor)\n\n            loss_d = losses.discriminator_logistic_simple_gp(d_result_fake, d_result_real, x)\n            return loss_d\n        else:\n            with torch.no_grad():\n                z = torch.randn(x.shape[0], self.latent_size)\n\n            self.encoder.requires_grad_(False)\n\n            rec = self.generate(lod, blend_factor, count=x.shape[0], z=z.detach(), noise=True)\n\n            _, d_result_fake = self.encode(rec, lod, blend_factor)\n\n            loss_g = losses.generator_logistic_non_saturating(d_result_fake)\n\n            return loss_g\n\n    def lerp(self, other, betta):\n        if hasattr(other, \'module\'):\n            other = other.module\n        with torch.no_grad():\n            params = list(self.mapping_tl.parameters()) + list(self.mapping_fl.parameters()) + list(self.decoder.parameters()) + list(self.encoder.parameters()) + list(self.dlatent_avg.parameters())\n            other_param = list(other.mapping_tl.parameters()) + list(other.mapping_fl.parameters()) + list(other.decoder.parameters()) + list(other.encoder.parameters()) + list(other.dlatent_avg.parameters())\n            for p, p_other in zip(params, other_param):\n                p.data.lerp_(p_other.data, 1.0 - betta)\n\n\nclass GenModel(nn.Module):\n    def __init__(self, startf=32, maxf=256, layer_count=3, latent_size=128, mapping_layers=5, dlatent_avg_beta=None,\n                 truncation_psi=None, truncation_cutoff=None, style_mixing_prob=None, channels=3, generator="""", encoder="""", z_regression=False):\n        super(GenModel, self).__init__()\n\n        self.layer_count = layer_count\n\n        self.mapping_fl = MAPPINGS[""MappingFromLatent""](\n            num_layers=2 * layer_count,\n            latent_size=latent_size,\n            dlatent_size=latent_size,\n            mapping_fmaps=latent_size,\n            mapping_layers=mapping_layers)\n\n        self.decoder = GENERATORS[generator](\n            startf=startf,\n            layer_count=layer_count,\n            maxf=maxf,\n            latent_size=latent_size,\n            channels=channels)\n\n        self.dlatent_avg = DLatent(latent_size, self.mapping_fl.num_layers)\n        self.latent_size = latent_size\n        self.dlatent_avg_beta = dlatent_avg_beta\n        self.truncation_psi = truncation_psi\n        self.style_mixing_prob = style_mixing_prob\n        self.truncation_cutoff = truncation_cutoff\n\n    def generate(self, lod, blend_factor, z=None):\n        styles = self.mapping_fl(z)[:, 0]\n        s = styles.view(styles.shape[0], 1, styles.shape[1])\n\n        styles = s.repeat(1, self.mapping_fl.num_layers, 1)\n\n        layer_idx = torch.arange(self.mapping_fl.num_layers)[np.newaxis, :, np.newaxis]\n        ones = torch.ones(layer_idx.shape, dtype=torch.float32)\n        coefs = torch.where(layer_idx < self.truncation_cutoff, self.truncation_psi * ones, ones)\n        styles = torch.lerp(self.dlatent_avg.buff.data, styles, coefs)\n\n        rec = self.decoder.forward(styles, lod, blend_factor, True)\n        return rec\n\n    def forward(self, x):\n        return self.generate(self.layer_count-1, 1.0, z=x)\n'"
model_separate.py,16,"b'# Copyright 2019 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport random\nimport losses\nfrom net import *\nimport numpy as np\n\n\nclass DLatent(nn.Module):\n    def __init__(self, dlatent_size, layer_count):\n        super(DLatent, self).__init__()\n        buffer = torch.zeros(layer_count, dlatent_size, dtype=torch.float32)\n        self.register_buffer(\'buff\', buffer)\n\n\nclass Model(nn.Module):\n    def __init__(self, startf=32, maxf=256, layer_count=3, latent_size=128, mapping_layers=5, dlatent_avg_beta=None,\n                 truncation_psi=None, truncation_cutoff=None, style_mixing_prob=None, channels=3, generator="""", encoder=""""):\n        super(Model, self).__init__()\n\n        self.layer_count = layer_count\n\n        self.mapping_fl = Mapping(\n            num_layers=2 * layer_count,\n            latent_size=latent_size,\n            dlatent_size=latent_size,\n            mapping_fmaps=latent_size,\n            mapping_layers=mapping_layers)\n\n        #self.decoder = Generator(\n        self.decoder = GENERATORS[generator](\n            startf=startf,\n            layer_count=layer_count,\n            maxf=maxf,\n            latent_size=latent_size,\n            channels=channels)\n\n        #self.encoder = Encoder_old(\n        self.encoder = ENCODERS[encoder](\n            startf=startf,\n            layer_count=layer_count,\n            maxf=maxf,\n            latent_size=latent_size,\n            channels=channels)\n\n        self.discriminator = Discriminator(\n            startf=startf,\n            layer_count=layer_count,\n            maxf=maxf,\n            channels=channels)\n\n        self.dlatent_avg = DLatent(latent_size, self.mapping_fl.num_layers)\n        self.latent_size = latent_size\n        self.dlatent_avg_beta = dlatent_avg_beta\n        self.truncation_psi = truncation_psi\n        self.style_mixing_prob = style_mixing_prob\n        self.truncation_cutoff = truncation_cutoff\n\n    def generate(self, lod, blend_factor, z=None, count=32, mixing=True, noise=True, return_styles=False, no_truncation=False):\n        if z is None:\n            z = torch.randn(count, self.latent_size)\n        styles = self.mapping_fl(z)[:, 0]\n        s = styles.view(styles.shape[0], 1, styles.shape[1])\n\n        styles = s.repeat(1, self.mapping_fl.num_layers, 1)\n\n        if self.dlatent_avg_beta is not None:\n            with torch.no_grad():\n                batch_avg = styles.mean(dim=0)\n                self.dlatent_avg.buff.data.lerp_(batch_avg.data, 1.0 - self.dlatent_avg_beta)\n\n        if mixing and self.style_mixing_prob is not None:\n            if random.random() < self.style_mixing_prob:\n                z2 = torch.randn(count, self.latent_size)\n                styles2 = self.mapping_fl(z2)[:, 0]\n                styles2 = styles2.view(styles2.shape[0], 1, styles2.shape[1]).repeat(1, self.mapping_fl.num_layers, 1)\n\n                layer_idx = torch.arange(self.mapping_fl.num_layers)[np.newaxis, :, np.newaxis]\n                cur_layers = (lod + 1) * 2\n                mixing_cutoff = random.randint(1, cur_layers)\n                styles = torch.where(layer_idx < mixing_cutoff, styles, styles2)\n\n        if (self.truncation_psi is not None) and not no_truncation:\n            layer_idx = torch.arange(self.mapping_fl.num_layers)[np.newaxis, :, np.newaxis]\n            ones = torch.ones(layer_idx.shape, dtype=torch.float32)\n            coefs = torch.where(layer_idx < self.truncation_cutoff, self.truncation_psi * ones, ones)\n            styles = torch.lerp(self.dlatent_avg.buff.data, styles, coefs)\n\n        rec = self.decoder.forward(styles, lod, blend_factor, noise)\n        if return_styles:\n            return s, rec\n        else:\n            return rec\n\n    def encode(self, x, lod, blend_factor):\n        Z = self.encoder(x, lod, blend_factor)\n        return Z\n\n    def forward(self, x, lod, blend_factor, d_train, ae, alt):\n        if ae:\n            self.encoder.requires_grad_(True)\n\n            z = torch.randn(x.shape[0], self.latent_size)\n            s, rec = self.generate(lod, blend_factor, z=z, mixing=False, noise=True, return_styles=True)\n\n            Z = self.encode(rec, lod, blend_factor)\n\n            assert Z.shape == s.shape\n\n            Lae = torch.mean(((Z - s.detach())**2))\n\n            return Lae\n\n        elif d_train:\n            with torch.no_grad():\n                Xp = self.generate(lod, blend_factor, count=x.shape[0], noise=True)\n\n            self.discriminator.requires_grad_(True)\n\n            d_result_real = self.discriminator(x, lod, blend_factor)\n\n            d_result_fake = self.discriminator(Xp.detach(), lod, blend_factor)\n\n            loss_d = losses.discriminator_logistic_simple_gp(d_result_fake, d_result_real, x)\n            return loss_d\n        else:\n            with torch.no_grad():\n                z = torch.randn(x.shape[0], self.latent_size)\n\n            self.discriminator.requires_grad_(False)\n\n            rec = self.generate(lod, blend_factor, count=x.shape[0], z=z.detach(), noise=True)\n\n            d_result_fake = self.discriminator(rec, lod, blend_factor)\n\n            loss_g = losses.generator_logistic_non_saturating(d_result_fake)\n\n            return loss_g\n\n    def lerp(self, other, betta):\n        if hasattr(other, \'module\'):\n            other = other.module\n        with torch.no_grad():\n            params = list(self.mapping_fl.parameters()) + list(self.decoder.parameters()) + list(self.encoder.parameters())  + list(self.discriminator.parameters()) + list(self.dlatent_avg.parameters())\n            other_param = list(other.mapping_fl.parameters()) + list(other.decoder.parameters()) + list(other.encoder.parameters()) + list(other.dlatent_avg.parameters())\n            for p, p_other in zip(params, other_param):\n                p.data.lerp_(p_other.data, 1.0 - betta)\n'"
net.py,48,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n# \n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n# \n#  http://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport os\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.nn import init\nfrom torch.nn.parameter import Parameter\nimport numpy as np\nimport lreq as ln\nimport math\nfrom registry import *\n\n\ndef pixel_norm(x, epsilon=1e-8):\n    return x * torch.rsqrt(torch.mean(x.pow(2.0), dim=1, keepdim=True) + epsilon)\n\n\ndef style_mod(x, style):\n    style = style.view(style.shape[0], 2, x.shape[1], 1, 1)\n    return torch.addcmul(style[:, 1], value=1.0, tensor1=x, tensor2=style[:, 0] + 1)\n\n\ndef upscale2d(x, factor=2):\n    s = x.shape\n    x = torch.reshape(x, [-1, s[1], s[2], 1, s[3], 1])\n    x = x.repeat(1, 1, 1, factor, 1, factor)\n    x = torch.reshape(x, [-1, s[1], s[2] * factor, s[3] * factor])\n    return x\n\n\ndef downscale2d(x, factor=2):\n    return F.avg_pool2d(x, factor, factor)\n\n\nclass Blur(nn.Module):\n    def __init__(self, channels):\n        super(Blur, self).__init__()\n        f = np.array([1, 2, 1], dtype=np.float32)\n        f = f[:, np.newaxis] * f[np.newaxis, :]\n        f /= np.sum(f)\n        kernel = torch.Tensor(f).view(1, 1, 3, 3).repeat(channels, 1, 1, 1)\n        self.register_buffer(\'weight\', kernel)\n        self.groups = channels\n\n    def forward(self, x):\n        return F.conv2d(x, weight=self.weight, groups=self.groups, padding=1)\n\n\nclass EncodeBlock(nn.Module):\n    def __init__(self, inputs, outputs, latent_size, last=False, fused_scale=True):\n        super(EncodeBlock, self).__init__()\n        self.conv_1 = ln.Conv2d(inputs, inputs, 3, 1, 1, bias=False)\n        # self.conv_1 = ln.Conv2d(inputs + (1 if last else 0), inputs, 3, 1, 1, bias=False)\n        self.bias_1 = nn.Parameter(torch.Tensor(1, inputs, 1, 1))\n        self.instance_norm_1 = nn.InstanceNorm2d(inputs, affine=False)\n        self.blur = Blur(inputs)\n        self.last = last\n        self.fused_scale = fused_scale\n        if last:\n            self.dense = ln.Linear(inputs * 4 * 4, outputs)\n        else:\n            if fused_scale:\n                self.conv_2 = ln.Conv2d(inputs, outputs, 3, 2, 1, bias=False, transform_kernel=True)\n            else:\n                self.conv_2 = ln.Conv2d(inputs, outputs, 3, 1, 1, bias=False)\n\n        self.bias_2 = nn.Parameter(torch.Tensor(1, outputs, 1, 1))\n        self.instance_norm_2 = nn.InstanceNorm2d(outputs, affine=False)\n        self.style_1 = ln.Linear(2 * inputs, latent_size)\n        if last:\n            self.style_2 = ln.Linear(outputs, latent_size)\n        else:\n            self.style_2 = ln.Linear(2 * outputs, latent_size)\n\n        with torch.no_grad():\n            self.bias_1.zero_()\n            self.bias_2.zero_()\n\n    def forward(self, x):\n        x = self.conv_1(x) + self.bias_1\n        x = F.leaky_relu(x, 0.2)\n\n        m = torch.mean(x, dim=[2, 3], keepdim=True)\n        std = torch.sqrt(torch.mean((x - m) ** 2, dim=[2, 3], keepdim=True))\n        style_1 = torch.cat((m, std), dim=1)\n\n        x = self.instance_norm_1(x)\n\n        if self.last:\n            x = self.dense(x.view(x.shape[0], -1))\n\n            x = F.leaky_relu(x, 0.2)\n            w1 = self.style_1(style_1.view(style_1.shape[0], style_1.shape[1]))\n            w2 = self.style_2(x.view(x.shape[0], x.shape[1]))\n        else:\n            x = self.conv_2(self.blur(x))\n            if not self.fused_scale:\n                x = downscale2d(x)\n            x = x + self.bias_2\n\n            x = F.leaky_relu(x, 0.2)\n\n            m = torch.mean(x, dim=[2, 3], keepdim=True)\n            std = torch.sqrt(torch.mean((x - m) ** 2, dim=[2, 3], keepdim=True))\n            style_2 = torch.cat((m, std), dim=1)\n\n            x = self.instance_norm_2(x)\n\n            w1 = self.style_1(style_1.view(style_1.shape[0], style_1.shape[1]))\n            w2 = self.style_2(style_2.view(style_2.shape[0], style_2.shape[1]))\n\n        return x, w1, w2\n\n\nclass DiscriminatorBlock(nn.Module):\n    def __init__(self, inputs, outputs, last=False, fused_scale=True, dense=False):\n        super(DiscriminatorBlock, self).__init__()\n        self.conv_1 = ln.Conv2d(inputs + (1 if last else 0), inputs, 3, 1, 1, bias=False)\n        self.bias_1 = nn.Parameter(torch.Tensor(1, inputs, 1, 1))\n        self.blur = Blur(inputs)\n        self.last = last\n        self.dense_ = dense\n        self.fused_scale = fused_scale\n        if self.dense_:\n            self.dense = ln.Linear(inputs * 4 * 4, outputs)\n        else:\n            if fused_scale:\n                self.conv_2 = ln.Conv2d(inputs, outputs, 3, 2, 1, bias=False, transform_kernel=True)\n            else:\n                self.conv_2 = ln.Conv2d(inputs, outputs, 3, 1, 1, bias=False)\n\n        self.bias_2 = nn.Parameter(torch.Tensor(1, outputs, 1, 1))\n\n        with torch.no_grad():\n            self.bias_1.zero_()\n            self.bias_2.zero_()\n\n    def forward(self, x):\n        if self.last:\n            x = minibatch_stddev_layer(x)\n\n        x = self.conv_1(x) + self.bias_1\n        x = F.leaky_relu(x, 0.2)\n\n        if self.dense_:\n            x = self.dense(x.view(x.shape[0], -1))\n        else:\n            x = self.conv_2(self.blur(x))\n            if not self.fused_scale:\n                x = downscale2d(x)\n            x = x + self.bias_2\n        x = F.leaky_relu(x, 0.2)\n\n        return x\n\n\nclass DecodeBlock(nn.Module):\n    def __init__(self, inputs, outputs, latent_size, has_first_conv=True, fused_scale=True, layer=0):\n        super(DecodeBlock, self).__init__()\n        self.has_first_conv = has_first_conv\n        self.inputs = inputs\n        self.has_first_conv = has_first_conv\n        self.fused_scale = fused_scale\n        if has_first_conv:\n            if fused_scale:\n                self.conv_1 = ln.ConvTranspose2d(inputs, outputs, 3, 2, 1, bias=False, transform_kernel=True)\n            else:\n                self.conv_1 = ln.Conv2d(inputs, outputs, 3, 1, 1, bias=False)\n\n        self.blur = Blur(outputs)\n        self.noise_weight_1 = nn.Parameter(torch.Tensor(1, outputs, 1, 1))\n        self.noise_weight_1.data.zero_()\n        self.bias_1 = nn.Parameter(torch.Tensor(1, outputs, 1, 1))\n        self.instance_norm_1 = nn.InstanceNorm2d(outputs, affine=False, eps=1e-8)\n        self.style_1 = ln.Linear(latent_size, 2 * outputs, gain=1)\n\n        self.conv_2 = ln.Conv2d(outputs, outputs, 3, 1, 1, bias=False)\n        self.noise_weight_2 = nn.Parameter(torch.Tensor(1, outputs, 1, 1))\n        self.noise_weight_2.data.zero_()\n        self.bias_2 = nn.Parameter(torch.Tensor(1, outputs, 1, 1))\n        self.instance_norm_2 = nn.InstanceNorm2d(outputs, affine=False, eps=1e-8)\n        self.style_2 = ln.Linear(latent_size, 2 * outputs, gain=1)\n\n        self.layer = layer\n\n        with torch.no_grad():\n            self.bias_1.zero_()\n            self.bias_2.zero_()\n\n    def forward(self, x, s1, s2, noise):\n        if self.has_first_conv:\n            if not self.fused_scale:\n                x = upscale2d(x)\n            x = self.conv_1(x)\n            x = self.blur(x)\n\n        if noise:\n            if noise == \'batch_constant\':\n                x = torch.addcmul(x, value=1.0, tensor1=self.noise_weight_1,\n                                  tensor2=torch.randn([1, 1, x.shape[2], x.shape[3]]))\n            else:\n                x = torch.addcmul(x, value=1.0, tensor1=self.noise_weight_1,\n                                  tensor2=torch.randn([x.shape[0], 1, x.shape[2], x.shape[3]]))\n        else:\n            s = math.pow(self.layer + 1, 0.5)\n            x = x + s * torch.exp(-x * x / (2.0 * s * s)) / math.sqrt(2 * math.pi) * 0.8\n        x = x + self.bias_1\n\n        x = F.leaky_relu(x, 0.2)\n\n        x = self.instance_norm_1(x)\n\n        x = style_mod(x, self.style_1(s1))\n\n        x = self.conv_2(x)\n\n        if noise:\n            if noise == \'batch_constant\':\n                x = torch.addcmul(x, value=1.0, tensor1=self.noise_weight_2,\n                                  tensor2=torch.randn([1, 1, x.shape[2], x.shape[3]]))\n            else:\n                x = torch.addcmul(x, value=1.0, tensor1=self.noise_weight_2,\n                                  tensor2=torch.randn([x.shape[0], 1, x.shape[2], x.shape[3]]))\n        else:\n            s = math.pow(self.layer + 1, 0.5)\n            x = x +  s * torch.exp(-x * x / (2.0 * s * s)) / math.sqrt(2 * math.pi) * 0.8\n\n        x = x + self.bias_2\n\n        x = F.leaky_relu(x, 0.2)\n        x = self.instance_norm_2(x)\n\n        x = style_mod(x, self.style_2(s2))\n\n        return x\n\n\nclass FromRGB(nn.Module):\n    def __init__(self, channels, outputs):\n        super(FromRGB, self).__init__()\n        self.from_rgb = ln.Conv2d(channels, outputs, 1, 1, 0)\n\n    def forward(self, x):\n        x = self.from_rgb(x)\n        x = F.leaky_relu(x, 0.2)\n\n        return x\n\n\nclass ToRGB(nn.Module):\n    def __init__(self, inputs, channels):\n        super(ToRGB, self).__init__()\n        self.inputs = inputs\n        self.channels = channels\n        self.to_rgb = ln.Conv2d(inputs, channels, 1, 1, 0, gain=0.03)\n\n    def forward(self, x):\n        x = self.to_rgb(x)\n        return x\n\n\n@ENCODERS.register(""EncoderDefault"")\nclass Encoder_old(nn.Module):\n    def __init__(self, startf, maxf, layer_count, latent_size, channels=3):\n        super(Encoder_old, self).__init__()\n        self.maxf = maxf\n        self.startf = startf\n        self.layer_count = layer_count\n        self.from_rgb: nn.ModuleList[FromRGB] = nn.ModuleList()\n        self.channels = channels\n        self.latent_size = latent_size\n\n        mul = 2\n        inputs = startf\n        self.encode_block: nn.ModuleList[EncodeBlock] = nn.ModuleList()\n\n        resolution = 2 ** (self.layer_count + 1)\n\n        for i in range(self.layer_count):\n            outputs = min(self.maxf, startf * mul)\n\n            self.from_rgb.append(FromRGB(channels, inputs))\n\n            fused_scale = resolution >= 128\n\n            block = EncodeBlock(inputs, outputs, latent_size, False, fused_scale=fused_scale)\n\n            resolution //= 2\n\n            #print(""encode_block%d %s styles out: %d"" % ((i + 1), millify(count_parameters(block)), inputs))\n            self.encode_block.append(block)\n            inputs = outputs\n            mul *= 2\n\n    def encode(self, x, lod):\n        styles = torch.zeros(x.shape[0], 1, self.latent_size)\n\n        x = self.from_rgb[self.layer_count - lod - 1](x)\n        x = F.leaky_relu(x, 0.2)\n\n        for i in range(self.layer_count - lod - 1, self.layer_count):\n            x, s1, s2 = self.encode_block[i](x)\n            styles[:, 0] += s1 + s2\n\n        return styles\n\n    def encode2(self, x, lod, blend):\n        x_orig = x\n        styles = torch.zeros(x.shape[0], 1, self.latent_size)\n\n        x = self.from_rgb[self.layer_count - lod - 1](x)\n        x = F.leaky_relu(x, 0.2)\n\n        x, s1, s2 = self.encode_block[self.layer_count - lod - 1](x)\n        styles[:, 0] += s1 * blend + s2 * blend\n\n        x_prev = F.avg_pool2d(x_orig, 2, 2)\n\n        x_prev = self.from_rgb[self.layer_count - (lod - 1) - 1](x_prev)\n        x_prev = F.leaky_relu(x_prev, 0.2)\n\n        x = torch.lerp(x_prev, x, blend)\n\n        for i in range(self.layer_count - (lod - 1) - 1, self.layer_count):\n            x, s1, s2 = self.encode_block[i](x)\n            styles[:, 0] += s1 + s2\n\n        return styles\n\n    def forward(self, x, lod, blend):\n        if blend == 1:\n            return self.encode(x, lod)\n        else:\n            return self.encode2(x, lod, blend)\n\n    def get_statistics(self, lod):\n        rgb_std = self.from_rgb[self.layer_count - lod - 1].from_rgb.weight.std().item()\n        rgb_std_c = self.from_rgb[self.layer_count - lod - 1].from_rgb.std\n\n        layers = []\n        for i in range(self.layer_count - lod - 1, self.layer_count):\n            conv_1 = self.encode_block[i].conv_1.weight.std().item()\n            conv_1_c = self.encode_block[i].conv_1.std\n            conv_2 = self.encode_block[i].conv_2.weight.std().item()\n            conv_2_c = self.encode_block[i].conv_2.std\n            layers.append(((conv_1 / conv_1_c), (conv_2 / conv_2_c)))\n        return rgb_std / rgb_std_c, layers\n\n\n@ENCODERS.register(""EncoderWithFC"")\nclass EncoderWithFC(nn.Module):\n    def __init__(self, startf, maxf, layer_count, latent_size, channels=3):\n        super(EncoderWithFC, self).__init__()\n        self.maxf = maxf\n        self.startf = startf\n        self.layer_count = layer_count\n        self.from_rgb: nn.ModuleList[FromRGB] = nn.ModuleList()\n        self.channels = channels\n        self.latent_size = latent_size\n\n        mul = 2\n        inputs = startf\n        self.encode_block: nn.ModuleList[EncodeBlock] = nn.ModuleList()\n\n        resolution = 2 ** (self.layer_count + 1)\n\n        for i in range(self.layer_count):\n            outputs = min(self.maxf, startf * mul)\n\n            self.from_rgb.append(FromRGB(channels, inputs))\n\n            fused_scale = resolution >= 128\n\n            block = EncodeBlock(inputs, outputs, latent_size, i == self.layer_count - 1, fused_scale=fused_scale)\n\n            resolution //= 2\n\n            #print(""encode_block%d %s styles out: %d"" % ((i + 1), millify(count_parameters(block)), inputs))\n            self.encode_block.append(block)\n            inputs = outputs\n            mul *= 2\n\n        self.fc2 = ln.Linear(inputs, 1, gain=1)\n\n    def encode(self, x, lod):\n        styles = torch.zeros(x.shape[0], 1, self.latent_size)\n\n        x = self.from_rgb[self.layer_count - lod - 1](x)\n        x = F.leaky_relu(x, 0.2)\n\n        for i in range(self.layer_count - lod - 1, self.layer_count):\n            x, s1, s2 = self.encode_block[i](x)\n            styles[:, 0] += s1 + s2\n\n        return styles, self.fc2(x)\n\n    def encode2(self, x, lod, blend):\n        x_orig = x\n        styles = torch.zeros(x.shape[0], 1, self.latent_size)\n\n        x = self.from_rgb[self.layer_count - lod - 1](x)\n        x = F.leaky_relu(x, 0.2)\n\n        x, s1, s2 = self.encode_block[self.layer_count - lod - 1](x)\n        styles[:, 0] += s1 * blend + s2 * blend\n\n        x_prev = F.avg_pool2d(x_orig, 2, 2)\n\n        x_prev = self.from_rgb[self.layer_count - (lod - 1) - 1](x_prev)\n        x_prev = F.leaky_relu(x_prev, 0.2)\n\n        x = torch.lerp(x_prev, x, blend)\n\n        for i in range(self.layer_count - (lod - 1) - 1, self.layer_count):\n            x, s1, s2 = self.encode_block[i](x)\n            styles[:, 0] += s1 + s2\n\n        return styles, self.fc2(x)\n\n    def forward(self, x, lod, blend):\n        if blend == 1:\n            return self.encode(x, lod)\n        else:\n            return self.encode2(x, lod, blend)\n\n    def get_statistics(self, lod):\n        rgb_std = self.from_rgb[self.layer_count - lod - 1].from_rgb.weight.std().item()\n        rgb_std_c = self.from_rgb[self.layer_count - lod - 1].from_rgb.std\n\n        layers = []\n        for i in range(self.layer_count - lod - 1, self.layer_count):\n            conv_1 = self.encode_block[i].conv_1.weight.std().item()\n            conv_1_c = self.encode_block[i].conv_1.std\n            conv_2 = self.encode_block[i].conv_2.weight.std().item()\n            conv_2_c = self.encode_block[i].conv_2.std\n            layers.append(((conv_1 / conv_1_c), (conv_2 / conv_2_c)))\n        return rgb_std / rgb_std_c, layers\n\n\n@ENCODERS.register(""EncoderWithStatistics"")\nclass Encoder(nn.Module):\n    def __init__(self, startf, maxf, layer_count, latent_size, channels=3):\n        super(Encoder, self).__init__()\n        self.maxf = maxf\n        self.startf = startf\n        self.layer_count = layer_count\n        self.from_rgb: nn.ModuleList[FromRGB] = nn.ModuleList()\n        self.channels = channels\n        self.latent_size = latent_size\n\n        mul = 2\n        inputs = startf\n        self.encode_block: nn.ModuleList[EncodeBlock] = nn.ModuleList()\n\n        resolution = 2 ** (self.layer_count + 1)\n\n        for i in range(self.layer_count):\n            outputs = min(self.maxf, startf * mul)\n\n            self.from_rgb.append(FromRGB(channels, inputs))\n\n            fused_scale = resolution >= 128\n\n            block = EncodeBlock(inputs, outputs, latent_size, i == self.layer_count - 1, fused_scale=fused_scale)\n\n            resolution //= 2\n\n            #print(""encode_block%d %s styles out: %d"" % ((i + 1), millify(count_parameters(block)), inputs))\n            self.encode_block.append(block)\n            inputs = outputs\n            mul *= 2\n\n    def encode(self, x, lod):\n        styles = torch.zeros(x.shape[0], 1, self.latent_size)\n\n        x = self.from_rgb[self.layer_count - lod - 1](x)\n        x = F.leaky_relu(x, 0.2)\n\n        for i in range(self.layer_count - lod - 1, self.layer_count):\n            x, s1, s2 = self.encode_block[i](x)\n            styles[:, 0] += s1 + s2\n\n        return styles\n\n    def encode2(self, x, lod, blend):\n        x_orig = x\n        styles = torch.zeros(x.shape[0], 1, self.latent_size)\n\n        x = self.from_rgb[self.layer_count - lod - 1](x)\n        x = F.leaky_relu(x, 0.2)\n\n        x, s1, s2 = self.encode_block[self.layer_count - lod - 1](x)\n        styles[:, 0] += s1 * blend + s2 * blend\n\n        x_prev = F.avg_pool2d(x_orig, 2, 2)\n\n        x_prev = self.from_rgb[self.layer_count - (lod - 1) - 1](x_prev)\n        x_prev = F.leaky_relu(x_prev, 0.2)\n\n        x = torch.lerp(x_prev, x, blend)\n\n        for i in range(self.layer_count - (lod - 1) - 1, self.layer_count):\n            x, s1, s2 = self.encode_block[i](x)\n            styles[:, 0] += s1 + s2\n\n        return styles\n\n    def forward(self, x, lod, blend):\n        if blend == 1:\n            return self.encode(x, lod)\n        else:\n            return self.encode2(x, lod, blend)\n\n    def get_statistics(self, lod):\n        rgb_std = self.from_rgb[self.layer_count - lod - 1].from_rgb.weight.std().item()\n        rgb_std_c = self.from_rgb[self.layer_count - lod - 1].from_rgb.std\n\n        layers = []\n        for i in range(self.layer_count - lod - 1, self.layer_count):\n            conv_1 = self.encode_block[i].conv_1.weight.std().item()\n            conv_1_c = self.encode_block[i].conv_1.std\n            conv_2 = self.encode_block[i].conv_2.weight.std().item()\n            conv_2_c = self.encode_block[i].conv_2.std\n            layers.append(((conv_1 / conv_1_c), (conv_2 / conv_2_c)))\n        return rgb_std / rgb_std_c, layers\n\n\n@ENCODERS.register(""EncoderNoStyle"")\nclass EncoderNoStyle(nn.Module):\n    def __init__(self, startf=32, maxf=256, layer_count=3, latent_size=512, channels=3):\n        super(EncoderNoStyle, self).__init__()\n        self.maxf = maxf\n        self.startf = startf\n        self.layer_count = layer_count\n        self.from_rgb = nn.ModuleList()\n        self.channels = channels\n\n        mul = 2\n        inputs = startf\n        self.encode_block: nn.ModuleList[DiscriminatorBlock] = nn.ModuleList()\n\n        resolution = 2 ** (self.layer_count + 1)\n\n        for i in range(self.layer_count):\n            outputs = min(self.maxf, startf * mul)\n\n            self.from_rgb.append(FromRGB(channels, inputs))\n\n            fused_scale = resolution >= 128\n\n            block = DiscriminatorBlock(inputs, outputs, last=False, fused_scale=fused_scale, dense=i == self.layer_count - 1)\n\n            resolution //= 2\n\n            #print(""encode_block%d %s"" % ((i + 1), millify(count_parameters(block))))\n            self.encode_block.append(block)\n            inputs = outputs\n            mul *= 2\n\n        self.fc2 = ln.Linear(inputs, latent_size, gain=1)\n\n    def encode(self, x, lod):\n        x = self.from_rgb[self.layer_count - lod - 1](x)\n        x = F.leaky_relu(x, 0.2)\n\n        for i in range(self.layer_count - lod - 1, self.layer_count):\n            x = self.encode_block[i](x)\n\n        return self.fc2(x).view(x.shape[0], 1, x.shape[1])\n\n    def encode2(self, x, lod, blend):\n        x_orig = x\n        x = self.from_rgb[self.layer_count - lod - 1](x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.encode_block[self.layer_count - lod - 1](x)\n\n        x_prev = F.avg_pool2d(x_orig, 2, 2)\n\n        x_prev = self.from_rgb[self.layer_count - (lod - 1) - 1](x_prev)\n        x_prev = F.leaky_relu(x_prev, 0.2)\n\n        x = torch.lerp(x_prev, x, blend)\n\n        for i in range(self.layer_count - (lod - 1) - 1, self.layer_count):\n            x = self.encode_block[i](x)\n\n        return self.fc2(x).view(x.shape[0], 1, x.shape[1])\n\n    def forward(self, x, lod, blend):\n        if blend == 1:\n            return self.encode(x, lod)\n        else:\n            return self.encode2(x, lod, blend)\n\n\n@DISCRIMINATORS.register(""DiscriminatorDefault"")\nclass Discriminator(nn.Module):\n    def __init__(self, startf=32, maxf=256, layer_count=3, channels=3):\n        super(Discriminator, self).__init__()\n        self.maxf = maxf\n        self.startf = startf\n        self.layer_count = layer_count\n        self.from_rgb = nn.ModuleList()\n        self.channels = channels\n\n        mul = 2\n        inputs = startf\n        self.encode_block: nn.ModuleList[DiscriminatorBlock] = nn.ModuleList()\n\n        resolution = 2 ** (self.layer_count + 1)\n\n        for i in range(self.layer_count):\n            outputs = min(self.maxf, startf * mul)\n\n            self.from_rgb.append(FromRGB(channels, inputs))\n\n            fused_scale = resolution >= 128\n\n            block = DiscriminatorBlock(inputs, outputs, i == self.layer_count - 1, fused_scale=fused_scale)\n\n            resolution //= 2\n\n            #print(""encode_block%d %s"" % ((i + 1), millify(count_parameters(block))))\n            self.encode_block.append(block)\n            inputs = outputs\n            mul *= 2\n\n        self.fc2 = ln.Linear(inputs, 1, gain=1)\n\n    def encode(self, x, lod):\n        x = self.from_rgb[self.layer_count - lod - 1](x)\n        x = F.leaky_relu(x, 0.2)\n\n        for i in range(self.layer_count - lod - 1, self.layer_count):\n            x = self.encode_block[i](x)\n\n        return self.fc2(x)\n\n    def encode2(self, x, lod, blend):\n        x_orig = x\n        x = self.from_rgb[self.layer_count - lod - 1](x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.encode_block[self.layer_count - lod - 1](x)\n\n        x_prev = F.avg_pool2d(x_orig, 2, 2)\n\n        x_prev = self.from_rgb[self.layer_count - (lod - 1) - 1](x_prev)\n        x_prev = F.leaky_relu(x_prev, 0.2)\n\n        x = torch.lerp(x_prev, x, blend)\n\n        for i in range(self.layer_count - (lod - 1) - 1, self.layer_count):\n            x = self.encode_block[i](x)\n\n        return self.fc2(x)\n\n    def forward(self, x, lod, blend):\n        if blend == 1:\n            return self.encode(x, lod)\n        else:\n            return self.encode2(x, lod, blend)\n\n\n@GENERATORS.register(""GeneratorDefault"")\nclass Generator(nn.Module):\n    def __init__(self, startf=32, maxf=256, layer_count=3, latent_size=128, channels=3):\n        super(Generator, self).__init__()\n        self.maxf = maxf\n        self.startf = startf\n        self.layer_count = layer_count\n\n        self.channels = channels\n        self.latent_size = latent_size\n\n        mul = 2 ** (self.layer_count - 1)\n\n        inputs = min(self.maxf, startf * mul)\n        self.const = Parameter(torch.Tensor(1, inputs, 4, 4))\n        init.ones_(self.const)\n\n        self.layer_to_resolution = [0 for _ in range(layer_count)]\n        resolution = 2\n\n        self.style_sizes = []\n\n        to_rgb = nn.ModuleList()\n\n        self.decode_block: nn.ModuleList[DecodeBlock] = nn.ModuleList()\n        for i in range(self.layer_count):\n            outputs = min(self.maxf, startf * mul)\n\n            has_first_conv = i != 0\n            fused_scale = resolution * 2 >= 128\n\n            block = DecodeBlock(inputs, outputs, latent_size, has_first_conv, fused_scale=fused_scale, layer=i)\n\n            resolution *= 2\n            self.layer_to_resolution[i] = resolution\n\n            self.style_sizes += [2 * (inputs if has_first_conv else outputs), 2 * outputs]\n\n            to_rgb.append(ToRGB(outputs, channels))\n\n            #print(""decode_block%d %s styles in: %dl out resolution: %d"" % (\n            #    (i + 1), millify(count_parameters(block)), outputs, resolution))\n            self.decode_block.append(block)\n            inputs = outputs\n            mul //= 2\n\n        self.to_rgb = to_rgb\n\n    def decode(self, styles, lod, noise):\n        x = self.const\n\n        for i in range(lod + 1):\n            x = self.decode_block[i](x, styles[:, 2 * i + 0], styles[:, 2 * i + 1], noise)\n\n        x = self.to_rgb[lod](x)\n        return x\n\n    def decode2(self, styles, lod, blend, noise):\n        x = self.const\n\n        for i in range(lod):\n            x = self.decode_block[i](x, styles[:, 2 * i + 0], styles[:, 2 * i + 1], noise)\n\n        x_prev = self.to_rgb[lod - 1](x)\n\n        x = self.decode_block[lod](x, styles[:, 2 * lod + 0], styles[:, 2 * lod + 1], noise)\n        x = self.to_rgb[lod](x)\n\n        needed_resolution = self.layer_to_resolution[lod]\n\n        x_prev = F.interpolate(x_prev, size=needed_resolution)\n        x = torch.lerp(x_prev, x, blend)\n\n        return x\n\n    def forward(self, styles, lod, blend, noise):\n        if blend == 1:\n            return self.decode(styles, lod, noise)\n        else:\n            return self.decode2(styles, lod, blend, noise)\n\n    def get_statistics(self, lod):\n        rgb_std = self.to_rgb[lod].to_rgb.weight.std().item()\n        rgb_std_c = self.to_rgb[lod].to_rgb.std\n\n        layers = []\n        for i in range(lod + 1):\n            conv_1 = 1.0\n            conv_1_c = 1.0\n            if i != 0:\n                conv_1 = self.decode_block[i].conv_1.weight.std().item()\n                conv_1_c = self.decode_block[i].conv_1.std\n            conv_2 = self.decode_block[i].conv_2.weight.std().item()\n            conv_2_c = self.decode_block[i].conv_2.std\n            layers.append(((conv_1 / conv_1_c), (conv_2 / conv_2_c)))\n        return rgb_std / rgb_std_c, layers\n\n\nimage_size = 64\n\n# Number of channels in the training images. For color images this is 3\nnc = 3\n\n# Size of z latent vector (i.e. size of generator input)\nnz = 24\n\n# Size of feature maps in generator\nngf = 64\n\n# Size of feature maps in discriminator\nndf = 64\n\n\n@GENERATORS.register(""DCGANGenerator"")\nclass DCGANGenerator(nn.Module):\n    def __init__(self):\n        super(DCGANGenerator, self).__init__()\n        self.main = nn.Sequential(\n            # input is Z, going into a convolution\n            nn.ConvTranspose2d( nz, 512, 4, 1, 0),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            # state size. (ngf*8) x 4 x 4\n            nn.ConvTranspose2d(512, 256, 4, 2, 1),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(),\n            # state size. (ngf*4) x 8 x 8\n            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            # state size. (ngf*2) x 16 x 16\n            nn.ConvTranspose2d(128, nc, 4, 2, 1),\n            #nn.BatchNorm2d(ngf),\n            #nn.ReLU(True),\n            # state size. (ngf) x 32 x 32\n            #nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=True),\n            nn.Tanh()\n            # state size. (nc) x 64 x 64\n        )\n\n    def forward(self, x):\n        return self.main(x.view(x.shape[0], nz, 1, 1))\n\n\n@ENCODERS.register(""DCGANEncoder"")\nclass DCGANEncoder(nn.Module):\n    def __init__(self):\n        super(DCGANEncoder, self).__init__()\n        self.main = nn.Sequential(\n            # input is (nc) x 64 x 64\n            #nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            #nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf) x 32 x 32\n            nn.Conv2d(nc, 64, 4, 2, 1),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(0.2),\n            # state size. (ndf*2) x 16 x 16\n            nn.Conv2d(64, 128, 4, 2, 1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2),\n            # state size. (ndf*4) x 8 x 8\n            nn.Conv2d(128, 256, 4, 2, 1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2),\n            # state size. (ndf*8) x 4 x 4\n            nn.Conv2d(256, 24, 4, 1, 0),\n            nn.LeakyReLU(0.01),\n        )\n\n    def forward(self, x):\n        x = self.main(x)\n        return x.view(x.shape[0], x.shape[1])\n\n\nclass MappingBlock(nn.Module):\n    def __init__(self, inputs, output, lrmul):\n        super(MappingBlock, self).__init__()\n        self.fc = ln.Linear(inputs, output, lrmul=lrmul)\n\n    def forward(self, x):\n        x = F.leaky_relu(self.fc(x), 0.2)\n        return x\n\n\n@MAPPINGS.register(""MappingDefault"")\nclass Mapping(nn.Module):\n    def __init__(self, num_layers, mapping_layers=5, latent_size=256, dlatent_size=256, mapping_fmaps=256):\n        super(Mapping, self).__init__()\n        inputs = latent_size\n        self.mapping_layers = mapping_layers\n        self.num_layers = num_layers\n        for i in range(mapping_layers):\n            outputs = dlatent_size if i == mapping_layers - 1 else mapping_fmaps\n            block = MappingBlock(inputs, outputs, lrmul=0.01)\n            inputs = outputs\n            setattr(self, ""block_%d"" % (i + 1), block)\n            #print(""dense %d %s"" % ((i + 1), millify(count_parameters(block))))\n\n    def forward(self, z):\n        x = pixel_norm(z)\n\n        for i in range(self.mapping_layers):\n            x = getattr(self, ""block_%d"" % (i + 1))(x)\n\n        return x.view(x.shape[0], 1, x.shape[1]).repeat(1, self.num_layers, 1)\n\n\n@MAPPINGS.register(""MappingToLatent"")\nclass VAEMappingToLatent_old(nn.Module):\n    def __init__(self, mapping_layers=5, latent_size=256, dlatent_size=256, mapping_fmaps=256):\n        super(VAEMappingToLatent_old, self).__init__()\n        inputs = latent_size\n        self.mapping_layers = mapping_layers\n        self.map_blocks: nn.ModuleList[MappingBlock] = nn.ModuleList()\n        for i in range(mapping_layers):\n            outputs = 2 * dlatent_size if i == mapping_layers - 1 else mapping_fmaps\n            block = ln.Linear(inputs, outputs, lrmul=0.1)\n            inputs = outputs\n            self.map_blocks.append(block)\n            #print(""dense %d %s"" % ((i + 1), millify(count_parameters(block))))\n\n    def forward(self, x):\n        for i in range(self.mapping_layers):\n            x = self.map_blocks[i](x)\n\n        return x.view(x.shape[0], 2, x.shape[2] // 2)\n\n\n@MAPPINGS.register(""MappingToLatentNoStyle"")\nclass VAEMappingToLatentNoStyle(nn.Module):\n    def __init__(self, mapping_layers=5, latent_size=256, dlatent_size=256, mapping_fmaps=256):\n        super(VAEMappingToLatentNoStyle, self).__init__()\n        inputs = latent_size\n        self.mapping_layers = mapping_layers\n        self.map_blocks: nn.ModuleList[MappingBlock] = nn.ModuleList()\n        for i in range(mapping_layers):\n            outputs = dlatent_size if i == mapping_layers - 1 else mapping_fmaps\n            block = ln.Linear(inputs, outputs, lrmul=0.1)\n            inputs = outputs\n            self.map_blocks.append(block)\n\n    def forward(self, x):\n        for i in range(self.mapping_layers):\n            if i == self.mapping_layers - 1:\n                #x = self.map_blocks[i](x)\n                x = self.map_blocks[i](x)\n            else:\n                #x = self.map_blocks[i](x)\n                x = self.map_blocks[i](x)\n        return x\n\n\n@MAPPINGS.register(""MappingFromLatent"")\nclass VAEMappingFromLatent(nn.Module):\n    def __init__(self, num_layers, mapping_layers=5, latent_size=256, dlatent_size=256, mapping_fmaps=256):\n        super(VAEMappingFromLatent, self).__init__()\n        inputs = dlatent_size\n        self.mapping_layers = mapping_layers\n        self.num_layers = num_layers\n        self.map_blocks: nn.ModuleList[MappingBlock] = nn.ModuleList()\n        for i in range(mapping_layers):\n            outputs = latent_size if i == mapping_layers - 1 else mapping_fmaps\n            block = MappingBlock(inputs, outputs, lrmul=0.1)\n            inputs = outputs\n            self.map_blocks.append(block)\n            #print(""dense %d %s"" % ((i + 1), millify(count_parameters(block))))\n\n    def forward(self, x):\n        x = pixel_norm(x)\n\n        for i in range(self.mapping_layers):\n            x = self.map_blocks[i](x)\n\n        return x.view(x.shape[0], 1, x.shape[1]).repeat(1, self.num_layers, 1)\n\n\n@ENCODERS.register(""EncoderFC"")\nclass EncoderFC(nn.Module):\n    def __init__(self, startf, maxf, layer_count, latent_size, channels=3):\n        super(EncoderFC, self).__init__()\n        self.maxf = maxf\n        self.startf = startf\n        self.layer_count = layer_count\n        self.channels = channels\n        self.latent_size = latent_size\n\n        self.fc_1 = ln.Linear(28 * 28, 1024)\n        self.fc_2 = ln.Linear(1024, 1024)\n        self.fc_3 = ln.Linear(1024, latent_size)\n\n    def encode(self, x, lod):\n        x = F.interpolate(x, 28)\n        x = x.view(x.shape[0], 28 * 28)\n\n        x = self.fc_1(x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.fc_2(x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.fc_3(x)\n        x = F.leaky_relu(x, 0.2)\n\n        return x\n\n    def forward(self, x, lod, blend):\n        return self.encode(x, lod)\n\n\n@GENERATORS.register(""GeneratorFC"")\nclass GeneratorFC(nn.Module):\n    def __init__(self, startf=32, maxf=256, layer_count=3, latent_size=128, channels=3):\n        super(GeneratorFC, self).__init__()\n        self.maxf = maxf\n        self.startf = startf\n        self.layer_count = layer_count\n        self.channels = channels\n        self.latent_size = latent_size\n\n        self.fc_1 = ln.Linear(latent_size, 1024)\n        self.fc_2 = ln.Linear(1024, 1024)\n        self.fc_3 = ln.Linear(1024, 28 * 28)\n\n        self.layer_to_resolution = [28] * 10\n\n    def decode(self, x, lod, blend_factor, noise):\n        if len(x.shape) == 3:\n            x = x[:, 0]  # no styles\n        x.view(x.shape[0], self.latent_size)\n\n        x = self.fc_1(x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.fc_2(x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.fc_3(x)\n\n        x = x.view(x.shape[0], 1, 28, 28)\n        x = F.interpolate(x, 2 ** (2 + lod))\n        return x\n\n    def forward(self, x, lod, blend_factor, noise):\n        return self.decode(x, lod, blend_factor, noise)\n'"
registry.py,0,b'from utils import Registry\n\nMODELS = Registry()\nENCODERS = Registry()\nGENERATORS = Registry()\nMAPPINGS = Registry()\nDISCRIMINATORS = Registry()\n'
scheduler.py,2,"b'from bisect import bisect_right\nimport torch\nimport numpy as np\n\n\nclass WarmupMultiStepLR(torch.optim.lr_scheduler._LRScheduler):\n    def __init__(\n        self,\n        optimizer,\n        milestones,\n        gamma=0.1,\n        warmup_factor=1.0 / 1.0,\n        warmup_iters=1,\n        last_epoch=-1,\n        reference_batch_size=128,\n        lr=[]\n    ):\n        if not list(milestones) == sorted(milestones):\n            raise ValueError(\n                ""Milestones should be a list of"" "" increasing integers. Got {}"",\n                milestones,\n            )\n        self.milestones = milestones\n        self.gamma = gamma\n        self.warmup_factor = warmup_factor\n        self.warmup_iters = warmup_iters\n        self.batch_size = 1\n        self.lod = 0\n        self.reference_batch_size = reference_batch_size\n\n        self.optimizer = optimizer\n        self.base_lrs = []\n        for _ in self.optimizer.param_groups:\n            self.base_lrs.append(lr)\n\n        self.last_epoch = last_epoch\n\n        if not isinstance(optimizer, torch.optim.Optimizer):\n            raise TypeError(\'{} is not an Optimizer\'.format(\n                type(optimizer).__name__))\n        self.optimizer = optimizer\n\n        if last_epoch == -1:\n            for group in optimizer.param_groups:\n                group.setdefault(\'initial_lr\', group[\'lr\'])\n            last_epoch = 0\n\n        self.last_epoch = last_epoch\n\n        self.optimizer._step_count = 0\n        self._step_count = 0\n        self.step(last_epoch)\n\n    def set_batch_size(self, batch_size, lod):\n        self.batch_size = batch_size\n        self.lod = lod\n        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n            param_group[\'lr\'] = lr\n\n    def get_lr(self):\n        warmup_factor = 1\n        if self.last_epoch < self.warmup_iters:\n            alpha = float(self.last_epoch) / self.warmup_iters\n            warmup_factor = self.warmup_factor * (1 - alpha) + alpha\n        return [\n            base_lr[self.lod]\n            * warmup_factor\n            * self.gamma ** bisect_right(self.milestones, self.last_epoch)\n            # * float(self.batch_size)\n            # / float(self.reference_batch_size)\n            for base_lr in self.base_lrs\n        ]\n\n    def state_dict(self):\n        return {\n            ""last_epoch"": self.last_epoch\n        }\n\n    def load_state_dict(self, state_dict):\n        self.__dict__.update(dict(last_epoch=state_dict[""last_epoch""]))\n\n\nclass ComboMultiStepLR:\n    def __init__(\n        self,\n        optimizers, base_lr,\n        **kwargs\n    ):\n        self.schedulers = dict()\n        for name, opt in optimizers.items():\n            self.schedulers[name] = WarmupMultiStepLR(opt, lr=base_lr, **kwargs)\n        self.last_epoch = 0\n\n    def set_batch_size(self, batch_size, lod):\n        for x in self.schedulers.values():\n            x.set_batch_size(batch_size, lod)\n\n    def step(self, epoch=None):\n        for x in self.schedulers.values():\n            x.step(epoch)\n        if epoch is None:\n            epoch = self.last_epoch + 1\n        self.last_epoch = epoch\n\n    def state_dict(self):\n        return {key: value.state_dict() for key, value in self.schedulers.items()}\n\n    def load_state_dict(self, state_dict):\n        for k, x in self.schedulers.items():\n            x.load_state_dict(state_dict[k])\n\n        last_epochs = [x.last_epoch for k, x in self.schedulers.items()]\n        assert np.all(np.asarray(last_epochs) == last_epochs[0])\n        self.last_epoch = last_epochs[0]\n\n    def start_epoch(self):\n        return self.last_epoch\n'"
tracker.py,3,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport csv\nfrom collections import OrderedDict\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport os\n\n\nclass RunningMean:\n    def __init__(self):\n        self.mean = 0.0\n        self.n = 0\n\n    def __iadd__(self, value):\n        self.mean = (float(value) + self.mean * self.n)/(self.n + 1)\n        self.n += 1\n        return self\n\n    def reset(self):\n        self.mean = 0.0\n        self.n = 0\n\n    def mean(self):\n        return self.mean\n\n\nclass RunningMeanTorch:\n    def __init__(self):\n        self.values = []\n\n    def __iadd__(self, value):\n        with torch.no_grad():\n            self.values.append(value.detach().cpu().unsqueeze(0))\n            return self\n\n    def reset(self):\n        self.values = []\n\n    def mean(self):\n        with torch.no_grad():\n            if len(self.values) == 0:\n                return 0.0\n            return float(torch.cat(self.values).mean().item())\n\n\nclass LossTracker:\n    def __init__(self, output_folder=\'.\'):\n        self.tracks = OrderedDict()\n        self.epochs = []\n        self.means_over_epochs = OrderedDict()\n        self.output_folder = output_folder\n\n    def update(self, d):\n        for k, v in d.items():\n            if k not in self.tracks:\n                self.add(k)\n            self.tracks[k] += v\n\n    def add(self, name, pytorch=True):\n        assert name not in self.tracks, ""Name is already used""\n        if pytorch:\n            track = RunningMeanTorch()\n        else:\n            track = RunningMean()\n        self.tracks[name] = track\n        self.means_over_epochs[name] = []\n        return track\n\n    def register_means(self, epoch):\n        self.epochs.append(epoch)\n\n        for key in self.means_over_epochs.keys():\n            if key in self.tracks:\n                value = self.tracks[key]\n                self.means_over_epochs[key].append(value.mean())\n                value.reset()\n            else:\n                self.means_over_epochs[key].append(None)\n\n        with open(os.path.join(self.output_folder, \'log.csv\'), mode=\'w\') as csv_file:\n            fieldnames = [\'epoch\'] + list(self.tracks.keys())\n            writer = csv.writer(csv_file, delimiter=\',\', quotechar=\'""\', quoting=csv.QUOTE_MINIMAL)\n            writer.writerow(fieldnames)\n            for i in range(len(self.epochs)):\n                writer.writerow([self.epochs[i]] + [self.means_over_epochs[x][i] for x in self.tracks.keys()])\n\n    def __str__(self):\n        result = """"\n        for key, value in self.tracks.items():\n            result += ""%s: %.7f, "" % (key, value.mean())\n        return result[:-2]\n\n    def plot(self):\n        plt.figure(figsize=(12, 8))\n        for key in self.tracks.keys():\n            plt.plot(self.epochs, self.means_over_epochs[key], label=key)\n\n        plt.xlabel(\'Epoch\')\n        plt.ylabel(\'Loss\')\n\n        plt.legend(loc=4)\n        plt.grid(True)\n        plt.tight_layout()\n\n        plt.savefig(os.path.join(self.output_folder, \'plot.png\'))\n        plt.close()\n\n    def state_dict(self):\n        return {\n            \'tracks\': self.tracks,\n            \'epochs\': self.epochs,\n            \'means_over_epochs\': self.means_over_epochs}\n\n    def load_state_dict(self, state_dict):\n        self.tracks = state_dict[\'tracks\']\n        self.epochs = state_dict[\'epochs\']\n        self.means_over_epochs = state_dict[\'means_over_epochs\']\n\n        counts = list(map(len, self.means_over_epochs.values()))\n\n        if len(counts) == 0:\n            counts = [0]\n        m = min(counts)\n\n        if m < len(self.epochs):\n            self.epochs = self.epochs[:m]\n\n        for key in self.means_over_epochs.keys():\n            if len(self.means_over_epochs[key]) > m:\n                self.means_over_epochs[key] = self.means_over_epochs[key][:m]\n'"
train_alae.py,12,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n# \n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n# \n#  http://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport torch.utils.data\nfrom torchvision.utils import save_image\nfrom net import *\nimport os\nimport utils\nfrom checkpointer import Checkpointer\nfrom scheduler import ComboMultiStepLR\nfrom custom_adam import LREQAdam\nfrom dataloader import *\nfrom tqdm import tqdm\nfrom dlutils.pytorch import count_parameters\nimport dlutils.pytorch.count_parameters as count_param_override\nfrom tracker import LossTracker\nfrom model import Model\nfrom launcher import run\nfrom defaults import get_cfg_defaults\nimport lod_driver\nfrom PIL import Image\n\n\ndef save_sample(lod2batch, tracker, sample, samplez, x, logger, model, cfg, encoder_optimizer, decoder_optimizer):\n    os.makedirs(\'results\', exist_ok=True)\n\n    logger.info(\'\\n[%d/%d] - ptime: %.2f, %s, blend: %.3f, lr: %.12f,  %.12f, max mem: %f"",\' % (\n        (lod2batch.current_epoch + 1), cfg.TRAIN.TRAIN_EPOCHS, lod2batch.per_epoch_ptime, str(tracker),\n        lod2batch.get_blend_factor(),\n        encoder_optimizer.param_groups[0][\'lr\'], decoder_optimizer.param_groups[0][\'lr\'],\n        torch.cuda.max_memory_allocated() / 1024.0 / 1024.0))\n\n    with torch.no_grad():\n        model.eval()\n        sample = sample[:lod2batch.get_per_GPU_batch_size()]\n        samplez = samplez[:lod2batch.get_per_GPU_batch_size()]\n\n        needed_resolution = model.decoder.layer_to_resolution[lod2batch.lod]\n        sample_in = sample\n        while sample_in.shape[2] > needed_resolution:\n            sample_in = F.avg_pool2d(sample_in, 2, 2)\n        assert sample_in.shape[2] == needed_resolution\n\n        blend_factor = lod2batch.get_blend_factor()\n        if lod2batch.in_transition:\n            needed_resolution_prev = model.decoder.layer_to_resolution[lod2batch.lod - 1]\n            sample_in_prev = F.avg_pool2d(sample_in, 2, 2)\n            sample_in_prev_2x = F.interpolate(sample_in_prev, needed_resolution)\n            sample_in = sample_in * blend_factor + sample_in_prev_2x * (1.0 - blend_factor)\n\n        Z, _ = model.encode(sample_in, lod2batch.lod, blend_factor)\n\n        if cfg.MODEL.Z_REGRESSION:\n            Z = model.mapping_fl(Z[:, 0])\n        else:\n            Z = Z.repeat(1, model.mapping_fl.num_layers, 1)\n\n        rec1 = model.decoder(Z, lod2batch.lod, blend_factor, noise=False)\n        rec2 = model.decoder(Z, lod2batch.lod, blend_factor, noise=True)\n\n        # rec1 = F.interpolate(rec1, sample.shape[2])\n        # rec2 = F.interpolate(rec2, sample.shape[2])\n        # sample_in = F.interpolate(sample_in, sample.shape[2])\n\n        Z = model.mapping_fl(samplez)\n        g_rec = model.decoder(Z, lod2batch.lod, blend_factor, noise=True)\n        # g_rec = F.interpolate(g_rec, sample.shape[2])\n\n        resultsample = torch.cat([sample_in, rec1, rec2, g_rec], dim=0)\n\n        @utils.async_func\n        def save_pic(x_rec):\n            tracker.register_means(lod2batch.current_epoch + lod2batch.iteration * 1.0 / lod2batch.get_dataset_size())\n            tracker.plot()\n\n            result_sample = x_rec * 0.5 + 0.5\n            result_sample = result_sample.cpu()\n            f = os.path.join(cfg.OUTPUT_DIR,\n                             \'sample_%d_%d.jpg\' % (\n                                 lod2batch.current_epoch + 1,\n                                 lod2batch.iteration // 1000)\n                             )\n            print(""Saved to %s"" % f)\n            save_image(result_sample, f, nrow=min(32, lod2batch.get_per_GPU_batch_size()))\n\n        save_pic(resultsample)\n\n\ndef train(cfg, logger, local_rank, world_size, distributed):\n    torch.cuda.set_device(local_rank)\n    model = Model(\n        startf=cfg.MODEL.START_CHANNEL_COUNT,\n        layer_count=cfg.MODEL.LAYER_COUNT,\n        maxf=cfg.MODEL.MAX_CHANNEL_COUNT,\n        latent_size=cfg.MODEL.LATENT_SPACE_SIZE,\n        dlatent_avg_beta=cfg.MODEL.DLATENT_AVG_BETA,\n        style_mixing_prob=cfg.MODEL.STYLE_MIXING_PROB,\n        mapping_layers=cfg.MODEL.MAPPING_LAYERS,\n        channels=cfg.MODEL.CHANNELS,\n        generator=cfg.MODEL.GENERATOR,\n        encoder=cfg.MODEL.ENCODER,\n        z_regression=cfg.MODEL.Z_REGRESSION\n    )\n    model.cuda(local_rank)\n    model.train()\n\n    if local_rank == 0:\n        model_s = Model(\n            startf=cfg.MODEL.START_CHANNEL_COUNT,\n            layer_count=cfg.MODEL.LAYER_COUNT,\n            maxf=cfg.MODEL.MAX_CHANNEL_COUNT,\n            latent_size=cfg.MODEL.LATENT_SPACE_SIZE,\n            truncation_psi=cfg.MODEL.TRUNCATIOM_PSI,\n            truncation_cutoff=cfg.MODEL.TRUNCATIOM_CUTOFF,\n            mapping_layers=cfg.MODEL.MAPPING_LAYERS,\n            channels=cfg.MODEL.CHANNELS,\n            generator=cfg.MODEL.GENERATOR,\n            encoder=cfg.MODEL.ENCODER,\n            z_regression=cfg.MODEL.Z_REGRESSION)\n        model_s.cuda(local_rank)\n        model_s.eval()\n        model_s.requires_grad_(False)\n\n    if distributed:\n        model = nn.parallel.DistributedDataParallel(\n            model,\n            device_ids=[local_rank],\n            broadcast_buffers=False,\n            bucket_cap_mb=25,\n            find_unused_parameters=True)\n        model.device_ids = None\n\n        decoder = model.module.decoder\n        encoder = model.module.encoder\n        mapping_tl = model.module.mapping_tl\n        mapping_fl = model.module.mapping_fl\n        dlatent_avg = model.module.dlatent_avg\n    else:\n        decoder = model.decoder\n        encoder = model.encoder\n        mapping_tl = model.mapping_tl\n        mapping_fl = model.mapping_fl\n        dlatent_avg = model.dlatent_avg\n\n    count_param_override.print = lambda a: logger.info(a)\n\n    logger.info(""Trainable parameters generator:"")\n    count_parameters(decoder)\n\n    logger.info(""Trainable parameters discriminator:"")\n    count_parameters(encoder)\n\n    arguments = dict()\n    arguments[""iteration""] = 0\n\n    decoder_optimizer = LREQAdam([\n        {\'params\': decoder.parameters()},\n        {\'params\': mapping_fl.parameters()}\n    ], lr=cfg.TRAIN.BASE_LEARNING_RATE, betas=(cfg.TRAIN.ADAM_BETA_0, cfg.TRAIN.ADAM_BETA_1), weight_decay=0)\n\n    encoder_optimizer = LREQAdam([\n        {\'params\': encoder.parameters()},\n        {\'params\': mapping_tl.parameters()},\n    ], lr=cfg.TRAIN.BASE_LEARNING_RATE, betas=(cfg.TRAIN.ADAM_BETA_0, cfg.TRAIN.ADAM_BETA_1), weight_decay=0)\n\n    scheduler = ComboMultiStepLR(optimizers=\n                                 {\n                                    \'encoder_optimizer\': encoder_optimizer,\n                                    \'decoder_optimizer\': decoder_optimizer\n                                 },\n                                 milestones=cfg.TRAIN.LEARNING_DECAY_STEPS,\n                                 gamma=cfg.TRAIN.LEARNING_DECAY_RATE,\n                                 reference_batch_size=32, base_lr=cfg.TRAIN.LEARNING_RATES)\n\n    model_dict = {\n        \'discriminator\': encoder,\n        \'generator\': decoder,\n        \'mapping_tl\': mapping_tl,\n        \'mapping_fl\': mapping_fl,\n        \'dlatent_avg\': dlatent_avg\n    }\n    if local_rank == 0:\n        model_dict[\'discriminator_s\'] = model_s.encoder\n        model_dict[\'generator_s\'] = model_s.decoder\n        model_dict[\'mapping_tl_s\'] = model_s.mapping_tl\n        model_dict[\'mapping_fl_s\'] = model_s.mapping_fl\n\n    tracker = LossTracker(cfg.OUTPUT_DIR)\n\n    checkpointer = Checkpointer(cfg,\n                                model_dict,\n                                {\n                                    \'encoder_optimizer\': encoder_optimizer,\n                                    \'decoder_optimizer\': decoder_optimizer,\n                                    \'scheduler\': scheduler,\n                                    \'tracker\': tracker\n                                },\n                                logger=logger,\n                                save=local_rank == 0)\n\n    extra_checkpoint_data = checkpointer.load()\n    logger.info(""Starting from epoch: %d"" % (scheduler.start_epoch()))\n\n    arguments.update(extra_checkpoint_data)\n\n    layer_to_resolution = decoder.layer_to_resolution\n\n    dataset = TFRecordsDataset(cfg, logger, rank=local_rank, world_size=world_size, buffer_size_mb=1024, channels=cfg.MODEL.CHANNELS)\n\n    rnd = np.random.RandomState(3456)\n    latents = rnd.randn(32, cfg.MODEL.LATENT_SPACE_SIZE)\n    samplez = torch.tensor(latents).float().cuda()\n\n    lod2batch = lod_driver.LODDriver(cfg, logger, world_size, dataset_size=len(dataset) * world_size)\n\n    if cfg.DATASET.SAMPLES_PATH:\n        path = cfg.DATASET.SAMPLES_PATH\n        src = []\n        with torch.no_grad():\n            for filename in list(os.listdir(path))[:32]:\n                img = np.asarray(Image.open(os.path.join(path, filename)))\n                if img.shape[2] == 4:\n                    img = img[:, :, :3]\n                im = img.transpose((2, 0, 1))\n                x = torch.tensor(np.asarray(im, dtype=np.float32), requires_grad=True).cuda() / 127.5 - 1.\n                if x.shape[0] == 4:\n                    x = x[:3]\n                src.append(x)\n            sample = torch.stack(src)\n    else:\n        dataset.reset(cfg.DATASET.MAX_RESOLUTION_LEVEL, 32)\n        sample = next(make_dataloader(cfg, logger, dataset, 32, local_rank))\n        sample = (sample / 127.5 - 1.)\n\n    lod2batch.set_epoch(scheduler.start_epoch(), [encoder_optimizer, decoder_optimizer])\n\n    for epoch in range(scheduler.start_epoch(), cfg.TRAIN.TRAIN_EPOCHS):\n        model.train()\n        lod2batch.set_epoch(epoch, [encoder_optimizer, decoder_optimizer])\n\n        logger.info(""Batch size: %d, Batch size per GPU: %d, LOD: %d - %dx%d, blend: %.3f, dataset size: %d"" % (\n                                                                lod2batch.get_batch_size(),\n                                                                lod2batch.get_per_GPU_batch_size(),\n                                                                lod2batch.lod,\n                                                                2 ** lod2batch.get_lod_power2(),\n                                                                2 ** lod2batch.get_lod_power2(),\n                                                                lod2batch.get_blend_factor(),\n                                                                len(dataset) * world_size))\n\n        dataset.reset(lod2batch.get_lod_power2(), lod2batch.get_per_GPU_batch_size())\n        batches = make_dataloader(cfg, logger, dataset, lod2batch.get_per_GPU_batch_size(), local_rank)\n\n        scheduler.set_batch_size(lod2batch.get_batch_size(), lod2batch.lod)\n\n        model.train()\n\n        need_permute = False\n        epoch_start_time = time.time()\n\n        i = 0\n        for x_orig in tqdm(batches):\n            i += 1\n            with torch.no_grad():\n                if x_orig.shape[0] != lod2batch.get_per_GPU_batch_size():\n                    continue\n                if need_permute:\n                    x_orig = x_orig.permute(0, 3, 1, 2)\n                x_orig = (x_orig / 127.5 - 1.)\n\n                blend_factor = lod2batch.get_blend_factor()\n\n                needed_resolution = layer_to_resolution[lod2batch.lod]\n                x = x_orig\n\n                if lod2batch.in_transition:\n                    needed_resolution_prev = layer_to_resolution[lod2batch.lod - 1]\n                    x_prev = F.avg_pool2d(x_orig, 2, 2)\n                    x_prev_2x = F.interpolate(x_prev, needed_resolution)\n                    x = x * blend_factor + x_prev_2x * (1.0 - blend_factor)\n\n            x.requires_grad = True\n\n            encoder_optimizer.zero_grad()\n            loss_d = model(x, lod2batch.lod, blend_factor, d_train=True, ae=False)\n            tracker.update(dict(loss_d=loss_d))\n            loss_d.backward()\n            encoder_optimizer.step()\n\n            decoder_optimizer.zero_grad()\n            loss_g = model(x, lod2batch.lod, blend_factor, d_train=False, ae=False)\n            tracker.update(dict(loss_g=loss_g))\n            loss_g.backward()\n            decoder_optimizer.step()\n\n            encoder_optimizer.zero_grad()\n            decoder_optimizer.zero_grad()\n            lae = model(x, lod2batch.lod, blend_factor, d_train=True, ae=True)\n            tracker.update(dict(lae=lae))\n            (lae).backward()\n            encoder_optimizer.step()\n            decoder_optimizer.step()\n\n            if local_rank == 0:\n                betta = 0.5 ** (lod2batch.get_batch_size() / (10 * 1000.0))\n                model_s.lerp(model, betta)\n\n            epoch_end_time = time.time()\n            per_epoch_ptime = epoch_end_time - epoch_start_time\n\n            lod_for_saving_model = lod2batch.lod\n            lod2batch.step()\n            if local_rank == 0:\n                if lod2batch.is_time_to_save():\n                    checkpointer.save(""model_tmp_intermediate_lod%d"" % lod_for_saving_model)\n                if lod2batch.is_time_to_report():\n                    save_sample(lod2batch, tracker, sample, samplez, x, logger, model_s, cfg, encoder_optimizer,\n                                decoder_optimizer)\n\n        scheduler.step()\n\n        if local_rank == 0:\n            checkpointer.save(""model_tmp_lod%d"" % lod_for_saving_model)\n            save_sample(lod2batch, tracker, sample, samplez, x, logger, model_s, cfg, encoder_optimizer, decoder_optimizer)\n\n    logger.info(""Training finish!... save training results"")\n    if local_rank == 0:\n        checkpointer.save(""model_final"").wait()\n\n\nif __name__ == ""__main__"":\n    gpu_count = torch.cuda.device_count()\n    run(train, get_cfg_defaults(), description=\'StyleGAN\', default_config=\'configs/ffhq.yaml\',\n        world_size=gpu_count)\n'"
train_alae_separate.py,13,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport torch.utils.data\nfrom torchvision.utils import save_image\nfrom net import *\nimport os\nimport utils\nfrom checkpointer import Checkpointer\nfrom scheduler import ComboMultiStepLR\nfrom custom_adam import LREQAdam\nfrom dataloader import *\nfrom tqdm import tqdm\nfrom dlutils.pytorch import count_parameters\nimport dlutils.pytorch.count_parameters as count_param_override\nfrom tracker import LossTracker\nfrom model_separate import Model\nfrom launcher import run\nfrom defaults import get_cfg_defaults\nimport lod_driver\nfrom PIL import Image\n\n\ndef save_sample(lod2batch, tracker, sample, samplez, x, logger, model, cfg, encoder_optimizer, decoder_optimizer):\n    os.makedirs(\'results\', exist_ok=True)\n\n    logger.info(\'\\n[%d/%d] - ptime: %.2f, %s, blend: %.3f, lr: %.12f,  %.12f, max mem: %f"",\' % (\n        (lod2batch.current_epoch + 1), cfg.TRAIN.TRAIN_EPOCHS, lod2batch.per_epoch_ptime, str(tracker),\n        lod2batch.get_blend_factor(),\n        encoder_optimizer.param_groups[0][\'lr\'], decoder_optimizer.param_groups[0][\'lr\'],\n        torch.cuda.max_memory_allocated() / 1024.0 / 1024.0))\n\n    with torch.no_grad():\n        model.eval()\n        sample = sample[:lod2batch.get_per_GPU_batch_size()]\n        samplez = samplez[:lod2batch.get_per_GPU_batch_size()]\n\n        needed_resolution = model.decoder.layer_to_resolution[lod2batch.lod]\n        sample_in = sample\n        while sample_in.shape[2] != needed_resolution:\n            sample_in = F.avg_pool2d(sample_in, 2, 2)\n\n        blend_factor = lod2batch.get_blend_factor()\n        if lod2batch.in_transition:\n            needed_resolution_prev = model.decoder.layer_to_resolution[lod2batch.lod - 1]\n            sample_in_prev = F.avg_pool2d(sample_in, 2, 2)\n            sample_in_prev_2x = F.interpolate(sample_in_prev, needed_resolution)\n            sample_in = sample_in * blend_factor + sample_in_prev_2x * (1.0 - blend_factor)\n\n        Z = model.encode(sample_in, lod2batch.lod, blend_factor)\n\n        Z = Z.repeat(1, model.mapping_fl.num_layers, 1)\n        rec1 = model.decoder(Z, lod2batch.lod, blend_factor, noise=False)\n        rec2 = model.decoder(Z, lod2batch.lod, blend_factor, noise=True)\n\n        rec1 = F.interpolate(rec1, sample.shape[2])\n        rec2 = F.interpolate(rec2, sample.shape[2])\n        sample_in = F.interpolate(sample_in, sample.shape[2])\n\n        Z = model.mapping_fl(samplez)\n        g_rec = model.decoder(Z, lod2batch.lod, blend_factor, noise=True)\n        g_rec = F.interpolate(g_rec, sample.shape[2])\n\n        resultsample = torch.cat([sample_in, rec1, rec2, g_rec], dim=0)\n\n        @utils.async_func\n        def save_pic(x_rec):\n            tracker.register_means(lod2batch.current_epoch + lod2batch.iteration * 1.0 / lod2batch.get_dataset_size())\n            tracker.plot()\n\n            result_sample = x_rec * 0.5 + 0.5\n            result_sample = result_sample.cpu()\n            f = os.path.join(cfg.OUTPUT_DIR,\n                             \'sample_%d_%d.jpg\' % (\n                                 lod2batch.current_epoch + 1,\n                                 lod2batch.iteration // 1000)\n                             )\n            print(""Saved to %s"" % f)\n            save_image(result_sample, f, nrow=min(32, lod2batch.get_per_GPU_batch_size()))\n\n        save_pic(resultsample)\n\n\ndef train(cfg, logger, local_rank, world_size, distributed):\n    torch.cuda.set_device(local_rank)\n    model = Model(\n        startf=cfg.MODEL.START_CHANNEL_COUNT,\n        layer_count=cfg.MODEL.LAYER_COUNT,\n        maxf=cfg.MODEL.MAX_CHANNEL_COUNT,\n        latent_size=cfg.MODEL.LATENT_SPACE_SIZE,\n        dlatent_avg_beta=cfg.MODEL.DLATENT_AVG_BETA,\n        style_mixing_prob=cfg.MODEL.STYLE_MIXING_PROB,\n        mapping_layers=cfg.MODEL.MAPPING_LAYERS,\n        channels=cfg.MODEL.CHANNELS,\n        generator=cfg.MODEL.GENERATOR,\n        encoder=cfg.MODEL.ENCODER\n    )\n    model.cuda(local_rank)\n    model.train()\n\n    if local_rank == 0:\n        model_s = Model(\n            startf=cfg.MODEL.START_CHANNEL_COUNT,\n            layer_count=cfg.MODEL.LAYER_COUNT,\n            maxf=cfg.MODEL.MAX_CHANNEL_COUNT,\n            latent_size=cfg.MODEL.LATENT_SPACE_SIZE,\n            truncation_psi=cfg.MODEL.TRUNCATIOM_PSI,\n            truncation_cutoff=cfg.MODEL.TRUNCATIOM_CUTOFF,\n            mapping_layers=cfg.MODEL.MAPPING_LAYERS,\n            channels=cfg.MODEL.CHANNELS,\n            generator=cfg.MODEL.GENERATOR,\n            encoder=cfg.MODEL.ENCODER)\n        model_s.cuda(local_rank)\n        model_s.eval()\n        model_s.requires_grad_(False)\n\n    if distributed:\n        model = nn.parallel.DistributedDataParallel(\n            model,\n            device_ids=[local_rank],\n            broadcast_buffers=False,\n            bucket_cap_mb=25,\n            find_unused_parameters=True)\n        model.device_ids = None\n\n        decoder = model.module.decoder\n        encoder = model.module.encoder\n        discriminator = model.module.discriminator\n        mapping_fl = model.module.mapping_fl\n        dlatent_avg = model.module.dlatent_avg\n    else:\n        decoder = model.decoder\n        encoder = model.encoder\n        discriminator = model.discriminator\n        mapping_fl = model.mapping_fl\n        dlatent_avg = model.dlatent_avg\n\n    count_param_override.print = lambda a: logger.info(a)\n\n    logger.info(""Trainable parameters generator:"")\n    count_parameters(decoder)\n\n    logger.info(""Trainable parameters discriminator:"")\n    count_parameters(encoder)\n\n    arguments = dict()\n    arguments[""iteration""] = 0\n\n    decoder_optimizer = LREQAdam([\n        {\'params\': decoder.parameters()},\n        {\'params\': mapping_fl.parameters()}\n    ], lr=cfg.TRAIN.BASE_LEARNING_RATE, betas=(cfg.TRAIN.ADAM_BETA_0, cfg.TRAIN.ADAM_BETA_1), weight_decay=0)\n\n    encoder_optimizer = LREQAdam([\n        {\'params\': encoder.parameters()},\n    ], lr=cfg.TRAIN.BASE_LEARNING_RATE, betas=(cfg.TRAIN.ADAM_BETA_0, cfg.TRAIN.ADAM_BETA_1), weight_decay=0)\n\n    discriminator_optimizer = LREQAdam([\n        {\'params\': discriminator.parameters()},\n    ], lr=cfg.TRAIN.BASE_LEARNING_RATE, betas=(cfg.TRAIN.ADAM_BETA_0, cfg.TRAIN.ADAM_BETA_1), weight_decay=0)\n\n    scheduler = ComboMultiStepLR(optimizers=\n                                 {\n                                    \'encoder_optimizer\': encoder_optimizer,\n                                    \'discriminator_optimizer\': discriminator_optimizer,\n                                    \'decoder_optimizer\': decoder_optimizer\n                                 },\n                                 milestones=cfg.TRAIN.LEARNING_DECAY_STEPS,\n                                 gamma=cfg.TRAIN.LEARNING_DECAY_RATE,\n                                 reference_batch_size=32, base_lr=cfg.TRAIN.LEARNING_RATES)\n\n    model_dict = {\n        \'discriminator\': discriminator,\n        \'encoder\': encoder,\n        \'generator\': decoder,\n        \'mapping_fl\': mapping_fl,\n        \'dlatent_avg\': dlatent_avg\n    }\n    if local_rank == 0:\n        model_dict[\'discriminator_s\'] = model_s.discriminator\n        model_dict[\'encoder_s\'] = model_s.encoder\n        model_dict[\'generator_s\'] = model_s.decoder\n        model_dict[\'mapping_fl_s\'] = model_s.mapping_fl\n\n    tracker = LossTracker(cfg.OUTPUT_DIR)\n\n    checkpointer = Checkpointer(cfg,\n                                model_dict,\n                                {\n                                    \'encoder_optimizer\': encoder_optimizer,\n                                    \'discriminator_optimizer\': discriminator_optimizer,\n                                    \'decoder_optimizer\': decoder_optimizer,\n                                    \'scheduler\': scheduler,\n                                    \'tracker\': tracker\n                                },\n                                logger=logger,\n                                save=local_rank == 0)\n\n    extra_checkpoint_data = checkpointer.load()\n    logger.info(""Starting from epoch: %d"" % (scheduler.start_epoch()))\n\n    arguments.update(extra_checkpoint_data)\n\n    layer_to_resolution = decoder.layer_to_resolution\n\n    dataset = TFRecordsDataset(cfg, logger, rank=local_rank, world_size=world_size, buffer_size_mb=1024, channels=cfg.MODEL.CHANNELS)\n\n    rnd = np.random.RandomState(3456)\n    latents = rnd.randn(32, cfg.MODEL.LATENT_SPACE_SIZE)\n    samplez = torch.tensor(latents).float().cuda()\n\n    lod2batch = lod_driver.LODDriver(cfg, logger, world_size, dataset_size=len(dataset) * world_size)\n\n    if cfg.DATASET.SAMPLES_PATH:\n        path = cfg.DATASET.SAMPLES_PATH\n        src = []\n        with torch.no_grad():\n            for filename in list(os.listdir(path))[:32]:\n                img = np.asarray(Image.open(os.path.join(path, filename)))\n                if img.shape[2] == 4:\n                    img = img[:, :, :3]\n                im = img.transpose((2, 0, 1))\n                x = torch.tensor(np.asarray(im, dtype=np.float32), requires_grad=True).cuda() / 127.5 - 1.\n                if x.shape[0] == 4:\n                    x = x[:3]\n                src.append(x)\n            sample = torch.stack(src)\n    else:\n        dataset.reset(cfg.DATASET.MAX_RESOLUTION_LEVEL, 32)\n        sample = next(make_dataloader(cfg, logger, dataset, 32, local_rank))\n        sample = (sample / 127.5 - 1.)\n\n    lod2batch.set_epoch(scheduler.start_epoch(), [encoder_optimizer, decoder_optimizer])\n\n    for epoch in range(scheduler.start_epoch(), cfg.TRAIN.TRAIN_EPOCHS):\n        model.train()\n        lod2batch.set_epoch(epoch, [encoder_optimizer, decoder_optimizer])\n\n        logger.info(""Batch size: %d, Batch size per GPU: %d, LOD: %d - %dx%d, blend: %.3f, dataset size: %d"" % (\n                                                                lod2batch.get_batch_size(),\n                                                                lod2batch.get_per_GPU_batch_size(),\n                                                                lod2batch.lod,\n                                                                2 ** lod2batch.get_lod_power2(),\n                                                                2 ** lod2batch.get_lod_power2(),\n                                                                lod2batch.get_blend_factor(),\n                                                                len(dataset) * world_size))\n\n        dataset.reset(lod2batch.get_lod_power2(), lod2batch.get_per_GPU_batch_size())\n        batches = make_dataloader(cfg, logger, dataset, lod2batch.get_per_GPU_batch_size(), local_rank)\n\n        scheduler.set_batch_size(lod2batch.get_batch_size(), lod2batch.lod)\n\n        model.train()\n\n        need_permute = False\n        epoch_start_time = time.time()\n\n        i = 0\n        with torch.autograd.profiler.profile(use_cuda=True, enabled=False) as prof:\n            for x_orig in tqdm(batches):\n                i +=1\n                with torch.no_grad():\n                    if x_orig.shape[0] != lod2batch.get_per_GPU_batch_size():\n                        continue\n                    if need_permute:\n                        x_orig = x_orig.permute(0, 3, 1, 2)\n                    x_orig = (x_orig / 127.5 - 1.)\n\n                    blend_factor = lod2batch.get_blend_factor()\n\n                    needed_resolution = layer_to_resolution[lod2batch.lod]\n                    x = x_orig\n\n                    if lod2batch.in_transition:\n                        needed_resolution_prev = layer_to_resolution[lod2batch.lod - 1]\n                        x_prev = F.avg_pool2d(x_orig, 2, 2)\n                        x_prev_2x = F.interpolate(x_prev, needed_resolution)\n                        x = x * blend_factor + x_prev_2x * (1.0 - blend_factor)\n\n                x.requires_grad = True\n\n                loss_d = model(x, lod2batch.lod, blend_factor, d_train=True, ae=False)\n                tracker.update(dict(loss_d=loss_d))\n                loss_d.backward()\n                discriminator_optimizer.step()\n                decoder_optimizer.zero_grad()\n                discriminator_optimizer.zero_grad()\n\n                loss_g = model(x, lod2batch.lod, blend_factor, d_train=False, ae=False)\n                tracker.update(dict(loss_g=loss_g))\n                loss_g.backward()\n                decoder_optimizer.step()\n                decoder_optimizer.zero_grad()\n                discriminator_optimizer.zero_grad()\n\n                lae = model(x, lod2batch.lod, blend_factor, d_train=True, ae=True)\n                tracker.update(dict(lae=lae))\n                (lae).backward()\n                encoder_optimizer.step()\n                decoder_optimizer.step()\n                encoder_optimizer.zero_grad()\n                decoder_optimizer.zero_grad()\n\n                if local_rank == 0:\n                    betta = 0.5 ** (lod2batch.get_batch_size() / (10 * 1000.0))\n                    model_s.lerp(model, betta)\n\n                epoch_end_time = time.time()\n                per_epoch_ptime = epoch_end_time - epoch_start_time\n\n                lod_for_saving_model = lod2batch.lod\n                lod2batch.step()\n                if local_rank == 0:\n                    if lod2batch.is_time_to_save():\n                        checkpointer.save(""model_tmp_intermediate_lod%d"" % lod_for_saving_model)\n                    if lod2batch.is_time_to_report():\n                        save_sample(lod2batch, tracker, sample, samplez, x, logger, model_s, cfg, encoder_optimizer, decoder_optimizer)\n\n        scheduler.step()\n\n        if local_rank == 0:\n            checkpointer.save(""model_tmp_lod%d"" % lod_for_saving_model)\n            save_sample(lod2batch, tracker, sample, samplez, x, logger, model_s, cfg, encoder_optimizer, decoder_optimizer)\n\n    logger.info(""Training finish!... save training results"")\n    if local_rank == 0:\n        checkpointer.save(""model_final"").wait()\n\n\nif __name__ == ""__main__"":\n    gpu_count = torch.cuda.device_count()\n    run(train, get_cfg_defaults(), description=\'StyleGAN\', default_config=\'configs/experiment_celeba_sep.yaml\',\n        world_size=gpu_count)\n'"
utils.py,2,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom torch import nn\nimport torch\nimport threading\nimport hashlib\nimport pickle\nimport os\n\n\nclass cache:\n    def __init__(self, function):\n        self.function = function\n        self.pickle_name = self.function.__name__\n\n    def __call__(self, *args, **kwargs):\n        m = hashlib.sha256()\n        m.update(pickle.dumps((self.function.__name__, args, frozenset(kwargs.items()))))\n        output_path = os.path.join(\'.cache\', ""%s_%s"" % (m.hexdigest(), self.pickle_name))\n        try:\n            with open(output_path, \'rb\') as f:\n                data = pickle.load(f)\n        except (FileNotFoundError, pickle.PickleError):\n            data = self.function(*args, **kwargs)\n            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n            with open(output_path, \'wb\') as f:\n                pickle.dump(data, f)\n        return data\n\n\ndef save_model(x, name):\n    if isinstance(x, nn.DataParallel):\n        torch.save(x.module.state_dict(), name)\n    else:\n        torch.save(x.state_dict(), name)\n\n\nclass AsyncCall(object):\n    def __init__(self, fnc, callback=None):\n        self.Callable = fnc\n        self.Callback = callback\n        self.result = None\n\n    def __call__(self, *args, **kwargs):\n        self.Thread = threading.Thread(target=self.run, name=self.Callable.__name__, args=args, kwargs=kwargs)\n        self.Thread.start()\n        return self\n\n    def wait(self, timeout=None):\n        self.Thread.join(timeout)\n        if self.Thread.isAlive():\n            raise TimeoutError\n        else:\n            return self.result\n\n    def run(self, *args, **kwargs):\n        self.result = self.Callable(*args, **kwargs)\n        if self.Callback:\n            self.Callback(self.result)\n\n\nclass AsyncMethod(object):\n    def __init__(self, fnc, callback=None):\n        self.Callable = fnc\n        self.Callback = callback\n\n    def __call__(self, *args, **kwargs):\n        return AsyncCall(self.Callable, self.Callback)(*args, **kwargs)\n\n\ndef async_func(fnc=None, callback=None):\n    if fnc is None:\n        def add_async_callback(f):\n            return AsyncMethod(f, callback)\n        return add_async_callback\n    else:\n        return AsyncMethod(fnc, callback)\n\n\nclass Registry(dict):\n    def __init__(self, *args, **kwargs):\n        super(Registry, self).__init__(*args, **kwargs)\n\n    def register(self, module_name):\n        def register_fn(module):\n            assert module_name not in self\n            self[module_name] = module\n            return module\n        return register_fn\n'"
dataset_preparation/prepare_celeba.py,0,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Create a pickle for celeba128x128 training.  Not used in current pipeline""""""\n\nimport random\nimport pickle\nimport zipfile\n\nimport numpy as np\n\nfrom scipy import misc\nimport tqdm\n\nfrom dlutils import download\n\n\ncorrupted = [\n    \'195995.jpg\',\n    \'131065.jpg\',\n    \'118355.jpg\',\n    \'080480.jpg\',\n    \'039459.jpg\',\n    \'153323.jpg\',\n    \'011793.jpg\',\n    \'156817.jpg\',\n    \'121050.jpg\',\n    \'198603.jpg\',\n    \'041897.jpg\',\n    \'131899.jpg\',\n    \'048286.jpg\',\n    \'179577.jpg\',\n    \'024184.jpg\',\n    \'016530.jpg\',\n]\n\ndownload.from_google_drive(""0B7EVK8r0v71pZjFTYXZWM3FlRnM"")\n\n\ndef center_crop(x, crop_h=128, crop_w=None, resize_w=128):\n    # crop the images to [crop_h,crop_w,3] then resize to [resize_h,resize_w,3]\n    if crop_w is None:\n        crop_w = crop_h # the width and height after cropped\n    h, w = x.shape[:2]\n    j = int(round((h - crop_h)/2.)) + 15\n    i = int(round((w - crop_w)/2.))\n    return misc.imresize(x[j:j+crop_h, i:i+crop_w], [resize_w, resize_w]) \n\n\narchive = zipfile.ZipFile(\'img_align_celeba.zip\', \'r\')\n\nnames = archive.namelist()\n\nnames = [x for x in names if x[-4:] == \'.jpg\']\n\ncount = len(names)\nprint(""Count: %d"" % count)\n\nnames = [x for x in names if x[-10:] not in corrupted]\n\nfolds = 5\n\nrandom.shuffle(names)\n\nimages = {}\n\ncount = len(names)\nprint(""Count: %d"" % count)\ncount_per_fold = count // folds\n\ni = 0\nim = 0\nfor x in tqdm.tqdm(names):\n    imgfile = archive.open(x)\n    image = center_crop(misc.imread(imgfile))\n    images[x] = image\n    im += 1\n\n    if im == count_per_fold:\n        output = open(\'data_fold_%d.pkl\' % i, \'wb\')\n        pickle.dump(list(images.values()), output)\n        output.close()\n        i += 1\n        im = 0\n        images.clear()\n'"
dataset_preparation/prepare_celeba_hq_tfrecords.py,3,"b'import zipfile\nimport tqdm\nfrom defaults import get_cfg_defaults\nimport sys\nimport logging\n\nfrom dlutils import download\n\nfrom scipy import misc\nfrom net import *\nimport numpy as np\nimport pickle\nimport random\nimport argparse\nimport os\nfrom dlutils.pytorch.cuda_helper import *\nimport tensorflow as tf\nimport imageio\nfrom PIL import Image\n\n\ndef prepare_celeba(cfg, logger, train=True):\n    if train:\n        directory = os.path.dirname(cfg.DATASET.PATH)\n    else:\n        directory = os.path.dirname(cfg.DATASET.PATH_TEST)\n\n    os.makedirs(directory, exist_ok=True)\n\n    images = []\n    # The official way of generating CelebA-HQ can be challenging.\n    # Please refer to this page: https://github.com/suvojit-0x55aa/celebA-HQ-dataset-download\n    # You can get pre-generated dataset from: https://drive.google.com/drive/folders/11Vz0fqHS2rXDb5pprgTjpD7S2BAJhi1P\n    source_path = \'/data/datasets/celeba-hq/data1024x1024\'\n    for filename in tqdm.tqdm(os.listdir(source_path)):\n        images.append((int(filename[:-4]), filename))\n\n    print(""Total count: %d"" % len(images))\n    if train:\n        images = images[:cfg.DATASET.SIZE]\n    else:\n        images = images[cfg.DATASET.SIZE_TEST:]\n\n    count = len(images)\n    print(""Count: %d"" % count)\n\n    random.seed(0)\n    random.shuffle(images)\n\n    folds = cfg.DATASET.PART_COUNT\n    celeba_folds = [[] for _ in range(folds)]\n\n    count_per_fold = count // folds\n    for i in range(folds):\n        celeba_folds[i] += images[i * count_per_fold: (i + 1) * count_per_fold]\n\n    for i in range(folds):\n        if train:\n            path = cfg.DATASET.PATH\n        else:\n            path = cfg.DATASET.PATH_TEST\n\n        writers = {}\n        for lod in range(cfg.DATASET.MAX_RESOLUTION_LEVEL, 1, -1):\n            tfr_opt = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.NONE)\n            part_path = path % (lod, i)\n            os.makedirs(os.path.dirname(part_path), exist_ok=True)\n            tfr_writer = tf.python_io.TFRecordWriter(part_path, tfr_opt)\n            writers[lod] = tfr_writer\n\n        for label, filename in tqdm.tqdm(celeba_folds[i]):\n            img = np.asarray(Image.open(os.path.join(source_path, filename)))\n            img = img.transpose((2, 0, 1))\n            for lod in range(cfg.DATASET.MAX_RESOLUTION_LEVEL, 1, -1):\n                ex = tf.train.Example(features=tf.train.Features(feature={\n                    \'shape\': tf.train.Feature(int64_list=tf.train.Int64List(value=img.shape)),\n                    \'label\': tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\n                    \'data\': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img.tostring()]))}))\n                writers[lod].write(ex.SerializeToString())\n\n                image = torch.tensor(np.asarray(img, dtype=np.float32)).view(1, 3, img.shape[1], img.shape[2])\n                image_down = F.avg_pool2d(image, 2, 2).clamp_(0, 255).to(\'cpu\', torch.uint8).view(3, image.shape[2] // 2, image.shape[3] // 2).numpy()\n\n                img = image_down\n\n\ndef run():\n    parser = argparse.ArgumentParser(description=""Adversarial, hierarchical style VAE"")\n    parser.add_argument(\n        ""--config-file"",\n        default=""configs/celeba-hq256.yaml"",\n        metavar=""FILE"",\n        help=""path to config file"",\n        type=str,\n    )\n    parser.add_argument(\n        ""opts"",\n        help=""Modify config options using the command-line"",\n        default=None,\n        nargs=argparse.REMAINDER,\n    )\n\n    args = parser.parse_args()\n    cfg = get_cfg_defaults()\n    cfg.merge_from_file(args.config_file)\n    cfg.merge_from_list(args.opts)\n    cfg.freeze()\n\n    logger = logging.getLogger(""logger"")\n    logger.setLevel(logging.DEBUG)\n\n    output_dir = cfg.OUTPUT_DIR\n    os.makedirs(output_dir, exist_ok=True)\n\n    ch = logging.StreamHandler(stream=sys.stdout)\n    ch.setLevel(logging.DEBUG)\n    formatter = logging.Formatter(""%(asctime)s %(name)s %(levelname)s: %(message)s"")\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n    logger.info(args)\n\n    logger.info(""Loaded configuration file {}"".format(args.config_file))\n    with open(args.config_file, ""r"") as cf:\n        config_str = ""\\n"" + cf.read()\n        logger.info(config_str)\n    logger.info(""Running with config:\\n{}"".format(cfg))\n\n    prepare_celeba(cfg, logger, True)\n    prepare_celeba(cfg, logger, False)\n\n\nif __name__ == \'__main__\':\n    run()\n\n'"
dataset_preparation/prepare_celeba_tfrecords.py,2,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Create a tfrecords for celeba128x128 training. """"""\n\nimport zipfile\nimport tqdm\nfrom defaults import get_cfg_defaults\nimport sys\nimport logging\nfrom net import *\nimport numpy as np\nimport random\nimport argparse\nimport os\nimport tensorflow as tf\nimport imageio\nfrom PIL import Image\n\n\ndef prepare_celeba(cfg, logger, train=True):\n    if train:\n        directory = os.path.dirname(cfg.DATASET.PATH)\n    else:\n        directory = os.path.dirname(cfg.DATASET.PATH_TEST)\n\n    with open(""/data/datasets/CelebA/Eval/list_eval_partition.txt"") as f:\n        lineList = f.readlines()\n    lineList = [x[:-1].split(\' \') for x in lineList]\n\n    split_map = {}\n    for x in lineList:\n        split_map[int(x[0][:-4])] = int(x[1])\n\n    os.makedirs(directory, exist_ok=True)\n\n    corrupted = [\n        \'195995.jpg\',\n        \'131065.jpg\',\n        \'118355.jpg\',\n        \'080480.jpg\',\n        \'039459.jpg\',\n        \'153323.jpg\',\n        \'011793.jpg\',\n        \'156817.jpg\',\n        \'121050.jpg\',\n        \'198603.jpg\',\n        \'041897.jpg\',\n        \'131899.jpg\',\n        \'048286.jpg\',\n        \'179577.jpg\',\n        \'024184.jpg\',\n        \'016530.jpg\',\n    ]\n\n    def center_crop(x, crop_h=128, crop_w=None, resize_w=128):\n        # crop the images to [crop_h,crop_w,3] then resize to [resize_h,resize_w,3]\n        if crop_w is None:\n            crop_w = crop_h # the width and height after cropped\n        h, w = x.shape[:2]\n        j = int(round((h - crop_h)/2.)) + 15\n        i = int(round((w - crop_w)/2.))\n        return np.array(Image.fromarray(x[j:j+crop_h, i:i+crop_w]).resize([resize_w, resize_w]))\n\n    archive = zipfile.ZipFile(os.path.join(directory, \'/data/datasets/CelebA/Img/img_align_celeba.zip\'), \'r\')\n\n    names = archive.namelist()\n\n    names = [x for x in names if x[-4:] == \'.jpg\']\n\n    if train:\n        names = [x for x in names if split_map[int(x[:-4][-6:])] != 2]\n    else:\n        names = [x for x in names if split_map[int(x[:-4][-6:])] == 2]\n\n    count = len(names)\n    print(""Count: %d"" % count)\n\n    names = [x for x in names if x[-10:] not in corrupted]\n\n    random.seed(0)\n    random.shuffle(names)\n\n    folds = cfg.DATASET.PART_COUNT\n    celeba_folds = [[] for _ in range(folds)]\n\n    spread_identiteis_across_folds = True\n\n    if spread_identiteis_across_folds:\n        # Reading indetities\n        # Has format of\n        # 000001.jpg 2880\n        # 000002.jpg 2937\n        with open(""/data/datasets/CelebA/Anno/identity_CelebA.txt"") as f:\n            lineList = f.readlines()\n\n        lineList = [x[:-1].split(\' \') for x in lineList]\n\n        identity_map = {}\n        for x in lineList:\n            identity_map[x[0]] = int(x[1])\n\n        names = [(identity_map[x.split(\'/\')[1]], x) for x in names]\n\n        class_bins = {}\n\n        for x in names:\n            if x[0] not in class_bins:\n                class_bins[x[0]] = []\n            img_file_name = x[1]\n            class_bins[x[0]].append((x[0], img_file_name))\n\n        left_overs = []\n\n        for _class, filenames in class_bins.items():\n            count = len(filenames)\n            print(""Class %d count: %d"" % (_class, count))\n\n            count_per_fold = count // folds\n\n            for i in range(folds):\n                celeba_folds[i] += filenames[i * count_per_fold: (i + 1) * count_per_fold]\n\n            left_overs += filenames[folds * count_per_fold:]\n\n        leftover_per_fold = len(left_overs) // folds\n        for i in range(folds):\n            celeba_folds[i] += left_overs[i * leftover_per_fold: (i + 1) * leftover_per_fold]\n\n        for i in range(folds):\n            random.shuffle(celeba_folds[i])\n\n        # strip ids\n        for i in range(folds):\n            celeba_folds[i] = [x[1] for x in celeba_folds[i]]\n\n        print(""Folds sizes:"")\n        for i in range(len(celeba_folds)):\n            print(len(celeba_folds[i]))\n    else:\n        count_per_fold = count // folds\n        for i in range(folds):\n            celeba_folds[i] += names[i * count_per_fold: (i + 1) * count_per_fold]\n\n    for i in range(folds):\n        images = []\n        for x in tqdm.tqdm(celeba_folds[i]):\n            imgfile = archive.open(x)\n            image = center_crop(imageio.imread(imgfile.read()))\n            images.append((int(x[:-4][-6:]), image.transpose((2, 0, 1))))\n\n        tfr_opt = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.NONE)\n\n        if train:\n            part_path = cfg.DATASET.PATH % (cfg.DATASET.MAX_RESOLUTION_LEVEL, i)\n        else:\n            part_path = cfg.DATASET.PATH_TEST % (cfg.DATASET.MAX_RESOLUTION_LEVEL, i)\n\n        tfr_writer = tf.python_io.TFRecordWriter(part_path, tfr_opt)\n\n        random.shuffle(images)\n\n        for label, image in images:\n            ex = tf.train.Example(features=tf.train.Features(feature={\n                \'shape\': tf.train.Feature(int64_list=tf.train.Int64List(value=image.shape)),\n                \'label\': tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\n                \'data\': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image.tostring()]))}))\n            tfr_writer.write(ex.SerializeToString())\n        tfr_writer.close()\n\n        for j in range(5):\n            images_down = []\n\n            for label, image in tqdm.tqdm(images):\n                h = image.shape[1]\n                w = image.shape[2]\n                image = torch.tensor(np.asarray(image, dtype=np.float32)).view(1, 3, h, w)\n\n                image_down = F.avg_pool2d(image, 2, 2).clamp_(0, 255).to(\'cpu\', torch.uint8)\n\n                image_down = image_down.view(3, h // 2, w // 2).numpy()\n                images_down.append((label, image_down))\n\n            if train:\n                part_path = cfg.DATASET.PATH % (cfg.DATASET.MAX_RESOLUTION_LEVEL - j - 1, i)\n            else:\n                part_path = cfg.DATASET.PATH_TEST % (cfg.DATASET.MAX_RESOLUTION_LEVEL - j - 1, i)\n\n            tfr_writer = tf.python_io.TFRecordWriter(part_path, tfr_opt)\n            for label, image in images_down:\n                ex = tf.train.Example(features=tf.train.Features(feature={\n                    \'shape\': tf.train.Feature(int64_list=tf.train.Int64List(value=image.shape)),\n                    \'label\': tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\n                    \'data\': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image.tostring()]))}))\n                tfr_writer.write(ex.SerializeToString())\n            tfr_writer.close()\n\n            images = images_down\n\n\ndef run():\n    parser = argparse.ArgumentParser(description=""ALAE. Prepare tfrecords for celeba128x128"")\n    parser.add_argument(\n        ""--config-file"",\n        default=""configs/celeba.yaml"",\n        metavar=""FILE"",\n        help=""path to config file"",\n        type=str,\n    )\n    parser.add_argument(\n        ""opts"",\n        help=""Modify config options using the command-line"",\n        default=None,\n        nargs=argparse.REMAINDER,\n    )\n\n    args = parser.parse_args()\n    cfg = get_cfg_defaults()\n    cfg.merge_from_file(args.config_file)\n    cfg.merge_from_list(args.opts)\n    cfg.freeze()\n\n    logger = logging.getLogger(""logger"")\n    logger.setLevel(logging.DEBUG)\n\n    output_dir = cfg.OUTPUT_DIR\n    os.makedirs(output_dir, exist_ok=True)\n\n    ch = logging.StreamHandler(stream=sys.stdout)\n    ch.setLevel(logging.DEBUG)\n    formatter = logging.Formatter(""%(asctime)s %(name)s %(levelname)s: %(message)s"")\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n    logger.info(args)\n\n    logger.info(""Loaded configuration file {}"".format(args.config_file))\n    with open(args.config_file, ""r"") as cf:\n        config_str = ""\\n"" + cf.read()\n        logger.info(config_str)\n    logger.info(""Running with config:\\n{}"".format(cfg))\n\n    prepare_celeba(cfg, logger, True)\n    prepare_celeba(cfg, logger, False)\n\n\nif __name__ == \'__main__\':\n    run()\n\n'"
dataset_preparation/prepare_imagenet.py,4,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Create a tfrecords for ImageNET. """"""\n\nimport os\nimport scipy.io as sio\nimport torch\nfrom PIL import Image\nimport random\nimport argparse\nfrom defaults import get_cfg_defaults\nimport sys\nimport logging\n\nimport tensorflow as tf\nfrom torchvision.transforms import functional as F\nfrom torch.nn.functional import avg_pool2d\nfrom utils import cache\nimport numpy as np\nimport tqdm\nfrom multiprocessing import Pool\nfrom threading import Thread\n\n\ndef process_fold(i, path, image_folds, train_root, wnid_to_indx, fixed=False):\n    writers = {}\n    for lod in range(8, 1, -1):\n        tfr_opt = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.NONE)\n        part_path = path % (lod, i)\n        os.makedirs(os.path.dirname(part_path), exist_ok=True)\n        tfr_writer = tf.python_io.TFRecordWriter(part_path, tfr_opt)\n        writers[lod] = tfr_writer\n\n    for s, image in image_folds[i]:\n        im = os.path.join(train_root, s, image)\n        img = Image.open(im)\n        if fixed:\n            img = F.resize(img, 288)\n            img = F.center_crop(img, 256)\n        else:\n            img = F.resize(img, 288)\n            img = F.center_crop(img, 288)\n        img = np.asarray(img)\n        if len(img.shape) == 2:\n            img = np.tile(img[:, :, None], (1, 1, 3))\n        img = img.transpose((2, 0, 1))\n        if img.shape[0] > 3:\n            img = img[:3]\n\n        for lod in range(8, 1, -1):\n            ex = tf.train.Example(features=tf.train.Features(feature={\n                \'shape\': tf.train.Feature(int64_list=tf.train.Int64List(value=img.shape)),\n                \'label\': tf.train.Feature(int64_list=tf.train.Int64List(value=[wnid_to_indx[s]])),\n                \'data\': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img.tostring()]))}))\n            writers[lod].write(ex.SerializeToString())\n\n            image = torch.tensor(np.asarray(img, dtype=np.float32)).view(1, 3, img.shape[1], img.shape[2])\n            image_down = avg_pool2d(image, 2, 2).clamp_(0, 255).to(\'cpu\', torch.uint8).view(3, image.shape[2] // 2,\n                                                                                            image.shape[3] // 2).numpy()\n\n            img = image_down\n\n    for lod in range(8, 1, -1):\n        writers[lod].close()\n\n\ndef parse_meta_mat(devkit_root):\n    metafile = os.path.join(devkit_root, ""data"", ""meta.mat"")\n    meta = sio.loadmat(metafile, squeeze_me=True)[\'synsets\']\n    nums_children = list(zip(*meta))[4]\n    meta = [meta[idx] for idx, num_children in enumerate(nums_children)\n            if num_children == 0]\n    idcs, wnids, classes = list(zip(*meta))[:3]\n    classes = [tuple(clss.split(\', \')) for clss in classes]\n    idx_to_wnid = {idx: wnid for idx, wnid in zip(idcs, wnids)}\n    wnid_to_classes = {wnid: clss for wnid, clss in zip(wnids, classes)}\n    return idx_to_wnid, wnid_to_classes\n\n\ndef parse_val_groundtruth_txt(devkit_root):\n    file = os.path.join(devkit_root, ""data"",\n                        ""ILSVRC2012_validation_ground_truth.txt"")\n    with open(file, \'r\') as txtfh:\n        val_idcs = txtfh.readlines()\n    return [int(val_idx) for val_idx in val_idcs]\n\n\n@cache\ndef get_names(train_root):\n    names = []\n    sets = os.listdir(train_root)\n    for s in sets:\n        images = os.listdir(os.path.join(train_root, s))\n        names += [(s, im) for im in images]\n    return names\n\n\ndef prepare_imagenet(cfg, logger):\n    devkit_root = ""/data/datasets/ImageNet_bak/ILSVRC2012_devkit_t12""\n    idx_to_wnid, wnid_to_classes = parse_meta_mat(devkit_root)\n    val_idcs = parse_val_groundtruth_txt(devkit_root)\n    val_wnids = [idx_to_wnid[idx] for idx in val_idcs]\n\n    for i in range(1, 1001):\n        w = idx_to_wnid[i]\n        c = wnid_to_classes[w]\n        print(""%d - %s"" % (i, c))\n\n    wnid_to_indx = dict([(v, k - 1) for k, v in idx_to_wnid.items()])\n\n    torch.save((wnid_to_classes, val_wnids), os.path.join("""", ""meta""))\n\n    train_root = ""/data/datasets/ImageNet_bak/raw-data/train""\n    validation_root = ""/data/datasets/ImageNet_bak/raw-data/validation""\n\n    ###\n    logger.info(""Savingexamples"")\n\n    path = \'dataset_samples/imagenet256x256\'\n    os.makedirs(path, exist_ok=True)\n    k = 0\n    names = get_names(train_root)\n    random.shuffle(names)\n    for s, image in names:\n        im = os.path.join(train_root, s, image)\n        img = Image.open(im)\n        img = F.resize(img, 288)\n        img = F.center_crop(img, 256)\n        img = np.asarray(img)\n        if len(img.shape) == 2:\n            img = np.tile(img[:, :, None], (1, 1, 3))\n        img = img.transpose((2, 0, 1))\n        if img.shape[0] > 3:\n            img = img[:3]\n        img = img.transpose((1, 2, 0))\n        img = Image.fromarray(img)\n        img.save(path + \'/\' + str(k) + "".png"")\n        k += 1\n        if k == 2000:\n            break\n    ###\n    exit()\n\n    if True:\n        random.seed(0)\n\n        names = get_names(train_root)\n        random.shuffle(names)\n\n        folds = 16 # cfg.DATASET.PART_COUNT\n        image_folds = [[] for _ in range(folds)]\n\n        count_per_fold = len(names) // folds\n        for i in range(folds):\n            image_folds[i] += names[i * count_per_fold: (i + 1) * count_per_fold]\n\n        threads = []\n        for i in range(folds):\n            thread = Thread(target=process_fold, args=(i, cfg.DATASET.PATH, image_folds, train_root, wnid_to_indx, False))\n            thread.start()\n            threads.append(thread)\n\n        for i in range(folds):\n            threads[i].join()\n    if False:\n        random.seed(0)\n\n        names = get_names(validation_root)\n        random.shuffle(names)\n\n        folds = 1 # cfg.DATASET.PART_COUNT\n        image_folds = [[] for _ in range(folds)]\n\n        count_per_fold = len(names) // folds\n        for i in range(folds):\n            image_folds[i] += names[i * count_per_fold: (i + 1) * count_per_fold]\n\n        threads = []\n        for i in range(folds):\n            thread = Thread(target=process_fold, args=(i, cfg.DATASET.PATH_TEST, image_folds, validation_root, wnid_to_indx, True))\n            thread.start()\n            threads.append(thread)\n\n        for i in range(folds):\n            threads[i].join()\n\n    print(idx_to_wnid, wnid_to_classes)\n\n\ndef run():\n    parser = argparse.ArgumentParser(description=""ALAE imagenet"")\n    parser.add_argument(\n        ""--config-file"",\n        default=""configs/imagenet.yaml"",\n        metavar=""FILE"",\n        help=""path to config file"",\n        type=str,\n    )\n    parser.add_argument(\n        ""opts"",\n        help=""Modify config options using the command-line"",\n        default=None,\n        nargs=argparse.REMAINDER,\n    )\n\n    args = parser.parse_args()\n    cfg = get_cfg_defaults()\n    cfg.merge_from_file(args.config_file)\n    cfg.merge_from_list(args.opts)\n    cfg.freeze()\n\n    logger = logging.getLogger(""logger"")\n    logger.setLevel(logging.DEBUG)\n\n    output_dir = cfg.OUTPUT_DIR\n    os.makedirs(output_dir, exist_ok=True)\n\n    ch = logging.StreamHandler(stream=sys.stdout)\n    ch.setLevel(logging.DEBUG)\n    formatter = logging.Formatter(""%(asctime)s %(name)s %(levelname)s: %(message)s"")\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n    logger.info(args)\n\n    logger.info(""Loaded configuration file {}"".format(args.config_file))\n    with open(args.config_file, ""r"") as cf:\n        config_str = ""\\n"" + cf.read()\n        logger.info(config_str)\n    logger.info(""Running with config:\\n{}"".format(cfg))\n\n    prepare_imagenet(cfg, logger)\n\n\nif __name__ == \'__main__\':\n    run()\n\n'"
dataset_preparation/prepare_mnist_tfrecords.py,4,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Create a tfrecords for MNIST. """"""\n\nfrom defaults import get_cfg_defaults\nimport sys\nimport logging\nfrom net import *\nimport numpy as np\nimport argparse\nimport os\nimport tensorflow as tf\nimport random\nimport dlutils\n\n\ndef prepare_mnist(cfg, logger, mnist_images, mnist_labels, train):\n    im_size = 32\n\n    if train:\n        mnist_images = mnist_images[:50000]\n        mnist_labels = mnist_labels[:50000]\n    else:\n        mnist_images = mnist_images[50000:]\n        mnist_labels = mnist_labels[50000:]\n\n    mnist_images = F.pad(torch.tensor(mnist_images).view(mnist_images.shape[0], 1, 28, 28), (2, 2, 2, 2)).detach().cpu().numpy()\n    # mnist_images = torch.tensor(mnist_images).view(mnist_images.shape[0], 1, 28, 28).detach().cpu().numpy()\n\n    if train:\n        path = cfg.DATASET.PATH\n    else:\n        path = cfg.DATASET.PATH_TEST\n\n    directory = os.path.dirname(path)\n\n    os.makedirs(directory, exist_ok=True)\n\n    folds = cfg.DATASET.PART_COUNT\n\n    if not train:\n        folds = 1\n\n    mnist_folds = [[] for _ in range(folds)]\n\n    count = len(mnist_images)\n\n    count_per_fold = count // folds\n    for i in range(folds):\n        mnist_folds[i] += (mnist_images[i * count_per_fold: (i + 1) * count_per_fold],\n                           mnist_labels[i * count_per_fold: (i + 1) * count_per_fold])\n\n    for i in range(folds):\n        images = mnist_folds[i][0]\n        labels = mnist_folds[i][1]\n        tfr_opt = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.NONE)\n        part_path = path % (2 + 3, i)\n        tfr_writer = tf.python_io.TFRecordWriter(part_path, tfr_opt)\n\n        for image, label in zip(images, labels):\n            ex = tf.train.Example(features=tf.train.Features(feature={\n                \'shape\': tf.train.Feature(int64_list=tf.train.Int64List(value=image.shape)),\n                \'label\': tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\n                \'data\': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image.tostring()]))}))\n            tfr_writer.write(ex.SerializeToString())\n        tfr_writer.close()\n\n        if train:\n            for j in range(3):\n                images_down = []\n\n                for image, label in zip(images, labels):\n                    h = image.shape[1]\n                    w = image.shape[2]\n                    image = torch.tensor(np.asarray(image, dtype=np.float32)).view(1, 1, h, w)\n\n                    image_down = F.avg_pool2d(image, 2, 2).clamp_(0, 255).to(\'cpu\', torch.uint8)\n\n                    image_down = image_down.view(1, h // 2, w // 2).numpy()\n                    images_down.append(image_down)\n\n                part_path = cfg.DATASET.PATH % (5 - j - 1, i)\n                tfr_writer = tf.python_io.TFRecordWriter(part_path, tfr_opt)\n                for image, label in zip(images_down, labels):\n                    ex = tf.train.Example(features=tf.train.Features(feature={\n                        \'shape\': tf.train.Feature(int64_list=tf.train.Int64List(value=image.shape)),\n                        \'label\': tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\n                        \'data\': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image.tostring()]))}))\n                    tfr_writer.write(ex.SerializeToString())\n                tfr_writer.close()\n\n                images = images_down\n\n\ndef run():\n    parser = argparse.ArgumentParser(description=""ALAE. prepare mnist"")\n    parser.add_argument(\n        ""--config-file"",\n        default=""configs/mnist.yaml"",\n        metavar=""FILE"",\n        help=""path to config file"",\n        type=str,\n    )\n    parser.add_argument(\n        ""opts"",\n        help=""Modify config options using the command-line"",\n        default=None,\n        nargs=argparse.REMAINDER,\n    )\n\n    args = parser.parse_args()\n    cfg = get_cfg_defaults()\n    cfg.merge_from_file(args.config_file)\n    cfg.merge_from_list(args.opts)\n    cfg.freeze()\n\n    logger = logging.getLogger(""logger"")\n    logger.setLevel(logging.DEBUG)\n\n    output_dir = cfg.OUTPUT_DIR\n    os.makedirs(output_dir, exist_ok=True)\n\n    ch = logging.StreamHandler(stream=sys.stdout)\n    ch.setLevel(logging.DEBUG)\n    formatter = logging.Formatter(""%(asctime)s %(name)s %(levelname)s: %(message)s"")\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n    logger.info(args)\n\n    logger.info(""Loaded configuration file {}"".format(args.config_file))\n    with open(args.config_file, ""r"") as cf:\n        config_str = ""\\n"" + cf.read()\n        logger.info(config_str)\n    logger.info(""Running with config:\\n{}"".format(cfg))\n\n    random.seed(0)\n\n    dlutils.download.mnist()\n    mnist = dlutils.reader.Mnist(\'mnist\', train=True, test=False).items\n    random.shuffle(mnist)\n\n    mnist_images = np.stack([x[1] for x in mnist])\n    mnist_labels = np.stack([x[0] for x in mnist])\n\n    prepare_mnist(cfg, logger, mnist_images, mnist_labels, train=False)\n    prepare_mnist(cfg, logger, mnist_images, mnist_labels, train=True)\n\n\nif __name__ == \'__main__\':\n    run()\n'"
dataset_preparation/prepare_svhn_tfrecords.py,4,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Create a tfrecords for SVHN. """"""\n\nfrom defaults import get_cfg_defaults\nimport sys\nimport logging\nfrom net import *\nimport numpy as np\nimport argparse\nimport os\nimport tensorflow as tf\nimport random\nfrom torchvision.datasets.svhn import SVHN\n\n\ndef prepare_mnist(cfg, logger, mnist_images, mnist_labels, train):\n    im_size = 32\n\n    mnist_images = torch.tensor(mnist_images).view(mnist_images.shape[0], 3, 32, 32).detach().cpu().numpy()\n    # mnist_images = torch.tensor(mnist_images).view(mnist_images.shape[0], 1, 28, 28).detach().cpu().numpy()\n\n    if train:\n        path = cfg.DATASET.PATH\n    else:\n        path = cfg.DATASET.PATH_TEST\n\n    directory = os.path.dirname(path)\n\n    os.makedirs(directory, exist_ok=True)\n\n    folds = cfg.DATASET.PART_COUNT\n\n    if not train:\n        folds = 1\n\n    mnist_folds = [[] for _ in range(folds)]\n\n    count = len(mnist_images)\n\n    count_per_fold = count // folds\n    for i in range(folds):\n        mnist_folds[i] += (mnist_images[i * count_per_fold: (i + 1) * count_per_fold],\n                           mnist_labels[i * count_per_fold: (i + 1) * count_per_fold])\n\n    for i in range(folds):\n        images = mnist_folds[i][0]\n        labels = mnist_folds[i][1]\n        tfr_opt = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.NONE)\n        part_path = path % (2 + 3, i)\n        tfr_writer = tf.python_io.TFRecordWriter(part_path, tfr_opt)\n\n        for image, label in zip(images, labels):\n            ex = tf.train.Example(features=tf.train.Features(feature={\n                \'shape\': tf.train.Feature(int64_list=tf.train.Int64List(value=image.shape)),\n                \'label\': tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\n                \'data\': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image.tostring()]))}))\n            tfr_writer.write(ex.SerializeToString())\n        tfr_writer.close()\n\n        if True:\n            for j in range(3):\n                images_down = []\n\n                for image, label in zip(images, labels):\n                    h = image.shape[1]\n                    w = image.shape[2]\n                    image = torch.tensor(np.asarray(image, dtype=np.float32)).view(1, 3, h, w)\n\n                    image_down = F.avg_pool2d(image, 2, 2).clamp_(0, 255).to(\'cpu\', torch.uint8)\n\n                    image_down = image_down.view(3, h // 2, w // 2).numpy()\n                    images_down.append(image_down)\n\n                part_path = cfg.DATASET.PATH % (5 - j - 1, i)\n                tfr_writer = tf.python_io.TFRecordWriter(part_path, tfr_opt)\n                for image, label in zip(images_down, labels):\n                    ex = tf.train.Example(features=tf.train.Features(feature={\n                        \'shape\': tf.train.Feature(int64_list=tf.train.Int64List(value=image.shape)),\n                        \'label\': tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\n                        \'data\': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image.tostring()]))}))\n                    tfr_writer.write(ex.SerializeToString())\n                tfr_writer.close()\n\n                images = images_down\n\n\ndef run():\n    parser = argparse.ArgumentParser(description=""ALAE prepare SVHN"")\n    parser.add_argument(\n        ""--config-file"",\n        default=""configs/svhn.yaml"",\n        metavar=""FILE"",\n        help=""path to config file"",\n        type=str,\n    )\n    parser.add_argument(\n        ""opts"",\n        help=""Modify config options using the command-line"",\n        default=None,\n        nargs=argparse.REMAINDER,\n    )\n\n    args = parser.parse_args()\n    cfg = get_cfg_defaults()\n    cfg.merge_from_file(args.config_file)\n    cfg.merge_from_list(args.opts)\n    cfg.freeze()\n\n    logger = logging.getLogger(""logger"")\n    logger.setLevel(logging.DEBUG)\n\n    output_dir = cfg.OUTPUT_DIR\n    os.makedirs(output_dir, exist_ok=True)\n\n    ch = logging.StreamHandler(stream=sys.stdout)\n    ch.setLevel(logging.DEBUG)\n    formatter = logging.Formatter(""%(asctime)s %(name)s %(levelname)s: %(message)s"")\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n    logger.info(args)\n\n    logger.info(""Loaded configuration file {}"".format(args.config_file))\n    with open(args.config_file, ""r"") as cf:\n        config_str = ""\\n"" + cf.read()\n        logger.info(config_str)\n    logger.info(""Running with config:\\n{}"".format(cfg))\n\n    random.seed(0)\n\n    os.makedirs(""SVHN"", exist_ok=True)\n    train = list(SVHN(\'.\', split=\'train\', download=True))\n    test = list(SVHN(\'.\', split=\'test\', download=True))\n\n    random.shuffle(train)\n\n    svhn_images = np.stack([np.transpose(x[0], (2, 0, 1)) for x in train])\n    svhn_labels = np.stack([x[1] for x in train])\n\n    prepare_mnist(cfg, logger, svhn_images, svhn_labels, train=True)\n\n    svhn_images = np.stack([np.transpose(x[0], (2, 0, 1)) for x in test])\n    svhn_labels = np.stack([x[1] for x in test])\n\n    prepare_mnist(cfg, logger, svhn_images, svhn_labels, train=False)\n\n\nif __name__ == \'__main__\':\n    run()\n\n'"
dataset_preparation/split_tfrecords_bedroom.py,0,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport os\nimport sys\nimport argparse\nimport logging\nimport tensorflow as tf\nfrom defaults import get_cfg_defaults\nimport tqdm\nimport dareblopy as db\nfrom PIL import Image\n\n\ndef split_tfrecord(cfg, logger):\n    tfrecord_path = cfg.DATASET.FFHQ_SOURCE\n\n    ffhq_size = cfg.DATASET.SIZE\n\n    part_size = ffhq_size // cfg.DATASET.PART_COUNT\n\n    logger.info(""Splitting into % size parts"" % part_size)\n\n    chunk_size = 1024\n\n    # # Commented code is for saving out samples of bedroom dataset\n    # with tf.Graph().as_default(), tf.Session() as sess:\n    #     ds = tf.data.TFRecordDataset(tfrecord_path % 8)\n    #     batch = ds.batch(256).make_one_shot_iterator().get_next()\n    #\n    #     features = {\n    #         # \'shape\': db.FixedLenFeature([3], db.int64),\n    #         \'data\': db.FixedLenFeature([3, 256, 256], db.uint8)\n    #     }\n    #     parser = db.RecordParser(features, False)\n    #     try:\n    #         path = \'dataset_samples/bedroom256x256\'\n    #         os.makedirs(path, exist_ok=True)\n    #         records = sess.run(batch)\n    #         k = 0\n    #         for record in records:\n    #             im = parser.parse_single_example(record)[0]\n    #             im = im.transpose((1, 2, 0))\n    #             image = Image.fromarray(im)\n    #             image.save(path + \'/\' + str(k) + "".png"")\n    #             k += 1\n    #\n    #     except tf.errors.OutOfRangeError:\n    #         pass\n\n    for i in range(0, cfg.DATASET.MAX_RESOLUTION_LEVEL + 1):\n        part_num = 0\n        with tf.Graph().as_default(), tf.Session() as sess:\n            ds = tf.data.TFRecordDataset(tfrecord_path % i)\n            batch = ds.batch(chunk_size).make_one_shot_iterator().get_next()\n            while True:\n                try:\n                    part_path = cfg.DATASET.PATH % (i, part_num)\n                    os.makedirs(os.path.dirname(part_path), exist_ok=True)\n                    k = 0\n                    with tf.python_io.TFRecordWriter(part_path) as writer:\n                        for k in tqdm.tqdm(range(part_size // chunk_size)):\n                            records = sess.run(batch)\n                            for record in records:\n                                writer.write(record)\n                    part_num += 1\n                except tf.errors.OutOfRangeError:\n                    break\n\n\ndef run():\n    parser = argparse.ArgumentParser(description=""ALAE. Split LSUN bedroom into parts"")\n    parser.add_argument(\n        ""--config-file"",\n        default=""configs/bedroom.yaml"",\n        metavar=""FILE"",\n        help=""path to config file"",\n        type=str,\n    )\n    parser.add_argument(\n        ""opts"",\n        help=""Modify config options using the command-line"",\n        default=None,\n        nargs=argparse.REMAINDER,\n    )\n\n    args = parser.parse_args()\n    cfg = get_cfg_defaults()\n    cfg.merge_from_file(args.config_file)\n    cfg.merge_from_list(args.opts)\n    cfg.freeze()\n\n    logger = logging.getLogger(""logger"")\n    logger.setLevel(logging.DEBUG)\n\n    output_dir = cfg.OUTPUT_DIR\n    os.makedirs(output_dir, exist_ok=True)\n\n    ch = logging.StreamHandler(stream=sys.stdout)\n    ch.setLevel(logging.DEBUG)\n    formatter = logging.Formatter(""%(asctime)s %(name)s %(levelname)s: %(message)s"")\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n\n    fh = logging.FileHandler(os.path.join(output_dir, \'log.txt\'))\n    fh.setLevel(logging.DEBUG)\n    fh.setFormatter(formatter)\n    logger.addHandler(fh)\n\n    logger.info(args)\n\n    logger.info(""Loaded configuration file {}"".format(args.config_file))\n    with open(args.config_file, ""r"") as cf:\n        config_str = ""\\n"" + cf.read()\n        logger.info(config_str)\n    logger.info(""Running with config:\\n{}"".format(cfg))\n\n    split_tfrecord(cfg, logger)\n\n\nif __name__ == \'__main__\':\n    run()\n\n'"
dataset_preparation/split_tfrecords_ffhq.py,0,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport os\nimport sys\nimport argparse\nimport logging\nimport tensorflow as tf\nfrom defaults import get_cfg_defaults\n\n\ndef split_tfrecord(cfg, logger):\n    tfrecord_path = cfg.DATASET.FFHQ_SOURCE\n\n    ffhq_train_size = 60000\n\n    part_size = ffhq_train_size // cfg.DATASET.PART_COUNT\n\n    logger.info(""Splitting into % size parts"" % part_size)\n\n    for i in range(2, cfg.DATASET.MAX_RESOLUTION_LEVEL + 1):\n        with tf.Graph().as_default(), tf.Session() as sess:\n            ds = tf.data.TFRecordDataset(tfrecord_path % i)\n            ds = ds.batch(part_size)\n            batch = ds.make_one_shot_iterator().get_next()\n            part_num = 0\n            while True:\n                try:\n                    records = sess.run(batch)\n                    if part_num < cfg.DATASET.PART_COUNT:\n                        part_path = cfg.DATASET.PATH % (i, part_num)\n                        os.makedirs(os.path.dirname(part_path), exist_ok=True)\n                        with tf.python_io.TFRecordWriter(part_path) as writer:\n                            for record in records:\n                                writer.write(record)\n                    else:\n                        part_path = cfg.DATASET.PATH_TEST % (i, part_num - cfg.DATASET.PART_COUNT)\n                        os.makedirs(os.path.dirname(part_path), exist_ok=True)\n                        with tf.python_io.TFRecordWriter(part_path) as writer:\n                            for record in records:\n                                writer.write(record)\n                    part_num += 1\n                except tf.errors.OutOfRangeError:\n                    break\n\n\ndef run():\n    parser = argparse.ArgumentParser(description=""ALAE. Split FFHQ into parts for training and testing"")\n    parser.add_argument(\n        ""--config-file"",\n        default=""configs/ffhq.yaml"",\n        metavar=""FILE"",\n        help=""path to config file"",\n        type=str,\n    )\n    parser.add_argument(\n        ""opts"",\n        help=""Modify config options using the command-line"",\n        default=None,\n        nargs=argparse.REMAINDER,\n    )\n\n    args = parser.parse_args()\n    cfg = get_cfg_defaults()\n    cfg.merge_from_file(args.config_file)\n    cfg.merge_from_list(args.opts)\n    cfg.freeze()\n\n    logger = logging.getLogger(""logger"")\n    logger.setLevel(logging.DEBUG)\n\n    output_dir = cfg.OUTPUT_DIR\n    os.makedirs(output_dir, exist_ok=True)\n\n    ch = logging.StreamHandler(stream=sys.stdout)\n    ch.setLevel(logging.DEBUG)\n    formatter = logging.Formatter(""%(asctime)s %(name)s %(levelname)s: %(message)s"")\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n\n    fh = logging.FileHandler(os.path.join(output_dir, \'log.txt\'))\n    fh.setLevel(logging.DEBUG)\n    fh.setFormatter(formatter)\n    logger.addHandler(fh)\n\n    logger.info(args)\n\n    logger.info(""Loaded configuration file {}"".format(args.config_file))\n    with open(args.config_file, ""r"") as cf:\n        config_str = ""\\n"" + cf.read()\n        logger.info(config_str)\n    logger.info(""Running with config:\\n{}"".format(cfg))\n\n    split_tfrecord(cfg, logger)\n\n\nif __name__ == \'__main__\':\n    run()\n\n'"
make_figures/make_generation_figure.py,3,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\nfrom net import *\nfrom model import Model\nfrom launcher import run\nfrom dataloader import *\nfrom checkpointer import Checkpointer\nfrom dlutils.pytorch import count_parameters\nfrom defaults import get_cfg_defaults\nfrom PIL import Image\nimport PIL\n\n\ndef draw_uncurated_result_figure(cfg, png, model, cx, cy, cw, ch, rows, lods, seed):\n    print(png)\n    N = sum(rows * 2**lod for lod in lods)\n    images = []\n\n    rnd = np.random.RandomState(5)\n    for i in range(N):\n        latents = rnd.randn(1, cfg.MODEL.LATENT_SPACE_SIZE)\n        samplez = torch.tensor(latents).float().cuda()\n        image = model.generate(cfg.DATASET.MAX_RESOLUTION_LEVEL-2, 1, samplez, 1, mixing=True)\n        images.append(image[0])\n\n    canvas = PIL.Image.new(\'RGB\', (sum(cw // 2**lod for lod in lods), ch * rows), \'white\')\n    image_iter = iter(list(images))\n    for col, lod in enumerate(lods):\n        for row in range(rows * 2**lod):\n            im = next(image_iter).cpu().numpy()\n            im = im.transpose(1, 2, 0)\n            im = im * 0.5 + 0.5\n            image = PIL.Image.fromarray(np.clip(im * 255, 0, 255).astype(np.uint8), \'RGB\')\n            image = image.crop((cx, cy, cx + cw, cy + ch))\n            image = image.resize((cw // 2**lod, ch // 2**lod), PIL.Image.ANTIALIAS)\n            canvas.paste(image, (sum(cw // 2**lod for lod in lods[:col]), row * ch // 2**lod))\n    canvas.save(png)\n\n\ndef sample(cfg, logger):\n    torch.cuda.set_device(0)\n    model = Model(\n        startf=cfg.MODEL.START_CHANNEL_COUNT,\n        layer_count=cfg.MODEL.LAYER_COUNT,\n        maxf=cfg.MODEL.MAX_CHANNEL_COUNT,\n        latent_size=cfg.MODEL.LATENT_SPACE_SIZE,\n        truncation_psi=cfg.MODEL.TRUNCATIOM_PSI,\n        truncation_cutoff=cfg.MODEL.TRUNCATIOM_CUTOFF,\n        style_mixing_prob=cfg.MODEL.STYLE_MIXING_PROB,\n        mapping_layers=cfg.MODEL.MAPPING_LAYERS,\n        channels=cfg.MODEL.CHANNELS,\n        generator=cfg.MODEL.GENERATOR,\n        encoder=cfg.MODEL.ENCODER)\n\n    model.cuda(0)\n    model.eval()\n    model.requires_grad_(False)\n\n    decoder = model.decoder\n    encoder = model.encoder\n    mapping_tl = model.mapping_tl\n    mapping_fl = model.mapping_fl\n\n    dlatent_avg = model.dlatent_avg\n\n    logger.info(""Trainable parameters generator:"")\n    count_parameters(decoder)\n\n    logger.info(""Trainable parameters discriminator:"")\n    count_parameters(encoder)\n\n    arguments = dict()\n    arguments[""iteration""] = 0\n\n    model_dict = {\n        \'discriminator_s\': encoder,\n        \'generator_s\': decoder,\n        \'mapping_tl_s\': mapping_tl,\n        \'mapping_fl_s\': mapping_fl,\n        \'dlatent_avg\': dlatent_avg\n    }\n\n    checkpointer = Checkpointer(cfg,\n                                model_dict,\n                                {},\n                                logger=logger,\n                                save=False)\n\n    checkpointer.load()\n\n    model.eval()\n\n    layer_count = cfg.MODEL.LAYER_COUNT\n\n    decoder = nn.DataParallel(decoder)\n\n    im_size = 2 ** (cfg.MODEL.LAYER_COUNT + 1)\n    with torch.no_grad():\n        draw_uncurated_result_figure(cfg, \'make_figures/output/%s/generations.jpg\' % cfg.NAME,\n                                     model, cx=0, cy=0, cw=im_size, ch=im_size, rows=6, lods=[0, 0, 0, 1, 1, 2], seed=5)\n\n\nif __name__ == ""__main__"":\n    gpu_count = 1\n    run(sample, get_cfg_defaults(), description=\'ALAE-generations\', default_config=\'configs/ffhq.yaml\',\n        world_size=gpu_count, write_log=False)\n'"
make_figures/make_recon_figure_celeba_pioneer.py,10,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport torch.utils.data\nfrom torchvision.utils import save_image\nimport random\nfrom net import *\nfrom model import Model\nfrom launcher import run\nfrom checkpointer import Checkpointer\nfrom dlutils.pytorch import count_parameters\nfrom defaults import get_cfg_defaults\nimport lreq\nfrom PIL import Image\n\n\nlreq.use_implicit_lreq.set(True)\n\nim_size = 256\n\n\ndef place(canvas, image, x, y):\n    im_size = image.shape[2]\n    if len(image.shape) == 4:\n        image = image[0]\n    canvas[:, y: y + im_size, x: x + im_size] = image * 0.5 + 0.5\n\n\ndef save_sample(model, sample, i):\n    os.makedirs(\'results\', exist_ok=True)\n\n    with torch.no_grad():\n        model.eval()\n        x_rec = model.generate(model.generator.layer_count - 1, 1, z=sample)\n\n        def save_pic(x_rec):\n            resultsample = x_rec * 0.5 + 0.5\n            resultsample = resultsample.cpu()\n            save_image(resultsample,\n                       \'sample_%i_lr.png\' % i, nrow=16)\n\n        save_pic(x_rec)\n\n\ndef sample(cfg, logger):\n    torch.cuda.set_device(0)\n    model = Model(\n        startf=cfg.MODEL.START_CHANNEL_COUNT,\n        layer_count=cfg.MODEL.LAYER_COUNT,\n        maxf=cfg.MODEL.MAX_CHANNEL_COUNT,\n        latent_size=cfg.MODEL.LATENT_SPACE_SIZE,\n        truncation_psi=cfg.MODEL.TRUNCATIOM_PSI,\n        truncation_cutoff=cfg.MODEL.TRUNCATIOM_CUTOFF,\n        mapping_layers=cfg.MODEL.MAPPING_LAYERS,\n        channels=cfg.MODEL.CHANNELS,\n        generator=cfg.MODEL.GENERATOR,\n        encoder=cfg.MODEL.ENCODER)\n    model.cuda(0)\n    model.eval()\n    model.requires_grad_(False)\n\n    decoder = model.decoder\n    encoder = model.encoder\n    mapping_tl = model.mapping_tl\n    mapping_fl = model.mapping_fl\n    dlatent_avg = model.dlatent_avg\n\n    logger.info(""Trainable parameters generator:"")\n    count_parameters(decoder)\n\n    logger.info(""Trainable parameters discriminator:"")\n    count_parameters(encoder)\n\n    arguments = dict()\n    arguments[""iteration""] = 0\n\n    model_dict = {\n        \'discriminator_s\': encoder,\n        \'generator_s\': decoder,\n        \'mapping_tl_s\': mapping_tl,\n        \'mapping_fl_s\': mapping_fl,\n        \'dlatent_avg\': dlatent_avg\n    }\n\n    checkpointer = Checkpointer(cfg,\n                                model_dict,\n                                {},\n                                logger=logger,\n                                save=False)\n\n    extra_checkpoint_data = checkpointer.load()\n\n    model.eval()\n\n    layer_count = cfg.MODEL.LAYER_COUNT\n\n    def encode(x):\n        Z, _ = model.encode(x, layer_count - 1, 1)\n        Z = Z.repeat(1, model.mapping_fl.num_layers, 1)\n        return Z\n\n    def decode(x):\n        layer_idx = torch.arange(2 * cfg.MODEL.LAYER_COUNT)[np.newaxis, :, np.newaxis]\n        ones = torch.ones(layer_idx.shape, dtype=torch.float32)\n        coefs = torch.where(layer_idx < model.truncation_cutoff, ones, ones)\n        # x = torch.lerp(model.dlatent_avg.buff.data, x, coefs)\n        return model.decoder(x, layer_count - 1, 1, noise=True)\n\n    path = \'dataset_samples/faces/pioneer256x256\'\n\n    paths = list(os.listdir(path))\n\n    def make(paths):\n        with torch.no_grad():\n            for filename in paths:\n                img = np.asarray(Image.open(path + \'/\' + filename))\n                if img.shape[2] == 4:\n                    img = img[:, :, :3]\n                im = img.transpose((2, 0, 1))\n\n                x = torch.tensor(np.asarray(im, dtype=np.float32), device=\'cpu\', requires_grad=True).cuda() / 127.5 - 1.\n                if x.shape[0] == 4:\n                    x = x[:3]\n\n                while x.shape[2] != model.decoder.layer_to_resolution[6]:\n                    x = F.avg_pool2d(x, 2, 2)\n\n                latents = encode(x[None, ...].cuda())\n                f = decode(latents)\n                r = torch.cat([x[None, ...].detach().cpu(), f.detach().cpu()], dim=3)\n                os.makedirs(\'make_figures/output/pioneer/\', exist_ok=True)\n                save_image(f.detach().cpu() * 0.5 + 0.5, \'make_figures/output/pioneer/%s_alae.png\' % filename[:-9], nrow=1, pad_value=1.0)\n\n    make(paths)\n\n\nif __name__ == ""__main__"":\n    gpu_count = 1\n    run(sample, get_cfg_defaults(), description=\'ALAE-reconstructions-celeb-hq256-on-pioneer-examples\',\n        default_config=\'configs/celeba-hq256.yaml\',\n        world_size=gpu_count, write_log=False)\n'"
make_figures/make_recon_figure_ffhq_real.py,12,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport torch.utils.data\nfrom torchvision.utils import save_image\nfrom net import *\nfrom model import Model\nfrom launcher import run\nfrom checkpointer import Checkpointer\nfrom dlutils.pytorch import count_parameters\nfrom defaults import get_cfg_defaults\nimport lreq\nfrom dataloader import *\n\nlreq.use_implicit_lreq.set(True)\n\n\ndef place(canvas, image, x, y):\n    im_size = image.shape[2]\n    if len(image.shape) == 4:\n        image = image[0]\n    canvas[:, y: y + im_size, x: x + im_size] = image * 0.5 + 0.5\n\n\ndef save_sample(model, sample, i):\n    os.makedirs(\'results\', exist_ok=True)\n\n    with torch.no_grad():\n        model.eval()\n        x_rec = model.generate(model.generator.layer_count - 1, 1, z=sample)\n\n        def save_pic(x_rec):\n            resultsample = x_rec * 0.5 + 0.5\n            resultsample = resultsample.cpu()\n            save_image(resultsample,\n                       \'sample_%i_lr.png\' % i, nrow=16)\n\n        save_pic(x_rec)\n\n\ndef sample(cfg, logger):\n    torch.cuda.set_device(0)\n    model = Model(\n        startf=cfg.MODEL.START_CHANNEL_COUNT,\n        layer_count=cfg.MODEL.LAYER_COUNT,\n        maxf=cfg.MODEL.MAX_CHANNEL_COUNT,\n        latent_size=cfg.MODEL.LATENT_SPACE_SIZE,\n        truncation_psi=cfg.MODEL.TRUNCATIOM_PSI,\n        truncation_cutoff=cfg.MODEL.TRUNCATIOM_CUTOFF,\n        mapping_layers=cfg.MODEL.MAPPING_LAYERS,\n        channels=cfg.MODEL.CHANNELS,\n        generator=cfg.MODEL.GENERATOR,\n        encoder=cfg.MODEL.ENCODER)\n    model.cuda(0)\n    model.eval()\n    model.requires_grad_(False)\n\n    decoder = model.decoder\n    encoder = model.encoder\n    mapping_tl = model.mapping_tl\n    mapping_fl = model.mapping_fl\n    dlatent_avg = model.dlatent_avg\n\n    logger.info(""Trainable parameters generator:"")\n    count_parameters(decoder)\n\n    logger.info(""Trainable parameters discriminator:"")\n    count_parameters(encoder)\n\n    arguments = dict()\n    arguments[""iteration""] = 0\n\n    model_dict = {\n        \'discriminator_s\': encoder,\n        \'generator_s\': decoder,\n        \'mapping_tl_s\': mapping_tl,\n        \'mapping_fl_s\': mapping_fl,\n        \'dlatent_avg\': dlatent_avg\n    }\n\n    checkpointer = Checkpointer(cfg,\n                                model_dict,\n                                {},\n                                logger=logger,\n                                save=False)\n\n    extra_checkpoint_data = checkpointer.load()\n\n    model.eval()\n\n    layer_count = cfg.MODEL.LAYER_COUNT\n\n    def encode(x):\n        Z, _ = model.encode(x, layer_count - 1, 1)\n        Z = Z.repeat(1, model.mapping_fl.num_layers, 1)\n        return Z\n\n    def decode(x):\n        layer_idx = torch.arange(2 * cfg.MODEL.LAYER_COUNT)[np.newaxis, :, np.newaxis]\n        ones = torch.ones(layer_idx.shape, dtype=torch.float32)\n        coefs = torch.where(layer_idx < model.truncation_cutoff, ones, ones)\n        # x = torch.lerp(model.dlatent_avg.buff.data, x, coefs)\n        return model.decoder(x, layer_count - 1, 1, noise=True)\n\n    rnd = np.random.RandomState(5)\n    latents = rnd.randn(1, cfg.MODEL.LATENT_SPACE_SIZE)\n\n    dataset = TFRecordsDataset(cfg, logger, rank=0, world_size=1, buffer_size_mb=10, channels=cfg.MODEL.CHANNELS, train=False)\n\n    dataset.reset(cfg.DATASET.MAX_RESOLUTION_LEVEL, 10)\n    b = iter(make_dataloader(cfg, logger, dataset, 10, 0, numpy=True))\n\n    def make(sample):\n        canvas = []\n        with torch.no_grad():\n            for img in sample:\n                x = torch.tensor(np.asarray(img, dtype=np.float32), device=\'cpu\', requires_grad=True).cuda() / 127.5 - 1.\n                if x.shape[0] == 4:\n                    x = x[:3]\n                latents = encode(x[None, ...].cuda())\n                f = decode(latents)\n                r = torch.cat([x[None, ...].detach().cpu(), f.detach().cpu()], dim=3)\n                canvas.append(r)\n        return canvas\n\n    sample = next(b)\n    canvas = make(sample)\n    canvas = torch.cat(canvas, dim=0)\n\n    save_image(canvas * 0.5 + 0.5, \'make_figures/reconstructions_ffhq_real_1.png\', nrow=2, pad_value=1.0)\n\n    sample = next(b)\n    canvas = make(sample)\n    canvas = torch.cat(canvas, dim=0)\n\n    save_image(canvas * 0.5 + 0.5, \'make_figures/reconstructions_ffhq_real_2.png\', nrow=2, pad_value=1.0)\n\n\nif __name__ == ""__main__"":\n    gpu_count = 1\n    run(sample, get_cfg_defaults(), description=\'ALAE-reconstruction-ffhq\', default_config=\'configs/ffhq.yaml\',\n        world_size=gpu_count, write_log=False)\n'"
make_figures/make_recon_figure_interpolation.py,11,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport torch.utils.data\nfrom torchvision.utils import save_image\nfrom net import *\nfrom model import Model\nfrom launcher import run\nfrom checkpointer import Checkpointer\nfrom dlutils.pytorch import count_parameters\nfrom defaults import get_cfg_defaults\nimport lreq\nfrom PIL import Image\n\n\nlreq.use_implicit_lreq.set(True)\n\n\ndef place(canvas, image, x, y):\n    im_size = image.shape[2]\n    if len(image.shape) == 4:\n        image = image[0]\n    canvas[:, y: y + im_size, x: x + im_size] = image * 0.5 + 0.5\n\n\ndef save_sample(model, sample, i):\n    os.makedirs(\'results\', exist_ok=True)\n\n    with torch.no_grad():\n        model.eval()\n        x_rec = model.generate(model.generator.layer_count - 1, 1, z=sample)\n\n        def save_pic(x_rec):\n            resultsample = x_rec * 0.5 + 0.5\n            resultsample = resultsample.cpu()\n            save_image(resultsample,\n                       \'sample_%i_lr.png\' % i, nrow=16)\n\n        save_pic(x_rec)\n\n\ndef sample(cfg, logger):\n    torch.cuda.set_device(0)\n    model = Model(\n        startf=cfg.MODEL.START_CHANNEL_COUNT,\n        layer_count=cfg.MODEL.LAYER_COUNT,\n        maxf=cfg.MODEL.MAX_CHANNEL_COUNT,\n        latent_size=cfg.MODEL.LATENT_SPACE_SIZE,\n        truncation_psi=cfg.MODEL.TRUNCATIOM_PSI,\n        truncation_cutoff=cfg.MODEL.TRUNCATIOM_CUTOFF,\n        mapping_layers=cfg.MODEL.MAPPING_LAYERS,\n        channels=cfg.MODEL.CHANNELS,\n        generator=cfg.MODEL.GENERATOR,\n        encoder=cfg.MODEL.ENCODER)\n    model.cuda(0)\n    model.eval()\n    model.requires_grad_(False)\n\n    decoder = model.decoder\n    encoder = model.encoder\n    mapping_tl = model.mapping_tl\n    mapping_fl = model.mapping_fl\n    dlatent_avg = model.dlatent_avg\n\n    logger.info(""Trainable parameters generator:"")\n    count_parameters(decoder)\n\n    logger.info(""Trainable parameters discriminator:"")\n    count_parameters(encoder)\n\n    arguments = dict()\n    arguments[""iteration""] = 0\n\n    model_dict = {\n        \'discriminator_s\': encoder,\n        \'generator_s\': decoder,\n        \'mapping_tl_s\': mapping_tl,\n        \'mapping_fl_s\': mapping_fl,\n        \'dlatent_avg\': dlatent_avg\n    }\n\n    checkpointer = Checkpointer(cfg,\n                                model_dict,\n                                {},\n                                logger=logger,\n                                save=False)\n\n    extra_checkpoint_data = checkpointer.load()\n\n    model.eval()\n\n    layer_count = cfg.MODEL.LAYER_COUNT\n\n    def encode(x):\n        Z, _ = model.encode(x, layer_count - 1, 1)\n        Z = Z.repeat(1, model.mapping_fl.num_layers, 1)\n        return Z\n\n    def decode(x):\n        layer_idx = torch.arange(2 * cfg.MODEL.LAYER_COUNT)[np.newaxis, :, np.newaxis]\n        ones = torch.ones(layer_idx.shape, dtype=torch.float32)\n        coefs = torch.where(layer_idx < model.truncation_cutoff, ones, ones)\n        # x = torch.lerp(model.dlatent_avg.buff.data, x, coefs)\n        return model.decoder(x, layer_count - 1, 1, noise=True)\n\n    rnd = np.random.RandomState(4)\n    latents = rnd.randn(1, cfg.MODEL.LATENT_SPACE_SIZE)\n\n    path = cfg.DATASET.SAMPLES_PATH\n    im_size = 2 ** (cfg.MODEL.LAYER_COUNT + 1)\n\n    pathA = \'00001.png\'\n    pathB = \'00022.png\'\n    pathC = \'00077.png\'\n    pathD = \'00016.png\'\n\n    def open_image(filename):\n        img = np.asarray(Image.open(path + \'/\' + filename))\n        if img.shape[2] == 4:\n            img = img[:, :, :3]\n        im = img.transpose((2, 0, 1))\n        x = torch.tensor(np.asarray(im, dtype=np.float32), device=\'cpu\', requires_grad=True).cuda() / 127.5 - 1.\n        if x.shape[0] == 4:\n            x = x[:3]\n        factor = x.shape[2] // im_size\n        if factor != 1:\n            x = torch.nn.functional.avg_pool2d(x[None, ...], factor, factor)[0]\n        assert x.shape[2] == im_size\n        _latents = encode(x[None, ...].cuda())\n        latents = _latents[0, 0]\n        return latents\n\n    def make(w):\n        with torch.no_grad():\n            w = w[None, None, ...].repeat(1, model.mapping_fl.num_layers, 1)\n            x_rec = decode(w)\n            return x_rec\n\n    wa = open_image(pathA)\n    wb = open_image(pathB)\n    wc = open_image(pathC)\n    wd = open_image(pathD)\n\n    height = 7\n    width = 7\n\n    images = []\n\n    for i in range(height):\n        for j in range(width):\n            kv = i / (height - 1.0)\n            kh = j / (width - 1.0)\n\n            ka = (1.0 - kh) * (1.0 - kv)\n            kb = kh * (1.0 - kv)\n            kc = (1.0 - kh) * kv\n            kd = kh * kv\n\n            w = ka * wa + kb * wb + kc * wc + kd * wd\n\n            interpolated = make(w)\n            images.append(interpolated)\n\n    images = torch.cat(images)\n\n    save_image(images * 0.5 + 0.5, \'make_figures/output/%s/interpolations.png\' % cfg.NAME, nrow=width)\n    save_image(images * 0.5 + 0.5, \'make_figures/output/%s/interpolations.jpg\' % cfg.NAME, nrow=width)\n\n\nif __name__ == ""__main__"":\n    gpu_count = 1\n    run(sample, get_cfg_defaults(), description=\'ALAE-interpolations\', default_config=\'configs/ffhq.yaml\',\n        world_size=gpu_count, write_log=False)\n'"
make_figures/make_recon_figure_multires.py,11,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport torch.utils.data\nfrom torchvision.utils import save_image\nimport random\nfrom net import *\nfrom model import Model\nfrom launcher import run\nfrom checkpointer import Checkpointer\nfrom dlutils.pytorch import count_parameters\nfrom defaults import get_cfg_defaults\nimport lreq\nfrom skimage.transform import resize\nfrom PIL import Image\n\nlreq.use_implicit_lreq.set(True)\n\n\ndef place(canvas, image, x, y):\n    im_size = image.shape[2]\n    if len(image.shape) == 4:\n        image = image[0]\n    canvas[:, y: y + im_size, x: x + im_size] = image * 0.5 + 0.5\n\n\ndef save_sample(model, sample, i):\n    os.makedirs(\'results\', exist_ok=True)\n\n    with torch.no_grad():\n        model.eval()\n        x_rec = model.generate(model.generator.layer_count - 1, 1, z=sample)\n\n        def save_pic(x_rec):\n            resultsample = x_rec * 0.5 + 0.5\n            resultsample = resultsample.cpu()\n            save_image(resultsample,\n                       \'sample_%i_lr.png\' % i, nrow=16)\n\n        save_pic(x_rec)\n\n\ndef sample(cfg, logger):\n    torch.cuda.set_device(0)\n    model = Model(\n        startf=cfg.MODEL.START_CHANNEL_COUNT,\n        layer_count=cfg.MODEL.LAYER_COUNT,\n        maxf=cfg.MODEL.MAX_CHANNEL_COUNT,\n        latent_size=cfg.MODEL.LATENT_SPACE_SIZE,\n        truncation_psi=cfg.MODEL.TRUNCATIOM_PSI,\n        truncation_cutoff=cfg.MODEL.TRUNCATIOM_CUTOFF,\n        mapping_layers=cfg.MODEL.MAPPING_LAYERS,\n        channels=cfg.MODEL.CHANNELS,\n        generator=cfg.MODEL.GENERATOR,\n        encoder=cfg.MODEL.ENCODER)\n    model.cuda(0)\n    model.eval()\n    model.requires_grad_(False)\n\n    decoder = model.decoder\n    encoder = model.encoder\n    mapping_tl = model.mapping_tl\n    mapping_fl = model.mapping_fl\n    dlatent_avg = model.dlatent_avg\n\n    logger.info(""Trainable parameters generator:"")\n    count_parameters(decoder)\n\n    logger.info(""Trainable parameters discriminator:"")\n    count_parameters(encoder)\n\n    arguments = dict()\n    arguments[""iteration""] = 0\n\n    model_dict = {\n        \'discriminator_s\': encoder,\n        \'generator_s\': decoder,\n        \'mapping_tl_s\': mapping_tl,\n        \'mapping_fl_s\': mapping_fl,\n        \'dlatent_avg\': dlatent_avg\n    }\n\n    checkpointer = Checkpointer(cfg,\n                                model_dict,\n                                {},\n                                logger=logger,\n                                save=False)\n\n    extra_checkpoint_data = checkpointer.load()\n\n    model.eval()\n\n    layer_count = cfg.MODEL.LAYER_COUNT\n\n    def encode(x):\n        Z, _ = model.encode(x, layer_count - 1, 1)\n        Z = Z.repeat(1, model.mapping_fl.num_layers, 1)\n        return Z\n\n    def decode(x):\n        layer_idx = torch.arange(2 * cfg.MODEL.LAYER_COUNT)[np.newaxis, :, np.newaxis]\n        ones = torch.ones(layer_idx.shape, dtype=torch.float32)\n        coefs = torch.where(layer_idx < model.truncation_cutoff, 1.0 * ones, ones)\n        # x = torch.lerp(model.dlatent_avg.buff.data, x, coefs)\n        return model.decoder(x, layer_count - 1, 1, noise=True)\n\n    path = cfg.DATASET.SAMPLES_PATH\n    # path = \'dataset_samples/faces/realign1024x1024_paper\'\n\n    im_size = 2 ** (cfg.MODEL.LAYER_COUNT + 1)\n\n    paths = list(os.listdir(path))\n\n    paths = sorted(paths)\n    random.seed(5)\n    random.shuffle(paths)\n\n    def move_to(list, item, new_index):\n        list.remove(item)\n        list.insert(new_index, item)\n\n    # move_to(paths, \'00026.png\', 0)\n    # move_to(paths, \'00074.png\', 1)\n    # move_to(paths, \'00134.png\', 2)\n    # move_to(paths, \'00036.png\', 3)\n\n    def make(paths):\n        src = []\n        for filename in paths:\n            img = np.asarray(Image.open(path + \'/\' + filename))\n            if img.shape[2] == 4:\n                img = img[:, :, :3]\n            im = img.transpose((2, 0, 1))\n            x = torch.tensor(np.asarray(im, dtype=np.float32), requires_grad=True).cuda() / 127.5 - 1.\n            if x.shape[0] == 4:\n                x = x[:3]\n            factor = x.shape[2] // im_size\n            if factor != 1:\n                x = torch.nn.functional.avg_pool2d(x[None, ...], factor, factor)[0]\n            assert x.shape[2] == im_size\n            src.append(x)\n\n        with torch.no_grad():\n            reconstructions = []\n            for s in src:\n                latents = encode(s[None, ...])\n                reconstructions.append(decode(latents).cpu().detach().numpy())\n        return src, reconstructions\n\n    def chunker_list(seq, size):\n        return list((seq[i::size] for i in range(size)))\n\n    final = chunker_list(paths, 4)\n    path0, path1, path2, path3 = final\n\n    path0.reverse()\n    path1.reverse()\n    path2.reverse()\n    path3.reverse()\n\n    src0, rec0 = make(path0)\n    src1, rec1 = make(path1)\n    src2, rec2 = make(path2)\n    src3, rec3 = make(path3)\n\n    initial_resolution = im_size\n\n    lods_down = 1\n    padding_step = 4\n\n    width = 0\n    height = 0\n\n    current_padding = 0\n\n    final_resolution = initial_resolution\n    for _ in range(lods_down):\n        final_resolution /= 2\n\n    for i in range(lods_down + 1):\n        width += current_padding * 2 ** (lods_down - i)\n        height += current_padding * 2 ** (lods_down - i)\n        current_padding += padding_step\n\n    width += 2 ** (lods_down + 1) * final_resolution\n    height += (lods_down + 1) * initial_resolution\n\n    width = int(width)\n    height = int(height)\n\n    def make_part(current_padding, src, rec):\n        canvas = np.ones([3, height + 20, width + 10])\n\n        padd = 0\n\n        initial_padding = current_padding\n\n        height_padding = 0\n\n        for i in range(lods_down + 1):\n            for x in range(2 ** i):\n                for y in range(2 ** i):\n                    try:\n                        ims = src.pop()\n                        imr = rec.pop()[0]\n                        ims = ims.cpu().detach().numpy()\n                        imr = imr\n\n                        res = int(initial_resolution / 2 ** i)\n\n                        ims = resize(ims, (3, initial_resolution / 2 ** i, initial_resolution / 2 ** i))\n                        imr = resize(imr, (3, initial_resolution / 2 ** i, initial_resolution / 2 ** i))\n\n                        place(canvas, ims,\n                              current_padding + x * (2 * res + current_padding),\n                              i * initial_resolution + height_padding + y * (res + current_padding))\n\n                        place(canvas, imr,\n                              current_padding + res + x * (2 * res + current_padding),\n                              i * initial_resolution + height_padding + y * (res + current_padding))\n\n                    except IndexError:\n                        return canvas\n\n            height_padding += initial_padding * 2\n\n            current_padding -= padding_step\n            padd += padding_step\n        return canvas\n\n    canvas = [make_part(current_padding, src0, rec0), make_part(current_padding, src1, rec1),\n              make_part(current_padding, src2, rec2), make_part(current_padding, src3, rec3)]\n\n    canvas = np.concatenate(canvas, axis=2)\n\n    print(\'Saving image\')\n    save_path = \'make_figures/output/%s/reconstructions_multiresolution.png\' % cfg.NAME\n    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n    save_image(torch.Tensor(canvas), save_path)\n\n\nif __name__ == ""__main__"":\n    gpu_count = 1\n    run(sample, get_cfg_defaults(), description=\'ALAE-reconstruction_figure\', default_config=\'configs/ffhq.yaml\',\n        world_size=gpu_count, write_log=False)\n'"
make_figures/make_recon_figure_paged.py,12,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport torch.utils.data\nfrom torchvision.utils import save_image\nimport random\nfrom net import *\nfrom model import Model\nfrom launcher import run\nfrom checkpointer import Checkpointer\nfrom dlutils.pytorch import count_parameters\nfrom defaults import get_cfg_defaults\nimport lreq\nimport tqdm\nfrom PIL import Image\n\n\nlreq.use_implicit_lreq.set(True)\n\n\ndef place(canvas, image, x, y):\n    im_size = image.shape[2]\n    if len(image.shape) == 4:\n        image = image[0]\n    canvas[:, y: y + im_size, x: x + im_size] = image * 0.5 + 0.5\n\n\ndef save_sample(model, sample, i):\n    os.makedirs(\'results\', exist_ok=True)\n\n    with torch.no_grad():\n        model.eval()\n        x_rec = model.generate(model.generator.layer_count - 1, 1, z=sample)\n\n        def save_pic(x_rec):\n            resultsample = x_rec * 0.5 + 0.5\n            resultsample = resultsample.cpu()\n            save_image(resultsample,\n                       \'sample_%i_lr.png\' % i, nrow=16)\n\n        save_pic(x_rec)\n\n\ndef sample(cfg, logger):\n    torch.cuda.set_device(0)\n    model = Model(\n        startf=cfg.MODEL.START_CHANNEL_COUNT,\n        layer_count=cfg.MODEL.LAYER_COUNT,\n        maxf=cfg.MODEL.MAX_CHANNEL_COUNT,\n        latent_size=cfg.MODEL.LATENT_SPACE_SIZE,\n        truncation_psi=cfg.MODEL.TRUNCATIOM_PSI,\n        truncation_cutoff=cfg.MODEL.TRUNCATIOM_CUTOFF,\n        mapping_layers=cfg.MODEL.MAPPING_LAYERS,\n        channels=cfg.MODEL.CHANNELS,\n        generator=cfg.MODEL.GENERATOR,\n        encoder=cfg.MODEL.ENCODER)\n    model.cuda(0)\n    model.eval()\n    model.requires_grad_(False)\n\n    decoder = model.decoder\n    encoder = model.encoder\n    mapping_tl = model.mapping_tl\n    mapping_fl = model.mapping_fl\n    dlatent_avg = model.dlatent_avg\n\n    logger.info(""Trainable parameters generator:"")\n    count_parameters(decoder)\n\n    logger.info(""Trainable parameters discriminator:"")\n    count_parameters(encoder)\n\n    arguments = dict()\n    arguments[""iteration""] = 0\n\n    model_dict = {\n        \'discriminator_s\': encoder,\n        \'generator_s\': decoder,\n        \'mapping_tl_s\': mapping_tl,\n        \'mapping_fl_s\': mapping_fl,\n        \'dlatent_avg\': dlatent_avg\n    }\n\n    checkpointer = Checkpointer(cfg,\n                                model_dict,\n                                {},\n                                logger=logger,\n                                save=False)\n\n    extra_checkpoint_data = checkpointer.load()\n\n    model.eval()\n\n    layer_count = cfg.MODEL.LAYER_COUNT\n\n    def encode(x):\n        Z, _ = model.encode(x, layer_count - 1, 1)\n        Z = Z.repeat(1, model.mapping_fl.num_layers, 1)\n        return Z\n\n    def decode(x):\n        layer_idx = torch.arange(2 * cfg.MODEL.LAYER_COUNT)[np.newaxis, :, np.newaxis]\n        ones = torch.ones(layer_idx.shape, dtype=torch.float32)\n        coefs = torch.where(layer_idx < model.truncation_cutoff, ones, ones)\n        # x = torch.lerp(model.dlatent_avg.buff.data, x, coefs)\n        return model.decoder(x, layer_count - 1, 1, noise=True)\n\n    path = cfg.DATASET.SAMPLES_PATH\n    im_size = 2 ** (cfg.MODEL.LAYER_COUNT + 1)\n\n    paths = list(os.listdir(path))\n\n    paths = sorted(paths)\n    random.seed(1)\n    random.shuffle(paths)\n\n    def make(paths):\n        canvas = []\n        with torch.no_grad():\n            for filename in paths:\n                img = np.asarray(Image.open(path + \'/\' + filename))\n                if img.shape[2] == 4:\n                    img = img[:, :, :3]\n                im = img.transpose((2, 0, 1))\n                x = torch.tensor(np.asarray(im, dtype=np.float32), device=\'cpu\', requires_grad=True).cuda() / 127.5 - 1.\n                if x.shape[0] == 4:\n                    x = x[:3]\n                factor = x.shape[2] // im_size\n                if factor != 1:\n                    x = torch.nn.functional.avg_pool2d(x[None, ...], factor, factor)[0]\n                assert x.shape[2] == im_size\n                latents = encode(x[None, ...].cuda())\n                f = decode(latents)\n                r = torch.cat([x[None, ...].detach().cpu(), f.detach().cpu()], dim=3)\n                canvas.append(r)\n        return canvas\n\n    def chunker_list(seq, n):\n        return [seq[i * n:(i + 1) * n] for i in range((len(seq) + n - 1) // n)]\n\n    paths = chunker_list(paths, 8 * 3)\n\n    for i, chunk in enumerate(paths):\n        canvas = make(chunk)\n        canvas = torch.cat(canvas, dim=0)\n\n        save_path = \'make_figures/output/%s/reconstructions_%d.png\' % (cfg.NAME, i)\n        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n        save_image(canvas * 0.5 + 0.5, save_path,\n                   nrow=3,\n                   pad_value=1.0)\n\n\nif __name__ == ""__main__"":\n    gpu_count = 1\n    run(sample, get_cfg_defaults(), description=\'ALAE-figure-reconstructions-paged\', default_config=\'configs/ffhq.yaml\',\n        world_size=gpu_count, write_log=False)\n'"
make_figures/make_traversarls.py,13,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport torch.utils.data\nfrom torchvision.utils import save_image\nfrom net import *\nfrom model import Model\nfrom launcher import run\nfrom checkpointer import Checkpointer\nfrom dlutils.pytorch import count_parameters\nfrom defaults import get_cfg_defaults\nimport lreq\nfrom PIL import Image\nimport random\n\n\nlreq.use_implicit_lreq.set(True)\n\n\ndef sample(cfg, logger):\n    torch.cuda.set_device(0)\n    model = Model(\n        startf=cfg.MODEL.START_CHANNEL_COUNT,\n        layer_count=cfg.MODEL.LAYER_COUNT,\n        maxf=cfg.MODEL.MAX_CHANNEL_COUNT,\n        latent_size=cfg.MODEL.LATENT_SPACE_SIZE,\n        truncation_psi=cfg.MODEL.TRUNCATIOM_PSI,\n        truncation_cutoff=cfg.MODEL.TRUNCATIOM_CUTOFF,\n        mapping_layers=cfg.MODEL.MAPPING_LAYERS,\n        channels=cfg.MODEL.CHANNELS,\n        generator=cfg.MODEL.GENERATOR,\n        encoder=cfg.MODEL.ENCODER)\n    model.cuda(0)\n    model.eval()\n    model.requires_grad_(False)\n\n    decoder = model.decoder\n    encoder = model.encoder\n    mapping_tl = model.mapping_tl\n    mapping_fl = model.mapping_fl\n    dlatent_avg = model.dlatent_avg\n\n    logger.info(""Trainable parameters generator:"")\n    count_parameters(decoder)\n\n    logger.info(""Trainable parameters discriminator:"")\n    count_parameters(encoder)\n\n    arguments = dict()\n    arguments[""iteration""] = 0\n\n    model_dict = {\n        \'discriminator_s\': encoder,\n        \'generator_s\': decoder,\n        \'mapping_tl_s\': mapping_tl,\n        \'mapping_fl_s\': mapping_fl,\n        \'dlatent_avg\': dlatent_avg\n    }\n\n    checkpointer = Checkpointer(cfg,\n                                model_dict,\n                                {},\n                                logger=logger,\n                                save=False)\n\n    extra_checkpoint_data = checkpointer.load()\n\n    model.eval()\n\n    layer_count = cfg.MODEL.LAYER_COUNT\n\n    def encode(x):\n        Z, _ = model.encode(x, layer_count - 1, 1)\n        Z = Z.repeat(1, model.mapping_fl.num_layers, 1)\n        return Z\n\n    def decode(x):\n        layer_idx = torch.arange(2 * cfg.MODEL.LAYER_COUNT)[np.newaxis, :, np.newaxis]\n        ones = torch.ones(layer_idx.shape, dtype=torch.float32)\n        coefs = torch.where(layer_idx < model.truncation_cutoff, ones, ones)\n        # x = torch.lerp(model.dlatent_avg.buff.data, x, coefs)\n        return model.decoder(x, layer_count - 1, 1, noise=True)\n\n    path = cfg.DATASET.SAMPLES_PATH\n    im_size = 2 ** (cfg.MODEL.LAYER_COUNT + 1)\n\n    def do_attribute_traversal(path, attrib_idx, start, end):\n        img = np.asarray(Image.open(path))\n        if img.shape[2] == 4:\n            img = img[:, :, :3]\n        im = img.transpose((2, 0, 1))\n        x = torch.tensor(np.asarray(im, dtype=np.float32), device=\'cpu\', requires_grad=True).cuda() / 127.5 - 1.\n        if x.shape[0] == 4:\n            x = x[:3]\n        factor = x.shape[2] // im_size\n        if factor != 1:\n            x = torch.nn.functional.avg_pool2d(x[None, ...], factor, factor)[0]\n        assert x.shape[2] == im_size\n        _latents = encode(x[None, ...].cuda())\n        latents = _latents[0, 0]\n\n        latents -= model.dlatent_avg.buff.data[0]\n\n        w0 = torch.tensor(np.load(""principal_directions/direction_%d.npy"" % attrib_idx), dtype=torch.float32)\n\n        attr0 = (latents * w0).sum()\n\n        latents = latents - attr0 * w0\n\n        def update_image(w):\n            with torch.no_grad():\n                w = w + model.dlatent_avg.buff.data[0]\n                w = w[None, None, ...].repeat(1, model.mapping_fl.num_layers, 1)\n\n                layer_idx = torch.arange(model.mapping_fl.num_layers)[np.newaxis, :, np.newaxis]\n                cur_layers = (7 + 1) * 2\n                mixing_cutoff = cur_layers\n                styles = torch.where(layer_idx < mixing_cutoff, w, _latents[0])\n\n                x_rec = decode(styles)\n                return x_rec\n\n        traversal = []\n\n        r = 7\n        inc = (end - start) / (r - 1)\n\n        for i in range(r):\n            W = latents + w0 * (attr0 + start)\n            im = update_image(W)\n\n            traversal.append(im)\n            attr0 += inc\n        res = torch.cat(traversal)\n\n        indices = [0, 1, 2, 3, 4, 10, 11, 17, 19]\n        labels = [""gender"",\n                  ""smile"",\n                  ""attractive"",\n                  ""wavy-hair"",\n                  ""young"",\n                  ""big_lips"",\n                  ""big_nose"",\n                  ""chubby"",\n                  ""glasses"",\n                  ]\n        save_image(res * 0.5 + 0.5, ""make_figures/output/%s/traversal_%s.jpg"" % (\n            cfg.NAME, labels[indices.index(attrib_idx)]), pad_value=1)\n\n    do_attribute_traversal(path + \'/00049.png\', 0, 0.6, -34)\n    do_attribute_traversal(path + \'/00125.png\', 1, -3, 15.0)\n    do_attribute_traversal(path + \'/00057.png\', 3, -2, 30.0)\n    do_attribute_traversal(path + \'/00031.png\', 4, -10, 30.0)\n    do_attribute_traversal(path + \'/00088.png\', 10, -0.3, 30.0)\n    do_attribute_traversal(path + \'/00004.png\', 11, -25, 20.0)\n    do_attribute_traversal(path + \'/00012.png\', 17, -40, 40.0)\n    do_attribute_traversal(path + \'/00017.png\', 19, 0, 30.0)\n\n\nif __name__ == ""__main__"":\n    gpu_count = 1\n    run(sample, get_cfg_defaults(), description=\'ALAE-traversals\', default_config=\'configs/ffhq.yaml\',\n        world_size=gpu_count, write_log=False)\n'"
metrics/fid.py,5,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n""""""Perceptual Path Length (PPL).""""""\n\nimport dnnlib.tflib\nimport pickle\nfrom net import *\nfrom model import Model\nfrom launcher import run\nfrom dataloader import *\nimport scipy.linalg\n\nfrom checkpointer import Checkpointer\n\nfrom dlutils.pytorch import count_parameters\nfrom dlutils import download\nfrom defaults import get_cfg_defaults\nfrom tqdm import tqdm\n\nimport utils\n\ndnnlib.tflib.init_tf()\ntf_config = {\'rnd.np_random_seed\': 1000}\n\ndownload.from_google_drive(\'1CIDc9i070KQhHlkr4yIwoJC8xqrwjE0_\', directory=""metrics"")\n\n\nclass FID:\n    def __init__(self, cfg, num_images, minibatch_size):\n        self.num_images = num_images\n        self.minibatch_size = minibatch_size\n        self.cfg = cfg\n\n    def evaluate(self, logger, mapping, decoder, model, lod):\n        gpu_count = torch.cuda.device_count()\n        inception = pickle.load(open(\'metrics/inception_v3_features.pkl\', \'rb\'))\n\n        # Sampling loop.\n        @utils.cache\n        def compute_for_reals(num_images, path, lod):\n            dataset = TFRecordsDataset(self.cfg, logger, rank=0, world_size=1, buffer_size_mb=1024, channels=self.cfg.MODEL.CHANNELS, train=True)\n            dataset.reset(lod + 2, self.minibatch_size)\n            batches = make_dataloader(self.cfg, logger, dataset, self.minibatch_size, 0, numpy=True)\n\n            activations = []\n            num_images_processed = 0\n            for idx, x in tqdm(enumerate(batches)):\n                res = inception.run(x, num_gpus=gpu_count, assume_frozen=True)\n                activations.append(res)\n                num_images_processed += x.shape[0]\n                if num_images_processed > num_images:\n                    break\n\n            activations = np.concatenate(activations)\n            print(activations.shape)\n            print(num_images)\n\n            assert activations.shape[0] >= num_images\n            activations = activations[:num_images]\n            assert activations.shape[0] == num_images\n\n            mu_real = np.mean(activations, axis=0)\n            sigma_real = np.cov(activations, rowvar=False)\n            return mu_real, sigma_real\n\n        mu_real, sigma_real = compute_for_reals(self.num_images, self.cfg.DATASET.PATH, lod)\n\n        activations = []\n        for _ in tqdm(range(0, self.num_images, self.minibatch_size)):\n            torch.cuda.set_device(0)\n            images = model.generate(lod, 1, count=self.minibatch_size, no_truncation=True)\n\n            images = np.clip((images.cpu().numpy() + 1.0) * 127, 0, 255).astype(np.uint8)\n\n            res = inception.run(images, num_gpus=gpu_count, assume_frozen=True)\n\n            activations.append(res)\n\n        activations = np.concatenate(activations)\n        print(activations.shape)\n        print(self.num_images)\n\n        assert activations.shape[0] >= self.num_images\n        activations = activations[:self.num_images]\n        assert activations.shape[0] == self.num_images\n\n        mu_fake = np.mean(activations, axis=0)\n        sigma_fake = np.cov(activations, rowvar=False)\n\n        # Calculate FID.\n        m = np.square(mu_fake - mu_real).sum()\n        s, _ = scipy.linalg.sqrtm(np.dot(sigma_fake, sigma_real), disp=False)\n        dist = m + np.trace(sigma_fake + sigma_real - 2*s)\n\n        logger.info(""Result = %f"" % (np.real(dist)))\n\n\ndef sample(cfg, logger):\n    torch.cuda.set_device(0)\n    model = Model(\n        startf=cfg.MODEL.START_CHANNEL_COUNT,\n        layer_count=cfg.MODEL.LAYER_COUNT,\n        maxf=cfg.MODEL.MAX_CHANNEL_COUNT,\n        latent_size=cfg.MODEL.LATENT_SPACE_SIZE,\n        truncation_psi=None,\n        truncation_cutoff=None,\n        style_mixing_prob=cfg.MODEL.STYLE_MIXING_PROB,\n        mapping_layers=cfg.MODEL.MAPPING_LAYERS,\n        channels=cfg.MODEL.CHANNELS,\n        generator=cfg.MODEL.GENERATOR,\n        encoder=cfg.MODEL.ENCODER)\n\n    model.cuda(0)\n    model.eval()\n    model.requires_grad_(False)\n\n    decoder = model.decoder\n    encoder = model.encoder\n\n    mapping_fl = model.mapping_fl\n    dlatent_avg = model.dlatent_avg\n\n    logger.info(""Trainable parameters generator:"")\n    count_parameters(decoder)\n\n    logger.info(""Trainable parameters discriminator:"")\n    count_parameters(encoder)\n\n    arguments = dict()\n    arguments[""iteration""] = 0\n\n    model_dict = {\n        \'discriminator_s\': encoder,\n        \'generator_s\': decoder,\n        \'mapping_fl_s\': mapping_fl,\n        \'dlatent_avg_s\': dlatent_avg\n    }\n\n    checkpointer = Checkpointer(cfg,\n                                model_dict,\n                                {},\n                                logger=logger,\n                                save=False)\n\n    extra_checkpoint_data = checkpointer.load()\n    last_epoch = list(extra_checkpoint_data[\'auxiliary\'][\'scheduler\'].values())[0][\'last_epoch\']\n    logger.info(""Model trained for %d epochs"" % last_epoch)\n\n    model.eval()\n\n    layer_count = cfg.MODEL.LAYER_COUNT\n\n    logger.info(""Evaluating FID metric"")\n\n    model.decoder = nn.DataParallel(decoder)\n\n    with torch.no_grad():\n        ppl = FID(cfg, num_images=50000, minibatch_size=16 * torch.cuda.device_count())\n        ppl.evaluate(logger, mapping_fl, model.decoder, model, cfg.DATASET.MAX_RESOLUTION_LEVEL - 2)\n\n\nif __name__ == ""__main__"":\n    gpu_count = 1\n    run(sample, get_cfg_defaults(), description=\'ALAE-fid\', default_config=\'configs/ffhq.yaml\',\n        world_size=gpu_count, write_log=""metrics/fid_score.txt"")\n'"
metrics/fid_rec.py,5,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n""""""Perceptual Path Length (PPL).""""""\n\nimport dnnlib.tflib\nimport pickle\nfrom net import *\nfrom model import Model\nfrom launcher import run\nfrom dataloader import *\nimport scipy.linalg\n\nfrom checkpointer import Checkpointer\n\nfrom dlutils.pytorch import count_parameters\nfrom dlutils import download\nfrom defaults import get_cfg_defaults\nfrom tqdm import tqdm\n\nimport utils\n\ndnnlib.tflib.init_tf()\ntf_config = {\'rnd.np_random_seed\': 1000}\n\ndownload.from_google_drive(\'1CIDc9i070KQhHlkr4yIwoJC8xqrwjE0_\', directory=""metrics"")\n\n\nclass FID:\n    def __init__(self, cfg, num_images, minibatch_size):\n        self.num_images = num_images\n        self.minibatch_size = minibatch_size\n        self.cfg = cfg\n\n    def evaluate(self, logger, mapping, decoder, encoder, lod):\n        gpu_count = torch.cuda.device_count()\n        inception = pickle.load(open(\'metrics/inception_v3_features.pkl\', \'rb\'))\n\n        # Sampling loop.\n        @utils.cache\n        def compute_for_reals(num_images, path, lod):\n            dataset = TFRecordsDataset(self.cfg, logger, rank=0, world_size=1, buffer_size_mb=1024, channels=self.cfg.MODEL.CHANNELS, train=True)\n            dataset.reset(lod + 2, self.minibatch_size)\n            batches = make_dataloader(self.cfg, logger, dataset, self.minibatch_size, 0, numpy=True)\n\n            activations = []\n            num_images_processed = 0\n            for idx, x in tqdm(enumerate(batches)):\n                res = inception.run(x, num_gpus=gpu_count, assume_frozen=True)\n                activations.append(res)\n                num_images_processed += x.shape[0]\n                if num_images_processed > num_images:\n                    break\n\n            activations = np.concatenate(activations)\n            print(activations.shape)\n            print(num_images)\n\n            assert activations.shape[0] >= num_images\n            activations = activations[:num_images]\n            assert activations.shape[0] == num_images\n\n            mu_real = np.mean(activations, axis=0)\n            sigma_real = np.cov(activations, rowvar=False)\n            return mu_real, sigma_real\n\n        mu_real, sigma_real = compute_for_reals(self.num_images, self.cfg.DATASET.PATH, lod)\n\n        dataset = TFRecordsDataset(self.cfg, logger, rank=0, world_size=1, buffer_size_mb=128,\n                                   channels=self.cfg.MODEL.CHANNELS, train=True)\n\n        dataset.reset(lod + 2, self.minibatch_size)\n        batches = make_dataloader(self.cfg, logger, dataset, self.minibatch_size, 0,)\n\n        activations = []\n        num_images_processed = 0\n        for idx, x in tqdm(enumerate(batches)):\n            torch.cuda.set_device(0)\n            x = (x / 127.5 - 1.)\n\n            Z = encoder(x, lod, 1)\n            Z = Z.repeat(1, mapping.num_layers, 1)\n\n            images = decoder(Z, lod, 1.0, noise=True)\n\n            images = np.clip((images.cpu().numpy() + 1.0) * 127, 0, 255).astype(np.uint8)\n\n            res = inception.run(images, num_gpus=gpu_count, assume_frozen=True)\n\n            activations.append(res)\n            num_images_processed += x.shape[0]\n            if num_images_processed > self.num_images:\n                break\n\n        activations = np.concatenate(activations)\n        print(activations.shape)\n        print(self.num_images)\n\n        assert activations.shape[0] >= self.num_images\n        activations = activations[:self.num_images]\n        assert activations.shape[0] == self.num_images\n\n        mu_fake = np.mean(activations, axis=0)\n        sigma_fake = np.cov(activations, rowvar=False)\n\n        # Calculate FID.\n        m = np.square(mu_fake - mu_real).sum()\n        s, _ = scipy.linalg.sqrtm(np.dot(sigma_fake, sigma_real), disp=False)\n        dist = m + np.trace(sigma_fake + sigma_real - 2*s)\n\n        logger.info(""Result = %f"" % (np.real(dist)))\n\n\ndef sample(cfg, logger):\n    torch.cuda.set_device(0)\n    model = Model(\n        startf=cfg.MODEL.START_CHANNEL_COUNT,\n        layer_count=cfg.MODEL.LAYER_COUNT,\n        maxf=cfg.MODEL.MAX_CHANNEL_COUNT,\n        latent_size=cfg.MODEL.LATENT_SPACE_SIZE,\n        truncation_psi=None,\n        truncation_cutoff=None,\n        style_mixing_prob=None,\n        mapping_layers=cfg.MODEL.MAPPING_LAYERS,\n        channels=cfg.MODEL.CHANNELS,\n        generator=cfg.MODEL.GENERATOR,\n        encoder=cfg.MODEL.ENCODER)\n\n    model.cuda(0)\n    model.eval()\n    model.requires_grad_(False)\n\n    decoder = model.decoder\n    encoder = model.encoder\n    mapping_tl = model.mapping_tl\n    mapping_fl = model.mapping_fl\n    dlatent_avg = model.dlatent_avg\n\n    logger.info(""Trainable parameters generator:"")\n    count_parameters(decoder)\n\n    logger.info(""Trainable parameters discriminator:"")\n    count_parameters(encoder)\n\n    arguments = dict()\n    arguments[""iteration""] = 0\n\n    model_dict = {\n        \'discriminator_s\': encoder,\n        \'generator_s\': decoder,\n        \'mapping_tl_s\': mapping_tl,\n        \'mapping_fl_s\': mapping_fl,\n        \'dlatent_avg\': dlatent_avg\n    }\n\n    checkpointer = Checkpointer(cfg,\n                                model_dict,\n                                {},\n                                logger=logger,\n                                save=False)\n\n    extra_checkpoint_data = checkpointer.load()\n    last_epoch = list(extra_checkpoint_data[\'auxiliary\'][\'scheduler\'].values())[0][\'last_epoch\']\n    logger.info(""Model trained for %d epochs"" % last_epoch)\n\n    model.eval()\n\n    layer_count = cfg.MODEL.LAYER_COUNT\n\n    logger.info(""Evaluating FID metric"")\n\n    encoder = nn.DataParallel(encoder)\n    decoder = nn.DataParallel(decoder)\n\n    with torch.no_grad():\n        ppl = FID(cfg, num_images=50000, minibatch_size=16 * torch.cuda.device_count())\n        ppl.evaluate(logger, mapping_fl, decoder, encoder, cfg.DATASET.MAX_RESOLUTION_LEVEL - 2)\n\n\nif __name__ == ""__main__"":\n    gpu_count = 1\n    run(sample, get_cfg_defaults(), description=\'ALAE-fid-reconstruction\', default_config=\'configs/ffhq.yaml\',\n        world_size=gpu_count, write_log=""metrics/fid_score-reconstruction.txt"")\n'"
metrics/fid_sep.py,5,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n""""""Perceptual Path Length (PPL).""""""\n\nimport dnnlib.tflib\nimport pickle\nfrom net import *\nfrom model_separate import Model\nfrom launcher import run\nfrom dataloader import *\nimport scipy.linalg\n\nfrom checkpointer import Checkpointer\n\nfrom dlutils.pytorch import count_parameters\nfrom dlutils import download\nfrom defaults import get_cfg_defaults\nfrom tqdm import tqdm\n\nimport utils\n\ndnnlib.tflib.init_tf()\n\ndownload.from_google_drive(\'1CIDc9i070KQhHlkr4yIwoJC8xqrwjE0_\', directory=""metrics"")\n\n\nclass FID:\n    def __init__(self, cfg, num_images, minibatch_size):\n        self.num_images = num_images\n        self.minibatch_size = minibatch_size\n        self.cfg = cfg\n\n    def evaluate(self, logger, mapping, decoder, model, lod):\n        gpu_count = torch.cuda.device_count()\n        inception = pickle.load(open(\'metrics/inception_v3_features.pkl\', \'rb\'))\n\n        # Sampling loop.\n        @utils.cache\n        def compute_for_reals(num_images, path, lod):\n            dataset = TFRecordsDataset(self.cfg, logger, rank=0, world_size=1, buffer_size_mb=1024, channels=self.cfg.MODEL.CHANNELS, train=True)\n            dataset.reset(lod + 2, self.minibatch_size)\n            batches = make_dataloader(self.cfg, logger, dataset, self.minibatch_size, 0, numpy=True)\n\n            activations = []\n            num_images_processed = 0\n            for idx, x in tqdm(enumerate(batches)):\n                res = inception.run(x, num_gpus=gpu_count, assume_frozen=True)\n                activations.append(res)\n                num_images_processed += x.shape[0]\n                if num_images_processed > num_images:\n                    break\n\n            activations = np.concatenate(activations)\n            print(activations.shape)\n            print(num_images)\n\n            assert activations.shape[0] >= num_images\n            activations = activations[:num_images]\n            assert activations.shape[0] == num_images\n\n            mu_real = np.mean(activations, axis=0)\n            sigma_real = np.cov(activations, rowvar=False)\n            return mu_real, sigma_real\n\n        mu_real, sigma_real = compute_for_reals(self.num_images, self.cfg.DATASET.PATH, lod)\n\n        activations = []\n        for _ in tqdm(range(0, self.num_images, self.minibatch_size)):\n            torch.cuda.set_device(0)\n            images = model.generate(lod, 1, count=self.minibatch_size, no_truncation=True)\n\n            images = np.clip((images.cpu().numpy() + 1.0) * 127, 0, 255).astype(np.uint8)\n\n            res = inception.run(images, num_gpus=gpu_count, assume_frozen=True)\n\n            activations.append(res)\n\n        activations = np.concatenate(activations)\n        print(activations.shape)\n        print(self.num_images)\n\n        assert activations.shape[0] >= self.num_images\n        activations = activations[:self.num_images]\n        assert activations.shape[0] == self.num_images\n\n        mu_fake = np.mean(activations, axis=0)\n        sigma_fake = np.cov(activations, rowvar=False)\n\n        # Calculate FID.\n        m = np.square(mu_fake - mu_real).sum()\n        s, _ = scipy.linalg.sqrtm(np.dot(sigma_fake, sigma_real), disp=False)\n        dist = m + np.trace(sigma_fake + sigma_real - 2*s)\n\n        logger.info(""Result = %f"" % (np.real(dist)))\n\n\ndef sample(cfg, logger):\n    torch.cuda.set_device(0)\n    model = Model(\n        startf=cfg.MODEL.START_CHANNEL_COUNT,\n        layer_count=cfg.MODEL.LAYER_COUNT,\n        maxf=cfg.MODEL.MAX_CHANNEL_COUNT,\n        latent_size=cfg.MODEL.LATENT_SPACE_SIZE,\n        truncation_psi=None,\n        truncation_cutoff=None,\n        style_mixing_prob=cfg.MODEL.STYLE_MIXING_PROB,\n        mapping_layers=cfg.MODEL.MAPPING_LAYERS,\n        channels=cfg.MODEL.CHANNELS,\n        generator=cfg.MODEL.GENERATOR,\n        encoder=cfg.MODEL.ENCODER)\n\n    model.cuda(0)\n    model.eval()\n    model.requires_grad_(False)\n\n    decoder = model.decoder\n    encoder = model.encoder\n\n    mapping_fl = model.mapping_fl\n    dlatent_avg = model.dlatent_avg\n\n    logger.info(""Trainable parameters generator:"")\n    count_parameters(decoder)\n\n    logger.info(""Trainable parameters discriminator:"")\n    count_parameters(encoder)\n\n    arguments = dict()\n    arguments[""iteration""] = 0\n\n    model_dict = {\n        \'discriminator_s\': encoder,\n        \'generator_s\': decoder,\n        \'mapping_fl_s\': mapping_fl,\n        \'dlatent_avg_s\': dlatent_avg\n    }\n\n    checkpointer = Checkpointer(cfg,\n                                model_dict,\n                                {},\n                                logger=logger,\n                                save=False)\n\n    extra_checkpoint_data = checkpointer.load()\n    last_epoch = list(extra_checkpoint_data[\'auxiliary\'][\'scheduler\'].values())[0][\'last_epoch\']\n    logger.info(""Model trained for %d epochs"" % last_epoch)\n\n    model.eval()\n\n    layer_count = cfg.MODEL.LAYER_COUNT\n\n    logger.info(""Evaluating FID metric"")\n\n    model.decoder = nn.DataParallel(decoder)\n\n    with torch.no_grad():\n        ppl = FID(cfg, num_images=50000, minibatch_size=16 * torch.cuda.device_count())\n        ppl.evaluate(logger, mapping_fl, model.decoder, model, cfg.DATASET.MAX_RESOLUTION_LEVEL - 2)\n\n\nif __name__ == ""__main__"":\n    gpu_count = 1\n    run(sample, get_cfg_defaults(), description=\'ALAE-fid\', default_config=\'configs/experiment_celeba_sep.yaml\',\n        world_size=gpu_count, write_log=""metrics/fid_score.txt"")\n'"
metrics/lpips.py,8,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\nimport dnnlib.tflib\nimport pickle\nfrom net import *\nfrom model import Model\nfrom launcher import run\nfrom dataloader import *\n\nfrom checkpointer import Checkpointer\n\nfrom dlutils.pytorch import count_parameters\nfrom dlutils import download\nfrom defaults import get_cfg_defaults\nfrom tqdm import tqdm\n\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nimport utils\n\ndnnlib.tflib.init_tf()\n\ndownload.from_google_drive(\'1CIDc9i070KQhHlkr4yIwoJC8xqrwjE0_\', directory=""metrics"")\n\n\ndef downscale(images):\n    if images.shape[2] > 256:\n        factor = images.shape[2] // 256\n        images = torch.reshape(images,\n                               [-1, images.shape[1], images.shape[2] // factor, factor, images.shape[3] // factor,\n                                factor])\n        images = torch.mean(images, dim=(3, 5))\n    images = np.clip((images.cpu().numpy() + 1.0) * 127, 0, 255).astype(np.uint8)\n    return images\n\n\nclass LPIPS:\n    def __init__(self, cfg, num_images, minibatch_size):\n        self.num_images = num_images\n        self.minibatch_size = minibatch_size\n        self.cfg = cfg\n\n    def evaluate(self, logger, mapping, decoder, encoder, lod):\n        gpu_count = torch.cuda.device_count()\n        distance_measure = pickle.load(open(\'metrics/vgg16_zhang_perceptual.pkl\', \'rb\'))\n\n        dataset = TFRecordsDataset(self.cfg, logger, rank=0, world_size=1, buffer_size_mb=128,\n                                   channels=self.cfg.MODEL.CHANNELS, train=False)\n\n        dataset.reset(lod + 2, self.minibatch_size)\n        batches = make_dataloader(self.cfg, logger, dataset, self.minibatch_size, 0,)\n\n        distance = []\n        num_images_processed = 0\n        for idx, x in tqdm(enumerate(batches)):\n            torch.cuda.set_device(0)\n            x = (x / 127.5 - 1.)\n\n            Z = encoder(x, lod, 1)\n            Z = Z.repeat(1, mapping.num_layers, 1)\n\n            images = decoder(Z, lod, 1.0, noise=True)\n\n            images = downscale(images)\n            images_ref = downscale(torch.tensor(x))\n\n            res = distance_measure.run(images, images_ref, num_gpus=gpu_count, assume_frozen=True)\n            distance.append(res)\n            num_images_processed += x.shape[0]\n            if num_images_processed > self.num_images:\n                break\n\n        print(len(distance))\n        logger.info(""Result = %f"" % (np.asarray(distance).mean()))\n\n\ndef sample(cfg, logger):\n    torch.cuda.set_device(0)\n    model = Model(\n        startf=cfg.MODEL.START_CHANNEL_COUNT,\n        layer_count=cfg.MODEL.LAYER_COUNT,\n        maxf=cfg.MODEL.MAX_CHANNEL_COUNT,\n        latent_size=cfg.MODEL.LATENT_SPACE_SIZE,\n        truncation_psi=None,\n        truncation_cutoff=None,\n        mapping_layers=cfg.MODEL.MAPPING_LAYERS,\n        channels=cfg.MODEL.CHANNELS,\n        generator=cfg.MODEL.GENERATOR,\n        encoder=cfg.MODEL.ENCODER)\n\n    model.cuda(0)\n    model.eval()\n    model.requires_grad_(False)\n\n    decoder = model.decoder\n    encoder = model.encoder\n\n    mapping_fl = model.mapping_fl\n    dlatent_avg = model.dlatent_avg\n\n    logger.info(""Trainable parameters generator:"")\n    count_parameters(decoder)\n\n    logger.info(""Trainable parameters discriminator:"")\n    count_parameters(encoder)\n\n    arguments = dict()\n    arguments[""iteration""] = 0\n\n    model_dict = {\n        \'discriminator_s\': encoder,\n        # \'encoder_s\': encoder,\n        \'generator_s\': decoder,\n        \'mapping_fl_s\': mapping_fl,\n        \'dlatent_avg_s\': dlatent_avg\n    }\n\n    checkpointer = Checkpointer(cfg,\n                                model_dict,\n                                {},\n                                logger=logger,\n                                save=False)\n\n    extra_checkpoint_data = checkpointer.load()\n    last_epoch = list(extra_checkpoint_data[\'auxiliary\'][\'scheduler\'].values())[0][\'last_epoch\']\n    logger.info(""Model trained for %d epochs"" % last_epoch)\n\n    model.eval()\n\n    layer_count = cfg.MODEL.LAYER_COUNT\n\n    logger.info(""Evaluating LPIPS metric"")\n\n    decoder = nn.DataParallel(decoder)\n    encoder = nn.DataParallel(encoder)\n\n    with torch.no_grad():\n        ppl = LPIPS(cfg, num_images=10000, minibatch_size=16 * torch.cuda.device_count())\n        ppl.evaluate(logger, mapping_fl, decoder, encoder, cfg.DATASET.MAX_RESOLUTION_LEVEL - 2)\n\n\nif __name__ == ""__main__"":\n    gpu_count = 1\n    run(sample, get_cfg_defaults(), description=\'ALAE-lpips\', default_config=\'configs/experiment_celeba.yaml\',\n        world_size=gpu_count, write_log=""metrics/lpips_score.txt"")\n'"
metrics/ppl.py,23,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n""""""Perceptual Path Length (PPL).""""""\n\nimport numpy as np\nimport dnnlib.tflib\nimport pickle\nfrom net import *\nfrom checkpointer import Checkpointer\nfrom model import Model\nfrom launcher import run\nfrom dlutils.pytorch import count_parameters\nfrom defaults import get_cfg_defaults\nfrom dlutils import download\nimport tqdm\n\nfrom matplotlib import pyplot as plt\n\ndnnlib.tflib.init_tf()\n\ndownload.from_google_drive(\'1CIDc9i070KQhHlkr4yIwoJC8xqrwjE0_\', directory=""metrics"")\n\n\n# Normalize batch of vectors.\ndef normalize(v):\n    return v / torch.sqrt(torch.sum(v * v, dim=-1, keepdim=True))\n\n\n# Spherical interpolation of a batch of vectors.\ndef slerp(a, b, t):\n    a = normalize(a)\n    b = normalize(b)\n    d = torch.sum(a * b, dim=-1, keepdim=True)\n    p = t * torch.acos(d)\n    c = normalize(b - d * a)\n    d = a * torch.cos(p) + c * torch.sin(p)\n    return normalize(d)\n\n\nclass PPL:\n    def __init__(self, cfg, num_samples, epsilon, space, sampling, minibatch_size, **kwargs):\n        assert space in [\'z\', \'w\']\n        assert sampling in [\'full\', \'end\']\n        self.num_samples = num_samples\n        self.epsilon = epsilon\n        self.space = space\n        self.sampling = sampling\n        self.minibatch_size = minibatch_size\n        self.cfg = cfg\n\n    def evaluate(self, logger, mapping, decoder, lod, celeba_style=False):\n        distance_measure = pickle.load(open(\'metrics/vgg16_zhang_perceptual.pkl\', \'rb\'))\n        gpu_count = torch.cuda.device_count()\n\n        # Sampling loop.\n        all_distances = []\n        for _ in tqdm.tqdm(range(0, self.num_samples, self.minibatch_size)):\n            torch.cuda.set_device(0)\n            # Generate random latents and interpolation t-values.\n            lat_t01 = torch.randn([self.minibatch_size * 2, self.cfg.MODEL.LATENT_SPACE_SIZE])\n            lerp_t = torch.rand(self.minibatch_size) * (1.0 if self.sampling == \'full\' else 0.0)\n\n            # Interpolate in W or Z.\n            if self.space == \'w\':\n                dlat_t01 = mapping(lat_t01)\n                dlat_t0, dlat_t1 = dlat_t01[0::2], dlat_t01[1::2]\n                dlat_e0 = torch.lerp(dlat_t0, dlat_t1, lerp_t[:, np.newaxis, np.newaxis])\n                dlat_e1 = torch.lerp(dlat_t0, dlat_t1, lerp_t[:, np.newaxis, np.newaxis] + self.epsilon)\n                dlat_e01 = torch.reshape(torch.stack([dlat_e0, dlat_e1], dim=1), dlat_t01.shape)\n            else:  # space == \'z\'\n                lat_t0, lat_t1 = lat_t01[0::2], lat_t01[1::2]\n                lat_e0 = slerp(lat_t0, lat_t1, lerp_t[:, np.newaxis])\n                lat_e1 = slerp(lat_t0, lat_t1, lerp_t[:, np.newaxis] + self.epsilon)\n                lat_e01 = torch.reshape(torch.stack([lat_e0, lat_e1], dim=1), lat_t01.shape)\n                dlat_e01 = mapping(lat_e01)\n\n            # Synthesize images.\n            images = decoder(dlat_e01, lod, 1.0, noise=\'batch_constant\')\n\n            # Crop only the face region.\n            # example: https://user-images.githubusercontent.com/3229783/79639054-1b658f80-8157-11ea-93e7-eba6f8b22a24.png\n            if not celeba_style:\n                c = int(images.shape[2] // 8)\n                images = images[:, :, c * 3: c * 7, c * 2: c * 6]\n\n            else:  # celeba128x128 style. Faces on celeba128x128 dataset cropped more tightly\n                   # example https://user-images.githubusercontent.com/3229783/79639067-2cae9c00-8157-11ea-8d29-021de71e3840.png\n                c = int(images.shape[2])\n                h = (7.0 - 3.0) / 8.0 * (2.0 / 1.6410)\n                w = (6.0 - 2.0) / 8.0 * (2.0 / 1.6410)\n                vc = (7.0 + 3.0) / 2.0 / 8.0\n                hc = (6.0 + 2.0) / 2.0 / 8.0\n                h = int(h * c)\n                w = int(w * c)\n                hc = int(hc * c)\n                vc = int(vc * c)\n                images = images[:, :, vc - h // 2: vc + h // 2, hc - w // 2: hc + w // 2]\n\n            # print(images.shape)\n            # plt.imshow(images[0].cpu().numpy().transpose(1, 2, 0), interpolation=\'nearest\')\n            # plt.show()\n            # exit()\n\n            # Downsample image to 256x256 if it\'s larger than that. VGG was built for 224x224 images.\n            if images.shape[2] > 256:\n                factor = images.shape[2] // 256\n                images = torch.reshape(images,\n                                       [-1, images.shape[1], images.shape[2] // factor, factor,\n                                        images.shape[3] // factor,\n                                        factor])\n                images = torch.mean(images, dim=(3, 5))\n\n            # Scale dynamic range from [-1,1] to [0,255] for VGG.\n            images = (images + 1) * (255 / 2)\n\n            # Evaluate perceptual distance.\n            img_e0, img_e1 = images[0::2], images[1::2]\n\n            res = distance_measure.run(img_e0.cpu().numpy(), img_e1.cpu().numpy(), num_gpus=gpu_count, assume_frozen=True) * (1 / self.epsilon ** 2)\n\n            all_distances.append(res)\n\n        all_distances = np.concatenate(all_distances, axis=0)\n\n        # Reject outliers.\n        lo = np.percentile(all_distances, 1, interpolation=\'lower\')\n        hi = np.percentile(all_distances, 99, interpolation=\'higher\')\n        filtered_distances = np.extract(np.logical_and(lo <= all_distances, all_distances <= hi), all_distances)\n        logger.info(""Result %s = %f"" % (self.sampling, np.mean(filtered_distances)))\n\n\ndef sample(cfg, logger):\n    torch.cuda.set_device(0)\n    model = Model(\n        startf=cfg.MODEL.START_CHANNEL_COUNT,\n        layer_count=cfg.MODEL.LAYER_COUNT,\n        maxf=cfg.MODEL.MAX_CHANNEL_COUNT,\n        latent_size=cfg.MODEL.LATENT_SPACE_SIZE,\n        truncation_psi=cfg.MODEL.TRUNCATIOM_PSI,\n        truncation_cutoff=cfg.MODEL.TRUNCATIOM_CUTOFF,\n        mapping_layers=cfg.MODEL.MAPPING_LAYERS,\n        channels=cfg.MODEL.CHANNELS,\n        generator=cfg.MODEL.GENERATOR,\n        encoder=cfg.MODEL.ENCODER)\n\n    model.cuda(0)\n    model.eval()\n    model.requires_grad_(False)\n\n    decoder = model.decoder\n    encoder = model.encoder\n    mapping_tl = model.mapping_tl\n    mapping_fl = model.mapping_fl\n    dlatent_avg = model.dlatent_avg\n\n    logger.info(""Trainable parameters generator:"")\n    count_parameters(decoder)\n\n    logger.info(""Trainable parameters discriminator:"")\n    count_parameters(encoder)\n\n    arguments = dict()\n    arguments[""iteration""] = 0\n\n    model_dict = {\n        \'discriminator_s\': encoder,\n        \'generator_s\': decoder,\n        \'mapping_tl_s\': mapping_tl,\n        \'mapping_fl_s\': mapping_fl,\n        \'dlatent_avg\': dlatent_avg\n    }\n\n    checkpointer = Checkpointer(cfg,\n                                model_dict,\n                                {},\n                                logger=logger,\n                                save=False)\n\n    checkpointer.load()\n\n    model.eval()\n\n    layer_count = cfg.MODEL.LAYER_COUNT\n\n    def encode(x):\n        Z, _ = model.encode(x, layer_count - 1, 1)\n        Z = Z.repeat(1, model.mapping_fl.num_layers, 1)\n        return Z\n\n    def decode(x):\n        layer_idx = torch.arange(2 * cfg.MODEL.LAYER_COUNT)[np.newaxis, :, np.newaxis]\n        ones = torch.ones(layer_idx.shape, dtype=torch.float32)\n        coefs = torch.where(layer_idx < model.truncation_cutoff, 1.2 * ones, ones)\n        x = torch.lerp(model.dlatent_avg.buff.data, x, coefs)\n        return model.decoder(x, layer_count - 1, 1, noise=True)\n\n    logger.info(""Evaluating PPL metric"")\n\n    decoder = nn.DataParallel(decoder)\n\n    with torch.no_grad():\n        ppl = PPL(cfg, num_samples=50000, epsilon=1e-4, space=\'w\', sampling=\'full\', minibatch_size=16 * torch.cuda.device_count())\n        ppl.evaluate(logger, mapping_fl, decoder, cfg.DATASET.MAX_RESOLUTION_LEVEL - 2, celeba_style=cfg.PPL_CELEBA_ADJUSTMENT)\n\n    with torch.no_grad():\n        ppl = PPL(cfg, num_samples=50000, epsilon=1e-4, space=\'w\', sampling=\'end\', minibatch_size=16 * torch.cuda.device_count())\n        ppl.evaluate(logger, mapping_fl, decoder, cfg.DATASET.MAX_RESOLUTION_LEVEL - 2, celeba_style=cfg.PPL_CELEBA_ADJUSTMENT)\n\n\nif __name__ == ""__main__"":\n    gpu_count = 1\n    run(sample, get_cfg_defaults(), description=\'ALAE-ppl\', default_config=\'configs/ffhq.yaml\',\n        world_size=gpu_count, write_log=False)\n'"
principal_directions/classifier.py,0,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# This work is licensed under the Creative Commons Attribution-NonCommercial\n# 4.0 International License. To view a copy of this license, visit\n# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\n\nimport dnnlib.tflib\nimport pickle\n\n_cache_dir = \'cache\'\ndnnlib.tflib.init_tf()\n\n\ndef load_pkl(file_or_url):\n    file = None\n    if dnnlib.util.is_url(file_or_url):\n        file = dnnlib.util.open_url(file_or_url, cache_dir=_cache_dir)\n    else:\n        file = open(file_or_url, \'rb\')\n    return pickle.load(file, encoding=\'latin1\')\n\n\nclassifier_urls = [\n    \'https://drive.google.com/uc?id=1Q5-AI6TwWhCVM7Muu4tBM7rp5nG_gmCX\',  # celebahq-classifier-00-male.pkl\n    \'https://drive.google.com/uc?id=1Q5c6HE__ReW2W8qYAXpao68V1ryuisGo\',  # celebahq-classifier-01-smiling.pkl\n    \'https://drive.google.com/uc?id=1Q7738mgWTljPOJQrZtSMLxzShEhrvVsU\',  # celebahq-classifier-02-attractive.pkl\n    \'https://drive.google.com/uc?id=1QBv2Mxe7ZLvOv1YBTLq-T4DS3HjmXV0o\',  # celebahq-classifier-03-wavy-hair.pkl\n    \'https://drive.google.com/uc?id=1QIvKTrkYpUrdA45nf7pspwAqXDwWOLhV\',  # celebahq-classifier-04-young.pkl\n    \'https://drive.google.com/uc?id=1QJPH5rW7MbIjFUdZT7vRYfyUjNYDl4_L\',  # celebahq-classifier-05-5-o-clock-shadow.pkl\n    \'https://drive.google.com/uc?id=1QPZXSYf6cptQnApWS_T83sqFMun3rULY\',  # celebahq-classifier-06-arched-eyebrows.pkl\n    \'https://drive.google.com/uc?id=1QPgoAZRqINXk_PFoQ6NwMmiJfxc5d2Pg\',  # celebahq-classifier-07-bags-under-eyes.pkl\n    \'https://drive.google.com/uc?id=1QQPQgxgI6wrMWNyxFyTLSgMVZmRr1oO7\',  # celebahq-classifier-08-bald.pkl\n    \'https://drive.google.com/uc?id=1QcSphAmV62UrCIqhMGgcIlZfoe8hfWaF\',  # celebahq-classifier-09-bangs.pkl\n    \'https://drive.google.com/uc?id=1QdWTVwljClTFrrrcZnPuPOR4mEuz7jGh\',  # celebahq-classifier-10-big-lips.pkl\n    \'https://drive.google.com/uc?id=1QgvEWEtr2mS4yj1b_Y3WKe6cLWL3LYmK\',  # celebahq-classifier-11-big-nose.pkl\n    \'https://drive.google.com/uc?id=1QidfMk9FOKgmUUIziTCeo8t-kTGwcT18\',  # celebahq-classifier-12-black-hair.pkl\n    \'https://drive.google.com/uc?id=1QthrJt-wY31GPtV8SbnZQZ0_UEdhasHO\',  # celebahq-classifier-13-blond-hair.pkl\n    \'https://drive.google.com/uc?id=1QvCAkXxdYT4sIwCzYDnCL9Nb5TDYUxGW\',  # celebahq-classifier-14-blurry.pkl\n    \'https://drive.google.com/uc?id=1QvLWuwSuWI9Ln8cpxSGHIciUsnmaw8L0\',  # celebahq-classifier-15-brown-hair.pkl\n    \'https://drive.google.com/uc?id=1QxW6THPI2fqDoiFEMaV6pWWHhKI_OoA7\',  # celebahq-classifier-16-bushy-eyebrows.pkl\n    \'https://drive.google.com/uc?id=1R71xKw8oTW2IHyqmRDChhTBkW9wq4N9v\',  # celebahq-classifier-17-chubby.pkl\n    \'https://drive.google.com/uc?id=1RDn_fiLfEGbTc7JjazRXuAxJpr-4Pl67\',  # celebahq-classifier-18-double-chin.pkl\n    \'https://drive.google.com/uc?id=1RGBuwXbaz5052bM4VFvaSJaqNvVM4_cI\',  # celebahq-classifier-19-eyeglasses.pkl\n    \'https://drive.google.com/uc?id=1RIxOiWxDpUwhB-9HzDkbkLegkd7euRU9\',  # celebahq-classifier-20-goatee.pkl\n    \'https://drive.google.com/uc?id=1RPaNiEnJODdr-fwXhUFdoSQLFFZC7rC-\',  # celebahq-classifier-21-gray-hair.pkl\n    \'https://drive.google.com/uc?id=1RQH8lPSwOI2K_9XQCZ2Ktz7xm46o80ep\',  # celebahq-classifier-22-heavy-makeup.pkl\n    \'https://drive.google.com/uc?id=1RXZM61xCzlwUZKq-X7QhxOg0D2telPow\',  # celebahq-classifier-23-high-cheekbones.pkl\n    \'https://drive.google.com/uc?id=1RgASVHW8EWMyOCiRb5fsUijFu-HfxONM\',  # celebahq-classifier-24-mouth-slightly-open.pkl\n    \'https://drive.google.com/uc?id=1RkC8JLqLosWMaRne3DARRgolhbtg_wnr\',  # celebahq-classifier-25-mustache.pkl\n    \'https://drive.google.com/uc?id=1RqtbtFT2EuwpGTqsTYJDyXdnDsFCPtLO\',  # celebahq-classifier-26-narrow-eyes.pkl\n    \'https://drive.google.com/uc?id=1Rs7hU-re8bBMeRHR-fKgMbjPh-RIbrsh\',  # celebahq-classifier-27-no-beard.pkl\n    \'https://drive.google.com/uc?id=1RynDJQWdGOAGffmkPVCrLJqy_fciPF9E\',  # celebahq-classifier-28-oval-face.pkl\n    \'https://drive.google.com/uc?id=1S0TZ_Hdv5cb06NDaCD8NqVfKy7MuXZsN\',  # celebahq-classifier-29-pale-skin.pkl\n    \'https://drive.google.com/uc?id=1S3JPhZH2B4gVZZYCWkxoRP11q09PjCkA\',  # celebahq-classifier-30-pointy-nose.pkl\n    \'https://drive.google.com/uc?id=1S3pQuUz-Jiywq_euhsfezWfGkfzLZ87W\',  # celebahq-classifier-31-receding-hairline.pkl\n    \'https://drive.google.com/uc?id=1S6nyIl_SEI3M4l748xEdTV2vymB_-lrY\',  # celebahq-classifier-32-rosy-cheeks.pkl\n    \'https://drive.google.com/uc?id=1S9P5WCi3GYIBPVYiPTWygrYIUSIKGxbU\',  # celebahq-classifier-33-sideburns.pkl\n    \'https://drive.google.com/uc?id=1SANviG-pp08n7AFpE9wrARzozPIlbfCH\',  # celebahq-classifier-34-straight-hair.pkl\n    \'https://drive.google.com/uc?id=1SArgyMl6_z7P7coAuArqUC2zbmckecEY\',  # celebahq-classifier-35-wearing-earrings.pkl\n    \'https://drive.google.com/uc?id=1SC5JjS5J-J4zXFO9Vk2ZU2DT82TZUza_\',  # celebahq-classifier-36-wearing-hat.pkl\n    \'https://drive.google.com/uc?id=1SDAQWz03HGiu0MSOKyn7gvrp3wdIGoj-\',  # celebahq-classifier-37-wearing-lipstick.pkl\n    \'https://drive.google.com/uc?id=1SEtrVK-TQUC0XeGkBE9y7L8VXfbchyKX\',  # celebahq-classifier-38-wearing-necklace.pkl\n    \'https://drive.google.com/uc?id=1SF_mJIdyGINXoV-I6IAxHB_k5dxiF6M-\',  # celebahq-classifier-39-wearing-necktie.pkl\n]\n\nclassifier_id_name = {\n    0: ""gender"",\n    1: ""smiling"",\n    2: ""attractive"",\n    3: ""wavy-hair"",\n    4: ""young"",\n    5: ""5-o-clock-shadow"",\n    6: ""arched-eyebrows"",\n    7: ""bags-under-eyes"",\n    8: ""bald"",\n    9: ""bangs"",\n    10: ""big-lips"",\n    11: ""big-nose"",\n    12: ""black-hair"",\n    13: ""blond-hair"",\n    14: ""blurry"",\n    15: ""brown-hair"",\n    16: ""bushy-eyebrows"",\n    17: ""chubby"",\n    18: ""double-chin"",\n    19: ""eyeglasses"",\n    20: ""goatee"",\n    21: ""gray-hair"",\n    22: ""heavy-makeup"",\n    23: ""high-cheekbones"",\n    24: ""mouth-slightly-open"",\n    25: ""mustache"",\n    26: ""narrow-eyes"",\n    27: ""no-beard"",\n    28: ""oval-face"",\n    29: ""pale-skin"",\n    30: ""pointy-nose"",\n    31: ""receding-hairline"",\n    32: ""rosy-cheeks"",\n    33: ""sideburns"",\n    34: ""straight-hair"",\n    35: ""wearing-earrings"",\n    36: ""wearing-hat"",\n    37: ""wearing-lipstick"",\n    38: ""wearing-necklace"",\n    39: ""wearing-necktie"",\n}\n\n\ndef make_classifier(attrib_idx):\n    classifier = load_pkl(classifier_urls[attrib_idx])\n    return classifier\n\n'"
principal_directions/extract_attributes.py,4,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom dataloader import *\nfrom checkpointer import Checkpointer\nfrom dlutils.pytorch import count_parameters\nfrom defaults import get_cfg_defaults\nfrom model import Model\nfrom launcher import run\nfrom net import *\nimport numpy as np\nimport tensorflow as tf\nimport principal_directions.classifier\n\n\ndef parse_tfrecord_np(record):\n    ex = tf.train.Example()\n    ex.ParseFromString(record)\n    shape = ex.features.feature[\'shape\'].int64_list.value\n    data = ex.features.feature[\'data\'].bytes_list.value[0]\n    dlat = ex.features.feature[\'dlat\'].bytes_list.value[0]\n    lat = ex.features.feature[\'lat\'].bytes_list.value[0]\n    return np.fromstring(data, np.uint8).reshape(shape), np.fromstring(dlat, np.float32), np.fromstring(lat, np.float32)\n\n\nclass Predictions:\n    def __init__(self, cfg, minibatch_gpu):\n        self.minibatch_size = minibatch_gpu\n        self.cfg = cfg\n\n    def evaluate(self, logger, mapping, decoder, lod, attrib_idx):\n        result_expr = []\n\n        rnd = np.random.RandomState(5)\n\n        with tf.Graph().as_default(), tf.Session() as sess:\n            ds = tf.data.TFRecordDataset(""principal_directions/generated_data.000"")\n            ds = ds.batch(self.minibatch_size)\n            batch = ds.make_one_shot_iterator().get_next()\n\n            classifier = principal_directions.classifier.make_classifier(attrib_idx)\n            i = 0\n            while True:\n                try:\n                    records = sess.run(batch)\n                    images = []\n                    dlats = []\n                    lats = []\n                    for r in records:\n                        im, dlat, lat = parse_tfrecord_np(r)\n\n                        # plt.imshow(im.transpose(1, 2, 0), interpolation=\'nearest\')\n                        # plt.show()\n\n                        images.append(im)\n                        dlats.append(dlat)\n                        lats.append(lat)\n                    images = np.stack(images)\n                    dlats = np.stack(dlats)\n                    lats = np.stack(lats)\n                    logits = classifier.run(images, None, num_gpus=1, assume_frozen=True)\n                    logits = torch.tensor(logits)\n                    predictions = torch.softmax(torch.cat([logits, -logits], dim=1), dim=1)\n\n                    result_dict = dict(latents=lats, dlatents=dlats)\n                    result_dict[attrib_idx] = predictions.cpu().numpy()\n                    result_expr.append(result_dict)\n                    i += 1\n                except tf.errors.OutOfRangeError:\n                    break\n\n        results = {key: np.concatenate([value[key] for value in result_expr], axis=0) for key in result_expr[0].keys()}\n\n        np.save(""principal_directions/wspace_att_%d"" % attrib_idx, results)\n\n\ndef main(cfg, logger):\n    torch.cuda.set_device(0)\n    model = Model(\n        startf=cfg.MODEL.START_CHANNEL_COUNT,\n        layer_count=cfg.MODEL.LAYER_COUNT,\n        maxf=cfg.MODEL.MAX_CHANNEL_COUNT,\n        latent_size=cfg.MODEL.LATENT_SPACE_SIZE,\n        truncation_psi=cfg.MODEL.TRUNCATIOM_PSI,\n        truncation_cutoff=cfg.MODEL.TRUNCATIOM_CUTOFF,\n        mapping_layers=cfg.MODEL.MAPPING_LAYERS,\n        channels=cfg.MODEL.CHANNELS,\n        generator=cfg.MODEL.GENERATOR,\n        encoder=cfg.MODEL.ENCODER)\n\n    model.cuda(0)\n    model.eval()\n    model.requires_grad_(False)\n\n    decoder = model.decoder\n    encoder = model.encoder\n    mapping_tl = model.mapping_tl\n    mapping_fl = model.mapping_fl\n    dlatent_avg = model.dlatent_avg\n\n    logger.info(""Trainable parameters generator:"")\n    count_parameters(decoder)\n\n    logger.info(""Trainable parameters discriminator:"")\n    count_parameters(encoder)\n\n    arguments = dict()\n    arguments[""iteration""] = 0\n\n    model_dict = {\n        \'discriminator_s\': encoder,\n        \'generator_s\': decoder,\n        \'mapping_tl_s\': mapping_tl,\n        \'mapping_fl_s\': mapping_fl,\n        \'dlatent_avg\': dlatent_avg\n    }\n\n    checkpointer = Checkpointer(cfg,\n                                model_dict,\n                                {},\n                                logger=logger,\n                                save=False)\n\n    checkpointer.load()\n\n    model.eval()\n\n    layer_count = cfg.MODEL.LAYER_COUNT\n\n    logger.info(""Extracting attributes"")\n\n    decoder = nn.DataParallel(decoder)\n\n    indices = [0, 1, 2, 3, 4, 10, 11, 17, 19]\n    with torch.no_grad():\n        p = Predictions(cfg, minibatch_gpu=4)\n        for i in indices:\n            p.evaluate(logger, mapping_fl, decoder, cfg.DATASET.MAX_RESOLUTION_LEVEL - 2, i)\n\n\nif __name__ == ""__main__"":\n    gpu_count = 1\n    run(main, get_cfg_defaults(), description=\'StyleGAN\', default_config=\'configs/ffhq.yaml\',\n        world_size=gpu_count, write_log=False)\n'"
principal_directions/find_principal_directions.py,0,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport numpy as np\nimport sklearn.svm\nimport multiprocessing as mp\n\nindices = [0, 1, 2, 3, 4, 10, 11, 17, 19]\n\n\ndef run(attrib_idx):\n    results = np.load(""principal_directions/wspace_att_%d.npy"" % attrib_idx).item()\n\n    pruned_indices = list(range(results[\'latents\'].shape[0]))\n    # pruned_indices = sorted(pruned_indices, key=lambda i: -np.max(results[attrib_idx][i]))\n    # keep = int(results[\'latents\'].shape[0] * 0.95)\n    # print(\'Keeping: %d\' % keep)\n    # pruned_indices = pruned_indices[:keep]\n\n    # Fit SVM to the remaining samples.\n    svm_targets = np.argmax(results[attrib_idx][pruned_indices], axis=1)\n    space = \'dlatents\'\n\n    svm_inputs = results[space][pruned_indices]\n\n    svm = sklearn.svm.LinearSVC(C=1.0, dual=False, max_iter=10000)\n    svm.fit(svm_inputs, svm_targets)\n    svm.score(svm_inputs, svm_targets)\n    svm_outputs = svm.predict(svm_inputs)\n\n    w = svm.coef_[0]\n\n    np.save(""principal_directions/direction_%d"" % attrib_idx, w)\n\n\np = mp.Pool(processes=4)\np.map(run, indices)\n'"
principal_directions/generate_images.py,5,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom dataloader import *\nfrom checkpointer import Checkpointer\nfrom dlutils.pytorch import count_parameters\nfrom defaults import get_cfg_defaults\nfrom model import Model\nfrom tqdm import tqdm\nfrom launcher import run\nfrom net import *\nimport numpy as np\nimport tensorflow as tf\n\n\nclass ImageGenerator:\n    def __init__(self, cfg, num_samples, minibatch_gpu):\n        self.num_samples = num_samples\n        self.minibatch_size = minibatch_gpu\n        self.cfg = cfg\n\n    def evaluate(self, logger, mapping, decoder, lod):\n        tfr_opt = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.NONE)\n        tfr_writer = tf.python_io.TFRecordWriter(""principal_directions/generated_data.000"", tfr_opt)\n\n        rnd = np.random.RandomState(5)\n\n        for _ in tqdm(range(0, self.num_samples, self.minibatch_size)):\n            torch.cuda.set_device(0)\n            latents = rnd.randn(self.minibatch_size, self.cfg.MODEL.LATENT_SPACE_SIZE)\n            lat = torch.tensor(latents).float().cuda()\n\n            dlat = mapping(lat)\n            images = decoder(dlat, lod, 1.0, True)\n\n            # Downsample to 256x256. The attribute classifiers were built for 256x256.\n            factor = images.shape[2] // 256\n            if factor != 1:\n                images = torch.nn.functional.avg_pool2d(images, factor, factor)\n            images = np.clip((images.cpu().numpy() + 1.0) * 127, 0, 255).astype(np.uint8)\n\n            for i, img in enumerate(images):\n                ex = tf.train.Example(features=tf.train.Features(feature={\n                    \'shape\': tf.train.Feature(int64_list=tf.train.Int64List(value=img.shape)),\n                    \'data\': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img.tostring()])),\n                    \'lat\': tf.train.Feature(bytes_list=tf.train.BytesList(value=[lat[i].cpu().numpy().tostring()])),\n                    \'dlat\': tf.train.Feature(bytes_list=tf.train.BytesList(value=[dlat[i, 0].cpu().numpy().tostring()]))}))\n                tfr_writer.write(ex.SerializeToString())\n\n\ndef sample(cfg, logger):\n    torch.cuda.set_device(0)\n    model = Model(\n        startf=cfg.MODEL.START_CHANNEL_COUNT,\n        layer_count=cfg.MODEL.LAYER_COUNT,\n        maxf=cfg.MODEL.MAX_CHANNEL_COUNT,\n        latent_size=cfg.MODEL.LATENT_SPACE_SIZE,\n        truncation_psi=cfg.MODEL.TRUNCATIOM_PSI,\n        truncation_cutoff=cfg.MODEL.TRUNCATIOM_CUTOFF,\n        mapping_layers=cfg.MODEL.MAPPING_LAYERS,\n        channels=cfg.MODEL.CHANNELS,\n        generator=cfg.MODEL.GENERATOR,\n        encoder=cfg.MODEL.ENCODER)\n\n    model.cuda()\n    model.eval()\n    model.requires_grad_(False)\n\n    decoder = model.decoder\n    encoder = model.encoder\n    mapping_tl = model.mapping_tl\n    mapping_fl = model.mapping_fl\n    dlatent_avg = model.dlatent_avg\n\n    logger.info(""Trainable parameters generator:"")\n    count_parameters(decoder)\n\n    logger.info(""Trainable parameters discriminator:"")\n    count_parameters(encoder)\n\n    arguments = dict()\n    arguments[""iteration""] = 0\n\n    model_dict = {\n        \'discriminator_s\': encoder,\n        \'generator_s\': decoder,\n        \'mapping_tl_s\': mapping_tl,\n        \'mapping_fl_s\': mapping_fl,\n        \'dlatent_avg\': dlatent_avg\n    }\n\n    checkpointer = Checkpointer(cfg,\n                                model_dict,\n                                {},\n                                logger=logger,\n                                save=False)\n\n    checkpointer.load()\n\n    model.eval()\n\n    layer_count = cfg.MODEL.LAYER_COUNT\n\n    logger.info(""Generating..."")\n\n    decoder = nn.DataParallel(decoder)\n    mapping_fl = nn.DataParallel(mapping_fl)\n\n    with torch.no_grad():\n        gen = ImageGenerator(cfg, num_samples=60000, minibatch_gpu=8)\n        gen.evaluate(logger, mapping_fl, decoder, cfg.DATASET.MAX_RESOLUTION_LEVEL - 2)\n\n\nif __name__ == ""__main__"":\n    gpu_count = 1\n    run(sample, get_cfg_defaults(), description=\'ALAE-generate-images-for-attribute-classifications\',\n        default_config=\'configs/ffhq.yaml\',\n        world_size=gpu_count, write_log=False)\n'"
style_mixing/stylemix.py,13,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport torch.utils.data\nfrom torchvision.utils import save_image\nfrom net import *\nfrom model import Model\nfrom launcher import run\nfrom checkpointer import Checkpointer\nfrom dlutils.pytorch import count_parameters\nfrom defaults import get_cfg_defaults\nimport lreq\n\nfrom PIL import Image\n\nlreq.use_implicit_lreq.set(True)\n\nsrc_len = 5\ndst_len = 6\n\n\ndef place(canvas, image, x, y):\n    image = image.cpu().detach().numpy()\n    im_size = image.shape[1]\n    canvas[:, y * im_size: (y + 1) * im_size, x * im_size: (x + 1) * im_size] = image * 0.5 + 0.5\n\n\ndef main(cfg, logger):\n    with torch.no_grad():\n        _main(cfg, logger)\n\n\ndef _main(cfg, logger):\n    torch.cuda.set_device(0)\n    model = Model(\n        startf=cfg.MODEL.START_CHANNEL_COUNT,\n        layer_count=cfg.MODEL.LAYER_COUNT,\n        maxf=cfg.MODEL.MAX_CHANNEL_COUNT,\n        latent_size=cfg.MODEL.LATENT_SPACE_SIZE,\n        truncation_psi=cfg.MODEL.TRUNCATIOM_PSI,\n        truncation_cutoff=cfg.MODEL.TRUNCATIOM_CUTOFF,\n        mapping_layers=cfg.MODEL.MAPPING_LAYERS,\n        channels=cfg.MODEL.CHANNELS,\n        generator=cfg.MODEL.GENERATOR,\n        encoder=cfg.MODEL.ENCODER)\n    model.cuda(0)\n    model.eval()\n    model.requires_grad_(False)\n\n    decoder = model.decoder\n    encoder = model.encoder\n    mapping_tl = model.mapping_tl\n    mapping_fl = model.mapping_fl\n    dlatent_avg = model.dlatent_avg\n\n    logger.info(""Trainable parameters generator:"")\n    count_parameters(decoder)\n\n    logger.info(""Trainable parameters discriminator:"")\n    count_parameters(encoder)\n\n    arguments = dict()\n    arguments[""iteration""] = 0\n\n    model_dict = {\n        \'discriminator_s\': encoder,\n        \'generator_s\': decoder,\n        \'mapping_tl_s\': mapping_tl,\n        \'mapping_fl_s\': mapping_fl,\n        \'dlatent_avg\': dlatent_avg\n    }\n\n    checkpointer = Checkpointer(cfg,\n                                model_dict,\n                                {},\n                                logger=logger,\n                                save=False)\n\n    extra_checkpoint_data = checkpointer.load()\n    last_epoch = list(extra_checkpoint_data[\'auxiliary\'][\'scheduler\'].values())[0][\'last_epoch\']\n    logger.info(""Model trained for %d epochs"" % last_epoch)\n\n    model.eval()\n\n    layer_count = cfg.MODEL.LAYER_COUNT\n\n    def encode(x):\n        layer_count = cfg.MODEL.LAYER_COUNT\n\n        zlist = []\n        for i in range(x.shape[0]):\n            Z, _ = model.encode(x[i][None, ...], layer_count - 1, 1)\n            zlist.append(Z)\n        Z = torch.cat(zlist)\n        Z = Z.repeat(1, model.mapping_fl.num_layers, 1)\n        return Z\n\n    def decode(x):\n        decoded = []\n        for i in range(x.shape[0]):\n            r = model.decoder(x[i][None, ...], layer_count - 1, 1, noise=True)\n            decoded.append(r)\n        return torch.cat(decoded)\n\n    path = cfg.DATASET.STYLE_MIX_PATH\n    im_size = 2 ** (cfg.MODEL.LAYER_COUNT + 1)\n\n    src_originals = []\n    for i in range(src_len):\n        try:\n            im = np.asarray(Image.open(os.path.join(path, \'src/%d.png\' % i)))\n        except FileNotFoundError:\n            im = np.asarray(Image.open(os.path.join(path, \'src/%d.jpg\' % i)))\n        im = im.transpose((2, 0, 1))\n        x = torch.tensor(np.asarray(im, dtype=np.float32), requires_grad=True).cuda() / 127.5 - 1.\n        if x.shape[0] == 4:\n            x = x[:3]\n        factor = x.shape[2] // im_size\n        if factor != 1:\n            x = torch.nn.functional.avg_pool2d(x[None, ...], factor, factor)[0]\n        assert x.shape[2] == im_size\n        src_originals.append(x)\n    src_originals = torch.stack([x for x in src_originals])\n    dst_originals = []\n    for i in range(dst_len):\n        try:\n            im = np.asarray(Image.open(os.path.join(path, \'dst/%d.png\' % i)))\n        except FileNotFoundError:\n            im = np.asarray(Image.open(os.path.join(path, \'dst/%d.jpg\' % i)))\n        im = im.transpose((2, 0, 1))\n        x = torch.tensor(np.asarray(im, dtype=np.float32), requires_grad=True).cuda() / 127.5 - 1.\n        if x.shape[0] == 4:\n            x = x[:3]\n        factor = x.shape[2] // im_size\n        if factor != 1:\n            x = torch.nn.functional.avg_pool2d(x[None, ...], factor, factor)[0]\n        assert x.shape[2] == im_size\n        dst_originals.append(x)\n    dst_originals = torch.stack([x for x in dst_originals])\n\n    src_latents = encode(src_originals)\n    src_images = decode(src_latents)\n\n    dst_latents = encode(dst_originals)\n    dst_images = decode(dst_latents)\n\n    canvas = np.zeros([3, im_size * (dst_len + 1), im_size * (src_len + 1)])\n\n    os.makedirs(\'style_mixing/output/%s/\' % cfg.NAME, exist_ok=True)\n\n    for i in range(src_len):\n        save_image(src_originals[i] * 0.5 + 0.5, \'style_mixing/output/%s/source_%d.png\' % (cfg.NAME, i))\n        place(canvas, src_originals[i], 1 + i, 0)\n\n    for i in range(dst_len):\n        save_image(dst_originals[i] * 0.5 + 0.5, \'style_mixing/output/%s/dst_coarse_%d.png\' % (cfg.NAME, i))\n        place(canvas, dst_originals[i], 0, 1 + i)\n\n    style_ranges = [range(0, 4)] * 3 + [range(4, 8)] * 2 + [range(8, layer_count * 2)]\n\n    def mix_styles(style_src, style_dst, r):\n        style = style_dst.clone()\n        style[:, r] = style_src[:, r]\n        return style\n\n    for row in range(dst_len):\n        row_latents = torch.stack([dst_latents[row]] * src_len)\n        style = mix_styles(src_latents, row_latents, style_ranges[row])\n        rec = model.decoder(style, layer_count - 1, 1, noise=True)\n        for j in range(rec.shape[0]):\n            save_image(rec[j] * 0.5 + 0.5, \'style_mixing/output/%s/rec_coarse_%d_%d.png\' % (cfg.NAME, row, j))\n            place(canvas, rec[j], 1 + j, 1 + row)\n\n    save_image(torch.Tensor(canvas), \'style_mixing/output/%s/stylemix.png\' % cfg.NAME)\n\n\nif __name__ == ""__main__"":\n    gpu_count = 1\n    run(main, get_cfg_defaults(), description=\'ALAE-style-mixing\', default_config=\'configs/ffhq.yaml\',\n        world_size=gpu_count, write_log=False)\n'"
training_artifacts/download_all.py,0,"b'import dlutils\nfrom packaging import version\n\n\nif not hasattr(dlutils, ""__version__"") or version.parse(dlutils.__version__) < version.parse(""0.0.11""):\n    raise RuntimeError(\'Please update dlutils: pip install dlutils --upgrade\')\n\ntry:\n    dlutils.download.from_google_drive(\'170Qldnn28IwnVm9CQEq1AZhVsK7PJ0Xz\', directory=\'training_artifacts/ffhq\')\n    dlutils.download.from_google_drive(\'1QESywJW8N-g3n0Csy0clztuJV99g8pRm\', directory=\'training_artifacts/ffhq\')\n    dlutils.download.from_google_drive(\'18BzFYKS3icFd1DQKKTeje7CKbEKXPVug\', directory=\'training_artifacts/ffhq\')\nexcept IOError:\n    dlutils.download.from_url(\'https://alaeweights.s3.us-east-2.amazonaws.com/ffhq/model_submitted.pth\', directory=\'training_artifacts/ffhq\')\n    dlutils.download.from_url(\'https://alaeweights.s3.us-east-2.amazonaws.com/ffhq/model_194.pth\', directory=\'training_artifacts/ffhq\')\n    dlutils.download.from_url(\'https://alaeweights.s3.us-east-2.amazonaws.com/ffhq/model_157.pth\', directory=\'training_artifacts/ffhq\')\n\ntry:\n    dlutils.download.from_google_drive(\'1T4gkE7-COHpX38qPwjMYO-xU-SrY_aT4\', directory=\'training_artifacts/celeba\')\nexcept IOError:\n    dlutils.download.from_url(\'https://alaeweights.s3.us-east-2.amazonaws.com/celeba/model_final.pth\', directory=\'training_artifacts/celeba\')\n\ntry:\n    dlutils.download.from_google_drive(\'1gmYbc6Z8qJHJwICYDsB4aBMxXjnKeXA_\', directory=\'training_artifacts/bedroom\')\nexcept IOError:\n    dlutils.download.from_url(\'https://alaeweights.s3.us-east-2.amazonaws.com/bedroom/model_final.pth\', directory=\'training_artifacts/bedroom\')\n\ntry:\n    dlutils.download.from_google_drive(\'1ihJvp8iJWcLxTIjkV5cyA7l9TrxlUPkG\', directory=\'training_artifacts/celeba-hq256\')\n    dlutils.download.from_google_drive(\'1gFQsGCNKo-frzKmA3aCvx07ShRymRIKZ\', directory=\'training_artifacts/celeba-hq256\')\nexcept IOError:\n    dlutils.download.from_url(\'https://alaeweights.s3.us-east-2.amazonaws.com/celeba-hq256/model_262r.pth\', directory=\'training_artifacts/celeba-hq256\')\n    dlutils.download.from_url(\'https://alaeweights.s3.us-east-2.amazonaws.com/celeba-hq256/model_580r.pth\', directory=\'training_artifacts/celeba-hq256\')\n'"
make_figures/old/make_recon_figure_bed.py,12,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport torch.utils.data\nfrom torchvision.utils import save_image\nimport random\nfrom net import *\nfrom model import Model\nfrom launcher import run\nfrom checkpointer import Checkpointer\nfrom dlutils.pytorch import count_parameters\nfrom defaults import get_cfg_defaults\nimport lreq\n\nfrom PIL import Image\n\n\nlreq.use_implicit_lreq.set(True)\n\nim_size = 256\n\n\ndef place(canvas, image, x, y):\n    im_size = image.shape[2]\n    if len(image.shape) == 4:\n        image = image[0]\n    canvas[:, y: y + im_size, x: x + im_size] = image * 0.5 + 0.5\n\n\ndef save_sample(model, sample, i):\n    os.makedirs(\'results\', exist_ok=True)\n\n    with torch.no_grad():\n        model.eval()\n        x_rec = model.generate(model.generator.layer_count - 1, 1, z=sample)\n\n        def save_pic(x_rec):\n            resultsample = x_rec * 0.5 + 0.5\n            resultsample = resultsample.cpu()\n            save_image(resultsample,\n                       \'sample_%i_lr.png\' % i, nrow=16)\n\n        save_pic(x_rec)\n\n\ndef sample(cfg, logger):\n    torch.cuda.set_device(0)\n    model = Model(\n        startf=cfg.MODEL.START_CHANNEL_COUNT,\n        layer_count=cfg.MODEL.LAYER_COUNT,\n        maxf=cfg.MODEL.MAX_CHANNEL_COUNT,\n        latent_size=cfg.MODEL.LATENT_SPACE_SIZE,\n        truncation_psi=cfg.MODEL.TRUNCATIOM_PSI,\n        truncation_cutoff=cfg.MODEL.TRUNCATIOM_CUTOFF,\n        mapping_layers=cfg.MODEL.MAPPING_LAYERS,\n        channels=cfg.MODEL.CHANNELS,\n        generator=cfg.MODEL.GENERATOR,\n        encoder=cfg.MODEL.ENCODER)\n    model.cuda(0)\n    model.eval()\n    model.requires_grad_(False)\n\n    decoder = model.decoder\n    encoder = model.encoder\n    mapping_tl = model.mapping_tl\n    mapping_fl = model.mapping_fl\n    dlatent_avg = model.dlatent_avg\n\n    logger.info(""Trainable parameters generator:"")\n    count_parameters(decoder)\n\n    logger.info(""Trainable parameters discriminator:"")\n    count_parameters(encoder)\n\n    arguments = dict()\n    arguments[""iteration""] = 0\n\n    model_dict = {\n        \'discriminator_s\': encoder,\n        \'generator_s\': decoder,\n        \'mapping_tl_s\': mapping_tl,\n        \'mapping_fl_s\': mapping_fl,\n        \'dlatent_avg\': dlatent_avg\n    }\n\n    checkpointer = Checkpointer(cfg,\n                                model_dict,\n                                {},\n                                logger=logger,\n                                save=False)\n\n    extra_checkpoint_data = checkpointer.load()\n\n    model.eval()\n\n    layer_count = cfg.MODEL.LAYER_COUNT\n\n    def encode(x):\n        Z, _ = model.encode(x, layer_count - 1, 1)\n        Z = Z.repeat(1, model.mapping_fl.num_layers, 1)\n        return Z\n\n    def decode(x):\n        layer_idx = torch.arange(2 * cfg.MODEL.LAYER_COUNT)[np.newaxis, :, np.newaxis]\n        ones = torch.ones(layer_idx.shape, dtype=torch.float32)\n        coefs = torch.where(layer_idx < model.truncation_cutoff, ones, ones)\n        # x = torch.lerp(model.dlatent_avg.buff.data, x, coefs)\n        return model.decoder(x, layer_count - 1, 1, noise=True)\n\n    rnd = np.random.RandomState(5)\n    latents = rnd.randn(1, cfg.MODEL.LATENT_SPACE_SIZE)\n\n    path = cfg.DATASET.SAMPLES_PATH\n\n    paths = list(os.listdir(path))\n\n    paths = sorted(paths)\n    random.seed(3456)\n    random.shuffle(paths)\n\n    def make(paths):\n        canvas = []\n        with torch.no_grad():\n            for filename in paths:\n                img = np.asarray(Image.open(path + \'/\' + filename))\n                if img.shape[2] == 4:\n                    img = img[:, :, :3]\n                im = img.transpose((2, 0, 1))\n                x = torch.tensor(np.asarray(im, dtype=np.float32), device=\'cpu\', requires_grad=True).cuda() / 127.5 - 1.\n                if x.shape[0] == 4:\n                    x = x[:3]\n                latents = encode(x[None, ...].cuda())\n                f = decode(latents)\n                r = torch.cat([x[None, ...].detach().cpu(), f.detach().cpu()], dim=3)\n                canvas.append(r)\n        return canvas\n\n    canvas = make(paths[:40])\n    canvas = torch.cat(canvas, dim=0)\n\n    save_image(canvas * 0.5 + 0.5, \'make_figures/output/reconstructions_bed_1.png\', nrow=4, pad_value=1.0)\n\n    canvas = make(paths[40:80])\n    canvas = torch.cat(canvas, dim=0)\n\n    save_image(canvas * 0.5 + 0.5, \'make_figures/output/reconstructions_bed_2.png\', nrow=4, pad_value=1.0)\n\n\nif __name__ == ""__main__"":\n    gpu_count = 1\n    run(sample, get_cfg_defaults(), description=\'ALAE-reconstruction-bedroom\', default_config=\'configs/bedroom.yaml\',\n        world_size=gpu_count, write_log=False)\n'"
make_figures/old/make_recon_figure_celeba.py,12,"b'# Copyright 2019-2020 Stanislav Pidhorskyi\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport torch.utils.data\nfrom torchvision.utils import save_image\nimport random\nfrom net import *\nfrom model import Model\nfrom launcher import run\nfrom checkpointer import Checkpointer\nfrom dlutils.pytorch import count_parameters\nfrom defaults import get_cfg_defaults\nimport lreq\n\nfrom PIL import Image\n\n\nlreq.use_implicit_lreq.set(True)\n\nim_size = 256\n\n\ndef place(canvas, image, x, y):\n    im_size = image.shape[2]\n    if len(image.shape) == 4:\n        image = image[0]\n    canvas[:, y: y + im_size, x: x + im_size] = image * 0.5 + 0.5\n\n\ndef save_sample(model, sample, i):\n    os.makedirs(\'results\', exist_ok=True)\n\n    with torch.no_grad():\n        model.eval()\n        x_rec = model.generate(model.generator.layer_count - 1, 1, z=sample)\n\n        def save_pic(x_rec):\n            resultsample = x_rec * 0.5 + 0.5\n            resultsample = resultsample.cpu()\n            save_image(resultsample,\n                       \'sample_%i_lr.png\' % i, nrow=16)\n\n        save_pic(x_rec)\n\n\ndef sample(cfg, logger):\n    torch.cuda.set_device(0)\n    model = Model(\n        startf=cfg.MODEL.START_CHANNEL_COUNT,\n        layer_count=cfg.MODEL.LAYER_COUNT,\n        maxf=cfg.MODEL.MAX_CHANNEL_COUNT,\n        latent_size=cfg.MODEL.LATENT_SPACE_SIZE,\n        truncation_psi=cfg.MODEL.TRUNCATIOM_PSI,\n        truncation_cutoff=cfg.MODEL.TRUNCATIOM_CUTOFF,\n        mapping_layers=cfg.MODEL.MAPPING_LAYERS,\n        channels=cfg.MODEL.CHANNELS,\n        generator=cfg.MODEL.GENERATOR,\n        encoder=cfg.MODEL.ENCODER)\n    model.cuda(0)\n    model.eval()\n    model.requires_grad_(False)\n\n    decoder = model.decoder\n    encoder = model.encoder\n    mapping_tl = model.mapping_tl\n    mapping_fl = model.mapping_fl\n    dlatent_avg = model.dlatent_avg\n\n    logger.info(""Trainable parameters generator:"")\n    count_parameters(decoder)\n\n    logger.info(""Trainable parameters discriminator:"")\n    count_parameters(encoder)\n\n    arguments = dict()\n    arguments[""iteration""] = 0\n\n    model_dict = {\n        \'discriminator_s\': encoder,\n        \'generator_s\': decoder,\n        \'mapping_tl_s\': mapping_tl,\n        \'mapping_fl_s\': mapping_fl,\n        \'dlatent_avg\': dlatent_avg\n    }\n\n    checkpointer = Checkpointer(cfg,\n                                model_dict,\n                                {},\n                                logger=logger,\n                                save=False)\n\n    extra_checkpoint_data = checkpointer.load()\n\n    model.eval()\n\n    layer_count = cfg.MODEL.LAYER_COUNT\n\n    def encode(x):\n        Z, _ = model.encode(x, layer_count - 1, 1)\n        Z = Z.repeat(1, model.mapping_fl.num_layers, 1)\n        return Z\n\n    def decode(x):\n        layer_idx = torch.arange(2 * cfg.MODEL.LAYER_COUNT)[np.newaxis, :, np.newaxis]\n        ones = torch.ones(layer_idx.shape, dtype=torch.float32)\n        coefs = torch.where(layer_idx < model.truncation_cutoff, ones, ones)\n        # x = torch.lerp(model.dlatent_avg.buff.data, x, coefs)\n        return model.decoder(x, layer_count - 1, 1, noise=True)\n\n    rnd = np.random.RandomState(5)\n    latents = rnd.randn(1, cfg.MODEL.LATENT_SPACE_SIZE)\n\n    path = cfg.DATASET.SAMPLES_PATH\n\n    paths = list(os.listdir(path))\n\n    paths = sorted(paths)\n    random.seed(3456)\n    random.shuffle(paths)\n\n    def make(paths):\n        canvas = []\n        with torch.no_grad():\n            for filename in paths:\n                img = np.asarray(Image.open(path + \'/\' + filename))\n                if img.shape[2] == 4:\n                    img = img[:, :, :3]\n                im = img.transpose((2, 0, 1))\n\n                x = torch.tensor(np.asarray(im, dtype=np.float32), device=\'cpu\', requires_grad=True).cuda() / 127.5 - 1.\n                if x.shape[0] == 4:\n                    x = x[:3]\n\n                while x.shape[2] != model.decoder.layer_to_resolution[6]:\n                    x = F.avg_pool2d(x, 2, 2)\n\n                latents = encode(x[None, ...].cuda())\n                f = decode(latents)\n                r = torch.cat([x[None, ...].detach().cpu(), f.detach().cpu()], dim=3)\n                canvas.append(r)\n        return canvas\n\n    canvas = make(paths[:10])\n    canvas = torch.cat(canvas, dim=0)\n\n    save_image(canvas * 0.5 + 0.5, \'make_figures/output/reconstructions_celeba_1.png\', nrow=2, pad_value=1.0)\n\n    canvas = make(paths[10:20])\n    canvas = torch.cat(canvas, dim=0)\n\n    save_image(canvas * 0.5 + 0.5, \'make_figures/output/reconstructions_celeba_2.png\', nrow=2, pad_value=1.0)\n\n\nif __name__ == ""__main__"":\n    gpu_count = 1\n    run(sample, get_cfg_defaults(), description=\'ALAE-reconstruction-bedroom\', default_config=\'configs/celeba-hq256.yaml\',\n        world_size=gpu_count, write_log=False)\n'"
