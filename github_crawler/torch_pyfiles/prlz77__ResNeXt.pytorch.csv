file_path,api_count,code
test.py,5,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division\n\n"""""" \nTest the ResNeXt Model on Cifar10 and Cifar 100. Implementation as defined in:\n\nXie, S., Girshick, R., Doll\xc3\xa1r, P., Tu, Z., & He, K. (2016). \nAggregated residual transformations for deep neural networks. \narXiv preprint arXiv:1611.05431.\n\n""""""\n\n__author__ = ""Pau Rodr\xc3\xadguez L\xc3\xb3pez, ISELAB, CVC-UAB""\n__email__ = ""pau.rodri1@gmail.com""\n\n__editor__ = ""Il-Ji Choi, Vuno. Inc."" # test file\n__editor_email__ = ""choiilji@gmail.com""\n\nimport argparse\nimport torch\nimport torch.nn.functional as F\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nfrom models.model import CifarResNeXt\n\ndef get_args():\n    parser = argparse.ArgumentParser(description=\'Test ResNeXt on CIFAR\', \n            formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    # Positional arguments\n    parser.add_argument(\'data_path\', type=str, help=\'Root for the Cifar dataset.\')\n    parser.add_argument(\'dataset\', type=str, choices=[\'cifar10\', \'cifar100\'], help=\'Choose between Cifar10/100.\')\n    # Optimization options\n    parser.add_argument(\'--batch_size\', \'-b\', type=int, default=128, help=\'Batch size.\')\n    parser.add_argument(\'--test_bs\', type=int, default=10)\n    # Checkpoints\n    parser.add_argument(\'--load\', \'-l\', type=str, help=\'Checkpoint path to resume / test.\')\n    parser.add_argument(\'--test\', \'-t\', action=\'store_true\', help=\'Test only flag.\')\n    # Architecture\n    parser.add_argument(\'--depth\', type=int, default=29, help=\'Model depth.\')\n    parser.add_argument(\'--cardinality\', type=int, default=8, help=\'Model cardinality (group).\')\n    parser.add_argument(\'--base_width\', type=int, default=64, help=\'Number of channels in each group.\')\n    parser.add_argument(\'--widen_factor\', type=int, default=4, help=\'Widen factor. 4 -> 64, 8 -> 128, ...\')\n    # Acceleration\n    parser.add_argument(\'--ngpu\', type=int, default=1, help=\'0 = CPU.\')\n    parser.add_argument(\'--prefetch\', type=int, default=2, help=\'Pre-fetching threads.\')\n    # i/o\n    parser.add_argument(\'--log\', type=str, default=\'./\', help=\'Log folder.\')\n    args = parser.parse_args()\n    return args\n\ndef test():\n    # define default variables\n    args = get_args()# divide args part and call it as function\n    mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n    std = [x / 255 for x in [63.0, 62.1, 66.7]]\n    state = {k: v for k, v in args._get_kwargs()}\n\n    # prepare test data parts\n    test_transform = transforms.Compose(\n        [transforms.ToTensor(), transforms.Normalize(mean, std)])\n    test_data = dset.CIFAR100(args.data_path, train=False, transform=test_transform, download=True)\n    if args.dataset == \'cifar10\':\n        test_data = dset.CIFAR10(args.data_path, train=False, transform=test_transform, download=True)\n        nlabels = 10\n    else:\n        test_data = dset.CIFAR100(args.data_path, train=False, transform=test_transform, download=True)\n        nlabels = 100\n\n    test_loader = torch.utils.data.DataLoader(test_data, batch_size=args.test_bs, shuffle=False,\n                                              num_workers=args.prefetch, pin_memory=True)\n\n    # initialize model and load from checkpoint\n    net = CifarResNeXt(args.cardinality, args.depth, nlabels, args.base_width, args.widen_factor)\n    loaded_state_dict = torch.load(args.load)\n    temp = {}\n    for key, val in list(loaded_state_dict.items()):\n        # parsing keys for ignoring \'module.\' in keys\n        temp[key[7:]] = val\n    loaded_state_dict = temp\n    net.load_state_dict(loaded_state_dict)\n\n    # paralleize model \n    if args.ngpu > 1:\n        net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))\n    if args.ngpu > 0:\n        net.cuda()\n   \n    # use network for evaluation \n    net.eval()\n\n    # calculation part\n    loss_avg = 0.0\n    correct = 0.0\n    for batch_idx, (data, target) in enumerate(test_loader):\n        data, target = torch.autograd.Variable(data.cuda()), torch.autograd.Variable(target.cuda())\n\n        # forward\n        output = net(data)\n        loss = F.cross_entropy(output, target)\n\n        # accuracy\n        pred = output.data.max(1)[1]\n        correct += pred.eq(target.data).sum()\n\n        # test loss average\n        loss_avg += loss.data[0]\n\n    state[\'test_loss\'] = loss_avg / len(test_loader)\n    state[\'test_accuracy\'] = correct / len(test_loader.dataset)\n\n    # finally print state dictionary\n    print(state)\n\nif __name__==\'__main__\':\n    test()\n'"
train.py,8,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division\n\n"""""" \nTrains a ResNeXt Model on Cifar10 and Cifar 100. Implementation as defined in:\n\nXie, S., Girshick, R., Doll\xc3\xa1r, P., Tu, Z., & He, K. (2016). \nAggregated residual transformations for deep neural networks. \narXiv preprint arXiv:1611.05431.\n\n""""""\n\n__author__ = ""Pau Rodr\xc3\xadguez L\xc3\xb3pez, ISELAB, CVC-UAB""\n__email__ = ""pau.rodri1@gmail.com""\n\nimport argparse\nimport os\nimport json\nimport torch\nimport torch.nn.functional as F\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nfrom models.model import CifarResNeXt\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'Trains ResNeXt on CIFAR\', \n            formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    # Positional arguments\n    parser.add_argument(\'data_path\', type=str, help=\'Root for the Cifar dataset.\')\n    parser.add_argument(\'dataset\', type=str, choices=[\'cifar10\', \'cifar100\'], help=\'Choose between Cifar10/100.\')\n    # Optimization options\n    parser.add_argument(\'--epochs\', \'-e\', type=int, default=300, help=\'Number of epochs to train.\')\n    parser.add_argument(\'--batch_size\', \'-b\', type=int, default=128, help=\'Batch size.\')\n    parser.add_argument(\'--learning_rate\', \'-lr\', type=float, default=0.1, help=\'The Learning Rate.\')\n    parser.add_argument(\'--momentum\', \'-m\', type=float, default=0.9, help=\'Momentum.\')\n    parser.add_argument(\'--decay\', \'-d\', type=float, default=0.0005, help=\'Weight decay (L2 penalty).\')\n    parser.add_argument(\'--test_bs\', type=int, default=10)\n    parser.add_argument(\'--schedule\', type=int, nargs=\'+\', default=[150, 225],\n                        help=\'Decrease learning rate at these epochs.\')\n    parser.add_argument(\'--gamma\', type=float, default=0.1, help=\'LR is multiplied by gamma on schedule.\')\n    # Checkpoints\n    parser.add_argument(\'--save\', \'-s\', type=str, default=\'./\', help=\'Folder to save checkpoints.\')\n    parser.add_argument(\'--load\', \'-l\', type=str, help=\'Checkpoint path to resume / test.\')\n    parser.add_argument(\'--test\', \'-t\', action=\'store_true\', help=\'Test only flag.\')\n    # Architecture\n    parser.add_argument(\'--depth\', type=int, default=29, help=\'Model depth.\')\n    parser.add_argument(\'--cardinality\', type=int, default=8, help=\'Model cardinality (group).\')\n    parser.add_argument(\'--base_width\', type=int, default=64, help=\'Number of channels in each group.\')\n    parser.add_argument(\'--widen_factor\', type=int, default=4, help=\'Widen factor. 4 -> 64, 8 -> 128, ...\')\n    # Acceleration\n    parser.add_argument(\'--ngpu\', type=int, default=1, help=\'0 = CPU.\')\n    parser.add_argument(\'--prefetch\', type=int, default=2, help=\'Pre-fetching threads.\')\n    # i/o\n    parser.add_argument(\'--log\', type=str, default=\'./\', help=\'Log folder.\')\n    args = parser.parse_args()\n\n    # Init logger\n    if not os.path.isdir(args.log):\n        os.makedirs(args.log)\n    log = open(os.path.join(args.log, \'log.txt\'), \'w\')\n    state = {k: v for k, v in args._get_kwargs()}\n    log.write(json.dumps(state) + \'\\n\')\n\n    # Calculate number of epochs wrt batch size\n    args.epochs = args.epochs * 128 // args.batch_size\n    args.schedule = [x * 128 // args.batch_size for x in args.schedule]\n\n    # Init dataset\n    if not os.path.isdir(args.data_path):\n        os.makedirs(args.data_path)\n\n    mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n    std = [x / 255 for x in [63.0, 62.1, 66.7]]\n\n    train_transform = transforms.Compose(\n        [transforms.RandomHorizontalFlip(), transforms.RandomCrop(32, padding=4), transforms.ToTensor(),\n         transforms.Normalize(mean, std)])\n    test_transform = transforms.Compose(\n        [transforms.ToTensor(), transforms.Normalize(mean, std)])\n    if args.dataset == \'cifar10\':\n        train_data = dset.CIFAR10(args.data_path, train=True, transform=train_transform, download=True)\n        test_data = dset.CIFAR10(args.data_path, train=False, transform=test_transform, download=True)\n        nlabels = 10\n    else:\n        train_data = dset.CIFAR100(args.data_path, train=True, transform=train_transform, download=True)\n        test_data = dset.CIFAR100(args.data_path, train=False, transform=test_transform, download=True)\n        nlabels = 100\n    train_loader = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size, shuffle=True,\n                                               num_workers=args.prefetch, pin_memory=True)\n    test_loader = torch.utils.data.DataLoader(test_data, batch_size=args.test_bs, shuffle=False,\n                                              num_workers=args.prefetch, pin_memory=True)\n\n    # Init checkpoints\n    if not os.path.isdir(args.save):\n        os.makedirs(args.save)\n\n    # Init model, criterion, and optimizer\n    net = CifarResNeXt(args.cardinality, args.depth, nlabels, args.base_width, args.widen_factor)\n    print(net)\n    if args.ngpu > 1:\n        net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))\n\n    if args.ngpu > 0:\n        net.cuda()\n\n    optimizer = torch.optim.SGD(net.parameters(), state[\'learning_rate\'], momentum=state[\'momentum\'],\n                                weight_decay=state[\'decay\'], nesterov=True)\n\n    # train function (forward, backward, update)\n    def train():\n        net.train()\n        loss_avg = 0.0\n        for batch_idx, (data, target) in enumerate(train_loader):\n            data, target = torch.autograd.Variable(data.cuda()), torch.autograd.Variable(target.cuda())\n\n            # forward\n            output = net(data)\n\n            # backward\n            optimizer.zero_grad()\n            loss = F.cross_entropy(output, target)\n            loss.backward()\n            optimizer.step()\n\n            # exponential moving average\n            loss_avg = loss_avg * 0.2 + float(loss) * 0.8\n\n        state[\'train_loss\'] = loss_avg\n\n\n    # test function (forward only)\n    def test():\n        net.eval()\n        loss_avg = 0.0\n        correct = 0\n        for batch_idx, (data, target) in enumerate(test_loader):\n            data, target = torch.autograd.Variable(data.cuda()), torch.autograd.Variable(target.cuda())\n\n            # forward\n            output = net(data)\n            loss = F.cross_entropy(output, target)\n\n            # accuracy\n            pred = output.data.max(1)[1]\n            correct += float(pred.eq(target.data).sum())\n\n            # test loss average\n            loss_avg += float(loss)\n\n        state[\'test_loss\'] = loss_avg / len(test_loader)\n        state[\'test_accuracy\'] = correct / len(test_loader.dataset)\n\n\n    # Main loop\n    best_accuracy = 0.0\n    for epoch in range(args.epochs):\n        if epoch in args.schedule:\n            state[\'learning_rate\'] *= args.gamma\n            for param_group in optimizer.param_groups:\n                param_group[\'lr\'] = state[\'learning_rate\']\n\n        state[\'epoch\'] = epoch\n        train()\n        test()\n        if state[\'test_accuracy\'] > best_accuracy:\n            best_accuracy = state[\'test_accuracy\']\n            torch.save(net.state_dict(), os.path.join(args.save, \'model.pytorch\'))\n        log.write(\'%s\\n\' % json.dumps(state))\n        log.flush()\n        print(state)\n        print(""Best accuracy: %f"" % best_accuracy)\n\n    log.close()\n'"
models/__init__.py,0,b''
models/model.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division\n\n"""""" \nCreates a ResNeXt Model as defined in:\n\nXie, S., Girshick, R., Doll\xc3\xa1r, P., Tu, Z., & He, K. (2016). \nAggregated residual transformations for deep neural networks. \narXiv preprint arXiv:1611.05431.\n\n""""""\n\n__author__ = ""Pau Rodr\xc3\xadguez L\xc3\xb3pez, ISELAB, CVC-UAB""\n__email__ = ""pau.rodri1@gmail.com""\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import init\n\n\nclass ResNeXtBottleneck(nn.Module):\n    """"""\n    RexNeXt bottleneck type C (https://github.com/facebookresearch/ResNeXt/blob/master/models/resnext.lua)\n    """"""\n\n    def __init__(self, in_channels, out_channels, stride, cardinality, base_width, widen_factor):\n        """""" Constructor\n\n        Args:\n            in_channels: input channel dimensionality\n            out_channels: output channel dimensionality\n            stride: conv stride. Replaces pooling layer.\n            cardinality: num of convolution groups.\n            base_width: base number of channels in each group.\n            widen_factor: factor to reduce the input dimensionality before convolution.\n        """"""\n        super(ResNeXtBottleneck, self).__init__()\n        width_ratio = out_channels / (widen_factor * 64.)\n        D = cardinality * int(base_width * width_ratio)\n        self.conv_reduce = nn.Conv2d(in_channels, D, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn_reduce = nn.BatchNorm2d(D)\n        self.conv_conv = nn.Conv2d(D, D, kernel_size=3, stride=stride, padding=1, groups=cardinality, bias=False)\n        self.bn = nn.BatchNorm2d(D)\n        self.conv_expand = nn.Conv2d(D, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn_expand = nn.BatchNorm2d(out_channels)\n\n        self.shortcut = nn.Sequential()\n        if in_channels != out_channels:\n            self.shortcut.add_module(\'shortcut_conv\',\n                                     nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, padding=0,\n                                               bias=False))\n            self.shortcut.add_module(\'shortcut_bn\', nn.BatchNorm2d(out_channels))\n\n    def forward(self, x):\n        bottleneck = self.conv_reduce.forward(x)\n        bottleneck = F.relu(self.bn_reduce.forward(bottleneck), inplace=True)\n        bottleneck = self.conv_conv.forward(bottleneck)\n        bottleneck = F.relu(self.bn.forward(bottleneck), inplace=True)\n        bottleneck = self.conv_expand.forward(bottleneck)\n        bottleneck = self.bn_expand.forward(bottleneck)\n        residual = self.shortcut.forward(x)\n        return F.relu(residual + bottleneck, inplace=True)\n\n\nclass CifarResNeXt(nn.Module):\n    """"""\n    ResNext optimized for the Cifar dataset, as specified in\n    https://arxiv.org/pdf/1611.05431.pdf\n    """"""\n\n    def __init__(self, cardinality, depth, nlabels, base_width, widen_factor=4):\n        """""" Constructor\n\n        Args:\n            cardinality: number of convolution groups.\n            depth: number of layers.\n            nlabels: number of classes\n            base_width: base number of channels in each group.\n            widen_factor: factor to adjust the channel dimensionality\n        """"""\n        super(CifarResNeXt, self).__init__()\n        self.cardinality = cardinality\n        self.depth = depth\n        self.block_depth = (self.depth - 2) // 9\n        self.base_width = base_width\n        self.widen_factor = widen_factor\n        self.nlabels = nlabels\n        self.output_size = 64\n        self.stages = [64, 64 * self.widen_factor, 128 * self.widen_factor, 256 * self.widen_factor]\n\n        self.conv_1_3x3 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)\n        self.bn_1 = nn.BatchNorm2d(64)\n        self.stage_1 = self.block(\'stage_1\', self.stages[0], self.stages[1], 1)\n        self.stage_2 = self.block(\'stage_2\', self.stages[1], self.stages[2], 2)\n        self.stage_3 = self.block(\'stage_3\', self.stages[2], self.stages[3], 2)\n        self.classifier = nn.Linear(self.stages[3], nlabels)\n        init.kaiming_normal(self.classifier.weight)\n\n        for key in self.state_dict():\n            if key.split(\'.\')[-1] == \'weight\':\n                if \'conv\' in key:\n                    init.kaiming_normal(self.state_dict()[key], mode=\'fan_out\')\n                if \'bn\' in key:\n                    self.state_dict()[key][...] = 1\n            elif key.split(\'.\')[-1] == \'bias\':\n                self.state_dict()[key][...] = 0\n\n    def block(self, name, in_channels, out_channels, pool_stride=2):\n        """""" Stack n bottleneck modules where n is inferred from the depth of the network.\n\n        Args:\n            name: string name of the current block.\n            in_channels: number of input channels\n            out_channels: number of output channels\n            pool_stride: factor to reduce the spatial dimensionality in the first bottleneck of the block.\n\n        Returns: a Module consisting of n sequential bottlenecks.\n\n        """"""\n        block = nn.Sequential()\n        for bottleneck in range(self.block_depth):\n            name_ = \'%s_bottleneck_%d\' % (name, bottleneck)\n            if bottleneck == 0:\n                block.add_module(name_, ResNeXtBottleneck(in_channels, out_channels, pool_stride, self.cardinality,\n                                                          self.base_width, self.widen_factor))\n            else:\n                block.add_module(name_,\n                                 ResNeXtBottleneck(out_channels, out_channels, 1, self.cardinality, self.base_width,\n                                                   self.widen_factor))\n        return block\n\n    def forward(self, x):\n        x = self.conv_1_3x3.forward(x)\n        x = F.relu(self.bn_1.forward(x), inplace=True)\n        x = self.stage_1.forward(x)\n        x = self.stage_2.forward(x)\n        x = self.stage_3.forward(x)\n        x = F.avg_pool2d(x, 8, 1)\n        x = x.view(-1, self.stages[3])\n        return self.classifier(x)\n'"
utils/plot_log.py,0,"b'import re\nimport matplotlib.pyplot as plt\n\nif __name__==\'__main__\':\n    file = open(\'./logs/log.txt\',\'r\')\n    accuracy = []\n    epochs = []\n    loss = []\n    for line in file:\n        test_accuracy = re.search(\'""test_accuracy"": ([0]\\.[0-9]+)*\', line)\n        if test_accuracy:\n            accuracy.append(test_accuracy.group(1))\n        \n        epoch = re.search(\'""epoch"": ([0-9]+)*\', line)\n        if epoch:\n            epochs.append(epoch.group(1))\n        \n        train_loss = re.search(\'""train_loss"": ([0-9]\\.[0-9]+)*\', line)\n        if train_loss:\n            loss.append(train_loss.group(1))\n    file.close()\n    plt.figure(\'test_accuracy vs epochs\')  \n    plt.xlabel(\'epoch\')\n    plt.ylabel(\'test_accuracy\')\n    plt.plot(epochs,accuracy,\'b*\')\n    plt.plot(epochs,accuracy,\'r\')\n    plt.grid(True)\n\n    plt.figure(\'train_loss vs epochs\')\n    plt.xlabel(\'epoch\')\n    plt.ylabel(\'train_loss\')\n    plt.plot(epochs,loss,\'b*\')\n    plt.plot(epochs,loss,\'y\')\n    plt.grid(True)\n\n    plt.show()\n    \n'"
