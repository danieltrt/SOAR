file_path,api_count,code
hubconf.py,5,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2019-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\nimport os\nimport pathlib\nimport typing\nimport functools\nimport shutil\nimport zipfile\nimport torch\nimport yaml\nfrom pyannote.audio.features import Pretrained as _Pretrained\nfrom pyannote.pipeline import Pipeline as _Pipeline\n\ndependencies = [\'pyannote.audio\', \'torch\']\n\n_HUB_REPO = \'https://github.com/pyannote/pyannote-audio-hub\'\n_ZIP_URL = f\'{_HUB_REPO}/raw/master/{{kind}}s/{{name}}.zip\'\n_PRETRAINED_URL = f\'{_HUB_REPO}/raw/master/pretrained.yml\'\n\n# path where pre-trained models and pipelines are downloaded and cached\n_HUB_DIR = pathlib.Path(os.environ.get(""PYANNOTE_AUDIO_HUB"",\n                                       ""~/.pyannote/hub"")).expanduser().resolve()\n\n# download pretrained.yml if needed\n_PRETRAINED_YML = _HUB_DIR / \'pretrained.yml\'\n\nif not _PRETRAINED_YML.exists():\n    msg = (\n        f\'Downloading list of pretrained models and pipelines \'\n        f\'to ""{_PRETRAINED_YML}"".\'\n    )\n    print(msg)\n    from pyannote.audio.utils.path import mkdir_p\n    mkdir_p(_PRETRAINED_YML.parent)\n    torch.hub.download_url_to_file(_PRETRAINED_URL,\n                                   _PRETRAINED_YML,\n                                   progress=True)\n\ndef _generic(name: str,\n             duration: float = None,\n             step: float = 0.25,\n             batch_size: int = 32,\n             device: typing.Optional[typing.Union[typing.Text, torch.device]] = None,\n             pipeline: typing.Optional[bool] = None,\n             force_reload: bool = False) -> typing.Union[_Pretrained, _Pipeline]:\n    """"""Load pretrained model or pipeline\n\n    Parameters\n    ----------\n    name : str\n        Name of pretrained model or pipeline\n    duration : float, optional\n        Override audio chunks duration.\n        Defaults to the one used during training.\n    step : float, optional\n        Ratio of audio chunk duration used for the internal sliding window.\n        Defaults to 0.25 (i.e. 75% overlap between two consecutive windows).\n        Reducing this value might lead to better results (at the expense of\n        slower processing).\n    batch_size : int, optional\n        Batch size used for inference. Defaults to 32.\n    device : torch.device, optional\n        Device used for inference.\n    pipeline : bool, optional\n        Wrap pretrained model in a (not fully optimized) pipeline.\n    force_reload : bool\n        Whether to discard the existing cache and force a fresh download.\n        Defaults to use existing cache.\n\n    Returns\n    -------\n    pretrained: `Pretrained` or `Pipeline`\n\n    Usage\n    -----\n    >>> sad_pipeline = torch.hub.load(\'pyannote/pyannote-audio\', \'sad_ami\')\n    >>> scores = model({\'audio\': \'/path/to/audio.wav\'})\n    """"""\n\n    model_exists = name in _MODELS\n    pipeline_exists = name in _PIPELINES\n\n    if model_exists and pipeline_exists:\n\n        if pipeline is None:\n            msg = (\n                f\'Both a pretrained model and a pretrained pipeline called \'\n                f\'""{name}"" are available. Use option ""pipeline=True"" to \'\n                f\'load the pipeline, and ""pipeline=False"" to load the model.\')\n            raise ValueError(msg)\n\n        if pipeline:\n            kind = \'pipeline\'\n            zip_url = _ZIP_URL.format(kind=kind, name=name)\n            sha256 = _PIPELINES[name]\n            return_pipeline = True\n\n        else:\n            kind = \'model\'\n            zip_url = _ZIP_URL.format(kind=kind, name=name)\n            sha256 = _MODELS[name]\n            return_pipeline = False\n\n    elif pipeline_exists:\n\n        if pipeline is None:\n            pipeline = True\n\n        if not pipeline:\n            msg = (\n                f\'Could not find any pretrained ""{name}"" model. \'\n                f\'A pretrained ""{name}"" pipeline does exist. \'\n                f\'Did you mean ""pipeline=True""?\'\n            )\n            raise ValueError(msg)\n\n        kind = \'pipeline\'\n        zip_url = _ZIP_URL.format(kind=kind, name=name)\n        sha256 = _PIPELINES[name]\n        return_pipeline = True\n\n    elif model_exists:\n\n        if pipeline is None:\n            pipeline = False\n\n        kind = \'model\'\n        zip_url = _ZIP_URL.format(kind=kind, name=name)\n        sha256 = _MODELS[name]\n        return_pipeline = pipeline\n\n        if name.startswith(\'emb_\') and return_pipeline:\n            msg = (\n                f\'Pretrained model ""{name}"" has no associated pipeline. Use \'\n                f\'""pipeline=False"" or remove ""pipeline"" option altogether.\'\n            )\n            raise ValueError(msg)\n\n    else:\n        msg = (\n            f\'Could not find any pretrained model nor pipeline called ""{name}"".\'\n        )\n        raise ValueError(msg)\n\n    if sha256 is None:\n        msg = (\n            f\'Pretrained {kind} ""{name}"" is not available yet but will be \'\n            f\'released shortly. Stay tuned...\'\n        )\n        raise NotImplementedError(msg)\n\n    pretrained_dir = _HUB_DIR / f\'{kind}s\'\n    pretrained_subdir = pretrained_dir / f\'{name}\'\n    pretrained_zip = pretrained_dir / f\'{name}.zip\'\n\n    if not pretrained_subdir.exists() or force_reload:\n\n        if pretrained_subdir.exists():\n            shutil.rmtree(pretrained_subdir)\n\n        from pyannote.audio.utils.path import mkdir_p\n        mkdir_p(pretrained_zip.parent)\n        try:\n            msg = (\n                f\'Downloading pretrained {kind} ""{name}"" to ""{pretrained_zip}"".\'\n            )\n            print(msg)\n            torch.hub.download_url_to_file(zip_url,\n                                           pretrained_zip,\n                                           hash_prefix=sha256,\n                                           progress=True)\n        except RuntimeError as e:\n            shutil.rmtree(pretrained_subdir)\n            msg = (\n                f\'Failed to download pretrained {kind} ""{name}"".\'\n                f\'Please try again.\')\n            raise RuntimeError(msg)\n\n        # unzip downloaded file\n        with zipfile.ZipFile(pretrained_zip) as z:\n            z.extractall(path=pretrained_dir)\n\n    if kind == \'model\':\n\n        params_yml, = pretrained_subdir.glob(\'*/*/*/*/params.yml\')\n        pretrained =  _Pretrained(validate_dir=params_yml.parent,\n                                  duration=duration,\n                                  step=step,\n                                  batch_size=batch_size,\n                                  device=device)\n\n        if return_pipeline:\n            if name.startswith(\'sad_\'):\n                from pyannote.audio.pipeline.speech_activity_detection import SpeechActivityDetection\n                pipeline = SpeechActivityDetection(scores=pretrained)\n            elif name.startswith(\'scd_\'):\n                from pyannote.audio.pipeline.speaker_change_detection import SpeakerChangeDetection\n                pipeline = SpeakerChangeDetection(scores=pretrained)\n            elif name.startswith(\'ovl_\'):\n                from pyannote.audio.pipeline.overlap_detection import OverlapDetection\n                pipeline = OverlapDetection(scores=pretrained)\n            else:\n                # this should never happen\n                msg = (\n                    f\'Pretrained model ""{name}"" has no associated pipeline. Use \'\n                    f\'""pipeline=False"" or remove ""pipeline"" option altogether.\'\n                )\n                raise ValueError(msg)\n\n            return pipeline.load_params(params_yml)\n\n        return pretrained\n\n    elif kind == \'pipeline\':\n\n        from pyannote.audio.pipeline.utils import load_pretrained_pipeline\n        params_yml, *_ = pretrained_subdir.glob(\'*/*/params.yml\')\n        return load_pretrained_pipeline(params_yml.parent)\n\nwith open(_PRETRAINED_YML, \'r\') as fp:\n    _pretrained = yaml.load(fp, Loader=yaml.SafeLoader)\n\n_MODELS = _pretrained[\'models\']\nfor name in _MODELS:\n    locals()[name] = functools.partial(_generic, name)\n\n_PIPELINES = _pretrained[\'pipelines\']\nfor name in _PIPELINES:\n    locals()[name] = functools.partial(_generic, name)\n\n_SHORTCUTS = _pretrained[\'shortcuts\']\nfor shortcut, name in _SHORTCUTS.items():\n    locals()[shortcut] = locals()[name]\n'"
setup.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2016-2019 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n\nimport versioneer\n\nfrom setuptools import setup, find_packages\n\nwith open(""README.md"") as f:\n    long_description = f.read()\n\nwith open(""requirements.txt"") as f:\n    requirements = f.read().splitlines()\n\nsetup(\n    name=""pyannote.audio"",\n    namespace_packages=[""pyannote""],\n    packages=find_packages(),\n    install_requires=requirements,\n    entry_points={\n        ""console_scripts"": [\n            ""pyannote-audio=pyannote.audio.applications.pyannote_audio:main"",\n            ""pyannote-speech-feature=pyannote.audio.applications.feature_extraction:main"",\n        ],\n        ""prodigy_recipes"": [\n            ""pyannote.sad.manual = pyannote.audio.interactive.recipes.sad:sad_manual"",\n            ""pyannote.dia.binary = pyannote.audio.interactive.recipes.dia:dia_binary"",\n            ""pyannote.dia.manual = pyannote.audio.interactive.recipes.dia:dia_manual"",\n        ],\n    },\n    version=versioneer.get_version(),\n    cmdclass=versioneer.get_cmdclass(),\n    description=""Neural building blocks for speaker diarization"",\n    long_description=long_description,\n    long_description_content_type=""text/markdown"",\n    author=""Herv\xc3\xa9 Bredin"",\n    author_email=""bredin@limsi.fr"",\n    url=""https://github.com/pyannote/pyannote-audio"",\n    classifiers=[\n        ""Development Status :: 4 - Beta"",\n        ""Intended Audience :: Science/Research"",\n        ""License :: OSI Approved :: MIT License"",\n        ""Natural Language :: English"",\n        ""Programming Language :: Python :: 3"",\n        ""Programming Language :: Python :: 3.7"",\n        ""Programming Language :: Python :: 3.8"",\n        ""Topic :: Scientific/Engineering"",\n    ],\n)\n'"
versioneer.py,0,"b'\n# Version: 0.15\n\n""""""\nThe Versioneer\n==============\n\n* like a rocketeer, but for versions!\n* https://github.com/warner/python-versioneer\n* Brian Warner\n* License: Public Domain\n* Compatible With: python2.6, 2.7, 3.2, 3.3, 3.4, and pypy\n* [![Latest Version]\n(https://pypip.in/version/versioneer/badge.svg?style=flat)\n](https://pypi.python.org/pypi/versioneer/)\n* [![Build Status]\n(https://travis-ci.org/warner/python-versioneer.png?branch=master)\n](https://travis-ci.org/warner/python-versioneer)\n\nThis is a tool for managing a recorded version number in distutils-based\npython projects. The goal is to remove the tedious and error-prone ""update\nthe embedded version string"" step from your release process. Making a new\nrelease should be as easy as recording a new tag in your version-control\nsystem, and maybe making new tarballs.\n\n\n## Quick Install\n\n* `pip install versioneer` to somewhere to your $PATH\n* add a `[versioneer]` section to your setup.cfg (see below)\n* run `versioneer install` in your source tree, commit the results\n\n## Version Identifiers\n\nSource trees come from a variety of places:\n\n* a version-control system checkout (mostly used by developers)\n* a nightly tarball, produced by build automation\n* a snapshot tarball, produced by a web-based VCS browser, like github\'s\n  ""tarball from tag"" feature\n* a release tarball, produced by ""setup.py sdist"", distributed through PyPI\n\nWithin each source tree, the version identifier (either a string or a number,\nthis tool is format-agnostic) can come from a variety of places:\n\n* ask the VCS tool itself, e.g. ""git describe"" (for checkouts), which knows\n  about recent ""tags"" and an absolute revision-id\n* the name of the directory into which the tarball was unpacked\n* an expanded VCS keyword ($Id$, etc)\n* a `_version.py` created by some earlier build step\n\nFor released software, the version identifier is closely related to a VCS\ntag. Some projects use tag names that include more than just the version\nstring (e.g. ""myproject-1.2"" instead of just ""1.2""), in which case the tool\nneeds to strip the tag prefix to extract the version identifier. For\nunreleased software (between tags), the version identifier should provide\nenough information to help developers recreate the same tree, while also\ngiving them an idea of roughly how old the tree is (after version 1.2, before\nversion 1.3). Many VCS systems can report a description that captures this,\nfor example `git describe --tags --dirty --always` reports things like\n""0.7-1-g574ab98-dirty"" to indicate that the checkout is one revision past the\n0.7 tag, has a unique revision id of ""574ab98"", and is ""dirty"" (it has\nuncommitted changes.\n\nThe version identifier is used for multiple purposes:\n\n* to allow the module to self-identify its version: `myproject.__version__`\n* to choose a name and prefix for a \'setup.py sdist\' tarball\n\n## Theory of Operation\n\nVersioneer works by adding a special `_version.py` file into your source\ntree, where your `__init__.py` can import it. This `_version.py` knows how to\ndynamically ask the VCS tool for version information at import time.\n\n`_version.py` also contains `$Revision$` markers, and the installation\nprocess marks `_version.py` to have this marker rewritten with a tag name\nduring the `git archive` command. As a result, generated tarballs will\ncontain enough information to get the proper version.\n\nTo allow `setup.py` to compute a version too, a `versioneer.py` is added to\nthe top level of your source tree, next to `setup.py` and the `setup.cfg`\nthat configures it. This overrides several distutils/setuptools commands to\ncompute the version when invoked, and changes `setup.py build` and `setup.py\nsdist` to replace `_version.py` with a small static file that contains just\nthe generated version data.\n\n## Installation\n\nFirst, decide on values for the following configuration variables:\n\n* `VCS`: the version control system you use. Currently accepts ""git"".\n\n* `style`: the style of version string to be produced. See ""Styles"" below for\n  details. Defaults to ""pep440"", which looks like\n  `TAG[+DISTANCE.gSHORTHASH[.dirty]]`.\n\n* `versionfile_source`:\n\n  A project-relative pathname into which the generated version strings should\n  be written. This is usually a `_version.py` next to your project\'s main\n  `__init__.py` file, so it can be imported at runtime. If your project uses\n  `src/myproject/__init__.py`, this should be `src/myproject/_version.py`.\n  This file should be checked in to your VCS as usual: the copy created below\n  by `setup.py setup_versioneer` will include code that parses expanded VCS\n  keywords in generated tarballs. The \'build\' and \'sdist\' commands will\n  replace it with a copy that has just the calculated version string.\n\n  This must be set even if your project does not have any modules (and will\n  therefore never import `_version.py`), since ""setup.py sdist"" -based trees\n  still need somewhere to record the pre-calculated version strings. Anywhere\n  in the source tree should do. If there is a `__init__.py` next to your\n  `_version.py`, the `setup.py setup_versioneer` command (described below)\n  will append some `__version__`-setting assignments, if they aren\'t already\n  present.\n\n* `versionfile_build`:\n\n  Like `versionfile_source`, but relative to the build directory instead of\n  the source directory. These will differ when your setup.py uses\n  \'package_dir=\'. If you have `package_dir={\'myproject\': \'src/myproject\'}`,\n  then you will probably have `versionfile_build=\'myproject/_version.py\'` and\n  `versionfile_source=\'src/myproject/_version.py\'`.\n\n  If this is set to None, then `setup.py build` will not attempt to rewrite\n  any `_version.py` in the built tree. If your project does not have any\n  libraries (e.g. if it only builds a script), then you should use\n  `versionfile_build = None` and override `distutils.command.build_scripts`\n  to explicitly insert a copy of `versioneer.get_version()` into your\n  generated script.\n\n* `tag_prefix`:\n\n  a string, like \'PROJECTNAME-\', which appears at the start of all VCS tags.\n  If your tags look like \'myproject-1.2.0\', then you should use\n  tag_prefix=\'myproject-\'. If you use unprefixed tags like \'1.2.0\', this\n  should be an empty string.\n\n* `parentdir_prefix`:\n\n  a optional string, frequently the same as tag_prefix, which appears at the\n  start of all unpacked tarball filenames. If your tarball unpacks into\n  \'myproject-1.2.0\', this should be \'myproject-\'. To disable this feature,\n  just omit the field from your `setup.cfg`.\n\nThis tool provides one script, named `versioneer`. That script has one mode,\n""install"", which writes a copy of `versioneer.py` into the current directory\nand runs `versioneer.py setup` to finish the installation.\n\nTo versioneer-enable your project:\n\n* 1: Modify your `setup.cfg`, adding a section named `[versioneer]` and\n  populating it with the configuration values you decided earlier (note that\n  the option names are not case-sensitive):\n\n  ````\n  [versioneer]\n  VCS = git\n  style = pep440\n  versionfile_source = src/myproject/_version.py\n  versionfile_build = myproject/_version.py\n  tag_prefix = """"\n  parentdir_prefix = myproject-\n  ````\n\n* 2: Run `versioneer install`. This will do the following:\n\n  * copy `versioneer.py` into the top of your source tree\n  * create `_version.py` in the right place (`versionfile_source`)\n  * modify your `__init__.py` (if one exists next to `_version.py`) to define\n    `__version__` (by calling a function from `_version.py`)\n  * modify your `MANIFEST.in` to include both `versioneer.py` and the\n    generated `_version.py` in sdist tarballs\n\n  `versioneer install` will complain about any problems it finds with your\n  `setup.py` or `setup.cfg`. Run it multiple times until you have fixed all\n  the problems.\n\n* 3: add a `import versioneer` to your setup.py, and add the following\n  arguments to the setup() call:\n\n        version=versioneer.get_version(),\n        cmdclass=versioneer.get_cmdclass(),\n\n* 4: commit these changes to your VCS. To make sure you won\'t forget,\n  `versioneer install` will mark everything it touched for addition using\n  `git add`. Don\'t forget to add `setup.py` and `setup.cfg` too.\n\n## Post-Installation Usage\n\nOnce established, all uses of your tree from a VCS checkout should get the\ncurrent version string. All generated tarballs should include an embedded\nversion string (so users who unpack them will not need a VCS tool installed).\n\nIf you distribute your project through PyPI, then the release process should\nboil down to two steps:\n\n* 1: git tag 1.0\n* 2: python setup.py register sdist upload\n\nIf you distribute it through github (i.e. users use github to generate\ntarballs with `git archive`), the process is:\n\n* 1: git tag 1.0\n* 2: git push; git push --tags\n\nVersioneer will report ""0+untagged.NUMCOMMITS.gHASH"" until your tree has at\nleast one tag in its history.\n\n## Version-String Flavors\n\nCode which uses Versioneer can learn about its version string at runtime by\nimporting `_version` from your main `__init__.py` file and running the\n`get_versions()` function. From the ""outside"" (e.g. in `setup.py`), you can\nimport the top-level `versioneer.py` and run `get_versions()`.\n\nBoth functions return a dictionary with different flavors of version\ninformation:\n\n* `[\'version\']`: A condensed version string, rendered using the selected\n  style. This is the most commonly used value for the project\'s version\n  string. The default ""pep440"" style yields strings like `0.11`,\n  `0.11+2.g1076c97`, or `0.11+2.g1076c97.dirty`. See the ""Styles"" section\n  below for alternative styles.\n\n* `[\'full-revisionid\']`: detailed revision identifier. For Git, this is the\n  full SHA1 commit id, e.g. ""1076c978a8d3cfc70f408fe5974aa6c092c949ac"".\n\n* `[\'dirty\']`: a boolean, True if the tree has uncommitted changes. Note that\n  this is only accurate if run in a VCS checkout, otherwise it is likely to\n  be False or None\n\n* `[\'error\']`: if the version string could not be computed, this will be set\n  to a string describing the problem, otherwise it will be None. It may be\n  useful to throw an exception in setup.py if this is set, to avoid e.g.\n  creating tarballs with a version string of ""unknown"".\n\nSome variants are more useful than others. Including `full-revisionid` in a\nbug report should allow developers to reconstruct the exact code being tested\n(or indicate the presence of local changes that should be shared with the\ndevelopers). `version` is suitable for display in an ""about"" box or a CLI\n`--version` output: it can be easily compared against release notes and lists\nof bugs fixed in various releases.\n\nThe installer adds the following text to your `__init__.py` to place a basic\nversion in `YOURPROJECT.__version__`:\n\n    from ._version import get_versions\n    __version__ = get_versions()[\'version\']\n    del get_versions\n\n## Styles\n\nThe setup.cfg `style=` configuration controls how the VCS information is\nrendered into a version string.\n\nThe default style, ""pep440"", produces a PEP440-compliant string, equal to the\nun-prefixed tag name for actual releases, and containing an additional ""local\nversion"" section with more detail for in-between builds. For Git, this is\nTAG[+DISTANCE.gHEX[.dirty]] , using information from `git describe --tags\n--dirty --always`. For example ""0.11+2.g1076c97.dirty"" indicates that the\ntree is like the ""1076c97"" commit but has uncommitted changes ("".dirty""), and\nthat this commit is two revisions (""+2"") beyond the ""0.11"" tag. For released\nsoftware (exactly equal to a known tag), the identifier will only contain the\nstripped tag, e.g. ""0.11"".\n\nOther styles are available. See details.md in the Versioneer source tree for\ndescriptions.\n\n## Debugging\n\nVersioneer tries to avoid fatal errors: if something goes wrong, it will tend\nto return a version of ""0+unknown"". To investigate the problem, run `setup.py\nversion`, which will run the version-lookup code in a verbose mode, and will\ndisplay the full contents of `get_versions()` (including the `error` string,\nwhich may help identify what went wrong).\n\n## Updating Versioneer\n\nTo upgrade your project to a new release of Versioneer, do the following:\n\n* install the new Versioneer (`pip install -U versioneer` or equivalent)\n* edit `setup.cfg`, if necessary, to include any new configuration settings\n  indicated by the release notes\n* re-run `versioneer install` in your source tree, to replace\n  `SRC/_version.py`\n* commit any changed files\n\n### Upgrading to 0.15\n\nStarting with this version, Versioneer is configured with a `[versioneer]`\nsection in your `setup.cfg` file. Earlier versions required the `setup.py` to\nset attributes on the `versioneer` module immediately after import. The new\nversion will refuse to run (raising an exception during import) until you\nhave provided the necessary `setup.cfg` section.\n\nIn addition, the Versioneer package provides an executable named\n`versioneer`, and the installation process is driven by running `versioneer\ninstall`. In 0.14 and earlier, the executable was named\n`versioneer-installer` and was run without an argument.\n\n### Upgrading to 0.14\n\n0.14 changes the format of the version string. 0.13 and earlier used\nhyphen-separated strings like ""0.11-2-g1076c97-dirty"". 0.14 and beyond use a\nplus-separated ""local version"" section strings, with dot-separated\ncomponents, like ""0.11+2.g1076c97"". PEP440-strict tools did not like the old\nformat, but should be ok with the new one.\n\n### Upgrading from 0.11 to 0.12\n\nNothing special.\n\n### Upgrading from 0.10 to 0.11\n\nYou must add a `versioneer.VCS = ""git""` to your `setup.py` before re-running\n`setup.py setup_versioneer`. This will enable the use of additional\nversion-control systems (SVN, etc) in the future.\n\n## Future Directions\n\nThis tool is designed to make it easily extended to other version-control\nsystems: all VCS-specific components are in separate directories like\nsrc/git/ . The top-level `versioneer.py` script is assembled from these\ncomponents by running make-versioneer.py . In the future, make-versioneer.py\nwill take a VCS name as an argument, and will construct a version of\n`versioneer.py` that is specific to the given VCS. It might also take the\nconfiguration arguments that are currently provided manually during\ninstallation by editing setup.py . Alternatively, it might go the other\ndirection and include code from all supported VCS systems, reducing the\nnumber of intermediate scripts.\n\n\n## License\n\nTo make Versioneer easier to embed, all its code is hereby released into the\npublic domain. The `_version.py` that it creates is also in the public\ndomain.\n\n""""""\n\nfrom __future__ import print_function\ntry:\n    import configparser\nexcept ImportError:\n    import ConfigParser as configparser\nimport errno\nimport json\nimport os\nimport re\nimport subprocess\nimport sys\n\n\nclass VersioneerConfig:\n    pass\n\n\ndef get_root():\n    # we require that all commands are run from the project root, i.e. the\n    # directory that contains setup.py, setup.cfg, and versioneer.py .\n    root = os.path.realpath(os.path.abspath(os.getcwd()))\n    setup_py = os.path.join(root, ""setup.py"")\n    versioneer_py = os.path.join(root, ""versioneer.py"")\n    if not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n        # allow \'python path/to/setup.py COMMAND\'\n        root = os.path.dirname(os.path.realpath(os.path.abspath(sys.argv[0])))\n        setup_py = os.path.join(root, ""setup.py"")\n        versioneer_py = os.path.join(root, ""versioneer.py"")\n    if not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n        err = (""Versioneer was unable to run the project root directory. ""\n               ""Versioneer requires setup.py to be executed from ""\n               ""its immediate directory (like \'python setup.py COMMAND\'), ""\n               ""or in a way that lets it use sys.argv[0] to find the root ""\n               ""(like \'python path/to/setup.py COMMAND\')."")\n        raise VersioneerBadRootError(err)\n    try:\n        # Certain runtime workflows (setup.py install/develop in a setuptools\n        # tree) execute all dependencies in a single python process, so\n        # ""versioneer"" may be imported multiple times, and python\'s shared\n        # module-import table will cache the first one. So we can\'t use\n        # os.path.dirname(__file__), as that will find whichever\n        # versioneer.py was first imported, even in later projects.\n        me = os.path.realpath(os.path.abspath(__file__))\n        if os.path.splitext(me)[0] != os.path.splitext(versioneer_py)[0]:\n            print(""Warning: build in %s is using versioneer.py from %s""\n                  % (os.path.dirname(me), versioneer_py))\n    except NameError:\n        pass\n    return root\n\n\ndef get_config_from_root(root):\n    # This might raise EnvironmentError (if setup.cfg is missing), or\n    # configparser.NoSectionError (if it lacks a [versioneer] section), or\n    # configparser.NoOptionError (if it lacks ""VCS=""). See the docstring at\n    # the top of versioneer.py for instructions on writing your setup.cfg .\n    setup_cfg = os.path.join(root, ""setup.cfg"")\n    parser = configparser.SafeConfigParser()\n    with open(setup_cfg, ""r"") as f:\n        parser.readfp(f)\n    VCS = parser.get(""versioneer"", ""VCS"")  # mandatory\n\n    def get(parser, name):\n        if parser.has_option(""versioneer"", name):\n            return parser.get(""versioneer"", name)\n        return None\n    cfg = VersioneerConfig()\n    cfg.VCS = VCS\n    cfg.style = get(parser, ""style"") or """"\n    cfg.versionfile_source = get(parser, ""versionfile_source"")\n    cfg.versionfile_build = get(parser, ""versionfile_build"")\n    cfg.tag_prefix = get(parser, ""tag_prefix"")\n    cfg.parentdir_prefix = get(parser, ""parentdir_prefix"")\n    cfg.verbose = get(parser, ""verbose"")\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    pass\n\n# these dictionaries contain VCS-specific tools\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    def decorate(f):\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(""unable to run %s"" % dispcmd)\n                print(e)\n            return None\n    else:\n        if verbose:\n            print(""unable to find command, tried %s"" % (commands,))\n        return None\n    stdout = p.communicate()[0].strip()\n    if sys.version_info[0] >= 3:\n        stdout = stdout.decode()\n    if p.returncode != 0:\n        if verbose:\n            print(""unable to run %s (error)"" % dispcmd)\n        return None\n    return stdout\nLONG_VERSION_PY[\'git\'] = \'\'\'\n# This file helps to compute a version number in source trees obtained from\n# git-archive tarball (such as those provided by githubs download-from-tag\n# feature). Distribution tarballs (built by setup.py sdist) and build\n# directories (produced by setup.py build) will contain a much shorter file\n# that just contains the computed version number.\n\n# This file is released into the public domain. Generated by\n# versioneer-0.15 (https://github.com/warner/python-versioneer)\n\nimport errno\nimport os\nimport re\nimport subprocess\nimport sys\n\n\ndef get_keywords():\n    # these strings will be replaced by git during git-archive.\n    # setup.py/versioneer.py will grep for the variable names, so they must\n    # each be defined on a line of their own. _version.py will just call\n    # get_keywords().\n    git_refnames = ""%(DOLLAR)sFormat:%%d%(DOLLAR)s""\n    git_full = ""%(DOLLAR)sFormat:%%H%(DOLLAR)s""\n    keywords = {""refnames"": git_refnames, ""full"": git_full}\n    return keywords\n\n\nclass VersioneerConfig:\n    pass\n\n\ndef get_config():\n    # these strings are filled in when \'setup.py versioneer\' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = ""git""\n    cfg.style = ""%(STYLE)s""\n    cfg.tag_prefix = ""%(TAG_PREFIX)s""\n    cfg.parentdir_prefix = ""%(PARENTDIR_PREFIX)s""\n    cfg.versionfile_source = ""%(VERSIONFILE_SOURCE)s""\n    cfg.verbose = False\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    pass\n\n\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    def decorate(f):\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(""unable to run %%s"" %% dispcmd)\n                print(e)\n            return None\n    else:\n        if verbose:\n            print(""unable to find command, tried %%s"" %% (commands,))\n        return None\n    stdout = p.communicate()[0].strip()\n    if sys.version_info[0] >= 3:\n        stdout = stdout.decode()\n    if p.returncode != 0:\n        if verbose:\n            print(""unable to run %%s (error)"" %% dispcmd)\n        return None\n    return stdout\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    # Source tarballs conventionally unpack into a directory that includes\n    # both the project name and a version string.\n    dirname = os.path.basename(root)\n    if not dirname.startswith(parentdir_prefix):\n        if verbose:\n            print(""guessing rootdir is \'%%s\', but \'%%s\' doesn\'t start with ""\n                  ""prefix \'%%s\'"" %% (root, dirname, parentdir_prefix))\n        raise NotThisMethod(""rootdir doesn\'t start with parentdir_prefix"")\n    return {""version"": dirname[len(parentdir_prefix):],\n            ""full-revisionid"": None,\n            ""dirty"": False, ""error"": None}\n\n\n@register_vcs_handler(""git"", ""get_keywords"")\ndef git_get_keywords(versionfile_abs):\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don\'t want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    try:\n        f = open(versionfile_abs, ""r"")\n        for line in f.readlines():\n            if line.strip().startswith(""git_refnames =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""refnames""] = mo.group(1)\n            if line.strip().startswith(""git_full =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""full""] = mo.group(1)\n        f.close()\n    except EnvironmentError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(""git"", ""keywords"")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    if not keywords:\n        raise NotThisMethod(""no keywords at all, weird"")\n    refnames = keywords[""refnames""].strip()\n    if refnames.startswith(""$Format""):\n        if verbose:\n            print(""keywords are unexpanded, not using"")\n        raise NotThisMethod(""unexpanded keywords, not a git-archive tarball"")\n    refs = set([r.strip() for r in refnames.strip(""()"").split("","")])\n    # starting in git-1.8.3, tags are listed as ""tag: foo-1.0"" instead of\n    # just ""foo-1.0"". If we see a ""tag: "" prefix, prefer those.\n    TAG = ""tag: ""\n    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\n    if not tags:\n        # Either we\'re using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %%d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like ""release"" and\n        # ""stabilization"", as well as ""HEAD"" and ""master"".\n        tags = set([r for r in refs if re.search(r\'\\d\', r)])\n        if verbose:\n            print(""discarding \'%%s\', no digits"" %% "","".join(refs-tags))\n    if verbose:\n        print(""likely tags: %%s"" %% "","".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. ""2.0"" over ""2.0rc1""\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix):]\n            if verbose:\n                print(""picking %%s"" %% r)\n            return {""version"": r,\n                    ""full-revisionid"": keywords[""full""].strip(),\n                    ""dirty"": False, ""error"": None\n                    }\n    # no suitable tags, so version is ""0+unknown"", but full hex is still there\n    if verbose:\n        print(""no suitable tags, using unknown + full revision id"")\n    return {""version"": ""0+unknown"",\n            ""full-revisionid"": keywords[""full""].strip(),\n            ""dirty"": False, ""error"": ""no suitable tags""}\n\n\n@register_vcs_handler(""git"", ""pieces_from_vcs"")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n    # this runs \'git\' from the root of the source tree. This only gets called\n    # if the git-archive \'subst\' keywords were *not* expanded, and\n    # _version.py hasn\'t already been rewritten with a short version string,\n    # meaning we\'re inside a checked out source tree.\n\n    if not os.path.exists(os.path.join(root, "".git"")):\n        if verbose:\n            print(""no .git in %%s"" %% root)\n        raise NotThisMethod(""no .git directory"")\n\n    GITS = [""git""]\n    if sys.platform == ""win32"":\n        GITS = [""git.cmd"", ""git.exe""]\n    # if there is a tag, this yields TAG-NUM-gHEX[-dirty]\n    # if there are no tags, this yields HEX[-dirty] (no NUM)\n    describe_out = run_command(GITS, [""describe"", ""--tags"", ""--dirty"",\n                                      ""--always"", ""--long""],\n                               cwd=root)\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(""\'git describe\' failed"")\n    describe_out = describe_out.strip()\n    full_out = run_command(GITS, [""rev-parse"", ""HEAD""], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(""\'git rev-parse\' failed"")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[""long""] = full_out\n    pieces[""short""] = full_out[:7]  # maybe improved later\n    pieces[""error""] = None\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(""-dirty"")\n    pieces[""dirty""] = dirty\n    if dirty:\n        git_describe = git_describe[:git_describe.rindex(""-dirty"")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if ""-"" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r\'^(.+)-(\\d+)-g([0-9a-f]+)$\', git_describe)\n        if not mo:\n            # unparseable. Maybe git-describe is misbehaving?\n            pieces[""error""] = (""unable to parse git-describe output: \'%%s\'""\n                               %% describe_out)\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = ""tag \'%%s\' doesn\'t start with prefix \'%%s\'""\n                print(fmt %% (full_tag, tag_prefix))\n            pieces[""error""] = (""tag \'%%s\' doesn\'t start with prefix \'%%s\'""\n                               %% (full_tag, tag_prefix))\n            return pieces\n        pieces[""closest-tag""] = full_tag[len(tag_prefix):]\n\n        # distance: number of commits since tag\n        pieces[""distance""] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[""short""] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[""closest-tag""] = None\n        count_out = run_command(GITS, [""rev-list"", ""HEAD"", ""--count""],\n                                cwd=root)\n        pieces[""distance""] = int(count_out)  # total number of commits\n\n    return pieces\n\n\ndef plus_or_dot(pieces):\n    if ""+"" in pieces.get(""closest-tag"", """"):\n        return "".""\n    return ""+""\n\n\ndef render_pep440(pieces):\n    # now build up version string, with post-release ""local version\n    # identifier"". Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    # get a tagged build and then dirty it, you\'ll get TAG+0.gHEX.dirty\n\n    # exceptions:\n    # 1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += plus_or_dot(pieces)\n            rendered += ""%%d.g%%s"" %% (pieces[""distance""], pieces[""short""])\n            if pieces[""dirty""]:\n                rendered += "".dirty""\n    else:\n        # exception #1\n        rendered = ""0+untagged.%%d.g%%s"" %% (pieces[""distance""],\n                                          pieces[""short""])\n        if pieces[""dirty""]:\n            rendered += "".dirty""\n    return rendered\n\n\ndef render_pep440_pre(pieces):\n    # TAG[.post.devDISTANCE] . No -dirty\n\n    # exceptions:\n    # 1: no tags. 0.post.devDISTANCE\n\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += "".post.dev%%d"" %% pieces[""distance""]\n    else:\n        # exception #1\n        rendered = ""0.post.dev%%d"" %% pieces[""distance""]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    # TAG[.postDISTANCE[.dev0]+gHEX] . The "".dev0"" means dirty. Note that\n    # .dev0 sorts backwards (a dirty tree will appear ""older"" than the\n    # corresponding clean one), but you shouldn\'t be releasing software with\n    # -dirty anyways.\n\n    # exceptions:\n    # 1: no tags. 0.postDISTANCE[.dev0]\n\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%%d"" %% pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n            rendered += plus_or_dot(pieces)\n            rendered += ""g%%s"" %% pieces[""short""]\n    else:\n        # exception #1\n        rendered = ""0.post%%d"" %% pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n        rendered += ""+g%%s"" %% pieces[""short""]\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    # TAG[.postDISTANCE[.dev0]] . The "".dev0"" means dirty.\n\n    # exceptions:\n    # 1: no tags. 0.postDISTANCE[.dev0]\n\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%%d"" %% pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n    else:\n        # exception #1\n        rendered = ""0.post%%d"" %% pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n    return rendered\n\n\ndef render_git_describe(pieces):\n    # TAG[-DISTANCE-gHEX][-dirty], like \'git describe --tags --dirty\n    # --always\'\n\n    # exceptions:\n    # 1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += ""-%%d-g%%s"" %% (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    # TAG-DISTANCE-gHEX[-dirty], like \'git describe --tags --dirty\n    # --always -long\'. The distance/hash is unconditional.\n\n    # exceptions:\n    # 1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        rendered += ""-%%d-g%%s"" %% (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render(pieces, style):\n    if pieces[""error""]:\n        return {""version"": ""unknown"",\n                ""full-revisionid"": pieces.get(""long""),\n                ""dirty"": None,\n                ""error"": pieces[""error""]}\n\n    if not style or style == ""default"":\n        style = ""pep440""  # the default\n\n    if style == ""pep440"":\n        rendered = render_pep440(pieces)\n    elif style == ""pep440-pre"":\n        rendered = render_pep440_pre(pieces)\n    elif style == ""pep440-post"":\n        rendered = render_pep440_post(pieces)\n    elif style == ""pep440-old"":\n        rendered = render_pep440_old(pieces)\n    elif style == ""git-describe"":\n        rendered = render_git_describe(pieces)\n    elif style == ""git-describe-long"":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(""unknown style \'%%s\'"" %% style)\n\n    return {""version"": rendered, ""full-revisionid"": pieces[""long""],\n            ""dirty"": pieces[""dirty""], ""error"": None}\n\n\ndef get_versions():\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don\'t do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the .git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split(\'/\'):\n            root = os.path.dirname(root)\n    except NameError:\n        return {""version"": ""0+unknown"", ""full-revisionid"": None,\n                ""dirty"": None,\n                ""error"": ""unable to find root of source tree""}\n\n    try:\n        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n        return render(pieces, cfg.style)\n    except NotThisMethod:\n        pass\n\n    try:\n        if cfg.parentdir_prefix:\n            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n    except NotThisMethod:\n        pass\n\n    return {""version"": ""0+unknown"", ""full-revisionid"": None,\n            ""dirty"": None,\n            ""error"": ""unable to compute version""}\n\'\'\'\n\n\n@register_vcs_handler(""git"", ""get_keywords"")\ndef git_get_keywords(versionfile_abs):\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don\'t want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    try:\n        f = open(versionfile_abs, ""r"")\n        for line in f.readlines():\n            if line.strip().startswith(""git_refnames =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""refnames""] = mo.group(1)\n            if line.strip().startswith(""git_full =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""full""] = mo.group(1)\n        f.close()\n    except EnvironmentError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(""git"", ""keywords"")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    if not keywords:\n        raise NotThisMethod(""no keywords at all, weird"")\n    refnames = keywords[""refnames""].strip()\n    if refnames.startswith(""$Format""):\n        if verbose:\n            print(""keywords are unexpanded, not using"")\n        raise NotThisMethod(""unexpanded keywords, not a git-archive tarball"")\n    refs = set([r.strip() for r in refnames.strip(""()"").split("","")])\n    # starting in git-1.8.3, tags are listed as ""tag: foo-1.0"" instead of\n    # just ""foo-1.0"". If we see a ""tag: "" prefix, prefer those.\n    TAG = ""tag: ""\n    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\n    if not tags:\n        # Either we\'re using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like ""release"" and\n        # ""stabilization"", as well as ""HEAD"" and ""master"".\n        tags = set([r for r in refs if re.search(r\'\\d\', r)])\n        if verbose:\n            print(""discarding \'%s\', no digits"" % "","".join(refs-tags))\n    if verbose:\n        print(""likely tags: %s"" % "","".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. ""2.0"" over ""2.0rc1""\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix):]\n            if verbose:\n                print(""picking %s"" % r)\n            return {""version"": r,\n                    ""full-revisionid"": keywords[""full""].strip(),\n                    ""dirty"": False, ""error"": None\n                    }\n    # no suitable tags, so version is ""0+unknown"", but full hex is still there\n    if verbose:\n        print(""no suitable tags, using unknown + full revision id"")\n    return {""version"": ""0+unknown"",\n            ""full-revisionid"": keywords[""full""].strip(),\n            ""dirty"": False, ""error"": ""no suitable tags""}\n\n\n@register_vcs_handler(""git"", ""pieces_from_vcs"")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n    # this runs \'git\' from the root of the source tree. This only gets called\n    # if the git-archive \'subst\' keywords were *not* expanded, and\n    # _version.py hasn\'t already been rewritten with a short version string,\n    # meaning we\'re inside a checked out source tree.\n\n    if not os.path.exists(os.path.join(root, "".git"")):\n        if verbose:\n            print(""no .git in %s"" % root)\n        raise NotThisMethod(""no .git directory"")\n\n    GITS = [""git""]\n    if sys.platform == ""win32"":\n        GITS = [""git.cmd"", ""git.exe""]\n    # if there is a tag, this yields TAG-NUM-gHEX[-dirty]\n    # if there are no tags, this yields HEX[-dirty] (no NUM)\n    describe_out = run_command(GITS, [""describe"", ""--tags"", ""--dirty"",\n                                      ""--always"", ""--long""],\n                               cwd=root)\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(""\'git describe\' failed"")\n    describe_out = describe_out.strip()\n    full_out = run_command(GITS, [""rev-parse"", ""HEAD""], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(""\'git rev-parse\' failed"")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[""long""] = full_out\n    pieces[""short""] = full_out[:7]  # maybe improved later\n    pieces[""error""] = None\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(""-dirty"")\n    pieces[""dirty""] = dirty\n    if dirty:\n        git_describe = git_describe[:git_describe.rindex(""-dirty"")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if ""-"" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r\'^(.+)-(\\d+)-g([0-9a-f]+)$\', git_describe)\n        if not mo:\n            # unparseable. Maybe git-describe is misbehaving?\n            pieces[""error""] = (""unable to parse git-describe output: \'%s\'""\n                               % describe_out)\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = ""tag \'%s\' doesn\'t start with prefix \'%s\'""\n                print(fmt % (full_tag, tag_prefix))\n            pieces[""error""] = (""tag \'%s\' doesn\'t start with prefix \'%s\'""\n                               % (full_tag, tag_prefix))\n            return pieces\n        pieces[""closest-tag""] = full_tag[len(tag_prefix):]\n\n        # distance: number of commits since tag\n        pieces[""distance""] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[""short""] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[""closest-tag""] = None\n        count_out = run_command(GITS, [""rev-list"", ""HEAD"", ""--count""],\n                                cwd=root)\n        pieces[""distance""] = int(count_out)  # total number of commits\n\n    return pieces\n\n\ndef do_vcs_install(manifest_in, versionfile_source, ipy):\n    GITS = [""git""]\n    if sys.platform == ""win32"":\n        GITS = [""git.cmd"", ""git.exe""]\n    files = [manifest_in, versionfile_source]\n    if ipy:\n        files.append(ipy)\n    try:\n        me = __file__\n        if me.endswith("".pyc"") or me.endswith("".pyo""):\n            me = os.path.splitext(me)[0] + "".py""\n        versioneer_file = os.path.relpath(me)\n    except NameError:\n        versioneer_file = ""versioneer.py""\n    files.append(versioneer_file)\n    present = False\n    try:\n        f = open("".gitattributes"", ""r"")\n        for line in f.readlines():\n            if line.strip().startswith(versionfile_source):\n                if ""export-subst"" in line.strip().split()[1:]:\n                    present = True\n        f.close()\n    except EnvironmentError:\n        pass\n    if not present:\n        f = open("".gitattributes"", ""a+"")\n        f.write(""%s export-subst\\n"" % versionfile_source)\n        f.close()\n        files.append("".gitattributes"")\n    run_command(GITS, [""add"", ""--""] + files)\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    # Source tarballs conventionally unpack into a directory that includes\n    # both the project name and a version string.\n    dirname = os.path.basename(root)\n    if not dirname.startswith(parentdir_prefix):\n        if verbose:\n            print(""guessing rootdir is \'%s\', but \'%s\' doesn\'t start with ""\n                  ""prefix \'%s\'"" % (root, dirname, parentdir_prefix))\n        raise NotThisMethod(""rootdir doesn\'t start with parentdir_prefix"")\n    return {""version"": dirname[len(parentdir_prefix):],\n            ""full-revisionid"": None,\n            ""dirty"": False, ""error"": None}\n\nSHORT_VERSION_PY = """"""\n# This file was generated by \'versioneer.py\' (0.15) from\n# revision-control system data, or from the parent directory name of an\n# unpacked source archive. Distribution tarballs contain a pre-generated copy\n# of this file.\n\nimport json\nimport sys\n\nversion_json = \'\'\'\n%s\n\'\'\'  # END VERSION_JSON\n\n\ndef get_versions():\n    return json.loads(version_json)\n""""""\n\n\ndef versions_from_file(filename):\n    try:\n        with open(filename) as f:\n            contents = f.read()\n    except EnvironmentError:\n        raise NotThisMethod(""unable to read _version.py"")\n    mo = re.search(r""version_json = \'\'\'\\n(.*)\'\'\'  # END VERSION_JSON"",\n                   contents, re.M | re.S)\n    if not mo:\n        raise NotThisMethod(""no version_json in _version.py"")\n    return json.loads(mo.group(1))\n\n\ndef write_to_version_file(filename, versions):\n    os.unlink(filename)\n    contents = json.dumps(versions, sort_keys=True,\n                          indent=1, separators=("","", "": ""))\n    with open(filename, ""w"") as f:\n        f.write(SHORT_VERSION_PY % contents)\n\n    print(""set %s to \'%s\'"" % (filename, versions[""version""]))\n\n\ndef plus_or_dot(pieces):\n    if ""+"" in pieces.get(""closest-tag"", """"):\n        return "".""\n    return ""+""\n\n\ndef render_pep440(pieces):\n    # now build up version string, with post-release ""local version\n    # identifier"". Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    # get a tagged build and then dirty it, you\'ll get TAG+0.gHEX.dirty\n\n    # exceptions:\n    # 1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += plus_or_dot(pieces)\n            rendered += ""%d.g%s"" % (pieces[""distance""], pieces[""short""])\n            if pieces[""dirty""]:\n                rendered += "".dirty""\n    else:\n        # exception #1\n        rendered = ""0+untagged.%d.g%s"" % (pieces[""distance""],\n                                          pieces[""short""])\n        if pieces[""dirty""]:\n            rendered += "".dirty""\n    return rendered\n\n\ndef render_pep440_pre(pieces):\n    # TAG[.post.devDISTANCE] . No -dirty\n\n    # exceptions:\n    # 1: no tags. 0.post.devDISTANCE\n\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += "".post.dev%d"" % pieces[""distance""]\n    else:\n        # exception #1\n        rendered = ""0.post.dev%d"" % pieces[""distance""]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    # TAG[.postDISTANCE[.dev0]+gHEX] . The "".dev0"" means dirty. Note that\n    # .dev0 sorts backwards (a dirty tree will appear ""older"" than the\n    # corresponding clean one), but you shouldn\'t be releasing software with\n    # -dirty anyways.\n\n    # exceptions:\n    # 1: no tags. 0.postDISTANCE[.dev0]\n\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%d"" % pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n            rendered += plus_or_dot(pieces)\n            rendered += ""g%s"" % pieces[""short""]\n    else:\n        # exception #1\n        rendered = ""0.post%d"" % pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n        rendered += ""+g%s"" % pieces[""short""]\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    # TAG[.postDISTANCE[.dev0]] . The "".dev0"" means dirty.\n\n    # exceptions:\n    # 1: no tags. 0.postDISTANCE[.dev0]\n\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%d"" % pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n    else:\n        # exception #1\n        rendered = ""0.post%d"" % pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n    return rendered\n\n\ndef render_git_describe(pieces):\n    # TAG[-DISTANCE-gHEX][-dirty], like \'git describe --tags --dirty\n    # --always\'\n\n    # exceptions:\n    # 1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += ""-%d-g%s"" % (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    # TAG-DISTANCE-gHEX[-dirty], like \'git describe --tags --dirty\n    # --always -long\'. The distance/hash is unconditional.\n\n    # exceptions:\n    # 1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        rendered += ""-%d-g%s"" % (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render(pieces, style):\n    if pieces[""error""]:\n        return {""version"": ""unknown"",\n                ""full-revisionid"": pieces.get(""long""),\n                ""dirty"": None,\n                ""error"": pieces[""error""]}\n\n    if not style or style == ""default"":\n        style = ""pep440""  # the default\n\n    if style == ""pep440"":\n        rendered = render_pep440(pieces)\n    elif style == ""pep440-pre"":\n        rendered = render_pep440_pre(pieces)\n    elif style == ""pep440-post"":\n        rendered = render_pep440_post(pieces)\n    elif style == ""pep440-old"":\n        rendered = render_pep440_old(pieces)\n    elif style == ""git-describe"":\n        rendered = render_git_describe(pieces)\n    elif style == ""git-describe-long"":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(""unknown style \'%s\'"" % style)\n\n    return {""version"": rendered, ""full-revisionid"": pieces[""long""],\n            ""dirty"": pieces[""dirty""], ""error"": None}\n\n\nclass VersioneerBadRootError(Exception):\n    pass\n\n\ndef get_versions(verbose=False):\n    # returns dict with two keys: \'version\' and \'full\'\n\n    if ""versioneer"" in sys.modules:\n        # see the discussion in cmdclass.py:get_cmdclass()\n        del sys.modules[""versioneer""]\n\n    root = get_root()\n    cfg = get_config_from_root(root)\n\n    assert cfg.VCS is not None, ""please set [versioneer]VCS= in setup.cfg""\n    handlers = HANDLERS.get(cfg.VCS)\n    assert handlers, ""unrecognized VCS \'%s\'"" % cfg.VCS\n    verbose = verbose or cfg.verbose\n    assert cfg.versionfile_source is not None, \\\n        ""please set versioneer.versionfile_source""\n    assert cfg.tag_prefix is not None, ""please set versioneer.tag_prefix""\n\n    versionfile_abs = os.path.join(root, cfg.versionfile_source)\n\n    # extract version from first of: _version.py, VCS command (e.g. \'git\n    # describe\'), parentdir. This is meant to work for developers using a\n    # source checkout, for users of a tarball created by \'setup.py sdist\',\n    # and for users of a tarball/zipball created by \'git archive\' or github\'s\n    # download-from-tag feature or the equivalent in other VCSes.\n\n    get_keywords_f = handlers.get(""get_keywords"")\n    from_keywords_f = handlers.get(""keywords"")\n    if get_keywords_f and from_keywords_f:\n        try:\n            keywords = get_keywords_f(versionfile_abs)\n            ver = from_keywords_f(keywords, cfg.tag_prefix, verbose)\n            if verbose:\n                print(""got version from expanded keyword %s"" % ver)\n            return ver\n        except NotThisMethod:\n            pass\n\n    try:\n        ver = versions_from_file(versionfile_abs)\n        if verbose:\n            print(""got version from file %s %s"" % (versionfile_abs, ver))\n        return ver\n    except NotThisMethod:\n        pass\n\n    from_vcs_f = handlers.get(""pieces_from_vcs"")\n    if from_vcs_f:\n        try:\n            pieces = from_vcs_f(cfg.tag_prefix, root, verbose)\n            ver = render(pieces, cfg.style)\n            if verbose:\n                print(""got version from VCS %s"" % ver)\n            return ver\n        except NotThisMethod:\n            pass\n\n    try:\n        if cfg.parentdir_prefix:\n            ver = versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n            if verbose:\n                print(""got version from parentdir %s"" % ver)\n            return ver\n    except NotThisMethod:\n        pass\n\n    if verbose:\n        print(""unable to compute version"")\n\n    return {""version"": ""0+unknown"", ""full-revisionid"": None,\n            ""dirty"": None, ""error"": ""unable to compute version""}\n\n\ndef get_version():\n    return get_versions()[""version""]\n\n\ndef get_cmdclass():\n    if ""versioneer"" in sys.modules:\n        del sys.modules[""versioneer""]\n        # this fixes the ""python setup.py develop"" case (also \'install\' and\n        # \'easy_install .\'), in which subdependencies of the main project are\n        # built (using setup.py bdist_egg) in the same python process. Assume\n        # a main project A and a dependency B, which use different versions\n        # of Versioneer. A\'s setup.py imports A\'s Versioneer, leaving it in\n        # sys.modules by the time B\'s setup.py is executed, causing B to run\n        # with the wrong versioneer. Setuptools wraps the sub-dep builds in a\n        # sandbox that restores sys.modules to it\'s pre-build state, so the\n        # parent is protected against the child\'s ""import versioneer"". By\n        # removing ourselves from sys.modules here, before the child build\n        # happens, we protect the child from the parent\'s versioneer too.\n        # Also see https://github.com/warner/python-versioneer/issues/52\n\n    cmds = {}\n\n    # we add ""version"" to both distutils and setuptools\n    from distutils.core import Command\n\n    class cmd_version(Command):\n        description = ""report generated version string""\n        user_options = []\n        boolean_options = []\n\n        def initialize_options(self):\n            pass\n\n        def finalize_options(self):\n            pass\n\n        def run(self):\n            vers = get_versions(verbose=True)\n            print(""Version: %s"" % vers[""version""])\n            print("" full-revisionid: %s"" % vers.get(""full-revisionid""))\n            print("" dirty: %s"" % vers.get(""dirty""))\n            if vers[""error""]:\n                print("" error: %s"" % vers[""error""])\n    cmds[""version""] = cmd_version\n\n    # we override ""build_py"" in both distutils and setuptools\n    #\n    # most invocation pathways end up running build_py:\n    #  distutils/build -> build_py\n    #  distutils/install -> distutils/build ->..\n    #  setuptools/bdist_wheel -> distutils/install ->..\n    #  setuptools/bdist_egg -> distutils/install_lib -> build_py\n    #  setuptools/install -> bdist_egg ->..\n    #  setuptools/develop -> ?\n\n    from distutils.command.build_py import build_py as _build_py\n\n    class cmd_build_py(_build_py):\n        def run(self):\n            root = get_root()\n            cfg = get_config_from_root(root)\n            versions = get_versions()\n            _build_py.run(self)\n            # now locate _version.py in the new build/ directory and replace\n            # it with an updated value\n            if cfg.versionfile_build:\n                target_versionfile = os.path.join(self.build_lib,\n                                                  cfg.versionfile_build)\n                print(""UPDATING %s"" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n    cmds[""build_py""] = cmd_build_py\n\n    if ""cx_Freeze"" in sys.modules:  # cx_freeze enabled?\n        from cx_Freeze.dist import build_exe as _build_exe\n\n        class cmd_build_exe(_build_exe):\n            def run(self):\n                root = get_root()\n                cfg = get_config_from_root(root)\n                versions = get_versions()\n                target_versionfile = cfg.versionfile_source\n                print(""UPDATING %s"" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n                _build_exe.run(self)\n                os.unlink(target_versionfile)\n                with open(cfg.versionfile_source, ""w"") as f:\n                    LONG = LONG_VERSION_PY[cfg.VCS]\n                    f.write(LONG %\n                            {""DOLLAR"": ""$"",\n                             ""STYLE"": cfg.style,\n                             ""TAG_PREFIX"": cfg.tag_prefix,\n                             ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,\n                             ""VERSIONFILE_SOURCE"": cfg.versionfile_source,\n                             })\n        cmds[""build_exe""] = cmd_build_exe\n        del cmds[""build_py""]\n\n    # we override different ""sdist"" commands for both environments\n    if ""setuptools"" in sys.modules:\n        from setuptools.command.sdist import sdist as _sdist\n    else:\n        from distutils.command.sdist import sdist as _sdist\n\n    class cmd_sdist(_sdist):\n        def run(self):\n            versions = get_versions()\n            self._versioneer_generated_versions = versions\n            # unless we update this, the command will keep using the old\n            # version\n            self.distribution.metadata.version = versions[""version""]\n            return _sdist.run(self)\n\n        def make_release_tree(self, base_dir, files):\n            root = get_root()\n            cfg = get_config_from_root(root)\n            _sdist.make_release_tree(self, base_dir, files)\n            # now locate _version.py in the new base_dir directory\n            # (remembering that it may be a hardlink) and replace it with an\n            # updated value\n            target_versionfile = os.path.join(base_dir, cfg.versionfile_source)\n            print(""UPDATING %s"" % target_versionfile)\n            write_to_version_file(target_versionfile,\n                                  self._versioneer_generated_versions)\n    cmds[""sdist""] = cmd_sdist\n\n    return cmds\n\n\nCONFIG_ERROR = """"""\nsetup.cfg is missing the necessary Versioneer configuration. You need\na section like:\n\n [versioneer]\n VCS = git\n style = pep440\n versionfile_source = src/myproject/_version.py\n versionfile_build = myproject/_version.py\n tag_prefix = """"\n parentdir_prefix = myproject-\n\nYou will also need to edit your setup.py to use the results:\n\n import versioneer\n setup(version=versioneer.get_version(),\n       cmdclass=versioneer.get_cmdclass(), ...)\n\nPlease read the docstring in ./versioneer.py for configuration instructions,\nedit setup.cfg, and re-run the installer or \'python versioneer.py setup\'.\n""""""\n\nSAMPLE_CONFIG = """"""\n# See the docstring in versioneer.py for instructions. Note that you must\n# re-run \'versioneer.py setup\' after changing this section, and commit the\n# resulting files.\n\n[versioneer]\n#VCS = git\n#style = pep440\n#versionfile_source =\n#versionfile_build =\n#tag_prefix =\n#parentdir_prefix =\n\n""""""\n\nINIT_PY_SNIPPET = """"""\nfrom ._version import get_versions\n__version__ = get_versions()[\'version\']\ndel get_versions\n""""""\n\n\ndef do_setup():\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (EnvironmentError, configparser.NoSectionError,\n            configparser.NoOptionError) as e:\n        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):\n            print(""Adding sample versioneer config to setup.cfg"",\n                  file=sys.stderr)\n            with open(os.path.join(root, ""setup.cfg""), ""a"") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n        return 1\n\n    print("" creating %s"" % cfg.versionfile_source)\n    with open(cfg.versionfile_source, ""w"") as f:\n        LONG = LONG_VERSION_PY[cfg.VCS]\n        f.write(LONG % {""DOLLAR"": ""$"",\n                        ""STYLE"": cfg.style,\n                        ""TAG_PREFIX"": cfg.tag_prefix,\n                        ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,\n                        ""VERSIONFILE_SOURCE"": cfg.versionfile_source,\n                        })\n\n    ipy = os.path.join(os.path.dirname(cfg.versionfile_source),\n                       ""__init__.py"")\n    if os.path.exists(ipy):\n        try:\n            with open(ipy, ""r"") as f:\n                old = f.read()\n        except EnvironmentError:\n            old = """"\n        if INIT_PY_SNIPPET not in old:\n            print("" appending to %s"" % ipy)\n            with open(ipy, ""a"") as f:\n                f.write(INIT_PY_SNIPPET)\n        else:\n            print("" %s unmodified"" % ipy)\n    else:\n        print("" %s doesn\'t exist, ok"" % ipy)\n        ipy = None\n\n    # Make sure both the top-level ""versioneer.py"" and versionfile_source\n    # (PKG/_version.py, used by runtime code) are in MANIFEST.in, so\n    # they\'ll be copied into source distributions. Pip won\'t be able to\n    # install the package without this.\n    manifest_in = os.path.join(root, ""MANIFEST.in"")\n    simple_includes = set()\n    try:\n        with open(manifest_in, ""r"") as f:\n            for line in f:\n                if line.startswith(""include ""):\n                    for include in line.split()[1:]:\n                        simple_includes.add(include)\n    except EnvironmentError:\n        pass\n    # That doesn\'t cover everything MANIFEST.in can do\n    # (http://docs.python.org/2/distutils/sourcedist.html#commands), so\n    # it might give some false negatives. Appending redundant \'include\'\n    # lines is safe, though.\n    if ""versioneer.py"" not in simple_includes:\n        print("" appending \'versioneer.py\' to MANIFEST.in"")\n        with open(manifest_in, ""a"") as f:\n            f.write(""include versioneer.py\\n"")\n    else:\n        print("" \'versioneer.py\' already in MANIFEST.in"")\n    if cfg.versionfile_source not in simple_includes:\n        print("" appending versionfile_source (\'%s\') to MANIFEST.in"" %\n              cfg.versionfile_source)\n        with open(manifest_in, ""a"") as f:\n            f.write(""include %s\\n"" % cfg.versionfile_source)\n    else:\n        print("" versionfile_source already in MANIFEST.in"")\n\n    # Make VCS-specific changes. For git, this means creating/changing\n    # .gitattributes to mark _version.py for export-time keyword\n    # substitution.\n    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)\n    return 0\n\n\ndef scan_setup_py():\n    found = set()\n    setters = False\n    errors = 0\n    with open(""setup.py"", ""r"") as f:\n        for line in f.readlines():\n            if ""import versioneer"" in line:\n                found.add(""import"")\n            if ""versioneer.get_cmdclass()"" in line:\n                found.add(""cmdclass"")\n            if ""versioneer.get_version()"" in line:\n                found.add(""get_version"")\n            if ""versioneer.VCS"" in line:\n                setters = True\n            if ""versioneer.versionfile_source"" in line:\n                setters = True\n    if len(found) != 3:\n        print("""")\n        print(""Your setup.py appears to be missing some important items"")\n        print(""(but I might be wrong). Please make sure it has something"")\n        print(""roughly like the following:"")\n        print("""")\n        print("" import versioneer"")\n        print("" setup( version=versioneer.get_version(),"")\n        print(""        cmdclass=versioneer.get_cmdclass(),  ...)"")\n        print("""")\n        errors += 1\n    if setters:\n        print(""You should remove lines like \'versioneer.VCS = \' and"")\n        print(""\'versioneer.versionfile_source = \' . This configuration"")\n        print(""now lives in setup.cfg, and should be removed from setup.py"")\n        print("""")\n        errors += 1\n    return errors\n\nif __name__ == ""__main__"":\n    cmd = sys.argv[1]\n    if cmd == ""setup"":\n        errors = do_setup()\n        errors += scan_setup_py()\n        if errors:\n            sys.exit(1)\n'"
pyannote/__init__.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2016-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n__import__(""pkg_resources"").declare_namespace(__name__)\n'"
doc/source/conf.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# pyannote.audio documentation build configuration file, created by\n# sphinx-quickstart on Sat Oct 26 15:37:15 2019.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath(\'.\'))\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.githubpages\',\n    \'sphinx.ext.napoleon\',\n    \'sphinx_rtd_theme\',\n]\n\nnapoleon_google_docstring = True\nnapoleon_numpy_docstring = True\nnapoleon_include_init_with_doc = False\nnapoleon_include_private_with_doc = False\nnapoleon_include_special_with_doc = True\nnapoleon_use_admonition_for_examples = False\nnapoleon_use_admonition_for_notes = False\nnapoleon_use_admonition_for_references = False\nnapoleon_use_ivar = False\nnapoleon_use_param = True\nnapoleon_use_rtype = True\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = \'pyannote.audio\'\ncopyright = \'2019, Herve Bredin\'\nauthor = \'Herve Bredin\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = \'2.0\'\n# The full version, including alpha/beta/rc tags.\nrelease = \'2.0\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = []\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'pyannoteaudiodoc\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'pyannoteaudio.tex\', \'pyannote.audio Documentation\',\n     \'Herve Bredin\', \'manual\'),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'pyannoteaudio\', \'pyannote.audio Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'pyannoteaudio\', \'pyannote.audio Documentation\',\n     author, \'pyannoteaudio\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {\n    \'python\': (\'https://docs.python.org/\', None),\n    \'pyannote.core\': (\'http://pyannote.github.io/pyannote-core\', None),\n    \'pyannote.metrics\': (\'http://pyannote.github.io/pyannote-metrics\', None),\n}\n'"
pyannote/audio/__init__.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2016-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n""""""\n`pyannote.audio` provides\n\n  * speech activity detection\n  * speaker change detection\n  * speaker embedding\n  * speaker diarization pipeline\n\n## Installation\n\n```bash\n$ pip install pyannote.audio\n```\n\n## Citation\n\nIf you use `pyannote.audio` please use the following citations.\n\n  - Speech  activity and speaker change detection\n\n        @inproceedings{Yin2017,\n          Author = {Ruiqing Yin and Herv\\\'e Bredin and Claude Barras},\n          Title = {{Speaker Change Detection in Broadcast TV using Bidirectional Long Short-Term Memory Networks}},\n          Booktitle = {{18th Annual Conference of the International Speech Communication Association, Interspeech 2017}},\n          Year = {2017},\n          Month = {August},\n          Address = {Stockholm, Sweden},\n          Url = {https://github.com/yinruiqing/change_detection}\n        }\n\n  - Speaker embedding\n\n        @inproceedings{Bredin2017,\n            author = {Herv\\\'{e} Bredin},\n            title = {{TristouNet: Triplet Loss for Speaker Turn Embedding}},\n            booktitle = {42nd IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2017},\n            year = {2017},\n            url = {http://arxiv.org/abs/1609.04301},\n        }\n\n  - Speaker diarization pipeline\n\n        @inproceedings{Yin2018,\n          Author = {Ruiqing Yin and Herv\\\'e Bredin and Claude Barras},\n          Title = {{Neural Speech Turn Segmentation and Affinity Propagation for Speaker Diarization}},\n          Booktitle = {{19th Annual Conference of the International Speech Communication Association, Interspeech 2018}},\n          Year = {2018},\n          Month = {September},\n          Address = {Hyderabad, India},\n        }\n\n""""""\n\nfrom ._version import get_versions\n\n__version__ = get_versions()[""version""]\ndel get_versions\n'"
pyannote/audio/_version.py,0,"b'# This file helps to compute a version number in source trees obtained from\n# git-archive tarball (such as those provided by githubs download-from-tag\n# feature). Distribution tarballs (built by setup.py sdist) and build\n# directories (produced by setup.py build) will contain a much shorter file\n# that just contains the computed version number.\n\n# This file is released into the public domain. Generated by\n# versioneer-0.15 (https://github.com/warner/python-versioneer)\n\nimport errno\nimport os\nimport re\nimport subprocess\nimport sys\n\n\ndef get_keywords():\n    # these strings will be replaced by git during git-archive.\n    # setup.py/versioneer.py will grep for the variable names, so they must\n    # each be defined on a line of their own. _version.py will just call\n    # get_keywords().\n    git_refnames = ""$Format:%d$""\n    git_full = ""$Format:%H$""\n    keywords = {""refnames"": git_refnames, ""full"": git_full}\n    return keywords\n\n\nclass VersioneerConfig:\n    pass\n\n\ndef get_config():\n    # these strings are filled in when \'setup.py versioneer\' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = ""git""\n    cfg.style = ""pep440""\n    cfg.tag_prefix = """"\n    cfg.parentdir_prefix = ""pyannote-audio-""\n    cfg.versionfile_source = ""pyannote/audio/_version.py""\n    cfg.verbose = False\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    pass\n\n\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    def decorate(f):\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False):\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen(\n                [c] + args,\n                cwd=cwd,\n                stdout=subprocess.PIPE,\n                stderr=(subprocess.PIPE if hide_stderr else None),\n            )\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(""unable to run %s"" % dispcmd)\n                print(e)\n            return None\n    else:\n        if verbose:\n            print(""unable to find command, tried %s"" % (commands,))\n        return None\n    stdout = p.communicate()[0].strip()\n    if sys.version_info[0] >= 3:\n        stdout = stdout.decode()\n    if p.returncode != 0:\n        if verbose:\n            print(""unable to run %s (error)"" % dispcmd)\n        return None\n    return stdout\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    # Source tarballs conventionally unpack into a directory that includes\n    # both the project name and a version string.\n    dirname = os.path.basename(root)\n    if not dirname.startswith(parentdir_prefix):\n        if verbose:\n            print(\n                ""guessing rootdir is \'%s\', but \'%s\' doesn\'t start with ""\n                ""prefix \'%s\'"" % (root, dirname, parentdir_prefix)\n            )\n        raise NotThisMethod(""rootdir doesn\'t start with parentdir_prefix"")\n    return {\n        ""version"": dirname[len(parentdir_prefix) :],\n        ""full-revisionid"": None,\n        ""dirty"": False,\n        ""error"": None,\n    }\n\n\n@register_vcs_handler(""git"", ""get_keywords"")\ndef git_get_keywords(versionfile_abs):\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don\'t want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    try:\n        f = open(versionfile_abs, ""r"")\n        for line in f.readlines():\n            if line.strip().startswith(""git_refnames =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""refnames""] = mo.group(1)\n            if line.strip().startswith(""git_full =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""full""] = mo.group(1)\n        f.close()\n    except EnvironmentError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(""git"", ""keywords"")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    if not keywords:\n        raise NotThisMethod(""no keywords at all, weird"")\n    refnames = keywords[""refnames""].strip()\n    if refnames.startswith(""$Format""):\n        if verbose:\n            print(""keywords are unexpanded, not using"")\n        raise NotThisMethod(""unexpanded keywords, not a git-archive tarball"")\n    refs = set([r.strip() for r in refnames.strip(""()"").split("","")])\n    # starting in git-1.8.3, tags are listed as ""tag: foo-1.0"" instead of\n    # just ""foo-1.0"". If we see a ""tag: "" prefix, prefer those.\n    TAG = ""tag: ""\n    tags = set([r[len(TAG) :] for r in refs if r.startswith(TAG)])\n    if not tags:\n        # Either we\'re using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like ""release"" and\n        # ""stabilization"", as well as ""HEAD"" and ""master"".\n        tags = set([r for r in refs if re.search(r""\\d"", r)])\n        if verbose:\n            print(""discarding \'%s\', no digits"" % "","".join(refs - tags))\n    if verbose:\n        print(""likely tags: %s"" % "","".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. ""2.0"" over ""2.0rc1""\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix) :]\n            if verbose:\n                print(""picking %s"" % r)\n            return {\n                ""version"": r,\n                ""full-revisionid"": keywords[""full""].strip(),\n                ""dirty"": False,\n                ""error"": None,\n            }\n    # no suitable tags, so version is ""0+unknown"", but full hex is still there\n    if verbose:\n        print(""no suitable tags, using unknown + full revision id"")\n    return {\n        ""version"": ""0+unknown"",\n        ""full-revisionid"": keywords[""full""].strip(),\n        ""dirty"": False,\n        ""error"": ""no suitable tags"",\n    }\n\n\n@register_vcs_handler(""git"", ""pieces_from_vcs"")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n    # this runs \'git\' from the root of the source tree. This only gets called\n    # if the git-archive \'subst\' keywords were *not* expanded, and\n    # _version.py hasn\'t already been rewritten with a short version string,\n    # meaning we\'re inside a checked out source tree.\n\n    if not os.path.exists(os.path.join(root, "".git"")):\n        if verbose:\n            print(""no .git in %s"" % root)\n        raise NotThisMethod(""no .git directory"")\n\n    GITS = [""git""]\n    if sys.platform == ""win32"":\n        GITS = [""git.cmd"", ""git.exe""]\n    # if there is a tag, this yields TAG-NUM-gHEX[-dirty]\n    # if there are no tags, this yields HEX[-dirty] (no NUM)\n    describe_out = run_command(\n        GITS, [""describe"", ""--tags"", ""--dirty"", ""--always"", ""--long""], cwd=root\n    )\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(""\'git describe\' failed"")\n    describe_out = describe_out.strip()\n    full_out = run_command(GITS, [""rev-parse"", ""HEAD""], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(""\'git rev-parse\' failed"")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[""long""] = full_out\n    pieces[""short""] = full_out[:7]  # maybe improved later\n    pieces[""error""] = None\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(""-dirty"")\n    pieces[""dirty""] = dirty\n    if dirty:\n        git_describe = git_describe[: git_describe.rindex(""-dirty"")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if ""-"" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r""^(.+)-(\\d+)-g([0-9a-f]+)$"", git_describe)\n        if not mo:\n            # unparseable. Maybe git-describe is misbehaving?\n            pieces[""error""] = ""unable to parse git-describe output: \'%s\'"" % describe_out\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = ""tag \'%s\' doesn\'t start with prefix \'%s\'""\n                print(fmt % (full_tag, tag_prefix))\n            pieces[""error""] = ""tag \'%s\' doesn\'t start with prefix \'%s\'"" % (\n                full_tag,\n                tag_prefix,\n            )\n            return pieces\n        pieces[""closest-tag""] = full_tag[len(tag_prefix) :]\n\n        # distance: number of commits since tag\n        pieces[""distance""] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[""short""] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[""closest-tag""] = None\n        count_out = run_command(GITS, [""rev-list"", ""HEAD"", ""--count""], cwd=root)\n        pieces[""distance""] = int(count_out)  # total number of commits\n\n    return pieces\n\n\ndef plus_or_dot(pieces):\n    if ""+"" in pieces.get(""closest-tag"", """"):\n        return "".""\n    return ""+""\n\n\ndef render_pep440(pieces):\n    # now build up version string, with post-release ""local version\n    # identifier"". Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    # get a tagged build and then dirty it, you\'ll get TAG+0.gHEX.dirty\n\n    # exceptions:\n    # 1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += plus_or_dot(pieces)\n            rendered += ""%d.g%s"" % (pieces[""distance""], pieces[""short""])\n            if pieces[""dirty""]:\n                rendered += "".dirty""\n    else:\n        # exception #1\n        rendered = ""0+untagged.%d.g%s"" % (pieces[""distance""], pieces[""short""])\n        if pieces[""dirty""]:\n            rendered += "".dirty""\n    return rendered\n\n\ndef render_pep440_pre(pieces):\n    # TAG[.post.devDISTANCE] . No -dirty\n\n    # exceptions:\n    # 1: no tags. 0.post.devDISTANCE\n\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += "".post.dev%d"" % pieces[""distance""]\n    else:\n        # exception #1\n        rendered = ""0.post.dev%d"" % pieces[""distance""]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    # TAG[.postDISTANCE[.dev0]+gHEX] . The "".dev0"" means dirty. Note that\n    # .dev0 sorts backwards (a dirty tree will appear ""older"" than the\n    # corresponding clean one), but you shouldn\'t be releasing software with\n    # -dirty anyways.\n\n    # exceptions:\n    # 1: no tags. 0.postDISTANCE[.dev0]\n\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%d"" % pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n            rendered += plus_or_dot(pieces)\n            rendered += ""g%s"" % pieces[""short""]\n    else:\n        # exception #1\n        rendered = ""0.post%d"" % pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n        rendered += ""+g%s"" % pieces[""short""]\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    # TAG[.postDISTANCE[.dev0]] . The "".dev0"" means dirty.\n\n    # exceptions:\n    # 1: no tags. 0.postDISTANCE[.dev0]\n\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%d"" % pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n    else:\n        # exception #1\n        rendered = ""0.post%d"" % pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n    return rendered\n\n\ndef render_git_describe(pieces):\n    # TAG[-DISTANCE-gHEX][-dirty], like \'git describe --tags --dirty\n    # --always\'\n\n    # exceptions:\n    # 1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += ""-%d-g%s"" % (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    # TAG-DISTANCE-gHEX[-dirty], like \'git describe --tags --dirty\n    # --always -long\'. The distance/hash is unconditional.\n\n    # exceptions:\n    # 1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        rendered += ""-%d-g%s"" % (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render(pieces, style):\n    if pieces[""error""]:\n        return {\n            ""version"": ""unknown"",\n            ""full-revisionid"": pieces.get(""long""),\n            ""dirty"": None,\n            ""error"": pieces[""error""],\n        }\n\n    if not style or style == ""default"":\n        style = ""pep440""  # the default\n\n    if style == ""pep440"":\n        rendered = render_pep440(pieces)\n    elif style == ""pep440-pre"":\n        rendered = render_pep440_pre(pieces)\n    elif style == ""pep440-post"":\n        rendered = render_pep440_post(pieces)\n    elif style == ""pep440-old"":\n        rendered = render_pep440_old(pieces)\n    elif style == ""git-describe"":\n        rendered = render_git_describe(pieces)\n    elif style == ""git-describe-long"":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(""unknown style \'%s\'"" % style)\n\n    return {\n        ""version"": rendered,\n        ""full-revisionid"": pieces[""long""],\n        ""dirty"": pieces[""dirty""],\n        ""error"": None,\n    }\n\n\ndef get_versions():\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don\'t do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix, verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the .git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split(""/""):\n            root = os.path.dirname(root)\n    except NameError:\n        return {\n            ""version"": ""0+unknown"",\n            ""full-revisionid"": None,\n            ""dirty"": None,\n            ""error"": ""unable to find root of source tree"",\n        }\n\n    try:\n        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n        return render(pieces, cfg.style)\n    except NotThisMethod:\n        pass\n\n    try:\n        if cfg.parentdir_prefix:\n            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n    except NotThisMethod:\n        pass\n\n    return {\n        ""version"": ""0+unknown"",\n        ""full-revisionid"": None,\n        ""dirty"": None,\n        ""error"": ""unable to compute version"",\n    }\n'"
tests/api/test_dummy.py,0,b'\n\ndef test_dummy():\n    assert 2 == 2\n    \n'
pyannote/audio/applications/__init__.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2017-2019 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n'"
pyannote/audio/applications/base.py,11,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2017-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\nimport io\nimport time\nimport yaml\nimport zipfile\nimport hashlib\nimport torch\nimport multiprocessing\nfrom typing_extensions import Literal\nfrom typing import Optional, Union\nfrom pathlib import Path\nfrom os.path import basename\nimport numpy as np\nfrom tqdm import tqdm\nfrom glob import glob\nfrom pyannote.database import FileFinder\nfrom pyannote.database import get_protocol\nfrom pyannote.database import get_annotated\nfrom pyannote.audio.features.utils import get_audio_duration\nfrom sortedcontainers import SortedDict\nfrom torch.utils.tensorboard import SummaryWriter\nfrom functools import partial\nfrom pyannote.core.utils.helper import get_class_by_name\nimport warnings\nfrom pyannote.audio.train.task import Task\n\nfrom pyannote.audio.features import Pretrained\nfrom pyannote.audio.features import Precomputed\nfrom pyannote.audio.features.wrapper import Wrapper\nfrom pyannote.audio.applications.config import load_config\n\n\ndef create_zip(validate_dir: Path):\n    """"""\n\n    # create zip file containing:\n    # config.yml\n    # {self.train_dir_}/specs.yml\n    # {self.train_dir_}/weights/{epoch:04d}*.pt\n    # {self.validate_dir_}/params.yml\n\n    """"""\n\n    existing_zips = list(validate_dir.glob(""*.zip""))\n    if len(existing_zips) == 1:\n        existing_zips[0].unlink()\n    elif len(existing_zips) > 1:\n        msg = (\n            f""Looks like there are too many torch.hub zip files "" f""in {validate_dir}.""\n        )\n        raise NotImplementedError(msg)\n\n    params_yml = validate_dir / ""params.yml""\n\n    with open(params_yml, ""r"") as fp:\n        params = yaml.load(fp, Loader=yaml.SafeLoader)\n        epoch = params[""epoch""]\n\n    xp_dir = validate_dir.parents[3]\n    config_yml = xp_dir / ""config.yml""\n\n    train_dir = validate_dir.parents[1]\n    weights_dir = train_dir / ""weights""\n    specs_yml = train_dir / ""specs.yml""\n\n    hub_zip = validate_dir / ""hub.zip""\n    with zipfile.ZipFile(hub_zip, ""w"") as z:\n        z.write(config_yml, arcname=config_yml.relative_to(xp_dir))\n        z.write(specs_yml, arcname=specs_yml.relative_to(xp_dir))\n        z.write(params_yml, arcname=params_yml.relative_to(xp_dir))\n        for pt in weights_dir.glob(f""{epoch:04d}*.pt""):\n            z.write(pt, arcname=pt.relative_to(xp_dir))\n\n    sha256_hash = hashlib.sha256()\n    with open(hub_zip, ""rb"") as fp:\n        for byte_block in iter(lambda: fp.read(4096), b""""):\n            sha256_hash.update(byte_block)\n\n    hash_prefix = sha256_hash.hexdigest()[:10]\n    target = validate_dir / f""{hash_prefix}.zip""\n    hub_zip.rename(target)\n\n    return target\n\n\nclass Application:\n\n    CONFIG_YML = ""{experiment_dir}/config.yml""\n    TRAIN_DIR = ""{experiment_dir}/train/{protocol}.{subset}""\n    WEIGHTS_DIR = ""{train_dir}/weights""\n    MODEL_PT = ""{train_dir}/weights/{epoch:04d}.pt""\n    VALIDATE_DIR = ""{train_dir}/validate{_criterion}/{protocol}.{subset}""\n    APPLY_DIR = ""{validate_dir}/apply/{epoch:04d}""\n\n    @classmethod\n    def from_train_dir(cls, train_dir: Path, training: bool = False):\n\n        app = cls(train_dir.parents[1], training=training)\n        app.train_dir_ = train_dir\n        return app\n\n    def __init__(\n        self,\n        experiment_dir: str,\n        training: bool = False,\n        pretrained_config_yml: Path = None,\n    ):\n        """"""\n\n        Parameters\n        ----------\n        experiment_dir : Path\n        training : boolean, optional\n            When False, data augmentation is disabled.\n        pretrained_config_yml : Path, optional\n        """"""\n\n        self.experiment_dir = experiment_dir\n\n        # load configuration\n        config_yml = self.CONFIG_YML.format(experiment_dir=self.experiment_dir)\n        config_default_module = getattr(\n            self, ""config_default_module"", ""pyannote.audio.labeling.tasks""\n        )\n\n        config = load_config(\n            Path(config_yml),\n            training=training,\n            config_default_module=config_default_module,\n            pretrained_config_yml=pretrained_config_yml,\n        )\n\n        for key, value in config.items():\n            setattr(self, f""{key}_"", value)\n\n    def train(\n        self,\n        protocol_name: str,\n        subset: str = ""train"",\n        warm_start: Union[int, Literal[""last""], Path] = 0,\n        epochs: int = 1000,\n        device: Optional[torch.device] = None,\n        n_jobs: int = 1,\n    ):\n        """"""Train model\n\n        Parameters\n        ----------\n        protocol_name : `str`\n        subset : {\'train\', \'development\', \'test\'}, optional\n            Defaults to \'train\'.\n        warm_start : `int`, ""last"", or `Path`, optional\n            When `int`, restart training at this epoch.\n            When ""last"", restart from last epoch.\n            When `Path`, restart from this model checkpoint.\n            Defaults to training from scratch (warm_start = 0).\n        epochs : `int`, optional\n            Train for that many epochs. Defaults to 1000.\n        device : `torch.device`, optional\n            Device on which the model will be allocated. Defaults to using CPU.\n        n_jobs : `int`, optional\n        """"""\n\n        # initialize batch generator\n        preprocessors = self.preprocessors_\n        if ""audio"" not in preprocessors:\n            preprocessors[""audio""] = FileFinder()\n        if ""duration"" not in preprocessors:\n            preprocessors[""duration""] = get_audio_duration\n        protocol = get_protocol(\n            protocol_name, progress=True, preprocessors=preprocessors\n        )\n\n        batch_generator = self.task_.get_batch_generator(\n            self.feature_extraction_,\n            protocol,\n            subset=subset,\n            resolution=self.model_resolution_,\n            alignment=self.model_alignment_,\n        )\n\n        # initialize model architecture based on specifications\n        model = self.get_model_from_specs_(batch_generator.specifications)\n\n        # freeze (when requested)\n        model.freeze(getattr(self, ""freeze_"", []))\n\n        train_dir = Path(\n            self.TRAIN_DIR.format(\n                experiment_dir=self.experiment_dir,\n                protocol=protocol_name,\n                subset=subset,\n            )\n        )\n\n        # use last available epoch as starting point\n        if warm_start == ""last"":\n            warm_start = self.get_number_of_epochs(train_dir=train_dir) - 1\n\n        iterations = self.task_.fit_iter(\n            model,\n            batch_generator,\n            warm_start=warm_start,\n            epochs=epochs,\n            get_optimizer=self.get_optimizer_,\n            scheduler=self.scheduler_,\n            learning_rate=self.learning_rate_,\n            train_dir=train_dir,\n            device=device,\n            callbacks=self.callbacks_,\n            n_jobs=n_jobs,\n        )\n\n        for _ in iterations:\n            pass\n\n    def get_number_of_epochs(self, train_dir=None, return_first=False):\n        """"""Get information about completed epochs\n\n        Parameters\n        ----------\n        train_dir : str, optional\n            Training directory. Defaults to self.train_dir_\n        return_first : bool, optional\n            Defaults (False) to return number of epochs.\n            Set to True to also return index of first epoch.\n\n        """"""\n\n        if train_dir is None:\n            train_dir = self.train_dir_\n\n        directory = self.MODEL_PT.format(train_dir=train_dir, epoch=0)[:-7]\n        weights = sorted(glob(directory + ""*[0-9][0-9][0-9][0-9].pt""))\n\n        if not weights:\n            number_of_epochs = 0\n            first_epoch = None\n\n        else:\n            number_of_epochs = int(basename(weights[-1])[:-3]) + 1\n            first_epoch = int(basename(weights[0])[:-3])\n\n        return (number_of_epochs, first_epoch) if return_first else number_of_epochs\n\n    def validate_init(self, protocol_name, subset=""development""):\n        raise NotImplementedError("""")\n\n    def validate_epoch(\n        self,\n        epoch,\n        validation_data,\n        protocol=None,\n        subset=""development"",\n        device: Optional[torch.device] = None,\n        batch_size: int = 32,\n        n_jobs: int = 1,\n        **kwargs,\n    ):\n\n        raise NotImplementedError("""")\n\n    def validation_criterion(self, protocol, **kwargs):\n        return None\n\n    def validate(\n        self,\n        protocol: str,\n        subset: str = ""development"",\n        every: int = 1,\n        start: Union[int, Literal[""last""]] = 1,\n        end: Union[int, Literal[""last""]] = 100,\n        chronological: bool = False,\n        device: Optional[torch.device] = None,\n        batch_size: int = 32,\n        n_jobs: int = 1,\n        **kwargs,\n    ):\n\n        # use last available epoch as starting point\n        if start == ""last"":\n            start = self.get_number_of_epochs() - 1\n\n        # use last available epoch as end point\n        if end == ""last"":\n            end = self.get_number_of_epochs() - 1\n\n        criterion = self.validation_criterion(protocol, **kwargs)\n\n        validate_dir = Path(\n            self.VALIDATE_DIR.format(\n                train_dir=self.train_dir_,\n                _criterion=f""_{criterion}"" if criterion is not None else """",\n                protocol=protocol,\n                subset=subset,\n            )\n        )\n\n        params_yml = validate_dir / ""params.yml""\n\n        validate_dir.mkdir(parents=True, exist_ok=True)\n        writer = SummaryWriter(log_dir=str(validate_dir), purge_step=start)\n\n        self.validate_dir_ = validate_dir\n\n        validation_data = self.validate_init(protocol, subset=subset)\n\n        if n_jobs > 1:\n            self.pool_ = multiprocessing.Pool(n_jobs)\n\n        progress_bar = tqdm(unit=""iteration"")\n\n        for i, epoch in enumerate(\n            self.validate_iter(\n                start=start, end=end, step=every, chronological=chronological\n            )\n        ):\n\n            # {\'metric\': \'detection_error_rate\',\n            #  \'minimize\': True,\n            #  \'value\': 0.9,\n            #  \'pipeline\': ...}\n            details = self.validate_epoch(\n                epoch,\n                validation_data,\n                protocol=protocol,\n                subset=subset,\n                device=device,\n                batch_size=batch_size,\n                n_jobs=n_jobs,\n                **kwargs,\n            )\n\n            # initialize\n            if i == 0:\n                # what is the name of the metric?\n                metric = details[""metric""]\n                # should the metric be minimized?\n                minimize = details[""minimize""]\n                # epoch -> value dictionary\n                values = SortedDict()\n\n                # load best epoch and value from past executions\n                if params_yml.exists():\n                    with open(params_yml, ""r"") as fp:\n                        params = yaml.load(fp, Loader=yaml.SafeLoader)\n                    best_epoch = params[""epoch""]\n                    best_value = params[metric]\n                    values[best_epoch] = best_value\n\n            # metric value for current epoch\n            values[epoch] = details[""value""]\n\n            # send value to tensorboard\n            writer.add_scalar(\n                f""validate/{protocol}.{subset}/{metric}"",\n                values[epoch],\n                global_step=epoch,\n            )\n\n            # keep track of best value so far\n            if minimize:\n                best_epoch = values.iloc[np.argmin(values.values())]\n                best_value = values[best_epoch]\n\n            else:\n                best_epoch = values.iloc[np.argmax(values.values())]\n                best_value = values[best_epoch]\n\n            # if current epoch leads to the best metric so far\n            # store both epoch number and best pipeline parameter to disk\n            if best_epoch == epoch:\n\n                best = {\n                    metric: best_value,\n                    ""epoch"": epoch,\n                }\n                if ""pipeline"" in details:\n                    pipeline = details[""pipeline""]\n                    best[""params""] = pipeline.parameters(instantiated=True)\n                with open(params_yml, mode=""w"") as fp:\n                    fp.write(yaml.dump(best, default_flow_style=False))\n\n                # create/update zip file for later upload to torch.hub\n                hub_zip = create_zip(validate_dir)\n\n            # progress bar\n            desc = (\n                f""{metric} | ""\n                f""Epoch #{best_epoch} = {100 * best_value:g}% (best) | ""\n                f\'Epoch #{epoch} = {100 * details[""value""]:g}%\'\n            )\n            progress_bar.set_description(desc=desc)\n            progress_bar.update(1)\n\n    def validate_iter(self, start=1, end=None, step=1, sleep=10, chronological=False):\n        """"""Continuously watches `train_dir` for newly completed epochs\n        and yields them for validation\n\n        Note that epochs will not necessarily be yielded in order.\n        The very last completed epoch will always be first on the list.\n\n        Parameters\n        ----------\n        start : int, optional\n            Start validating after `start` epochs. Defaults to 1.\n        end : int, optional\n            Stop validating after epoch `end`. Defaults to never stop.\n        step : int, optional\n            Validate every `step`th epoch. Defaults to 1.\n        sleep : int, optional\n        chronological : bool, optional\n            Force chronological validation.\n\n        Usage\n        -----\n        >>> for epoch in app.validate_iter():\n        ...     app.validate(epoch)\n\n\n        """"""\n\n        if end is None:\n            end = np.inf\n\n        validated_epochs = set()\n        next_epoch_to_validate_in_order = start\n\n        while next_epoch_to_validate_in_order < end:\n\n            # wait for first epoch to complete\n            _, first_epoch = self.get_number_of_epochs(return_first=True)\n            if first_epoch is None:\n                print(""waiting for first epoch to complete..."")\n                time.sleep(sleep)\n                continue\n\n            # corner case: make sure this does not wait forever\n            # for epoch \'start\' as it might never happen, in case\n            # training is started after n pre-existing epochs\n            if next_epoch_to_validate_in_order < first_epoch:\n                next_epoch_to_validate_in_order = first_epoch\n\n            # first epoch has completed\n            break\n\n        while True:\n\n            # check last completed epoch\n            last_completed_epoch = self.get_number_of_epochs() - 1\n\n            # if last completed epoch has not been processed yet,\n            # always process it first (except if \'in order\')\n            if (not chronological) and (last_completed_epoch not in validated_epochs):\n                next_epoch_to_validate = last_completed_epoch\n                time.sleep(5)  # HACK give checkpoint time to save weights\n\n            # in case no new epoch has completed since last time\n            # process the next epoch in chronological order (if available)\n            elif next_epoch_to_validate_in_order <= last_completed_epoch:\n                next_epoch_to_validate = next_epoch_to_validate_in_order\n\n            # otherwise, just wait for a new epoch to complete\n            else:\n                time.sleep(sleep)\n                continue\n\n            if next_epoch_to_validate not in validated_epochs:\n\n                # yield next epoch to process\n                yield next_epoch_to_validate\n\n                # stop validation when the last epoch has been reached\n                if next_epoch_to_validate >= end:\n                    return\n\n                # remember which epoch was processed\n                validated_epochs.add(next_epoch_to_validate)\n\n            # increment \'chronological\' processing\n            if next_epoch_to_validate_in_order == next_epoch_to_validate:\n                next_epoch_to_validate_in_order += step\n\n\n# TODO: add support for torch.hub models directly in docopt\n\n\ndef apply_pretrained(\n    validate_dir: Path,\n    protocol_name: str,\n    subset: Optional[str] = ""test"",\n    duration: Optional[float] = None,\n    step: float = 0.25,\n    device: Optional[torch.device] = None,\n    batch_size: int = 32,\n    pretrained: Optional[str] = None,\n    Pipeline: type = None,\n    **kwargs,\n):\n    """"""Apply pre-trained model\n\n    Parameters\n    ----------\n    validate_dir : Path\n    protocol_name : `str`\n    subset : \'train\' | \'development\' | \'test\', optional\n        Defaults to \'test\'.\n    duration : `float`, optional\n    step : `float`, optional\n    device : `torch.device`, optional\n    batch_size : `int`, optional\n    pretrained : `str`, optional\n    Pipeline : `type`\n    """"""\n\n    if pretrained is None:\n        pretrained = Pretrained(\n            validate_dir=validate_dir,\n            duration=duration,\n            step=step,\n            batch_size=batch_size,\n            device=device,\n        )\n        output_dir = validate_dir / ""apply"" / f""{pretrained.epoch_:04d}""\n    else:\n\n        if pretrained in torch.hub.list(""pyannote/pyannote-audio""):\n            output_dir = validate_dir / pretrained\n        else:\n            output_dir = validate_dir\n\n        pretrained = Wrapper(\n            pretrained,\n            duration=duration,\n            step=step,\n            batch_size=batch_size,\n            device=device,\n        )\n\n    params = {}\n    try:\n        params[""classes""] = pretrained.classes\n    except AttributeError as e:\n        pass\n    try:\n        params[""dimension""] = pretrained.dimension\n    except AttributeError as e:\n        pass\n\n    # create metadata file at root that contains\n    # sliding window and dimension information\n    precomputed = Precomputed(\n        root_dir=output_dir, sliding_window=pretrained.sliding_window, **params\n    )\n\n    # file generator\n    preprocessors = getattr(pretrained, ""preprocessors_"", dict())\n    if ""audio"" not in preprocessors:\n        preprocessors[""audio""] = FileFinder()\n    if ""duration"" not in preprocessors:\n        preprocessors[""duration""] = get_audio_duration\n    protocol = get_protocol(protocol_name, progress=True, preprocessors=preprocessors)\n\n    for current_file in getattr(protocol, subset)():\n        fX = pretrained(current_file)\n        precomputed.dump(current_file, fX)\n\n    # do not proceed with the full pipeline\n    # when there is no such thing for current task\n    if Pipeline is None:\n        return\n\n    # do not proceed with the full pipeline when its parameters cannot be loaded.\n    # this might happen when applying a model that has not been validated yet\n    try:\n        pipeline_params = pretrained.pipeline_params_\n    except AttributeError as e:\n        return\n\n    # instantiate pipeline\n    pipeline = Pipeline(scores=output_dir)\n    pipeline.instantiate(pipeline_params)\n\n    # load pipeline metric (when available)\n    try:\n        metric = pipeline.get_metric()\n    except NotImplementedError as e:\n        metric = None\n\n    # apply pipeline and dump output to RTTM files\n    output_rttm = output_dir / f""{protocol_name}.{subset}.rttm""\n    with open(output_rttm, ""w"") as fp:\n        for current_file in getattr(protocol, subset)():\n            hypothesis = pipeline(current_file)\n            pipeline.write_rttm(fp, hypothesis)\n\n            # compute evaluation metric (when possible)\n            if ""annotation"" not in current_file:\n                metric = None\n\n            # compute evaluation metric (when available)\n            if metric is None:\n                continue\n\n            reference = current_file[""annotation""]\n            uem = get_annotated(current_file)\n            _ = metric(reference, hypothesis, uem=uem)\n\n    # print pipeline metric (when available)\n    if metric is None:\n        return\n\n    output_eval = output_dir / f""{protocol_name}.{subset}.eval""\n    with open(output_eval, ""w"") as fp:\n        fp.write(str(metric))\n'"
pyannote/audio/applications/base_labeling.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2019 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\nfrom typing import Optional\nfrom pathlib import Path\nfrom tqdm import tqdm\n\nimport torch\nfrom .base import Application\nfrom pyannote.database import get_protocol\nfrom pyannote.database import get_annotated\nfrom pyannote.database import FileFinder\nfrom pyannote.audio.features import Precomputed\nfrom pyannote.audio.features import RawAudio\nfrom pyannote.audio.features.utils import get_audio_duration\n\n\nclass BaseLabeling(Application):\n    @property\n    def config_default_module(self):\n        return ""pyannote.audio.labeling.tasks""\n\n    def validate_init(self, protocol_name, subset=""development""):\n        """"""Initialize validation data\n\n        Parameters\n        ----------\n        protocol_name : `str`\n        subset : {\'train\', \'development\', \'test\'}\n            Defaults to \'development\'.\n\n        Returns\n        -------\n        validation_data : object\n            Validation data.\n\n        """"""\n\n        preprocessors = self.preprocessors_\n        if ""audio"" not in preprocessors:\n            preprocessors[""audio""] = FileFinder()\n        if ""duration"" not in preprocessors:\n            preprocessors[""duration""] = get_audio_duration\n        protocol = get_protocol(\n            protocol_name, progress=False, preprocessors=preprocessors\n        )\n        files = getattr(protocol, subset)()\n\n        # convert lazy ProtocolFile to regular dict for multiprocessing\n        files = [dict(file) for file in files]\n\n        if isinstance(self.feature_extraction_, (Precomputed, RawAudio)):\n            return files\n\n        validation_data = []\n        for current_file in tqdm(files, desc=""Feature extraction""):\n            current_file[""features""] = self.feature_extraction_(current_file)\n            validation_data.append(current_file)\n\n        return validation_data\n'"
pyannote/audio/applications/change_detection.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2017-2019 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Ruiqing YIN - yin@limsi.fr\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\nfrom functools import partial\nimport scipy.optimize\nfrom .base_labeling import BaseLabeling\nfrom pyannote.database import get_annotated\n\nfrom pyannote.audio.features import Pretrained\nfrom pyannote.audio.pipeline.speaker_change_detection import (\n    SpeakerChangeDetection as SpeakerChangeDetectionPipeline,\n)\n\n\ndef validate_helper_func(current_file, pipeline=None, metric=None):\n    reference = current_file[""annotation""]\n    uem = get_annotated(current_file)\n    hypothesis = pipeline(current_file)\n    return metric(reference, hypothesis, uem=uem)\n\n\nclass SpeakerChangeDetection(BaseLabeling):\n\n    Pipeline = SpeakerChangeDetectionPipeline\n\n    def validation_criterion(self, protocol, diarization=False, **kwargs):\n        if diarization:\n            return ""diarization_fscore""\n        else:\n            return ""segmentation_fscore""\n\n    def validate_epoch(\n        self,\n        epoch,\n        validation_data,\n        device=None,\n        batch_size=32,\n        diarization=False,\n        n_jobs=1,\n        duration=None,\n        step=0.25,\n        **kwargs\n    ):\n\n        # compute (and store) SCD scores\n        pretrained = Pretrained(\n            validate_dir=self.validate_dir_,\n            epoch=epoch,\n            duration=duration,\n            step=step,\n            batch_size=batch_size,\n            device=device,\n        )\n\n        for current_file in validation_data:\n            current_file[""scores""] = pretrained(current_file)\n\n        # pipeline\n        pipeline = self.Pipeline(scores=""@scores"", fscore=True, diarization=diarization)\n\n        def fun(threshold):\n            pipeline.instantiate({""alpha"": threshold, ""min_duration"": 0.100})\n            metric = pipeline.get_metric(parallel=True)\n            validate = partial(validate_helper_func, pipeline=pipeline, metric=metric)\n            if n_jobs > 1:\n                _ = self.pool_.map(validate, validation_data)\n            else:\n                for file in validation_data:\n                    _ = validate(file)\n\n            return 1.0 - abs(metric)\n\n        res = scipy.optimize.minimize_scalar(\n            fun, bounds=(0.0, 1.0), method=""bounded"", options={""maxiter"": 10}\n        )\n\n        threshold = res.x.item()\n\n        return {\n            ""metric"": self.validation_criterion(None, diarization=diarization),\n            ""minimize"": False,\n            ""value"": float(1.0 - res.fun),\n            ""pipeline"": pipeline.instantiate(\n                {""alpha"": threshold, ""min_duration"": 0.100}\n            ),\n        }\n'"
pyannote/audio/applications/config.py,1,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\nimport warnings\nfrom functools import partial\n\nfrom pathlib import Path\nimport shutil\nfrom typing import Text\nfrom typing import Dict\nimport yaml\n\nimport collections\nfrom pyannote.core.utils.helper import get_class_by_name\nfrom pyannote.database import FileFinder\nfrom pyannote.audio.features.utils import get_audio_duration\nfrom pyannote.audio.train.task import Task\n\n\ndef merge_cfg(pretrained_cfg, cfg):\n    for k, v in cfg.items():\n\n        # case where the user purposedly set a section value to ""null""\n        # this might happen when fine-tuning a pretrained model\n        if v is None:\n            _ = pretrained_cfg.pop(k, None)\n\n        # if v is a dictionary, go deeper and merge recursively\n        elif isinstance(v, collections.abc.Mapping):\n            pretrained_cfg[k] = merge_cfg(pretrained_cfg.get(k, {}), v)\n\n        # in any other case, override pretrained_cfg[k] by cfg[k]\n        else:\n            pretrained_cfg[k] = v\n\n    return pretrained_cfg\n\n\ndef load_config(\n    config_yml: Path,\n    training: bool = False,\n    config_default_module: Text = None,\n    pretrained_config_yml: Path = None,\n) -> Dict:\n    """"""\n\n    Returns\n    -------\n    config : Dict\n        [\'preprocessors\']\n        [\'learning_rate\']\n        [\'scheduler\']\n        [\'get_optimizer\']\n        [\'callbacks\']\n        [\'feature_extraction\']\n        [\'task\']\n        [\'get_model_from_specs\']\n        [\'model_resolution\']\n        [\'model_alignment\']\n    """"""\n\n    # load pretrained model configuration\n    pretrained_cfg = dict()\n    if pretrained_config_yml is not None:\n        with open(pretrained_config_yml, ""r"") as fp:\n            pretrained_cfg = yaml.load(fp, Loader=yaml.SafeLoader)\n\n    # load configuration or complain it\'s missing\n    cfg = dict()\n    if config_yml.exists():\n        with open(config_yml, ""r"") as fp:\n            cfg = yaml.load(fp, Loader=yaml.SafeLoader)\n\n        # backup user-provided config because it will be updated\n        if pretrained_config_yml is not None:\n            shutil.copy(config_yml, config_yml.parent / ""backup+config.yml"")\n\n    elif pretrained_config_yml is None:\n        msg = f""{config_yml} configuration file is missing.""\n        raise FileNotFoundError(msg)\n\n    # override pretrained model config with user-provided config\n    cfg = merge_cfg(pretrained_cfg, cfg)\n\n    # save (updated) config to disk\n    if pretrained_config_yml is not None:\n        with open(config_yml, ""w"") as fp:\n            yaml.dump(cfg, fp, default_flow_style=False)\n\n    # preprocessors\n    preprocessors = dict()\n\n    for key, preprocessor in cfg.get(""preprocessors"", {}).items():\n        # preprocessors:\n        #    key:\n        #       name: package.module.ClassName\n        #       params:\n        #          param1: value1\n        #          param2: value2\n        if isinstance(preprocessor, dict):\n            Klass = get_class_by_name(preprocessor[""name""])\n            preprocessors[key] = Klass(**preprocessor.get(""params"", {}))\n            continue\n\n        try:\n            # preprocessors:\n            #    key: /path/to/database.yml\n            preprocessors[key] = FileFinder(database_yml=preprocessor)\n\n        except FileNotFoundError as e:\n            # preprocessors:\n            #    key: /path/to/{uri}.wav\n            preprocessors[key] = preprocessor\n\n    cfg[""preprocessors""] = preprocessors\n\n    # scheduler\n    SCHEDULER_DEFAULT = {\n        ""name"": ""DavisKingScheduler"",\n        ""params"": {""learning_rate"": ""auto""},\n    }\n    scheduler_cfg = cfg.get(""scheduler"", SCHEDULER_DEFAULT)\n    Scheduler = get_class_by_name(\n        scheduler_cfg[""name""], default_module_name=""pyannote.audio.train.schedulers""\n    )\n    scheduler_params = scheduler_cfg.get(""params"", {})\n\n    cfg[""learning_rate""] = scheduler_params.pop(""learning_rate"", ""auto"")\n    cfg[""scheduler""] = Scheduler(**scheduler_params)\n\n    # optimizer\n    OPTIMIZER_DEFAULT = {\n        ""name"": ""SGD"",\n        ""params"": {\n            ""momentum"": 0.9,\n            ""dampening"": 0,\n            ""weight_decay"": 0,\n            ""nesterov"": True,\n        },\n    }\n    optimizer_cfg = cfg.get(""optimizer"", OPTIMIZER_DEFAULT)\n    try:\n        Optimizer = get_class_by_name(\n            optimizer_cfg[""name""], default_module_name=""torch.optim""\n        )\n        optimizer_params = optimizer_cfg.get(""params"", {})\n        cfg[""get_optimizer""] = partial(Optimizer, **optimizer_params)\n\n    # do not raise an error here as it is possible that the optimizer is\n    # not really needed (e.g. in pipeline training)\n    except ModuleNotFoundError as e:\n        warnings.warn(e.args[0])\n\n    # data augmentation should only be active when training a model\n    if training and ""data_augmentation"" in cfg:\n        DataAugmentation = get_class_by_name(\n            cfg[""data_augmentation""][""name""],\n            default_module_name=""pyannote.audio.augmentation"",\n        )\n        augmentation = DataAugmentation(**cfg[""data_augmentation""].get(""params"", {}))\n    else:\n        augmentation = None\n\n    # custom callbacks\n    callbacks = []\n    for callback_config in cfg.get(""callbacks"", {}):\n        Callback = get_class_by_name(callback_config[""name""])\n        callback = Callback(**callback_config.get(""params"", {}))\n        callbacks.append(callback)\n    cfg[""callbacks""] = callbacks\n\n    # feature extraction\n    FEATURE_DEFAULT = {""name"": ""RawAudio"", ""params"": {""sample_rate"": 16000}}\n    feature_cfg = cfg.get(""feature_extraction"", FEATURE_DEFAULT)\n    FeatureExtraction = get_class_by_name(\n        feature_cfg[""name""], default_module_name=""pyannote.audio.features""\n    )\n    feature_params = feature_cfg.get(""params"", {})\n    cfg[""feature_extraction""] = FeatureExtraction(\n        **feature_params, augmentation=augmentation\n    )\n\n    # task\n    if config_default_module is None:\n        config_default_module = ""pyannote.audio.labeling.tasks""\n\n    try:\n        TaskClass = get_class_by_name(\n            cfg[""task""][""name""], default_module_name=config_default_module\n        )\n    except AttributeError:\n        TaskClass = get_class_by_name(\n            cfg[""task""][""name""],\n            default_module_name=""pyannote.audio.embedding.approaches"",\n        )\n\n    cfg[""task""] = TaskClass(**cfg[""task""].get(""params"", {}))\n\n    # architecture\n    Architecture = get_class_by_name(\n        cfg[""architecture""][""name""], default_module_name=""pyannote.audio.models""\n    )\n    params = cfg[""architecture""].get(""params"", {})\n\n    cfg[""get_model_from_specs""] = partial(Architecture, **params)\n    task = cfg[""task""].task\n    cfg[""model_resolution""] = Architecture.get_resolution(task, **params)\n    cfg[""model_alignment""] = Architecture.get_alignment(task, **params)\n\n    return cfg\n\n\ndef load_specs(specs_yml: Path) -> Dict:\n    """"""\n\n    Returns\n    -------\n    specs : Dict\n        [\'task\']\n        [and others]\n    """"""\n\n    with open(specs_yml, ""r"") as fp:\n        specifications = yaml.load(fp, Loader=yaml.SafeLoader)\n    specifications[""task""] = Task.from_str(specifications[""task""])\n    return specifications\n\n\ndef load_params(params_yml: Path) -> Dict:\n\n    with open(params_yml, ""r"") as fp:\n        params = yaml.load(fp, Loader=yaml.SafeLoader)\n\n    return params\n'"
pyannote/audio/applications/domain_classification.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2019-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n\nimport numpy as np\nfrom .base_labeling import BaseLabeling\nfrom pyannote.audio.features import Pretrained\nfrom collections import Counter\nfrom sklearn.metrics import confusion_matrix\n\nimport matplotlib\n\nmatplotlib.use(""Agg"")\nimport matplotlib.pyplot as plt\n\n\ndef plot_confusion_matrix(\n    y_true, y_pred, classes, normalize=False, title=None, cmap=plt.cm.Blues\n):\n    """"""\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    """"""\n    if not title:\n        if normalize:\n            title = ""Normalized confusion matrix""\n        else:\n            title = ""Confusion matrix, without normalization""\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    if normalize:\n        cm = cm.astype(""float"") / cm.sum(axis=1)[:, np.newaxis]\n        print(""Normalized confusion matrix"")\n    else:\n        print(""Confusion matrix, without normalization"")\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation=""nearest"", cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(\n        xticks=np.arange(cm.shape[1]),\n        yticks=np.arange(cm.shape[0]),\n        # ... and label them with the respective list entries\n        xticklabels=classes,\n        yticklabels=classes,\n        title=title,\n        ylabel=""True label"",\n        xlabel=""Predicted label"",\n    )\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=""right"", rotation_mode=""anchor"")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = "".1f"" if normalize else ""d""\n    thresh = cm.max() / 2.0\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            if cm[i, j] == 0.0:\n                continue\n            ax.text(\n                j,\n                i,\n                format(cm[i, j], fmt),\n                ha=""center"",\n                va=""center"",\n                color=""white"" if cm[i, j] > thresh else ""black"",\n                size=6,\n            )\n    fig.tight_layout()\n    return ax\n\n\nclass DomainClassification(BaseLabeling):\n    def validate_epoch(\n        self,\n        epoch,\n        validation_data,\n        device=None,\n        batch_size=32,\n        n_jobs=1,\n        duration=None,\n        step=0.25,\n        **kwargs\n    ):\n\n        pretrained = Pretrained(\n            validate_dir=self.validate_dir_,\n            epoch=epoch,\n            duration=duration,\n            step=step,\n            batch_size=batch_size,\n            device=device,\n        )\n\n        domain = self.task_.domain\n        domains = pretrained.classes\n\n        y_true_file, y_pred_file = [], []\n\n        for current_file in validation_data:\n\n            y_pred = pretrained(current_file).data.argmax(axis=1)\n            y_pred_file.append(Counter(y_pred).most_common(1)[0][0])\n\n            y_true = domains.index(current_file[domain])\n            y_true_file.append(y_true)\n\n        accuracy = np.mean(np.array(y_true_file) == np.array(y_pred_file))\n\n        return {""metric"": ""accuracy"", ""minimize"": False, ""value"": float(accuracy)}\n'"
pyannote/audio/applications/feature_extraction.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2016-2018 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n""""""\nFeature extraction\n\nUsage:\n  pyannote-speech-feature [--robust --parallel] <experiment_dir> <database.task.protocol>\n  pyannote-speech-feature check <experiment_dir> <database.task.protocol>\n  pyannote-speech-feature -h | --help\n  pyannote-speech-feature --version\n\nOptions:\n  <experiment_dir>           Set experiment root directory. This script expects\n                             a configuration file called ""config.yml"" to live\n                             in this directory. See ""Configuration file""\n                             section below for more details.\n  <database.task.protocol>   Set evaluation protocol (e.g. ""Etape.SpeakerDiarization.TV"")\n  --robust                   When provided, skip files for which feature extraction fails.\n  --parallel                 When provided, process files in parallel.\n  -h --help                  Show this screen.\n  --version                  Show version.\n\nConfiguration file:\n    The configuration of each experiment is described in a file called\n    <experiment_dir>/config.yml, that describes the feature extraction process\n    (e.g. MFCCs).\n\n    ................... <experiment_dir>/config.yml ...................\n    feature_extraction:\n       name: LibrosaMFCC\n       params:\n          e: False                   # this experiments relies\n          De: True                   # on 11 MFCC coefficients\n          DDe: True                  # with 1st and 2nd derivatives\n          D: True                    # without energy, but with\n          DD: True                   # energy derivatives\n    ...................................................................\n\n""""""\n\nimport yaml\nimport os.path\nimport numpy as np\nimport functools\nfrom docopt import docopt\n\nfrom pyannote.database import FileFinder\nfrom pyannote.database import get_unique_identifier\nfrom pyannote.database import get_protocol\n\nfrom pyannote.core.utils.helper import get_class_by_name\n\nfrom pyannote.audio.features import Precomputed\nfrom pyannote.audio.features.utils import get_audio_duration\nfrom pyannote.audio.features.precomputed import PyannoteFeatureExtractionError\n\nfrom multiprocessing import cpu_count, Pool\n\n\ndef init_feature_extraction(experiment_dir):\n\n    # load configuration file\n    config_yml = experiment_dir + ""/config.yml""\n    with open(config_yml, ""r"") as fp:\n        config = yaml.load(fp, Loader=yaml.SafeLoader)\n\n    FeatureExtraction = get_class_by_name(\n        config[""feature_extraction""][""name""],\n        default_module_name=""pyannote.audio.features"",\n    )\n    feature_extraction = FeatureExtraction(\n        **config[""feature_extraction""].get(""params"", {})\n    )\n\n    return feature_extraction\n\n\ndef process_current_file(\n    current_file,\n    file_finder=None,\n    precomputed=None,\n    feature_extraction=None,\n    robust=False,\n):\n\n    try:\n        current_file[""audio""] = file_finder(current_file)\n    except ValueError as e:\n        if not robust:\n            raise PyannoteFeatureExtractionError(*e.args)\n        return e\n\n    uri = get_unique_identifier(current_file)\n    path = precomputed.get_path(current_file)\n\n    if os.path.exists(path):\n        return\n\n    try:\n        features = feature_extraction(current_file)\n    except PyannoteFeatureExtractionError as e:\n        msg = \'Feature extraction failed for file ""{uri}"".\'\n        return msg.format(uri=uri)\n\n    if features is None:\n        msg = \'Feature extraction returned None for file ""{uri}"".\'\n        return msg.format(uri=uri)\n\n    if np.any(np.isnan(features.data)):\n        msg = \'Feature extraction returned NaNs for file ""{uri}"".\'\n        return msg.format(uri=uri)\n\n    precomputed.dump(current_file, features)\n\n    return\n\n\ndef helper_extract(\n    current_file,\n    file_finder=None,\n    experiment_dir=None,\n    config_yml=None,\n    feature_extraction=None,\n    robust=False,\n):\n\n    if feature_extraction is None:\n        feature_extraction = init_feature_extraction(experiment_dir)\n\n    precomputed = Precomputed(root_dir=experiment_dir)\n    return process_current_file(\n        current_file,\n        file_finder=file_finder,\n        precomputed=precomputed,\n        feature_extraction=feature_extraction,\n        robust=robust,\n    )\n\n\ndef extract(protocol_name, file_finder, experiment_dir, robust=False, parallel=False):\n\n    protocol = get_protocol(protocol_name, progress=False)\n\n    # load configuration file\n    config_yml = experiment_dir + ""/config.yml""\n    with open(config_yml, ""r"") as fp:\n        config = yaml.load(fp, Loader=yaml.SafeLoader)\n\n    FeatureExtraction = get_class_by_name(\n        config[""feature_extraction""][""name""],\n        default_module_name=""pyannote.audio.features"",\n    )\n    feature_extraction = FeatureExtraction(\n        **config[""feature_extraction""].get(""params"", {})\n    )\n\n    sliding_window = feature_extraction.sliding_window\n    dimension = feature_extraction.dimension\n\n    # create metadata file at root that contains\n    # sliding window and dimension information\n\n    precomputed = Precomputed(\n        root_dir=experiment_dir, sliding_window=sliding_window, dimension=dimension\n    )\n\n    if parallel:\n\n        extract_one = functools.partial(\n            helper_extract,\n            file_finder=file_finder,\n            experiment_dir=experiment_dir,\n            config_yml=config_yml,\n            robust=robust,\n        )\n\n        n_jobs = cpu_count()\n        pool = Pool(n_jobs)\n        imap = pool.imap\n\n    else:\n\n        feature_extraction = init_feature_extraction(experiment_dir)\n        extract_one = functools.partial(\n            helper_extract,\n            file_finder=file_finder,\n            experiment_dir=experiment_dir,\n            feature_extraction=feature_extraction,\n            robust=robust,\n        )\n        imap = map\n\n    for result in imap(extract_one, protocol.files()):\n        if result is None:\n            continue\n        print(result)\n\n\ndef check(protocol_name, file_finder, experiment_dir):\n\n    protocol = get_protocol(protocol_name)\n    precomputed = Precomputed(experiment_dir)\n\n    for subset in [""development"", ""test"", ""train""]:\n\n        try:\n            file_generator = getattr(protocol, subset)()\n            first_item = next(file_generator)\n        except NotImplementedError as e:\n            continue\n\n        for current_file in getattr(protocol, subset)():\n\n            try:\n                audio = file_finder(current_file)\n                current_file[""audio""] = audio\n            except ValueError as e:\n                print(e)\n                continue\n\n            duration = get_audio_duration(current_file)\n\n            try:\n                features = precomputed(current_file)\n            except PyannoteFeatureExtractionError as e:\n                print(e)\n                continue\n\n            if not np.isclose(duration, features.getExtent().duration, atol=1.0):\n                uri = get_unique_identifier(current_file)\n                print(\'Duration mismatch for ""{uri}""\'.format(uri=uri))\n\n            if np.any(np.isnan(features.data)):\n                uri = get_unique_identifier(current_file)\n                print(\'NaN for ""{uri}""\'.format(uri=uri))\n\n\ndef main():\n\n    arguments = docopt(__doc__, version=""Feature extraction"")\n\n    file_finder = FileFinder()\n\n    protocol_name = arguments[""<database.task.protocol>""]\n    experiment_dir = arguments[""<experiment_dir>""]\n\n    if arguments[""check""]:\n        check(protocol_name, file_finder, experiment_dir)\n    else:\n        robust = arguments[""--robust""]\n        parallel = arguments[""--parallel""]\n        extract(\n            protocol_name, file_finder, experiment_dir, robust=robust, parallel=parallel\n        )\n'"
pyannote/audio/applications/overlap_detection.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2018-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\nfrom pyannote.audio.pipeline.overlap_detection import (\n    OverlapDetection as OverlapDetectionPipeline,\n)\nfrom .speech_detection import SpeechActivityDetection\n\n\nclass OverlapDetection(SpeechActivityDetection):\n    Pipeline = OverlapDetectionPipeline\n'"
pyannote/audio/applications/pyannote_audio.py,6,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2019-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n\n""""""\nNeural building blocks for speaker diarization\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nUsage:\n  pyannote-audio (sad | scd | ovl | emb | dom) train    [--cpu | --gpu] [options] <root>     <protocol>\n  pyannote-audio (sad | scd | ovl | emb | dom) validate [--cpu | --gpu] [options] <train>    <protocol>\n  pyannote-audio (sad | scd | ovl | emb | dom) apply    [--cpu | --gpu] [options] <validate> <protocol>\n  pyannote-audio -h | --help\n  pyannote-audio --version\n\nThis command line tool can be used to train, validate, and apply neural networks\nfor the following blocks of a speaker diarization pipeline:\n\n    * (sad) speech activity detection consists in detecting speech regions in\n            an audio recording.\n    * (scd) speaker change detection consists in detecting timestamps of\n            speaker change point.\n    * (ovl) overlapped speech detection consists in detection regions with two\n            or more simultaneous speakers.\n    * (emb) speaker embedding consists in projecting audio chunk into a\n            (usually high-dimensional) vector space where same speaker\n            embeddings are close to each other, and different speaker embeddings\n            are not.\n    * (dom) domain classification consists in predicting the domain of an\n            audio recording\n\nRunning a complete speech activity detection experiment on the provided\n""debug"" dataset would go like this:\n\n    * Run experiment on this pyannote.database protocol\n      $ export DATABASE=Debug.SpeakerDiarization.Debug\n\n    * This directory will contain experiments artifacts:\n      $ mkdir my_experiment && cd my_experiment\n\n    * A unique configuration file describes the experiment hyper-parameters\n      (see ""Configuration file"" below for details):\n      $ edit config.yml\n\n    * This will train the model on the training set:\n      $ pyannote-audio sad train ${PWD} ${DATABASE}\n\n    * Training artifacts (including model weights) are stored in a sub-directory\n      whose name makes it clear which dataset and subset (train, by default)\n      were used for training the model.\n      $ cd train/${DATABASE}.train\n\n    * This will validate the model on the development set:\n      $ pyannote-audio sad validate ${PWD} ${DATABASE}\n\n    * Validation artifacts (including the selection of the best epoch) are\n      stored in a sub-directory named after the dataset and subset (development,\n      by default) used for validating the model.\n      $ cd validate/${DATABASE}.development\n\n    * This will apply the best model (according to the validation step) to the\n      test set:\n      $ pyannote-audio sad apply ${PWD} ${DATABASE}\n\n    * Inference artifacts are stored in a sub-directory whose name makes it\n      clear which epoch has been used (e.g. apply/0125). Artifacts include:\n        * raw output of the best model (one numpy array per file  than can be\n          loaded with pyannote.audio.features.Precomputed API and handled with\n          pyannote.core.SlidingWindowFeature API)\n        * (depending on the task) a file ""${DATABASE}.test.rttm"" containing the\n          post-processing of raw output.\n        * (depending on the task) a file ""${DATABASE}.test.eval"" containing the\n          evaluation result computed with pyannote.metrics.\n\npyannote.database support\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nPYANNOTE_DATABASE_CONFIG=\n\nConfiguration file <root>/config.yml\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n    Reproducible research is facilitated by the systematic use of configuration\n    files stored in <root>/config.yml in YAML format.\n\n    .......................... <root>/config.yml ..........................\n    task:\n        name:\n        params:\n\n    feature_extraction:\n        name:\n        params:\n\n    data_augmentation:\n        name:\n        params:\n\n    architecture:\n        name:\n        params:\n\n    scheduler:\n        name:\n        params:\n\n    preprocessors:\n\n    callbacks:\n    ...................................................................\n\n    File <root>/config.yml is mandatory, unless option --pretrained is used.\n\n    When fine-tuning a model with option --pretrained=<model>, one can omit it\n    and the original <model> configuration file is used instead. If (a possibly\n    partial) <root>/config.yml file is provided anyway, it is used to override\n    <model> configuration file.\n\nTensorboard support\n~~~~~~~~~~~~~~~~~~~\n\n    A bunch of metrics are logged during training and validation (e.g. loss,\n    learning rate, computation time, validation metric). They can be visualized\n    using tensorboard:\n\n        $ tensorboard --logdir=<root>\n\nCommon options\n~~~~~~~~~~~~~~\n\n  <root>                  Experiment root directory. Should contain config.yml\n                          configuration file, unless --pretrained option is\n                          used (for which config.yml is optional).\n\n  <protocol>              Name of protocol to use for training, validation, or\n                          inference. Have a look at pyannote.database\n                          documentation for instructions on how to define a\n                          protocol with your own dataset:\n                          https://github.com/pyannote/pyannote-database#custom-protocols\n\n  <train>                 Path to <root> sub-directory containing training\n                          artifacts (e.g. <root>/train/<protocol>.train)\n\n  <validate>              Path to <train> sub-directory containing validation\n                          artifacts (e.g. <train>/validate/<protocol>.development)\n                          In case option --pretrained=<model> is used, the\n                          output of the pretrained model is dumped into the\n                          <validate> directory.\n\n  --subset=<subset>       Subset to use for training (resp. validation,\n                          inference). Defaults to ""train"" (resp. ""development"",\n                          ""test"") for strict enforcement of machine learning\n                          good practices.\n\n  --gpu                   Run on GPU. When multiple GPUs are available, use\n                          CUDA_VISIBLE_DEVICES environment variable to force\n                          using a specific one. Defaults to using CPU if no GPU\n                          is available.\n\n  --cpu                   Run on CPU. Defaults to using GPU when available.\n\n  --debug                 Run using PyTorch\'s anomaly detection. This will throw\n                          an error if a NaN value is produced, and the stacktrace\n                          will point to the origin of it. This option can\n                          considerably slow execution.\n\n  --from=<epoch>          Start training (resp. validating) at epoch <epoch>.\n                          Use --from=last to start from last available epoch at\n                          launch time. Not used for inference [default: 0].\n\n  --to=<epoch>            End training (resp. validating) at epoch <epoch>.\n                          Use --end=last to validate until last available epoch\n                          at launch time. Not used for inference [default: 100].\n\n  --batch=<size>          Set batch size used for validation and inference.\n                          Has no effect when training as this parameter should\n                          be defined in the configuration file [default: 32].\n\n  --step=<ratio>          Ratio of audio chunk duration used as step between\n                          two consecutive audio chunks [default: 0.25]\n\n  --parallel=<n_jobs>     Use at most that many threads for generating training\n                          samples or validating files. Defaults to using all\n                          CPUs but one.\n\n\nSpeaker embedding\n~~~~~~~~~~~~~~~~~\n\n  --duration=<duration>   Use audio chunks with that duration. Defaults to the\n                          fixed duration used during training, when available.\n\n  --metric=<metric>       Use this metric (e.g. ""cosine"" or ""euclidean"") to\n                          compare embeddings. Defaults to the metric defined in\n                          <root>/config.yml configuration file.\n\nPretrained model options\n~~~~~~~~~~~~~~~~~~~~~~~~\n\n  --pretrained=<model>    Warm start training with pre-trained model. Can be\n                          either a path to an existing checkpoint (e.g.\n                          <train>/weights/0050.pt) or the name of a model\n                          available in torch.hub.list(\'pyannote/pyannote.audio\')\n                          This option can also be used to apply a pretrained\n                          model. See description of <validate> for more details.\n\nValidation options\n~~~~~~~~~~~~~~~~~~\n\n  --every=<epoch>         Validate model every <epoch> epochs [default: 1].\n\n  --evergreen             Prioritize validation of most recent epoch.\n\n  For speech activity and overlapped speech detection, validation consists in\n  looking for the value of the detection threshold that maximizes the f-score\n  of recall and precision.\n\n  For speaker change detection, validation consists in looking for the value of\n  the peak detection threshold that maximizes the f-score of purity and\n  coverage:\n\n  --diarization           Use diarization purity and coverage instead of\n                          (default) segmentation purity and coverage.\n\n  For speaker embedding and verification protocols, validation runs the actual\n  speaker verification experiment (representing each recording by its average\n  embedding) and reports equal error rate.\n\n  For speaker embedding and diarization protocols, validation runs a speaker\n  diarization pipeline based on oracle segmentation and ""pool-linkage""\n  agglomerative clustering of speech turns (represented by their average\n  embedding), and looks for the threshold that maximizes the f-score of purity\n  and coverage.\n\n""""""\n\nimport sys\nimport warnings\nfrom docopt import docopt\nfrom pathlib import Path\nimport multiprocessing\n\nimport torch\nfrom .base import apply_pretrained\nfrom .speech_detection import SpeechActivityDetection\nfrom .change_detection import SpeakerChangeDetection\nfrom .overlap_detection import OverlapDetection\nfrom .speaker_embedding import SpeakerEmbedding\nfrom .domain_classification import DomainClassification\n\n\ndef main():\n\n    # TODO: update version automatically\n    arg = docopt(__doc__, version=""pyannote-audio 2.0"")\n\n    params = {}\n\n    if arg[""sad""]:\n        Application = SpeechActivityDetection\n\n    elif arg[""scd""]:\n        Application = SpeakerChangeDetection\n\n    elif arg[""ovl""]:\n        Application = OverlapDetection\n\n    elif arg[""emb""]:\n        Application = SpeakerEmbedding\n\n    elif arg[""dom""]:\n        Application = DomainClassification\n\n    device = ""cuda"" if torch.cuda.is_available() else ""cpu""\n    if arg[""--gpu""] and device == ""cpu"":\n        msg = ""No GPU is available. Using CPU instead.""\n        warnings.warn(msg)\n    if arg[""--cpu""] and device == ""cuda"":\n        device = ""cpu""\n\n    params[""device""] = torch.device(device)\n\n    protocol = arg[""<protocol>""]\n    subset = arg[""--subset""]\n\n    if arg[""--debug""]:\n        msg = ""Debug mode is enabled, this option might slow execution considerably.""\n        warnings.warn(msg, RuntimeWarning)\n        torch.autograd.set_detect_anomaly(True)\n\n    n_jobs = arg[""--parallel""]\n    if n_jobs is None:\n        n_jobs = max(1, multiprocessing.cpu_count() - 1)\n    params[""n_jobs""] = int(n_jobs)\n\n    if arg[""train""]:\n\n        params[""subset""] = ""train"" if subset is None else subset\n\n        # start training at this epoch (defaults to 0, but \'last\' is supported)\n        warm_start = arg[""--from""]\n        if warm_start != ""last"":\n            warm_start = int(warm_start)\n\n        # or start from pretrained model\n        pretrained = arg[""--pretrained""]\n        pretrained_config_yml = None\n        if pretrained is not None:\n\n            # start from an existing model checkpoint\n            # (from a different experiment)\n            if Path(pretrained).exists():\n                warm_start = Path(pretrained)\n\n            else:\n                try:\n                    warm_start = torch.hub.load(\n                        # TODO. change to \'pyannote/pyannote-audio\'\n                        # after 2.0 release\n                        ""pyannote/pyannote-audio:develop"",\n                        pretrained,\n                    ).weights_pt_\n                except Exception as e:\n                    msg = (\n                        f\'Could not load ""{warm_start}"" model from torch.hub.\'\n                        f""The following exception was raised:\\n\\n{e}\\n\\n""\n                    )\n                    sys.exit(msg)\n\n            pretrained_config_yml = warm_start.parents[3] / ""config.yml""\n\n        params[""warm_start""] = warm_start\n\n        # stop training at this epoch (defaults to never stop)\n        params[""epochs""] = int(arg[""--to""])\n\n        root_dir = Path(arg[""<root>""]).expanduser().resolve(strict=True)\n        app = Application(\n            root_dir, training=True, pretrained_config_yml=pretrained_config_yml\n        )\n        app.train(protocol, **params)\n\n    if arg[""validate""]:\n\n        train_dir = Path(arg[""<train>""]).expanduser().resolve(strict=True)\n        app = Application.from_train_dir(train_dir, training=False)\n\n        params[""subset""] = ""development"" if subset is None else subset\n\n        start = arg[""--from""]\n        if start != ""last"":\n            start = int(start)\n        params[""start""] = start\n\n        end = arg[""--to""]\n        if end != ""last"":\n            end = int(end)\n        params[""end""] = end\n\n        params[""every""] = int(arg[""--every""])\n        params[""chronological""] = not arg[""--evergreen""]\n        params[""batch_size""] = int(arg[""--batch""])\n\n        params[""diarization""] = arg[""--diarization""]\n\n        duration = arg[""--duration""]\n        if duration is None:\n            duration = getattr(app.task_, ""duration"", None)\n            if duration is None:\n                msg = (\n                    ""Task has no \'duration\' defined. ""\n                    ""Use \'--duration\' option to provide one.""\n                )\n                raise ValueError(msg)\n        else:\n            duration = float(duration)\n        params[""duration""] = duration\n\n        params[""step""] = float(arg[""--step""])\n\n        if arg[""emb""]:\n\n            metric = arg[""--metric""]\n            if metric is None:\n                metric = getattr(app.task_, ""metric"", None)\n                if metric is None:\n                    msg = (\n                        ""Approach has no \'metric\' defined. ""\n                        ""Use \'--metric\' option to provide one.""\n                    )\n                    raise ValueError(msg)\n            params[""metric""] = metric\n\n        # FIXME: parallel is broken in pyannote.metrics\n        params[""n_jobs""] = 1\n\n        app.validate(protocol, **params)\n\n    if arg[""apply""]:\n\n        validate_dir = Path(arg[""<validate>""]).expanduser().resolve(strict=True)\n\n        params[""subset""] = ""test"" if subset is None else subset\n        params[""batch_size""] = int(arg[""--batch""])\n\n        duration = arg[""--duration""]\n        if duration is not None:\n            duration = float(duration)\n        params[""duration""] = duration\n\n        params[""step""] = float(arg[""--step""])\n        params[""Pipeline""] = getattr(Application, ""Pipeline"", None)\n\n        params[""pretrained""] = arg[""--pretrained""]\n\n        apply_pretrained(validate_dir, protocol, **params)\n'"
pyannote/audio/applications/speaker_embedding.py,2,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2017-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\nimport torch\nimport numpy as np\nfrom typing import Optional\n\nfrom .base import Application\n\nfrom pyannote.core import Segment, Timeline, Annotation\n\nfrom pyannote.database import get_protocol\nfrom pyannote.database import get_annotated\nfrom pyannote.database import get_unique_identifier\nfrom pyannote.database import FileFinder\nfrom pyannote.database.protocol import SpeakerDiarizationProtocol\nfrom pyannote.database.protocol import SpeakerVerificationProtocol\n\nimport scipy.optimize\nfrom scipy.cluster.hierarchy import fcluster\nfrom pyannote.core.utils.hierarchy import linkage\n\nfrom pyannote.core.utils.distance import pdist\nfrom pyannote.core.utils.distance import cdist\nfrom pyannote.audio.features.precomputed import Precomputed\n\nfrom pyannote.metrics.binary_classification import det_curve\nfrom pyannote.metrics.diarization import DiarizationPurityCoverageFMeasure\n\nfrom pyannote.audio.features import Pretrained\nfrom pyannote.audio.features.utils import get_audio_duration\n\n\nclass SpeakerEmbedding(Application):\n    @property\n    def config_default_module(self):\n        return ""pyannote.audio.embedding.approaches""\n\n    def validation_criterion(self, protocol_name, **kwargs):\n        protocol = get_protocol(protocol_name)\n        if isinstance(protocol, SpeakerVerificationProtocol):\n            return ""equal_error_rate""\n        elif isinstance(protocol, SpeakerDiarizationProtocol):\n            return ""diarization_fscore""\n\n    def validate_init(self, protocol_name, subset=""development""):\n\n        protocol = get_protocol(protocol_name)\n\n        if isinstance(\n            protocol, (SpeakerVerificationProtocol, SpeakerDiarizationProtocol)\n        ):\n            return\n\n        msg = (\n            ""Only SpeakerVerification or SpeakerDiarization tasks are""\n            \'supported in ""validation"" mode.\'\n        )\n        raise ValueError(msg)\n\n    def validate_epoch(self, epoch, validation_data, protocol=None, **kwargs):\n\n        _protocol = get_protocol(protocol)\n\n        if isinstance(_protocol, SpeakerVerificationProtocol):\n            return self._validate_epoch_verification(\n                epoch, validation_data, protocol=protocol, **kwargs\n            )\n\n        elif isinstance(_protocol, SpeakerDiarizationProtocol):\n            return self._validate_epoch_diarization(\n                epoch, validation_data, protocol=protocol, **kwargs\n            )\n\n        else:\n            msg = (\n                ""Only SpeakerVerification or SpeakerDiarization tasks are""\n                \'supported in ""validation"" mode.\'\n            )\n            raise ValueError(msg)\n\n    @staticmethod\n    def get_hash(file):\n        hashable = []\n        for f in file.files():\n            hashable.append((f[""uri""], tuple(f[""try_with""])))\n        return hash(tuple(sorted(hashable)))\n\n    @staticmethod\n    def get_embedding(file, pretrained):\n        emb = []\n        for f in file.files():\n            if isinstance(f[""try_with""], Segment):\n                segments = [f[""try_with""]]\n            else:\n                segments = f[""try_with""]\n            for segment in segments:\n                emb.append(pretrained.crop(f, segment, mode=""center""))\n\n        return np.mean(np.vstack(emb), axis=0, keepdims=True)\n\n    def _validate_epoch_verification(\n        self,\n        epoch,\n        validation_data,\n        protocol=None,\n        subset=""development"",\n        device: Optional[torch.device] = None,\n        batch_size: int = 32,\n        n_jobs: int = 1,\n        duration: float = None,\n        step: float = 0.25,\n        metric: str = None,\n        **kwargs,\n    ):\n\n        # initialize embedding extraction\n        pretrained = Pretrained(\n            validate_dir=self.validate_dir_,\n            epoch=epoch,\n            duration=duration,\n            step=step,\n            batch_size=batch_size,\n            device=device,\n        )\n\n        preprocessors = self.preprocessors_\n        if ""audio"" not in preprocessors:\n            preprocessors[""audio""] = FileFinder()\n        if ""duration"" not in preprocessors:\n            preprocessors[""duration""] = get_audio_duration\n        _protocol = get_protocol(protocol, progress=False, preprocessors=preprocessors)\n\n        y_true, y_pred, cache = [], [], {}\n\n        for trial in getattr(_protocol, ""{0}_trial"".format(subset))():\n\n            # compute embedding for file1\n            file1 = trial[""file1""]\n            hash1 = self.get_hash(file1)\n            if hash1 in cache:\n                emb1 = cache[hash1]\n            else:\n                emb1 = self.get_embedding(file1, pretrained)\n                cache[hash1] = emb1\n\n            # compute embedding for file2\n            file2 = trial[""file2""]\n            hash2 = self.get_hash(file2)\n            if hash2 in cache:\n                emb2 = cache[hash2]\n            else:\n                emb2 = self.get_embedding(file2, pretrained)\n                cache[hash2] = emb2\n\n            # compare embeddings\n            distance = cdist(emb1, emb2, metric=metric)[0, 0]\n            y_pred.append(distance)\n\n            y_true.append(trial[""reference""])\n\n        _, _, _, eer = det_curve(np.array(y_true), np.array(y_pred), distances=True)\n\n        return {""metric"": ""equal_error_rate"", ""minimize"": True, ""value"": float(eer)}\n\n    def _validate_epoch_diarization(\n        self,\n        epoch,\n        validation_data,\n        protocol=None,\n        subset=""development"",\n        device: Optional[torch.device] = None,\n        batch_size: int = 32,\n        n_jobs: int = 1,\n        duration: float = None,\n        step: float = 0.25,\n        metric: str = None,\n        **kwargs,\n    ):\n\n        # initialize embedding extraction\n        pretrained = Pretrained(\n            validate_dir=self.validate_dir_,\n            epoch=epoch,\n            duration=duration,\n            step=step,\n            batch_size=batch_size,\n            device=device,\n        )\n\n        preprocessors = self.preprocessors_\n        if ""audio"" not in preprocessors:\n            preprocessors[""audio""] = FileFinder()\n        if ""duration"" not in preprocessors:\n            preprocessors[""duration""] = get_audio_duration\n        _protocol = get_protocol(protocol, progress=False, preprocessors=preprocessors)\n\n        Z, t = dict(), dict()\n        min_d, max_d = np.inf, -np.inf\n\n        for current_file in getattr(_protocol, subset)():\n\n            uri = get_unique_identifier(current_file)\n            uem = get_annotated(current_file)\n            reference = current_file[""annotation""]\n\n            X_, t_ = [], []\n            embedding = pretrained(current_file)\n            for i, (turn, _) in enumerate(reference.itertracks()):\n\n                # extract embedding for current speech turn\n                x_ = embedding.crop(turn, mode=""center"")\n                if len(x_) < 1:\n                    x_ = embedding.crop(turn, mode=""loose"")\n                if len(x_) < 1:\n                    msg = f""No embedding for {turn} in {uri:s}.""\n                    raise ValueError(msg)\n\n                # each speech turn is represented by its average embedding\n                X_.append(np.mean(x_, axis=0))\n                t_.append(turn)\n\n            X_ = np.array(X_)\n            # apply hierarchical agglomerative clustering\n            # all the way up to just one cluster (ie complete dendrogram)\n            D = pdist(X_, metric=metric)\n            min_d = min(np.min(D), min_d)\n            max_d = max(np.max(D), max_d)\n\n            Z[uri] = linkage(X_, method=""pool"", metric=metric)\n            t[uri] = np.array(t_)\n\n        def fun(threshold):\n\n            _metric = DiarizationPurityCoverageFMeasure(weighted=False)\n\n            for current_file in getattr(_protocol, subset)():\n\n                uri = get_unique_identifier(current_file)\n                uem = get_annotated(current_file)\n                reference = current_file[""annotation""]\n\n                clusters = fcluster(Z[uri], threshold, criterion=""distance"")\n\n                hypothesis = Annotation(uri=uri)\n                for (start_time, end_time), cluster in zip(t[uri], clusters):\n                    hypothesis[Segment(start_time, end_time)] = cluster\n\n                _ = _metric(reference, hypothesis, uem=uem)\n\n            return 1.0 - abs(_metric)\n\n        res = scipy.optimize.minimize_scalar(\n            fun, bounds=(0.0, 1.0), method=""bounded"", options={""maxiter"": 10}\n        )\n\n        threshold = res.x.item()\n\n        return {\n            ""metric"": ""diarization_fscore"",\n            ""minimize"": False,\n            ""value"": float(1.0 - res.fun),\n        }\n'"
pyannote/audio/applications/speech_detection.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2017-2019 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n\nfrom functools import partial\nimport scipy.optimize\nfrom .base_labeling import BaseLabeling\nfrom pyannote.database import get_annotated\nfrom pyannote.audio.features import Pretrained\nfrom pyannote.audio.pipeline import (\n    SpeechActivityDetection as SpeechActivityDetectionPipeline,\n)\n\n\ndef validate_helper_func(current_file, pipeline=None, metric=None):\n    reference = current_file[""annotation""]\n    uem = get_annotated(current_file)\n    hypothesis = pipeline(current_file)\n    return metric(reference, hypothesis, uem=uem)\n\n\nclass SpeechActivityDetection(BaseLabeling):\n\n    Pipeline = SpeechActivityDetectionPipeline\n\n    def validation_criterion(self, protocol, **kwargs):\n        return f""detection_fscore""\n\n    def validate_epoch(\n        self,\n        epoch,\n        validation_data,\n        device=None,\n        batch_size=32,\n        n_jobs=1,\n        duration=None,\n        step=0.25,\n        **kwargs,\n    ):\n\n        # compute (and store) SAD scores\n        pretrained = Pretrained(\n            validate_dir=self.validate_dir_,\n            epoch=epoch,\n            duration=duration,\n            step=step,\n            batch_size=batch_size,\n            device=device,\n        )\n\n        for current_file in validation_data:\n            current_file[""scores""] = pretrained(current_file)\n\n        # pipeline\n        pipeline = self.Pipeline(scores=""@scores"", fscore=True)\n\n        def fun(threshold):\n            pipeline.instantiate(\n                {\n                    ""onset"": threshold,\n                    ""offset"": threshold,\n                    ""min_duration_on"": 0.100,\n                    ""min_duration_off"": 0.100,\n                    ""pad_onset"": 0.0,\n                    ""pad_offset"": 0.0,\n                }\n            )\n            metric = pipeline.get_metric(parallel=True)\n            validate = partial(validate_helper_func, pipeline=pipeline, metric=metric)\n            if n_jobs > 1:\n                _ = self.pool_.map(validate, validation_data)\n            else:\n                for file in validation_data:\n                    _ = validate(file)\n\n            return 1.0 - abs(metric)\n\n        res = scipy.optimize.minimize_scalar(\n            fun, bounds=(0.0, 1.0), method=""bounded"", options={""maxiter"": 10}\n        )\n\n        threshold = res.x.item()\n\n        return {\n            ""metric"": self.validation_criterion(None),\n            ""minimize"": False,\n            ""value"": float(1.0 - res.fun),\n            ""pipeline"": pipeline.instantiate(\n                {\n                    ""onset"": threshold,\n                    ""offset"": threshold,\n                    ""min_duration_on"": 0.100,\n                    ""min_duration_off"": 0.100,\n                    ""pad_onset"": 0.0,\n                    ""pad_offset"": 0.0,\n                }\n            ),\n        }\n'"
pyannote/audio/augmentation/__init__.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2018-2019 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n""""""\n# Data augmentation\n""""""\n\nfrom .base import Augmentation\nfrom .base import NoAugmentation\nfrom .noise import AddNoise\nfrom .noise import AddNoiseFromGaps\n\n# from .reverb import Reverb\n'"
pyannote/audio/augmentation/base.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2018 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n\nclass Augmentation(object):\n    def __call__(self, waveform, sample_rate):\n        return waveform\n\n\nNoAugmentation = Augmentation\n'"
pyannote/audio/augmentation/noise.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2018-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n""""""\n# Noise-based data augmentation\n""""""\n\n\nimport random\nimport numpy as np\nfrom pyannote.core import Segment\nfrom pyannote.audio.features.utils import RawAudio\nfrom pyannote.audio.features.utils import get_audio_duration\nfrom pyannote.core.utils.random import random_subsegment\nfrom pyannote.core.utils.random import random_segment\nfrom pyannote.database import get_protocol\nfrom pyannote.database import get_annotated\nfrom pyannote.database import FileFinder\nfrom .base import Augmentation\n\n\nnormalize = lambda wav: wav / (np.sqrt(np.mean(wav ** 2)) + 1e-8)\n\n\nclass AddNoise(Augmentation):\n    """"""Additive noise data augmentation\n\n    Parameters\n    ----------\n    collection : `str` or `list` of `str`\n        `pyannote.database` collection(s) used for adding noise. Defaults to\n        \'MUSAN.Collection.BackgroundNoise\' available in `pyannote.db.musan`\n        package.\n    snr_min, snr_max : int, optional\n        Defines Signal-to-Noise Ratio (SNR) range in dB. Defaults to [5, 20].\n    """"""\n\n    def __init__(self, collection=None, snr_min=5, snr_max=20):\n        super().__init__()\n\n        if collection is None:\n            collection = ""MUSAN.Collection.BackgroundNoise""\n        if not isinstance(collection, (list, tuple)):\n            collection = [collection]\n        self.collection = collection\n\n        self.snr_min = snr_min\n        self.snr_max = snr_max\n\n        # load noise database\n        self.files_ = []\n        preprocessors = {""audio"": FileFinder(), ""duration"": get_audio_duration}\n        for collection in self.collection:\n            protocol = get_protocol(collection, preprocessors=preprocessors)\n            self.files_.extend(protocol.files())\n\n    def __call__(self, original, sample_rate):\n        """"""Augment original waveform\n\n        Parameters\n        ----------\n        original : `np.ndarray`\n            (n_samples, n_channels) waveform.\n        sample_rate : `int`\n            Sample rate.\n\n        Returns\n        -------\n        augmented : `np.ndarray`\n            (n_samples, n_channels) noise-augmented waveform.\n        """"""\n\n        raw_audio = RawAudio(sample_rate=sample_rate, mono=True)\n\n        original_duration = len(original) / sample_rate\n\n        # accumulate enough noise to cover duration of original waveform\n        noises = []\n        left = original_duration\n        while left > 0:\n\n            # select noise file at random\n            file = random.choice(self.files_)\n            duration = file[""duration""]\n\n            # if noise file is longer than what is needed, crop it\n            if duration > left:\n                segment = next(random_subsegment(Segment(0, duration), left))\n                noise = raw_audio.crop(file, segment, mode=""center"", fixed=left)\n                left = 0\n\n            # otherwise, take the whole file\n            else:\n                noise = raw_audio(file).data\n                left -= duration\n\n            noise = normalize(noise)\n            noises.append(noise)\n\n        # concatenate\n        # FIXME: use fade-in between concatenated noises\n        noise = np.vstack(noises)\n\n        # select SNR at random\n        snr = (self.snr_max - self.snr_min) * np.random.random_sample() + self.snr_min\n        alpha = np.exp(-np.log(10) * snr / 20)\n\n        return normalize(original) + alpha * noise\n\n\nclass AddNoiseFromGaps(Augmentation):\n    """"""Additive noise data augmentation.\n\n    While AddNoise assumes that files contain only noise, this class uses\n    non-speech regions (= gaps) as noise. This is expected to generate more\n    realistic noises.\n\n    Parameters\n    ----------\n    protocol : `str` or `pyannote.database.Protocol`\n        Protocol name (e.g. AMI.SpeakerDiarization.MixHeadset)\n    subset : {\'train\', \'development\', \'test\'}, optional\n        Use this subset. Defaults to \'train\'.\n    snr_min, snr_max : int, optional\n        Defines Signal-to-Noise Ratio (SNR) range in dB. Defaults to [5, 20].\n\n    See also\n    --------\n    `AddNoise`\n    """"""\n\n    def __init__(self, protocol=None, subset=""train"", snr_min=5, snr_max=20):\n        super().__init__()\n\n        self.protocol = protocol\n        self.subset = subset\n        self.snr_min = snr_min\n        self.snr_max = snr_max\n\n        # returns gaps in annotation as pyannote.core.Timeline instance\n        get_gaps = (\n            lambda f: f[""annotation""].get_timeline().gaps(support=get_annotated(f))\n        )\n\n        if isinstance(protocol, str):\n            preprocessors = {\n                ""audio"": FileFinder(),\n                ""duration"": get_audio_duration,\n                ""gaps"": get_gaps,\n            }\n            protocol = get_protocol(self.protocol, preprocessors=preprocessors)\n        else:\n            protocol.preprocessors[""gaps""] = get_gaps\n\n        self.files_ = list(getattr(protocol, self.subset)())\n\n    def __call__(self, original, sample_rate):\n        """"""Augment original waveform\n\n        Parameters\n        ----------\n        original : `np.ndarray`\n            (n_samples, n_channels) waveform.\n        sample_rate : `int`\n            Sample rate.\n\n        Returns\n        -------\n        augmented : `np.ndarray`\n            (n_samples, n_channels) noise-augmented waveform.\n        """"""\n\n        raw_audio = RawAudio(sample_rate=sample_rate, mono=True)\n\n        # accumulate enough noise to cover duration of original waveform\n        noises = []\n        len_left = len(original)\n        while len_left > 0:\n\n            # select noise file at random\n            file = random.choice(self.files_)\n\n            # select noise segment at random\n            segment = next(random_segment(file[""gaps""], weighted=False))\n            duration = segment.duration\n            segment_len = duration * sample_rate\n\n            # if noise segment is longer than what is needed, crop it at random\n            if segment_len > len_left:\n                duration = len_left / sample_rate\n                segment = next(random_subsegment(segment, duration))\n\n            noise = raw_audio.crop(file, segment, mode=""center"", fixed=duration)\n\n            # decrease the `len_left` value by the size of the returned noise\n            len_left -= len(noise)\n\n            noise = normalize(noise)\n            noises.append(noise)\n\n        # concatenate\n        # FIXME: use fade-in between concatenated noises\n        noise = np.vstack(noises)\n\n        # select SNR at random\n        snr = (self.snr_max - self.snr_min) * np.random.random_sample() + self.snr_min\n        alpha = np.exp(-np.log(10) * snr / 20)\n\n        return normalize(original) + alpha * noise\n'"
pyannote/audio/augmentation/reverb.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2019-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n\nfrom typing import Tuple\nfrom typing import Optional\nfrom .utils import NoiseCollection\n\nfrom .base import Augmentation\nfrom .utils import Noise\nimport pyroomacoustics as pra\nimport numpy as np\nimport threading\nimport collections\n\nnormalize = lambda wav: wav / (np.sqrt(np.mean(wav ** 2)) + 1e-8)\n\n\nclass Reverb(Augmentation):\n    """"""Simulate indoor reverberation\n\n    Parameters\n    ----------\n    depth : (float, float), optional\n        Minimum and maximum values for room depth (in meters).\n        Defaults to (2.0, 10.0).\n    width : (float, float), optional\n        Minimum and maximum values for room width (in meters).\n        Defaults to (1.0, 10.0).\n    height : (float, float), optional\n        Minimum and maximum values for room heigth (in meters).\n        Defaults to (2.0, 5.0).\n    absorption : (float, float), optional\n        Minimum and maximum values of walls absorption coefficient.\n        Defaults to (0.2, 0.9).\n    noise : str or list of str, optional\n        `pyannote.database` collection(s) used for adding noise.\n        Defaults to ""MUSAN.Collection.BackgroundNoise""\n    snr : (float, float), optional\n        Minimum and maximum values of signal-to-noise ratio.\n        Defaults to (5.0, 15.0)\n\n    """"""\n\n    def __init__(\n        self,\n        depth: Tuple[float, float] = (2.0, 10.0),\n        width: Tuple[float, float] = (1.0, 10.0),\n        height: Tuple[float, float] = (2.0, 5.0),\n        absorption: Tuple[float, float] = (0.2, 0.9),\n        noise: Optional[NoiseCollection] = None,\n        snr: Tuple[float, float] = (5.0, 15.0),\n    ):\n\n        super().__init__()\n        self.depth = depth\n        self.width = width\n        self.height = height\n        self.absorption = absorption\n        self.max_order_ = 17\n\n        self.noise = noise\n        self.snr = snr\n        self.noise_ = Noise(collection=self.noise)\n\n        self.n_rooms_ = 128\n        self.new_rooms_prob_ = 0.001\n        self.main_lock_ = threading.Lock()\n        self.rooms_ = collections.deque(maxlen=self.n_rooms_)\n        self.room_lock_ = [threading.Lock() for _ in range(self.n_rooms_)]\n\n    @staticmethod\n    def random(m: float, M: float):\n        return (M - m) * np.random.random_sample() + m\n\n    def new_room(self, sample_rate: int):\n\n        # generate a room at random\n        depth = self.random(*self.depth)\n        width = self.random(*self.width)\n        height = self.random(*self.height)\n        absorption = self.random(*self.absorption)\n        room = pra.ShoeBox(\n            [depth, width, height],\n            fs=sample_rate,\n            absorption=absorption,\n            max_order=self.max_order_,\n        )\n\n        # play the original audio chunk at a random location\n        original = [\n            self.random(0, depth),\n            self.random(0, width),\n            self.random(0, height),\n        ]\n        room.add_source(original)\n\n        # play the noise audio chunk at a random location\n        noise = [self.random(0, depth), self.random(0, width), self.random(0, height)]\n        room.add_source(noise)\n\n        # place the microphone at a random location\n        microphone = [\n            self.random(0, depth),\n            self.random(0, width),\n            self.random(0, height),\n        ]\n        room.add_microphone_array(\n            pra.MicrophoneArray(np.c_[microphone, microphone], sample_rate)\n        )\n\n        room.compute_rir()\n\n        return room\n\n    def __call__(self, original: np.ndarray, sample_rate: int) -> np.ndarray:\n\n        with self.main_lock_:\n\n            # initialize rooms (with 2 sources and 1 microphone)\n            while len(self.rooms_) < self.n_rooms_:\n                room = self.new_room(sample_rate)\n                self.rooms_.append(room)\n\n            # create new room with probability new_rooms_prob_\n            if np.random.rand() > 1.0 - self.new_rooms_prob_:\n                room = self.new_room(sample_rate)\n                self.rooms_.append(room)\n\n            # choose one room at random\n            index = np.random.choice(self.n_rooms_)\n\n        # lock chosen room to ensure room.sources are not updated concurrently\n        with self.room_lock_[index]:\n\n            room = self.rooms_[index]\n\n            # play normalized original audio chunk at source #1\n            n_samples = len(original)\n            original = normalize(original).squeeze()\n            room.sources[0].add_signal(original)\n\n            # generate noise with random SNR\n            noise = self.noise_(n_samples, sample_rate).squeeze()\n            snr = self.random(*self.snr)\n            alpha = np.exp(-np.log(10) * snr / 20)\n            noise *= alpha\n\n            # play noise at source #2\n            room.sources[1].add_signal(noise)\n\n            # simulate room and return microphone signal\n            room.simulate()\n            return room.mic_array.signals[0, :n_samples, np.newaxis]\n'"
pyannote/audio/augmentation/utils.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2019-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n\nfrom typing import Union\nfrom typing import List\nfrom typing import Optional\n\nNoiseCollection = Union[str, List[str]]\n\nimport random\nimport numpy as np\n\nfrom pyannote.core import Segment\nfrom pyannote.core.utils.random import random_subsegment\n\nfrom pyannote.audio.features import RawAudio\nfrom pyannote.audio.features.utils import get_audio_duration\n\nfrom pyannote.database import FileFinder\nfrom pyannote.database import get_protocol\n\n\nnormalize = lambda wav: wav / (np.sqrt(np.mean(wav ** 2)) + 1e-8)\n\n\nclass Noise:\n    """"""Noise generator\n\n    Parameters\n    ----------\n    collection : str or list of str\n        `pyannote.database` collection(s) used for adding noise.\n        Defaults to ""MUSAN.Collection.BackgroundNoise""\n    """"""\n\n    def __init__(self, collection: Optional[NoiseCollection] = None):\n\n        if collection is None:\n            collection = ""MUSAN.Collection.BackgroundNoise""\n\n        if not isinstance(collection, (list, tuple)):\n            collection = [collection]\n\n        self.collection = collection\n\n        self.files_ = []\n        preprocessors = {""audio"": FileFinder(), ""duration"": get_audio_duration}\n        for collection in self.collection:\n            protocol = get_protocol(collection, preprocessors=preprocessors)\n            self.files_.extend(protocol.files())\n\n    def __call__(self, n_samples: int, sample_rate: int) -> np.ndarray:\n        """"""Generate noise\n\n        Parameters\n        ----------\n        n_samples : int\n        sample_rate : int\n\n        Returns\n        -------\n        noise : (n_samples, 1) np.ndarray\n        """"""\n\n        target_duration = n_samples / sample_rate\n\n        raw_audio = RawAudio(sample_rate=sample_rate, mono=True)\n\n        # accumulate enough noise to cover duration\n        noises = []\n        left = target_duration\n        while left > 0:\n\n            # select noise file at random\n            file = random.choice(self.files_)\n            duration = file[""duration""]\n\n            # if noise file is longer than what is needed, crop it\n            if duration > left:\n                segment = next(random_subsegment(Segment(0, duration), left))\n                noise = raw_audio.crop(file, segment, mode=""center"", fixed=left)\n                left = 0\n\n            # otherwise, take the whole file\n            else:\n                noise = raw_audio(file).data\n                left -= duration\n\n            noise = normalize(noise)\n            noises.append(noise)\n\n        # concatenate\n        noise = np.vstack(noises)\n        # TODO: use fade-in between concatenated noises\n\n        return noise\n'"
pyannote/audio/embedding/__init__.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2014-2017 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n""""""\n# Sequence embedding\n""""""\n'"
pyannote/audio/embedding/generators.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2017-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\nfrom typing import Optional\nfrom typing import Text\nfrom pyannote.database.protocol.protocol import Protocol\nimport itertools\nimport numpy as np\nfrom pyannote.core import Segment\nfrom pyannote.core.utils.random import random_segment\nfrom pyannote.core.utils.random import random_subsegment\nfrom pyannote.audio.train.task import Task, TaskType, TaskOutput\nfrom ..train.generator import BatchGenerator\nfrom pyannote.audio.features.wrapper import Wrapper, Wrappable\nfrom pyannote.audio.train.task import Task\n\n\nclass SpeechSegmentGenerator(BatchGenerator):\n    """"""Generate batch of pure speech segments with associated speaker labels\n\n    Parameters\n    ----------\n    feature_extraction : `pyannote.audio.features.FeatureExtraction`\n        Feature extraction.\n    protocol : `pyannote.database.Protocol`\n    subset : {\'train\', \'development\', \'test\'}\n    duration : float, optional\n        Chunks duration, in seconds. Defaults to 1.\n    min_duration : float, optional\n        When provided, generate chunks of random duration between `min_duration`\n        and `duration`. All chunks in a batch will still use the same duration.\n        Defaults to generating fixed duration chunks.\n    per_turn : int, optional\n        Number of chunks per speech turn. Defaults to 1.\n    per_label : int, optional\n        Number of speech turns per speaker in each batch.\n        Defaults to 3.\n    per_fold : int, optional\n        Number of different speakers in each batch.\n        Defaults to all speakers.\n    per_epoch : float, optional\n        Force total audio duration per epoch, in days.\n        Defaults to total duration of protocol subset.\n    label_min_duration : float, optional\n        Remove speakers with less than `label_min_duration` seconds of speech.\n        Defaults to 0 (i.e. keep it all).\n    """"""\n\n    def __init__(\n        self,\n        feature_extraction: Wrappable,\n        protocol: Protocol,\n        subset: Text = ""train"",\n        duration: float = 1.0,\n        min_duration: float = None,\n        per_turn: int = 1,\n        per_label: int = 3,\n        per_fold: Optional[int] = None,\n        per_epoch: float = None,\n        label_min_duration: float = 0.0,\n    ):\n\n        self.feature_extraction = Wrapper(feature_extraction)\n        self.per_turn = per_turn\n        self.per_label = per_label\n        self.per_fold = per_fold\n        self.duration = duration\n        self.min_duration = duration if min_duration is None else min_duration\n        self.label_min_duration = label_min_duration\n        self.weighted_ = True\n\n        total_duration = self._load_metadata(protocol, subset=subset)\n        if per_epoch is None:\n            per_epoch = total_duration / (24 * 60 * 60)\n        self.per_epoch = per_epoch\n\n    def _load_metadata(self, protocol: Protocol, subset: Text = ""train"") -> float:\n        """"""Load training set metadata\n\n        This function is called once at instantiation time, returns the total\n        training set duration, and populates the following attributes:\n\n        Attributes\n        ----------\n        data_ : dict\n            Dictionary where keys are speaker labels and values are lists of\n            (segments, duration, current_file) tuples where\n            - segments is a list of segments by the speaker in the file\n            - duration is total duration of speech by the speaker in the file\n            - current_file is the file (as ProtocolFile)\n\n        segment_labels_ : list\n            Sorted list of (unique) labels in protocol.\n\n        file_labels_ : dict of list\n            Sorted lists of (unique) file-level labels in protocol\n\n        Returns\n        -------\n        duration : float\n            Total duration of annotated segments, in seconds.\n        """"""\n\n        self.data_ = {}\n        segment_labels, file_labels = set(), dict()\n\n        # loop once on all files\n        for current_file in getattr(protocol, subset)():\n\n            # keep track of unique file labels\n            for key in current_file:\n                if key in [""annotation"", ""annotated"", ""audio"", ""duration""]:\n                    continue\n                if key not in file_labels:\n                    file_labels[key] = set()\n                file_labels[key].add(current_file[key])\n\n            # get annotation for current file\n            # ensure annotation is cropped to actual file duration\n            support = Segment(start=0, end=current_file[""duration""])\n            current_file[""annotation""] = current_file[""annotation""].crop(\n                support, mode=""intersection""\n            )\n            annotation = current_file[""annotation""]\n\n            # loop on each label in current file\n            for label in annotation.labels():\n\n                # get all segments with current label\n                timeline = annotation.label_timeline(label)\n\n                # remove segments shorter than maximum chunk duration\n                segments = [s for s in timeline if s.duration > self.duration]\n\n                # corner case where no segment is long enough\n                # and we removed them all...\n                if not segments:\n                    continue\n\n                # total duration of label in current_file (after removal of\n                # short segments).\n                duration = sum(s.duration for s in segments)\n\n                # store all these in data_ dictionary\n                # datum = (segment_generator, duration, current_file, features)\n                datum = (segments, duration, current_file)\n                self.data_.setdefault(label, []).append(datum)\n\n        # remove labels with less than \'label_min_duration\' of speech\n        # otherwise those may generate the same segments over and over again\n        dropped_labels = set()\n        for label, data in self.data_.items():\n            total_duration = sum(datum[1] for datum in data)\n            if total_duration < self.label_min_duration:\n                dropped_labels.add(label)\n\n        for label in dropped_labels:\n            self.data_.pop(label)\n\n        self.file_labels_ = {k: sorted(file_labels[k]) for k in file_labels}\n        self.segment_labels_ = sorted(self.data_)\n\n        return sum(sum(datum[1] for datum in data) for data in self.data_.values())\n\n    def samples(self):\n\n        labels = list(self.data_)\n\n        # batch_counter counts samples in current batch.\n        # as soon as it reaches batch_size, a new random duration is selected\n        # so that the next batch will use a different chunk duration\n        batch_counter = 0\n        batch_size = self.batch_size\n        batch_duration = self.min_duration + np.random.rand() * (\n            self.duration - self.min_duration\n        )\n\n        while True:\n\n            # shuffle labels\n            np.random.shuffle(labels)\n\n            # loop on each label\n            for label in labels:\n\n                # load data for this label\n                # segment_generators, durations, files, features = \\\n                #     zip(*self.data_[label])\n                segments, durations, files = zip(*self.data_[label])\n\n                # choose \'per_label\' files at random with probability\n                # proportional to the total duration of \'label\' in those files\n                probabilities = durations / np.sum(durations)\n                chosen = np.random.choice(\n                    len(files), size=self.per_label, p=probabilities\n                )\n\n                # loop on (randomly) chosen files\n                for i in chosen:\n\n                    # choose one segment at random with\n                    # probability proportional to duration\n                    # segment = next(segment_generators[i])\n                    segment = next(random_segment(segments[i], weighted=self.weighted_))\n\n                    # choose per_turn chunk(s) at random\n                    for chunk in itertools.islice(\n                        random_subsegment(segment, batch_duration), self.per_turn\n                    ):\n\n                        yield {\n                            ""X"": self.feature_extraction.crop(\n                                files[i], chunk, mode=""center"", fixed=batch_duration\n                            ),\n                            ""y"": self.segment_labels_.index(label),\n                        }\n\n                        # increment number of samples in current batch\n                        batch_counter += 1\n\n                        # as soon as the batch is complete, a new random\n                        # duration is selected so that the next batch will use\n                        # a different chunk duration\n                        if batch_counter == batch_size:\n                            batch_counter = 0\n                            batch_duration = self.min_duration + np.random.rand() * (\n                                self.duration - self.min_duration\n                            )\n\n    @property\n    def batch_size(self) -> int:\n        if self.per_fold is not None:\n            return self.per_turn * self.per_label * self.per_fold\n        return self.per_turn * self.per_label * len(self.data_)\n\n    @property\n    def batches_per_epoch(self) -> int:\n\n        # duration per epoch\n        duration_per_epoch = self.per_epoch * 24 * 60 * 60\n\n        # (average) duration per batch\n        duration_per_batch = 0.5 * (self.min_duration + self.duration) * self.batch_size\n\n        # number of batches per epoch\n        return int(np.ceil(duration_per_epoch / duration_per_batch))\n\n    @property\n    def specifications(self):\n        return {\n            ""X"": {""dimension"": self.feature_extraction.dimension},\n            ""y"": {""classes"": self.segment_labels_},\n            ""task"": Task(\n                type=TaskType.REPRESENTATION_LEARNING, output=TaskOutput.VECTOR\n            ),\n        }\n'"
pyannote/audio/features/__init__.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2014-2018 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n""""""\n# Feature extraction\n""""""\n\nfrom .base import FeatureExtraction\n\ntry:\n    from .with_librosa import LibrosaMFCC, LibrosaSpectrogram, LibrosaMelSpectrogram\nexcept Exception as e:\n    msg = (\n        f\'Feature extractors based on ""librosa"" are not available \'\n        f\'because something went wrong when importing them: ""{e}"".\'\n    )\n    print(msg)\n\nfrom .precomputed import Precomputed\n\ntry:\n    from .utils import RawAudio\nexcept Exception as e:\n    msg = f""Loading raw audio might fail because something went wrong: {e}.""\n    print(msg)\n\ntry:\n    from .pretrained import Pretrained\nexcept Exception as e:\n    msg = (\n        f""Feature extraction using pretrained models are not available ""\n        f\'because something went wrong at import: ""{e}"".\'\n    )\n    print(msg)\n'"
pyannote/audio/features/base.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2016-2019 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\nimport warnings\nimport numpy as np\n\nfrom .utils import RawAudio\n\nfrom pyannote.core import Segment\nfrom pyannote.core import SlidingWindow\nfrom pyannote.core import SlidingWindowFeature\n\nfrom pyannote.database import get_unique_identifier\n\nfrom librosa.util import valid_audio\nfrom librosa.util.exceptions import ParameterError\n\n\nclass FeatureExtraction:\n    """"""Base class for feature extraction\n\n    Parameters\n    ----------\n    augmentation : `pyannote.audio.augmentation.Augmentation`, optional\n        Data augmentation.\n    sample_rate : int, optional\n        Convert audio to use this sample rate.\n\n    See also\n    --------\n    `pyannote.audio.augmentation.AddNoise`\n    """"""\n\n    def __init__(self, augmentation=None, sample_rate=None):\n        super().__init__()\n        self.sample_rate = sample_rate\n\n        # used in FeatureExtraction.crop\n        self.raw_audio_ = RawAudio(\n            sample_rate=self.sample_rate, mono=True, augmentation=augmentation\n        )\n\n    def augmentation():\n        doc = ""Data augmentation.""\n\n        def fget(self):\n            return self.raw_audio_.augmentation\n\n        def fset(self, augmentation):\n            self.raw_audio_.augmentation = augmentation\n\n        return locals()\n\n    augmentation = property(**augmentation())\n\n    def get_dimension(self):\n        """"""Get dimension of feature vectors\n\n        Returns\n        -------\n        dimension : int\n            Dimension of feature vectors\n        """"""\n        msg = ""`FeatureExtraction subclasses must implement "" ""`get_dimension` method.""\n        raise NotImplementedError(msg)\n\n    @property\n    def dimension(self):\n        """"""Dimension of feature vectors""""""\n        return self.get_dimension()\n\n    def get_resolution(self):\n        """"""Get sliding window used for feature extraction\n\n        Returns\n        -------\n        sliding_window : `pyannote.core.SlidingWindow`\n            Sliding window used for feature extraction.\n        """"""\n\n        msg = (\n            ""`FeatureExtraction` subclasses must implement "" ""`get_resolution` method.""\n        )\n        raise NotImplementedError(msg)\n\n    @property\n    def sliding_window(self):\n        """"""Sliding window used for feature extraction""""""\n        return self.get_resolution()\n\n    def get_features(self, y, sample_rate):\n        """"""Extract features from waveform\n\n        Parameters\n        ----------\n        y : (n_samples, 1) numpy array\n            Waveform.\n        sample_rate : int\n            Sample rate.\n\n        Returns\n        -------\n        features : (n_frames, dimension) numpy array\n            Extracted features\n        """"""\n        msg = ""`FeatureExtractions subclasses must implement "" ""`get_features` method.""\n        raise NotImplementedError(msg)\n\n    def __call__(self, current_file) -> SlidingWindowFeature:\n        """"""Extract features from file\n\n        Parameters\n        ----------\n        current_file : dict\n            `pyannote.database` files.\n\n        Returns\n        -------\n        features : `pyannote.core.SlidingWindowFeature`\n            Extracted features\n        """"""\n\n        # load waveform, re-sample, convert to mono, augment, normalize\n        y, sample_rate = self.raw_audio_(current_file, return_sr=True)\n\n        # compute features\n        features = self.get_features(y.data, sample_rate)\n\n        # basic quality check\n        if np.any(np.isnan(features)):\n            uri = get_unique_identifier(current_file)\n            msg = f\'Features extracted from ""{uri}"" contain NaNs.\'\n            warnings.warn(msg.format(uri=uri))\n\n        # wrap features in a `SlidingWindowFeature` instance\n        return SlidingWindowFeature(features, self.sliding_window)\n\n    def get_context_duration(self) -> float:\n        """"""\n\n        TODO. explain why this is needed\n\n        Returns\n        -------\n        context : float\n            Context duration, in seconds.\n        """"""\n        return 0.0\n\n    def crop(self, current_file, segment, mode=""center"", fixed=None) -> np.ndarray:\n        """"""Fast version of self(current_file).crop(segment, mode=\'center\',\n+                                                  fixed=segment.duration)\n\n        Parameters\n        ----------\n        current_file : dict\n            `pyannote.database` file. Must contain a \'duration\' key that\n            provides the duration (in seconds) of the audio file.\n        segment : `pyannote.core.Segment`\n            Segment from which to extract features.\n\n        Returns\n        -------\n        features : (n_frames, dimension) numpy array\n            Extracted features\n\n        See also\n        --------\n        `pyannote.core.SlidingWindowFeature.crop`\n        """"""\n\n        context = self.get_context_duration()\n\n        # extend segment on both sides with requested context\n        xsegment = Segment(\n            max(0, segment.start - context),\n            min(current_file[""duration""], segment.end + context),\n        )\n\n        # obtain (augmented) waveform on this extended segment\n        y = self.raw_audio_.crop(\n            current_file, xsegment, mode=""center"", fixed=xsegment.duration\n        )\n\n        features = self.get_features(y, self.sample_rate)\n\n        # get rid of additional context before returning\n        frames = self.sliding_window\n        shifted_frames = SlidingWindow(\n            start=xsegment.start - frames.step,\n            step=frames.step,\n            duration=frames.duration,\n        )\n        ((start, end),) = shifted_frames.crop(\n            segment, mode=mode, fixed=fixed, return_ranges=True\n        )\n\n        # HACK for when start (returned by shifted_frames.crop) is negative\n        # due to floating point precision.\n        if start < 0:\n            if fixed is not None:\n                end -= start\n            start = 0\n\n        return features[start:end]\n'"
pyannote/audio/features/normalization.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2016-2018 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n\nimport numpy as np\nimport pandas as pd\nfrom pyannote.core import SlidingWindowFeature\n\n\nclass GlobalStandardization(object):\n    """"""Mean/variance normalization""""""\n\n    def get_context_duration(self):\n        return 0.0\n\n    def __call__(self, features, sliding_window=None):\n        """"""Apply global standardization\n\n        Parameters\n        ----------\n        features : `SlidingWindowFeature` or (n_samples, n_features ) `numpy.ndarray`\n            Features.\n        sliding_window : `SlidingWindow`, optional\n            Not used.\n\n        Returns\n        -------\n        normalized : `SlidingWindowFeature` or (n_samples, n_features ) `numpy.ndarray`\n            Standardized features\n        """"""\n\n        if isinstance(features, SlidingWindowFeature):\n            data = features.data\n        else:\n            data = features\n\n        mu = np.mean(data, axis=0)\n        sigma = np.std(data, axis=0, ddof=1)\n        sigma[sigma == 0.0] = 1e-6\n\n        normalized = (data - mu) / sigma\n\n        if isinstance(features, SlidingWindowFeature):\n            return SlidingWindowFeature(normalized, features.sliding_window)\n        else:\n            return normalized\n\n\nclass ShortTermStandardization(object):\n    """"""Short term mean/variance normalization\n\n    Parameters\n    ----------\n    duration : float\n        Window duration in seconds.\n    """"""\n\n    def __init__(self, duration=3.0):\n        super(ShortTermStandardization, self).__init__()\n        self.duration = duration\n\n    def get_context_duration(self):\n        return 0.5 * self.duration\n\n    def __call__(self, features, sliding_window=None):\n        """"""Apply short-term standardization\n\n        Parameters\n        ----------\n        features : `SlidingWindowFeature` or (n_samples, n_features ) `numpy.ndarray`\n            Features.\n        sliding_window : `SlidingWindow`, optional\n            Sliding window when `features` is a `numpy.ndarray`.\n            Not used when `features` is a `SlidingWindowFeature` instance.\n\n        Returns\n        -------\n        normalized : `SlidingWindowFeature` or (n_samples, n_features ) `numpy.ndarray`\n            Standardized features\n        """"""\n\n        if isinstance(features, SlidingWindowFeature):\n            features_ = features\n        else:\n            features_ = SlidingWindowFeature(features, sliding_window)\n\n        window = features_.sliding_window.samples(self.duration, mode=""center"")\n        if not window % 2:\n            window += 1\n\n        rolling = pd.DataFrame(features_.data).rolling(\n            window=window, center=True, min_periods=window\n        )\n        mu = np.array(rolling.mean())\n        sigma = np.array(rolling.std(ddof=1))\n\n        for i in range(window // 2):\n\n            data = features_.data[: i + window // 2 + 1, :]\n            mu[i] = np.mean(data, axis=0)\n            sigma[i] = np.std(data, axis=0, ddof=1)\n\n            data = features_.data[-i - window // 2 - 1 :, :]\n            mu[-i - 1] = np.mean(data, axis=0)\n            sigma[-i - 1] = np.std(data, axis=0, ddof=1)\n\n        sigma[sigma == 0.0] = 1e-6\n\n        normalized_ = (features_.data - mu) / sigma\n\n        if isinstance(features, SlidingWindowFeature):\n            return SlidingWindowFeature(normalized_, features.sliding_window)\n        else:\n            return normalized_\n'"
pyannote/audio/features/precomputed.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2018-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n\nimport yaml\nimport io\nfrom pathlib import Path\nfrom glob import glob\nimport numpy as np\nfrom numpy.lib.format import open_memmap\n\nfrom pyannote.core import SlidingWindow, SlidingWindowFeature\nfrom pyannote.database.util import get_unique_identifier\nfrom pyannote.audio.utils.path import mkdir_p\n\n\nclass PyannoteFeatureExtractionError(Exception):\n    pass\n\n\nclass Precomputed:\n    """"""Precomputed features\n\n    Parameters\n    ----------\n    root_dir : `str`\n        Path to directory where precomputed features are stored.\n    use_memmap : `bool`, optional\n        Defaults to False.\n    sliding_window : `SlidingWindow`, optional\n        Sliding window used for feature extraction. This is not used when\n        `root_dir` already exists and contains `metadata.yml`.\n    dimension : `int`, optional\n        Dimension of feature vectors. This is not used when `root_dir` already\n        exists and contains `metadata.yml`.\n    classes : iterable, optional\n        Human-readable name for each dimension.\n\n    Notes\n    -----\n    If `root_dir` directory does not exist, one must provide both\n    `sliding_window` and `dimension` parameters in order to create and\n    populate file `root_dir/metadata.yml` when instantiating.\n\n    """"""\n\n    def get_path(self, item):\n        uri = get_unique_identifier(item)\n        path = ""{root_dir}/{uri}.npy"".format(root_dir=self.root_dir, uri=uri)\n        return path\n\n    def __init__(\n        self,\n        root_dir=None,\n        use_memmap=False,\n        sliding_window=None,\n        dimension=None,\n        classes=None,\n        augmentation=None,\n    ):\n\n        if augmentation is not None:\n            msg = ""Data augmentation is not supported by `Precomputed`.""\n            raise ValueError(msg)\n\n        super(Precomputed, self).__init__()\n        self.root_dir = Path(root_dir).expanduser().resolve(strict=False)\n        self.use_memmap = use_memmap\n\n        path = self.root_dir / ""metadata.yml""\n        if path.exists():\n\n            with io.open(path, ""r"") as f:\n                params = yaml.load(f, Loader=yaml.SafeLoader)\n\n            self.dimension_ = params.pop(""dimension"")\n            self.classes_ = params.pop(""classes"", None)\n            self.sliding_window_ = SlidingWindow(**params)\n\n            if dimension is not None and self.dimension_ != dimension:\n                msg = \'inconsistent ""dimension"" (is: {0}, should be: {1})\'\n                raise ValueError(msg.format(dimension, self.dimensions_))\n\n            if classes is not None and self.classes_ != classes:\n                msg = \'inconsistent ""classes"" (is {0}, should be: {1})\'\n                raise ValueError(msg.format(classes, self.classes_))\n\n            if (sliding_window is not None) and (\n                (sliding_window.start != self.sliding_window_.start)\n                or (sliding_window.duration != self.sliding_window_.duration)\n                or (sliding_window.step != self.sliding_window_.step)\n            ):\n                msg = \'inconsistent ""sliding_window""\'\n                raise ValueError(msg)\n\n        else:\n\n            if dimension is None:\n                if classes is None:\n                    msg = (\n                        f""Please provide either `dimension` or `classes` ""\n                        f""parameters (or both) when instantiating ""\n                        f""`Precomputed`.""\n                    )\n                dimension = len(classes)\n\n            if sliding_window is None or dimension is None:\n                msg = (\n                    f""Either directory {self.root_dir} does not exist or it ""\n                    f""does not contain precomputed features. In case it exists ""\n                    f""and this was done on purpose, please provide both ""\n                    f""`sliding_window` and `dimension` parameters when ""\n                    f""instantianting `Precomputed`.""\n                )\n                raise ValueError(msg)\n\n            # create parent directory\n            mkdir_p(path.parent)\n\n            params = {\n                ""start"": sliding_window.start,\n                ""duration"": sliding_window.duration,\n                ""step"": sliding_window.step,\n                ""dimension"": dimension,\n            }\n            if classes is not None:\n                params[""classes""] = classes\n\n            with io.open(path, ""w"") as f:\n                yaml.dump(params, f, default_flow_style=False)\n\n            self.sliding_window_ = sliding_window\n            self.dimension_ = dimension\n            self.classes_ = classes\n\n    def augmentation():\n        doc = ""Data augmentation.""\n\n        def fget(self):\n            return None\n\n        def fset(self, augmentation):\n            if augmentation is not None:\n                msg = ""Data augmentation is not supported by `Precomputed`.""\n                raise AttributeError(msg)\n\n        return locals()\n\n    augmentation = property(**augmentation())\n\n    @property\n    def sliding_window(self):\n        """"""Sliding window used for feature extraction""""""\n        return self.sliding_window_\n\n    @property\n    def dimension(self):\n        """"""Dimension of feature vectors""""""\n        return self.dimension_\n\n    @property\n    def classes(self):\n        """"""Human-readable label of each dimension""""""\n        return self.classes_\n\n    def __call__(self, current_file):\n        """"""Obtain features for file\n\n        Parameters\n        ----------\n        current_file : dict\n            `pyannote.database` files.\n\n        Returns\n        -------\n        features : `pyannote.core.SlidingWindowFeature`\n            Features\n        """"""\n\n        path = Path(self.get_path(current_file))\n\n        if not path.exists():\n            uri = current_file[""uri""]\n            database = current_file[""database""]\n            msg = (\n                f""Directory {self.root_dir} does not contain ""\n                f\'precomputed features for file ""{uri}"" of \'\n                f\'""{database}"" database.\'\n            )\n            raise PyannoteFeatureExtractionError(msg)\n\n        if self.use_memmap:\n            data = np.load(str(path), mmap_mode=""r"")\n        else:\n            data = np.load(str(path))\n\n        return SlidingWindowFeature(data, self.sliding_window_)\n\n    def crop(self, current_file, segment, mode=""center"", fixed=None):\n        """"""Fast version of self(current_file).crop(segment, **kwargs)\n\n        Parameters\n        ----------\n        current_file : dict\n            `pyannote.database` file.\n        segment : `pyannote.core.Segment`\n            Segment from which to extract features.\n\n        Returns\n        -------\n        features : (n_frames, dimension) numpy array\n            Extracted features\n\n        See also\n        --------\n        `pyannote.core.SlidingWindowFeature.crop`\n        """"""\n\n        # match default FeatureExtraction.crop behavior\n        if mode == ""center"" and fixed is None:\n            fixed = segment.duration\n\n        memmap = open_memmap(self.get_path(current_file), mode=""r"")\n        swf = SlidingWindowFeature(memmap, self.sliding_window_)\n        result = swf.crop(segment, mode=mode, fixed=fixed)\n        del memmap\n        return result\n\n    def shape(self, item):\n        """"""Faster version of precomputed(item).data.shape""""""\n        memmap = open_memmap(self.get_path(item), mode=""r"")\n        shape = memmap.shape\n        del memmap\n        return shape\n\n    def dump(self, item, features):\n        path = Path(self.get_path(item))\n        mkdir_p(path.parent)\n        np.save(path, features.data)\n'"
pyannote/audio/features/pretrained.py,4,"b'# The MIT License (MIT)\n#\n# Copyright (c) 2020 CNRS\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# AUTHOR\n# Herv\xc3\xa9 Bredin - http://herve.niderb.fr\n\nimport warnings\nfrom typing import Optional\nfrom typing import Union\nfrom typing import Text\nfrom pathlib import Path\n\nimport torch\nimport numpy as np\n\nfrom pyannote.core import SlidingWindow\nfrom pyannote.core import SlidingWindowFeature\n\nfrom pyannote.audio.train.model import RESOLUTION_FRAME\nfrom pyannote.audio.train.model import RESOLUTION_CHUNK\n\nfrom pyannote.audio.augmentation import Augmentation\nfrom pyannote.audio.features import FeatureExtraction\n\nfrom pyannote.audio.applications.config import load_config\nfrom pyannote.audio.applications.config import load_specs\nfrom pyannote.audio.applications.config import load_params\n\n\nclass Pretrained(FeatureExtraction):\n    """"""\n\n    Parameters\n    ----------\n    validate_dir : Path\n        Path to a validation directory.\n    epoch : int, optional\n        If provided, force loading this epoch.\n        Defaults to reading epoch in validate_dir/params.yml.\n    augmentation : Augmentation, optional\n    duration : float, optional\n        Use audio chunks with that duration. Defaults to the fixed duration\n        used during training, when available.\n    step : float, optional\n        Ratio of audio chunk duration used as step between two consecutive\n        audio chunks. Defaults to 0.25.\n    device : optional\n    return_intermediate : optional\n    """"""\n\n    # TODO: add progress bar (at least for demo purposes)\n\n    def __init__(\n        self,\n        validate_dir: Path = None,\n        epoch: int = None,\n        augmentation: Optional[Augmentation] = None,\n        duration: float = None,\n        step: float = None,\n        batch_size: int = 32,\n        device: Optional[Union[Text, torch.device]] = None,\n        return_intermediate=None,\n        progress_hook=None,\n    ):\n\n        try:\n            validate_dir = Path(validate_dir)\n        except TypeError as e:\n            msg = (\n                f\'""validate_dir"" must be str, bytes or os.PathLike object, \'\n                f""not {type(validate_dir).__name__}.""\n            )\n            raise TypeError(msg)\n\n        strict = epoch is None\n        self.validate_dir = validate_dir.expanduser().resolve(strict=strict)\n\n        train_dir = self.validate_dir.parents[1]\n        root_dir = train_dir.parents[1]\n\n        config_yml = root_dir / ""config.yml""\n        config = load_config(config_yml, training=False)\n\n        # use feature extraction from config.yml configuration file\n        self.feature_extraction_ = config[""feature_extraction""]\n\n        super().__init__(\n            augmentation=augmentation, sample_rate=self.feature_extraction_.sample_rate\n        )\n\n        self.feature_extraction_.augmentation = self.augmentation\n\n        specs_yml = train_dir / ""specs.yml""\n        specifications = load_specs(specs_yml)\n\n        if epoch is None:\n            params_yml = self.validate_dir / ""params.yml""\n            params = load_params(params_yml)\n            self.epoch_ = params[""epoch""]\n            # keep track of pipeline parameters\n            self.pipeline_params_ = params.get(""params"", {})\n        else:\n            self.epoch_ = epoch\n\n        self.preprocessors_ = config[""preprocessors""]\n\n        self.weights_pt_ = train_dir / ""weights"" / f""{self.epoch_:04d}.pt""\n\n        model = config[""get_model_from_specs""](specifications)\n        model.load_state_dict(\n            torch.load(self.weights_pt_, map_location=lambda storage, loc: storage)\n        )\n\n        # defaults to using GPU when available\n        if device is None:\n            device = ""cuda"" if torch.cuda.is_available() else ""cpu""\n        self.device = torch.device(device)\n\n        # send model to device\n        self.model_ = model.eval().to(self.device)\n\n        # initialize chunks duration with that used during training\n        self.duration = getattr(config[""task""], ""duration"", None)\n\n        self.min_duration = getattr(config[""task""], ""min_duration"", None)\n\n        # override chunks duration by user-provided value\n        if duration is not None:\n            # warn that this might be sub-optimal\n            if self.duration is not None and duration != self.duration:\n                # TODO: do not show this message if min_duration < new_duration < duration\n                msg = (\n                    f""Model was trained with {self.duration:g}s chunks and ""\n                    f""is applied on {duration:g}s chunks. This might lead ""\n                    f""to sub-optimal results.""\n                )\n                warnings.warn(msg)\n            # do it anyway\n            self.duration = duration\n\n        if step is None:\n            step = 0.25\n        self.step = step\n        self.chunks_ = SlidingWindow(\n            duration=self.duration, step=self.step * self.duration\n        )\n\n        self.batch_size = batch_size\n\n        self.return_intermediate = return_intermediate\n        self.progress_hook = progress_hook\n\n    @property\n    def duration(self):\n        return self.duration_\n\n    @duration.setter\n    def duration(self, duration: float):\n        self.duration_ = duration\n        self.chunks_ = SlidingWindow(\n            duration=self.duration, step=self.step * self.duration\n        )\n\n    @property\n    def step(self):\n        return getattr(self, ""step_"", 0.25)\n\n    @step.setter\n    def step(self, step: float):\n        self.step_ = step\n        self.chunks_ = SlidingWindow(\n            duration=self.duration, step=self.step * self.duration\n        )\n\n    @property\n    def classes(self):\n        return self.model_.classes\n\n    def get_dimension(self) -> int:\n        try:\n            dimension = self.model_.dimension\n        except AttributeError:\n            dimension = len(self.model_.classes)\n        return dimension\n\n    def get_resolution(self) -> SlidingWindow:\n\n        resolution = self.model_.resolution\n\n        # model returns one vector per input frame\n        if resolution == RESOLUTION_FRAME:\n            resolution = self.feature_extraction_.sliding_window\n\n        # model returns one vector per input window\n        if resolution == RESOLUTION_CHUNK:\n            resolution = self.chunks_\n\n        return resolution\n\n    def get_features(self, y, sample_rate) -> np.ndarray:\n\n        features = SlidingWindowFeature(\n            self.feature_extraction_.get_features(y, sample_rate),\n            self.feature_extraction_.sliding_window,\n        )\n\n        return self.model_.slide(\n            features,\n            self.chunks_,\n            batch_size=self.batch_size,\n            device=self.device,\n            return_intermediate=self.return_intermediate,\n            progress_hook=self.progress_hook,\n        ).data\n\n    def get_context_duration(self) -> float:\n        # FIXME: add half window duration to context?\n        return self.feature_extraction_.get_context_duration()\n'"
pyannote/audio/features/utils.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2016-2019 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\nimport warnings\nimport numpy as np\n\nimport librosa\nfrom librosa.util import valid_audio\nfrom librosa.util.exceptions import ParameterError\n\nfrom pyannote.core import SlidingWindow, SlidingWindowFeature\n\nfrom soundfile import SoundFile\nimport soundfile as sf\n\n\ndef get_audio_duration(current_file):\n    """"""Return audio file duration\n\n    Parameters\n    ----------\n    current_file : dict\n        Dictionary given by pyannote.database.\n\n    Returns\n    -------\n    duration : float\n        Audio file duration.\n    """"""\n\n    with SoundFile(current_file[""audio""], ""r"") as f:\n        duration = float(f.frames) / f.samplerate\n\n    return duration\n\n\ndef get_audio_sample_rate(current_file):\n    """"""Return audio file sampling rate\n\n    Parameters\n    ----------\n    current_file : dict\n        Dictionary given by pyannote.database.\n\n    Returns\n    -------\n    sample_rate : int\n        Sampling rate\n    """"""\n    with SoundFile(current_file[""audio""], ""r"") as f:\n        sample_rate = f.samplerate\n\n    return sample_rate\n\n\ndef read_audio(current_file, sample_rate=None, mono=True):\n    """"""Read audio file\n\n    Parameters\n    ----------\n    current_file : dict\n        Dictionary given by pyannote.database.\n    sample_rate: int, optional\n        Target sampling rate. Defaults to using native sampling rate.\n    mono : int, optional\n        Convert multi-channel to mono. Defaults to True.\n\n    Returns\n    -------\n    y : (n_samples, n_channels) np.array\n        Audio samples.\n    sample_rate : int\n        Sampling rate.\n\n    Notes\n    -----\n    In case `current_file` contains a `channel` key, data of this (1-indexed)\n    channel will be returned.\n\n    """"""\n\n    y, file_sample_rate = sf.read(\n        current_file[""audio""], dtype=""float32"", always_2d=True\n    )\n\n    # extract specific channel if requested\n    channel = current_file.get(""channel"", None)\n    if channel is not None:\n        y = y[:, channel - 1 : channel]\n\n    # convert to mono\n    if mono and y.shape[1] > 1:\n        y = np.mean(y, axis=1, keepdims=True)\n\n    # resample if sample rates mismatch\n    if (sample_rate is not None) and (file_sample_rate != sample_rate):\n        y = librosa.core.resample(y.T, file_sample_rate, sample_rate).T\n    else:\n        sample_rate = file_sample_rate\n\n    return y, sample_rate\n\n\nclass RawAudio:\n    """"""Raw audio with on-the-fly data augmentation\n\n    Parameters\n    ----------\n    sample_rate: int, optional\n        Target sampling rate. Defaults to using native sampling rate.\n    mono : int, optional\n        Convert multi-channel to mono. Defaults to True.\n    augmentation : `pyannote.audio.augmentation.Augmentation`, optional\n        Data augmentation.\n    """"""\n\n    def __init__(self, sample_rate=None, mono=True, augmentation=None):\n\n        super().__init__()\n        self.sample_rate = sample_rate\n        self.mono = mono\n\n        self.augmentation = augmentation\n\n        if sample_rate is not None:\n            self.sliding_window_ = SlidingWindow(\n                start=-0.5 / sample_rate,\n                duration=1.0 / sample_rate,\n                step=1.0 / sample_rate,\n            )\n\n    @property\n    def dimension(self):\n        return 1\n\n    @property\n    def sliding_window(self):\n        return self.sliding_window_\n\n    def get_features(self, y, sample_rate):\n\n        # convert to mono\n        if self.mono:\n            y = np.mean(y, axis=1, keepdims=True)\n\n        # resample if sample rates mismatch\n        if (self.sample_rate is not None) and (self.sample_rate != sample_rate):\n            y = librosa.core.resample(y.T, sample_rate, self.sample_rate).T\n            sample_rate = self.sample_rate\n\n        # augment data\n        if self.augmentation is not None:\n            y = self.augmentation(y, sample_rate)\n\n        # TODO: how time consuming is this thing (needs profiling...)\n        try:\n            valid = valid_audio(y[:, 0], mono=True)\n        except ParameterError as e:\n            msg = f""Something went wrong when augmenting waveform.""\n            raise ValueError(msg)\n\n        return y\n\n    def __call__(self, current_file, return_sr=False):\n        """"""Obtain waveform\n\n        Parameters\n        ----------\n        current_file : dict\n            `pyannote.database` files.\n        return_sr : `bool`, optional\n            Return sample rate. Defaults to False\n\n        Returns\n        -------\n        waveform : `pyannote.core.SlidingWindowFeature`\n            Waveform\n        sample_rate : `int`\n            Only when `return_sr` is set to True\n        """"""\n\n        if ""waveform"" in current_file:\n\n            if self.sample_rate is None:\n                msg = (\n                    ""`RawAudio` needs to be instantiated with an actual ""\n                    ""`sample_rate` if one wants to use precomputed ""\n                    ""waveform.""\n                )\n                raise ValueError(msg)\n            sample_rate = self.sample_rate\n\n            y = current_file[""waveform""]\n\n            if len(y.shape) != 2:\n                msg = (\n                    f""Precomputed waveform should be provided as a ""\n                    f""(n_samples, n_channels) `np.ndarray`.""\n                )\n                raise ValueError(msg)\n\n        else:\n            y, sample_rate = sf.read(\n                current_file[""audio""], dtype=""float32"", always_2d=True\n            )\n\n        # extract specific channel if requested\n        channel = current_file.get(""channel"", None)\n        if channel is not None:\n            y = y[:, channel - 1 : channel]\n\n        y = self.get_features(y, sample_rate)\n\n        sliding_window = SlidingWindow(\n            start=-0.5 / sample_rate, duration=1.0 / sample_rate, step=1.0 / sample_rate\n        )\n\n        if return_sr:\n            return (\n                SlidingWindowFeature(y, sliding_window),\n                sample_rate if self.sample_rate is None else self.sample_rate,\n            )\n\n        return SlidingWindowFeature(y, sliding_window)\n\n    def get_context_duration(self):\n        return 0.0\n\n    def crop(self, current_file, segment, mode=""center"", fixed=None):\n        """"""Fast version of self(current_file).crop(segment, **kwargs)\n\n        Parameters\n        ----------\n        current_file : dict\n            `pyannote.database` file.\n        segment : `pyannote.core.Segment`\n            Segment from which to extract features.\n        mode : {\'loose\', \'strict\', \'center\'}, optional\n            In \'strict\' mode, only frames fully included in \'segment\' are\n            returned. In \'loose\' mode, any intersecting frames are returned. In\n            \'center\' mode, first and last frames are chosen to be the ones\n            whose centers are the closest to \'focus\' start and end times.\n            Defaults to \'center\'.\n        fixed : float, optional\n            Overrides `Segment` \'focus\' duration and ensures that the number of\n            returned frames is fixed (which might otherwise not be the case\n            because of rounding errors). Has no effect in \'strict\' or \'loose\'\n            modes.\n\n        Returns\n        -------\n        waveform : (n_samples, n_channels) numpy array\n            Waveform\n\n        See also\n        --------\n        `pyannote.core.SlidingWindowFeature.crop`\n        """"""\n\n        if self.sample_rate is None:\n            msg = (\n                ""`RawAudio` needs to be instantiated with an actual ""\n                ""`sample_rate` if one wants to use `crop`.""\n            )\n            raise ValueError(msg)\n\n        # find the start and end positions of the required segment\n        ((start, end),) = self.sliding_window_.crop(\n            segment, mode=mode, fixed=fixed, return_ranges=True\n        )\n\n        # this is expected number of samples.\n        # this will be useful later in case of on-the-fly resampling\n        n_samples = end - start\n\n        if ""waveform"" in current_file:\n\n            y = current_file[""waveform""]\n\n            if len(y.shape) != 2:\n                msg = (\n                    f""Precomputed waveform should be provided as a ""\n                    f""(n_samples, n_channels) `np.ndarray`.""\n                )\n                raise ValueError(msg)\n\n            sample_rate = self.sample_rate\n            data = y[start:end]\n\n        else:\n            # read file with SoundFile, which supports various fomats\n            # including NIST sphere\n            with SoundFile(current_file[""audio""], ""r"") as audio_file:\n\n                sample_rate = audio_file.samplerate\n\n                # if the sample rates are mismatched,\n                # recompute the start and end\n                if sample_rate != self.sample_rate:\n\n                    sliding_window = SlidingWindow(\n                        start=-0.5 / sample_rate,\n                        duration=1.0 / sample_rate,\n                        step=1.0 / sample_rate,\n                    )\n                    ((start, end),) = sliding_window.crop(\n                        segment, mode=mode, fixed=fixed, return_ranges=True\n                    )\n\n                try:\n                    audio_file.seek(start)\n                    data = audio_file.read(end - start, dtype=""float32"", always_2d=True)\n                except RuntimeError as e:\n                    msg = (\n                        f""SoundFile failed to seek-and-read in ""\n                        f""{current_file[\'audio\']}: loading the whole file...""\n                    )\n                    warnings.warn(msg)\n                    return self(current_file).crop(segment, mode=mode, fixed=fixed)\n\n        # extract specific channel if requested\n        channel = current_file.get(""channel"", None)\n        if channel is not None:\n            data = data[:, channel - 1 : channel]\n\n        return self.get_features(data, sample_rate)\n\n\n# # THIS SCRIPT CAN BE USED TO CRASH-TEST THE ON-THE-FLY RESAMPLING\n\n# import numpy as np\n# from pyannote.audio.features import RawAudio\n# from pyannote.core import Segment\n# from pyannote.audio.features.utils import get_audio_duration\n# from tqdm import tqdm\n#\n# TEST_FILE = \'/Users/bredin/Corpora/etape/BFMTV_BFMStory_2010-09-03_175900.wav\'\n# current_file = {\'audio\': TEST_FILE}\n# duration = get_audio_duration(current_file)\n#\n# for sample_rate in [8000, 16000, 44100, 48000]:\n#     raw_audio = RawAudio(sample_rate=sample_rate)\n#     for i in tqdm(range(1000), desc=f\'{sample_rate:d}Hz\'):\n#         start = np.random.rand() * (duration - 1.)\n#         data = raw_audio.crop(current_file, Segment(start, start + 1), fixed=1.)\n#         assert len(data) == sample_rate\n'"
pyannote/audio/features/with_librosa.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2016-2019 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n""""""\nFeature extraction using [`librosa`](https://librosa.github.io/librosa/)\n""""""\n\nimport librosa\nimport numpy as np\n\nfrom .base import FeatureExtraction\nfrom pyannote.core.segment import SlidingWindow\n\n\nclass LibrosaFeatureExtraction(FeatureExtraction):\n    """"""librosa feature extraction base class\n\n    Parameters\n    ----------\n    sample_rate : int, optional\n        Defaults to 16000 (i.e. 16kHz)\n    augmentation : `pyannote.audio.augmentation.Augmentation`, optional\n        Data augmentation.\n    duration : float, optional\n        Defaults to 0.025 (25ms).\n    step : float, optional\n        Defaults to 0.010 (10ms).\n    """"""\n\n    def __init__(self, sample_rate=16000, augmentation=None, duration=0.025, step=0.01):\n\n        super().__init__(sample_rate=sample_rate, augmentation=augmentation)\n        self.duration = duration\n        self.step = step\n\n        self.sliding_window_ = SlidingWindow(\n            start=-0.5 * self.duration, duration=self.duration, step=self.step\n        )\n\n    def get_resolution(self):\n        return self.sliding_window_\n\n\nclass LibrosaSpectrogram(LibrosaFeatureExtraction):\n    """"""librosa spectrogram\n\n    Parameters\n    ----------\n    sample_rate : int, optional\n        Defaults to 16000 (i.e. 16kHz)\n    augmentation : `pyannote.audio.augmentation.Augmentation`, optional\n        Data augmentation.\n    duration : float, optional\n        Defaults to 0.025.\n    step : float, optional\n        Defaults to 0.010.\n    """"""\n\n    def __init__(\n        self, sample_rate=16000, augmentation=None, duration=0.025, step=0.010\n    ):\n\n        super().__init__(\n            sample_rate=sample_rate,\n            augmentation=augmentation,\n            duration=duration,\n            step=step,\n        )\n\n        self.n_fft_ = int(self.duration * self.sample_rate)\n        self.hop_length_ = int(self.step * self.sample_rate)\n\n    def get_dimension(self):\n        return self.n_fft_ // 2 + 1\n\n    def get_features(self, y, sample_rate):\n        """"""Feature extraction\n\n        Parameters\n        ----------\n        y : (n_samples, 1) numpy array\n            Waveform\n        sample_rate : int\n            Sample rate\n\n        Returns\n        -------\n        data : (n_frames, n_dimensions) numpy array\n            Features\n        """"""\n\n        fft = librosa.core.stft(\n            y=y.squeeze(),\n            n_fft=self.n_fft_,\n            hop_length=self.hop_length_,\n            center=True,\n            window=""hamming"",\n        )\n        return np.abs(fft).T\n\n\nclass LibrosaMelSpectrogram(LibrosaFeatureExtraction):\n    """"""librosa mel-spectrogram\n\n    Parameters\n    ----------\n    sample_rate : int, optional\n        Defaults to 16000 (i.e. 16kHz)\n    augmentation : `pyannote.audio.augmentation.Augmentation`, optional\n        Data augmentation.\n    duration : float, optional\n        Defaults to 0.025.\n    step : float, optional\n        Defaults to 0.010.\n    n_mels : int, optional\n        Defaults to 96.\n    """"""\n\n    def __init__(\n        self,\n        sample_rate=16000,\n        augmentation=None,\n        duration=0.025,\n        step=0.010,\n        n_mels=96,\n    ):\n\n        super().__init__(\n            sample_rate=sample_rate,\n            augmentation=augmentation,\n            duration=duration,\n            step=step,\n        )\n\n        self.n_mels = n_mels\n        self.n_fft_ = int(self.duration * self.sample_rate)\n        self.hop_length_ = int(self.step * self.sample_rate)\n\n    def get_dimension(self):\n        return self.n_mels\n\n    def get_features(self, y, sample_rate):\n        """"""Feature extraction\n\n        Parameters\n        ----------\n        y : (n_samples, 1) numpy array\n            Waveform\n        sample_rate : int\n            Sample rate\n\n        Returns\n        -------\n        data : (n_frames, n_mels) numpy array\n            Features\n        """"""\n\n        X = librosa.feature.melspectrogram(\n            y.squeeze(),\n            sr=sample_rate,\n            n_mels=self.n_mels,\n            n_fft=self.n_fft_,\n            hop_length=self.hop_length_,\n            power=2.0,\n        )\n\n        return librosa.amplitude_to_db(X, ref=1.0, amin=1e-5, top_db=80.0).T\n\n\nclass LibrosaMFCC(LibrosaFeatureExtraction):\n    """"""librosa MFCC\n\n    ::\n\n            | e    |  energy\n            | c1   |\n            | c2   |  coefficients\n            | c3   |\n            | ...  |\n            | \xce\x94e   |  energy first derivative\n            | \xce\x94c1  |\n        x = | \xce\x94c2  |  coefficients first derivatives\n            | \xce\x94c3  |\n            | ...  |\n            | \xce\x94\xce\x94e  |  energy second derivative\n            | \xce\x94\xce\x94c1 |\n            | \xce\x94\xce\x94c2 |  coefficients second derivatives\n            | \xce\x94\xce\x94c3 |\n            | ...  |\n\n\n    Parameters\n    ----------\n    sample_rate : int, optional\n        Defaults to 16000 (i.e. 16kHz)\n    augmentation : `pyannote.audio.augmentation.Augmentation`, optional\n        Data augmentation.\n    duration : float, optional\n        Defaults to 0.025.\n    step : float, optional\n        Defaults to 0.010.\n    e : bool, optional\n        Energy. Defaults to True.\n    coefs : int, optional\n        Number of coefficients. Defaults to 11.\n    De : bool, optional\n        Keep energy first derivative. Defaults to False.\n    D : bool, optional\n        Add first order derivatives. Defaults to False.\n    DDe : bool, optional\n        Keep energy second derivative. Defaults to False.\n    DD : bool, optional\n        Add second order derivatives. Defaults to False.\n\n    Notes\n    -----\n    Internal setup\n        * fftWindow = Hanning\n        * melMaxFreq = 6854.0\n        * melMinFreq = 130.0\n        * melNbFilters = 40\n\n    """"""\n\n    def __init__(\n        self,\n        sample_rate=16000,\n        augmentation=None,\n        duration=0.025,\n        step=0.01,\n        e=False,\n        De=True,\n        DDe=True,\n        coefs=19,\n        D=True,\n        DD=True,\n        fmin=0.0,\n        fmax=None,\n        n_mels=40,\n    ):\n\n        super().__init__(\n            sample_rate=sample_rate,\n            augmentation=augmentation,\n            duration=duration,\n            step=step,\n        )\n\n        self.e = e\n        self.coefs = coefs\n        self.De = De\n        self.DDe = DDe\n        self.D = D\n        self.DD = DD\n\n        self.n_mels = n_mels  # yaafe / 40\n        self.fmin = fmin  # yaafe / 130.0\n        self.fmax = fmax  # yaafe / 6854.0\n\n    def get_context_duration(self):\n        return 0.0\n\n    def get_features(self, y, sample_rate):\n        """"""Feature extraction\n\n        Parameters\n        ----------\n        y : (n_samples, 1) numpy array\n            Waveform\n        sample_rate : int\n            Sample rate\n\n        Returns\n        -------\n        data : (n_frames, n_dimensions) numpy array\n            Features\n        """"""\n\n        # adding because C0 is the energy\n        n_mfcc = self.coefs + 1\n\n        n_fft = int(self.duration * sample_rate)\n        hop_length = int(self.step * sample_rate)\n\n        mfcc = librosa.feature.mfcc(\n            y=y.squeeze(),\n            sr=sample_rate,\n            n_mfcc=n_mfcc,\n            n_fft=n_fft,\n            hop_length=hop_length,\n            n_mels=self.n_mels,\n            htk=True,\n            fmin=self.fmin,\n            fmax=self.fmax,\n        )\n\n        if self.De or self.D:\n            mfcc_d = librosa.feature.delta(mfcc, width=9, order=1, axis=-1)\n\n        if self.DDe or self.DD:\n            mfcc_dd = librosa.feature.delta(mfcc, width=9, order=2, axis=-1)\n\n        stack = []\n\n        if self.e:\n            stack.append(mfcc[0, :])\n\n        stack.append(mfcc[1:, :])\n\n        if self.De:\n            stack.append(mfcc_d[0, :])\n\n        if self.D:\n            stack.append(mfcc_d[1:, :])\n\n        if self.DDe:\n            stack.append(mfcc_dd[0, :])\n\n        if self.DD:\n            stack.append(mfcc_dd[1:, :])\n\n        return np.vstack(stack).T\n\n    def get_dimension(self):\n        n_features = 0\n        n_features += self.e\n        n_features += self.De\n        n_features += self.DDe\n        n_features += self.coefs\n        n_features += self.coefs * self.D\n        n_features += self.coefs * self.DD\n        return n_features\n'"
pyannote/audio/features/wrapper.py,7,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n\nfrom pathlib import Path\nfrom typing import Text\nfrom typing import Union\nfrom typing import Dict\nfrom functools import partial\nfrom pyannote.database.protocol.protocol import ProtocolFile\nfrom pyannote.core import Segment\nfrom pyannote.core import SlidingWindowFeature\nimport numpy as np\n\nWrappable = Union[\n    ""Precomputed"", ""Pretrained"", ""RawAudio"", ""FeatureExtraction"", Dict, Text, Path\n]\n\n# this needs to go here to make Wrapper instances pickable\ndef _use_existing_key(key, file):\n    return file[key]\n\n\nclass Wrapper:\n    """"""FeatureExtraction-compliant wrapper\n\n    Parameters\n    ----------\n    wrappable : Wrappable\n        Wrappable object. See ""Usage"" section for a detailed description.\n    **params : Dict\n        Keyword parameters passed to the wrapped object when supported.\n\n    Usage\n    -----\n    If `wrappable` already complies with the `FeatureExtraction` API , it is\n    kept unchanged. This includes instances of any `FeatureExtraction` subclass,\n    `RawAudio` instances, `Precomputed` instances, and `Pretrained instances.\n    In this case, keyword parameters are not used.\n\n    * If `wrappable` is a `Path` to a directory containing precomputed features\n      or scores (e.g. the one created by `pyannote-audio [...] apply [...]`), it\n      stands for:\n\n      Precomputed(root_dir=wrappable, **params)\n\n    * If `wrappable` is a `Path` to the validation directory created by calling\n      `pyannote-audio [...] validate [...]`, it stands for:\n\n      Pretrained(validate_dir=wrappable, **params)\n\n    * If `wrappable` is a `Path` to a checkpoint created by calling\n      `pyannote-audio [...] train [...]`, (i.e. with the following structure:\n      \'{root_dir}/train/{protocol}.train/weights/{epoch}.pt)\', it stands for:\n\n      Pretrained(validate_dir=\'{root_dir}/train/{protocol}.train/validate/fake\',\n                 epoch=int({epoch}), **params)\n\n    * If `wrappable` is a `Text` containing the name of an existing `torch.hub`\n      model, it stands for:\n\n      torch.hub.load(\'pyannote/pyannote-audio\', wrappable, **params)\n\n    * If `wrappable` is a `Text` starting with \'@\' such as \'@key\', it stands for:\n\n      lambda current_file: current_file[\'key\']\n\n    In any other situation, it will raise an error.\n\n    Notes\n    -----\n    It is also possible to provide a `Dict` `wrappable`, in which case it is\n    expected to contain a unique key which is the name of a `torch.hub` model\n    (or any supported `Path` described above), whose corresponding value is a\n    dictionary of custom parameters. For instance,\n\n      Wrapper({\'sad\': {\'step\': 0.1}}) is the same as Wrapper(\'sad\', step=0.1)\n\n    This is especially useful in `pyannote-pipeline` configuration files:\n\n        ~~~~~ content of \'config.yml\' ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n          pipeline:\n             name: pyannote.audio.pipeline.SpeechActivityDetection\n             params:\n                scores:\n                   sad:\n                      step: 0.1\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    """"""\n\n    def __init__(self, wrappable: Wrappable, **params):\n        super().__init__()\n\n        from pyannote.audio.features import Pretrained\n        from pyannote.audio.features import Precomputed\n        from pyannote.audio.features import FeatureExtraction\n        from pyannote.audio.features import RawAudio\n\n        scorer = None\n        msg = """"\n\n        # corner\n        if isinstance(wrappable, dict):\n            wrappable, custom_params = dict(wrappable).popitem()\n            params.update(**custom_params)\n\n        # If `wrappable` already complies with the `FeatureExtraction` API , it\n        # is kept unchanged. This includes instances of any `FeatureExtraction`\n        # subclass,`RawAudio` instances, `Precomputed` instances, and\n        # `Pretrained` instances.\n        if isinstance(\n            wrappable, (FeatureExtraction, RawAudio, Pretrained, Precomputed)\n        ):\n            scorer = wrappable\n\n        elif Path(wrappable).is_dir():\n            directory = Path(wrappable)\n\n            # If `wrappable` is a `Path` to a directory containing precomputed\n            # features or scores, wrap the corresponding `Precomputed` instance\n            try:\n                scorer = Precomputed(root_dir=directory)\n            except Exception as e:\n                scorer = None\n\n            # If `wrappable` is a `Path` to a validation directory,\n            # wrap the corresponding `Pretrained` instance\n            if scorer is None:\n                try:\n                    scorer = Pretrained(validate_dir=directory, **params)\n                except Exception as e:\n                    scorer = None\n\n            if scorer is None:\n                msg = (\n                    f\'""{wrappable}"" directory does not seem to be the path \'\n                    f""to precomputed features nor the path to a model ""\n                    f""validation step.""\n                )\n\n        # If `wrappable` is a `Path` to a pretrined model checkpoint,\n        # wrap the corresponding `Pretrained` instance\n        elif Path(wrappable).is_file():\n            checkpoint = Path(wrappable)\n\n            try:\n                validate_dir = checkpoint.parents[1] / ""validate"" / ""fake""\n                epoch = int(checkpoint.stem)\n                scorer = Pretrained(validate_dir=validate_dir, epoch=epoch, **params)\n            except Exception as e:\n                msg = (\n                    f\'""{wrappable}"" directory does not seem to be the path \'\n                    f""to a pretrained model checkpoint.""\n                )\n                scorer = None\n\n        elif isinstance(wrappable, Text):\n\n            # If `wrappable` is a `Text` starting with \'@\' such as \'@key\',\n            # it means that one should read the ""key"" key of protocol files\n            if wrappable.startswith(""@""):\n                key = wrappable[1:]\n\n                scorer = partial(_use_existing_key, key)\n                # scorer = lambda current_file: current_file[key]\n\n            # If `wrappable` is a `Text` containing the name of an existing\n            # `torch.hub` model, wrap the corresponding `Pretrained`.\n            else:\n                try:\n                    import torch\n\n                    scorer = torch.hub.load(\n                        ""pyannote/pyannote-audio"", wrappable, **params\n                    )\n                    if not isinstance(scorer, Pretrained):\n                        msg = (\n                            f\'""{wrappable}"" exists on torch.hub but does not \'\n                            f""return a `Pretrained` model instance.""\n                        )\n                        scorer = None\n\n                except Exception as e:\n                    msg = (\n                        f""Could not load {wrappable} model from torch.hub. ""\n                        f""The following exception was raised:\\n{e}""\n                    )\n                    scorer = None\n\n        # warn the user the something went wrong\n        if scorer is None:\n            raise ValueError(msg)\n\n        self.scorer_ = scorer\n\n    def crop(\n        self,\n        current_file: ProtocolFile,\n        segment: Segment,\n        mode: Text = ""center"",\n        fixed: float = None,\n    ) -> np.ndarray:\n        """"""Extract frames from a specific region\n\n        Parameters\n        ----------\n        current_file : ProtocolFile\n            Protocol file\n        segment : Segment\n            Region of the file to process.\n        mode : {\'loose\', \'strict\', \'center\'}, optional\n            In \'strict\' mode, only frames fully included in \'segment\' support are\n            returned. In \'loose\' mode, any intersecting frames are returned. In\n            \'center\' mode, first and last frames are chosen to be the ones\n            whose centers are the closest to \'segment\' start and end times.\n            Defaults to \'center\'.\n        fixed : float, optional\n            Overrides \'segment\' duration and ensures that the number of\n            returned frames is fixed (which might otherwise not be the case\n            because of rounding errors).\n\n        Returns\n        -------\n        frames : np.ndarray\n            Frames.\n        """"""\n\n        from pyannote.audio.features import Precomputed\n        from pyannote.audio.features import Pretrained\n        from pyannote.audio.features import RawAudio\n        from pyannote.audio.features import FeatureExtraction\n\n        if isinstance(\n            self.scorer_, (FeatureExtraction, RawAudio, Pretrained, Precomputed)\n        ):\n            return self.scorer_.crop(current_file, segment, mode=mode, fixed=fixed)\n\n        return self.scorer_(current_file).crop(\n            segment, mode=mode, fixed=fixed, return_data=True\n        )\n\n    def __call__(self, current_file) -> SlidingWindowFeature:\n        """"""Extract frames from the whole file\n\n        Parameters\n        ----------\n        current_file : ProtocolFile\n            Protocol file\n\n        Returns\n        -------\n        frames : np.ndarray\n            Frames.\n        """"""\n        return self.scorer_(current_file)\n\n    # used to ""inherit"" most scorer_ attributes\n    def __getattr__(self, name):\n\n        # prevents a ""RecursionError: maximum recursion depth exceeded"" when pickling Wrapper\n        # https://stackoverflow.com/questions/49380224/how-to-make-classes-with-getattr-pickable\n        if ""scorer_"" not in vars(self):\n            raise AttributeError\n\n        return getattr(self.scorer_, name)\n\n    def __setattr__(self, name, value):\n        if name == ""scorer_"":\n            object.__setattr__(self, name, value)\n\n        else:\n            setattr(self.scorer_, name, value)\n'"
pyannote/audio/interactive/__init__.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n'"
pyannote/audio/interactive/pipeline.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n\nfrom typing import Text, Union, Tuple, List\nfrom pathlib import Path\nfrom pyannote.core import (\n    Segment,\n    SlidingWindow,\n    Timeline,\n    Annotation,\n    SlidingWindowFeature,\n)\nfrom pyannote.database.protocol.protocol import ProtocolFile\n\nimport numpy as np\n\nfrom pyannote.pipeline import Pipeline\nfrom pyannote.pipeline.parameter import Uniform\n\nfrom pyannote.audio.features.wrapper import Wrapper\nfrom pyannote.audio.features.utils import get_audio_duration\n\nfrom pyannote.metrics.diarization import DiarizationErrorRate\nfrom pyannote.metrics.detection import DetectionErrorRate\n\nfrom pyannote.audio.utils.signal import Binarize\n\nfrom pyannote.core.utils.hierarchy import pool\nfrom scipy.cluster.hierarchy import fcluster\nfrom scipy.spatial.distance import cdist\n\n\nfrom .utils import time2index\nfrom .utils import index2index\n\n# Hyper-parameters tuned to minimize diarization error rate\n# on one third of DIHARD II training set\nPRETRAINED_PARAMS = {\n    ""emb_duration"": 1.7657045140297274,\n    ""emb_step_ratio"": 0.20414598809353782,\n    ""emb_threshold"": 0.5274911675340328,\n    ""sad_min_duration_off"": 0.13583405625051126,\n    ""sad_min_duration_on"": 0.0014190874731107286,\n    ""sad_threshold_off"": 0.7878607185085043,\n    ""sad_threshold_on"": 0.5940560764213958,\n}\n\n\nclass InteractiveDiarization(Pipeline):\n    """"""Interactive diarization pipeline\n\n    Parameters\n    ----------\n    sad : str or Path, optional\n        Pretrained speech activity detection model. Defaults to ""sad"".\n    emb : str or Path, optional\n        Pretrained speaker embedding model. Defaults to ""emb"".\n    batch_size : int, optional\n        Batch size.\n    only_sad : bool, optional\n        Set to True if you only care about speech activity detection.\n\n    Hyper-parameters\n    ----------------\n    sad_threshold_on, sad_threshold_off : float\n        Onset/offset speech activity detection thresholds.\n    sad_min_duration_on, sad_min_duration_off : float\n        Minimum duration of speech/non-speech regions.\n    emb_duration, emb_step_ratio : float\n        Sliding window used for embedding extraction.\n    emb_threshold : float\n        Distance threshold used as stopping criterion for hierarchical\n        agglomerative clustering.\n    """"""\n\n    def __init__(\n        self,\n        sad: Union[Text, Path] = {""sad"": {""duration"": 2.0, ""step"": 0.1}},\n        emb: Union[Text, Path] = ""emb"",\n        batch_size: int = None,\n        only_sad: bool = False,\n    ):\n\n        super().__init__()\n\n        self.sad = Wrapper(sad)\n        if batch_size is not None:\n            self.sad.batch_size = batch_size\n        self.sad_speech_index_ = self.sad.classes.index(""speech"")\n\n        self.sad_threshold_on = Uniform(0.0, 1.0)\n        self.sad_threshold_off = Uniform(0.0, 1.0)\n        self.sad_min_duration_on = Uniform(0.0, 0.5)\n        self.sad_min_duration_off = Uniform(0.0, 0.5)\n\n        self.only_sad = only_sad\n        if self.only_sad:\n            return\n\n        self.emb = Wrapper(emb)\n        if batch_size is not None:\n            self.emb.batch_size = batch_size\n\n        max_duration = self.emb.duration\n        min_duration = getattr(self.emb, ""min_duration"", 0.25 * max_duration)\n        self.emb_duration = Uniform(min_duration, max_duration)\n        self.emb_step_ratio = Uniform(0.1, 1.0)\n        self.emb_threshold = Uniform(0.0, 2.0)\n\n    def initialize(self):\n        """"""Initialize pipeline internals with current hyper-parameter values""""""\n\n        self.sad_binarize_ = Binarize(\n            onset=self.sad_threshold_on,\n            offset=self.sad_threshold_off,\n            min_duration_on=self.sad_min_duration_on,\n            min_duration_off=self.sad_min_duration_off,\n        )\n\n        if not self.only_sad:\n            # embeddings will be extracted with a sliding window\n            # of ""emb_duration"" duration and ""emb_step_ratio x emb_duration"" step.\n            self.emb.duration = self.emb_duration\n            self.emb.step = self.emb_step_ratio\n\n    def compute_speech(self, current_file: ProtocolFile) -> Timeline:\n        """"""Apply speech activity detection\n\n        Parameters\n        ----------\n        current_file : ProtocolFile\n            Protocol file.\n\n        Returns\n        -------\n        speech : Timeline\n            Speech activity detection result.\n        """"""\n\n        # speech activity detection\n        if ""sad_scores"" in current_file:\n            sad_scores: SlidingWindowFeature = current_file[""sad_scores""]\n        else:\n            sad_scores = self.sad(current_file)\n            if np.nanmean(sad_scores) < 0:\n                sad_scores = np.exp(sad_scores)\n            current_file[""sad_scores""] = sad_scores\n\n        speech: Timeline = self.sad_binarize_.apply(\n            sad_scores, dimension=self.sad_speech_index_\n        )\n\n        return speech\n\n    def compute_embedding(self, current_file: ProtocolFile) -> SlidingWindowFeature:\n        """"""Extract speaker embedding\n\n        Parameters\n        ----------\n        current_file : ProtocolFile\n            Protocol file\n\n        Returns\n        -------\n        embedding : SlidingWindowFeature\n            Speaker embedding.\n        """"""\n\n        return self.emb(current_file)\n\n    def get_segment_assignment(\n        self, embedding: SlidingWindowFeature, speech: Timeline\n    ) -> np.ndarray:\n        """"""Get segment assignment\n\n        Parameters\n        ----------\n        embedding : SlidingWindowFeature\n            Embeddings.\n        speech : Timeline\n            Speech regions.\n\n        Returns\n        -------\n        assignment : (num_embedding, ) np.ndarray\n            * assignment[i] = s with s > 0 means that ith embedding is strictly\n            contained in (1-based) sth segment.\n            * assignment[i] = s with s < 0 means that more than half of ith\n            embedding is part of (1-based) sth segment.\n            * assignment[i] = 0 means that none of the above is true.\n        """"""\n\n        assignment: np.ndarray = np.zeros((len(embedding),), dtype=np.int32)\n\n        for s, segment in enumerate(speech):\n            indices = embedding.sliding_window.crop(segment, mode=""strict"")\n            if len(indices) > 0:\n                strict = 1\n            else:\n                strict = -1\n                indices = embedding.sliding_window.crop(segment, mode=""center"")\n            for i in indices:\n                if i < 0 or i >= len(embedding):\n                    continue\n                assignment[i] = strict * (s + 1)\n\n        return assignment\n\n    def __call__(\n        self,\n        current_file: ProtocolFile,\n        cannot_link: List[Tuple[float, float]] = None,\n        must_link: List[Tuple[float, float]] = None,\n    ) -> Annotation:\n        """"""Apply speaker diarization\n\n        Parameters\n        ----------\n        current_file : ProtocolFile\n            Protocol file.\n        cannot_link :\n            List of time-based ""cannot link"" constraints.\n        must_link :\n            List of time-based ""must link"" constraints.\n\n        Returns\n        -------\n        diarization : Annotation\n            Speaker diarization result.\n        """"""\n\n        if cannot_link is None:\n            cannot_link = []\n        if must_link is None:\n            must_link = []\n\n        if ""duration"" not in current_file:\n            current_file[""duration""] = get_audio_duration(current_file)\n\n        # in ""interactive annotation"" mode, there is no need to recompute speech\n        # regions every time a file is processed: they can be passed with the\n        # file directly\n        if ""speech"" in current_file:\n            speech: Timeline = current_file[""speech""]\n\n        # in ""pipeline optimization"" mode, pipeline hyper-parameters are different\n        # every time a file is processed: speech regions must be recomputed\n        else:\n            speech = self.compute_speech(current_file)\n\n        if self.only_sad:\n            return speech.to_annotation(generator=iter(lambda: ""SPEECH"", None))\n\n        # in ""interactive annotation"" mode, pipeline hyper-parameters are fixed.\n        # therefore, there is no need to recompute embeddings every time a file\n        # is processed: they can be passed with the file directly.\n        if ""embedding"" in current_file:\n            embedding: SlidingWindowFeature = current_file[""embedding""]\n\n        # in ""pipeline optimization"" mode, pipeline hyper-parameters are different\n        # every time a file is processed: embeddings must be recomputed\n        else:\n            embedding = self.compute_embedding(current_file)\n\n        window: SlidingWindow = embedding.sliding_window\n\n        # segment_assignment[i] = s with s > 0 means that ith embedding is\n        #       strictly contained in (1-based) sth segment.\n        # segment_assignment[i] = s with s < 0 means that more than half of ith\n        #       embedding is part of (1-based) sth segment.\n        # segment_assignment[i] = 0 means that none of the above is true.\n        segment_assignment: np.ndarray = self.get_segment_assignment(embedding, speech)\n\n        # cluster_assignment[i] = k (k > 0) means that the ith embedding belongs\n        #                           to kth cluster\n        # cluster_assignment[i] = 0 when segment_assignment[i] = 0\n        cluster_assignment: np.ndarray = np.zeros((len(embedding),), dtype=np.int32)\n\n        clean = segment_assignment > 0\n        noisy = segment_assignment < 0\n        clean_indices = np.where(clean)[0]\n        if len(clean_indices) < 2:\n            cluster_assignment[clean_indices] = 1\n\n        else:\n\n            # convert time-based constraints to index-based constraints\n            cannot_link = index2index(time2index(cannot_link, window), clean)\n            must_link = index2index(time2index(must_link, window), clean)\n\n            dendrogram = pool(\n                embedding[clean_indices],\n                metric=""cosine"",\n                cannot_link=cannot_link,\n                must_link=must_link,\n                must_link_method=""propagate"",\n            )\n            clusters = fcluster(dendrogram, self.emb_threshold, criterion=""distance"")\n            for i, k in zip(clean_indices, clusters):\n                cluster_assignment[i] = k\n\n        loose_indices = np.where(noisy)[0]\n        if len(clean_indices) == 0:\n            if len(loose_indices) < 2:\n                clusters = [1] * len(loose_indices)\n            else:\n                dendrogram = pool(embedding[loose_indices], metric=""cosine"")\n                clusters = fcluster(\n                    dendrogram, self.emb_threshold, criterion=""distance""\n                )\n            for i, k in zip(loose_indices, clusters):\n                cluster_assignment[i] = k\n\n        else:\n            # NEAREST NEIGHBOR\n            distance = cdist(\n                embedding[clean_indices], embedding[loose_indices], metric=""cosine""\n            )\n            nearest_neighbor = np.argmin(distance, axis=0)\n            for loose_index, nn in zip(loose_indices, nearest_neighbor):\n                strict_index = clean_indices[nn]\n                cluster_assignment[loose_index] = cluster_assignment[strict_index]\n\n            # # NEAREST CLUSTER\n            # centroid = np.vstack(\n            #     [\n            #         np.mean(embedding[cluster_assignment == k], axis=0)\n            #         for k in np.unique(clusters)\n            #     ]\n            # )\n            # distance = cdist(centroid, embedding[loose_indices], metric=""cosine"")\n            # cluster_assignment[loose_indices] = np.argmin(distance, axis=0) + 1\n\n        # convert cluster assignment to pyannote.core.Annotation\n        # (make sure to keep speech regions unchanged)\n        hypothesis = Annotation(uri=current_file.get(""uri"", None))\n        for s, segment in enumerate(speech):\n\n            indices = np.where(segment_assignment == s + 1)[0]\n            if len(indices) == 0:\n                indices = np.where(segment_assignment == -(s + 1))[0]\n                if len(indices) == 0:\n                    continue\n\n            clusters = cluster_assignment[indices]\n\n            start, k = segment.start, clusters[0]\n            change_point = np.diff(clusters) != 0\n            for i, new_k in zip(indices[1:][change_point], clusters[1:][change_point]):\n                end = window[i].middle + 0.5 * window.step\n                hypothesis[Segment(start, end)] = k\n                start = end\n                k = new_k\n            hypothesis[Segment(start, segment.end)] = k\n\n        return hypothesis.support()\n\n    def get_metric(self) -> Union[DetectionErrorRate, DiarizationErrorRate]:\n        if self.only_sad:\n            return DetectionErrorRate(collar=0.0)\n        else:\n            return DiarizationErrorRate(collar=0.0, skip_overlap=False)\n'"
pyannote/audio/interactive/utils.py,0,"b'# The MIT License (MIT)\n#\n# Copyright (c) 2020 CNRS\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# AUTHORS\n# Herv\xc3\xa9 Bredin - http://herve.niderb.fr\n\n\nfrom typing import List, Tuple\n\nTime = float\nfrom pyannote.core import SlidingWindow\nimport numpy as np\n\n\ndef time2index(\n    constraints_time: List[Tuple[Time, Time]], window: SlidingWindow,\n) -> List[Tuple[int, int]]:\n    """"""Convert time-based constraints to index-based constraints\n\n    Parameters\n    ----------\n    constraints_time : list of (float, float)\n        Time-based constraints\n    window : SlidingWindow\n        Window used for embedding extraction\n\n    Returns\n    -------\n    constraints : list of (int, int)\n        Index-based constraints\n    """"""\n\n    constraints = []\n    for t1, t2 in constraints_time:\n        i1 = window.closest_frame(t1)\n        i2 = window.closest_frame(t2)\n        if i1 == i2:\n            continue\n        constraints.append((i1, i2))\n    return constraints\n\n\ndef index2index(\n    constraints: List[Tuple[int, int]],\n    keep: np.ndarray,\n    reverse=False,\n    return_mapping=False,\n) -> List[Tuple[int, int]]:\n    """"""Map constraints from original to keep-only index base\n\n    Parameters\n    ----------\n    constraints : list of pairs\n        Constraints in original index base.\n    keep : np.ndarray\n        Boolean array indicating whether to keep observations.\n    reverse : bool\n        Set to True to go from keep-only to original index base.\n    return_mapping : bool, optional\n        Return mapping instead of mapped constraints.\n\n    Returns\n    -------\n    shifted_constraints : list of index pairs\n        Constraints in keep-only index base.\n    """"""\n\n    if reverse:\n        mapping = np.arange(len(keep))[keep]\n    else:\n        mapping = np.cumsum(keep) - 1\n\n    if return_mapping:\n        return mapping\n\n    return [\n        (mapping[i1], mapping[i2]) for i1, i2 in constraints if keep[i1] and keep[i2]\n    ]\n'"
pyannote/audio/labeling/__init__.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2016-2018 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n""""""\n# Frame-based audio labeling\n""""""\n'"
pyannote/audio/labeling/gradient_reversal.py,2,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright \xc2\xa9 2019 Jan Freyberg\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n\nimport torch.nn as nn\nfrom torch.autograd import Function\n\n\nclass RevGrad(Function):\n    @staticmethod\n    def forward(ctx, input_):\n        ctx.save_for_backward(input_)\n        output = input_\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):  # pragma: no cover\n        grad_input = None\n        if ctx.needs_input_grad[0]:\n            grad_input = -grad_output\n        return grad_input\n\n\nrevgrad = RevGrad.apply\n\n\nclass GradientReversal(nn.Module):\n    def __init__(self, *args, **kwargs):\n        """"""\n        A gradient reversal layer.\n        This layer has no parameters, and simply reverses the gradient\n        in the backward pass.\n        """"""\n\n        super().__init__(*args, **kwargs)\n\n    def forward(self, input_):\n        return revgrad(input_)\n'"
pyannote/audio/models/__init__.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2019-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\nfrom .models import PyanNet, SincTDNN, ACRoPoLiS\n'"
pyannote/audio/models/convolutional.py,4,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n\nimport torch\nimport torch.nn as nn\nfrom typing import Text\nfrom typing import List\nfrom pyannote.core import SlidingWindow\nfrom pyannote.audio.train.task import Task\n\n\nclass Convolutional(nn.Module):\n    """"""Convolutional layers\n\n    Parameters\n    ----------\n    n_features : int,\n        Input feature shape. Should be 1.\n    sample_rate: int, optional\n        Input sample rate (in Hz). Defaults to 16000.\n    out_channels : list of int, optional\n        Number of channels produced by the convolutions.\n    kernel_size : list of int, optional\n        Size of the convolving kernels.\n    stride : list of int, optional\n        Stride of the convolutions\n    max_pool : list of int, optional\n        Size and stride of the size of the windows to take a max over.\n    instance_normalize : bool, optional\n        Apply instance normalization after pooling. Set to False to not apply\n        any normalization. Defaults to True.\n    dropout : float, optional\n        If non-zero, introduces a Dropout layer on the outputs of each layer\n        except the last layer, with dropout probability equal to dropout.\n    """"""\n\n    def __init__(\n        self,\n        n_features: int,\n        sample_rate: int = 16000,\n        out_channels: List[int] = [512, 512, 512, 512, 512, 512],\n        kernel_size: List[int] = [251, 5, 5, 5, 5, 5],\n        stride: List[int] = [5, 1, 1, 1, 1, 1],\n        max_pool: List[int] = [3, 3, 3, 3, 3, 3],\n        instance_normalize: bool = True,\n        dropout: float = 0.0,\n    ):\n        super().__init__()\n\n        self.n_features = n_features\n        self.sample_rate = sample_rate\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.max_pool = max_pool\n        self.instance_normalize = instance_normalize\n        self.dropout = dropout\n\n        self.conv1ds = nn.ModuleList()\n        self.pool1ds = nn.ModuleList()\n        if self.instance_normalize:\n            self.norm1ds = nn.ModuleList()\n        self.activation = nn.LeakyReLU(negative_slope=1e-2, inplace=False)\n        if self.dropout > 0.0:\n            self._dropout = nn.Dropout(p=self.dropout, inplace=False)\n\n        in_channels = self.n_features\n        for i, (out_channels, kernel_size, stride, max_pool) in enumerate(\n            zip(self.out_channels, self.kernel_size, self.stride, self.max_pool)\n        ):\n            conv1d = nn.Conv1d(\n                in_channels,\n                out_channels,\n                kernel_size,\n                stride=stride,\n                padding=0,\n                dilation=1,\n                groups=1,\n                bias=True,\n                padding_mode=""zeros"",\n            )\n            self.conv1ds.append(conv1d)\n\n            pool1d = nn.MaxPool1d(\n                max_pool,\n                stride=max_pool,\n                padding=0,\n                dilation=1,\n                return_indices=False,\n                ceil_mode=False,\n            )\n            self.pool1ds.append(pool1d)\n\n            if self.instance_normalize:\n                norm1d = nn.InstanceNorm1d(out_channels, affine=True)\n                self.norm1ds.append(norm1d)\n\n            in_channels = out_channels\n\n    def forward(self, waveforms: torch.Tensor) -> torch.Tensor:\n        """"""Forward pass\n\n        Parameter\n        ---------\n        waveform : torch.Tensor\n            Waveform tensor with shape (batch_size, n_samples, 1)\n\n        Returns\n        -------\n        features : torch.Tensor\n            Output features (batch_size, n_frames, n_features)\n        """"""\n\n        output = waveforms.transpose(1, 2)\n\n        for i, (conv1d, pool1d) in enumerate(zip(self.conv1ds, self.pool1ds)):\n            output = conv1d(output)\n            output = pool1d(output)\n            if self.instance_normalize:\n                output = self.norm1ds[i](output)\n            output = self.activation(output)\n            if self.dropout > 0.0 and i + 1 < self.num_layers:\n                output = self._dropout(output)\n\n        return output.transpose(1, 2)\n\n    @property\n    def num_layers(self):\n        """"""Number of convolutional layers""""""\n        return len(self.out_channels)\n\n    @property\n    def dimension(self):\n        """"""Dimension of output features""""""\n        return self.out_channels[-1]\n\n    @staticmethod\n    def get_alignment(task: Task, **kwargs) -> Text:\n        """"""Output frame alignment""""""\n        return ""strict""\n\n    @staticmethod\n    def get_resolution(\n        task: Task,\n        sample_rate: int = 16000,\n        out_channels: List[int] = [512, 512, 512, 512, 512, 512],\n        kernel_size: List[int] = [251, 5, 5, 5, 5, 5],\n        stride: List[int] = [5, 1, 1, 1, 1, 1],\n        max_pool: List[int] = [3, 3, 3, 3, 3, 3],\n        **kwargs,\n    ) -> SlidingWindow:\n        """"""Output frame resolution""""""\n\n        # https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807\n        padding = 0\n        receptive_field, jump, start = 1, 1, 0.5\n        for ks, s, mp in zip(kernel_size, stride, max_pool):\n            # increase due to (Sinc)Conv1d\n            receptive_field += (ks - 1) * jump\n            start += ((ks - 1) / 2 - padding) * jump\n            jump *= s\n            # increase in receptive field due to MaxPool1d\n            receptive_field += (mp - 1) * jump\n            start += ((mp - 1) / 2 - padding) * jump\n            jump *= mp\n\n        return SlidingWindow(\n            duration=receptive_field / sample_rate, step=jump / sample_rate, start=0.0\n        )\n'"
pyannote/audio/models/linear.py,4,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n\nimport torch\nimport torch.nn as nn\nfrom typing import List\n\n\nclass Linear(nn.Module):\n    """"""Linear layers\n\n    Parameters\n    ----------\n    n_features : int\n        Input feature shape.\n    hidden_size : list of int, optional\n        Number of features in hidden. Defaults to [256, 128].\n    bias : bool, optional\n        If set to False, the layer will not learn an additive bias.\n    """"""\n\n    def __init__(\n        self, n_features: int, hidden_size: List[int] = [256, 128], bias: bool = True,\n    ):\n        super().__init__()\n\n        self.n_features = n_features\n        self.hidden_size = hidden_size\n        self.bias = bias\n\n        self.linear = nn.ModuleList()\n\n        for out_features in hidden_size:\n            linear = nn.Linear(n_features, out_features, bias=self.bias)\n            self.linear.append(linear)\n            n_features = out_features\n\n        self.activation = nn.Tanh()\n\n    def forward(self, features: torch.Tensor) -> torch.Tensor:\n        """"""Forward pass\n\n        Parameter\n        ---------\n        features : torch.Tensor\n            Feature tensor with shape (batch_size, n_frames, n_features) or\n            (batch_size, n_features).\n\n        Returns\n        -------\n        output : torch.Tensor\n            Output features with shape (batch_size, n_frames, hidden_size[-1])\n            or (batch_size, hidden_size[-1])\n        """"""\n\n        output = features\n        for linear in self.linear:\n            output = linear(output)\n            output = self.activation(output)\n        return output\n\n    @property\n    def num_layers(self) -> int:\n        """"""Number of linear layers""""""\n        return len(self.hidden_size)\n\n    @property\n    def dimension(self) -> int:\n        """"""Dimension of output features""""""\n        return self.hidden_size[-1]\n'"
pyannote/audio/models/models.py,17,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2019 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n# Juan Manuel Coria\n\nfrom typing import Optional\nfrom typing import Text\n\nimport torch\nimport torch.nn as nn\n\nfrom .sincnet import SincNet\nfrom .tdnn import XVectorNet\nfrom .pooling import TemporalPooling\n\n\nfrom .convolutional import Convolutional\nfrom .recurrent import Recurrent\nfrom .linear import Linear\nfrom .pooling import Pooling\nfrom .scaling import Scaling\n\nfrom pyannote.audio.train.model import Model\nfrom pyannote.audio.train.model import Resolution\nfrom pyannote.audio.train.model import RESOLUTION_CHUNK\nfrom pyannote.audio.train.model import RESOLUTION_FRAME\nfrom pyannote.audio.train.task import Task\n\n\nclass RNN(nn.Module):\n    """"""Recurrent layers\n\n    Parameters\n    ----------\n    n_features : `int`\n        Input feature shape.\n    unit : {\'LSTM\', \'GRU\'}, optional\n        Defaults to \'LSTM\'.\n    hidden_size : `int`, optional\n        Number of features in the hidden state h. Defaults to 16.\n    num_layers : `int`, optional\n        Number of recurrent layers. Defaults to 1.\n    bias : `boolean`, optional\n        If False, then the layer does not use bias weights. Defaults to True.\n    dropout : `float`, optional\n        If non-zero, introduces a Dropout layer on the outputs of each layer\n        except the last layer, with dropout probability equal to dropout.\n        Defaults to 0.\n    bidirectional : `boolean`, optional\n        If True, becomes a bidirectional RNN. Defaults to False.\n    concatenate : `boolean`, optional\n        Concatenate output of each layer instead of using only the last one\n        (which is the default behavior).\n    pool : {\'sum\', \'max\', \'last\', \'x-vector\'}, optional\n        Temporal pooling strategy. Defaults to no pooling.\n    """"""\n\n    def __init__(\n        self,\n        n_features,\n        unit=""LSTM"",\n        hidden_size=16,\n        num_layers=1,\n        bias=True,\n        dropout=0,\n        bidirectional=False,\n        concatenate=False,\n        pool=None,\n    ):\n        super().__init__()\n\n        self.n_features = n_features\n\n        self.unit = unit\n        Klass = getattr(nn, self.unit)\n\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.bias = bias\n        self.dropout = dropout\n        self.bidirectional = bidirectional\n        self.concatenate = concatenate\n        self.pool = pool\n        self.pool_ = TemporalPooling.create(pool) if pool is not None else None\n\n        if num_layers < 1:\n            msg = \'""bidirectional"" must be set to False when num_layers < 1\'\n            if bidirectional:\n                raise ValueError(msg)\n            msg = \'""concatenate"" must be set to False when num_layers < 1\'\n            if concatenate:\n                raise ValueError(msg)\n            return\n\n        if self.concatenate:\n\n            self.rnn_ = nn.ModuleList([])\n            for i in range(self.num_layers):\n\n                if i > 0:\n                    input_dim = self.hidden_size\n                    if self.bidirectional:\n                        input_dim *= 2\n                else:\n                    input_dim = self.n_features\n\n                if i + 1 == self.num_layers:\n                    dropout = 0\n                else:\n                    dropout = self.dropout\n\n                rnn = Klass(\n                    input_dim,\n                    self.hidden_size,\n                    num_layers=1,\n                    bias=self.bias,\n                    batch_first=True,\n                    dropout=dropout,\n                    bidirectional=self.bidirectional,\n                )\n\n                self.rnn_.append(rnn)\n\n        else:\n            self.rnn_ = Klass(\n                self.n_features,\n                self.hidden_size,\n                num_layers=self.num_layers,\n                bias=self.bias,\n                batch_first=True,\n                dropout=self.dropout,\n                bidirectional=self.bidirectional,\n            )\n\n    def forward(self, features, return_intermediate=False):\n        """"""Apply recurrent layer (and optional temporal pooling)\n\n        Parameters\n        ----------\n        features : `torch.Tensor`\n            Features shaped as (batch_size, n_frames, n_features)\n        return_intermediate : `boolean`, optional\n            Return intermediate RNN hidden state.\n\n        Returns\n        -------\n        output : `torch.Tensor`\n            TODO. Shape depends on parameters...\n        intermediate : `torch.Tensor`\n            (num_layers, batch_size, hidden_size * num_directions)\n        """"""\n\n        if self.num_layers < 1:\n\n            if return_intermediate:\n                msg = (\n                    \'""return_intermediate"" must be set to False \' ""when num_layers < 1""\n                )\n                raise ValueError(msg)\n\n            output = features\n\n        else:\n\n            if return_intermediate:\n                num_directions = 2 if self.bidirectional else 1\n\n            if self.concatenate:\n\n                if return_intermediate:\n                    msg = (\n                        \'""return_intermediate"" is not supported \'\n                        \'when ""concatenate"" is True\'\n                    )\n                    raise NotADirectoryError(msg)\n\n                outputs = []\n\n                hidden = None\n                output = None\n                # apply each layer separately...\n                for i, rnn in enumerate(self.rnn_):\n                    if i > 0:\n                        output, hidden = rnn(output, hidden)\n                    else:\n                        output, hidden = rnn(features)\n                    outputs.append(output)\n\n                # ... and concatenate their output\n                output = torch.cat(outputs, dim=2)\n\n            else:\n                output, hidden = self.rnn_(features)\n\n                if return_intermediate:\n                    if self.unit == ""LSTM"":\n                        h = hidden[0]\n                    elif self.unit == ""GRU"":\n                        h = hidden\n\n                    # to (num_layers, batch_size, num_directions * hidden_size)\n                    h = h.view(self.num_layers, num_directions, -1, self.hidden_size)\n                    intermediate = (\n                        h.transpose(2, 1)\n                        .contiguous()\n                        .view(self.num_layers, -1, num_directions * self.hidden_size)\n                    )\n\n        if self.pool_ is not None:\n            output = self.pool_(output)\n\n        if return_intermediate:\n            return output, intermediate\n\n        return output\n\n    def dimension():\n        doc = ""Output features dimension.""\n\n        def fget(self):\n            if self.num_layers < 1:\n                dimension = self.n_features\n            else:\n                dimension = self.hidden_size\n\n            if self.bidirectional:\n                dimension *= 2\n\n            if self.concatenate:\n                dimension *= self.num_layers\n\n            if self.pool == ""x-vector"":\n                dimension *= 2\n\n            return dimension\n\n        return locals()\n\n    dimension = property(**dimension())\n\n    def intermediate_dimension(self, layer):\n        if self.num_layers < 1:\n            dimension = self.n_features\n        else:\n            dimension = self.hidden_size\n\n        if self.bidirectional:\n            dimension *= 2\n\n        return dimension\n\n\nclass FF(nn.Module):\n    """"""Feedforward layers\n\n    Parameters\n    ----------\n    n_features : `int`\n        Input dimension.\n    hidden_size : `list` of `int`, optional\n        Linear layers hidden dimensions. Defaults to [16, ].\n    """"""\n\n    def __init__(self, n_features, hidden_size=[16,]):\n        super().__init__()\n\n        self.n_features = n_features\n        self.hidden_size = hidden_size\n\n        self.linear_ = nn.ModuleList([])\n        for hidden_size in self.hidden_size:\n            linear = nn.Linear(n_features, hidden_size, bias=True)\n            self.linear_.append(linear)\n            n_features = hidden_size\n\n    def forward(self, features):\n        """"""\n\n        Parameters\n        ----------\n        features : `torch.Tensor`\n            (batch_size, n_samples, n_features) or (batch_size, n_features)\n\n        Returns\n        -------\n        output : `torch.Tensor`\n            (batch_size, n_samples, hidden_size[-1]) or (batch_size, hidden_size[-1])\n        """"""\n\n        output = features\n        for linear in self.linear_:\n            output = linear(output)\n            output = torch.tanh(output)\n        return output\n\n    def dimension():\n        doc = ""Output dimension.""\n\n        def fget(self):\n            if self.hidden_size:\n                return self.hidden_size[-1]\n            return self.n_features\n\n        return locals()\n\n    dimension = property(**dimension())\n\n\nclass Embedding(nn.Module):\n    """"""Embedding\n\n    Parameters\n    ----------\n    n_features : `int`\n        Input dimension.\n    batch_normalize : `boolean`, optional\n        Apply batch normalization. This is more or less equivalent to\n        embedding whitening.\n    scale : {""fixed"", ""logistic""}, optional\n        Scaling method. Defaults to no scaling.\n    unit_normalize : deprecated in favor of \'scale\'\n    """"""\n\n    def __init__(\n        self,\n        n_features: int,\n        batch_normalize: bool = False,\n        scale: Text = None,\n        unit_normalize: bool = False,\n    ):\n        super().__init__()\n\n        self.n_features = n_features\n\n        self.batch_normalize = batch_normalize\n        if self.batch_normalize:\n            self.batch_normalize_ = nn.BatchNorm1d(\n                n_features, eps=1e-5, momentum=0.1, affine=False\n            )\n\n        self.scale = scale\n        self.scaling = Scaling(n_features, method=scale)\n\n        if unit_normalize is True:\n            msg = f""\'unit_normalize\' has been deprecated in favor of \'scale\'.""\n            raise ValueError(msg)\n\n    def forward(self, embedding):\n\n        if self.batch_normalize:\n            embedding = self.batch_normalize_(embedding)\n\n        return self.scaling(embedding)\n\n    @property\n    def dimension(self):\n        """"""Output dimension.""""""\n        return self.n_features\n\n\nclass PyanNet(Model):\n    """"""waveform -> SincNet -> RNN [-> merge] [-> time_pool] -> FC -> output\n\n    Parameters\n    ----------\n    sincnet : `dict`, optional\n        SincNet parameters. Defaults to `pyannote.audio.models.sincnet.SincNet`\n        default parameters. Use {\'skip\': True} to use handcrafted features\n        instead of waveforms: [ waveform -> SincNet -> RNN -> ... ] then\n        becomes [ features -> RNN -> ...].\n    rnn : `dict`, optional\n        Recurrent network parameters. Defaults to `RNN` default parameters.\n    ff : `dict`, optional\n        Feed-forward layers parameters. Defaults to `FF` default parameters.\n    embedding : `dict`, optional\n        Embedding parameters. Defaults to `Embedding` default parameters. This\n        only has effect when model is used for representation learning.\n    """"""\n\n    @staticmethod\n    def get_alignment(task: Task, sincnet=None, **kwargs):\n        """"""Get frame alignment""""""\n\n        if sincnet is None:\n            sincnet = dict()\n\n        if sincnet.get(""skip"", False):\n            return ""center""\n\n        return SincNet.get_alignment(task, **sincnet)\n\n    @staticmethod\n    def get_resolution(\n        task: Task,\n        sincnet: Optional[dict] = None,\n        rnn: Optional[dict] = None,\n        **kwargs,\n    ) -> Resolution:\n        """"""Get sliding window used for feature extraction\n\n        Parameters\n        ----------\n        task : Task\n        sincnet : dict, optional\n        rnn : dict, optional\n\n        Returns\n        -------\n        sliding_window : `pyannote.core.SlidingWindow` or {`window`, `frame`}\n            Returns RESOLUTION_CHUNK if model returns one vector per input\n            chunk, RESOLUTION_FRAME if model returns one vector per input\n            frame, and specific sliding window otherwise.\n        """"""\n\n        if rnn is None:\n            rnn = {""pool"": None}\n\n        if rnn.get(""pool"", None) is not None:\n            return RESOLUTION_CHUNK\n\n        if sincnet is None:\n            sincnet = {""skip"": False}\n\n        if sincnet.get(""skip"", False):\n            return RESOLUTION_FRAME\n\n        return SincNet.get_resolution(task, **sincnet)\n\n    def init(\n        self,\n        sincnet: Optional[dict] = None,\n        rnn: Optional[dict] = None,\n        ff: Optional[dict] = None,\n        embedding: Optional[dict] = None,\n    ):\n        """"""waveform -> SincNet -> RNN [-> merge] [-> time_pool] -> FC -> output\n\n        Parameters\n        ----------\n        sincnet : `dict`, optional\n            SincNet parameters. Defaults to `pyannote.audio.models.sincnet.SincNet`\n            default parameters. Use {\'skip\': True} to use handcrafted features\n            instead of waveforms: [ waveform -> SincNet -> RNN -> ... ] then\n            becomes [ features -> RNN -> ...].\n        rnn : `dict`, optional\n            Recurrent network parameters. Defaults to `RNN` default parameters.\n        ff : `dict`, optional\n            Feed-forward layers parameters. Defaults to `FF` default parameters.\n        embedding : `dict`, optional\n            Embedding parameters. Defaults to `Embedding` default parameters. This\n            only has effect when model is used for representation learning.\n        """"""\n\n        n_features = self.n_features\n\n        if sincnet is None:\n            sincnet = dict()\n        self.sincnet = sincnet\n\n        if not sincnet.get(""skip"", False):\n            if n_features != 1:\n                msg = (\n                    f""SincNet only supports mono waveforms. ""\n                    f""Here, waveform has {n_features} channels.""\n                )\n                raise ValueError(msg)\n            self.sincnet_ = SincNet(**sincnet)\n            n_features = self.sincnet_.dimension\n\n        if rnn is None:\n            rnn = dict()\n        self.rnn = rnn\n        self.rnn_ = RNN(n_features, **rnn)\n        n_features = self.rnn_.dimension\n\n        if ff is None:\n            ff = dict()\n        self.ff = ff\n        self.ff_ = FF(n_features, **ff)\n        n_features = self.ff_.dimension\n\n        if self.task.is_representation_learning:\n            if embedding is None:\n                embedding = dict()\n            self.embedding = embedding\n            self.embedding_ = Embedding(n_features, **embedding)\n            return\n\n        self.linear_ = nn.Linear(n_features, len(self.classes), bias=True)\n        self.activation_ = self.task.default_activation\n\n    def forward(self, waveforms, return_intermediate=None):\n        """"""Forward pass\n\n        Parameters\n        ----------\n        waveforms : (batch_size, n_samples, 1) `torch.Tensor`\n            Batch of waveforms. In case SincNet is skipped, a tensor with shape\n            (batch_size, n_samples, n_features) is expected.\n        return_intermediate : `int`, optional\n            Index of RNN layer. Returns RNN intermediate hidden state.\n            Defaults to only return the final output.\n\n        Returns\n        -------\n        output : `torch.Tensor`\n            Final network output.\n        intermediate : `torch.Tensor`\n            Intermediate network output (only when `return_intermediate`\n            is provided).\n        """"""\n\n        if self.sincnet.get(""skip"", False):\n            output = waveforms\n        else:\n            output = self.sincnet_(waveforms)\n\n        if return_intermediate is None:\n            output = self.rnn_(output)\n        else:\n            if return_intermediate == 0:\n                intermediate = output\n                output = self.rnn_(output)\n            else:\n                return_intermediate -= 1\n                # get RNN final AND intermediate outputs\n                output, intermediate = self.rnn_(output, return_intermediate=True)\n                # only keep hidden state of requested layer\n                intermediate = intermediate[return_intermediate]\n\n        output = self.ff_(output)\n\n        if self.task.is_representation_learning:\n            return self.embedding_(output)\n\n        output = self.linear_(output)\n        output = self.activation_(output)\n\n        if return_intermediate is None:\n            return output\n        return output, intermediate\n\n    @property\n    def dimension(self):\n        if self.task.is_representation_learning:\n            return self.embedding_.dimension\n\n        return Model.dimension.fget(self)\n\n    def intermediate_dimension(self, layer):\n        if layer == 0:\n            return self.sincnet_.dimension\n        return self.rnn_.intermediate_dimension(layer - 1)\n\n\nclass SincTDNN(Model):\n    """"""waveform -> SincNet -> XVectorNet (TDNN -> FC) -> output\n\n    Parameters\n    ----------\n    sincnet : `dict`, optional\n        SincNet parameters. Defaults to `pyannote.audio.models.sincnet.SincNet`\n        default parameters.\n    tdnn : `dict`, optional\n        X-Vector Time-Delay neural network parameters.\n        Defaults to `pyannote.audio.models.tdnn.XVectorNet` default parameters.\n    embedding : `dict`, optional\n        Embedding parameters. Defaults to `Embedding` default parameters. This\n        only has effect when model is used for representation learning.\n    """"""\n\n    @staticmethod\n    def get_alignment(task: Task, sincnet=None, **kwargs):\n        """"""Get frame alignment""""""\n\n        if sincnet is None:\n            sincnet = dict()\n\n        return SincNet.get_alignment(task, **sincnet)\n\n    supports_packed = False\n\n    @staticmethod\n    def get_resolution(\n        task: Task, sincnet: Optional[dict] = None, **kwargs\n    ) -> Resolution:\n        """"""Get sliding window used for feature extraction\n\n        Parameters\n        ----------\n        task : Task\n        sincnet : dict, optional\n\n        Returns\n        -------\n        sliding_window : `pyannote.core.SlidingWindow` or {`window`, `frame`}\n        """"""\n\n        # TODO add support for frame-wise and sequence labeling tasks\n        # TODO https://github.com/pyannote/pyannote-audio/issues/290\n        return RESOLUTION_CHUNK\n\n    def init(\n        self,\n        sincnet: Optional[dict] = None,\n        tdnn: Optional[dict] = None,\n        embedding: Optional[dict] = None,\n    ):\n        """"""waveform -> SincNet -> XVectorNet (TDNN -> FC) -> output\n\n        Parameters\n        ----------\n        sincnet : `dict`, optional\n            SincNet parameters. Defaults to `pyannote.audio.models.sincnet.SincNet`\n            default parameters.\n        tdnn : `dict`, optional\n            X-Vector Time-Delay neural network parameters.\n            Defaults to `pyannote.audio.models.tdnn.XVectorNet` default parameters.\n        embedding : `dict`, optional\n            Embedding parameters. Defaults to `Embedding` default parameters. This\n            only has effect when model is used for representation learning.\n        """"""\n\n        n_features = self.n_features\n\n        if sincnet is None:\n            sincnet = dict()\n        self.sincnet = sincnet\n\n        if n_features != 1:\n            raise ValueError(\n                ""SincNet only supports mono waveforms. ""\n                f""Here, waveform has {n_features} channels.""\n            )\n        self.sincnet_ = SincNet(**sincnet)\n        n_features = self.sincnet_.dimension\n\n        if tdnn is None:\n            tdnn = dict()\n        self.tdnn = tdnn\n        self.tdnn_ = XVectorNet(n_features, **tdnn)\n        n_features = self.tdnn_.dimension\n\n        if self.task.is_representation_learning:\n            if embedding is None:\n                embedding = dict()\n            self.embedding = embedding\n            self.embedding_ = Embedding(n_features, **embedding)\n        else:\n            self.linear_ = nn.Linear(n_features, len(self.classes), bias=True)\n            self.activation_ = self.task.default_activation\n\n    def forward(self, waveforms: torch.Tensor, **kwargs) -> torch.Tensor:\n        """"""Forward pass\n\n        Parameters\n        ----------\n        waveforms : (batch_size, n_samples, 1) `torch.Tensor`\n            Batch of waveforms\n\n        Returns\n        -------\n        output : `torch.Tensor`\n            Final network output or intermediate network output\n            (only when `return_intermediate` is provided).\n        """"""\n\n        output = self.sincnet_(waveforms)\n\n        return_intermediate = (\n            ""segment6"" if self.task.is_representation_learning else None\n        )\n        output = self.tdnn_(output, return_intermediate=return_intermediate)\n\n        if self.task.is_representation_learning:\n            return self.embedding_(output)\n\n        return self.activation_(self.linear_(output))\n\n    @property\n    def dimension(self):\n        if self.task.is_representation_learning:\n            return self.embedding_.dimension\n\n        return Model.dimension.fget(self)\n\n\nclass ACRoPoLiS(Model):\n    """"""Audio -> Convolutional -> Recurrent (-> optional Pooling) -> Linear -> Scores\n\n    Parameters\n    ----------\n    specifications : dict\n        Task specifications.\n    convolutional : dict, optional\n        Definition of convolutional layers.\n        Defaults to convolutional.Convolutional default hyper-parameters.\n    recurrent : dict, optional\n        Definition of recurrent layers.\n        Defaults to recurrent.Recurrent default hyper-parameters.\n    pooling : {""last"", """"}, optional\n        Definition of pooling layer. Only used when self.task.returns_vector is\n        True, in which case it defaults to ""last"" pooling.\n    linear : dict, optional\n        Definition of linear layers.\n        Defaults to linear.Linear default hyper-parameters.\n    scale : dict, optional\n    """"""\n\n    def init(\n        self,\n        convolutional: dict = None,\n        recurrent: dict = None,\n        linear: dict = None,\n        pooling: Text = None,\n        scale: dict = None,\n    ):\n\n        self.normalize = nn.InstanceNorm1d(self.n_features)\n\n        if convolutional is None:\n            convolutional = dict()\n        self.convolutional = convolutional\n        self.cnn = Convolutional(self.n_features, **convolutional)\n\n        if recurrent is None:\n            recurrent = dict()\n        self.recurrent = recurrent\n        self.rnn = Recurrent(self.cnn.dimension, **recurrent)\n\n        if pooling is None and self.task.returns_vector:\n            pooling = ""last""\n        if pooling is not None and self.task.returns_sequence:\n            msg = f""\'pooling\' should not be used for labeling tasks (is: {pooling}).""\n            raise ValueError(msg)\n\n        self.pooling = pooling\n        self.pool = Pooling(\n            self.rnn.dimension,\n            method=self.pooling,\n            bidirectional=self.rnn.bidirectional,\n        )\n\n        if linear is None:\n            linear = dict()\n        self.linear = linear\n        self.ff = Linear(self.pool.dimension, **linear)\n\n        if self.task.is_representation_learning and scale is None:\n            scale = dict()\n        if not self.task.is_representation_learning and scale is not None:\n            msg = (\n                f""\'scale\' should not be used for representation learning (is: {scale}).""\n            )\n            raise ValueError(msg)\n\n        self.scale = scale\n        self.scaling = Scaling(self.ff.dimension, **scale)\n\n        if not self.task.is_representation_learning:\n            self.final_linear = nn.Linear(\n                self.scaling.dimension, len(self.classes), bias=True\n            )\n            self.final_activation = self.task.default_activation\n\n    def forward(self, waveforms: torch.Tensor, **kwargs) -> torch.Tensor:\n        """"""Forward pass\n\n        Parameters\n        ----------\n        waveforms : (batch_size, n_samples, 1) `torch.Tensor`\n            Batch of waveforms\n\n        Returns\n        -------\n        output : `torch.Tensor`\n            Final network output or intermediate network output\n            (only when `return_intermediate` is provided).\n        """"""\n\n        output = self.normalize(waveforms.transpose(1, 2)).transpose(1, 2)\n        output = self.cnn(output)\n        output = self.rnn(output)\n        output = self.pool(output)\n        output = self.ff(output)\n\n        if not self.task.is_representation_learning:\n            output = self.final_linear(output)\n            output = self.final_activation(output)\n        return output\n\n    @property\n    def dimension(self):\n        if self.task.is_representation_learning:\n            return self.ff.dimension\n        else:\n            return len(self.classes)\n\n    @staticmethod\n    def get_alignment(task: Task, convolutional: Optional[dict] = None, **kwargs):\n        """"""Get frame alignment""""""\n\n        if convolutional is None:\n            convolutional = dict()\n\n        return Convolutional.get_alignment(task, **convolutional)\n\n    @staticmethod\n    def get_resolution(\n        task: Task, convolutional: Optional[dict] = None, **kwargs\n    ) -> Resolution:\n        """"""Get frame resolution\n\n        Parameters\n        ----------\n        task : Task\n        convolutional : dict, optional\n\n        Returns\n        -------\n        sliding_window : `pyannote.core.SlidingWindow` or {`window`, `frame`}\n        """"""\n\n        if task.returns_vector:\n            return RESOLUTION_CHUNK\n\n        if convolutional is None:\n            convolutional = dict()\n\n        return Convolutional.get_resolution(task, **convolutional)\n'"
pyannote/audio/models/pooling.py,22,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Juan Manuel Coria\n# Herv\xc3\xa9 Bredin - http://herve.niderb.fr\n\nfrom typing_extensions import Literal\nfrom warnings import warn\n\nimport torch\nimport torch.nn as nn\n\n\nclass TemporalPooling(nn.Module):\n    """"""Pooling strategy over temporal sequences.""""""\n\n    @staticmethod\n    def create(method: Literal[""sum"", ""max"", ""last"", ""stats""]) -> nn.Module:\n        """"""Pooling strategy factory. returns an instance of `TemporalPooling` given its name.\n\n        Parameters\n        ----------\n        method : {\'sum\', \'max\', \'last\', \'stats\', \'x-vector\'}\n            Temporal pooling strategy. The `x-vector` method name\n            for stats pooling (equivalent to `stats`) is kept for\n            retrocompatibility but it will be removed in a future version.\n        Returns\n        -------\n        output : nn.Module\n            The temporal pooling strategy object\n        """"""\n        if method == ""sum"":\n            klass = SumPool\n        elif method == ""max"":\n            klass = MaxPool\n        elif method == ""last"":\n            klass = LastPool\n        elif method == ""stats"":\n            klass = StatsPool\n        elif method == ""x-vector"":\n            klass = StatsPool\n            warn(\n                ""`x-vector` is deprecated and will be removed in a future version. Please use `stats` instead""\n            )\n        else:\n            raise ValueError(f""`{method}` is not a valid temporal pooling method"")\n        return klass()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        raise NotImplementedError(""TemporalPooling subclass must implement `forward`"")\n\n\nclass SumPool(TemporalPooling):\n    """"""Calculate pooling as the sum over a sequence""""""\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        """"""\n        Parameters\n        ----------\n        x : `torch.Tensor`, shape (batch_size, seq_len, hidden_size)\n            A batch of sequences.\n\n        Returns\n        -------\n        output : `torch.Tensor`, shape (batch_size, hidden_size)\n        """"""\n        return x.sum(dim=1)\n\n\nclass MaxPool(TemporalPooling):\n    """"""Calculate pooling as the maximum over a sequence""""""\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        """"""\n        Parameters\n        ----------\n        x : `torch.Tensor`, shape (batch_size, seq_len, hidden_size)\n            A batch of sequences.\n\n        Returns\n        -------\n        output : `torch.Tensor`, shape (batch_size, hidden_size)\n        """"""\n        return x.max(dim=1)[0]\n\n\nclass LastPool(TemporalPooling):\n    """"""Calculate pooling as the last element of a sequence""""""\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        """"""\n        Parameters\n        ----------\n        x : `torch.Tensor`, shape (batch_size, seq_len, hidden_size)\n            A batch of sequences.\n\n        Returns\n        -------\n        output : `torch.Tensor`, shape (batch_size, hidden_size)\n        """"""\n        return x[:, -1]\n\n\nclass StatsPool(TemporalPooling):\n    """"""Calculate pooling as the concatenated mean and standard deviation of a sequence""""""\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        """"""\n        Parameters\n        ----------\n        x : `torch.Tensor`, shape (batch_size, seq_len, hidden_size)\n            A batch of sequences.\n\n        Returns\n        -------\n        output : `torch.Tensor`, shape (batch_size, 2 * hidden_size)\n        """"""\n        mean, std = torch.mean(x, dim=1), torch.std(x, dim=1)\n        return torch.cat((mean, std), dim=1)\n\n\nclass Pooling(nn.Module):\n    """"""Pooling over the time dimension\n\n    Parameters\n    ----------\n    method : {""last"", ""max"", ""average""}, optional\n        Use ""max"" for max pooling, ""average"" for average pooling.\n        Use ""average"" for average pooling.\n        Use ""last"" for returning the last element of the sequence.\n    bidirectional : bool, optional\n        When using ""last"" pooling, indicate whether the input sequence should\n        be considered as the output of a bidirectional recurrent layer, in which\n        case the last element in both directions are concatenated.\n    """"""\n\n    def __init__(\n        self,\n        n_features,\n        method: Literal[""last"", ""max"", ""average""] = None,\n        bidirectional: bool = None,\n    ):\n        super().__init__()\n\n        if method == ""last"" and bidirectional is None:\n            msg = ""\'last\' pooling expects an additional \'bidirectional\' parameter.""\n            raise ValueError(msg)\n\n        self.n_features = n_features\n        self.method = method\n        self.bidirectional = bidirectional\n\n    def forward(self, sequences: torch.Tensor) -> torch.Tensor:\n        """"""Temporal pooling\n\n        Parameters\n        ----------\n        sequences : torch.Tensor\n            Input sequences with shape (batch_size, n_frames, n_features)\n\n        Returns\n        -------\n        pooled : torch.Tensor\n            Pooled sequences with shape (batch_size, n_features)\n        """"""\n\n        if self.method is None:\n            return sequences\n\n        if self.method == ""last"":\n            if self.bidirectional:\n                batch_size, n_frames, _ = sequences.shape\n                reshaped = sequences.view(batch_size, n_frames, 2, -1)\n                return torch.cat([reshaped[:, -1, 0], reshaped[:, 0, 1]], dim=1)\n            else:\n                return sequences[:, -1]\n\n        if self.method == ""max"":\n            return torch.max(sequences, dim=1, keepdim=False, out=None)[0]\n\n        if self.method == ""average"":\n            return torch.mean(sequences, dim=1, keepdim=False, out=None)\n\n    @property\n    def dimension(self):\n        ""Dimension of output features""\n        return self.n_features\n'"
pyannote/audio/models/recurrent.py,4,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\nimport torch\nimport torch.nn as nn\nfrom typing import Text\n\n\nclass Recurrent(nn.Module):\n    """"""Recurrent layers\n\n    Parameters\n    ----------\n    n_features : int\n        Input feature shape.\n    unit : {""LSTM"", ""GRU""}, optional\n        Defaults to ""LSTM"".\n    hidden_size : int, optional\n        Number of features in the hidden state h. Defaults to 512.\n    num_layers : int, optional\n        Number of recurrent layers. Defaults to 1.\n    bias : bool, optional\n        If False, then the layer does not use bias weights. Defaults to True.\n    dropout : float, optional\n        If non-zero, introduces a Dropout layer on the outputs of each layer\n        except the last layer, with dropout probability equal to dropout.\n        Defaults to 0.\n    bidirectional : bool, optional\n        Use bidirectional RNN. Defaults to True.\n    probes : bool, optional\n        Split multi-layer RNN into multiple one-layer RNNs to expose\n        corresponding probes (see pyannote.audio.train.model.Model.probes).\n        Might be useful when using a multi-layer RNN as the trunk of a larger\n        multi-task RNN tree.\n    """"""\n\n    def __init__(\n        self,\n        n_features: int,\n        unit: Text = ""LSTM"",\n        hidden_size: int = 512,\n        num_layers: int = 1,\n        bias: int = True,\n        dropout: float = 0.0,\n        bidirectional: bool = True,\n        probes: bool = False,\n    ):\n        super().__init__()\n\n        self.n_features = n_features\n        self.unit = unit\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.bias = bias\n        self.dropout = dropout\n        self.bidirectional = bidirectional\n        self.probes = probes\n\n        if num_layers < 1:\n            if bidirectional:\n                msg = ""\'bidirectional\' must be set to False when num_layers < 1""\n                raise ValueError(msg)\n            return\n\n        Klass = getattr(nn, self.unit)\n\n        if probes:\n\n            self.rnn = nn.ModuleList([])\n\n            for i in range(self.num_layers):\n\n                if i > 0:\n                    input_dim = self.hidden_size\n                    if self.bidirectional:\n                        input_dim *= 2\n                else:\n                    input_dim = self.n_features\n\n                dropout = 0 if (i + 1 == self.num_layers) else self.dropout\n\n                rnn = Klass(\n                    input_dim,\n                    self.hidden_size,\n                    num_layers=1,\n                    bias=self.bias,\n                    batch_first=True,\n                    dropout=dropout,\n                    bidirectional=self.bidirectional,\n                )\n\n                self.rnn.append(rnn)\n\n        else:\n            self.rnn = Klass(\n                self.n_features,\n                self.hidden_size,\n                num_layers=self.num_layers,\n                bias=self.bias,\n                batch_first=True,\n                dropout=self.dropout,\n                bidirectional=self.bidirectional,\n            )\n\n    def forward(self, features: torch.Tensor, **kwargs) -> torch.Tensor:\n        """"""Apply recurrent layer\n\n        Parameters\n        ----------\n        features : torch.Tensor\n            Input feature sequence with shape (batch_size, n_frames, n_features).\n\n        Returns\n        -------\n        output : torch.Tensor\n            Output sequence with shape (batch_size, n_frames, hidden_size x n_directions).\n        """"""\n\n        if self.num_layers < 1:\n            return features\n\n        if self.probes:\n\n            output, hidden = None, None\n            for i, rnn in enumerate(self.rnn):\n\n                rnn = getattr(self, f""rnn{i+1:02d}"")\n\n                if i > 0:\n                    output, hidden = rnn(output, hidden)\n                else:\n                    output, hidden = rnn(features)\n\n        else:\n            output, hidden = self.rnn(features)\n\n        return output\n\n    @property\n    def dimension(self):\n\n        if self.num_layers < 1:\n            return self.n_features\n\n        dimension = self.hidden_size\n        if self.bidirectional:\n            dimension *= 2\n\n        return dimension\n'"
pyannote/audio/models/scaling.py,4,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\nimport torch\nimport torch.nn as nn\n\ntry:\n    from typing import Literal\nexcept ImportError as e:\n    from typing_extensions import Literal\n\n\nclass Scaling(nn.Module):\n    """"""Scale feature vectors\n\n    Parameters\n    ----------\n    n_features : int\n        Number of input features.\n    method : {""unit"", ""logistic""}, optional\n        Defaults to no scaling.\n    """"""\n\n    def __init__(self, n_features: int, method: Literal[""fixed"", ""logistic""] = None):\n        super().__init__()\n        self.n_features = n_features\n        self.method = method\n\n        if self.method == ""logistic"":\n            self.batch_norm = nn.BatchNorm1d(\n                1, eps=1e-5, momentum=0.1, affine=True, track_running_stats=True\n            )\n            self.activation = nn.Sigmoid()\n\n    def forward(self, features: torch.Tensor) -> torch.Tensor:\n        """"""Scale features\n\n        Parameters\n        ----------\n        features : torch.Tensor\n            Input features of shape (batch_size, *, n_features)\n\n        Returns\n        -------\n        scaled : torch.Tensor\n            Scaled features of shape (batch_size, *, n_features)\n        """"""\n\n        if self.method is None:\n            return features\n\n        norm = features.norm(p=2, dim=-1, keepdim=True)\n\n        if self.method == ""unit"":\n            new_norm = 1.0\n\n        if self.method == ""logistic"":\n            new_norm = self.activation(self.batch_norm(norm))\n\n        return new_norm / (norm + 1e-6) * features\n\n    @property\n    def dimension(self):\n        return self.n_features\n'"
pyannote/audio/models/sincnet.py,20,"b'# The MIT License (MIT)\n#\n# Copyright (c) 2019 Mirco Ravanelli\n# Copyright (c) 2019-2020 CNRS\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# AUTHOR\n# Herv\xc3\xa9 Bredin - http://herve.niderb.fr\n\n# Part of this code was taken from https://github.com/mravanelli/SincNet\n# (see above license terms).\n\n# Please give proper credit to the authors if you are using SincNet-based\n# models  by citing their paper:\n\n# Mirco Ravanelli, Yoshua Bengio.\n# ""Speaker Recognition from raw waveform with SincNet"".\n# SLT 2018. https://arxiv.org/abs/1808.00158\n\nfrom typing import List\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport math\nfrom pyannote.core import SlidingWindow\nfrom pyannote.audio.train.task import Task\n\n\nclass SincConv1d(nn.Module):\n    """"""Sinc-based 1D convolution\n\n    Parameters\n    ----------\n    in_channels : `int`\n        Should be 1.\n    out_channels : `int`\n        Number of filters.\n    kernel_size : `int`\n        Filter length.\n    stride : `int`, optional\n        Defaults to 1.\n    sample_rate : `int`, optional\n        Sample rate. Defaults to 16000.\n    min_low_hz: `int`, optional\n        Defaults to 50.\n    min_band_hz: `int`, optional\n        Defaults to 50.\n\n    Usage\n    -----\n    Same as `torch.nn.Conv1d`\n\n    Reference\n    ---------\n    Mirco Ravanelli, Yoshua Bengio. ""Speaker Recognition from raw waveform with\n    SincNet"". SLT 2018. https://arxiv.org/abs/1808.00158\n    """"""\n\n    @staticmethod\n    def to_mel(hz):\n        return 2595 * np.log10(1 + hz / 700)\n\n    @staticmethod\n    def to_hz(mel):\n        return 700 * (10 ** (mel / 2595) - 1)\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        sample_rate=16000,\n        min_low_hz=50,\n        min_band_hz=50,\n        stride=1,\n        padding=0,\n        dilation=1,\n        bias=False,\n        groups=1,\n    ):\n\n        super().__init__()\n\n        if in_channels != 1:\n            msg = (\n                f""SincConv1d only supports one input channel. ""\n                f""Here, in_channels = {in_channels}.""\n            )\n            raise ValueError(msg)\n        self.in_channels = in_channels\n\n        self.out_channels = out_channels\n\n        if kernel_size % 2 == 0:\n            msg = (\n                f""SincConv1d only support odd kernel size. ""\n                f""Here, kernel_size = {kernel_size}.""\n            )\n            raise ValueError(msg)\n        self.kernel_size = kernel_size\n\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n\n        if bias:\n            raise ValueError(""SincConv1d does not support bias."")\n        if groups > 1:\n            raise ValueError(""SincConv1d does not support groups."")\n\n        self.sample_rate = sample_rate\n        self.min_low_hz = min_low_hz\n        self.min_band_hz = min_band_hz\n\n        # initialize filterbanks such that they are equally spaced in Mel scale\n        low_hz = 30\n        high_hz = self.sample_rate / 2 - (self.min_low_hz + self.min_band_hz)\n\n        mel = np.linspace(\n            self.to_mel(low_hz), self.to_mel(high_hz), self.out_channels + 1\n        )\n        hz = self.to_hz(mel)\n\n        # filter lower frequency (out_channels, 1)\n        self.low_hz_ = nn.Parameter(torch.Tensor(hz[:-1]).view(-1, 1))\n\n        # filter frequency band (out_channels, 1)\n        self.band_hz_ = nn.Parameter(torch.Tensor(np.diff(hz)).view(-1, 1))\n\n        # Half Hamming half window\n        n_lin = torch.linspace(\n            0, self.kernel_size / 2 - 1, steps=int((self.kernel_size / 2))\n        )\n        self.window_ = 0.54 - 0.46 * torch.cos(2 * math.pi * n_lin / self.kernel_size)\n\n        # (kernel_size, 1)\n        # Due to symmetry, I only need half of the time axes\n        n = (self.kernel_size - 1) / 2.0\n        self.n_ = 2 * math.pi * torch.arange(-n, 0).view(1, -1) / self.sample_rate\n\n    def forward(self, waveforms):\n        """"""Get sinc filters activations\n\n        Parameters\n        ----------\n        waveforms : `torch.Tensor` (batch_size, 1, n_samples)\n            Batch of waveforms.\n\n        Returns\n        -------\n        features : `torch.Tensor` (batch_size, out_channels, n_samples_out)\n            Batch of sinc filters activations.\n        """"""\n\n        self.n_ = self.n_.to(waveforms.device)\n        self.window_ = self.window_.to(waveforms.device)\n\n        low = self.min_low_hz + torch.abs(self.low_hz_)\n\n        high = torch.clamp(\n            low + self.min_band_hz + torch.abs(self.band_hz_),\n            self.min_low_hz,\n            self.sample_rate / 2,\n        )\n        band = (high - low)[:, 0]\n\n        f_times_t_low = torch.matmul(low, self.n_)\n        f_times_t_high = torch.matmul(high, self.n_)\n\n        # Equivalent to Eq.4 of the reference paper\n        # I just have expanded the sinc and simplified the terms.\n        # This way I avoid several useless computations.\n        band_pass_left = (\n            (torch.sin(f_times_t_high) - torch.sin(f_times_t_low)) / (self.n_ / 2)\n        ) * self.window_\n        band_pass_center = 2 * band.view(-1, 1)\n        band_pass_right = torch.flip(band_pass_left, dims=[1])\n\n        band_pass = torch.cat(\n            [band_pass_left, band_pass_center, band_pass_right], dim=1\n        )\n\n        band_pass = band_pass / (2 * band[:, None])\n\n        self.filters = (band_pass).view(self.out_channels, 1, self.kernel_size)\n\n        return F.conv1d(\n            waveforms,\n            self.filters,\n            stride=self.stride,\n            padding=self.padding,\n            dilation=self.dilation,\n            bias=None,\n            groups=1,\n        )\n\n\nclass SincNet(nn.Module):\n    """"""SincNet (learnable) feature extraction\n\n    Parameters\n    ----------\n    waveform_normalize : `bool`, optional\n        Standardize waveforms (to zero mean and unit standard deviation) and\n        apply (learnable) affine transform. Defaults to True.\n    instance_normalize : `bool`, optional\n        Standardize internal representation (to zero mean and unit standard\n        deviation) and apply (learnable) affine transform. Defaults to True.\n\n\n    Reference\n    ---------\n    Mirco Ravanelli, Yoshua Bengio. ""Speaker Recognition from raw waveform with\n    SincNet"". SLT 2018. https://arxiv.org/abs/1808.00158\n\n    """"""\n\n    @staticmethod\n    def get_alignment(task: Task, **kwargs):\n        """"""Get frame alignment""""""\n        return ""strict""\n\n    @staticmethod\n    def get_resolution(\n        task: Task,\n        sample_rate: int = 16000,\n        kernel_size: List[int] = [251, 5, 5],\n        stride: List[int] = [1, 1, 1],\n        max_pool: List[int] = [3, 3, 3],\n        **kwargs,\n    ) -> SlidingWindow:\n        """"""Get frame resolution\n\n        Parameters\n        ----------\n        task : Task\n        sample_rate : int, optional\n        kerne_size : list of int, optional\n        stride : list of int, optional\n        max_pool : list of int, optional\n\n        Returns\n        -------\n        resolution : SlidingWindow\n            Frame resolution.\n        """"""\n\n        # https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807\n        padding = 0\n        receptive_field, jump, start = 1, 1, 0.5\n        for ks, s, mp in zip(kernel_size, stride, max_pool):\n            # increase due to (Sinc)Conv1d\n            receptive_field += (ks - 1) * jump\n            start += ((ks - 1) / 2 - padding) * jump\n            jump *= s\n            # increase in receptive field due to MaxPool1d\n            receptive_field += (mp - 1) * jump\n            start += ((mp - 1) / 2 - padding) * jump\n            jump *= mp\n\n        return SlidingWindow(\n            duration=receptive_field / sample_rate, step=jump / sample_rate, start=0.0\n        )\n\n    def __init__(\n        self,\n        waveform_normalize=True,\n        sample_rate=16000,\n        min_low_hz=50,\n        min_band_hz=50,\n        out_channels=[80, 60, 60],\n        kernel_size: List[int] = [251, 5, 5],\n        stride=[1, 1, 1],\n        max_pool=[3, 3, 3],\n        instance_normalize=True,\n        activation=""leaky_relu"",\n        dropout=0.0,\n    ):\n        super().__init__()\n\n        # check parameters values\n        n_layers = len(out_channels)\n        if len(kernel_size) != n_layers:\n            msg = (\n                f""out_channels ({len(out_channels):d}) and kernel_size ""\n                f""({len(kernel_size):d}) should have the same length.""\n            )\n            raise ValueError(msg)\n        if len(stride) != n_layers:\n            msg = (\n                f""out_channels ({len(out_channels):d}) and stride ""\n                f""({len(stride):d}) should have the same length.""\n            )\n            raise ValueError(msg)\n        if len(max_pool) != n_layers:\n            msg = (\n                f""out_channels ({len(out_channels):d}) and max_pool ""\n                f""({len(max_pool):d}) should have the same length.""\n            )\n            raise ValueError(msg)\n\n        # Waveform normalization\n        self.waveform_normalize = waveform_normalize\n        if self.waveform_normalize:\n            self.waveform_normalize_ = torch.nn.InstanceNorm1d(1, affine=True)\n\n        # SincNet-specific parameters\n        self.sample_rate = sample_rate\n        self.min_low_hz = min_low_hz\n        self.min_band_hz = min_band_hz\n\n        # Conv1D parameters\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.conv1d_ = nn.ModuleList([])\n\n        # Max-pooling parameters\n        self.max_pool = max_pool\n        self.max_pool1d_ = nn.ModuleList([])\n\n        # Instance normalization\n        self.instance_normalize = instance_normalize\n        if self.instance_normalize:\n            self.instance_norm1d_ = nn.ModuleList([])\n\n        config = zip(self.out_channels, self.kernel_size, self.stride, self.max_pool)\n\n        in_channels = None\n        for i, (out_channels, kernel_size, stride, max_pool) in enumerate(config):\n\n            # 1D convolution\n            if i > 0:\n                conv1d = nn.Conv1d(\n                    in_channels,\n                    out_channels,\n                    kernel_size,\n                    stride=stride,\n                    padding=0,\n                    dilation=1,\n                    groups=1,\n                    bias=True,\n                )\n            else:\n                conv1d = SincConv1d(\n                    1,\n                    out_channels,\n                    kernel_size,\n                    sample_rate=self.sample_rate,\n                    min_low_hz=self.min_low_hz,\n                    min_band_hz=self.min_band_hz,\n                    stride=stride,\n                    padding=0,\n                    dilation=1,\n                    bias=False,\n                    groups=1,\n                )\n            self.conv1d_.append(conv1d)\n\n            # 1D max-pooling\n            max_pool1d = nn.MaxPool1d(max_pool, stride=max_pool, padding=0, dilation=1)\n            self.max_pool1d_.append(max_pool1d)\n\n            # 1D instance normalization\n            if self.instance_normalize:\n                instance_norm1d = nn.InstanceNorm1d(out_channels, affine=True)\n                self.instance_norm1d_.append(instance_norm1d)\n\n            in_channels = out_channels\n\n        # Activation function\n        self.activation = activation\n        if self.activation == ""leaky_relu"":\n            self.activation_ = nn.LeakyReLU(negative_slope=0.2)\n        else:\n            msg = f\'Only ""leaky_relu"" activation is supported.\'\n            raise ValueError(msg)\n\n        # Dropout\n        self.dropout = dropout\n        if self.dropout:\n            self.dropout_ = nn.Dropout(p=self.dropout)\n\n    def forward(self, waveforms):\n        """"""Extract SincNet features\n\n        Parameters\n        ----------\n        waveforms : (batch_size, n_samples, 1)\n            Batch of waveforms\n\n        Returns\n        -------\n        features : (batch_size, n_frames, out_channels[-1])\n        """"""\n\n        output = waveforms.transpose(1, 2)\n\n        # standardize waveforms\n        if self.waveform_normalize:\n            output = self.waveform_normalize_(output)\n\n        layers = zip(self.conv1d_, self.max_pool1d_)\n        for i, (conv1d, max_pool1d) in enumerate(layers):\n\n            output = conv1d(output)\n            if i == 0:\n                output = torch.abs(output)\n\n            output = max_pool1d(output)\n\n            if self.instance_normalize:\n                output = self.instance_norm1d_[i](output)\n\n            output = self.activation_(output)\n\n            if self.dropout:\n                output = self.dropout_(output)\n\n        return output.transpose(1, 2)\n\n    def dimension():\n        doc = ""Output features dimension.""\n\n        def fget(self):\n            return self.out_channels[-1]\n\n        return locals()\n\n    dimension = property(**dimension())\n'"
pyannote/audio/models/tdnn.py,6,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2019 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Juan Manuel Coria\n\n# The TDNN class defined here was taken from https://github.com/jonasvdd/TDNN\n\n# Please give proper credit to the authors if you are using TDNN based or X-Vector based\n# models by citing their papers:\n\n# Waibel, Alexander H., Toshiyuki Hanazawa, Geoffrey E. Hinton, Kiyohiro Shikano and Kevin J. Lang.\n# ""Phoneme recognition using time-delay neural networks.""\n# IEEE Trans. Acoustics, Speech, and Signal Processing 37 (1989): 328-339.\n# https://pdfs.semanticscholar.org/cd62/c9976534a6a2096a38244f6cbb03635a127e.pdf?_ga=2.86820248.1800960571.1579515113-23298545.1575886658\n\n# Peddinti, Vijayaditya, Daniel Povey and Sanjeev Khudanpur.\n# ""A time delay neural network architecture for efficient modeling of long temporal contexts.""\n# INTERSPEECH (2015).\n# https://www.danielpovey.com/files/2015_interspeech_multisplice.pdf\n\n# Snyder, David, Daniel Garcia-Romero, Gregory Sell, Daniel Povey and Sanjeev Khudanpur.\n# ""X-Vectors: Robust DNN Embeddings for Speaker Recognition.""\n# 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (2018): 5329-5333.\n# https://www.danielpovey.com/files/2018_icassp_xvectors.pdf\n\nfrom typing import Optional\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn.utils import weight_norm\nimport torch.nn.functional as F\n\nfrom .pooling import StatsPool\n\n\nclass TDNN(nn.Module):\n    def __init__(\n        self,\n        context: list,\n        input_channels: int,\n        output_channels: int,\n        full_context: bool = True,\n    ):\n        """"""\n        Implementation of a \'Fast\' TDNN layer by exploiting the dilation argument of the PyTorch Conv1d class\n\n        Due to its fastness the context has gained two constraints:\n            * The context must be symmetric\n            * The context must have equal spacing between each consecutive element\n\n        For example: the non-full and symmetric context {-3, -2, 0, +2, +3} is not valid since it doesn\'t have\n        equal spacing; The non-full context {-6, -3, 0, 3, 6} is both symmetric and has an equal spacing, this is\n        considered valid.\n\n        :param context: The temporal context\n        :param input_channels: The number of input channels\n        :param output_channels: The number of channels produced by the temporal convolution\n        :param full_context: Indicates whether a full context needs to be used\n        """"""\n        super(TDNN, self).__init__()\n        self.full_context = full_context\n        self.input_dim = input_channels\n        self.output_dim = output_channels\n\n        context = sorted(context)\n        self.check_valid_context(context, full_context)\n\n        if full_context:\n            kernel_size = context[-1] - context[0] + 1 if len(context) > 1 else 1\n            self.temporal_conv = weight_norm(\n                nn.Conv1d(input_channels, output_channels, kernel_size)\n            )\n        else:\n            # use dilation\n            delta = context[1] - context[0]\n            self.temporal_conv = weight_norm(\n                nn.Conv1d(\n                    input_channels,\n                    output_channels,\n                    kernel_size=len(context),\n                    dilation=delta,\n                )\n            )\n\n    def forward(self, x):\n        """"""\n        :param x: is one batch of data, x.size(): [batch_size, sequence_length, input_channels]\n            sequence length is the dimension of the arbitrary length data\n        :return: [batch_size, len(valid_steps), output_dim]\n        """"""\n        x = self.temporal_conv(torch.transpose(x, 1, 2))\n        return F.relu(torch.transpose(x, 1, 2))\n\n    @staticmethod\n    def check_valid_context(context: list, full_context: bool) -> None:\n        """"""\n        Check whether the context is symmetrical and whether and whether the passed\n        context can be used for creating a convolution kernel with dil\n\n        :param full_context: indicates whether the full context (dilation=1) will be used\n        :param context: The context of the model, must be symmetric if no full context and have an equal spacing.\n        """"""\n        if full_context:\n            assert (\n                len(context) <= 2\n            ), ""If the full context is given one must only define the smallest and largest""\n            if len(context) == 2:\n                assert context[0] + context[-1] == 0, ""The context must be symmetric""\n        else:\n            assert len(context) % 2 != 0, ""The context size must be odd""\n            assert (\n                context[len(context) // 2] == 0\n            ), ""The context contain 0 in the center""\n            if len(context) > 1:\n                delta = [context[i] - context[i - 1] for i in range(1, len(context))]\n                assert all(\n                    delta[0] == delta[i] for i in range(1, len(delta))\n                ), ""Intra context spacing must be equal!""\n\n\nclass XVectorNet(nn.Module):\n    """"""\n    X-Vector neural network architecture as defined by https://www.danielpovey.com/files/2018_icassp_xvectors.pdf\n\n    Parameters\n    ----------\n    input_dim : int, default 24\n        dimension of the input frames\n    embedding_dim : int, default 512\n        dimension of latent embeddings\n    """"""\n\n    @property\n    def dimension(self):\n        return self.embedding_dim\n\n    def __init__(self, input_dim: int = 24, embedding_dim: int = 512):\n        super(XVectorNet, self).__init__()\n        frame1 = TDNN(\n            context=[-2, 2],\n            input_channels=input_dim,\n            output_channels=512,\n            full_context=True,\n        )\n        frame2 = TDNN(\n            context=[-2, 0, 2],\n            input_channels=512,\n            output_channels=512,\n            full_context=False,\n        )\n        frame3 = TDNN(\n            context=[-3, 0, 3],\n            input_channels=512,\n            output_channels=512,\n            full_context=False,\n        )\n        frame4 = TDNN(\n            context=[0], input_channels=512, output_channels=512, full_context=True\n        )\n        frame5 = TDNN(\n            context=[0], input_channels=512, output_channels=1500, full_context=True\n        )\n        self.tdnn = nn.Sequential(frame1, frame2, frame3, frame4, frame5, StatsPool())\n        self.segment6 = nn.Linear(3000, embedding_dim)\n        self.segment7 = nn.Linear(embedding_dim, embedding_dim)\n        self.embedding_dim = embedding_dim\n\n    def forward(self, x: torch.Tensor, return_intermediate: Optional[str] = None):\n        """"""Calculate X-Vector network activations.\n           Return the requested intermediate layer without computing unnecessary activations.\n\n        Parameters\n        ----------\n        x : (batch_size, n_frames, out_channels)\n            Batch of frames\n        return_intermediate : \'stats_pool\' | \'segment6\' | \'segment7\' | None\n            If specified, return the activation of this specific layer.\n            segment6 and segment7 activations are returned before the application of non linearity.\n\n        Returns\n        -------\n        activations :\n            (batch_size, 3000)               if return_intermediate == \'stats_pool\'\n            (batch_size, embedding_dim)      if return_intermediate == \'segment6\' | \'segment7\' | None\n        """"""\n\n        x = self.tdnn(x)\n\n        if return_intermediate == ""stats_pool"":\n            return x\n\n        x = self.segment6(x)\n\n        if return_intermediate == ""segment6"":\n            return x\n\n        x = self.segment7(F.relu(x))\n\n        if return_intermediate == ""segment7"":\n            return x\n\n        return F.relu(x)\n'"
pyannote/audio/pipeline/__init__.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2017-2019 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n""""""\n# Pipelines\n""""""\n\nfrom .speech_activity_detection import SpeechActivityDetection\nfrom .overlap_detection import OverlapDetection\nfrom .speech_turn_segmentation import SpeechTurnSegmentation\nfrom .speech_turn_segmentation import OracleSpeechTurnSegmentation\nfrom .speaker_diarization import SpeakerDiarization\n\ntry:\n    from .resegmentation import Resegmentation\nexcept NotImplementedError:\n    pass\n'"
pyannote/audio/pipeline/overlap_detection.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2018-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\nfrom typing import Optional\nimport numpy as np\n\nfrom pyannote.pipeline import Pipeline\nfrom pyannote.pipeline.parameter import Uniform\n\nfrom pyannote.core import Timeline\nfrom pyannote.core import Annotation\nfrom pyannote.core import SlidingWindowFeature\n\nfrom pyannote.database import get_annotated\n\nfrom pyannote.audio.utils.signal import Binarize\nfrom pyannote.audio.features import Precomputed\n\nfrom pyannote.metrics.detection import DetectionPrecision\nfrom pyannote.metrics.detection import DetectionRecall\nfrom pyannote.metrics.detection import DetectionPrecisionRecallFMeasure\nfrom pyannote.metrics import f_measure\nfrom pyannote.audio.features.wrapper import Wrapper, Wrappable\n\n\nclass OverlapDetection(Pipeline):\n    """"""Overlap detection pipeline\n\n    Parameters\n    ----------\n    scores : Wrappable, optional\n        Describes how raw overlapped speech detection scores should be obtained.\n        See pyannote.audio.features.wrapper.Wrapper documentation for details.\n        Defaults to ""@ovl_scores"" that indicates that protocol files provide\n        the scores in the ""ovl_scores"" key.\n    precision : `float`, optional\n        Target detection precision. Defaults to 0.9.\n    fscore : bool, optional\n        Optimize (precision/recall) fscore. Defaults to optimizing recall at\n        target precision.\n\n\n    Hyper-parameters\n    ----------------\n    onset, offset : `float`\n        Onset/offset detection thresholds\n    min_duration_on, min_duration_off : `float`\n        Minimum duration in either state (overlap or not)\n    pad_onset, pad_offset : `float`\n        Padding duration.\n    """"""\n\n    def __init__(\n        self, scores: Wrappable = None, precision: float = 0.9, fscore: bool = False\n    ):\n        super().__init__()\n\n        if scores is None:\n            scores = ""@ovl_scores""\n        self.scores = scores\n        self._scores = Wrapper(self.scores)\n\n        self.precision = precision\n        self.fscore = fscore\n\n        # hyper-parameters\n        self.onset = Uniform(0.0, 1.0)\n        self.offset = Uniform(0.0, 1.0)\n        self.min_duration_on = Uniform(0.0, 2.0)\n        self.min_duration_off = Uniform(0.0, 2.0)\n        self.pad_onset = Uniform(-1.0, 1.0)\n        self.pad_offset = Uniform(-1.0, 1.0)\n\n    def initialize(self):\n        """"""Initialize pipeline with current set of parameters""""""\n\n        self._binarize = Binarize(\n            onset=self.onset,\n            offset=self.offset,\n            min_duration_on=self.min_duration_on,\n            min_duration_off=self.min_duration_off,\n            pad_onset=self.pad_onset,\n            pad_offset=self.pad_offset,\n        )\n\n    def __call__(self, current_file: dict) -> Annotation:\n        """"""Apply overlap detection\n\n        Parameters\n        ----------\n        current_file : `dict`\n            File as provided by a pyannote.database protocol. May contain a\n            \'ovl_scores\' key providing precomputed scores.\n\n        Returns\n        -------\n        overlap : `pyannote.core.Annotation`\n            Overlap regions.\n        """"""\n\n        ovl_scores = self._scores(current_file)\n\n        # if this check has not been done yet, do it once and for all\n        if not hasattr(self, ""log_scale_""):\n            # heuristic to determine whether scores are log-scaled\n            if np.nanmean(ovl_scores.data) < 0:\n                self.log_scale_ = True\n            else:\n                self.log_scale_ = False\n\n        data = np.exp(ovl_scores.data) if self.log_scale_ else ovl_scores.data\n\n        # overlap vs. non-overlap\n        if data.shape[1] > 1:\n            overlap_prob = SlidingWindowFeature(\n                1.0 - data[:, 0], ovl_scores.sliding_window\n            )\n        else:\n            overlap_prob = SlidingWindowFeature(data, ovl_scores.sliding_window)\n\n        overlap = self._binarize.apply(overlap_prob)\n\n        overlap.uri = current_file.get(""uri"", None)\n        return overlap.to_annotation(generator=""string"", modality=""overlap"")\n\n    @staticmethod\n    def to_overlap(reference: Annotation) -> Annotation:\n        """"""Get overlapped speech reference annotation\n\n        Parameters\n        ----------\n        reference : Annotation\n            File yielded by pyannote.database protocols.\n\n        Returns\n        -------\n        overlap : `pyannote.core.Annotation`\n            Overlapped speech reference.\n        """"""\n\n        overlap = Timeline(uri=reference.uri)\n        for (s1, t1), (s2, t2) in reference.co_iter(reference):\n            l1 = reference[s1, t1]\n            l2 = reference[s2, t2]\n            if l1 == l2:\n                continue\n            overlap.add(s1 & s2)\n        return overlap.support().to_annotation()\n\n    def get_metric(self, **kwargs) -> DetectionPrecisionRecallFMeasure:\n        """"""Get overlapped speech detection metric\n\n        Returns\n        -------\n        metric : DetectionPrecisionRecallFMeasure\n            Detection metric.\n        """"""\n\n        if not self.fscore:\n            raise NotImplementedError()\n\n        class _Metric(DetectionPrecisionRecallFMeasure):\n            def compute_components(\n                _self,\n                reference: Annotation,\n                hypothesis: Annotation,\n                uem: Timeline = None,\n                **kwargs\n            ) -> dict:\n                return super().compute_components(\n                    self.to_overlap(reference), hypothesis, uem=uem, **kwargs\n                )\n\n        return _Metric()\n\n    def loss(self, current_file: dict, hypothesis: Annotation) -> float:\n        """"""Compute (1 - recall) at target precision\n\n        If precision < target, return 1 + (1 - precision)\n\n        Parameters\n        ----------\n        current_file : `dict`\n            File as provided by a pyannote.database protocol.\n        hypothesis : `pyannote.core.Annotation`\n            Overlap regions.\n\n        Returns\n        -------\n        error : `float`\n            1. - segment coverage.\n        """"""\n\n        precision = DetectionPrecision()\n        recall = DetectionRecall()\n\n        if ""overlap_reference"" in current_file:\n            overlap_reference = current_file[""overlap_reference""]\n\n        else:\n            reference = current_file[""annotation""]\n            overlap_reference = self.to_overlap(reference)\n            current_file[""overlap_reference""] = overlap_reference\n\n        uem = get_annotated(current_file)\n        p = precision(overlap_reference, hypothesis, uem=uem)\n        r = recall(overlap_reference, hypothesis, uem=uem)\n\n        if p > self.precision:\n            return 1.0 - r\n        return 1.0 + (1.0 - p)\n'"
pyannote/audio/pipeline/resegmentation.py,1,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2019-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\nraise NotImplementedError(""FIXME"")\n\nfrom typing import Optional\nfrom pathlib import Path\nimport yaml\n\nimport torch\nfrom functools import partial\nfrom pyannote.core.utils.helper import get_class_by_name\n\nfrom pyannote.pipeline import Pipeline\nfrom pyannote.pipeline.parameter import Integer\nfrom pyannote.pipeline.parameter import LogUniform\nfrom pyannote.pipeline.parameter import Uniform\n\nfrom pyannote.core import Annotation\nfrom pyannote.metrics.diarization import GreedyDiarizationErrorRate\n\nfrom pyannote.audio.labeling.tasks.resegmentation import (\n    Resegmentation as _Resegmentation,\n)\nfrom pyannote.audio.labeling.tasks.resegmentation import (\n    ResegmentationWithOverlap as _ResegmentationWithOverlap,\n)\n\n\nclass Resegmentation(Pipeline):\n    """"""Resegmentation pipeline\n\n    Parameters\n    ----------\n    feature_extraction : `dict`, optional\n        Configuration dict for feature extraction.\n    architecture : `dict`, optional\n        Configuration dict for network architecture.\n    overlap : `boolean`, optional\n        Assign overlapped speech segments. Defaults to False.\n    keep_sad: `boolean`, optional\n        Keep speech/non-speech state unchanged. Defaults to False.\n    mask : `dict`, optional\n        Configuration dict for masking.\n        - dimension : `int`, optional\n        - log_scale : `bool`, optional\n    augmentation : `bool`, optional\n        Augment (self-)training data by adding noise from non-speech regions.\n        Defaults to False.\n    duration : `float`, optional\n        Defaults to 2s.\n    batch_size : `int`, optional\n        Defaults to 32.\n    gpu : `boolean`, optional\n        Defaults to False.\n\n    Sample configuration file\n    -------------------------\n    pipeline:\n        name: ResegmentationPipeline\n        params:\n            duration: 3\n            batch_size: 32\n            gpu: True\n            overlap: True\n            keep_sad: True\n            feature_extraction:\n               name: Precomputed\n               params:\n                  root_dir: /path/to/precomputed/features\n            architecture:\n               name: StackedLSTM\n               params:\n                  rnn: LSTM\n            mask:\n                dimension: 0\n                log_scale: True\n\n    preprocessors:\n        audio: /path/to/database.yml\n        hypothesis:\n           name: pyannote.database.util.RTTMLoader\n           params:\n              train: /path/to/input.train.rttm\n              development: /path/to/input.development.rttm\n              test: /path/to/input.test.rttm\n        overlap:\n           name: pyannote.audio.features.Precomputed\n           params:\n              root_dir: /path/to/precomputed/overlap_scores\n        mask:\n           name: pyannote.audio.features.Precomputed\n           params:\n              root_dir: /path/to/precomputed/overlap_scores\n\n    """"""\n\n    CONFIG_YML = ""{experiment_dir}/config.yml""\n\n    # TODO. add support for data augmentation\n    def __init__(\n        self,\n        feature_extraction: Optional[dict] = None,\n        architecture: Optional[dict] = None,\n        overlap: Optional[bool] = False,\n        keep_sad: Optional[bool] = False,\n        mask: Optional[dict] = None,\n        augmentation: Optional[bool] = False,\n        duration: Optional[float] = 2.0,\n        batch_size: Optional[float] = 32,\n        gpu: Optional[bool] = False,\n    ):\n\n        # feature extraction\n        if feature_extraction is None:\n            from pyannote.audio.features import LibrosaMFCC\n\n            self.feature_extraction_ = LibrosaMFCC(\n                e=False,\n                De=True,\n                DDe=True,\n                coefs=19,\n                D=True,\n                DD=True,\n                duration=0.025,\n                step=0.010,\n                sample_rate=16000,\n            )\n        else:\n            FeatureExtraction = get_class_by_name(\n                feature_extraction[""name""],\n                default_module_name=""pyannote.audio.features"",\n            )\n            self.feature_extraction_ = FeatureExtraction(\n                **feature_extraction.get(""params"", {}), augmentation=None\n            )\n\n        # network architecture\n        if architecture is None:\n            from pyannote.audio.models import PyanNet\n\n            self.Architecture_ = PyanNet\n            self.architecture_params_ = {""sincnet"": {""skip"": True}}\n\n        else:\n            self.Architecture_ = get_class_by_name(\n                architecture[""name""], default_module_name=""pyannote.audio.models""\n            )\n            self.architecture_params_ = architecture.get(""params"", {})\n\n        self.overlap = overlap\n        self.keep_sad = keep_sad\n\n        self.mask = mask\n        if mask is None:\n            self.mask_dimension_ = None\n            self.mask_logscale_ = False\n        else:\n            self.mask_dimension_ = mask[""dimension""]\n            self.mask_logscale_ = mask[""log_scale""]\n\n        self.augmentation = augmentation\n\n        self.duration = duration\n        self.batch_size = batch_size\n        self.gpu = gpu\n        self.device_ = torch.device(""cuda"") if self.gpu else torch.device(""cpu"")\n\n        # hyper-parameters\n        self.learning_rate = LogUniform(1e-3, 1)\n        self.epochs = Integer(10, 50)\n        self.ensemble = Integer(1, 5)\n        if self.overlap:\n            self.overlap_threshold = Uniform(0, 1)\n\n    def initialize(self):\n        """"""Initialize pipeline with current set of parameters""""""\n\n        ensemble = min(self.epochs, self.ensemble)\n\n        if self.overlap:\n            self._resegmentation = _ResegmentationWithOverlap(\n                self.feature_extraction_,\n                self.Architecture_,\n                self.architecture_params_,\n                keep_sad=self.keep_sad,\n                mask_dimension=self.mask_dimension_,\n                mask_logscale=self.mask_logscale_,\n                overlap_threshold=self.overlap_threshold,\n                epochs=self.epochs,\n                learning_rate=self.learning_rate,\n                ensemble=ensemble,\n                device=self.device_,\n                duration=self.duration,\n                batch_size=self.batch_size,\n            )\n\n        else:\n            self._resegmentation = _Resegmentation(\n                self.feature_extraction_,\n                self.Architecture_,\n                self.architecture_params_,\n                keep_sad=self.keep_sad,\n                mask_dimension=self.mask_dimension_,\n                mask_logscale=self.mask_logscale_,\n                epochs=self.epochs,\n                learning_rate=self.learning_rate,\n                ensemble=ensemble,\n                device=self.device_,\n                duration=self.duration,\n                batch_size=self.batch_size,\n            )\n\n    def __call__(self, current_file: dict) -> Annotation:\n        """"""Apply resegmentation\n\n        Parameters\n        ----------\n        current_file : `dict`\n            File as provided by a pyannote.database protocol. Should contain a\n            \'hypothesis\' key providing diarization before resegmentation (and\n            a \'overlap\' key in case overlap handling).\n\n        Returns\n        -------\n        new_hypothesis : `pyannote.core.Annotation`\n            Resegmented hypothesis.\n        """"""\n\n        return self._resegmentation.apply(current_file)\n\n    def get_metric(self) -> GreedyDiarizationErrorRate:\n        """"""Return new instance of detection error rate metric""""""\n        return GreedyDiarizationErrorRate(collar=0.0, skip_overlap=False)\n'"
pyannote/audio/pipeline/speaker_change_detection.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2018-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\nfrom typing import Optional\nfrom typing import Union\nimport numpy as np\n\nfrom pyannote.pipeline import Pipeline\nfrom pyannote.pipeline.parameter import Uniform\n\nfrom pyannote.core import Annotation\nfrom pyannote.core import SlidingWindowFeature\n\nfrom pyannote.audio.utils.signal import Peak\nfrom pyannote.audio.features import Precomputed\n\nfrom pyannote.database import get_annotated\nfrom pyannote.database import get_unique_identifier\nfrom pyannote.metrics.segmentation import SegmentationPurityCoverageFMeasure\nfrom pyannote.metrics.diarization import DiarizationPurityCoverageFMeasure\n\nfrom pyannote.audio.features.wrapper import Wrapper, Wrappable\n\n\nclass SpeakerChangeDetection(Pipeline):\n    """"""Speaker change detection pipeline\n\n    Parameters\n    ----------\n    scores : Wrappable, optional\n        Describes how raw speaker change detection scores should be obtained.\n        See pyannote.audio.features.wrapper.Wrapper documentation for details.\n        Defaults to ""@scd_scores"" that indicates that protocol files provide\n        the scores in the ""scd_scores"" key.\n    purity : `float`, optional\n        Target segments purity. Defaults to 0.95.\n    fscore : bool, optional\n        Optimize (precision/recall) fscore. Defaults to optimizing coverage at\n        given target `purity`.\n    diarization : bool, optional\n        Use diarization purity and coverage. Defaults to segmentation purity\n        and coverage.\n\n    Hyper-parameters\n    ----------------\n    alpha : `float`\n        Peak detection threshold.\n    min_duration : `float`\n        Segment minimum duration.\n    """"""\n\n    def __init__(\n        self,\n        scores: Wrappable = None,\n        purity: Optional[float] = 0.95,\n        fscore: bool = False,\n        diarization: bool = False,\n    ):\n        super().__init__()\n\n        if scores is None:\n            scores = ""@scd_scores""\n        self.scores = scores\n        self._scores = Wrapper(self.scores)\n\n        self.purity = purity\n        self.fscore = fscore\n        self.diarization = diarization\n\n        # hyper-parameters\n        self.alpha = Uniform(0.0, 1.0)\n        self.min_duration = Uniform(0.0, 10.0)\n\n    def initialize(self):\n        """"""Initialize pipeline with current set of parameters""""""\n\n        self._peak = Peak(alpha=self.alpha, min_duration=self.min_duration)\n\n    def __call__(self, current_file: dict) -> Annotation:\n        """"""Apply change detection\n\n        Parameters\n        ----------\n        current_file : `dict`\n            File as provided by a pyannote.database protocol.  May contain a\n            \'scd_scores\' key providing precomputed scores.\n\n        Returns\n        -------\n        speech : `pyannote.core.Annotation`\n            Speech regions.\n        """"""\n\n        scd_scores = self._scores(current_file)\n\n        # if this check has not been done yet, do it once and for all\n        if not hasattr(self, ""log_scale_""):\n            # heuristic to determine whether scores are log-scaled\n            if np.nanmean(scd_scores.data) < 0:\n                self.log_scale_ = True\n            else:\n                self.log_scale_ = False\n\n        data = np.exp(scd_scores.data) if self.log_scale_ else scd_scores.data\n\n        # take the final dimension\n        # (in order to support both classification, multi-class classification,\n        # and regression scores)\n        change_prob = SlidingWindowFeature(data[:, -1], scd_scores.sliding_window)\n\n        # peak detection\n        change = self._peak.apply(change_prob)\n        change.uri = current_file.get(""uri"", None)\n\n        return change.to_annotation(generator=""string"", modality=""audio"")\n\n    def get_metric(\n        self, parallel=False\n    ) -> Union[DiarizationPurityCoverageFMeasure, SegmentationPurityCoverageFMeasure]:\n        """"""Return new instance of f-score metric""""""\n\n        if not self.fscore:\n            raise NotImplementedError()\n\n        if self.diarization:\n            return DiarizationPurityCoverageFMeasure(parallel=parallel)\n\n        return SegmentationPurityCoverageFMeasure(tolerance=0.5, parallel=parallel)\n\n    def loss(self, current_file: dict, hypothesis: Annotation) -> float:\n        """"""Compute (1 - coverage) at target purity\n\n        If purity < target, return 1 + (1 - purity)\n\n        Parameters\n        ----------\n        current_file : `dict`\n            File as provided by a pyannote.database protocol.\n        hypothesis : `pyannote.core.Annotation`\n            Speech regions.\n\n        Returns\n        -------\n        error : `float`\n            1. - segment coverage.\n        """"""\n\n        metric = SegmentationPurityCoverageFMeasure(tolerance=0.500, beta=1)\n        reference = current_file[""annotation""]\n        uem = get_annotated(current_file)\n        f_measure = metric(reference, hypothesis, uem=uem)\n        purity, coverage, _ = metric.compute_metrics()\n        if purity > self.purity:\n            return 1.0 - coverage\n        else:\n            return 1.0 + (1.0 - purity)\n'"
pyannote/audio/pipeline/speaker_diarization.py,5,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2017-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\nfrom pathlib import Path\nfrom typing import Optional\nfrom typing import Union\nfrom typing import Text\n\nfrom pyannote.core import Annotation\nfrom pyannote.database import get_annotated\n\nfrom pyannote.metrics.diarization import GreedyDiarizationErrorRate\nfrom pyannote.metrics.diarization import DiarizationPurityCoverageFMeasure\n\nfrom .speech_turn_segmentation import SpeechTurnSegmentation\nfrom .speech_turn_segmentation import OracleSpeechTurnSegmentation\n\nfrom .speech_turn_clustering import SpeechTurnClustering\nfrom .speech_turn_assignment import SpeechTurnClosestAssignment\n\nfrom pyannote.pipeline import Pipeline\nfrom pyannote.pipeline.parameter import Uniform\n\n\nclass SpeakerDiarization(Pipeline):\n    """"""Speaker diarization pipeline\n\n    Parameters\n    ----------\n    sad_scores : Text or Path or \'oracle\', optional\n        Describes how raw speech activity detection scores\n        should be obtained. It can be either the name of a torch.hub model, or\n        the path to the output of the validation step of a model trained\n        locally, or the path to scores precomputed on disk.\n        Defaults to ""@sad_scores"", indicating that protocol\n        files provide the scores in the corresponding ""sad_scores"" key.\n        Use \'oracle\' to assume perfect speech activity detection.\n    scd_scores : Text or Path or \'oracle\', optional\n        Describes how raw speaker change detection scores\n        should be obtained. It can be either the name of a torch.hub model, or\n        the path to the output of the validation step of a model trained\n        locally, or the path to scores precomputed on disk.\n        Defaults to ""@scd_scores"", indicating that protocol\n        files provide the scores in the corresponding ""scd_scores"" key.\n        Use \'oracle\' to assume perfect speech turn segmentation,\n        `sad_scores` should then be set to \'oracle\' too.\n    embedding : Text or Path, optional\n        Describes how raw speaker embeddings should be obtained. It can be\n        either the name of a torch.hub model, or the path to the output of the\n        validation step of a model trained locally, or the path to embeddings\n        precomputed on disk. Defaults to ""@emb"" that indicates that protocol\n        files provide the embeddings in the ""emb"" key.\n    metric : {\'euclidean\', \'cosine\', \'angular\'}, optional\n        Metric used for comparing embeddings. Defaults to \'cosine\'.\n    method : {\'pool\', \'affinity_propagation\'}\n        Clustering method. Defaults to \'pool\'.\n    evaluation_only : `bool`\n        Only process the evaluated regions. Default to False.\n    purity : `float`, optional\n        Optimize coverage for target purity.\n        Defaults to optimizing diarization error rate.\n\n    Hyper-parameters\n    ----------------\n    min_duration : `float`\n        Do not cluster speech turns shorter than `min_duration`. Assign them to\n        the closest cluster (of long speech turns) instead.\n    """"""\n\n    def __init__(\n        self,\n        sad_scores: Union[Text, Path] = None,\n        scd_scores: Union[Text, Path] = None,\n        embedding: Union[Text, Path] = None,\n        metric: Optional[str] = ""cosine"",\n        method: Optional[str] = ""pool"",\n        evaluation_only: Optional[bool] = False,\n        purity: Optional[float] = None,\n    ):\n\n        super().__init__()\n        self.sad_scores = sad_scores\n        self.scd_scores = scd_scores\n        if self.scd_scores == ""oracle"":\n            if self.sad_scores == ""oracle"":\n                self.speech_turn_segmentation = OracleSpeechTurnSegmentation()\n            else:\n                msg = (\n                    f""Both sad_scores and scd_scores should be set to \'oracle\' ""\n                    f""for oracle speech turn segmentation, ""\n                    f""got {self.sad_scores} and {self.scd_scores}, respectively.""\n                )\n                raise ValueError(msg)\n        else:\n            self.speech_turn_segmentation = SpeechTurnSegmentation(\n                sad_scores=self.sad_scores, scd_scores=self.scd_scores\n            )\n        self.evaluation_only = evaluation_only\n        self.purity = purity\n\n        self.min_duration = Uniform(0, 10)\n\n        self.embedding = embedding\n        self.metric = metric\n        self.method = method\n        self.speech_turn_clustering = SpeechTurnClustering(\n            embedding=self.embedding, metric=self.metric, method=self.method\n        )\n\n        self.speech_turn_assignment = SpeechTurnClosestAssignment(\n            embedding=self.embedding, metric=self.metric\n        )\n\n    def __call__(self, current_file: dict) -> Annotation:\n        """"""Apply speaker diarization\n\n        Parameters\n        ----------\n        current_file : `dict`\n            File as provided by a pyannote.database protocol.\n\n        Returns\n        -------\n        hypothesis : `pyannote.core.Annotation`\n            Speaker diarization output.\n        """"""\n\n        # segmentation into speech turns\n        speech_turns = self.speech_turn_segmentation(current_file)\n\n        # some files are only partially annotated and therefore one cannot\n        # evaluate speaker diarization results on the whole file.\n        # this option simply avoids trying to cluster those\n        # (potentially messy) un-annotated refions by focusing only on\n        # speech turns contained in the annotated regions.\n        if self.evaluation_only:\n            annotated = get_annotated(current_file)\n            speech_turns = speech_turns.crop(annotated, mode=""intersection"")\n\n        # in case there is one speech turn or less, there is no need to apply\n        # any kind of clustering approach.\n        if len(speech_turns) < 2:\n            return speech_turns\n\n        # split short/long speech turns. the idea is to first cluster long\n        # speech turns (i.e. those for which we can trust embeddings) and then\n        # assign each speech turn to the closest cluster.\n        long_speech_turns = speech_turns.empty()\n        shrt_speech_turns = speech_turns.empty()\n        for segment, track, label in speech_turns.itertracks(yield_label=True):\n            if segment.duration < self.min_duration:\n                shrt_speech_turns[segment, track] = label\n            else:\n                long_speech_turns[segment, track] = label\n\n        # in case there are no long speech turn to cluster, we return the\n        # original speech turns (= shrt_speech_turns)\n        if len(long_speech_turns) < 1:\n            return speech_turns\n\n        # first: cluster long speech turns\n        long_speech_turns = self.speech_turn_clustering(current_file, long_speech_turns)\n\n        # then: assign short speech turns to clusters\n        long_speech_turns.rename_labels(generator=""string"", copy=False)\n\n        if len(shrt_speech_turns) > 0:\n            shrt_speech_turns.rename_labels(generator=""int"", copy=False)\n            shrt_speech_turns = self.speech_turn_assignment(\n                current_file, shrt_speech_turns, long_speech_turns\n            )\n        # merge short/long speech turns\n        return long_speech_turns.update(shrt_speech_turns, copy=False).support(\n            collar=0.0\n        )\n\n        # TODO. add overlap detection\n        # TODO. add overlap-aware resegmentation\n\n    def loss(self, current_file: dict, hypothesis: Annotation) -> float:\n        """"""Compute (1 - coverage) at target purity\n\n        If purity < target, return 1 + (1 - purity)\n\n        Parameters\n        ----------\n        current_file : `dict`\n            File as provided by a pyannote.database protocol.\n        hypothesis : `pyannote.core.Annotation`\n            Speech turns.\n\n        Returns\n        -------\n        loss : `float`\n            1. - cluster coverage.\n        """"""\n\n        metric = DiarizationPurityCoverageFMeasure()\n        reference = current_file[""annotation""]\n        uem = get_annotated(current_file)\n        f_measure = metric(reference, hypothesis, uem=uem)\n        purity, coverage, _ = metric.compute_metrics()\n        if purity > self.purity:\n            return 1.0 - coverage\n        else:\n            return 1.0 + (1.0 - purity)\n\n    def get_metric(self) -> GreedyDiarizationErrorRate:\n        """"""Return new instance of diarization error rate metric""""""\n\n        # defaults to optimizing diarization error rate\n        if self.purity is None:\n            return GreedyDiarizationErrorRate(collar=0.0, skip_overlap=False)\n\n        # fallbacks to using self.loss(...)\n        raise NotImplementedError()\n\n\nclass Yin2018(SpeakerDiarization):\n    """"""Speaker diarization pipeline introduced in Yin et al., 2018\n\n    Ruiqing Yin, Herv\xc3\xa9 Bredin, and Claude Barras. ""Neural speech turn\n    segmentation and affinity propagation for speaker diarization"".\n    Interspeech 2018.\n\n    Parameters\n    ----------\n    sad_scores : Text or Path or \'oracle\', optional\n    scd_scores : Text or Path, optional\n        Describes how raw speech activity and speaker change detection scores\n        should be obtained. It can be either the name of a torch.hub model, or\n        the path to the output of the validation step of a model trained\n        locally, or the path to scores precomputed on disk. Defaults to\n        ""@sad_scores"" and ""@scd_scores"" respectively, indicating that protocol\n        files provide the scores in the corresponding ""sad_scores"" and\n        ""scd_scores"" keys. Use \'oracle\' to assume perfect speech activity detection.\n        Path to precomputed speaker change detection scores\n    embedding : Text or Path, optional\n        Describes how raw speaker embeddings should be obtained. It can be\n        either the name of a torch.hub model, or the path to the output of the\n        validation step of a model trained locally, or the path to embeddings\n        precomputed on disk. Defaults to ""@emb"" that indicates that protocol\n        files provide the embeddings in the ""emb"" key.\n    metric : {\'euclidean\', \'cosine\', \'angular\'}, optional\n        Metric used for comparing embeddings. Defaults to \'cosine\'.\n    evaluation_only : `bool`\n        Only process the evaluated regions. Default to False.\n    """"""\n\n    def __init__(\n        self,\n        sad_scores: Union[Text, Path] = None,\n        scd_scores: Union[Text, Path] = None,\n        embedding: Union[Text, Path] = None,\n        metric: Optional[str] = ""cosine"",\n        evaluation_only: Optional[bool] = False,\n    ):\n\n        super().__init__(\n            sad_scores=sad_scores,\n            scd_scores=scd_scores,\n            embedding=embedding,\n            metric=metric,\n            method=""affinity_propagation"",\n            evaluation_only=evaluation_only,\n        )\n\n        self.freeze(\n            {\n                ""min_duration"": 0.0,\n                ""speech_turn_segmentation"": {\n                    ""speech_activity_detection"": {\n                        ""min_duration_on"": 0.0,\n                        ""min_duration_off"": 0.0,\n                        ""pad_onset"": 0.0,\n                        ""pad_offset"": 0.0,\n                    }\n                },\n            }\n        )\n'"
pyannote/audio/pipeline/speech_activity_detection.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2018-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n""""""Speech activity detection pipelines""""""\n\nfrom typing import Union\nimport numpy as np\nimport warnings\n\nfrom pyannote.pipeline import Pipeline\nfrom pyannote.pipeline.parameter import Uniform\n\nfrom pyannote.core import Annotation\nfrom pyannote.core import SlidingWindowFeature\n\nfrom pyannote.audio.utils.signal import Binarize\nfrom pyannote.audio.features import Precomputed\n\nfrom pyannote.metrics.detection import DetectionErrorRate\nfrom pyannote.metrics.detection import DetectionPrecisionRecallFMeasure\nfrom pyannote.audio.features.wrapper import Wrapper, Wrappable\n\n\nclass OracleSpeechActivityDetection(Pipeline):\n    """"""Oracle speech activity detection""""""\n\n    def __call__(self, current_file: dict) -> Annotation:\n        """"""Return groundtruth speech activity detection\n\n        Parameter\n        ---------\n        current_file : `dict`\n            Dictionary as provided by `pyannote.database`.\n\n        Returns\n        -------\n        hypothesis : `pyannote.core.Annotation`\n            Speech regions\n        """"""\n\n        speech = current_file[""annotation""].get_timeline().support()\n        return speech.to_annotation(generator=""string"", modality=""speech"")\n\n\nclass SpeechActivityDetection(Pipeline):\n    """"""Speech activity detection pipeline\n\n    Parameters\n    ----------\n    scores : Wrappable, optional\n        Describes how raw speech activity detection scores should be obtained.\n        See pyannote.audio.features.wrapper.Wrapper documentation for details.\n        Defaults to ""@sad_scores"" that indicates that protocol files provide\n        the scores in the ""sad_scores"" key.\n    fscore : bool, optional\n        Optimize (precision/recall) fscore. Defaults to optimizing detection\n        error rate.\n\n    Hyper-parameters\n    ----------------\n    onset, offset : `float`\n        Onset/offset detection thresholds\n    min_duration_on, min_duration_off : `float`\n        Minimum duration in either state (speech or not)\n    pad_onset, pad_offset : `float`\n        Padding duration.\n    """"""\n\n    def __init__(self, scores: Wrappable = None, fscore: bool = False):\n        super().__init__()\n\n        if scores is None:\n            scores = ""@sad_scores""\n        self.scores = scores\n        self._scores = Wrapper(self.scores)\n\n        self.fscore = fscore\n\n        # hyper-parameters\n        self.onset = Uniform(0.0, 1.0)\n        self.offset = Uniform(0.0, 1.0)\n        self.min_duration_on = Uniform(0.0, 2.0)\n        self.min_duration_off = Uniform(0.0, 2.0)\n        self.pad_onset = Uniform(-1.0, 1.0)\n        self.pad_offset = Uniform(-1.0, 1.0)\n\n    def initialize(self):\n        """"""Initialize pipeline with current set of parameters""""""\n\n        self._binarize = Binarize(\n            onset=self.onset,\n            offset=self.offset,\n            min_duration_on=self.min_duration_on,\n            min_duration_off=self.min_duration_off,\n            pad_onset=self.pad_onset,\n            pad_offset=self.pad_offset,\n        )\n\n    def __call__(self, current_file: dict) -> Annotation:\n        """"""Apply speech activity detection\n\n        Parameters\n        ----------\n        current_file : `dict`\n            File as provided by a pyannote.database protocol. May contain a\n            \'sad_scores\' key providing precomputed scores.\n\n        Returns\n        -------\n        speech : `pyannote.core.Annotation`\n            Speech regions.\n        """"""\n\n        sad_scores = self._scores(current_file)\n\n        # if this check has not been done yet, do it once and for all\n        if not hasattr(self, ""log_scale_""):\n            # heuristic to determine whether scores are log-scaled\n            if np.nanmean(sad_scores.data) < 0:\n                self.log_scale_ = True\n            else:\n                self.log_scale_ = False\n\n        data = np.exp(sad_scores.data) if self.log_scale_ else sad_scores.data\n\n        # speech vs. non-speech\n        if data.shape[1] > 1:\n            speech_prob = SlidingWindowFeature(\n                1.0 - data[:, 0], sad_scores.sliding_window\n            )\n        else:\n            speech_prob = SlidingWindowFeature(data, sad_scores.sliding_window)\n\n        speech = self._binarize.apply(speech_prob)\n\n        speech.uri = current_file.get(""uri"", None)\n        return speech.to_annotation(generator=""string"", modality=""speech"")\n\n    def get_metric(\n        self, parallel=False\n    ) -> Union[DetectionErrorRate, DetectionPrecisionRecallFMeasure]:\n        """"""Return new instance of detection metric""""""\n\n        if self.fscore:\n            return DetectionPrecisionRecallFMeasure(\n                collar=0.0, skip_overlap=False, parallel=parallel\n            )\n        else:\n            return DetectionErrorRate(collar=0.0, skip_overlap=False, parallel=parallel)\n'"
pyannote/audio/pipeline/speech_turn_assignment.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2017-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n\nfrom typing import Optional\nimport numpy as np\n\nfrom pyannote.pipeline import Pipeline\nfrom pyannote.pipeline.blocks.classification import ClosestAssignment\nfrom pyannote.core import Annotation\nfrom .utils import assert_int_labels\nfrom .utils import assert_string_labels\nfrom ..features import Precomputed\n\nfrom pyannote.audio.features.wrapper import Wrapper, Wrappable\n\n\nclass SpeechTurnClosestAssignment(Pipeline):\n    """"""Assign speech turn to closest cluster\n\n    Parameters\n    ----------\n    embedding : Wrappable, optional\n        Describes how raw speaker embeddings should be obtained.\n        See pyannote.audio.features.wrapper.Wrapper documentation for details.\n        Defaults to ""@emb"" that indicates that protocol files provide\n        the scores in the ""emb"" key.\n    metric : {\'euclidean\', \'cosine\', \'angular\'}, optional\n        Metric used for comparing embeddings. Defaults to \'cosine\'.\n    """"""\n\n    def __init__(self, embedding: Wrappable = None, metric: Optional[str] = ""cosine""):\n        super().__init__()\n\n        if embedding is None:\n            embedding = ""@emb""\n        self.embedding = embedding\n        self._embedding = Wrapper(self.embedding)\n\n        self.metric = metric\n\n        self.closest_assignment = ClosestAssignment(metric=self.metric)\n\n    def __call__(\n        self, current_file: dict, speech_turns: Annotation, targets: Annotation\n    ) -> Annotation:\n        """"""Assign each speech turn to closest target (if close enough)\n\n        Parameters\n        ----------\n        current_file : `dict`\n            File as provided by a pyannote.database protocol.\n        speech_turns : `Annotation`\n            Speech turns. Should only contain `int` labels.\n        targets : `Annotation`\n            Targets. Should only contain `str` labels.\n\n        Returns\n        -------\n        assigned : `Annotation`\n            Assigned speech turns.\n        """"""\n\n        assert_string_labels(targets, ""targets"")\n        assert_int_labels(speech_turns, ""speech_turns"")\n\n        embedding = self._embedding(current_file)\n\n        # gather targets embedding\n        labels = targets.labels()\n        X_targets, targets_labels = [], []\n        for l, label in enumerate(labels):\n\n            timeline = targets.label_timeline(label, copy=False)\n\n            # be more and more permissive until we have\n            # at least one embedding for current speech turn\n            for mode in [""strict"", ""center"", ""loose""]:\n                x = embedding.crop(timeline, mode=mode)\n                if len(x) > 0:\n                    break\n\n            # skip labels so small we don\'t have any embedding for it\n            if len(x) < 1:\n                continue\n\n            targets_labels.append(label)\n            X_targets.append(np.mean(x, axis=0))\n\n        # gather speech turns embedding\n        labels = speech_turns.labels()\n        X, assigned_labels, skipped_labels = [], [], []\n        for l, label in enumerate(labels):\n\n            timeline = speech_turns.label_timeline(label, copy=False)\n\n            # be more and more permissive until we have\n            # at least one embedding for current speech turn\n            for mode in [""strict"", ""center"", ""loose""]:\n                x = embedding.crop(timeline, mode=mode)\n                if len(x) > 0:\n                    break\n\n            # skip labels so small we don\'t have any embedding for it\n            if len(x) < 1:\n                skipped_labels.append(label)\n                continue\n\n            assigned_labels.append(label)\n            X.append(np.mean(x, axis=0))\n\n        # assign speech turns to closest class\n        assignments = self.closest_assignment(np.vstack(X_targets), np.vstack(X))\n        mapping = {\n            label: targets_labels[k]\n            for label, k in zip(assigned_labels, assignments)\n            if not k < 0\n        }\n        return speech_turns.rename_labels(mapping=mapping)\n'"
pyannote/audio/pipeline/speech_turn_clustering.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2018-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\nimport numpy as np\nfrom typing import Optional\n\nfrom pyannote.core import Annotation\nfrom pyannote.core import Timeline\nfrom pyannote.core.utils.numpy import one_hot_decoding\nfrom pyannote.pipeline import Pipeline\nfrom pyannote.audio.features import Precomputed\nfrom pyannote.pipeline.blocks.clustering import HierarchicalAgglomerativeClustering\nfrom pyannote.pipeline.blocks.clustering import AffinityPropagationClustering\nfrom .utils import assert_string_labels\n\nfrom pyannote.audio.features.wrapper import Wrapper, Wrappable\n\n\nclass SpeechTurnClustering(Pipeline):\n    """"""Speech turn clustering\n\n    Parameters\n    ----------\n    embedding : Wrappable, optional\n        Describes how raw speaker embeddings should be obtained.\n        See pyannote.audio.features.wrapper.Wrapper documentation for details.\n        Defaults to ""@emb"" that indicates that protocol files provide\n        the scores in the ""emb"" key.\n    metric : {\'euclidean\', \'cosine\', \'angular\'}, optional\n        Metric used for comparing embeddings. Defaults to \'cosine\'.\n    method : {\'pool\', \'affinity_propagation\'}\n        Set method used for clustering. ""pool"" stands for agglomerative\n        hierarchical clustering with embedding pooling. ""affinity_propagation""\n        is for clustering based on affinity propagation. Defaults to ""pool"".\n    window_wise : `bool`, optional\n        Set `window_wise` to True to apply clustering on embedding extracted\n        using the built-in sliding window. Defaults to apply clustering at\n        speech turn level (one average embedding per speech turn).\n    """"""\n\n    def __init__(\n        self,\n        embedding: Wrappable = None,\n        metric: Optional[str] = ""cosine"",\n        method: Optional[str] = ""pool"",\n        window_wise: Optional[bool] = False,\n    ):\n        super().__init__()\n\n        if embedding is None:\n            embedding = ""@emb""\n        self.embedding = embedding\n        self._embedding = Wrapper(self.embedding)\n\n        self.metric = metric\n        self.method = method\n\n        if self.method == ""affinity_propagation"":\n            self.clustering = AffinityPropagationClustering(metric=self.metric)\n\n            # sklearn documentation: Preferences for each point - points with\n            # larger values of preferences are more likely to be chosen as\n            # exemplars. The number of exemplars, ie of clusters, is influenced by\n            # the input preferences value. If the preferences are not passed as\n            # arguments, they will be set to the median of the input similarities.\n\n            # NOTE one could set the preference value of each speech turn\n            # according to their duration. longer speech turns are expected to\n            # have more accurate embeddings, therefore should be prefered for\n            # exemplars\n\n        else:\n            self.clustering = HierarchicalAgglomerativeClustering(\n                method=self.method, metric=self.metric, use_threshold=True\n            )\n\n        self.window_wise = window_wise\n\n    def _window_level(self, current_file: dict, speech_regions: Timeline) -> Annotation:\n        """"""Apply clustering at window level\n\n        Parameters\n        ----------\n        current_file : `dict`\n            File as provided by a pyannote.database protocol.\n        speech_regions : `Timeline`\n            Speech regions.\n\n        Returns\n        -------\n        hypothesis : `pyannote.core.Annotation`\n            Clustering result.\n        """"""\n\n        # load embeddings\n        embedding = self._embedding(current_file)\n        window = embedding.sliding_window\n\n        # extract and stack embeddings of speech regions\n        X = np.vstack(\n            [\n                embedding.crop(segment, mode=""center"", fixed=segment.duration)\n                for segment in speech_regions\n            ]\n        )\n\n        # apply clustering\n        y_pred = self.clustering(X)\n\n        # reconstruct\n        y = np.zeros(len(embedding), dtype=np.int8)\n\n        # n = total number of ""speech"" embeddings\n        # s_pred = current position in y_pred\n        s_pred, n = 0, len(y_pred)\n\n        for segment in speech_regions:\n\n            # get indices of current speech segment\n            ((s, e),) = window.crop(\n                segment, mode=""center"", fixed=segment.duration, return_ranges=True\n            )\n\n            # hack for the very last segment that might overflow by 1\n            e_pred = min(s_pred + e - s, n - 1)\n            e = s + (e_pred - s_pred)\n\n            # assign y_pred to the corresponding speech regions\n            y[s:e] = y_pred[s_pred:e_pred]\n\n            # increment current position in y_red\n            s_pred += e - s\n\n        # reconstruct hypothesis\n        return one_hot_decoding(y, window)\n\n    def _turn_level(self, current_file: dict, speech_turns: Annotation) -> Annotation:\n        """"""Apply clustering at speech turn level\n\n        Parameters\n        ----------\n        current_file : `dict`\n            File as provided by a pyannote.database protocol.\n        speech_turns : `Annotation`\n            Speech turns. Should only contain `str` labels.\n\n        Returns\n        -------\n        hypothesis : `pyannote.core.Annotation`\n            Clustering result.\n        """"""\n\n        assert_string_labels(speech_turns, ""speech_turns"")\n\n        embedding = self._embedding(current_file)\n\n        labels = speech_turns.labels()\n        X, clustered_labels, skipped_labels = [], [], []\n        for l, label in enumerate(labels):\n\n            timeline = speech_turns.label_timeline(label, copy=False)\n\n            # be more and more permissive until we have\n            # at least one embedding for current speech turn\n            for mode in [""strict"", ""center"", ""loose""]:\n                x = embedding.crop(timeline, mode=mode)\n                if len(x) > 0:\n                    break\n\n            # skip labels so small we don\'t have any embedding for it\n            if len(x) < 1:\n                skipped_labels.append(label)\n                continue\n\n            clustered_labels.append(label)\n            X.append(np.mean(x, axis=0))\n\n        # apply clustering of label embeddings\n        clusters = self.clustering(np.vstack(X))\n\n        # map each clustered label to its cluster (between 1 and N_CLUSTERS)\n        mapping = {label: k for label, k in zip(clustered_labels, clusters)}\n\n        # map each skipped label to its own cluster\n        # (between -1 and -N_SKIPPED_LABELS)\n        for l, label in enumerate(skipped_labels):\n            mapping[label] = -(l + 1)\n\n        # do the actual mapping\n        return speech_turns.rename_labels(mapping=mapping)\n\n    def __call__(\n        self, current_file: dict, speech_turns: Optional[Annotation] = None\n    ) -> Annotation:\n        """"""Apply speech turn clustering\n\n        Parameters\n        ----------\n        current_file : `dict`\n            File as provided by a pyannote.database protocol.\n        speech_turns : `Annotation`, optional\n            Speech turns. Should only contain `str` labels.\n            Defaults to `current_file[\'speech_turns\']`.\n\n        Returns\n        -------\n        speech_turns : `pyannote.core.Annotation`\n            Clustered speech turns (or windows in case `window_wise` is True)\n        """"""\n\n        if speech_turns is None:\n            speech_turns = current_file[""speech_turns""]\n\n        if self.window_wise:\n            return self._window_level(\n                current_file, speech_turns.get_timeline().support()\n            )\n\n        return self._turn_level(current_file, speech_turns)\n'"
pyannote/audio/pipeline/speech_turn_segmentation.py,1,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2018-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\nfrom typing import Optional\nfrom typing import Union\nfrom typing import Text\nfrom pathlib import Path\n\nfrom pyannote.core import Annotation\nfrom pyannote.pipeline import Pipeline\nfrom .speaker_change_detection import SpeakerChangeDetection\nfrom .speech_activity_detection import SpeechActivityDetection\nfrom .speech_activity_detection import OracleSpeechActivityDetection\n\nfrom pyannote.database import get_annotated\nfrom pyannote.metrics.diarization import DiarizationPurityCoverageFMeasure\n\n\nclass OracleSpeechTurnSegmentation(Pipeline):\n    """"""Oracle segmentation""""""\n\n    def __call__(self, current_file: dict) -> Annotation:\n        """"""Return groundtruth segmentation\n\n        Parameter\n        ---------\n        current_file : `dict`\n            Dictionary as provided by `pyannote.database`.\n\n        Returns\n        -------\n        hypothesis : `pyannote.core.Annotation`\n            Speech turns\n        """"""\n\n        return current_file[""annotation""].relabel_tracks(generator=""string"")\n\n\nclass SpeechTurnSegmentation(Pipeline):\n    """"""Combine speech activity and speaker change detections for segmentation\n\n    Parameters\n    ----------\n    sad_scores : Text or Path or \'oracle\', optional\n    scd_scores : Text or Path, optional\n        Describes how raw speech activity and speaker change detection scores\n        should be obtained. It can be either the name of a torch.hub model, or\n        the path to the output of the validation step of a model trained\n        locally, or the path to scores precomputed on disk. Defaults to\n        ""@sad_scores"" and ""@scd_scores"" respectively, indicating that protocol\n        files provide the scores in the corresponding ""sad_scores"" and\n        ""scd_scores"" keys. Use \'oracle\' to assume perfect speech activity detection.\n        Path to precomputed speaker change detection scores\n    non_speech : `bool`\n        Mark non-speech regions as speaker change. Defaults to True.\n    purity : `float`, optional\n        Target purity. Defaults to 0.95\n    """"""\n\n    def __init__(\n        self,\n        sad_scores: Union[Text, Path] = None,\n        scd_scores: Union[Text, Path] = None,\n        non_speech: Optional[bool] = True,\n        purity: Optional[float] = 0.95,\n    ):\n        super().__init__()\n\n        self.sad_scores = sad_scores\n        if self.sad_scores == ""oracle"":\n            self.speech_activity_detection = OracleSpeechActivityDetection()\n        else:\n            self.speech_activity_detection = SpeechActivityDetection(\n                scores=self.sad_scores\n            )\n\n        self.scd_scores = scd_scores\n        self.speaker_change_detection = SpeakerChangeDetection(scores=self.scd_scores)\n\n        self.non_speech = non_speech\n        self.purity = purity\n\n    def __call__(self, current_file: dict) -> Annotation:\n        """"""Apply speech turn segmentation\n\n        Parameter\n        ---------\n        current_file : `dict`\n            Dictionary as provided by `pyannote.database`.\n\n        Returns\n        -------\n        hypothesis : `pyannote.core.Annotation`\n            Hypothesized speech turns\n        """"""\n\n        # speech regions\n        sad = self.speech_activity_detection(current_file).get_timeline()\n\n        scd = self.speaker_change_detection(current_file)\n        speech_turns = scd.crop(sad, mode=""intersection"")\n\n        # at this point, consecutive speech turns separated by non-speech\n        # might be assigned the same label (because scd might have missed\n        # speech/non-speech boundaries)\n\n        # assign one unique label per speech turn\n        if self.non_speech:\n            speech_turns = speech_turns.relabel_tracks(generator=""string"")\n\n        speech_turns.modality = ""speaker""\n        return speech_turns\n\n    def loss(self, current_file: dict, hypothesis: Annotation) -> float:\n        """"""Compute (1 - coverage) at target purity\n\n        If purity < target, return 1 + (1 - purity)\n\n        Parameters\n        ----------\n        current_file : `dict`\n            File as provided by a pyannote.database protocol.\n        hypothesis : `pyannote.core.Annotation`\n            Speech turns.\n\n        Returns\n        -------\n        error : `float`\n            1. - cluster coverage.\n        """"""\n\n        metric = DiarizationPurityCoverageFMeasure()\n        reference = current_file[""annotation""]\n        uem = get_annotated(current_file)\n        f_measure = metric(reference, hypothesis, uem=uem)\n        purity, coverage, _ = metric.compute_metrics()\n        if purity > self.purity:\n            return 1.0 - coverage\n        else:\n            return 1.0 + (1.0 - purity)\n'"
pyannote/audio/pipeline/utils.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2017-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n\nimport yaml\nfrom pathlib import Path\nfrom pyannote.core import Annotation\nfrom pyannote.pipeline import Pipeline\nfrom pyannote.core.utils.helper import get_class_by_name\n\n\ndef assert_string_labels(annotation: Annotation, name: str):\n    """"""Check that annotation only contains string labels\n\n    Parameters\n    ----------\n    annotation : `pyannote.core.Annotation`\n        Annotation.\n    name : `str`\n        Name of the annotation (used for user feedback in case of failure)\n    """"""\n    if any(not isinstance(label, str) for label in annotation.labels()):\n        msg = f""{name} must contain `str` labels only.""\n        raise ValueError(msg)\n\n\ndef assert_int_labels(annotation: Annotation, name: str):\n    """"""Check that annotation only contains integer labels\n\n    Parameters\n    ----------\n    annotation : `pyannote.core.Annotation`\n        Annotation.\n    name : `str`\n        Name of the annotation (used for user feedback in case of failure)\n    """"""\n    if any(not isinstance(label, int) for label in annotation.labels()):\n        msg = f""{name} must contain `int` labels only.""\n        raise ValueError(msg)\n\n\ndef load_pretrained_pipeline(train_dir: Path) -> Pipeline:\n    """"""Load pretrained pipeline\n\n    Parameters\n    ----------\n    train_dir : Path\n        Path to training directory (i.e. the one that contains `params.yml`\n        created by calling `pyannote-pipeline train ...`)\n\n    Returns\n    -------\n    pipeline : Pipeline\n        Pretrained pipeline\n    """"""\n\n    train_dir = Path(train_dir).expanduser().resolve(strict=True)\n\n    config_yml = train_dir.parents[1] / ""config.yml""\n    with open(config_yml, ""r"") as fp:\n        config = yaml.load(fp, Loader=yaml.SafeLoader)\n\n    pipeline_name = config[""pipeline""][""name""]\n    Klass = get_class_by_name(\n        pipeline_name, default_module_name=""pyannote.audio.pipeline""\n    )\n    pipeline = Klass(**config[""pipeline""].get(""params"", {}))\n\n    return pipeline.load_params(train_dir / ""params.yml"")\n'"
pyannote/audio/preprocessors/__init__.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n\nfrom .speech_activity_detection import RemoveNonSpeech\n'"
pyannote/audio/preprocessors/speech_activity_detection.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\nfrom pathlib import Path\nfrom pyannote.database.protocol.protocol import ProtocolFile\nfrom pyannote.core import Annotation\nfrom pyannote.database.util import load_rttm\n\n\nclass RemoveNonSpeech:\n    """"""Remove (precomputed) non-speech regions from annotation\n\n    This is useful for speaker verification databases where files contain only\n    one speaker but actual speech regions are not annotated.\n\n    Parameters\n    ----------\n    sad_rttm : Path\n        Path to RTTM file containing speech activity detection result.\n\n    Usage\n    -----\n    1. Prepare a file containing speech activity detection results in RTTM\n       format (here /path/to/sad.rttm)\n    2. Add a ""preprocessors"" section in your ""config.yml"" configuration file\n        --------------------------------------------------------------------\n        preprocessors:\n            annotation:\n                name: pyannote.audio.preprocessors.RemoveNonSpeech\n                params:\n                    sad_rttm: /path/to/sad.rttm\n        --------------------------------------------------------------------\n    3. Enjoy your updated ""annotation"" key (where non-speech regions are\n       removed).\n    """"""\n\n    def __init__(self, sad_rttm: Path = None):\n        self.sad_rttm = sad_rttm\n        self.sad_ = load_rttm(self.sad_rttm)\n\n    def __call__(self, current_file: ProtocolFile) -> Annotation:\n\n        # get speech regions as Annotation instances\n        speech_regions = self.sad_.get(current_file[""uri""], Annotation())\n\n        # remove non-speech regions from current annotation\n        # aka only keep speech regions\n        try:\n            annotation = current_file[""annotation""]\n            return annotation.crop(speech_regions.get_timeline())\n\n        # this haapens when current_file has no ""annotation"" key\n        # (e.g. for file1 and file2 in speaker verification trials)\n        # in that case, we return speech regions directly\n        except KeyError:\n            return speech_regions\n'"
pyannote/audio/train/__init__.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2018-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n'"
pyannote/audio/train/callback.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2019 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n\nclass Callback:\n    def on_train_start(self, trainer):\n        pass\n\n    def on_epoch_start(self, trainer):\n        pass\n\n    def on_batch_start(self, trainer, batch):\n        return batch\n\n    def on_batch_end(self, trainer, loss):\n        """"""Called after parameters update\n\n        Parameters\n        ----------\n        trainer : `Trainer`\n        loss : `dict`\n        """"""\n        pass\n\n    def on_epoch_end(self, trainer):\n        pass\n\n    def on_train_end(self, trainer):\n        pass\n\n\nclass Debug(Callback):\n    def on_train_start(self, trainer):\n        print("""")\n        print(""-- Debug callback -----------------"")\n        print(""on_train_start"")\n        print(""-----------------------------------"")\n        print("""")\n\n    def on_epoch_start(self, trainer):\n        print("""")\n        print(""-- Debug callback -----------------"")\n        print(""on_epoch_start"")\n        print(""-----------------------------------"")\n        print("""")\n\n    def on_batch_start(self, trainer, batch):\n        print("""")\n        print(""-- Debug callback -----------------"")\n        print(""on_batch_start"")\n        print(""-----------------------------------"")\n        print("""")\n        return batch\n\n    def on_batch_end(self, trainer, loss):\n        print("""")\n        print(""-- Debug callback -----------------"")\n        print(""on_batch_end"")\n        print(""-----------------------------------"")\n        print("""")\n\n    def on_epoch_end(self, trainer):\n        print("""")\n        print(""-- Debug callback -----------------"")\n        print(""on_epoch_end"")\n        print(""-----------------------------------"")\n        print("""")\n\n    def on_train_end(self, trainer):\n        print("""")\n        print(""-- Debug callback -----------------"")\n        print(""on_train_end"")\n        print(""-----------------------------------"")\n        print("""")\n\n\nclass Callbacks:\n    def __init__(self, callbacks):\n        super().__init__()\n        self.callbacks = callbacks\n\n    def on_train_start(self, trainer):\n        for callback in self.callbacks:\n            callback.on_train_start(trainer)\n\n    def on_epoch_start(self, trainer):\n        for callback in self.callbacks:\n            callback.on_epoch_start(trainer)\n\n    def on_batch_start(self, trainer, batch):\n        for callback in self.callbacks:\n            batch = callback.on_batch_start(trainer, batch)\n        return batch\n\n    def on_batch_end(self, trainer, loss):\n        """"""Called after parameters update\n\n        Parameters\n        ----------\n        trainer : `Trainer`\n        loss : `dict`\n        """"""\n        trainer.on_batch_end(loss)\n        for callback in self.callbacks:\n            callback.on_batch_end(trainer, loss)\n\n    def on_epoch_end(self, trainer):\n        trainer.on_epoch_end()\n        for callback in self.callbacks:\n            callback.on_epoch_end(trainer)\n\n    def on_train_end(self, trainer):\n        trainer.on_train_end()\n        for callback in self.callbacks:\n            callback.on_train_end(trainer)\n'"
pyannote/audio/train/generator.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2019-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n""""""\nTODO\n""""""\n\nfrom abc import ABCMeta, abstractmethod\nfrom typing import Iterator\n\n# TODO: move this to pyannote.database\nfrom typing_extensions import Literal\n\nSubset = Literal[""train"", ""development"", ""test""]\n\nfrom pyannote.audio.features.base import FeatureExtraction\nfrom pyannote.database.protocol.protocol import Protocol\n\nimport warnings\nimport numpy as np\nimport pescador\n\n\nclass BatchGenerator(metaclass=ABCMeta):\n    """"""Batch generator base class\n\n    Parameters\n    ----------\n    feature_extraction : `FeatureExtraction`\n    protocol : `Protocol`\n        pyannote.database protocol used by the generator.\n    subset : {\'train\', \'development\', \'test\'}, optional\n        Subset used by the generator. Defaults to \'train\'.\n    """"""\n\n    @abstractmethod\n    def __init__(\n        self,\n        feature_extraction: FeatureExtraction,\n        protocol: Protocol,\n        subset: Subset = ""train"",\n        **kwargs,\n    ):\n        pass\n\n    @property\n    @abstractmethod\n    def specifications(self) -> dict:\n        """"""Generator specifications\n\n        Returns\n        -------\n        specifications : `dict`\n            Dictionary describing generator specifications.\n        """"""\n        pass\n\n    @property\n    @abstractmethod\n    def batches_per_epoch(self) -> int:\n        """"""Number of batches per epoch\n\n        Returns\n        -------\n        n_batches : `int`\n            Number of batches to make an epoch.\n        """"""\n        pass\n\n    @abstractmethod\n    def samples(self) -> Iterator:\n        pass\n\n    def __call__(self) -> Iterator:\n        batches = pescador.maps.buffer_stream(\n            self.samples(), self.batch_size, partial=False, axis=None\n        )\n\n        while True:\n            next_batch = next(batches)\n            # HACK in some rare cases, .samples() yields samples\n            # HACK with different length leading to batch being of\n            # HACK type ""object"". for now, we simply discard those\n            # HACK buggy batches.\n            # TODO fix the problem upstream in .samples()\n            if any(batch.dtype == np.object_ for batch in next_batch.values()):\n                msg = f""Skipping malformed batch.""\n                warnings.warn(msg)\n                continue\n            yield next_batch\n'"
pyannote/audio/train/logging.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2019 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\nimport time\nimport numpy as np\nfrom tqdm import tqdm\nfrom .callback import Callback\n\n\nclass Logging(Callback):\n    """"""Log loss and processing time to tensorboard and progress bar\n\n    Parameters\n    ----------\n    epochs : `int`, optional\n        Total number of epochs. Main progress bar will be prettier if provided.\n    verbosity : `int`, optional\n        Set level of verbosity.\n        Use 0 (default) to not show any progress bar.\n        Use 1 to show a progress bar updated at the end of each epoch.\n        Use 2 to add a second progress bar updated at the end of each batch.\n    """"""\n\n    def __init__(self, epochs: int = None, verbosity: int = 0):\n        super().__init__()\n        self.epochs = epochs\n        self.verbosity = verbosity\n        self.beta_ = 0.98\n\n    def on_train_start(self, trainer):\n        if self.verbosity > 0:\n            self.epochs_pbar_ = tqdm(\n                desc=f""Training"",\n                total=self.epochs,\n                leave=True,\n                ncols=80,\n                unit=""epoch"",\n                initial=trainer.epoch_,\n                position=0,\n            )\n\n    def on_epoch_start(self, trainer):\n\n        # time spent in batch generation\n        self.t_batch_ = list()\n        # time spent in forward/backward\n        self.t_model_ = list()\n\n        # loss moving average\n        self.n_batches_ = 0\n        self.loss_moving_avg_ = dict()\n\n        if self.verbosity > 0:\n            self.epochs_pbar_.update(1)\n\n        if self.verbosity > 1:\n            self.batches_pbar_ = tqdm(\n                desc=f""Epoch #{trainer.epoch_}"",\n                total=trainer.batches_per_epoch_,\n                leave=False,\n                ncols=80,\n                unit=""batch"",\n                position=1,\n            )\n\n        self.t_batch_end_ = time.time()\n\n    def on_batch_start(self, trainer, batch):\n\n        # mark time just before forward/backward\n        self.t_batch_start_ = time.time()\n\n        # time spent in batch generation\n        self.t_batch_.append(self.t_batch_start_ - self.t_batch_end_)\n\n        return batch\n\n    def on_batch_end(self, trainer, batch_loss):\n\n        # mark time just after forward/backward\n        self.t_batch_end_ = time.time()\n\n        # time spent in forward/backward\n        self.t_model_.append(self.t_batch_end_ - self.t_batch_start_)\n\n        self.n_batches_ += 1\n\n        self.loss = dict()\n        for key in batch_loss:\n            if not key.startswith(""loss""):\n                continue\n            loss = batch_loss[key].detach().cpu().item()\n            self.loss_moving_avg_[key] = (\n                self.beta_ * self.loss_moving_avg_.setdefault(key, 0.0)\n                + (1 - self.beta_) * loss\n            )\n            self.loss[key] = self.loss_moving_avg_[key] / (\n                1 - self.beta_ ** self.n_batches_\n            )\n\n        if self.verbosity > 1:\n            self.batches_pbar_.set_postfix(ordered_dict=self.loss)\n            self.batches_pbar_.update(1)\n\n    def on_epoch_end(self, trainer):\n\n        for key, loss in self.loss.items():\n            trainer.tensorboard_.add_scalar(\n                f""train/{key}"", loss, global_step=trainer.epoch_\n            )\n\n        trainer.tensorboard_.add_histogram(\n            ""profiling/model"",\n            np.array(self.t_model_),\n            global_step=trainer.epoch_,\n            bins=""fd"",\n        )\n\n        trainer.tensorboard_.add_histogram(\n            ""profiling/batch"",\n            np.array(self.t_batch_),\n            global_step=trainer.epoch_,\n            bins=""fd"",\n        )\n'"
pyannote/audio/train/model.py,11,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2019-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n""""""Models\n\n## Parts\n\n>>> model.parts\n[""ff.1"", ""ff.2"", ""ff.3""]\n\n## Probes\n\n>>> model.probes = [""ff.1"", ""ff.2""]\n>>> output, probes = model(input)\n>>> ff1 = probes[""ff.1""]\n>>> ff2 = probes[""ff.2""]\n\n>>> del model.probes\n>>> output = model(input)\n\n## Freeze/unfreeze layers\n\n>>> model.freeze([""ff.1"", ""ff.2""])\n>>> model.unfreeze([""ff.2""])\n\n""""""\n\nfrom typing import Union\nfrom typing import List\nfrom typing import Text\nfrom typing import Tuple\nfrom typing import Dict\n\ntry:\n    from typing import Literal\nexcept ImportError as e:\n    from typing_extensions import Literal\nfrom typing import Callable\nfrom pyannote.core import SlidingWindow\nfrom pyannote.core import SlidingWindowFeature\n\nRESOLUTION_FRAME = ""frame""\nRESOLUTION_CHUNK = ""chunk""\nResolution = Union[SlidingWindow, Literal[RESOLUTION_FRAME, RESOLUTION_CHUNK]]\n\nALIGNMENT_CENTER = ""center""\nALIGNMENT_STRICT = ""strict""\nALIGNMENT_LOOSE = ""loose""\nAlignment = Literal[ALIGNMENT_CENTER, ALIGNMENT_STRICT, ALIGNMENT_LOOSE]\n\nfrom pyannote.audio.train.task import Task\nimport numpy as np\nimport pescador\nimport torch\nfrom torch.nn import Module\nfrom functools import partial\n\n\nclass Model(Module):\n    """"""Model\n\n    A `Model` is nothing but a `torch.nn.Module` instance with a bunch of\n    additional methods and properties specific to `pyannote.audio`.\n\n    It is expected to be instantiated with a unique `specifications` positional\n    argument describing the task addressed by the model, and a user-defined\n    number of keyword arguments describing the model architecture.\n\n    Parameters\n    ----------\n    specifications : `dict`\n        Task specifications.\n    **architecture_params : `dict`\n        Architecture hyper-parameters.\n    """"""\n\n    def __init__(self, specifications: dict, **architecture_params):\n        super().__init__()\n        self.specifications = specifications\n        self.resolution_ = self.get_resolution(self.task, **architecture_params)\n        self.alignment_ = self.get_alignment(self.task, **architecture_params)\n        self.init(**architecture_params)\n\n    def init(self, **architecture_params):\n        """"""Initialize model architecture\n\n        This method is called by Model.__init__ after attributes\n        \'specifications\', \'resolution_\', and \'alignment_\' have been set.\n\n        Parameters\n        ----------\n        **architecture_params : `dict`\n            Architecture hyper-parameters\n\n        """"""\n        msg = \'Method ""init"" must be overriden.\'\n        raise NotImplementedError(msg)\n\n    @property\n    def probes(self):\n        """"""Get list of probes""""""\n        return list(getattr(self, ""_probes"", []))\n\n    @probes.setter\n    def probes(self, names: List[Text]):\n        """"""Set list of probes\n\n        Parameters\n        ----------\n        names : list of string\n            Names of modules to probe.\n        """"""\n\n        for handle in getattr(self, ""handles_"", []):\n            handle.remove()\n\n        self._probes = []\n\n        if not names:\n            return\n\n        handles = []\n\n        def _init(module, input):\n            self.probed_ = dict()\n\n        handles.append(self.register_forward_pre_hook(_init))\n\n        def _append(name, module, input, output):\n            self.probed_[name] = output\n\n        for name, module in self.named_modules():\n            if name in names:\n                handles.append(module.register_forward_hook(partial(_append, name)))\n                self._probes.append(name)\n\n        def _return(module, input, output):\n            return output, self.probed_\n\n        handles.append(self.register_forward_hook(_return))\n\n        self.handles_ = handles\n\n    @probes.deleter\n    def probes(self):\n        """"""Remove all probes""""""\n        for handle in getattr(self, ""handles_"", []):\n            handle.remove()\n        self._probes = []\n\n    @property\n    def parts(self):\n        """"""Names of (freezable / probable) modules""""""\n        return [n for n, _ in self.named_modules()]\n\n    def freeze(self, names: List[Text]):\n        """"""Freeze parts of the model\n\n        Parameters\n        ----------\n        names : list of string\n            Names of modules to freeze.\n        """"""\n        for name, module in self.named_modules():\n            if name in names:\n                for parameter in module.parameters(recurse=True):\n                    parameter.requires_grad = False\n\n    def unfreeze(self, names: List[Text]):\n        """"""Unfreeze parts of the model\n\n        Parameters\n        ----------\n        names : list of string\n            Names of modules to unfreeze.\n        """"""\n\n        for name, module in self.named_modules():\n            if name in names:\n                for parameter in module.parameters(recurse=True):\n                    parameter.requires_grad = True\n\n    def forward(\n        self, sequences: torch.Tensor, **kwargs\n    ) -> Union[torch.Tensor, Tuple[torch.Tensor, Dict[Text, torch.Tensor]]]:\n        """"""TODO\n\n        Parameters\n        ----------\n        sequences : (batch_size, n_samples, n_features) `torch.Tensor`\n        **kwargs : `dict`\n\n        Returns\n        -------\n        output : (batch_size, ...) `torch.Tensor`\n        probes : dict, optional\n        """"""\n\n        # TODO\n        msg = ""...""\n        raise NotImplementedError(msg)\n\n    @property\n    def task(self) -> Task:\n        """"""Type of task addressed by the model\n\n        Shortcut for self.specifications[\'task\']\n        """"""\n        return self.specifications[""task""]\n\n    def get_resolution(self, task: Task, **architecture_params) -> Resolution:\n        """"""Get frame resolution\n\n        This method is called by `BatchGenerator` instances to determine how\n        target tensors should be built.\n\n        Depending on the task and the architecture, the output of a model will\n        have different resolution. The default behavior is to return\n        - `RESOLUTION_CHUNK` if the model returns just one output for the whole\n          input sequence\n        - `RESOLUTION_FRAME` if the model returns one output for each frame of\n          the input sequence\n\n        In case neither of these options is valid, this method needs to be\n        overriden to return a custom `SlidingWindow` instance.\n\n        Parameters\n        ----------\n        task : Task\n        **architecture_params\n            Parameters used for instantiating the model architecture.\n\n        Returns\n        -------\n        resolution : `Resolution`\n            - `RESOLUTION_CHUNK` if the model returns one single output for the\n              whole input sequence;\n            - `RESOLUTION_FRAME` if the model returns one output for each frame\n               of the input sequence.\n        """"""\n\n        if task.returns_sequence:\n            return RESOLUTION_FRAME\n\n        elif task.returns_vector:\n            return RESOLUTION_CHUNK\n\n        else:\n            # this should never happened\n            msg = f""{task} tasks are not supported.""\n            raise NotImplementedError(msg)\n\n    @property\n    def resolution(self) -> Resolution:\n        return self.resolution_\n\n    def get_alignment(self, task: Task, **architecture_params) -> Alignment:\n        """"""Get frame alignment\n\n        This method is called by `BatchGenerator` instances to dermine how\n        target tensors should be aligned with the output of the model.\n\n        Default behavior is to return \'center\'. In most cases, you should not\n        need to worry about this but if you do, this method can be overriden to\n        return \'strict\' or \'loose\'.\n\n        Parameters\n        ----------\n        task : Task\n        architecture_params : dict\n            Architecture hyper-parameters.\n\n        Returns\n        -------\n        alignment : `Alignment`\n            Target alignment. Must be one of \'center\', \'strict\', or \'loose\'.\n            Always returns \'center\'.\n        """"""\n\n        return ALIGNMENT_CENTER\n\n    @property\n    def alignment(self) -> Alignment:\n        return self.alignment_\n\n    @property\n    def n_features(self) -> int:\n        """"""Number of input features\n\n        Shortcut for self.specifications[\'X\'][\'dimension\']\n\n        Returns\n        -------\n        n_features : `int`\n            Number of input features\n        """"""\n        return self.specifications[""X""][""dimension""]\n\n    @property\n    def dimension(self) -> int:\n        """"""Output dimension\n\n        This method needs to be overriden for representation learning tasks,\n        because output dimension cannot be inferred from the task\n        specifications.\n\n        Returns\n        -------\n        dimension : `int`\n            Dimension of model output.\n\n        Raises\n        ------\n        AttributeError\n            If the model addresses a classification or regression task.\n        """"""\n\n        if self.task.is_representation_learning:\n            msg = (\n                f""Class {self.__class__.__name__} needs to define ""\n                f""\'dimension\' property.""\n            )\n            raise NotImplementedError(msg)\n\n        msg = f""{self.task} tasks do not define attribute \'dimension\'.""\n        raise AttributeError(msg)\n\n    @property\n    def classes(self) -> List[str]:\n        """"""Names of classes\n\n        Shortcut for self.specifications[\'y\'][\'classes\']\n\n        Returns\n        -------\n        classes : `list` of `str`\n            List of names of classes.\n\n\n        Raises\n        ------\n        AttributeError\n            If the model does not address a classification task.\n        """"""\n\n        if not self.task.is_representation_learning:\n            return self.specifications[""y""][""classes""]\n\n        msg = f""{self.task} tasks do not define attribute \'classes\'.""\n        raise AttributeError(msg)\n\n    def slide(\n        self,\n        features: SlidingWindowFeature,\n        sliding_window: SlidingWindow,\n        batch_size: int = 32,\n        device: torch.device = None,\n        skip_average: bool = None,\n        postprocess: Callable[[np.ndarray], np.ndarray] = None,\n        return_intermediate=None,\n        progress_hook=None,\n    ) -> SlidingWindowFeature:\n        """"""Slide and apply model on features\n\n        Parameters\n        ----------\n        features : SlidingWindowFeature\n            Input features.\n        sliding_window : SlidingWindow\n            Sliding window used to apply the model.\n        batch_size : int\n            Batch size. Defaults to 32. Use large batch for faster inference.\n        device : torch.device\n            Device used for inference.\n        skip_average : bool, optional\n            For sequence labeling tasks (i.e. when model outputs a sequence of\n            scores), each time step may be scored by several consecutive\n            locations of the sliding window. Default behavior is to average\n            those multiple scores. Set `skip_average` to False to return raw\n            scores without averaging them.\n        postprocess : callable, optional\n            Function applied to the predictions of the model, for each batch\n            separately. Expects a (batch_size, n_samples, n_features) np.ndarray\n            as input, and returns a (batch_size, n_samples, any) np.ndarray.\n        return_intermediate :\n            Experimental. Not documented yet.\n        progress_hook : callable\n            Experimental. Not documented yet.\n        """"""\n\n        if device is None:\n            device = ""cuda"" if torch.cuda.is_available() else ""cpu""\n        device = torch.device(device)\n\n        if skip_average is None:\n            skip_average = (self.resolution == RESOLUTION_CHUNK) or (\n                return_intermediate is not None\n            )\n\n        try:\n            dimension = self.dimension\n        except AttributeError:\n            dimension = len(self.classes)\n\n        resolution = self.resolution\n\n        # model returns one vector per input frame\n        if resolution == RESOLUTION_FRAME:\n            resolution = features.sliding_window\n\n        # model returns one vector per input window\n        if resolution == RESOLUTION_CHUNK:\n            resolution = sliding_window\n\n        support = features.extent\n        if support.duration < sliding_window.duration:\n            chunks = [support]\n            fixed = support.duration\n        else:\n            chunks = list(sliding_window(support, align_last=True))\n            fixed = sliding_window.duration\n\n        if progress_hook is not None:\n            n_chunks = len(chunks)\n            n_done = 0\n            progress_hook(n_done, n_chunks)\n\n        batches = pescador.maps.buffer_stream(\n            iter(\n                {""X"": features.crop(window, mode=""center"", fixed=fixed)}\n                for window in chunks\n            ),\n            batch_size,\n            partial=True,\n        )\n\n        fX = []\n        for batch in batches:\n\n            tX = torch.tensor(batch[""X""], dtype=torch.float32, device=device)\n\n            # FIXME: fix support for return_intermediate\n            tfX = self(tX, return_intermediate=return_intermediate)\n\n            tfX_npy = tfX.detach().to(""cpu"").numpy()\n            if postprocess is not None:\n                tfX_npy = postprocess(tfX_npy)\n\n            fX.append(tfX_npy)\n\n            if progress_hook is not None:\n                n_done += len(batch[""X""])\n                progress_hook(n_done, n_chunks)\n\n        fX = np.vstack(fX)\n\n        if skip_average:\n            return SlidingWindowFeature(fX, sliding_window)\n\n        # get total number of frames (based on last window end time)\n        n_frames = resolution.samples(chunks[-1].end, mode=""center"")\n\n        # data[i] is the sum of all predictions for frame #i\n        data = np.zeros((n_frames, dimension), dtype=np.float32)\n\n        # k[i] is the number of chunks that overlap with frame #i\n        k = np.zeros((n_frames, 1), dtype=np.int8)\n\n        for chunk, fX_ in zip(chunks, fX):\n\n            # indices of frames overlapped by chunk\n            indices = resolution.crop(chunk, mode=self.alignment, fixed=fixed)\n\n            # accumulate the outputs\n            data[indices] += fX_\n\n            # keep track of the number of overlapping sequence\n            # TODO - use smarter weights (e.g. Hamming window)\n            k[indices] += 1\n\n        # compute average embedding of each frame\n        data = data / np.maximum(k, 1)\n\n        return SlidingWindowFeature(data, resolution)\n'"
pyannote/audio/train/schedulers.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2018-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n\nimport numpy as np\nimport scipy.stats\nfrom collections import deque\nfrom .callback import Callback\nfrom tqdm import tqdm\nfrom scipy.signal import convolve\n\nfrom typing import TYPE_CHECKING\n\nAUTOLR_MIN = 0.000001\nAUTOLR_MAX = 10\nAUTOLR_EPOCHS = 8\nAUTOLR_BETA = 0.98\n\nMOMENTUM_MAX = 0.95\nMOMENTUM_MIN = 0.85\n\nif False:\n    from .trainer import Trainer\n\n\ndef decreasing_probability(values: np.ndarray) -> float:\n    """"""Compute probability that a sequence is decreasing\n\n    Parameters\n    ----------\n    values : np.ndarray\n        Sequence of values\n\n    Returns\n    -------\n    probability : float\n        Probability that sequence of values is decreasing\n\n    Reference\n    ---------\n    Davis King. ""Automatic Learning Rate Scheduling That Really Works"".\n    http://blog.dlib.net/2018/02/automatic-learning-rate-scheduling-that.html\n    """"""\n    n_steps = len(values)\n    steps = np.arange(n_steps)\n\n    A = np.vstack([steps, np.ones(n_steps)]).T\n    loc, shift = np.linalg.lstsq(A, values, rcond=None)[0]\n\n    values_ = loc * steps + shift\n    sigma2 = np.sum((values - values_) ** 2) / (n_steps - 2)\n\n    scale = np.sqrt(12 * sigma2 / (n_steps ** 3 - n_steps))\n    return scipy.stats.norm.cdf(0.0, loc=loc, scale=scale)\n\n\ndef steps_without_decrease(values: np.ndarray, robust: bool = False) -> int:\n    """"""Count number of steps without decrease\n\n    Parameters\n    ----------\n    values : np.ndarray\n        Sequence of values\n    robust : bool\n        Remove 10% highest values before counting steps.\n\n    Returns\n    -------\n    n_steps : int\n        Number of steps\n\n    Reference\n    ---------\n    Davis King. ""Automatic Learning Rate Scheduling That Really Works"".\n    http://blog.dlib.net/2018/02/automatic-learning-rate-scheduling-that.html\n    """"""\n\n    if robust:\n        values = values[values < np.percentile(values, 90)]\n\n    steps_without_decrease = 0\n    n_steps = len(values)\n    for i in reversed(range(n_steps - 2)):\n        p = decreasing_probability(values[i:])\n        if p < 0.51:\n            steps_without_decrease = n_steps - i\n    return steps_without_decrease\n\n\nclass BaseSchedulerCallback(Callback):\n    """"""Base scheduler with support for AutoLR\n\n    Reference\n    ---------\n    Leslie N. Smith. ""Cyclical Learning Rates for Training Neural Networks""\n    IEEE Winter Conference on Applications of Computer Vision (WACV, 2017).\n    """"""\n\n    def on_train_start(self, trainer: ""Trainer"") -> None:\n        self.optimizer_ = trainer.optimizer\n        if trainer.base_learning_rate_ == ""auto"":\n            trainer.base_learning_rate_ = self.auto_lr(trainer)\n        self.learning_rate = trainer.base_learning_rate_\n\n    def on_epoch_start(self, trainer: ""Trainer"") -> None:\n        """"""Log learning rate to tensorboard""""""\n\n        trainer.tensorboard_.add_scalar(\n            f""train/lr"", self.learning_rate, global_step=trainer.epoch_\n        )\n\n    @property\n    def learning_rate(self) -> float:\n        return self._learning_rate\n\n    @learning_rate.setter\n    def learning_rate(self, learning_rate: float) -> None:\n        for g in self.optimizer_.param_groups:\n            g[""lr""] = learning_rate\n            g[""momentum""] = MOMENTUM_MAX\n        self._learning_rate = learning_rate\n\n    @staticmethod\n    def _choose_lr(lrs: np.ndarray, losses: np.ndarray) -> float:\n        """"""Choose learning rate upper bound\n\n        This is done by selecting the [0.1 x lr, lr] range that leads to the\n        largest decrease in terms of loss value.\n\n        Parameters\n        ----------\n        lrs : np.ndarray\n            Sequence of learning rates\n        losses : np.ndarray\n            Corresponding sequence of loss values\n\n        Returns\n        -------\n        max_lr : float\n            Return learning rate upper bound\n        """"""\n\n        min_lr, max_lr = np.min(lrs), np.max(lrs)\n        n_batches = len(lrs)\n\n        # `factor` by which the learning rate is multiplied after every batch,\n        # to get from `min_lr` to `max_lr` in `n_batches` step.\n        factor = (max_lr / min_lr) ** (1 / n_batches)\n\n        # K batches to increase the learning rate by one order of magnitude\n        K = int(np.log(10) / np.log(factor))\n\n        # loss improvement on each [0.1 x lr, lr] range\n        improvement = losses[:-K] - losses[K:]\n\n        # return half LR such that [0.1 x lr, lr] leads to the best improvement\n        return 0.5 * lrs[K + np.argmax(improvement)]\n\n    def auto_lr(self, trainer: ""Trainer"") -> float:\n        """"""Find optimal learning rate automatically\n\n        Parameters\n        ----------\n        trainer : \'Trainer\'\n\n        Returns\n        -------\n        learning_rate : float\n            Optimal learning rate\n\n        Reference\n        ---------\n        Leslie N. Smith. ""Cyclical Learning Rates for Training Neural Networks""\n        IEEE Winter Conference on Applications of Computer Vision (WACV, 2017).\n\n            There is a simple way to estimate reasonable minimum and maximum\n            boundary values with one training run of the network for a few\n            epochs. It is a ""LR range test""; run your model for several epochs\n            while letting the learning rate increase linearly between low and\n            high LR values. This test is enormously valuable whenever you are\n            facing a new architecture or dataset. Next, plot the accuracy\n            versus learning rate. Note the learning rate value when the\n            accuracy starts to increase and when the accuracy slows, becomes\n            ragged, or starts to fall. These two learning rates are good\n            choices for bounds; that is, set base_lr to the first value and\n            set max_lr to the latter value. Alternatively, one can use the rule\n            of thumb that the optimum learning rate is usually within a factor\n            of two of the largest one that converges and set base_lr to 1/3 or\n            1/4 of max_lr. [...] Whenever one is starting with a new\n            architecture or dataset, a single LR range test provides both a\n            good LR value and a good range. Then one should compare runs with a\n            fixed LR versus CLR with this range. Whichever wins can be used\n            with confidence for the rest of one\xe2\x80\x99s experiments.\n        """"""\n        # save states to disk\n        trainer.save_state()\n\n        # initialize optimizer with a low learning rate\n        self.learning_rate = AUTOLR_MIN\n\n        # `factor` by which the learning rate is multiplied after every batch,\n        # to get from `min_lr` to `max_lr` in `n_batches` step.\n        n_batches: int = AUTOLR_EPOCHS * trainer.batches_per_epoch\n        factor = (AUTOLR_MAX / AUTOLR_MIN) ** (1 / n_batches)\n\n        # progress bar\n        pbar = tqdm(\n            desc=""AutoLR"",\n            total=n_batches,\n            leave=False,\n            ncols=80,\n            unit=""batch"",\n            position=1,\n        )\n\n        loss_moving_avg = 0.0\n        losses, losses_smoothened, lrs = [], [], []\n\n        # loop on n_batches batches\n        for i in range(n_batches):\n\n            batch = trainer.get_new_batch()\n            loss = trainer.batch_loss(batch)\n            loss[""loss""].backward()\n            trainer.optimizer_.step()\n            trainer.optimizer_.zero_grad()\n\n            l = loss[""loss""].item()\n            if np.isnan(l):\n                break\n\n            losses.append(l)\n            lrs.append(self.learning_rate)\n\n            loss_moving_avg = (\n                AUTOLR_BETA * loss_moving_avg + (1 - AUTOLR_BETA) * losses[-1]\n            )\n            losses_smoothened.append(loss_moving_avg / (1 - AUTOLR_BETA ** (i + 1)))\n\n            # update progress bar\n            pbar.update(1)\n            pbar.set_postfix(\n                ordered_dict={""loss"": losses_smoothened[-1], ""lr"": self.learning_rate}\n            )\n\n            # increase learning rate\n            self.learning_rate = factor * self.learning_rate\n\n            # stop AutoLR early when loss starts to explode\n            if i > 1 and losses_smoothened[-1] > 100 * np.nanmin(losses_smoothened):\n                break\n\n        # reload model using its initial state\n        trainer.load_state()\n\n        # choose learning rate based on loss = f(learning_rate) curve\n        auto_lr = self._choose_lr(np.array(lrs), np.array(losses_smoothened))\n\n        # log curve and auto_lr to tensorboard as an image\n        try:\n            # import matplotlib with headless backend\n            import matplotlib\n\n            matplotlib.use(""Agg"")\n            import matplotlib.pyplot as plt\n\n            # create AutoLR loss = f(learning_rate) curve\n            fig, ax = plt.subplots()\n            ax.semilogx(lrs, losses, ""."", alpha=0.3, label=""Raw loss"")\n            ax.semilogx(lrs, losses_smoothened, linewidth=2, label=""Smoothened loss"")\n            ax.set_xlabel(""Learning rate"")\n            ax.set_ylabel(""Loss"")\n            ax.legend()\n\n            # indicate selected learning rate by a vertical line\n            ax.plot(\n                [auto_lr, auto_lr],\n                [np.nanmin(losses_smoothened), np.nanmax(losses_smoothened)],\n                linewidth=3,\n            )\n\n            # zoom on meaningful part of the curve\n            m = np.nanmin(losses_smoothened)\n            M = 1.1 * losses_smoothened[10]\n            ax.set_ylim(m, M)\n\n            # indicate selected learning rate in the figure title\n            ax.set_title(f""AutoLR = {auto_lr:g}"")\n\n            # send matplotlib figure to Tensorboard\n            trainer.tensorboard_.add_figure(\n                ""train/auto_lr"", fig, global_step=trainer.epoch_, close=True\n            )\n\n        except ImportError as e:\n            msg = (\n                f""Something went wrong when trying to send AutoLR figure ""\n                f""to Tensorboard. Did you install matplotlib?\\n\\n{e}\\n\\n""\n            )\n            print(msg)\n            print(e)\n\n        except Exception as e:\n            msg = (\n                f""Something went wrong when trying to send AutoLR figure ""\n                f""to Tensorboard. It is OK but you might want to have a ""\n                f""look at why this happened.\\n\\n{e}\\n\\n""\n            )\n            print(msg)\n\n        return auto_lr\n\n\nclass ConstantScheduler(BaseSchedulerCallback):\n    """"""Constant learning rate""""""\n\n    pass\n\n\nclass DavisKingScheduler(BaseSchedulerCallback):\n    """"""Automatic Learning Rate Scheduling That Really Works\n\n    http://blog.dlib.net/2018/02/automatic-learning-rate-scheduling-that.html\n\n    Parameters\n    ----------\n    factor : float, optional\n        Factor by which the learning rate will be reduced.\n        new_lr = old_lr * factor. Defaults to 0.5\n    patience : int, optional\n        Number of epochs with no improvement after which learning rate will\n        be reduced. Defaults to 10.\n    """"""\n\n    def __init__(self, factor: float = 0.5, patience: int = 10):\n        super().__init__()\n        self.factor = factor\n        self.patience = patience\n\n    def on_train_start(self, trainer: ""Trainer"") -> None:\n        super().on_train_start(trainer)\n        maxlen = 2 * self.patience * trainer.batches_per_epoch\n        self.losses_ = deque([], maxlen=maxlen)\n\n    def on_epoch_end(self, trainer: ""Trainer"") -> None:\n\n        # compute statistics on batch loss trend\n        count = steps_without_decrease(np.array(self.losses_))\n        count_robust = steps_without_decrease(np.array(self.losses_), robust=True)\n\n        # if batch loss hasn\'t been decreasing for a while\n        patience = self.patience * trainer.batches_per_epoch\n        if count > patience and count_robust > patience:\n            self.learning_rate = self.factor * self.learning_rate\n            self.losses_.clear()\n\n    def on_batch_end(self, trainer, batch_loss):\n        super().on_batch_end(trainer, batch_loss)\n\n        # store current batch loss\n        self.losses_.append(batch_loss[""loss""].item())\n\n\nclass CyclicScheduler(BaseSchedulerCallback):\n    """"""Cyclic learning rate (and momentum)\n\n    Parameters\n    ----------\n    epochs_per_cycle : int, optional\n        Number of epochs per cycle. Defaults to 10.\n    decay : {float, \'auto\'}, optional\n        Update base learning rate at the end of each cycle:\n            - when `float`, multiply base learning rate by this amount;\n            - when \'auto\', apply AutoLR;\n            - defaults to doing nothing.\n\n    Reference\n    ---------\n    Leslie N. Smith. ""Cyclical Learning Rates for Training Neural Networks""\n    IEEE Winter Conference on Applications of Computer Vision (WACV, 2017).\n    """"""\n\n    def __init__(self, epochs_per_cycle: int = 10, decay=None):\n        super().__init__()\n        self.epochs_per_cycle = epochs_per_cycle\n        self.decay = decay\n\n    @property\n    def momentum(self) -> float:\n        return self._momentum\n\n    @momentum.setter\n    def momentum(self, momentum: float) -> None:\n        for g in self.optimizer_.param_groups:\n            g[""momentum""] = momentum\n        self._momentum = momentum\n\n    def on_train_start(self, trainer: ""Trainer"") -> None:\n        """"""Initialize batch/epoch counters""""""\n\n        super().on_train_start(trainer)\n        self.batches_per_cycle_ = self.epochs_per_cycle * trainer.batches_per_epoch\n        self.n_batches_ = 0\n        self.n_epochs_ = 0\n\n        self.learning_rate = trainer.base_learning_rate_ * 0.1\n\n    def on_epoch_end(self, trainer: ""Trainer"") -> None:\n        """"""Update base learning rate at the end of cycle""""""\n\n        super().on_epoch_end(trainer)\n\n        # reached end of cycle?\n        self.n_epochs_ += 1\n        if self.n_epochs_ % self.epochs_per_cycle == 0:\n\n            # apply AutoLR\n            if self.decay == ""auto"":\n                trainer.base_learning_rate_ = self.auto_lr(trainer)\n\n            # decay base learning rate\n            elif self.decay is not None:\n                trainer.base_learning_rate_ *= self.decay\n\n            # reset epoch/batch counters\n            self.n_epochs_ = 0\n            self.n_batches_ = 0\n\n    def on_batch_start(self, trainer: ""Trainer"", batch: dict) -> dict:\n        """"""Update learning rate & momentum according to position in cycle""""""\n\n        super().on_batch_start(trainer, batch)\n\n        # position within current cycle (reversed V)\n        rho = 1.0 - abs(2 * (self.n_batches_ / self.batches_per_cycle_ - 0.5))\n\n        self.learning_rate = trainer.base_learning_rate_ * (0.1 + 0.9 * rho)\n        self.momentum = MOMENTUM_MAX - (MOMENTUM_MAX - MOMENTUM_MIN) * rho\n\n        self.n_batches_ += 1\n\n        return batch\n'"
pyannote/audio/train/task.py,7,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2019 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n""""""\nTasks\n#####\n\nThis module provides a `Task` class meant to specify machine learning tasks\n(e.g. classification or regression).\n\nThis may be used to infer parts of the network architecture and the associated\nloss function automatically.\n\nExample\n-------\n>>> voice_activity_detection = Task(type=TaskType.MULTI_CLASS_CLASSIFICATION,\n...                                 output=TaskOutput.SEQUENCE)\n""""""\n\nfrom enum import Enum\nfrom typing import Union\nfrom typing import NamedTuple\nfrom typing import Callable\n\n\nclass TaskType(Enum):\n    """"""Type of machine learning task\n\n    Attributes\n    ----------\n    MULTI_CLASS_CLASSIFICATION\n        multi-class classification\n    MULTI_LABEL_CLASSIFICATION\n        multi-label classification\n    REGRESSION\n        regression\n    REPRESENTATION_LEARNING\n        representation learning\n    """"""\n\n    MULTI_CLASS_CLASSIFICATION = 0\n    MULTI_LABEL_CLASSIFICATION = 1\n    REGRESSION = 2\n    REPRESENTATION_LEARNING = 3\n\n\nclass TaskOutput(Enum):\n    """"""Expected output\n\n    Attributes\n    ----------\n    SEQUENCE\n        A sequence of vector is expected.\n    VECTOR\n        A single vector is expected.\n    """"""\n\n    SEQUENCE = 0\n    VECTOR = 1\n\n\nclass Task(NamedTuple):\n    type: TaskType\n    output: TaskOutput\n\n    @classmethod\n    def from_str(cls, representation: str):\n        task_output, task_type = representation.split("" "", 1)\n\n        if task_output == ""frame-wise"":\n            task_output = TaskOutput.SEQUENCE\n\n        elif task_output == ""chunk-wise"":\n            task_output = TaskOutput.VECTOR\n\n        else:\n            msg = f\'""{task_output}"" task output is not supported.\'\n            raise NotImplementedError(msg)\n\n        if task_type == ""multi-class classification"":\n            task_type = TaskType.MULTI_CLASS_CLASSIFICATION\n\n        elif task_type == ""multi-label classification"":\n            task_type = TaskType.MULTI_LABEL_CLASSIFICATION\n\n        elif task_type == ""regression"":\n            task_type = TaskType.REGRESSION\n\n        elif task_type == ""representation learning"":\n            task_type = TaskType.REPRESENTATION_LEARNING\n\n        else:\n            msg = f\'""{task_type}"" task type is not supported.\'\n            raise NotImplementedError(msg)\n\n        return cls(type=task_type, output=task_output)\n\n    def __str__(self) -> str:\n        """"""String representation""""""\n\n        if self.returns_sequence:\n            name = ""frame-wise""\n\n        elif self.returns_vector:\n            name = ""chunk-wise""\n\n        else:\n            msg = (\n                ""string representation (__str__) is not implemented ""\n                ""for this task output.""\n            )\n            raise NotImplementedError(msg)\n\n        if self.is_multiclass_classification:\n            name = f""{name} multi-class classification""\n\n        elif self.is_multilabel_classification:\n            name = f""{name} multi-label classification""\n\n        elif self.is_regression:\n            name = f""{name} regression""\n\n        elif self.is_representation_learning:\n            name = f""{name} representation learning""\n\n        else:\n            msg = (\n                ""string representation (__str__) is not implemented ""\n                ""for this type of task.""\n            )\n            raise NotImplementedError(msg)\n\n        return name\n\n    @property\n    def returns_sequence(self) -> bool:\n        """"""Is the output expected to be a sequence?\n\n        Returns\n        -------\n        `bool`\n            `True` if the task output is a sequence, `False` otherwise.\n        """"""\n        return self.output == TaskOutput.SEQUENCE\n\n    @property\n    def returns_vector(self) -> bool:\n        """"""Is the output expected to be a single vector?\n\n        Returns\n        -------\n        `bool`\n            `True` if the task output is a single vector, `False` otherwise.\n        """"""\n        return self.output == TaskOutput.VECTOR\n\n    @property\n    def is_multiclass_classification(self) -> bool:\n        """"""Is it multi-class classification?\n\n        Returns\n        -------\n        `bool`\n            `True` if the task is multi-class classification\n        """"""\n        return self.type == TaskType.MULTI_CLASS_CLASSIFICATION\n\n    @property\n    def is_multilabel_classification(self) -> bool:\n        """"""Is it multi-label classification?\n\n        Returns\n        -------\n        `bool`\n            `True` if the task is multi-label classification\n        """"""\n        return self.type == TaskType.MULTI_LABEL_CLASSIFICATION\n\n    @property\n    def is_regression(self) -> bool:\n        """"""Is it regression?\n\n        Returns\n        -------\n        `bool`\n            `True` if the task is regression\n        """"""\n        return self.type == TaskType.REGRESSION\n\n    @property\n    def is_representation_learning(self) -> bool:\n        """"""Is it representation learning?\n\n        Returns\n        -------\n        `bool`\n            `True` if the task is representation learning\n        """"""\n        return self.type == TaskType.REPRESENTATION_LEARNING\n\n    @property\n    def default_activation(self):\n        """"""Default final activation\n\n        Returns\n        -------\n        `torch.nn.LogSoftmax(dim=-1)` for multi-class classification\n        `torch.nn.Sigmoid()` for multi-label classification\n        `torch.nn.Identity()` for regression\n\n        Raises\n        ------\n        NotImplementedError\n            If the default activation cannot be guessed.\n        """"""\n\n        import torch.nn\n\n        if self.is_multiclass_classification:\n            return torch.nn.LogSoftmax(dim=-1)\n\n        elif self.is_multilabel_classification:\n            return torch.nn.Sigmoid()\n\n        elif self.is_regression:\n            return torch.nn.Identity()\n\n        else:\n            msg = f""Unknown default activation for {self} task.""\n            raise NotImplementedError(msg)\n'"
pyannote/audio/train/trainer.py,15,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2018-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\nfrom typing import Optional, Iterator, Callable, Union, List\n\ntry:\n    from typing import Literal\nexcept ImportError as e:\n    from typing_extensions import Literal\nfrom pathlib import Path\nimport io\nimport os\nimport sys\nimport yaml\nimport torch\nimport tempfile\nimport warnings\nfrom itertools import chain\nfrom torch.nn import Module\nfrom torch.optim import Optimizer, SGD\nfrom torch.utils.tensorboard import SummaryWriter\nfrom .logging import Logging\nfrom .callback import Callback\nfrom .callback import Callbacks\nfrom .schedulers import BaseSchedulerCallback\nfrom .schedulers import ConstantScheduler\nfrom .generator import BatchGenerator\nfrom .model import Model\nfrom ..utils.timeout import timeout\nfrom ..utils.background import AdaptiveBackgroundGenerator\n\n\nARBITRARY_LR = 0.1\n\n\nclass Trainer:\n    """"""Trainer\n\n    Attributes\n    ----------\n    model : Model\n    specifications : dict\n    device : torch.device\n    """"""\n\n    SPECS_YML = ""{train_dir}/specs.yml""\n    MODEL_PT = ""{train_dir}/weights/{epoch:04d}.pt""\n    OPTIMIZER_PT = ""{train_dir}/weights/{epoch:04d}.optimizer.pt""\n\n    def load_state(self, model_pt: Optional[Path] = None) -> bool:\n        """"""Load model and optimizer states from disk\n\n        Parameters\n        ----------\n        model_pt : `Path`, optional\n            Path to file containing model state.\n            Defaults to guessing it from trainer status.\n\n        Returns\n        -------\n        success : bool\n            True if state was loaded successfully, False otherwise.\n        """"""\n\n        if model_pt is None:\n            _model_pt = self.MODEL_PT.format(\n                train_dir=self.train_dir_, epoch=self.epoch_\n            )\n            optimizer_pt = self.OPTIMIZER_PT.format(\n                train_dir=self.train_dir_, epoch=self.epoch_\n            )\n\n        else:\n            _model_pt = model_pt\n            optimizer_pt = model_pt.with_suffix("".optimizer.pt"")\n\n        model_state = torch.load(_model_pt, map_location=lambda storage, loc: storage)\n        missing_keys, unexpected_keys = self.model_.load_state_dict(\n            model_state, strict=False\n        )\n        if missing_keys:\n            msg = f""Checkpoint misses the following weights: {missing_keys}.""\n            warnings.warn(msg)\n        if unexpected_keys:\n            msg = f""Checkpoint contains unexpected weights: {unexpected_keys}.""\n            warnings.warn(msg)\n\n        success = self.load_more(model_pt=model_pt)\n\n        if success:\n\n            try:\n                optimizer_state = torch.load(\n                    optimizer_pt, map_location=lambda storage, loc: storage\n                )\n                self.optimizer_.load_state_dict(optimizer_state)\n            except Exception as e:\n                msg = (\n                    f""Did not load optimizer state (most likely because current ""\n                    f""training session uses a different loss than the one used ""\n                    f""for pre-training).""\n                )\n                warnings.warn(msg)\n\n        return success\n\n    def load_more(self, model_pt=None) -> bool:\n        """"""Called after model state is loaded\n\n        This method can be overriden to load additional states.\n        For instance, it can be used to load the state of a domain classifier\n        in domain-adversarial training, or the class centers in center loss.\n\n        Parameters\n        ----------\n        model_pt : `Path`, optional\n            Path to file containing model state.\n            Defaults to guessing it from trainer status.\n\n        Returns\n        -------\n        success : bool\n            True if state was loaded successfully, False otherwise.\n        """"""\n        return True\n\n    def save_state(self):\n        """"""Save model and optimizer states to disk""""""\n\n        # save model state\n        model_pt = self.MODEL_PT.format(train_dir=self.train_dir_, epoch=self.epoch_)\n        torch.save(self.model_.state_dict(), model_pt)\n\n        # save optimizer state\n        optimizer_pt = self.OPTIMIZER_PT.format(\n            train_dir=self.train_dir_, epoch=self.epoch_\n        )\n        torch.save(self.optimizer_.state_dict(), optimizer_pt)\n\n        self.save_more()\n\n    def save_more(self):\n        """"""Called after model and optimizer states are saves\n\n        This method can be overriden to save additional states.\n        For instance, it can be used to save the state of a domain classifier\n        in domain-adversarial training, or the class centers in center loss.\n        """"""\n        pass\n\n    def parameters(self):\n        return chain(self.model_.parameters(), self.more_parameters())\n\n    def more_parameters(self):\n        """"""Called by `parameters` method\n\n        This method can be overriden to define additional modules.\n        For instance, it can be used to define a domain classifier for\n        for domain-adversarial training.\n\n        It should be an iterator yielding the parameters of these additional\n        modules.\n\n        Yields\n        ------\n        parameter : nn.Parameter\n            Trainable trainer parameters.\n        """"""\n        return []\n\n    def on_train_start(self):\n        """"""Called just before training starts""""""\n        pass\n\n    def on_epoch_start(self):\n        """"""Called just before epoch starts""""""\n        pass\n\n    def on_batch_start(self, batch):\n        """"""Called just before batch is processed\n\n        Parameters\n        ----------\n        batch : `dict`\n            Current batch.\n\n        Returns\n        -------\n        batch : `dict`\n            Updated batch.\n        """"""\n        return batch\n\n    def on_batch_end(self, loss):\n        """"""Called just after loss is computed\n\n        Parameters\n        ----------\n        loss : `dict`\n            [\'loss\'] (`torch.Tensor`)\n        """"""\n        pass\n\n    def on_epoch_end(self):\n        """"""Called when epoch ends""""""\n        pass\n\n    def on_train_end(self):\n        """"""Called when training stops""""""\n        pass\n\n    @property\n    def model(self) -> Model:\n        return self.model_\n\n    @property\n    def optimizer(self):\n        return self.optimizer_\n\n    @property\n    def specifications(self):\n        return self.batch_generator_.specifications\n\n    @property\n    def device(self) -> torch.device:\n        return self.device_\n\n    @property\n    def epoch(self) -> int:\n        return self.epoch_\n\n    @property\n    def batches_per_epoch(self) -> int:\n        return self.batches_per_epoch_\n\n    def get_new_batch(self):\n        return next(self.batches_)\n\n    def fit_iter(\n        self,\n        model: Model,\n        batch_generator: BatchGenerator,\n        warm_start: Union[int, Path] = 0,\n        epochs: int = 1000,\n        get_optimizer: Callable[..., Optimizer] = SGD,\n        scheduler: Optional[BaseSchedulerCallback] = None,\n        learning_rate: Union[Literal[""auto""], float] = ""auto"",\n        train_dir: Optional[Path] = None,\n        verbosity: int = 2,\n        device: Optional[torch.device] = None,\n        callbacks: Optional[List[Callback]] = None,\n        n_jobs: int = 1,\n    ) -> Iterator[Model]:\n        """"""Train model\n\n        Parameters\n        ----------\n        model : `Model`\n            Model.\n        batch_generator : `BatchGenerator`\n            Batch generator.\n        warm_start : `int` or `Path`, optional\n            Restart training at this epoch or from this model.\n            Default behavior (0) is to train the model from scratch.\n        epochs : `int`, optional\n            Train model for that many epochs. Defaults to 1000.\n        get_optimizer : `callable`, optional\n            Callable taking model parameters as input and returns an instance of\n            `torch.optim.Optimizer`. May also support `lr` keyword argument.\n            Defaults to `torch.optim.SGD`.\n        scheduler : `BaseSchedulerCallback`, optional\n            Learning rate scheduler. Defaults to `ConstantScheduler`.\n        learning_rate : {float, \'auto\'}, optional\n            Learning rate. Default behavior (\'auto\') is to use the AutoLR\n            heuristic to determine the learning rate automatically.\n        train_dir : `Path`, optional\n            Directory where models and other log files are stored.\n            Defaults to a temporary directory.\n        verbosity : `int`, optional\n            Set level of verbosity.\n            Use 0 (default) to not show any progress bar.\n            Use 1 to show a progress bar updated at the end of each epoch.\n            Use 2 to add a second progress bar updated at the end of each batch.\n        device : `torch.device`, optional\n            Device on which the model will be allocated. Defaults to using CPU.\n        callbacks : `list` of `Callback` instances\n            Add custom callbacks.\n        n_jobs : `int`, optional\n            Defaults to 1.\n\n        Yields\n        ------\n        model : `Model`\n            Model at current iteration\n        """"""\n\n        #\n        if train_dir is None:\n            train_dir = Path(tempfile.mkdtemp())\n        self.train_dir_ = train_dir\n\n        # DEVICE\n        self.device_ = torch.device(""cpu"") if device is None else device\n\n        # MODEL\n        self.model_ = model.to(self.device_)\n\n        # BATCH GENERATOR\n        self.batch_generator_ = batch_generator\n        self.batches_ = AdaptiveBackgroundGenerator(\n            self.batch_generator_, n_jobs=n_jobs\n        )\n        self.batches_per_epoch_ = self.batch_generator_.batches_per_epoch\n\n        # OPTIMIZER\n        lr = ARBITRARY_LR if learning_rate == ""auto"" else learning_rate\n        self.optimizer_ = get_optimizer(self.parameters(), lr=lr)\n        self.base_learning_rate_ = learning_rate\n\n        # make sure that \'train_dir\' directory does not exist when\n        # fine-tuning a pre-trained model or starting from scratch\n        # as it might contain the output of very long computations:\n        # you do not want to erase them by mistake!\n\n        if isinstance(warm_start, Path) or warm_start == 0:\n\n            try:\n                # this will fail if the directory already exists\n                os.makedirs(self.train_dir_ / ""weights"")\n\n            except FileExistsError as e:\n\n                # ask user whether it is OK to continue\n                try:\n                    with timeout(60):\n                        msg = (\n                            f\'Directory ""{self.train_dir_}"" exists.\\n\'\n                            f""Are you OK to overwrite existing models? [y/N]: ""\n                        )\n                        overwrite = (input(msg) or ""n"").lower()\n                except TimeoutError:\n                    # defaults to ""no"" after 60 seconds\n                    overwrite = ""n""\n\n                # stop everything if the user did not say ""yes"" after a while\n                if overwrite != ""y"":\n                    sys.exit()\n\n        # defaults to 0\n        self.epoch_ = 0\n\n        # when warm_start is an integer, it means that the user wants to\n        # restart training at a given epoch. we intialize the model state and\n        # set epoch_ attribute accordingly.\n        if isinstance(warm_start, int):\n\n            if warm_start > 0:\n\n                # set epoch_ to requested value...\n                self.epoch_ = warm_start\n\n                # ... and load corresponding model if requested\n                success = self.load_state(model_pt=None)\n\n        # when warm_start is a Path, it means that the user wants to\n        # restart training from a pretrained model\n        else:\n            try:\n                success = self.load_state(model_pt=warm_start)\n\n            except Exception as e:\n                msg = (\n                    f""Could not assign model weights. The following exception ""\n                    f""was raised:\\n\\n{e}\\n\\nAre you sure the architectures ""\n                    f""are consistent?""\n                )\n                sys.exit(msg)\n\n            # save pretrained model as epoch 0\n            self.save_state()\n\n        # save specifications to weights/specs.yml\n        specs_yml = self.SPECS_YML.format(train_dir=self.train_dir_)\n        with io.open(specs_yml, ""w"") as fp:\n            specifications = dict(self.specifications)\n            specifications[""task""] = str(specifications[""task""])\n            yaml.dump(specifications, fp, default_flow_style=False)\n\n        # TODO in case success = False, one should freeze the main network for\n        # TODO a few epochs and train the ""more"" part alone first before\n        # TODO unfreezing everything. this could be done through a callback\n\n        callbacks_ = []\n\n        # SCHEDULER\n        if scheduler is None:\n            scheduler = ConstantScheduler()\n        callbacks_.append(scheduler)\n\n        logger = Logging(epochs=epochs, verbosity=verbosity)\n        callbacks_.append(logger)\n\n        # CUSTOM CALLBACKS\n        if callbacks is not None:\n            callbacks_.extend(callbacks)\n\n        callbacks = Callbacks(callbacks_)\n\n        # TRAINING STARTS\n        self.tensorboard_ = SummaryWriter(\n            log_dir=self.train_dir_, purge_step=self.epoch_\n        )\n        self.on_train_start()\n        callbacks.on_train_start(self)\n\n        while self.epoch_ < epochs:\n\n            # EPOCH STARTS\n            self.epoch_ += 1\n            self.on_epoch_start()\n            callbacks.on_epoch_start(self)\n\n            for i in range(self.batches_per_epoch_):\n\n                batch = self.get_new_batch()\n\n                # BATCH IS READY FOR FORWARD PASS\n                batch = self.on_batch_start(batch)\n                batch = callbacks.on_batch_start(self, batch)\n\n                # FORWARD PASS + LOSS COMPUTATION\n                loss = self.batch_loss(batch)\n\n                # BACKWARD PASS\n                loss[""loss""].backward()\n                self.optimizer_.step()\n                self.optimizer_.zero_grad()\n\n                # OPTIMIZATION STEP IS DONE\n                self.on_batch_end(loss)\n                callbacks.on_batch_end(self, loss)\n\n            self.on_epoch_end()\n            callbacks.on_epoch_end(self)\n\n            yield self.model_\n\n            self.save_state()\n\n        callbacks.on_train_end(self)\n\n        self.batches_.deactivate()\n'"
pyannote/audio/utils/__init__.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2019 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n'"
pyannote/audio/utils/background.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2019-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n""""""\nBackground generators\n=====================\n\nThis module provides ways to send a data generator to one (or multiple)\nbackground thread(s).\n\nA typical use case is when training a neural network using mini-batches:\nbatches are produced on CPU and consumed (often faster) on GPU, leading to\nsub-optimal usage of GPU resource (hence slow training).\n\n`BackgroundGenerator` allows to send the CPU producer to a background thread so\nthat it can generate a new batch while the previous one is being consumed by\nthe GPU.\n\n`AdaptiveBackgroundGenerator` goes one step further and uses a pool of\nbackground threads whose size automatically (and continuously) adapts the\nproduction rate to the consumption rate.\n""""""\n\nimport threading\nimport collections\nimport queue\nimport time\nfrom typing import Iterator, Callable\nimport numpy as np\n\n\nclass BackgroundGenerator(threading.Thread):\n    """"""Background generator with production/consumption time estimates\n\n    Parameters\n    ----------\n    producer: generator function\n        Generator function that takes no argument and yield (a possibly\n        infinite number of) samples. This would typically be a BatchGenerator\n        instance but can be any function that ""yields"" samples.\n    prefetch: int, optional\n        Maximum number of samples that can be prefetched and stored in a queue.\n        Defaults to 1. In case the consumer is slower than the producer and the\n        queue is full, the producer is paused until one sample is consumed.\n\n    Usage\n    -----\n    >>> import time\n\n    # a dummy producer that yield \'sample\' string every 10ms.\n    >>> def produce():\n    ...     while True:\n    ...        time.sleep(0.010)\n    ...        yield \'sample\'\n\n    # a dummy consumer that takes 1ms to consume a sample\n    >>> def consume(sample):\n    ...     time.sleep(0.001)\n\n    # create background generator from producer\n    >>> generator = BackgroundGenerator(produce)\n\n    # produce and consume 100 samples\n    >>> for i in range(100):\n    ...     sample = next(generator)\n    ...     consume(sample)\n\n    >>> p = generator.production_time\n    >>> print(f\'Production time estimate: {1000 * p:.0f}ms\')\n    # Production time estimate: 10ms\n\n    >>> c = generator.consumption_time\n    >>> print(f\'Consumption time estimate: {1000 * c:.0f}ms\')\n    # Consumption time estimate: 1ms\n\n    # kill background generator (and associated thread)\n    >>> generator.deactivate()\n    >>> sample = next(generator)\n    # StopIteration: Background generator is no longer active.\n    """"""\n\n    def __init__(self, producer: Callable[[], Iterator], prefetch: int = 1):\n        super().__init__(daemon=True)\n        self.producer = producer\n        self.prefetch = prefetch\n\n        self.activated_ = True\n        self.producer_ = producer()\n\n        # used to keep track of how long it took to generate latest samples\n        self.production_time_ = collections.deque([], max(10, 2 * self.prefetch))\n\n        # used to keep track of how long it took to consume latest samples\n        self.consumption_time_ = collections.deque([], max(10, 2 * self.prefetch))\n\n        # used to keep track of last time\n        self.last_ready_ = None\n\n        # queue meant to store at most \'self.prefetch\' prefetched samples\n        self.queue_ = queue.Queue(self.prefetch)\n\n        # start generator in a new thread\n        self.start()\n\n    def reset(self) -> None:\n        """"""Reset production and consumption time estimators""""""\n        self.production_time_.clear()\n        self.consumption_time_.clear()\n\n    def deactivate(self) -> None:\n        """"""Stop background generator""""""\n        self.activated_ = False\n        # unlock queue stuck at line queue.put() in self.run()\n        _ = self.queue_.get()\n\n    @property\n    def production_time(self) -> float:\n        """"""Estimated time needed by the generator to yield a sample.\n\n        This is computed as the median production time of the last few samples.\n\n        Returns\n        -------\n        production_time : float or np.NAN\n            Estimated time needed by the generator to yield a new sample, in\n            seconds. Until enough samples have been yielded to accurately\n            estimate production time, it is set to np.NAN.\n        """"""\n\n        if len(self.production_time_) < max(10, 2 * self.prefetch):\n            return np.NAN\n        return np.median(self.production_time_)\n\n    @property\n    def consumption_time(self) -> float:\n        """"""Estimated time needed by the consumer to process a sample\n\n        This is computed as the median consumption time of the last few samples.\n\n        Returns\n        -------\n        consumption_time : float or np.NAN\n            Estimated time needed by the consumer to process a sample, in\n            seconds. Until enough samples have been consumed to accurately\n            estimate consumption time, it is set to np.NAN.\n        """"""\n        if len(self.consumption_time_) < max(10, 2 * self.prefetch):\n            return np.NAN\n        return np.median(self.consumption_time_)\n\n    def run(self) -> None:\n        """"""Called by self.start(), should not be called directly.""""""\n\n        # keep going until the background generator is deactivated\n        while self.activated_:\n\n            # produce a new sample\n            _t = time.time()\n            try:\n                sample = next(self.producer_)\n            except StopIteration:\n                sample = None\n\n            # keep track of how long it took to produce\n            self.production_time_.append(time.time() - _t)\n\n            # put the new sample into the queue for later consumption.\n            # note that this line is blocking when the queue is full.\n            # calling self.queue_.get() in self.__next__() or self.deactivate()\n            # will eventually unblock it.\n            self.queue_.put(sample)\n\n    def __next__(self):\n        """"""Produce new sample""""""\n\n        # raise a StopIteration once the generator has been deactivated\n        if not self.activated_:\n            msg = ""Background generator is no longer active.""\n            raise StopIteration(msg)\n\n        # keep track of how long it took to consume the last sample\n        t = time.time()\n        if self.last_ready_ is not None:\n            self.consumption_time_.append(t - self.last_ready_)\n\n        # get a new sample from the queue\n        sample = self.queue_.get()\n\n        # this happens when producer stopped yielding samples\n        if sample is None:\n            msg = ""Producer stopped yielding samples.""\n            raise StopIteration(msg)\n\n        # keep track of the last time a sample was taken from the queue\n        self.last_ready_ = time.time()\n\n        # actually return the new sample\n        return sample\n\n    def __iter__(self):\n        return self\n\n\nclass AdaptiveBackgroundGenerator:\n    """"""Adaptive pool of background generators\n\n    The pool is initialized with only one background generator.\n\n    Once production and consumption time estimates are available (after a short\n    warm-up period of time), the pool will incrementally adapt the number of\n    background generators to ensure that it produces samples fast enough for\n    the consumer.\n\n    Parameters\n    ----------\n    producer: generator function\n        Generator function that takes no argument and yield (a possibly\n        infinite number of) samples. This would typically be a BatchGenerator\n        instance but can be any function that ""yields"" samples.\n    n_jobs : int, optional\n        Maximum number of background generators that can be created to keep up\n        with consumer. Defaults to 4.\n    prefetch : int, optional\n        Maximum number of samples that can be prefetched by each background\n        generator. See BackgroundGenerator documentation for more details.\n        Defaults to 10.\n    verbose : bool, optional\n        Print a message when a background generator is added to (or removed\n        from) the pool.\n\n    Usage\n    -----\n    >>> import time\n\n    # A producer that takes 5ms to produce a new sample\n    >>> def producer():\n    ...     while True:\n    ...         time.sleep(0.005)\n    ...         yield \'data\'\n\n    # A slow consumer that takes 5ms to consume a sample\n    >>> def slow_consumer(data): time.sleep(0.005)\n\n    # A fast consumer that takes 1ms to consume a sample\n    >>> def fast_consumer(data): time.sleep(0.001)\n\n    # send producer to the background and allows for at most 6 threads\n    >>> generator = AdaptiveBackgroundGenerator(producer, n_jobs=6)\n\n    >>> for _ in range(1000): fast_consumer(next(generator))\n    >>> print(f\'When consumer is fast, generator uses {len(generator)} thread(s).\')\n    # prints: ""When consumer is fast, generator uses 4 thread(s).""\n\n    >>> for _ in range(1000): slow_consumer(next(generator))\n    >>> print(f\'When consumer is slow, generator uses {len(generator)} thread(s).\')\n    # prints: ""When consumer is slow, generator uses 1 thread(s).""\n\n    # deactivate generator (and stop background threads)\n    >>> generator.deactivate()\n    >>> _ = next(generator)\n    # raises: ""StopIteration: Background generator is no longer active.""\n    """"""\n\n    def __init__(\n        self,\n        producer: Callable[[], Iterator],\n        n_jobs: int = 4,\n        prefetch: int = 10,\n        verbose: bool = False,\n    ):\n\n        self.producer = producer\n        self.n_jobs = n_jobs\n        self.prefetch = prefetch\n        self.verbose = verbose\n\n        # current pool of active background generators\n        self.generators_ = []\n\n        if self.verbose:\n            msg = f""Starting with one producer.""\n            print(msg)\n\n        # start by creating one background generator to the pool\n        self._add_generator()\n\n        # initialize main sample generator (used in __next__)\n        self.samples_ = self._sample()\n\n        # set to True once the maximum number of generators is reached.\n        # (used to avoid repeating a message when verbose is True)\n        self.reached_max_ = False\n\n    def deactivate(self) -> None:\n        """"""Stop background generator""""""\n        n_jobs = len(self.generators_)\n        for _ in range(n_jobs):\n            self._remove_generator()\n\n    def _add_generator(self) -> None:\n        """"""Add one more producer to the pool""""""\n\n        self.generators_.append(\n            BackgroundGenerator(self.producer, prefetch=self.prefetch)\n        )\n\n        for g in self.generators_:\n            g.reset()\n\n    def _remove_generator(self, index: int = None) -> None:\n        """"""Remove one producer from the pool\n\n        Parameters\n        ----------\n        index : int, optional\n            When provided, remove `index`th producer.\n            Defaults to removing the last producer.\n        """"""\n\n        if index is None:\n            n_jobs = len(self.generators_)\n            index = n_jobs - 1\n\n        g = self.generators_.pop(index)\n        g.deactivate()\n\n        for g in self.generators_:\n            g.reset()\n\n        self.reached_max_ = False\n\n    def __len__(self):\n        """"""Return current number of producers""""""\n        return len(self.generators_)\n\n    @property\n    def consumption_time(self) -> float:\n        """"""Estimated time needed by the consumer to process a sample\n\n        This is computed as the average of estimated consumption times of all\n        currently active background generators.\n\n        Returns\n        -------\n        consumption_time : float or np.NAN\n            Estimated time needed by the consumer to process a sample, in\n            seconds. Until enough samples have been consumed to accurately\n            estimate consumption time, it is set to np.NAN.\n        """"""\n\n        # corner case when generator has been deactivated\n        if not self.generators_:\n            return np.NAN\n\n        return np.mean([g.consumption_time for g in self.generators_])\n\n    @property\n    def production_time(self) -> float:\n        """"""Estimated time needed by the generator to yield a sample.\n\n        This is computed as the average estimated production time of all\n        currently active background generators.\n\n        Returns\n        -------\n        production_time : float or np.NAN\n            Estimated time needed by the generator to yield a new sample, in\n            seconds. Until enough samples have been yielded to accurately\n            estimate production time, it is set to np.NAN.\n        """"""\n\n        # corner case when generator has been deactivated\n        if not self.generators_:\n            return np.NAN\n\n        return np.mean([g.production_time for g in self.generators_])\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        return next(self.samples_)\n\n    def _sample(self) -> Iterator:\n        """"""Iterate over (and manage) pool of generators""""""\n\n        # loop forever\n        while True:\n\n            if not self.generators_:\n                msg = ""Background generator is no longer active.""\n                raise StopIteration(msg)\n\n            dead_generators = []\n            for index, g in enumerate(self.generators_):\n\n                try:\n                    sample = next(g)\n                except StopIteration:\n                    # mark this generator as dead\n                    dead_generators.append(index)\n                    continue\n\n                yield sample\n\n            if self.verbose and dead_generators:\n                msg = f""Replacing {len(dead_generators)} exhausted producers.""\n                print(msg)\n\n            # replace dead generators by new ones\n            for index in reversed(dead_generators):\n                self._remove_generator(index=index)\n            for _ in dead_generators:\n                self._add_generator()\n\n            consumption_time = self.consumption_time\n            production_time = self.production_time\n\n            if np.isnan(consumption_time) or np.isnan(production_time):\n                continue\n\n            n_jobs = len(self.generators_)\n            ratio = production_time / consumption_time\n\n            # consumption_time < production_time\n            if ratio > 1:\n                if n_jobs < self.n_jobs:\n\n                    if self.verbose:\n                        msg = (\n                            f""Adding one producer because consumer is ""\n                            f""{ratio:.2f}x faster than current {n_jobs:d} ""\n                            f""producer(s).""\n                        )\n                        print(msg)\n\n                    self._add_generator()\n\n                else:\n                    if not self.reached_max_ and self.verbose:\n                        msg = (\n                            f""Consumer is {ratio:.2f}x faster than the pool of ""\n                            f""{n_jobs:d} producer(s) but the maximum number of ""\n                            f""producers has been reached.""\n                        )\n                        print(msg)\n\n                    self.reached_max_ = True\n\n            # production_time < consumption_time * (n_jobs - 1) / n_jobs\n            elif (ratio < (n_jobs - 1) / n_jobs) and n_jobs > 1:\n\n                if self.verbose:\n                    msg = (\n                        f""Removing one producer because consumer is ""\n                        f""{1 / ratio:.2f}x slower than current {n_jobs:d} ""\n                        f""producer(s).""\n                    )\n                    print(msg)\n\n                self._remove_generator()\n'"
pyannote/audio/utils/path.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2016-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\nimport os\nimport errno\nfrom pathlib import Path\nfrom typing import Text\nfrom typing import Union\n\n\ndef mkdir_p(path: Union[Text, Path]):\n    """"""Create directory and all its parents if they do not exist\n\n    This is the equivalent of Unix \'mkdir -p path\'\n\n    Parameter\n    ---------\n    path : str\n        Path to new directory.\n\n    Reference\n    ---------\n    http://stackoverflow.com/questions/600268/mkdir-p-functionality-in-python\n    """"""\n\n    try:\n        os.makedirs(path)\n    except OSError as exc:  # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else:\n            raise exc\n'"
pyannote/audio/utils/signal.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2016-2018 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n""""""\n# Signal processing\n""""""\n\n\nimport numpy as np\nimport scipy.signal\nfrom pyannote.core import Segment, Timeline\nfrom pyannote.core.utils.generators import pairwise\nfrom sklearn.mixture import GaussianMixture\nfrom pyannote.core.utils.numpy import one_hot_decoding\n\n\nclass Peak(object):\n    """"""Peak detection\n\n    Parameters\n    ----------\n    alpha : float, optional\n        Adaptative threshold coefficient. Defaults to 0.5\n    scale : {\'absolute\', \'relative\', \'percentile\'}\n        Set to \'relative\' to make onset/offset relative to min/max.\n        Set to \'percentile\' to make them relative 1% and 99% percentiles.\n        Defaults to \'absolute\'.\n    min_duration : float, optional\n        Defaults to 1 second.\n    log_scale : bool, optional\n        Set to True to indicate that binarized scores are log scaled.\n        Defaults to False.\n\n    """"""\n\n    def __init__(self, alpha=0.5, min_duration=1.0, scale=""absolute"", log_scale=False):\n        super(Peak, self).__init__()\n        self.alpha = alpha\n        self.scale = scale\n        self.min_duration = min_duration\n        self.log_scale = log_scale\n\n    def apply(self, predictions, dimension=0):\n        """"""Peak detection\n\n        Parameter\n        ---------\n        predictions : SlidingWindowFeature\n            Predictions returned by segmentation approaches.\n\n        Returns\n        -------\n        segmentation : Timeline\n            Partition.\n        """"""\n\n        if len(predictions.data.shape) == 1:\n            y = predictions.data\n        elif predictions.data.shape[1] == 1:\n            y = predictions.data[:, 0]\n        else:\n            y = predictions.data[:, dimension]\n\n        if self.log_scale:\n            y = np.exp(y)\n\n        sw = predictions.sliding_window\n\n        precision = sw.step\n        order = max(1, int(np.rint(self.min_duration / precision)))\n        indices = scipy.signal.argrelmax(y, order=order)[0]\n\n        if self.scale == ""absolute"":\n            mini = 0\n            maxi = 1\n\n        elif self.scale == ""relative"":\n            mini = np.nanmin(y)\n            maxi = np.nanmax(y)\n\n        elif self.scale == ""percentile"":\n            mini = np.nanpercentile(y, 1)\n            maxi = np.nanpercentile(y, 99)\n\n        threshold = mini + self.alpha * (maxi - mini)\n\n        peak_time = np.array([sw[i].middle for i in indices if y[i] > threshold])\n\n        n_windows = len(y)\n        start_time = sw[0].start\n        end_time = sw[n_windows].end\n\n        boundaries = np.hstack([[start_time], peak_time, [end_time]])\n        segmentation = Timeline()\n        for i, (start, end) in enumerate(pairwise(boundaries)):\n            segment = Segment(start, end)\n            segmentation.add(segment)\n\n        return segmentation\n\n\nclass Binarize(object):\n    """"""Binarize predictions using onset/offset thresholding\n\n    Parameters\n    ----------\n    onset : float, optional\n        Relative onset threshold. Defaults to 0.5.\n    offset : float, optional\n        Relative offset threshold. Defaults to 0.5.\n    scale : {\'absolute\', \'relative\', \'percentile\'}\n        Set to \'relative\' to make onset/offset relative to min/max.\n        Set to \'percentile\' to make them relative 1% and 99% percentiles.\n        Defaults to \'absolute\'.\n    log_scale : bool, optional\n        Set to True to indicate that binarized scores are log scaled.\n        Will apply exponential first. Defaults to False.\n\n    Reference\n    ---------\n    Gregory Gelly and Jean-Luc Gauvain. ""Minimum Word Error Training of\n    RNN-based Voice Activity Detection"", InterSpeech 2015.\n    """"""\n\n    def __init__(\n        self,\n        onset=0.5,\n        offset=0.5,\n        scale=""absolute"",\n        log_scale=False,\n        pad_onset=0.0,\n        pad_offset=0.0,\n        min_duration_on=0.0,\n        min_duration_off=0.0,\n    ):\n\n        super(Binarize, self).__init__()\n\n        self.onset = onset\n        self.offset = offset\n        self.scale = scale\n        self.log_scale = log_scale\n\n        self.pad_onset = pad_onset\n        self.pad_offset = pad_offset\n\n        self.min_duration_on = min_duration_on\n        self.min_duration_off = min_duration_off\n\n    def apply(self, predictions, dimension=0):\n        """"""\n        Parameters\n        ----------\n        predictions : SlidingWindowFeature\n            Must be mono-dimensional\n        dimension : int, optional\n            Which dimension to process\n        """"""\n\n        if len(predictions.data.shape) == 1:\n            data = predictions.data\n        elif predictions.data.shape[1] == 1:\n            data = predictions.data[:, 0]\n        else:\n            data = predictions.data[:, dimension]\n\n        if self.log_scale:\n            data = np.exp(data)\n\n        n_samples = predictions.getNumber()\n        window = predictions.sliding_window\n        timestamps = [window[i].middle for i in range(n_samples)]\n\n        # initial state\n        start = timestamps[0]\n        label = data[0] > self.onset\n\n        if self.scale == ""absolute"":\n            mini = 0\n            maxi = 1\n\n        elif self.scale == ""relative"":\n            mini = np.nanmin(data)\n            maxi = np.nanmax(data)\n\n        elif self.scale == ""percentile"":\n            mini = np.nanpercentile(data, 1)\n            maxi = np.nanpercentile(data, 99)\n\n        onset = mini + self.onset * (maxi - mini)\n        offset = mini + self.offset * (maxi - mini)\n\n        # timeline meant to store \'active\' segments\n        active = Timeline()\n\n        for t, y in zip(timestamps[1:], data[1:]):\n\n            # currently active\n            if label:\n                # switching from active to inactive\n                if y < offset:\n                    segment = Segment(start - self.pad_onset, t + self.pad_offset)\n                    active.add(segment)\n                    start = t\n                    label = False\n\n            # currently inactive\n            else:\n                # switching from inactive to active\n                if y > onset:\n                    start = t\n                    label = True\n\n        # if active at the end, add final segment\n        if label:\n            segment = Segment(start - self.pad_onset, t + self.pad_offset)\n            active.add(segment)\n\n        # because of padding, some \'active\' segments might be overlapping\n        # therefore, we merge those overlapping segments\n        active = active.support()\n\n        # remove short \'active\' segments\n        active = Timeline([s for s in active if s.duration > self.min_duration_on])\n\n        # fill short \'inactive\' segments\n        inactive = active.gaps()\n        for s in inactive:\n            if s.duration < self.min_duration_off:\n                active.add(s)\n        active = active.support()\n\n        return active\n\n\nclass GMMResegmentation(object):\n    """"""\n    Parameters\n    ----------\n    n_components : int, optional\n        Number of Gaussian components of the GMMs. Defaults to 128.\n    n_iter : int, optional\n        Number of EM iterations to train the models. Defaults to 10.\n    window : float, optional\n        Duration of the smoothing window. Defaults to 1 second.\n\n    Note\n    ----\n    Recommended feature extraction is to use LibrosaMFCC with 19 static MFCCs\n    (no energy nor zero-coefficient)\n    >>> feature_extraction = LibrosaMFCC(duration=0.025, step=0.01,\n                                         e=False, De=False, DDe=False,\n                                         coefs=19, D=False, DD=False,\n                                         fmin=0.0, fmax=None, n_mels=40)\n\n    TODO: add option to also resegment speech/non-speech\n\n    """"""\n\n    def __init__(self, n_components=128, n_iter=10, window=1.0):\n        super().__init__()\n        self.n_components = n_components\n        self.n_iter = n_iter\n        self.window = window\n\n    def apply(self, annotation, features):\n        """"""\n\n        Parameters\n        ----------\n        annotation : `pyannote.core.Annotation`\n            Original annotation to be resegmented.\n        features : `SlidingWindowFeature`\n            Features\n\n        Returns\n        -------\n        hypothesis : `pyannote.core.Annotation`\n            Resegmented annotation.\n\n        """"""\n\n        sliding_window = features.sliding_window\n        window = np.ones((1, sliding_window.samples(self.window)))\n\n        log_probs = []\n        labels = annotation.labels()\n\n        # FIXME: embarrasingly parallel\n        for label in labels:\n\n            # gather all features for current label\n            span = annotation.label_timeline(label)\n            data = features.crop(span, mode=""center"")\n\n            # train a GMM\n            gmm = GaussianMixture(\n                n_components=self.n_components,\n                covariance_type=""diag"",\n                tol=0.001,\n                reg_covar=1e-06,\n                max_iter=self.n_iter,\n                n_init=1,\n                init_params=""kmeans"",\n                weights_init=None,\n                means_init=None,\n                precisions_init=None,\n                random_state=None,\n                warm_start=False,\n                verbose=0,\n                verbose_interval=10,\n            ).fit(data)\n\n            # compute log-probability across the whole file\n            log_prob = gmm.score_samples(features.data)\n            log_probs.append(log_prob)\n\n        # smooth log-probability using a sliding window\n        log_probs = scipy.signal.convolve(np.vstack(log_probs), window, mode=""same"")\n\n        # assign each frame to the most likely label\n        y = np.argmax(log_probs, axis=0)\n\n        # reconstruct the annotation\n        hypothesis = one_hot_decoding(y, sliding_window, labels=labels)\n\n        # remove original non-speech regions\n        return hypothesis.crop(annotation.get_timeline().support())\n'"
pyannote/audio/utils/timeout.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# Shamelessly stolen from\n# https://gist.github.com/TySkby/143190ad1b88c6115597c45f996b030c\n\n""""""Easily put time restrictions on things\n\nNote: Requires Python 3.x\n\nUsage as a context manager:\n```\nwith timeout(10):\n    something_that_should_not_exceed_ten_seconds()\n```\n\nUsage as a decorator:\n```\n@timeout(10)\ndef something_that_should_not_exceed_ten_seconds():\n    do_stuff_with_a_timeout()\n```\n\nHandle timeouts:\n```\ntry:\n   with timeout(10):\n       something_that_should_not_exceed_ten_seconds()\n   except TimeoutError:\n       log(\'Got a timeout, couldn\'t finish\')\n```\n\nSuppress TimeoutError and just die after expiration:\n```\nwith timeout(10, suppress_timeout_errors=True):\n    something_that_should_not_exceed_ten_seconds()\n\nprint(\'Maybe exceeded 10 seconds, but finished either way\')\n```\n""""""\nimport contextlib\nimport errno\nimport os\nimport signal\n\n\nDEFAULT_TIMEOUT_MESSAGE = os.strerror(errno.ETIME)\n\n\nclass timeout(contextlib.ContextDecorator):\n    def __init__(\n        self,\n        seconds,\n        *,\n        timeout_message=DEFAULT_TIMEOUT_MESSAGE,\n        suppress_timeout_errors=False\n    ):\n        self.seconds = int(seconds)\n        self.timeout_message = timeout_message\n        self.suppress = bool(suppress_timeout_errors)\n\n    def _timeout_handler(self, signum, frame):\n        raise TimeoutError(self.timeout_message)\n\n    def __enter__(self):\n        signal.signal(signal.SIGALRM, self._timeout_handler)\n        signal.alarm(self.seconds)\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        signal.alarm(0)\n        if self.suppress and exc_type is TimeoutError:\n            return True\n'"
pyannote/audio/embedding/approaches/__init__.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2017-2019 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n# Juan Manuel CORIA\n\n\nfrom .classification import Classification\nfrom .triplet_loss import TripletLoss\nfrom .arcface_loss import AdditiveAngularMarginLoss\nfrom .coco_loss import CongenerousCosineLoss\nfrom .contrastive_loss import ContrastiveLoss\nfrom .center_loss import CenterLoss\n'"
pyannote/audio/embedding/approaches/arcface_loss.py,10,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2019-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n# Juan Manuel CORIA - https://juanmc2005.github.io\n\nimport math\nimport warnings\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .classification import Classification\n\n\nclass ArcLinear(nn.Module):\n    """"""Additive Angular Margin classification module\n\n    Parameters\n    ----------\n    nfeat : int\n        Embedding dimension\n    nclass : int\n        Number of classes\n    margin : float\n        Angular margin to penalize distances between embeddings and centers\n    scale : float\n        Scaling factor for the logits\n    """"""\n\n    def __init__(self, nfeat, nclass, margin, scale):\n        super(ArcLinear, self).__init__()\n        eps = 1e-4\n        self.min_cos = eps - 1\n        self.max_cos = 1 - eps\n        self.nclass = nclass\n        self.margin = margin\n        self.scale = scale\n        self.W = nn.Parameter(torch.Tensor(nclass, nfeat))\n        nn.init.xavier_uniform_(self.W)\n\n    def forward(self, x, target=None):\n        """"""Apply the angular margin transformation\n\n        Parameters\n        ----------\n        x : `torch.Tensor`\n            an embedding batch\n        target : `torch.Tensor`\n            a non one-hot label batch\n\n        Returns\n        -------\n        fX : `torch.Tensor`\n            logits after the angular margin transformation\n        """"""\n        # normalize the feature vectors and W\n        xnorm = F.normalize(x)\n        Wnorm = F.normalize(self.W)\n        target = target.long().view(-1, 1)\n        # calculate cos\xce\xb8j (the logits)\n        cos_theta_j = torch.matmul(xnorm, torch.transpose(Wnorm, 0, 1))\n        # get the cos\xce\xb8 corresponding to the classes\n        cos_theta_yi = cos_theta_j.gather(1, target)\n        # for numerical stability\n        cos_theta_yi = cos_theta_yi.clamp(min=self.min_cos, max=self.max_cos)\n        # get the angle separating xi and Wyi\n        theta_yi = torch.acos(cos_theta_yi)\n        # apply the margin to the angle\n        cos_theta_yi_margin = torch.cos(theta_yi + self.margin)\n        # one hot encode  y\n        one_hot = torch.zeros_like(cos_theta_j)\n        one_hot.scatter_(1, target, 1.0)\n        # project margin differences into cos\xce\xb8j\n        return self.scale * (cos_theta_j + one_hot * (cos_theta_yi_margin - cos_theta_yi))\n\n\nclass AdditiveAngularMarginLoss(Classification):\n    """"""Additive angular margin loss\n\n    Penalize the distance between an example and its class center\n    with an angular margin, applying a scaling parameter to all logits.\n    The intuition is to bring examples from a single class closer together\n    in the embedding space by means of the angular margin.\n\n    Parameters\n    ----------\n    duration : float, optional\n        Chunks duration, in seconds. Defaults to 1.\n    min_duration : float, optional\n        When provided, use chunks of random duration between `min_duration` and\n        `duration` for training. Defaults to using fixed duration chunks.\n    per_turn : int, optional\n        Number of chunks per speech turn. Defaults to 1.\n        If per_turn is greater than one, embeddings of the same speech turn\n        are averaged before classification. The intuition is that it might\n        help learn embeddings meant to be averaged/summed.\n    per_label : `int`, optional\n        Number of sequences per speaker in each batch. Defaults to 1.\n    per_fold : `int`, optional\n        Number of different speakers per batch. Defaults to 32.\n    per_epoch : `float`, optional\n        Force total audio duration per epoch, in days.\n        Defaults to total duration of protocol subset.\n    label_min_duration : `float`, optional\n        Remove speakers with less than that many seconds of speech.\n        Defaults to 0 (i.e. keep them all).\n    margin : float, optional\n        Angular margin value. Defaults to 0.1.\n    scale : float, optional\n        Scaling parameter value for the logits. Defaults to sqrt(2) * log(n_classes - 1).\n\n    Reference\n    ---------\n    ArcFace: Additive Angular Margin Loss for Deep Face Recognition\n    http://openaccess.thecvf.com/content_CVPR_2019/html/Deng_ArcFace_Additive_Angular_Margin_Loss_for_Deep_Face_Recognition_CVPR_2019_paper.html\n    """"""\n\n    def __init__(\n        self,\n        duration: float = 1.0,\n        min_duration: float = None,\n        per_turn: int = 1,\n        per_label: int = 1,\n        per_fold: int = 32,\n        per_epoch: float = None,\n        label_min_duration: float = 0.0,\n        margin: float = 0.1,\n        # `s` is deprecated in favor of `scale`\n        s: float = None,\n        scale: float = None\n    ):\n\n        super().__init__(\n            duration=duration,\n            min_duration=min_duration,\n            per_turn=per_turn,\n            per_label=per_label,\n            per_fold=per_fold,\n            per_epoch=per_epoch,\n            label_min_duration=label_min_duration,\n        )\n\n        self.margin = margin\n        if s is not None:\n            msg = ""The \'s\' parameter is deprecated in favor of \'scale\', "" \\\n                  ""and will be removed in a future release""\n            warnings.warn(msg, FutureWarning)\n            self.scale = s\n        else:\n            self.scale = scale\n\n    def more_parameters(self):\n        """"""Initialize classifier layer\n\n        Yields\n        ------\n        parameter : nn.Parameter\n            Parameters\n        """"""\n\n        nclass = len(self.specifications[""y""][""classes""])\n        # Use scaling initialization trick from AdaCos\n        # Reference: https://arxiv.org/abs/1905.00292\n        scale = math.sqrt(2) * math.log(nclass - 1) if self.scale is None else self.scale\n\n        self.classifier_ = ArcLinear(\n            self.model.dimension,\n            nclass,\n            self.margin,\n            scale,\n        ).to(self.device)\n\n        return self.classifier_.parameters()\n'"
pyannote/audio/embedding/approaches/base.py,9,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2019-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\nimport torch\nimport torch.nn.functional as F\nfrom pyannote.audio.train.trainer import Trainer\nimport numpy as np\n\nfrom typing import Text\nfrom typing import Optional\nfrom pyannote.audio.embedding.generators import SpeechSegmentGenerator\nfrom pyannote.audio.features import FeatureExtraction\nfrom pyannote.database.protocol.protocol import Protocol\nfrom pyannote.audio.features.wrapper import Wrappable\nfrom pyannote.audio.train.task import Task, TaskType, TaskOutput\n\n\nclass RepresentationLearning(Trainer):\n    """"""\n\n    Parameters\n    ----------\n    duration : float, optional\n        Chunks duration, in seconds. Defaults to 1.\n    min_duration : float, optional\n        When provided, use chunks of random duration between `min_duration` and\n        `duration` for training. Defaults to using fixed duration chunks.\n    per_turn : int, optional\n        Number of chunks per speech turn. Defaults to 1.\n        If per_turn is greater than one, embeddings of the same speech turn\n        are averaged before classification. The intuition is that it might\n        help learn embeddings meant to be averaged/summed.\n    per_label : `int`, optional\n        Number of sequences per speaker in each batch. Defaults to 1.\n    per_fold : `int`, optional\n        Number of different speakers per batch. Defaults to 32.\n    per_epoch : `float`, optional\n        Force total audio duration per epoch, in days.\n        Defaults to total duration of protocol subset.\n    label_min_duration : `float`, optional\n        Remove speakers with less than that many seconds of speech.\n        Defaults to 0 (i.e. keep them all).\n    """"""\n\n    def __init__(\n        self,\n        duration: float = 1.0,\n        min_duration: float = None,\n        per_turn: int = 1,\n        per_label: int = 1,\n        per_fold: Optional[int] = None,\n        per_epoch: Optional[float] = None,\n        label_min_duration: float = 0.0,\n    ):\n\n        super().__init__()\n        self.duration = duration\n        self.min_duration = min_duration\n        self.per_turn = per_turn\n        self.per_label = per_label\n        self.per_fold = per_fold\n        self.per_epoch = per_epoch\n        self.label_min_duration = label_min_duration\n\n    def get_batch_generator(\n        self,\n        feature_extraction: Wrappable,\n        protocol: Protocol,\n        subset: Text = ""train"",\n        **kwargs\n    ) -> SpeechSegmentGenerator:\n        """"""Get batch generator\n\n        Parameters\n        ----------\n        feature_extraction : `FeatureExtraction`\n        protocol : `Protocol`\n        subset : {\'train\', \'development\', \'test\'}, optional\n\n        Returns\n        -------\n        generator : `SpeechSegmentGenerator`\n        """"""\n\n        return SpeechSegmentGenerator(\n            feature_extraction,\n            protocol,\n            subset=subset,\n            duration=self.duration,\n            min_duration=self.min_duration,\n            per_turn=self.per_turn,\n            per_label=self.per_label,\n            per_fold=self.per_fold,\n            per_epoch=self.per_epoch,\n            label_min_duration=self.label_min_duration,\n        )\n\n    @property\n    def max_distance(self):\n        if self.metric == ""cosine"":\n            return 2.0\n        elif self.metric == ""angular"":\n            return np.pi\n        elif self.metric == ""euclidean"":\n            # FIXME. incorrect if embedding are not unit-normalized\n            return 2.0\n        else:\n            msg = ""\'metric\' must be one of {\'euclidean\', \'cosine\', \'angular\'}.""\n            raise ValueError(msg)\n\n    def pdist(self, fX):\n        """"""Compute pdist \xc3\xa0-la scipy.spatial.distance.pdist\n\n        Parameters\n        ----------\n        fX : (n, d) torch.Tensor\n            Embeddings.\n\n        Returns\n        -------\n        distances : (n * (n-1) / 2,) torch.Tensor\n            Condensed pairwise distance matrix\n        """"""\n\n        if self.metric == ""euclidean"":\n            return F.pdist(fX)\n\n        elif self.metric in (""cosine"", ""angular""):\n\n            distance = 0.5 * torch.pow(F.pdist(F.normalize(fX)), 2)\n            if self.metric == ""cosine"":\n                return distance\n\n            return torch.acos(torch.clamp(1.0 - distance, -1 + 1e-12, 1 - 1e-12))\n\n    def embed(self, batch):\n        """"""Extract embeddings (and aggregate per turn)\n\n        Parameters\n        ----------\n        batch : `dict`\n            [\'X\'] (batch_size, n_samples, n_features) `np.ndarray`\n            [\'y\'] (batch_size, ) `np.ndarray`\n\n        Returns\n        -------\n        fX : (batch_size / per_turn, n_dimensions) `torch.Tensor`\n        y : (batch_size / per_turn, ) `np.ndarray`\n        """"""\n\n        X = torch.tensor(batch[""X""], dtype=torch.float32, device=self.device_)\n        fX = self.model_(X)\n\n        if self.per_turn > 1:\n            # TODO. add support for other aggregation functions, e.g. replacing\n            # mean by product may encourage sparse representation\n            agg_fX = fX.view(self.per_fold * self.per_label, self.per_turn, -1).mean(\n                axis=1\n            )\n\n            agg_y = batch[""y""][:: self.per_turn]\n\n        else:\n            agg_fX = fX\n            agg_y = batch[""y""]\n\n        return agg_fX, agg_y\n\n    def to_numpy(self, tensor):\n        """"""Convert torch.Tensor to numpy array""""""\n        cpu = torch.device(""cpu"")\n        return tensor.detach().to(cpu).numpy()\n\n    @property\n    def task(self):\n        return Task(type=TaskType.REPRESENTATION_LEARNING, output=TaskOutput.VECTOR)\n'"
pyannote/audio/embedding/approaches/center_loss.py,14,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2018-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n# Juan Manuel CORIA\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom itertools import chain\nfrom .base import RepresentationLearning\nfrom .classification import Linear\nimport warnings\n\n\nclass CenterDistanceModule(nn.Module):\n    """"""Sum of embedding-to-center cosine distances\n\n    Parameters\n    ----------\n    nfeat : int\n        Embedding size\n    nclass : int\n        Number of classes\n    """"""\n\n    def __init__(self, nfeat, nclass):\n        super().__init__()\n        self.centers = nn.Parameter(torch.randn(nclass, nfeat))\n        self.nfeat = nfeat\n\n    def forward(self, feat, target=None):\n        """"""Calculate the sum of cosine distances from embeddings to centers\n\n        Parameters\n        ----------\n        feat : `torch.Tensor`\n            Embedding batch\n        target : `torch.Tensor`\n            Non one-hot labels\n        Returns\n        -------\n        distance_sum : float\n            Sum of cosine distances from embeddings to centers\n        """"""\n        batch_size = feat.size(0)\n        feat = feat.view(batch_size, -1)\n        # Select appropriate centers for this batch\'s labels\n        centers_batch = self.centers.index_select(0, target.long())\n        # Return the sum of the squared distance normalized by the batch size\n        dists = 1 - F.cosine_similarity(feat, centers_batch, dim=1, eps=1e-8)\n        return torch.sum(torch.pow(dists, 2)) / 2.0 / batch_size\n\n\nclass CenterLoss(RepresentationLearning):\n    """"""Center loss\n\n    Parameters\n    ----------\n    duration : float, optional\n        Chunks duration, in seconds. Defaults to 1.\n    min_duration : float, optional\n        When provided, use chunks of random duration between `min_duration` and\n        `duration` for training. Defaults to using fixed duration chunks.\n    per_turn : int, optional\n        Number of chunks per speech turn. Defaults to 1.\n        If per_turn is greater than one, embeddings of the same speech turn\n        are averaged before classification. The intuition is that it might\n        help learn embeddings meant to be averaged/summed.\n    per_label : `int`, optional\n        Number of sequences per speaker in each batch. Defaults to 1.\n    per_fold : `int`, optional\n        Number of different speakers per batch. Defaults to 32.\n    per_epoch : `float`, optional\n        Force total audio duration per epoch, in days.\n        Defaults to total duration of protocol subset.\n    label_min_duration : `float`, optional\n        Remove speakers with less than that many seconds of speech.\n        Defaults to 0 (i.e. keep them all).\n    loss_weight : float, optional\n        Lambda parameter controlling the effect of center distance in the loss value.\n        Defaults to 1.\n    """"""\n\n    CLASSIFIER_PT = ""{train_dir}/weights/{epoch:04d}.classifier.pt""\n    CENTERS_PT = ""{train_dir}/weights/{epoch:04d}.centers.pt""\n\n    def __init__(\n        self,\n        duration: float = 1.0,\n        min_duration: float = None,\n        per_turn: int = 1,\n        per_label: int = 1,\n        per_fold: int = 32,\n        per_epoch: float = None,\n        label_min_duration: float = 0.0,\n        loss_weight: float = 1.0,\n    ):\n\n        super().__init__(\n            duration=duration,\n            min_duration=min_duration,\n            per_turn=per_turn,\n            per_label=per_label,\n            per_fold=per_fold,\n            per_epoch=per_epoch,\n            label_min_duration=label_min_duration,\n        )\n\n        self.loss_weight = loss_weight\n        self.logsoftmax_ = nn.LogSoftmax(dim=1)\n        self.nll_ = nn.NLLLoss()\n\n    def more_parameters(self):\n        """"""Initialize trainable trainer parameters\n\n        Yields\n        ------\n        parameter : nn.Parameter\n            Trainable trainer parameters\n        """"""\n\n        n_classes = len(self.specifications[""y""][""classes""])\n        self.classifier_ = Linear(self.model.dimension, n_classes, bias=False).to(\n            self.device\n        )\n        self.center_dist_ = CenterDistanceModule(self.model.dimension, n_classes).to(\n            self.device\n        )\n\n        return chain(self.classifier_.parameters(), self.center_dist_.parameters())\n\n    def load_more(self, model_pt=None) -> bool:\n        """"""Load classifier and centers from disk\n\n        Returns\n        -------\n        success : bool\n            True if state was loaded successfully, False otherwise.\n\n        """"""\n\n        if model_pt is None:\n            classifier_pt = self.CLASSIFIER_PT.format(\n                train_dir=self.train_dir_, epoch=self.epoch_\n            )\n            centers_pt = self.CENTERS_PT.format(\n                train_dir=self.train_dir_, epoch=self.epoch_\n            )\n\n        else:\n            classifier_pt = model_pt.with_suffix("".classifier.pt"")\n            centers_pt = model_pt.with_suffix("".centers.pt"")\n\n        try:\n            classifier_state = torch.load(\n                classifier_pt, map_location=lambda storage, loc: storage\n            )\n            self.classifier_.load_state_dict(classifier_state)\n\n            centers_state = torch.load(\n                centers_pt, map_location=lambda storage, loc: storage\n            )\n            self.center_dist_.load_state_dict(centers_state)\n\n            success = True\n        except Exception as e:\n            msg = (\n                f""Did not load classifier and center states (most likely because current ""\n                f""training session uses a different training set than the one ""\n                f""used for pre-training).""\n            )\n            warnings.warn(msg)\n            success = False\n\n        return success\n\n    def save_more(self):\n        """"""Save classifier and centers to disk""""""\n\n        classifier_pt = self.CLASSIFIER_PT.format(\n            train_dir=self.train_dir_, epoch=self.epoch_\n        )\n        centers_pt = self.CENTERS_PT.format(\n            train_dir=self.train_dir_, epoch=self.epoch_\n        )\n\n        torch.save(self.classifier_.state_dict(), classifier_pt)\n        torch.save(self.center_dist_.state_dict(), centers_pt)\n\n    @property\n    def metric(self):\n        return ""cosine""\n\n    def batch_loss(self, batch):\n        """"""Compute loss for current `batch`\n\n        Parameters\n        ----------\n        batch : `dict`\n            [\'X\'] (`numpy.ndarray`)\n            [\'y\'] (`numpy.ndarray`)\n\n        Returns\n        -------\n        batch_loss : `dict`\n            [\'loss\'] (`torch.Tensor`) :\n            [\'loss_classification\'] (`torch.Tensor`) :\n            [\'loss_centers\'] (`torch.Tensor`) :\n        """"""\n\n        # extract and aggregate embeddings\n        fX, y = self.embed(batch)\n        target = torch.tensor(y, dtype=torch.int64, device=self.device)\n\n        # distance to centers\n        loss_centers = self.center_dist_(fX, target=target)\n\n        # apply classification layer\n        logits = self.logsoftmax_(self.classifier_(fX, target=target))\n\n        # compute classification loss\n        loss_classification = self.nll_(logits, target)\n\n        return {\n            ""loss"": loss_classification + self.loss_weight * loss_centers,\n            # add this for Tensorboard comparison with other compound losses\n            ""loss_classification"": loss_classification,\n            ""loss_center"": loss_centers,\n        }\n'"
pyannote/audio/embedding/approaches/classification.py,5,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2019-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\nimport warnings\nimport torch\nimport torch.nn as nn\nfrom .base import RepresentationLearning\n\n\nclass Linear(nn.Linear):\n    def forward(self, x, target=None):\n        return super().forward(x)\n\n\nclass Classification(RepresentationLearning):\n    """"""Classification\n\n    TODO explain\n\n    Parameters\n    ----------\n    duration : float, optional\n        Chunks duration, in seconds. Defaults to 1.\n    min_duration : float, optional\n        When provided, use chunks of random duration between `min_duration` and\n        `duration` for training. Defaults to using fixed duration chunks.\n    per_turn : int, optional\n        Number of chunks per speech turn. Defaults to 1.\n        If per_turn is greater than one, embeddings of the same speech turn\n        are averaged before classification. The intuition is that it might\n        help learn embeddings meant to be averaged/summed.\n    per_label : `int`, optional\n        Number of sequences per speaker in each batch. Defaults to 1.\n    per_fold : `int`, optional\n        Number of different speakers per batch. Defaults to 32.\n    per_epoch : `float`, optional\n        Force total audio duration per epoch, in days.\n        Defaults to total duration of protocol subset.\n    label_min_duration : `float`, optional\n        Remove speakers with less than that many seconds of speech.\n        Defaults to 0 (i.e. keep them all).\n    bias : `bool`, optional\n        Use bias in the classification layer\n        Defaults to False.\n    """"""\n\n    # TODO. add option to see this classification step\n    #       as cosine similarity to centroids (ie center loss?)\n\n    CLASSIFIER_PT = ""{train_dir}/weights/{epoch:04d}.classifier.pt""\n\n    def __init__(\n        self,\n        duration: float = 1.0,\n        min_duration: float = None,\n        per_turn: int = 1,\n        per_label: int = 1,\n        per_fold: int = 32,\n        per_epoch: float = None,\n        label_min_duration: float = 0.0,\n        bias: bool = False,\n    ):\n\n        super().__init__(\n            duration=duration,\n            min_duration=min_duration,\n            per_turn=per_turn,\n            per_label=per_label,\n            per_fold=per_fold,\n            per_epoch=per_epoch,\n            label_min_duration=label_min_duration,\n        )\n\n        self.bias = bias\n        self.logsoftmax_ = nn.LogSoftmax(dim=1)\n        self.loss_ = nn.NLLLoss()\n\n    def more_parameters(self):\n        """"""Initialize trainable trainer parameters\n\n        Returns\n        -------\n        parameters : iterable\n            Trainable trainer parameters\n        """"""\n\n        self.classifier_ = Linear(\n            self.model.dimension,\n            len(self.specifications[""y""][""classes""]),\n            bias=self.bias,\n        ).to(self.device)\n\n        return self.classifier_.parameters()\n\n    def load_more(self, model_pt=None) -> bool:\n        """"""Load classifier from disk\n\n        Returns\n        -------\n        success : bool\n            True if state was loaded successfully, False otherwise.\n        """"""\n\n        if model_pt is None:\n            classifier_pt = self.CLASSIFIER_PT.format(\n                train_dir=self.train_dir_, epoch=self.epoch_\n            )\n        else:\n            classifier_pt = model_pt.with_suffix("".classifier.pt"")\n\n        try:\n            classifier_state = torch.load(\n                classifier_pt, map_location=lambda storage, loc: storage\n            )\n            self.classifier_.load_state_dict(classifier_state)\n            success = True\n        except Exception as e:\n            msg = (\n                f""Did not load classifier state (most likely because current ""\n                f""training session uses a different training set than the one ""\n                f""used for pre-training).""\n            )\n            warnings.warn(msg)\n            success = False\n\n        return success\n\n    def save_more(self):\n        """"""Save classifier weights to disk""""""\n\n        classifier_pt = self.CLASSIFIER_PT.format(\n            train_dir=self.train_dir_, epoch=self.epoch_\n        )\n        torch.save(self.classifier_.state_dict(), classifier_pt)\n\n    @property\n    def metric(self):\n        return ""cosine""\n\n    def batch_loss(self, batch):\n        """"""Compute loss for current `batch`\n\n        Parameters\n        ----------\n        batch : `dict`\n            [\'X\'] (`numpy.ndarray`)\n            [\'y\'] (`numpy.ndarray`)\n\n        Returns\n        -------\n        batch_loss : `dict`\n            [\'loss\'] (`torch.Tensor`) : Cross-entropy loss\n        """"""\n\n        # extract and aggregate embeddings\n        fX, y = self.embed(batch)\n        target = torch.tensor(y, dtype=torch.int64, device=self.device)\n\n        # apply classification layer\n        logits = self.logsoftmax_(self.classifier_(fX, target=target))\n\n        # compute classification loss\n        loss_classification = self.loss_(logits, target)\n\n        return {\n            ""loss"": loss_classification,\n            # add this for Tensorboard comparison with other compound losses\n            ""loss_classification"": loss_classification,\n        }\n'"
pyannote/audio/embedding/approaches/coco_loss.py,5,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2019-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n# Juan Manuel CORIA - https://juanmc2005.github.io\n\nimport math\nimport warnings\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom .classification import Classification\n\n\nclass CocoLinear(nn.Module):\n    """"""Congenerous Cosine linear module (for CoCo loss)\n\n    Parameters\n    ----------\n    nfeat : int\n        Embedding dimension\n    nclass : int\n        Number of classes\n    scale : float\n        Scaling factor used in embedding L2-normalization\n    """"""\n\n    def __init__(self, nfeat, nclass, scale):\n        super(CocoLinear, self).__init__()\n        self.scale = scale\n        self.centers = nn.Parameter(torch.randn(nclass, nfeat))\n\n    def forward(self, x, target=None):\n        """"""Apply the angular margin transformation\n\n        Parameters\n        ----------\n        x : `torch.Tensor`\n            an embedding batch\n\n        Returns\n        -------\n        fX : `torch.Tensor`\n            logits after the congenerous cosine transformation\n        """"""\n        # normalize centers\n        cnorm = F.normalize(self.centers)\n        # normalize scaled embeddings\n        xnorm = self.scale * F.normalize(x)\n        # calculate logits like in `nn.Linear`\n        logits = torch.matmul(xnorm, torch.transpose(cnorm, 0, 1))\n        return logits\n\n\nclass CongenerousCosineLoss(Classification):\n    """"""Congenerous cosine loss\n\n    Train embeddings by maximizing the cosine similarity between\n    an embedding and its class center.\n    A hyper-parameter `alpha` is used to scale logits before\n    applying softmax.\n\n    Parameters\n    ----------\n    duration : float, optional\n        Chunks duration, in seconds. Defaults to 1.\n    min_duration : float, optional\n        When provided, use chunks of random duration between `min_duration` and\n        `duration` for training. Defaults to using fixed duration chunks.\n    per_turn : int, optional\n        Number of chunks per speech turn. Defaults to 1.\n        If per_turn is greater than one, embeddings of the same speech turn\n        are averaged before classification. The intuition is that it might\n        help learn embeddings meant to be averaged/summed.\n    per_label : `int`, optional\n        Number of sequences per speaker in each batch. Defaults to 1.\n    per_fold : `int`, optional\n        Number of different speakers per batch. Defaults to 32.\n    per_epoch : `float`, optional\n        Force total audio duration per epoch, in days.\n        Defaults to total duration of protocol subset.\n    label_min_duration : `float`, optional\n        Remove speakers with less than that many seconds of speech.\n        Defaults to 0 (i.e. keep them all).\n    scale : float\n        Scaling factor used in embedding L2-normalization. Defaults to sqrt(2) * log(n_classes - 1).\n\n    Reference\n    ---------\n    Rethinking Feature Discrimination and Polymerization for Large-scale Recognition\n    https://arxiv.org/abs/1710.00870\n    """"""\n\n    def __init__(\n        self,\n        duration: float = 1.0,\n        min_duration: float = None,\n        per_turn: int = 1,\n        per_label: int = 1,\n        per_fold: int = 32,\n        per_epoch: float = None,\n        label_min_duration: float = 0.0,\n        # `alpha` is deprecated in favor of `scale`\n        alpha: float = None,\n        scale: float = None\n    ):\n\n        super().__init__(\n            duration=duration,\n            min_duration=min_duration,\n            per_turn=per_turn,\n            per_label=per_label,\n            per_fold=per_fold,\n            per_epoch=per_epoch,\n            label_min_duration=label_min_duration,\n        )\n\n        if alpha is not None:\n            msg = ""The \'alpha\' parameter is deprecated in favor of \'scale\', "" \\\n                  ""and will be removed in a future release""\n            warnings.warn(msg, FutureWarning)\n            self.scale = alpha\n        else:\n            self.scale = scale\n\n    def more_parameters(self):\n        """"""Initialize trainable trainer parameters\n\n        Yields\n        ------\n        parameter : nn.Parameter\n            Trainable trainer parameters\n        """"""\n\n        nclass = len(self.specifications[""y""][""classes""])\n        # Use scaling initialization trick from AdaCos\n        # Reference: https://arxiv.org/abs/1905.00292\n        scale = math.sqrt(2) * math.log(nclass - 1) if self.scale is None else self.scale\n\n        self.classifier_ = CocoLinear(\n            self.model.dimension, nclass, scale\n        ).to(self.device)\n\n        return self.classifier_.parameters()\n'"
pyannote/audio/embedding/approaches/contrastive_loss.py,6,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2018-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n# Juan Manuel CORIA\n\nimport torch\nfrom .base import RepresentationLearning\n\n\nclass ContrastiveLoss(RepresentationLearning):\n    """"""Contrasitve loss\n\n    TODO explain\n\n    Parameters\n    ----------\n    duration : float, optional\n        Chunks duration, in seconds. Defaults to 1.\n    min_duration : float, optional\n        When provided, use chunks of random duration between `min_duration` and\n        `duration` for training. Defaults to using fixed duration chunks.\n    per_turn : int, optional\n        Number of chunks per speech turn. Defaults to 1.\n        If per_turn is greater than one, embeddings of the same speech turn\n        are averaged before comparison. The intuition is that it might help\n        learn embeddings meant to be averaged/summed.\n    per_label : `int`, optional\n        Number of sequences per speaker in each batch. Defaults to 1.\n    per_fold : `int`, optional\n        Number of different speakers per batch. Defaults to 32.\n    per_epoch : `float`, optional\n        Force total audio duration per epoch, in days.\n        Defaults to total duration of protocol subset.\n    label_min_duration : `float`, optional\n        Remove speakers with less than that many seconds of speech.\n        Defaults to 0 (i.e. keep them all).\n    metric : {\'euclidean\', \'cosine\', \'angular\'}, optional\n        Defaults to \'cosine\'.\n    margin: float, optional\n        Margin multiplicative factor. Defaults to 0.2.\n\n    Reference\n    ---------\n    TODO\n    """"""\n\n    def __init__(\n        self,\n        duration: float = 1.0,\n        min_duration: float = None,\n        per_turn: int = 1,\n        per_label: int = 1,\n        per_fold: int = 32,\n        per_epoch: float = None,\n        label_min_duration: float = 0.0,\n        # FIXME create a Literal type for metric\n        # FIXME maybe in pyannote.core.utils.distance\n        metric: str = ""cosine"",\n        # FIXME homogeneize the meaning of margin parameter\n        # FIXME it has a different meaning in ArcFace, right?\n        margin: float = 0.2,\n    ):\n\n        super().__init__(\n            duration=duration,\n            min_duration=min_duration,\n            per_turn=per_turn,\n            per_label=per_label,\n            per_fold=per_fold,\n            per_epoch=per_epoch,\n            label_min_duration=label_min_duration,\n        )\n\n        self.metric = metric\n        self.margin = margin\n        # FIXME see above\n        self.margin_ = self.margin * self.max_distance\n\n    def batch_loss(self, batch):\n        """"""Compute loss for current `batch`\n\n        Parameters\n        ----------\n        batch : `dict`\n            [\'X\'] (`numpy.ndarray`)\n            [\'y\'] (`numpy.ndarray`)\n\n        Returns\n        -------\n        batch_loss : `dict`\n            [\'loss\'] (`torch.Tensor`) : Triplet loss\n        """"""\n\n        fX, y = self.embed(batch)\n\n        # calculate the distances between every sample in the batch\n        batch_size = fX.size(0)\n        dist = self.pdist(fX).to(self.device_)\n\n        # calculate the ground truth for each pair\n        # TODO. this can be done much more cleanly with\n        # pyannote.core.utils.distance.pdist(y, metric=\'equal\')\n        gt = []\n        for i in range(batch_size - 1):\n            for j in range(i + 1, batch_size):\n                gt.append(int(y[i] != y[j]))\n        gt = torch.Tensor(gt).float().to(self.device_)\n\n        # Calculate the losses as described in the paper\n        losses = (1 - gt) * torch.pow(dist, 2) + gt * torch.pow(\n            torch.clamp(self.margin_ - dist, min=1e-8), 2\n        )\n\n        # FIXME: why divive by 2?\n        losses = torch.sum(losses) / 2\n        # Average by batch size if requested\n        # FIXME: switch to torch.mean directly (size_average has been removed)\n        loss = losses / dist.size(0)\n\n        return {""loss"": loss, ""loss_contrastive"": loss}\n'"
pyannote/audio/embedding/approaches/triplet_loss.py,10,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2018-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\nfrom typing import Optional\nimport numpy as np\nimport torch\nfrom pyannote.core.utils.distance import to_condensed\nfrom scipy.spatial.distance import squareform\nfrom .base import RepresentationLearning\n\n\nclass TripletLoss(RepresentationLearning):\n    """"""Triplet loss\n\n    TODO explain\n\n    Parameters\n    ----------\n    duration : float, optional\n        Chunks duration, in seconds. Defaults to 1.\n    min_duration : float, optional\n        When provided, use chunks of random duration between `min_duration` and\n        `duration` for training. Defaults to using fixed duration chunks.\n    per_turn : int, optional\n        Number of chunks per speech turn. Defaults to 1.\n        If per_turn is greater than one, embeddings of the same speech turn\n        are averaged before comparison. The intuition is that it might help\n        learn embeddings meant to be averaged/summed.\n    per_label : `int`, optional\n        Number of sequences per speaker in each batch. Defaults to 1.\n    per_fold : `int`, optional\n        Number of different speakers per batch. Defaults to 32.\n    per_epoch : `float`, optional\n        Force total audio duration per epoch, in days.\n        Defaults to total duration of protocol subset.\n    label_min_duration : `float`, optional\n        Remove speakers with less than that many seconds of speech.\n        Defaults to 0 (i.e. keep them all).\n    metric : {\'euclidean\', \'cosine\', \'angular\'}, optional\n        Defaults to \'cosine\'.\n    margin: float, optional\n        Margin multiplicative factor. Defaults to 0.2.\n    clamp : {\'positive\', \'sigmoid\', \'softmargin\'}, optional\n        Defaults to \'positive\'.\n    sampling : {\'all\', \'hard\', \'negative\', \'easy\'}, optional\n        Triplet sampling strategy. Defaults to \'all\' (i.e. use all possible\n        triplets). \'hard\' sampling use both hardest positive and negative for\n        each anchor. \'negative\' sampling use hardest negative for each\n        (anchor, positive) pairs. \'easy\' sampling only use easy triplets (i.e.\n        those for which d(anchor, positive) < d(anchor, negative)).\n\n    Notes\n    -----\n    delta = d(anchor, positive) - d(anchor, negative)\n\n    * with \'positive\' clamping:\n        loss = max(0, delta + margin x D)\n    * with \'sigmoid\' clamping:\n        loss = sigmoid(10 * delta)\n\n    where d(., .) varies in range [0, D] (e.g. D=2 for euclidean distance).\n\n    Reference\n    ---------\n    TODO\n    """"""\n\n    def __init__(\n        self,\n        duration: float = 1.0,\n        min_duration: float = None,\n        per_turn: int = 1,\n        per_label: int = 1,\n        per_fold: int = 32,\n        per_epoch: Optional[float] = None,\n        label_min_duration: float = 0.0,\n        # FIXME create a Literal type for metric\n        # FIXME maybe in pyannote.core.utils.distance\n        metric: str = ""cosine"",\n        # FIXME homogeneize the meaning of margin parameter\n        # FIXME it has a different meaning in ArcFace, right?\n        margin: float = 0.2,\n        # FIXME create a Literal type for clamp\n        clamp=""positive"",\n        # FIXME create a Literal type for sampling\n        sampling=""all"",\n    ):\n\n        super().__init__(\n            duration=duration,\n            min_duration=min_duration,\n            per_turn=per_turn,\n            per_label=per_label,\n            per_fold=per_fold,\n            per_epoch=per_epoch,\n            label_min_duration=label_min_duration,\n        )\n\n        self.metric = metric\n        self.margin = margin\n        # FIXME see above\n        self.margin_ = self.margin * self.max_distance\n\n        if clamp not in {""positive"", ""sigmoid"", ""softmargin""}:\n            msg = ""\'clamp\' must be one of {\'positive\', \'sigmoid\', \'softmargin\'}.""\n            raise ValueError(msg)\n        self.clamp = clamp\n\n        if sampling not in {""all"", ""hard"", ""negative"", ""easy""}:\n            msg = ""\'sampling\' must be one of {\'all\', \'hard\', \'negative\', \'easy\'}.""\n            raise ValueError(msg)\n        self.sampling = sampling\n\n    def batch_easy(self, y, distances):\n        """"""Build easy triplets""""""\n\n        anchors, positives, negatives = [], [], []\n\n        distances = squareform(self.to_numpy(distances))\n\n        for anchor, y_anchor in enumerate(y):\n            for positive, y_positive in enumerate(y):\n\n                # if same embedding or different labels, skip\n                if (anchor == positive) or (y_anchor != y_positive):\n                    continue\n\n                d = distances[anchor, positive]\n\n                for negative, y_negative in enumerate(y):\n\n                    if y_negative == y_anchor:\n                        continue\n\n                    if d > distances[anchor, negative]:\n                        continue\n\n                    anchors.append(anchor)\n                    positives.append(positive)\n                    negatives.append(negative)\n\n        return anchors, positives, negatives\n\n    def batch_hard(self, y, distances):\n        """"""Build triplet with both hardest positive and hardest negative\n\n        Parameters\n        ----------\n        y : list\n            Sequence labels.\n        distances : (n * (n-1) / 2,) torch.Tensor\n            Condensed pairwise distance matrix\n\n        Returns\n        -------\n        anchors, positives, negatives : list of int\n            Triplets indices.\n        """"""\n\n        anchors, positives, negatives = [], [], []\n\n        distances = squareform(self.to_numpy(distances))\n        y = np.array(y)\n\n        for anchor, y_anchor in enumerate(y):\n\n            d = distances[anchor]\n\n            # hardest positive\n            pos = np.where(y == y_anchor)[0]\n            pos = [p for p in pos if p != anchor]\n            positive = int(pos[np.argmax(d[pos])])\n\n            # hardest negative\n            neg = np.where(y != y_anchor)[0]\n            negative = int(neg[np.argmin(d[neg])])\n\n            anchors.append(anchor)\n            positives.append(positive)\n            negatives.append(negative)\n\n        return anchors, positives, negatives\n\n    def batch_negative(self, y, distances):\n        """"""Build triplet with hardest negative\n\n        Parameters\n        ----------\n        y : list\n            Sequence labels.\n        distances : (n * (n-1) / 2,) torch.Tensor\n            Condensed pairwise distance matrix\n\n        Returns\n        -------\n        anchors, positives, negatives : list of int\n            Triplets indices.\n        """"""\n\n        anchors, positives, negatives = [], [], []\n\n        distances = squareform(self.to_numpy(distances))\n        y = np.array(y)\n\n        for anchor, y_anchor in enumerate(y):\n\n            # hardest negative\n            d = distances[anchor]\n            neg = np.where(y != y_anchor)[0]\n            negative = int(neg[np.argmin(d[neg])])\n\n            for positive in np.where(y == y_anchor)[0]:\n                if positive == anchor:\n                    continue\n\n                anchors.append(anchor)\n                positives.append(positive)\n                negatives.append(negative)\n\n        return anchors, positives, negatives\n\n    def batch_all(self, y, distances):\n        """"""Build all possible triplet\n\n        Parameters\n        ----------\n        y : list\n            Sequence labels.\n        distances : (n * (n-1) / 2,) torch.Tensor\n            Condensed pairwise distance matrix\n\n        Returns\n        -------\n        anchors, positives, negatives : list of int\n            Triplets indices.\n        """"""\n\n        anchors, positives, negatives = [], [], []\n\n        for anchor, y_anchor in enumerate(y):\n            for positive, y_positive in enumerate(y):\n\n                # if same embedding or different labels, skip\n                if (anchor == positive) or (y_anchor != y_positive):\n                    continue\n\n                for negative, y_negative in enumerate(y):\n\n                    if y_negative == y_anchor:\n                        continue\n\n                    anchors.append(anchor)\n                    positives.append(positive)\n                    negatives.append(negative)\n\n        return anchors, positives, negatives\n\n    def triplet_loss(self, distances, anchors, positives, negatives):\n        """"""Compute triplet loss\n\n        Parameters\n        ----------\n        distances : torch.Tensor\n            Condensed matrix of pairwise distances.\n        anchors, positives, negatives : list of int\n            Triplets indices.\n\n        Returns\n        -------\n        loss : torch.Tensor\n            Triplet loss.\n        """"""\n\n        # estimate total number of embeddings from pdist shape\n        n = int(0.5 * (1 + np.sqrt(1 + 8 * len(distances))))\n\n        # convert indices from squared matrix\n        # to condensed matrix referential\n        pos = to_condensed(n, anchors, positives)\n        neg = to_condensed(n, anchors, negatives)\n\n        # compute raw triplet loss (no margin, no clamping)\n        # the lower, the better\n        delta = distances[pos] - distances[neg]\n\n        # clamp triplet loss\n        if self.clamp == ""positive"":\n            loss = torch.clamp(delta + self.margin_, min=0)\n\n        elif self.clamp == ""softmargin"":\n            loss = torch.log1p(torch.exp(delta))\n\n        elif self.clamp == ""sigmoid"":\n            # TODO. tune this ""10"" hyperparameter\n            # TODO. log-sigmoid\n            loss = torch.sigmoid(10 * (delta + self.margin_))\n\n        return loss\n\n    def batch_loss(self, batch):\n        """"""Compute loss for current `batch`\n\n        Parameters\n        ----------\n        batch : `dict`\n            [\'X\'] (`numpy.ndarray`)\n            [\'y\'] (`numpy.ndarray`)\n\n        Returns\n        -------\n        batch_loss : `dict`\n            [\'loss\'] (`torch.Tensor`) : Triplet loss\n        """"""\n\n        fX, y = self.embed(batch)\n        distances = self.pdist(fX)\n\n        # sample triplets\n        triplets = getattr(self, ""batch_{0}"".format(self.sampling))\n        anchors, positives, negatives = triplets(y, distances)\n\n        # compute loss for each triplet\n        losses = self.triplet_loss(distances, anchors, positives, negatives)\n\n        loss = torch.mean(losses)\n\n        # average over all triplets\n        return {""loss"": loss, ""loss_triplet"": loss}\n'"
pyannote/audio/embedding/models/__init__.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2018 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n'"
pyannote/audio/embedding/models/tristounet.py,10,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2017-2018 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n\n# TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO\n# Update TristouNet to latest API. Commenting it out for now.\n# TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO\n\n\n# import torch\n# import torch.nn as nn\n# import torch.nn.functional as F\n# from torch.nn.utils.rnn import PackedSequence\n# from torch.nn.utils.rnn import pad_packed_sequence\n#\n#\n# class TristouNet(nn.Module):\n#     """"""TristouNet sequence embedding\n#\n#     RNN ( \xc2\xbb ... \xc2\xbb RNN ) \xc2\xbb temporal pooling \xe2\x80\xba ( MLP \xe2\x80\xba ... \xe2\x80\xba ) MLP \xe2\x80\xba normalize\n#\n#     Parameters\n#     ----------\n#     specifications : `dict`\n#         Batch specifications:\n#             {\'X\': {\'dimension\': n_features}}\n#     rnn : {\'LSTM\', \'GRU\'}, optional\n#         Defaults to \'LSTM\'.\n#     recurrent: list, optional\n#         List of output dimension of stacked RNNs.\n#         Defaults to [16, ] (i.e. one RNN with output dimension 16)\n#     bidirectional : bool, optional\n#         Use bidirectional recurrent layers. Defaults to False.\n#     pooling : {\'sum\', \'max\'}\n#         Temporal pooling strategy. Defaults to \'sum\'.\n#     linear : list, optional\n#         List of hidden dimensions of linear layers. Defaults to [16, 16].\n#\n#     Reference\n#     ---------\n#     Herv\xc3\xa9 Bredin. ""TristouNet: Triplet Loss for Speaker Turn Embedding.""\n#     ICASSP 2017 (https://arxiv.org/abs/1609.04301)\n#     """"""\n#\n#     supports_packed = True\n#\n#     def __init__(self, specifications,\n#                  rnn=\'LSTM\', recurrent=[16], bidirectional=False,\n#                  pooling=\'sum\', linear=[16, 16]):\n#\n#         super(TristouNet, self).__init__()\n#\n#         self.specifications = specifications\n#         self.n_features_ = specifications[\'X\'][\'dimension\']\n#         self.rnn = rnn\n#         self.recurrent = recurrent\n#         self.bidirectional = bidirectional\n#         self.pooling = pooling\n#         self.linear = [] if linear is None else linear\n#\n#         self.num_directions_ = 2 if self.bidirectional else 1\n#\n#         if self.pooling not in {\'sum\', \'max\'}:\n#             raise ValueError(\'""pooling"" must be one of {""sum"", ""max""}\')\n#\n#         # create list of recurrent layers\n#         self.recurrent_layers_ = []\n#         input_dim = self.n_features_\n#         for i, hidden_dim in enumerate(self.recurrent):\n#             if self.rnn == \'LSTM\':\n#                 recurrent_layer = nn.LSTM(input_dim, hidden_dim,\n#                                           bidirectional=self.bidirectional,\n#                                           batch_first=True)\n#             elif self.rnn == \'GRU\':\n#                 recurrent_layer = nn.GRU(input_dim, hidden_dim,\n#                                          bidirectional=self.bidirectional,\n#                                          batch_first=True)\n#             else:\n#                 raise ValueError(\'""rnn"" must be one of {""LSTM"", ""GRU""}.\')\n#             self.add_module(\'recurrent_{0}\'.format(i), recurrent_layer)\n#             self.recurrent_layers_.append(recurrent_layer)\n#             input_dim = hidden_dim * (2 if self.bidirectional else 1)\n#\n#         # create list of linear layers\n#         self.linear_layers_ = []\n#         for i, hidden_dim in enumerate(self.linear):\n#             linear_layer = nn.Linear(input_dim, hidden_dim, bias=True)\n#             self.add_module(\'linear_{0}\'.format(i), linear_layer)\n#             self.linear_layers_.append(linear_layer)\n#             input_dim = hidden_dim\n#\n#     @property\n#     def dimension(self):\n#         if self.linear:\n#             return self.linear[-1]\n#         return self.recurrent[-1] * (2 if self.bidirectional else 1)\n#\n#     def forward(self, sequence):\n#         """"""\n#\n#         Parameters\n#         ----------\n#         sequence : (batch_size, n_samples, n_features) torch.Tensor\n#\n#         """"""\n#\n#         packed_sequences = isinstance(sequence, PackedSequence)\n#\n#         if packed_sequences:\n#             _, n_features = sequence.data.size()\n#             batch_size = sequence.batch_sizes[0].item()\n#             device = sequence.data.device\n#         else:\n#             # check input feature dimension\n#             batch_size, _, n_features = sequence.size()\n#             device = sequence.device\n#\n#         if n_features != self.n_features_:\n#             msg = \'Wrong feature dimension. Found {0}, should be {1}\'\n#             raise ValueError(msg.format(n_features, self.n_features_))\n#\n#         output = sequence\n#\n#         # recurrent layers\n#         for hidden_dim, layer in zip(self.recurrent, self.recurrent_layers_):\n#\n#             if self.rnn == \'LSTM\':\n#                 # initial hidden and cell states\n#                 h = torch.zeros(self.num_directions_, batch_size, hidden_dim,\n#                                 device=device, requires_grad=False)\n#                 c = torch.zeros(self.num_directions_, batch_size, hidden_dim,\n#                                 device=device, requires_grad=False)\n#                 hidden = (h, c)\n#\n#             elif self.rnn == \'GRU\':\n#                 # initial hidden state\n#                 hidden = torch.zeros(\n#                     self.num_directions_, batch_size, hidden_dim,\n#                     device=device, requires_grad=False)\n#\n#             # apply current recurrent layer and get output sequence\n#             output, _ = layer(output, hidden)\n#\n#         if packed_sequences:\n#             output, lengths = pad_packed_sequence(output, batch_first=True)\n#\n#         # batch_size, n_samples, dimension\n#\n#         # average temporal pooling\n#         if self.pooling == \'sum\':\n#             output = output.sum(dim=1)\n#         elif self.pooling == \'max\':\n#             if packed_sequences:\n#                 msg = (\'""max"" pooling is not yet implemented \'\n#                        \'for variable length sequences.\')\n#                 raise NotImplementedError(msg)\n#             output, _ = output.max(dim=1)\n#\n#         # batch_size, dimension\n#\n#         # stack linear layers\n#         for hidden_dim, layer in zip(self.linear, self.linear_layers_):\n#\n#             # apply current linear layer\n#             output = layer(output)\n#\n#             # apply non-linear activation function\n#             output = torch.tanh(output)\n#\n#         # batch_size, dimension\n#\n#         # unit-normalize\n#         norm = torch.norm(output, 2, 1, keepdim=True)\n#         output = output / norm\n#\n#         return output\n'"
pyannote/audio/embedding/models/utils.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2018 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n\nimport math\n\n\ndef get_conv1d_output_shape(input_shape, kernel_size, stride=1, padding=0, dilation=1):\n    """"""Predict output shape of Conv1D""""""\n\n    out_shape = input_shape + 2 * padding - dilation * (kernel_size - 1) - 1\n    out_shape = out_shape / stride + 1\n    return int(math.floor(out_shape))\n\n\ndef get_conv2d_output_shape(input_shape, kernel_size, stride=1, padding=0, dilation=1):\n    """"""Predict output shape of Conv2D""""""\n\n    if not isinstance(kernel_size, tuple):\n        kernel_size = (kernel_size, kernel_size)\n    if not isinstance(stride, tuple):\n        stride = (stride, stride)\n    if not isinstance(padding, tuple):\n        padding = (padding, padding)\n    if not isinstance(dilation, tuple):\n        dilation = (dilation, dilation)\n\n    h_in, w_in = input_shape\n\n    h_out = h_in + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1\n    h_out = h_out / stride[0] + 1\n\n    w_out = w_in + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1\n    w_out = w_out / stride[1] + 1\n\n    return int(math.floor(h_out)), int(math.floor(w_out))\n'"
pyannote/audio/embedding/models/vggvox.py,6,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2018 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n# TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO\n# Update VGGVox to latest API. Commenting it out for now.\n# TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO\n\n# import torch\n# import torch.nn as nn\n# import torch.nn.functional as F\n# from .utils import get_conv2d_output_shape\n\n\n# class VGGVox(nn.Module):\n#     """"""VGGVox implementation\n#\n#     Reference\n#     ---------\n#     Arsha Nagrani, Joon Son Chung, Andrew Zisserman. ""VoxCeleb: a large-scale\n#     speaker identification dataset.""\n#\n#     """"""\n#\n#     def __init__(self, specifications, dimension=256):\n#\n#         super().__init__()\n#         self.specifications = specifications\n#         self.n_features_ = specifications[\'X\'][\'dimension\']\n#\n#         if self.n_features_ < 97:\n#             msg = (f\'VGGVox expects features with at least 97 dimensions \'\n#                    f\'(here, n_features = {self.n_features_:d})\')\n#             raise ValueError(msg)\n#\n#         self.dimension = dimension\n#\n#         h = self.n_features_  # 512 in VoxCeleb paper. 201 in practice.\n#         w = 301 # typically 3s with 10ms steps\n#\n#         self.conv1_ = nn.Conv2d(1, 96, (7, 7), stride=(2, 2), padding=1)\n#         # 254 x 148 when n_features = 512\n#         # 99 x 148 when n_features = 201\n#         h, w = get_conv2d_output_shape((h, w), (7, 7), stride=(2, 2), padding=1)\n#\n#         self.bn1_ = nn.BatchNorm2d(96)\n#         self.mpool1_ = nn.MaxPool2d((3, 3), stride=(2, 2))\n#         # 126 x 73 when n_features = 512\n#         # 49 x 73 when n_features = 201\n#         h, w = get_conv2d_output_shape((h, w), (3, 3), stride=(2, 2))\n#\n#         self.conv2_ = nn.Conv2d(96, 256, (5, 5), stride=(2, 2), padding=1)\n#         # 62 x 36 when n_features = 512\n#         # 24 x 36 when n_features = 201\n#         h, w = get_conv2d_output_shape((h, w), (5, 5), stride=(2, 2), padding=1)\n#\n#         self.bn2_ = nn.BatchNorm2d(256)\n#         self.mpool2_ = nn.MaxPool2d((3, 3), stride=(2, 2))\n#         # 30 x 17 when n_features = 512\n#         # 11 x 17 when n_features = 201\n#         h, w = get_conv2d_output_shape((h, w), (3, 3), stride=(2, 2))\n#\n#         self.conv3_ = nn.Conv2d(256, 256, (3, 3), stride=(1, 1), padding=1)\n#         # 30 x 17 when n_features = 512\n#         # 11 x 17 when n_features = 201\n#         h, w = get_conv2d_output_shape((h, w), (3, 3), stride=(1, 1), padding=1)\n#\n#         self.bn3_ = nn.BatchNorm2d(256)\n#\n#         self.conv4_ = nn.Conv2d(256, 256, (3, 3), stride=(1, 1), padding=1)\n#         # 30 x 17 when n_features = 512\n#         # 11 x 17 when n_features = 201\n#         h, w = get_conv2d_output_shape((h, w), (3, 3), stride=(1, 1), padding=1)\n#\n#         self.bn4_ = nn.BatchNorm2d(256)\n#\n#         self.conv5_ = nn.Conv2d(256, 256, (3, 3), stride=(1, 1), padding=1)\n#         # 30 x 17 when n_features = 512\n#         # 11 x 17 when n_features = 201\n#         h, w = get_conv2d_output_shape((h, w), (3, 3), stride=(1, 1), padding=1)\n#\n#         self.bn5_ = nn.BatchNorm2d(256)\n#\n#         self.mpool5_ = nn.MaxPool2d((5, 3), stride=(3, 2))\n#         # 9 x 8 when n_features = 512\n#         # 3 x 8 when n_features = 201\n#         h, w = get_conv2d_output_shape((h, w), (5, 3), stride=(3, 2))\n#\n#         self.fc6_ = nn.Conv2d(256, 4096, (h, 1), stride=(1, 1))\n#         # 1 x 8\n#         h, w = get_conv2d_output_shape((h, w), (h, 1), stride=(1, 1))\n#\n#         self.fc7_ = nn.Linear(4096, 1024)\n#         self.fc8_ = nn.Linear(1024, self.dimension)\n#\n#\n#     def forward(self, sequences):\n#         """"""Embed sequences\n#\n#         Parameters\n#         ----------\n#         sequences : torch.Tensor (batch_size, n_samples, n_features)\n#             Batch of sequences.\n#\n#         Returns\n#         -------\n#         embeddings : torch.Tensor (batch_size, dimension)\n#             Batch of embeddings.\n#         """"""\n#\n#         # reshape batch to (batch_size, n_channels, n_features, n_samples)\n#         batch_size, n_samples, n_features = sequences.size()\n#\n#         if n_features != self.n_features_:\n#             msg = (f\'Mismatch in feature dimension \'\n#                    f\'(should be: {self.n_features_:d}, is: {n_features:d})\')\n#             raise ValueError(msg)\n#\n#         if n_samples < 65:\n#             msg = (f\'VGGVox expects sequences with at least 65 samples \'\n#                    f\'(here, n_samples = {n_samples:d})\')\n#             raise ValueError(msg)\n#\n#         x = torch.transpose(sequences, 1, 2).view(\n#             batch_size, 1, n_features, n_samples)\n#\n#         # conv1. shape => 254 x 148 => 126 x 73\n#         x = self.mpool1_(F.relu(self.bn1_(self.conv1_(x))))\n#\n#         # conv2. shape =>\n#         x = self.mpool2_(F.relu(self.bn2_(self.conv2_(x))))\n#\n#         # conv3. shape = 62 x 36\n#         x = F.relu(self.bn3_(self.conv3_(x)))\n#\n#         # conv4. shape = 30 x 17\n#         x = F.relu(self.bn4_(self.conv4_(x)))\n#\n#         # conv5. shape = 30 x 17\n#         x = self.mpool5_(F.relu(self.bn5_(self.conv5_(x))))\n#\n#         # fc6. shape =\n#         x = F.dropout(F.relu(self.fc6_(x)))\n#\n#         # (average) temporal pooling. shape =\n#         x = torch.mean(x, dim=-1)\n#\n#         # fc7. shape =\n#         x = x.view(x.size(0), -1)\n#         x = F.dropout(F.relu(self.fc7_(x)))\n#\n#         # fc8. shape =\n#         x = self.fc8_(x)\n#\n#         return x\n'"
pyannote/audio/interactive/recipes/__init__.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n'"
pyannote/audio/interactive/recipes/dia.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n# types\nfrom pathlib import Path\nfrom typing import Text, Dict, List, Tuple, Iterable\nfrom pyannote.core import Segment\n\n# pyannote.audio\nfrom ..pipeline import InteractiveDiarization\nfrom ..pipeline import PRETRAINED_PARAMS\nfrom ..pipeline import time2index\nfrom ..pipeline import index2index\nfrom pyannote.audio.features import RawAudio\nfrom pyannote.audio.features.utils import get_audio_duration\n\n# prodigy\nimport prodigy\nfrom prodigy.components.loaders import Audio\nfrom prodigy.components.db import connect\nfrom .utils import SAMPLE_RATE\nfrom .utils import remove_audio_before_db\nfrom .utils import normalize\nfrom .utils import to_base64\nfrom .utils import to_audio_spans\n\n# clustering\nCONSTRAINT = Tuple[float, float]\nCONSTRAINTS = List[CONSTRAINT]\nfrom pyannote.core.utils.hierarchy import propagate_constraints\nfrom pyannote.core.utils.hierarchy import pool\nfrom scipy.cluster.hierarchy import fcluster\nfrom scipy.spatial.distance import cdist\nfrom scipy.spatial.distance import pdist\nfrom scipy.spatial.distance import squareform\n\n# others\nimport numpy as np\nfrom copy import deepcopy\nfrom .sad import load_sad_manual\nfrom itertools import filterfalse\n\n\nclass DiaRecipeHelper:\n    def __init__(self, pipeline: InteractiveDiarization, dataset: Text, source: Path):\n        self.pipeline = pipeline\n        self.dataset = dataset\n        self.source = source\n\n    def load_dia_binary(self, path: Text):\n        """"""Load existing examples as constraints for diarization\n\n        This will set (or overwrite) the following attributes and return them\n            * cannot_link_time\n            * must_link_time\n            * dont_know_time\n\n        Parameters\n        ----------\n        path : Text\n            Only load examples for this file.\n        """"""\n\n        db = connect()\n\n        examples = [\n            eg\n            for eg in db.get_dataset(self.dataset)\n            if eg[""recipe""] == ""pyannote.dia.binary"" and eg[""path""] == path\n        ]\n\n        cannot_link: CONSTRAINTS = [\n            (eg[""t1""], eg[""t2""]) for eg in examples if eg[""answer""] == ""reject""\n        ]\n        must_link: CONSTRAINTS = [\n            (eg[""t1""], eg[""t2""]) for eg in examples if eg[""answer""] == ""accept""\n        ]\n        dont_know: CONSTRAINTS = [\n            (eg[""t1""], eg[""t2""])\n            for eg in examples\n            if eg[""answer""] not in [""accept"", ""reject""]\n        ]\n\n        if len(cannot_link) > 0:\n            prodigy.log(\n                f""RECIPE: {path}: init: {len(cannot_link)} cannot link constraints""\n            )\n        if len(must_link) > 0:\n            prodigy.log(f""RECIPE: {path}: init: {len(must_link)} must link constraints"")\n\n        # expand list of ""cannot link"" constraints thanks to the following rule\n        # (u != v) & (v == w) ==> u != w\n        cannot_link = propagate_constraints(cannot_link, must_link)\n\n        self.cannot_link_time = cannot_link\n        self.must_link_time = must_link\n        self.dont_know_time = dont_know\n\n    def dia_binary_stream(self) -> Iterable[Dict]:\n\n        raw_audio = RawAudio(sample_rate=SAMPLE_RATE, mono=True)\n\n        for audio_source in Audio(self.source):\n\n            path = audio_source[""path""]\n            text = audio_source[""text""]\n\n            # load speech/non-speech annotations (from pyannote.sad.manual recipe)\n            file = load_sad_manual(self.dataset, path)\n            speech = file[""speech""]\n\n            if not speech:\n                prodigy.log(f""RECIPE: {path}: skip: no annotated speech"")\n                continue\n\n            # load existing same/different annotations (from this very recipe)\n            self.load_dia_binary(path)\n\n            # extract speaker embedding\n            embedding = self.pipeline.compute_embedding(file)\n\n            # number of consecutive steps with overlap\n            window = embedding.sliding_window\n            n_steps = int(np.ceil(window.duration / window.step))\n\n            # find clean embedding (i.e. those fully included in speech regions)\n            assignment = self.pipeline.get_segment_assignment(embedding, speech)\n            clean = assignment > 0\n            clean_embedding = embedding[clean]\n            if len(clean_embedding) < 2:\n                prodigy.log(f""RECIPE: {path}: skip: not enough speech"")\n                continue\n\n            # conversion from ""clean-only"" index base to ""all"" index base (used later)\n            clean2all = index2index(None, clean, reverse=True, return_mapping=True)\n\n            done_with_current_file = False\n            while not done_with_current_file:\n\n                # IMPROVE do not recompute if no new constraint since last time\n\n                # filter and convert time-based constraints in whole file referential\n                # to index-based constraints in clean-only embeddings referential\n                cannot_link = index2index(\n                    time2index(self.cannot_link_time, window), clean\n                )\n                must_link = index2index(time2index(self.must_link_time, window), clean)\n\n                dendrogram = pool(\n                    clean_embedding,\n                    metric=""cosine"",\n                    cannot_link=cannot_link if cannot_link else None,\n                    must_link=must_link if must_link else None,\n                    must_link_method=""propagate"",\n                )\n\n                # # iterate from dendrogram top to bottom\n                # iterations = iter(range(len(dendrogram) - 1, 0, -1))\n\n                # iterate from merging step whose distance is the most similar\n                # to the ""optimal"" threshold and progressively wander away from it\n                iterations = filterfalse(\n                    lambda i: i < 1,\n                    iter(\n                        np.argsort(\n                            np.abs(self.pipeline.emb_threshold - dendrogram[:, 2])\n                        )\n                    ),\n                )\n\n                # IDEA stop annotating early once the current distance is much\n                # smaller/greater than the ""optimal"" and we can be sure that all\n                # further iterations are easy to decide.\n\n                while True:\n\n                    try:\n                        i = next(iterations)\n                    except StopIteration as e:\n                        done_with_current_file = True\n                        break\n\n                    distance = dendrogram[i, 2]\n\n                    # if distance is infinite, this is a fake clustering step\n                    # prevented by a ""cannot link"" constraint.\n                    # see pyannote.core.hierarchy.pool for details\n                    if distance == np.infty:\n                        prodigy.log(f""RECIPE: {path}: depth {i}: skip: cannot link"")\n                        continue\n\n                    # find clusters k1 and k2 that were merged at iteration i\n                    current = fcluster(\n                        dendrogram, dendrogram[i, 2], criterion=""distance""\n                    )\n                    previous = fcluster(\n                        dendrogram, dendrogram[i - 1, 2], criterion=""distance"",\n                    )\n                    n_current, n_previous = max(current), max(previous)\n\n                    # TODO handle these corner cases better\n                    if n_current >= n_previous or n_previous - n_current > 1:\n                        prodigy.log(f""RECIPE: {path}: depth {i}: skip: corner case"")\n                        continue\n                    C = np.zeros((n_current, n_previous))\n                    for k_current, k_previous in zip(current, previous):\n                        C[k_current - 1, k_previous - 1] += 1\n                    k1, k2 = (\n                        np.where(C[int(np.where(np.sum(C > 0, axis=1) == 2)[0])] > 0)[0]\n                        + 1\n                    )\n\n                    # find indices of embeddings fully included in clusters k1 and k2\n                    neighbors1 = np.convolve(previous == k1, [1] * n_steps, mode=""same"")\n                    indices1 = np.where(neighbors1 == n_steps)[0]\n                    # if indices1.size == 0:\n                    #     indices1 = np.where(neighbors1 == np.max(neighbors1))[0]\n\n                    neighbors2 = np.convolve(previous == k2, [1] * n_steps, mode=""same"")\n                    indices2 = np.where(neighbors2 == n_steps)[0]\n                    # if indices2.size == 0:\n                    #     indices2 = np.where(neighbors2 == np.max(neighbors2))[0]\n\n                    if indices1.size == 0 or indices2.size == 0:\n                        prodigy.log(\n                            f""RECIPE: {path}: depth {i}: skip: too short segments""\n                        )\n                        continue\n\n                    # find centroids of clusters k1 and k2\n                    i1 = indices1[\n                        np.argmin(\n                            np.mean(\n                                squareform(\n                                    pdist(clean_embedding[indices1], metric=""cosine"")\n                                ),\n                                axis=1,\n                            )\n                        )\n                    ]\n\n                    i2 = indices2[\n                        np.argmin(\n                            np.mean(\n                                squareform(\n                                    pdist(clean_embedding[indices2], metric=""cosine"")\n                                ),\n                                axis=1,\n                            )\n                        )\n                    ]\n\n                    i1, i2 = sorted([i1, i2])\n                    distance = cdist(\n                        clean_embedding[np.newaxis, i1],\n                        clean_embedding[np.newaxis, i2],\n                        metric=""cosine"",\n                    )[0, 0]\n\n                    segment1 = window[clean2all[i1]]\n                    t1 = segment1.middle\n                    segment2 = window[clean2all[i2]]\n                    t2 = segment2.middle\n\n                    # did the human in the loop already provide feedback on this pair of segments?\n                    pair = (t1, t2)\n\n                    if (\n                        pair in self.cannot_link_time\n                        or pair in self.must_link_time\n                        or pair in self.dont_know_time\n                    ):\n                        # do not annotate the same pair twice\n                        prodigy.log(f""RECIPE: {path}: depth {i}: skip: exists"")\n                        continue\n\n                    prodigy.log(f""RECIPE: {path}: depth {i}: annotate"")\n\n                    task_text = f""{text} t={t1:.1f}s vs. t={t2:.1f}s""\n\n                    waveform1 = normalize(raw_audio.crop(file, segment1))\n                    waveform2 = normalize(raw_audio.crop(file, segment2))\n                    task_audio = to_base64(\n                        np.vstack([waveform1, waveform2]), sample_rate=SAMPLE_RATE\n                    )\n\n                    task_audio_spans = [\n                        {""start"": 0.0, ""end"": segment1.duration, ""label"": ""SPEAKER"",},\n                        {\n                            ""start"": segment1.duration,\n                            ""end"": segment1.duration + segment2.duration,\n                            ""label"": ""SAME_SPEAKER"",\n                        },\n                    ]\n\n                    yield {\n                        ""path"": path,\n                        ""text"": task_text,\n                        ""audio"": task_audio,\n                        ""audio_spans"": task_audio_spans,\n                        ""t1"": t1,\n                        ""t2"": t2,\n                        ""meta"": {\n                            ""t1"": f""{t1:.1f}s"",\n                            ""t2"": f""{t2:.1f}s"",\n                            ""file"": text,\n                            ""distance"": f""{distance:.2f}"",\n                        },\n                        ""recipe"": ""pyannote.dia.binary"",\n                    }\n\n                    # at that point, ""dia_binary_update"" is called. hence,\n                    # we exit the loop because the dendrogram needs to be updated\n                    break\n\n    def dia_binary_update(self, examples: List[Dict]) -> List[Dict]:\n\n        needs_update = False\n\n        for eg in examples:\n\n            t1, t2 = eg[""t1""], eg[""t2""]\n\n            if eg[""answer""] == ""accept"":\n                self.must_link_time.append((t1, t2))\n                needs_update = True\n                prodigy.log(f""RECIPE: new constraint: +1 must link"")\n\n            elif eg[""answer""] == ""reject"":\n                self.cannot_link_time.append((t1, t2))\n                needs_update = True\n                prodigy.log(f""RECIPE: new constraint: +1 cannot link"")\n\n            else:\n                self.dont_know_time.append((t1, t2))\n                prodigy.log(f""RECIPE: new constraint: skip"")\n\n        # expand list of ""cannot link"" constraints thanks to the following rule\n        # (u != v) & (v == w) ==> u != w\n        if needs_update:\n            num_cannot = len(self.cannot_link_time)\n            self.cannot_link_time = propagate_constraints(\n                self.cannot_link_time, self.must_link_time\n            )\n            new_num_cannot = len(self.cannot_link_time)\n            if new_num_cannot > num_cannot:\n                prodigy.log(\n                    f""RECIPE: propagate constraint: +{new_num_cannot - num_cannot} cannot link""\n                )\n\n    def dia_manual_stream(self) -> Iterable[Dict]:\n\n        for audio_source in Audio(self.source):\n\n            path = audio_source[""path""]\n            text = audio_source[""text""]\n\n            # load speech/non-speech annotations (from pyannote.sad.manual recipe)\n            file = load_sad_manual(self.dataset, path)\n            manual_speech = file[""speech""]\n            annotated = file[""annotated""]\n\n            # use manual speech/non-speech annotation where available,\n            # and automatic speech/non-speech else where\n            duration = get_audio_duration(file)\n            file_extent = Segment(0, duration)\n            non_annotated = annotated.gaps(file_extent)\n            if non_annotated:\n                automatic_speech = self.pipeline.compute_speech(file)\n                file[""speech""] = automatic_speech.crop(non_annotated).update(\n                    manual_speech\n                )\n\n            # load existing same/different annotations (from pyannote.dia.binary recipe)\n            self.load_dia_binary(path)\n\n            # apply speaker diarization pipeline using same/different speaker\n            # binary annotation as must link/cannot link constraints\n            hypothesis = self.pipeline(\n                file, cannot_link=self.cannot_link_time, must_link=self.must_link_time\n            )\n\n            # rename 9 most talkative speakers to {SPEAKER_1, ..., SPEAKER_9}\n            # and remaining speakers as OTHER\n            mapping = {\n                label: f""SPEAKER_{s+1}"" if s < 9 else ""OTHER""\n                for s, (label, duration) in enumerate(hypothesis.chart())\n            }\n            hypothesis = hypothesis.rename_labels(mapping=mapping)\n\n            audio_spans = to_audio_spans(hypothesis)\n            audio_source[""audio_spans""] = audio_spans\n            audio_source[""audio_spans_original""] = deepcopy(audio_spans)\n            audio_source[""recipe""] = ""pyannote.dia.manual""\n\n            yield audio_source\n\n\n@prodigy.recipe(\n    ""pyannote.dia.binary"",\n    dataset=(""Dataset to save annotations to"", ""positional"", None, str),\n    source=(""Directory containing audio files to annotate"", ""positional"", None, Path),\n)\ndef dia_binary(dataset: Text, source: Path) -> Dict:\n\n    pipeline = InteractiveDiarization().instantiate(PRETRAINED_PARAMS)\n    helper = DiaRecipeHelper(pipeline, dataset, source)\n\n    return {\n        ""dataset"": dataset,\n        ""view_id"": ""audio"",\n        ""stream"": helper.dia_binary_stream(),\n        ""update"": helper.dia_binary_update,\n        ""before_db"": remove_audio_before_db,\n        ""config"": {\n            ""audio_autoplay"": True,\n            ""audio_loop"": True,\n            ""show_audio_minimap"": False,\n            ""audio_bar_width"": 3,\n            ""audio_bar_height"": 1,\n            ""labels"": [""SPEAKER"", ""SAME_SPEAKER""],\n            ""batch_size"": 1,\n            ""instant_submit"": True,\n        },\n    }\n\n\n@prodigy.recipe(\n    ""pyannote.dia.manual"",\n    dataset=(""Dataset to save annotations to"", ""positional"", None, str),\n    source=(""Directory containing audio files to annotate"", ""positional"", None, Path),\n)\ndef dia_manual(dataset: Text, source: Path) -> Dict:\n\n    pipeline = InteractiveDiarization().instantiate(PRETRAINED_PARAMS)\n    helper = DiaRecipeHelper(pipeline, dataset, source)\n\n    return {\n        ""dataset"": dataset,\n        ""view_id"": ""audio_manual"",\n        ""stream"": helper.dia_manual_stream(),\n        ""before_db"": remove_audio_before_db,\n        ""config"": {\n            ""audio_autoplay"": True,\n            ""audio_loop"": True,\n            ""show_audio_minimap"": True,\n            ""audio_bar_width"": 3,\n            ""audio_bar_height"": 1,\n            ""labels"": [f""SPEAKER_{i+1}"" for i in range(10)] + [""OTHER""],\n            ""batch_size"": 1,\n        },\n    }\n'"
pyannote/audio/interactive/recipes/sad.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n# types\nfrom pathlib import Path\nfrom typing import Text, Dict, List, Iterable\nfrom pyannote.core import Segment, Timeline, Annotation\n\n# prodigy\nimport prodigy\nfrom prodigy.components.loaders import Audio\nfrom prodigy.components.db import connect\nfrom .utils import SAMPLE_RATE\nfrom .utils import normalize\nfrom .utils import to_base64\nfrom .utils import to_audio_spans\nfrom .utils import chunks\nfrom .utils import remove_audio_before_db\nfrom copy import deepcopy\n\n# pyannote.audio\nfrom ..pipeline import InteractiveDiarization\nfrom ..pipeline import PRETRAINED_PARAMS\nfrom pyannote.audio.features import RawAudio\nfrom pyannote.audio.features.utils import get_audio_duration\n\n\ndef sad_manual_stream(\n    pipeline: InteractiveDiarization, source: Path, chunk: float = 10.0\n) -> Iterable[Dict]:\n    """"""Stream for pyannote.sad.manual recipe\n\n    Applies (pretrained) speech activity detection and sends the results for\n    manual correction chunk by chunk.\n\n    Parameters\n    ----------\n    pipeline : InteractiveDiarization\n        Pretrained speaker diarization interactive pipeline.\n        Note that only the speech activity detection part is used.\n    source : Path\n        Directory containing audio files to process.\n    chunk : float, optional\n        Duration of chunks, in seconds. Defaults to 10s.\n\n    Yields\n    ------\n    task : dict\n        Prodigy task with the following keys:\n        ""path"" : path to audio file\n        ""text"" : name of audio file\n        ""chunk"" : chunk start and end times\n        ""audio"" : base64 encoding of audio chunk\n        ""audio_spans"" : speech spans detected by pretrained SAD model\n        ""audio_spans_original"" : copy of ""audio_spans""\n        ""meta"" : additional meta-data displayed in Prodigy UI\n        ""recipe"" : ""pyannote.sad.manual""\n\n    """"""\n\n    raw_audio = RawAudio(sample_rate=SAMPLE_RATE, mono=True)\n\n    for audio_source in Audio(source):\n\n        path = audio_source[""path""]\n        text = audio_source[""text""]\n        file = {""uri"": text, ""database"": source, ""audio"": path}\n\n        duration = get_audio_duration(file)\n        file[""duration""] = duration\n\n        prodigy.log(f""RECIPE: detecting speech regions in \'{path}\'"")\n\n        speech: Annotation = pipeline.compute_speech(file).to_annotation(\n            generator=iter(lambda: ""SPEECH"", None)\n        )\n\n        if duration <= chunk:\n            waveform = raw_audio.crop(file, Segment(0, duration))\n            task_audio = to_base64(normalize(waveform), sample_rate=SAMPLE_RATE)\n            task_audio_spans = to_audio_spans(speech)\n\n            yield {\n                ""path"": path,\n                ""text"": text,\n                ""audio"": task_audio,\n                ""audio_spans"": task_audio_spans,\n                ""audio_spans_original"": deepcopy(task_audio_spans),\n                ""chunk"": {""start"": 0, ""end"": duration},\n                ""meta"": {""file"": text},\n                # this is needed by other recipes\n                ""recipe"": ""pyannote.sad.manual"",\n            }\n\n        else:\n            for focus in chunks(duration, chunk=chunk, shuffle=True):\n                task_text = f""{text} [{focus.start:.1f}, {focus.end:.1f}]""\n                waveform = raw_audio.crop(file, focus)\n                task_audio = to_base64(normalize(waveform), sample_rate=SAMPLE_RATE)\n                task_audio_spans = to_audio_spans(\n                    speech.crop(focus, mode=""intersection""), focus=focus\n                )\n\n                yield {\n                    ""path"": path,\n                    ""text"": task_text,\n                    ""audio"": task_audio,\n                    ""audio_spans"": task_audio_spans,\n                    ""audio_spans_original"": deepcopy(task_audio_spans),\n                    ""chunk"": {""start"": focus.start, ""end"": focus.end},\n                    ""meta"": {\n                        ""file"": text,\n                        ""start"": f""{focus.start:.1f}"",\n                        ""end"": f""{focus.end:.1f}"",\n                    },\n                    # this is needed by other recipes\n                    ""recipe"": ""pyannote.sad.manual"",\n                }\n\n\ndef sad_manual_before_db(examples: List[Dict]) -> List[Dict]:\n    """"""Remove \'audio\' key and shift spans back to t0=0 base\n\n    Parameters\n    ----------\n    examples : list of dict\n        Examples.\n\n    Returns\n    -------\n    examples : list of dict\n        Examples with ""audio"" key removed and shifted.\n    """"""\n\n    examples = remove_audio_before_db(examples)\n\n    for eg in examples:\n\n        # shift audio spans back to the t0=0 base\n        chunk = eg.get(""chunk"", None)\n        if chunk is not None:\n            start = chunk[""start""]\n            for span in eg[""audio_spans""]:\n                span[""start""] += start\n                span[""end""] += start\n            for span in eg[""audio_spans_original""]:\n                span[""start""] += start\n                span[""end""] += start\n\n    return examples\n\n\n@prodigy.recipe(\n    ""pyannote.sad.manual"",\n    dataset=(""Dataset to save annotations to"", ""positional"", None, str),\n    source=(""Directory containing audio files to annotate"", ""positional"", None, Path),\n    chunk=(\n        ""split long audio files into shorter chunks of that many seconds each"",\n        ""option"",\n        None,\n        float,\n    ),\n    speed=(\n        ""set the playback rate (0.5 means half the normal speed, 2 means double speed and so on)"",\n        ""option"",\n        None,\n        float,\n    ),\n)\ndef sad_manual(\n    dataset: Text, source: Path, chunk: float = 10.0, speed: float = 1.0\n) -> Dict:\n\n    pipeline = InteractiveDiarization().instantiate(PRETRAINED_PARAMS)\n\n    return {\n        ""dataset"": dataset,\n        ""view_id"": ""audio_manual"",\n        ""stream"": sad_manual_stream(pipeline, source, chunk=chunk),\n        ""before_db"": sad_manual_before_db,\n        ""config"": {\n            ""audio_autoplay"": True,\n            ""audio_loop"": True,\n            ""show_audio_minimap"": False,\n            ""audio_bar_width"": 3,\n            ""audio_bar_height"": 1,\n            ""audio_rate"": speed,\n            ""labels"": [""SPEECH"",],\n        },\n    }\n\n\ndef load_sad_manual(dataset: Text, path: Text) -> Dict:\n    """"""Load accepted pyannote.sad.manual examples\n\n    Parameters\n    ----------\n    dataset : str\n        Dataset containing annotations.\n    path : str\n        Path to annotated file\n\n    Returns\n    -------\n    file : dict\n        Dictionary containing the following keys:\n        ""audio"" (Path) : path to audio file\n        ""annotated"" (Timeline) : part of the audio annotated and accepted\n        ""speech"" (Timeline) : part of the audio accepted as speech\n    """"""\n\n    db = connect()\n\n    examples = [\n        eg\n        for eg in db.get_dataset(dataset)\n        if eg[""recipe""] == ""pyannote.sad.manual""\n        and eg[""path""] == path\n        and eg[""answer""] == ""accept""\n    ]\n\n    speech = Timeline(\n        segments=[\n            Segment(span[""start""], span[""end""])\n            for eg in examples\n            for span in eg[""audio_spans""]\n        ],\n    ).support()\n\n    annotated = Timeline(segments=[Segment(**eg[""chunk""]) for eg in examples]).support()\n\n    prodigy.log(f""RECIPE: {path}: loaded speech regions"")\n\n    return {\n        ""audio"": Path(path),\n        ""speech"": speech,\n        ""annotated"": annotated,\n    }\n'"
pyannote/audio/interactive/recipes/utils.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\nfrom typing import Text, List, Dict, Iterator\nfrom pyannote.core import Segment, SlidingWindow, Annotation\n\nimport numpy as np\nimport random\n\nimport io\nimport base64\nimport scipy.io.wavfile\n\nSAMPLE_RATE = 16000\n\n\ndef normalize(waveform: np.ndarray) -> np.ndarray:\n    """"""Normalize waveform for better display in Prodigy UI""""""\n    return waveform / (np.max(np.abs(waveform)) + 1e-8)\n\n\ndef to_base64(waveform: np.ndarray, sample_rate: int = SAMPLE_RATE) -> Text:\n    """"""Convert waveform to base64 data""""""\n    with io.BytesIO() as content:\n        scipy.io.wavfile.write(content, sample_rate, waveform)\n        content.seek(0)\n        b64 = base64.b64encode(content.read()).decode()\n        b64 = f""data:audio/x-wav;base64,{b64}""\n    return b64\n\n\ndef to_audio_spans(annotation: Annotation, focus: Segment = None) -> Dict:\n    """"""Convert pyannote.core.Annotation to Prodigy\'s audio_spans\n\n    Parameters\n    ----------\n    annotation : Annotation\n        Annotation with t=0s time origin.\n    focus : Segment, optional\n        When provided, use its start time as audio_spans time origin.\n\n    Returns\n    -------\n    audio_spans : list of dict\n    """"""\n    shift = 0.0 if focus is None else focus.start\n    return [\n        {""start"": segment.start - shift, ""end"": segment.end - shift, ""label"": label}\n        for segment, _, label in annotation.itertracks(yield_label=True)\n    ]\n\n\ndef remove_audio_before_db(examples: List[Dict]) -> List[Dict]:\n    """"""Remove (potentially heavy) \'audio\' key from examples\n\n    Parameters\n    ----------\n    examples : list of dict\n        Examples.\n\n    Returns\n    -------\n    examples : list of dict\n        Examples with \'audio\' key removed.\n    """"""\n    for eg in examples:\n        if ""audio"" in eg:\n            del eg[""audio""]\n\n    return examples\n\n\ndef chunks(\n    duration: float, chunk: float = 30, shuffle: bool = False\n) -> Iterator[Segment]:\n    """"""Partition [0, duration] time range into smaller chunks\n\n    Parameters\n    ----------\n    duration : float\n        Total duration, in seconds.\n    chunk : float, optional\n        Chunk duration, in seconds. Defaults to 30.\n    shuffle : bool, optional\n        Yield chunks in random order. Defaults to chronological order.\n\n    Yields\n    ------\n    focus : Segment\n    """"""\n\n    sliding_window = SlidingWindow(start=0.0, step=chunk, duration=chunk)\n    whole = Segment(0, duration)\n\n    if shuffle:\n        chunks_ = list(chunks(duration, chunk=chunk, shuffle=False))\n        random.shuffle(chunks_)\n        for chunk in chunks_:\n            yield chunk\n\n    else:\n        for window in sliding_window(whole):\n            yield window\n        if window.end < duration:\n            yield Segment(window.end, duration)\n'"
pyannote/audio/labeling/tasks/__init__.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2018-2019 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n\nfrom .speech_activity_detection import SpeechActivityDetection\nfrom .speech_activity_detection import DomainAwareSpeechActivityDetection\nfrom .speech_activity_detection import DomainAdversarialSpeechActivityDetection\n\nfrom .overlap_detection import OverlapDetection\nfrom .speaker_change_detection import SpeakerChangeDetection\nfrom .domain_classification import DomainClassification\n\nfrom .resegmentation import Resegmentation\n\n\n__all__ = [\n    ""SpeechActivityDetection"",\n    ""OverlapDetection"",\n    ""SpeakerChangeDetection"",\n    ""Resegmentation"",\n]\n'"
pyannote/audio/labeling/tasks/base.py,13,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2018-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\nfrom typing import Optional\nfrom typing import Text\n\nimport torch\nimport torch.nn.functional as F\n\nimport numpy as np\nimport scipy.signal\n\nfrom pyannote.core import Segment\nfrom pyannote.core import SlidingWindow\nfrom pyannote.core import Timeline\nfrom pyannote.core import Annotation\nfrom pyannote.core import SlidingWindowFeature\n\nfrom pyannote.database import get_unique_identifier\nfrom pyannote.database import get_annotated\nfrom pyannote.database.protocol.protocol import Protocol\n\nfrom pyannote.core.utils.numpy import one_hot_encoding\n\nfrom pyannote.audio.features import RawAudio\nfrom pyannote.audio.features.wrapper import Wrapper, Wrappable\n\nfrom pyannote.core.utils.random import random_segment\nfrom pyannote.core.utils.random import random_subsegment\n\nfrom pyannote.audio.train.trainer import Trainer\nfrom pyannote.audio.train.generator import BatchGenerator\n\nfrom pyannote.audio.train.task import Task, TaskType, TaskOutput\n\nfrom pyannote.audio.train.model import Resolution\nfrom pyannote.audio.train.model import RESOLUTION_CHUNK\nfrom pyannote.audio.train.model import RESOLUTION_FRAME\nfrom pyannote.audio.train.model import Alignment\n\nSECONDS_IN_A_DAY = 24 * 60 * 60\n\n\nclass LabelingTaskGenerator(BatchGenerator):\n    """"""Base batch generator for various labeling tasks\n\n    This class should be inherited from: it should not be used directy\n\n    Parameters\n    ----------\n    task : Task\n        Task\n    feature_extraction : Wrappable\n        Describes how features should be obtained.\n        See pyannote.audio.features.wrapper.Wrapper documentation for details.\n    protocol : Protocol\n    subset : {\'train\', \'development\', \'test\'}, optional\n        Protocol and subset.\n    resolution : `pyannote.core.SlidingWindow`, optional\n        Override `feature_extraction.sliding_window`. This is useful for\n        models that include the feature extraction step (e.g. SincNet) and\n        therefore output a lower sample rate than that of the input.\n        Defaults to `feature_extraction.sliding_window`\n    alignment : {\'center\', \'loose\', \'strict\'}, optional\n        Which mode to use when cropping labels. This is useful for models that\n        include the feature extraction step (e.g. SincNet) and therefore use a\n        different cropping mode. Defaults to \'center\'.\n    duration : float, optional\n        Duration of audio chunks. Defaults to 2s.\n    batch_size : int, optional\n        Batch size. Defaults to 32.\n    per_epoch : float, optional\n        Force total audio duration per epoch, in days.\n        Defaults to total duration of protocol subset.\n    exhaustive : bool, optional\n        Ensure training files are covered exhaustively (useful in case of\n        non-uniform label distribution).\n    step : `float`, optional\n        Ratio of audio chunk duration used as step between two consecutive\n        audio chunks. Defaults to 0.1. Has not effect when exhaustive is False.\n    mask : str, optional\n        When provided, protocol files are expected to contain a key named after\n        this `mask` variable and providing a `SlidingWindowFeature` instance.\n        Generated batches will contain an additional ""mask"" key (on top of\n        existing ""X"" and ""y"" keys) computed as an excerpt of `current_file[mask]`\n        time-aligned with ""y"". Defaults to not add any ""mask"" key.\n    local_labels : bool, optional\n        Set to True to yield samples with local (file-level) labels.\n        Defaults to use global (protocol-level) labels.\n    """"""\n\n    def __init__(\n        self,\n        task: Task,\n        feature_extraction: Wrappable,\n        protocol: Protocol,\n        subset: Text = ""train"",\n        resolution: Optional[Resolution] = None,\n        alignment: Optional[Alignment] = None,\n        duration: float = 2.0,\n        batch_size: int = 32,\n        per_epoch: float = None,\n        exhaustive: bool = False,\n        step: float = 0.1,\n        mask: Text = None,\n        local_labels: bool = False,\n    ):\n\n        self.task = task\n        self.feature_extraction = Wrapper(feature_extraction)\n        self.duration = duration\n        self.exhaustive = exhaustive\n        self.step = step\n        self.mask = mask\n        self.local_labels = local_labels\n\n        self.resolution_ = resolution\n\n        if alignment is None:\n            alignment = ""center""\n        self.alignment = alignment\n\n        self.batch_size = batch_size\n\n        # load metadata and estimate total duration of training data\n        total_duration = self._load_metadata(protocol, subset=subset)\n\n        #\n        if per_epoch is None:\n\n            # 1 epoch = covering the whole training set once\n            #\n            per_epoch = total_duration / SECONDS_IN_A_DAY\n\n            # when exhaustive is False, this is not completely correct.\n            # in practice, it will randomly sample audio chunk until their\n            # overall duration reaches the duration of the training set.\n            # but nothing guarantees that every single part of the training set\n            # has been seen exactly once: it might be more than once, it might\n            # be less than once. on average, however, after a certain amount of\n            # epoch, this will be correct\n\n            # when exhaustive is True, however, we can actually make sure every\n            # single part of the training set has been seen. we just have to\n            # make sur we account for the step used by the exhaustive sliding\n            # window\n            if self.exhaustive:\n                per_epoch *= np.ceil(1 / self.step)\n\n        self.per_epoch = per_epoch\n\n    # TODO. use cached property (Python 3.8 only)\n    # https://docs.python.org/fr/3/library/functools.html#functools.cached_property\n    @property\n    def resolution(self):\n\n        if self.resolution_ in [None, RESOLUTION_FRAME]:\n            return self.feature_extraction.sliding_window\n\n        if self.resolution_ == RESOLUTION_CHUNK:\n            return self.SlidingWindow(\n                duration=self.duration, step=self.step * self.duration\n            )\n\n        return self.resolution_\n\n    def postprocess_y(self, Y: np.ndarray) -> np.ndarray:\n        """"""This function does nothing but return its input.\n        It should be overriden by subclasses.\n\n        Parameters\n        ----------\n        Y : (n_samples, n_speakers) numpy.ndarray\n\n        Returns\n        -------\n        postprocessed :\n\n        """"""\n        return Y\n\n    def initialize_y(self, current_file):\n        """"""Precompute y for the whole file\n\n        Parameters\n        ----------\n        current_file : `dict`\n            File as provided by a pyannote.database protocol.\n\n        Returns\n        -------\n        y : `SlidingWindowFeature`\n            Precomputed y for the whole file\n        """"""\n\n        if self.local_labels:\n            labels = current_file[""annotation""].labels()\n        else:\n            labels = self.segment_labels_\n\n        y = one_hot_encoding(\n            current_file[""annotation""],\n            get_annotated(current_file),\n            self.resolution,\n            labels=labels,\n            mode=""center"",\n        )\n\n        y.data = self.postprocess_y(y.data)\n        return y\n\n    def crop_y(self, y, segment):\n        """"""Extract y for specified segment\n\n        Parameters\n        ----------\n        y : `pyannote.core.SlidingWindowFeature`\n            Output of `initialize_y` above.\n        segment : `pyannote.core.Segment`\n            Segment for which to obtain y.\n\n        Returns\n        -------\n        cropped_y : (n_samples, dim) `np.ndarray`\n            y for specified `segment`\n        """"""\n\n        return y.crop(segment, mode=self.alignment, fixed=self.duration)\n\n    def _load_metadata(self, protocol, subset=""train"") -> float:\n        """"""Load training set metadata\n\n        This function is called once at instantiation time, returns the total\n        training set duration, and populates the following attributes:\n\n        Attributes\n        ----------\n        data_ : dict\n\n            {\'segments\': <list of annotated segments>,\n             \'duration\': <total duration of annotated segments>,\n             \'current_file\': <protocol dictionary>,\n             \'y\': <labels as numpy array>}\n\n        segment_labels_ : list\n            Sorted list of (unique) labels in protocol.\n\n        file_labels_ : dict of list\n            Sorted lists of (unique) file labels in protocol\n\n        Returns\n        -------\n        duration : float\n            Total duration of annotated segments, in seconds.\n        """"""\n\n        self.data_ = {}\n        segment_labels, file_labels = set(), dict()\n\n        # loop once on all files\n        for current_file in getattr(protocol, subset)():\n\n            # ensure annotation/annotated are cropped to actual file duration\n            support = Segment(start=0, end=current_file[""duration""])\n            current_file[""annotated""] = get_annotated(current_file).crop(\n                support, mode=""intersection""\n            )\n            current_file[""annotation""] = current_file[""annotation""].crop(\n                support, mode=""intersection""\n            )\n\n            # keep track of unique segment labels\n            segment_labels.update(current_file[""annotation""].labels())\n\n            # keep track of unique file labels\n            for key, value in current_file.items():\n                if isinstance(value, (Annotation, Timeline, SlidingWindowFeature)):\n                    continue\n                if key not in file_labels:\n                    file_labels[key] = set()\n                file_labels[key].add(value)\n\n            segments = [\n                s for s in current_file[""annotated""] if s.duration > self.duration\n            ]\n\n            # corner case where no segment is long enough\n            # and we removed them all...\n            if not segments:\n                continue\n\n            # total duration of label in current_file (after removal of\n            # short segments).\n            duration = sum(s.duration for s in segments)\n\n            # store all these in data_ dictionary\n            datum = {\n                ""segments"": segments,\n                ""duration"": duration,\n                ""current_file"": current_file,\n            }\n            uri = get_unique_identifier(current_file)\n            self.data_[uri] = datum\n\n        self.file_labels_ = {k: sorted(file_labels[k]) for k in file_labels}\n        self.segment_labels_ = sorted(segment_labels)\n\n        for uri in list(self.data_):\n            current_file = self.data_[uri][""current_file""]\n            y = self.initialize_y(current_file)\n            self.data_[uri][""y""] = y\n            if self.mask is not None:\n                mask = current_file[self.mask]\n                current_file[self.mask] = mask.align(y)\n\n        return sum(datum[""duration""] for datum in self.data_.values())\n\n    @property\n    def specifications(self):\n        """"""Task & sample specifications\n\n        Returns\n        -------\n        specs : `dict`\n            [\'task\'] (`pyannote.audio.train.Task`) : task\n            [\'X\'][\'dimension\'] (`int`) : features dimension\n            [\'y\'][\'classes\'] (`list`) : list of classes\n        """"""\n\n        specs = {\n            ""task"": self.task,\n            ""X"": {""dimension"": self.feature_extraction.dimension},\n        }\n\n        if not self.local_labels:\n            specs[""y""] = {""classes"": self.segment_labels_}\n\n        return specs\n\n    def samples(self):\n        if self.exhaustive:\n            return self._sliding_samples()\n        else:\n            return self._random_samples()\n\n    def _random_samples(self):\n        """"""Random samples\n\n        Returns\n        -------\n        samples : generator\n            Generator that yields {\'X\': ..., \'y\': ...} samples indefinitely.\n        """"""\n\n        uris = list(self.data_)\n        durations = np.array([self.data_[uri][""duration""] for uri in uris])\n        probabilities = durations / np.sum(durations)\n\n        while True:\n\n            # choose file at random with probability\n            # proportional to its (annotated) duration\n            uri = uris[np.random.choice(len(uris), p=probabilities)]\n\n            datum = self.data_[uri]\n            current_file = datum[""current_file""]\n\n            # choose one segment at random with probability\n            # proportional to its duration\n            segment = next(random_segment(datum[""segments""], weighted=True))\n\n            # choose fixed-duration subsegment at random\n            subsegment = next(random_subsegment(segment, self.duration))\n\n            X = self.feature_extraction.crop(\n                current_file, subsegment, mode=""center"", fixed=self.duration\n            )\n\n            y = self.crop_y(datum[""y""], subsegment)\n            sample = {""X"": X, ""y"": y}\n\n            if self.mask is not None:\n                mask = self.crop_y(current_file[self.mask], subsegment)\n                sample[""mask""] = mask\n\n            for key, classes in self.file_labels_.items():\n                sample[key] = classes.index(current_file[key])\n\n            yield sample\n\n    def _sliding_samples(self):\n\n        uris = list(self.data_)\n        durations = np.array([self.data_[uri][""duration""] for uri in uris])\n        probabilities = durations / np.sum(durations)\n        sliding_segments = SlidingWindow(\n            duration=self.duration, step=self.step * self.duration\n        )\n\n        while True:\n\n            np.random.shuffle(uris)\n\n            # loop on all files\n            for uri in uris:\n\n                datum = self.data_[uri]\n\n                # make a copy of current file\n                current_file = dict(datum[""current_file""])\n\n                # compute features for the whole file\n                features = self.feature_extraction(current_file)\n\n                # randomly shift \'annotated\' segments start time so that\n                # we avoid generating exactly the same subsequence twice\n                annotated = Timeline()\n                for segment in get_annotated(current_file):\n                    shifted_segment = Segment(\n                        segment.start + np.random.random() * self.duration, segment.end\n                    )\n                    if shifted_segment:\n                        annotated.add(shifted_segment)\n\n                samples = []\n                for sequence in sliding_segments(annotated):\n\n                    X = features.crop(sequence, mode=""center"", fixed=self.duration)\n                    y = self.crop_y(datum[""y""], sequence)\n                    sample = {""X"": X, ""y"": y}\n\n                    if self.mask is not None:\n\n                        # extract mask for current sub-segment\n                        mask = current_file[self.mask].crop(\n                            sequence, mode=""center"", fixed=self.duration\n                        )\n\n                        # it might happen that ""mask"" and ""y"" use different\n                        # sliding windows. therefore, we simply resample ""mask""\n                        # to match ""y""\n                        if len(mask) != len(y):\n                            mask = scipy.signal.resample(mask, len(y), axis=0)\n                        sample[""mask""] = mask\n\n                    for key, classes in self.file_labels_.items():\n                        sample[key] = classes.index(current_file[key])\n\n                    samples.append(sample)\n\n                np.random.shuffle(samples)\n                for sample in samples:\n                    yield sample\n\n    @property\n    def batches_per_epoch(self):\n        """"""Number of batches needed to complete an epoch""""""\n        duration_per_epoch = self.per_epoch * SECONDS_IN_A_DAY\n        duration_per_batch = self.duration * self.batch_size\n        return int(np.ceil(duration_per_epoch / duration_per_batch))\n\n\nclass LabelingTask(Trainer):\n    """"""Base class for various labeling tasks\n\n    This class should be inherited from: it should not be used directy\n\n    Parameters\n    ----------\n    duration : float, optional\n        Duration of audio chunks. Defaults to 2s.\n    batch_size : int, optional\n        Batch size. Defaults to 32.\n    per_epoch : float, optional\n        Force total audio duration per epoch, in days.\n        Defaults to total duration of protocol subset.\n    exhaustive : bool, optional\n        Ensure training files are covered exhaustively (useful in case of\n        non-uniform label distribution).\n    step : `float`, optional\n        Ratio of audio chunk duration used as step between two consecutive\n        audio chunks. Defaults to 0.1. Has not effect when exhaustive is False.\n    """"""\n\n    def __init__(\n        self,\n        duration: float = 2.0,\n        batch_size: int = 32,\n        per_epoch: float = None,\n        exhaustive: bool = False,\n        step: float = 0.1,\n    ):\n        super(LabelingTask, self).__init__()\n        self.duration = duration\n        self.batch_size = batch_size\n        self.per_epoch = per_epoch\n        self.exhaustive = exhaustive\n        self.step = step\n\n    def get_batch_generator(\n        self,\n        feature_extraction: Wrappable,\n        protocol: Protocol,\n        subset: Text = ""train"",\n        resolution: Optional[Resolution] = None,\n        alignment: Optional[Alignment] = None,\n    ) -> LabelingTaskGenerator:\n        """"""This method should be overriden by subclass\n\n        Parameters\n        ----------\n        feature_extraction : Wrappable\n            Describes how features should be obtained.\n            See pyannote.audio.features.wrapper.Wrapper documentation for details.\n        protocol : Protocol\n        subset : {\'train\', \'development\'}, optional\n            Defaults to \'train\'.\n        resolution : `pyannote.core.SlidingWindow`, optional\n            Override `feature_extraction.sliding_window`. This is useful for\n            models that include the feature extraction step (e.g. SincNet) and\n            therefore output a lower sample rate than that of the input.\n        alignment : {\'center\', \'loose\', \'strict\'}, optional\n            Which mode to use when cropping labels. This is useful for models\n            that include the feature extraction step (e.g. SincNet) and\n            therefore use a different cropping mode. Defaults to \'center\'.\n\n        Returns\n        -------\n        batch_generator : `LabelingTaskGenerator`\n        """"""\n\n        return LabelingTaskGenerator(\n            self.task,\n            feature_extraction,\n            protocol,\n            subset=subset,\n            resolution=resolution,\n            alignment=alignment,\n            duration=self.duration,\n            per_epoch=self.per_epoch,\n            batch_size=self.batch_size,\n            exhaustive=self.exhaustive,\n            step=self.step,\n        )\n\n    @property\n    def weight(self):\n        """"""Class/task weights\n\n        Returns\n        -------\n        weight : None or `torch.Tensor`\n        """"""\n        return None\n\n    def on_train_start(self):\n        """"""Set loss function (with support for class weights)\n\n        loss_func_ = Function f(input, target, weight=None) -> loss value\n        """"""\n\n        self.task_ = self.model_.task\n\n        if self.task_.is_multiclass_classification:\n\n            self.n_classes_ = len(self.model_.classes)\n\n            def loss_func(input, target, weight=None, mask=None):\n                if mask is None:\n                    return F.nll_loss(input, target, weight=weight, reduction=""mean"")\n                else:\n                    return torch.mean(\n                        mask\n                        * F.nll_loss(input, target, weight=weight, reduction=""none"")\n                    )\n\n        if self.task_.is_multilabel_classification:\n\n            def loss_func(input, target, weight=None, mask=None):\n                if mask is None:\n                    return F.binary_cross_entropy(\n                        input, target, weight=weight, reduction=""mean""\n                    )\n                else:\n                    return torch.mean(\n                        mask\n                        * F.binary_cross_entropy(\n                            input, target, weight=weight, reduction=""none""\n                        )\n                    )\n\n        if self.task_.is_regression:\n\n            def loss_func(input, target, weight=None, mask=None):\n                if mask is None:\n                    return F.mse_loss(input, target, reduction=""mean"")\n                else:\n                    return torch.mean(\n                        mask * F.mse_loss(input, target, reduction=""none"")\n                    )\n\n        self.loss_func_ = loss_func\n\n    def batch_loss(self, batch):\n        """"""Compute loss for current `batch`\n\n        Parameters\n        ----------\n        batch : `dict`\n            [\'X\'] (`numpy.ndarray`)\n            [\'y\'] (`numpy.ndarray`)\n            [\'mask\'] (`numpy.ndarray`, optional)\n\n        Returns\n        -------\n        batch_loss : `dict`\n            [\'loss\'] (`torch.Tensor`) : Loss\n        """"""\n\n        # forward pass\n        X = torch.tensor(batch[""X""], dtype=torch.float32, device=self.device_)\n        fX = self.model_(X)\n\n        mask = None\n        if self.task_.is_multiclass_classification:\n\n            fX = fX.view((-1, self.n_classes_))\n\n            target = (\n                torch.tensor(batch[""y""], dtype=torch.int64, device=self.device_)\n                .contiguous()\n                .view((-1,))\n            )\n\n            if ""mask"" in batch:\n                mask = (\n                    torch.tensor(\n                        batch[""mask""], dtype=torch.float32, device=self.device_\n                    )\n                    .contiguous()\n                    .view((-1,))\n                )\n\n        elif self.task_.is_multilabel_classification or self.task_.is_regression:\n\n            target = torch.tensor(batch[""y""], dtype=torch.float32, device=self.device_)\n\n            if ""mask"" in batch:\n                mask = torch.tensor(\n                    batch[""mask""], dtype=torch.float32, device=self.device_\n                )\n\n        weight = self.weight\n        if weight is not None:\n            weight = weight.to(device=self.device_)\n\n        return {\n            ""loss"": self.loss_func_(fX, target, weight=weight, mask=mask),\n        }\n\n    @property\n    def task(self):\n        return Task(\n            type=TaskType.MULTI_CLASS_CLASSIFICATION, output=TaskOutput.SEQUENCE\n        )\n'"
pyannote/audio/labeling/tasks/domain_classification.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2019 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n""""""Domain classification""""""\n\nfrom typing import Optional\nfrom typing import Text\n\nimport numpy as np\nfrom .base import LabelingTask\nfrom .base import LabelingTaskGenerator\nfrom pyannote.audio.train.task import Task, TaskType, TaskOutput\nfrom pyannote.audio.features.wrapper import Wrappable\nfrom pyannote.database.protocol.protocol import Protocol\nfrom pyannote.audio.train.model import Resolution\nfrom pyannote.audio.train.model import Alignment\n\n\nclass DomainClassificationGenerator(LabelingTaskGenerator):\n    """"""Batch generator for training domain classification\n\n    Parameters\n    ----------\n    task : Task\n        Task\n    feature_extraction : Wrappable\n        Describes how features should be obtained.\n        See pyannote.audio.features.wrapper.Wrapper documentation for details.\n    protocol : Protocol\n    subset : {\'train\', \'development\', \'test\'}, optional\n        Protocol and subset.\n    resolution : `pyannote.core.SlidingWindow`, optional\n        Override `feature_extraction.sliding_window`. This is useful for\n        models that include the feature extraction step (e.g. SincNet) and\n        therefore output a lower sample rate than that of the input.\n        Defaults to `feature_extraction.sliding_window`\n    alignment : {\'center\', \'loose\', \'strict\'}, optional\n        Which mode to use when cropping labels. This is useful for models that\n        include the feature extraction step (e.g. SincNet) and therefore use a\n        different cropping mode. Defaults to \'center\'.\n    duration : float, optional\n        Duration of audio chunks. Defaults to 2s.\n    batch_size : int, optional\n        Batch size. Defaults to 32.\n    per_epoch : float, optional\n        Force total audio duration per epoch, in days.\n        Defaults to total duration of protocol subset.\n    domain : `str`, optional\n        Key to use as domain. Defaults to \'domain\'.\n    """"""\n\n    def __init__(\n        self,\n        task: Task,\n        feature_extraction: Wrappable,\n        protocol: Protocol,\n        subset: Text = ""train"",\n        resolution: Optional[Resolution] = None,\n        alignment: Optional[Alignment] = None,\n        duration: float = 2.0,\n        batch_size: int = 32,\n        per_epoch: float = None,\n        domain: Text = ""domain"",\n    ):\n\n        self.domain = domain\n\n        super().__init__(\n            task,\n            feature_extraction,\n            protocol,\n            subset=subset,\n            resolution=resolution,\n            alignment=alignment,\n            duration=duration,\n            batch_size=batch_size,\n            per_epoch=per_epoch,\n            exhaustive=False,\n        )\n\n    def initialize_y(self, current_file):\n        return self.file_labels_[self.domain].index(current_file[self.domain])\n\n    def crop_y(self, y, segment):\n        return y\n\n    @property\n    def specifications(self):\n        return {\n            ""task"": self.task,\n            ""X"": {""dimension"": self.feature_extraction.dimension},\n            ""y"": {""classes"": self.file_labels_[self.domain]},\n        }\n\n\nclass DomainClassification(LabelingTask):\n    """"""Train domain classification\n\n    Parameters\n    ----------\n    domain : `str`, optional\n        Key to use as domain. Defaults to \'domain\'.\n    duration : float, optional\n        Duration of sub-sequences. Defaults to 3.2s.\n    batch_size : int, optional\n        Batch size. Defaults to 32.\n    per_epoch : float, optional\n        Total audio duration per epoch, in days.\n        Defaults to one day (1).\n    """"""\n\n    def __init__(self, domain=""domain"", **kwargs):\n        super().__init__(**kwargs)\n        self.domain = domain\n\n    def get_batch_generator(\n        self,\n        feature_extraction: Wrappable,\n        protocol: Protocol,\n        subset: Text = ""train"",\n        **kwargs\n    ) -> DomainClassificationGenerator:\n        """"""Get batch generator for domain classification\n\n        Parameters\n        ----------\n        feature_extraction : Wrappable\n            Feature extraction.\n        protocol : Protocol\n        subset : {\'train\', \'development\', \'test\'}, optional\n            Protocol and subset used for batch generation.\n\n        Returns\n        -------\n        batch_generator : `DomainClassificationGenerator`\n            Batch generator\n        """"""\n        return DomainClassificationGenerator(\n            self.task,\n            feature_extraction,\n            protocol,\n            subset=subset,\n            domain=self.domain,\n            duration=self.duration,\n            per_epoch=self.per_epoch,\n            batch_size=self.batch_size,\n        )\n\n    @property\n    def task(self):\n        return Task(type=TaskType.MULTI_CLASS_CLASSIFICATION, output=TaskOutput.VECTOR)\n'"
pyannote/audio/labeling/tasks/overlap_detection.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2019-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\nfrom typing import Optional\nfrom typing import Text\n\nimport numpy as np\nfrom pyannote.database import get_annotated\nfrom pyannote.core import Segment\nfrom pyannote.core import SlidingWindow\nfrom pyannote.core import Timeline\n\nfrom pyannote.core.utils.random import random_segment\nfrom pyannote.core.utils.random import random_subsegment\n\nfrom pyannote.audio.features import RawAudio\n\nfrom .base import LabelingTask\nfrom .base import LabelingTaskGenerator\nfrom pyannote.audio.train.task import Task, TaskType, TaskOutput\n\nfrom pyannote.audio.features.wrapper import Wrappable\nfrom pyannote.database.protocol.protocol import Protocol\nfrom pyannote.audio.train.model import Resolution\nfrom pyannote.audio.train.model import Alignment\n\n\nnormalize = lambda wav: wav / (np.sqrt(np.mean(wav ** 2)) + 1e-8)\n\n\nclass OverlapDetectionGenerator(LabelingTaskGenerator):\n    """"""Batch generator for training overlap detection\n\n    Parameters\n    ----------\n    task : Task\n        Task\n    feature_extraction : Wrappable\n        Describes how features should be obtained.\n        See pyannote.audio.features.wrapper.Wrapper documentation for details.\n    protocol : Protocol\n    subset : {\'train\', \'development\', \'test\'}, optional\n        Protocol and subset.\n    resolution : `pyannote.core.SlidingWindow`, optional\n        Override `feature_extraction.sliding_window`. This is useful for\n        models that include the feature extraction step (e.g. SincNet) and\n        therefore output a lower sample rate than that of the input.\n        Defaults to `feature_extraction.sliding_window`\n    alignment : {\'center\', \'loose\', \'strict\'}, optional\n        Which mode to use when cropping labels. This is useful for models that\n        include the feature extraction step (e.g. SincNet) and therefore use a\n        different cropping mode. Defaults to \'center\'.\n    duration : float, optional\n        Duration of audio chunks. Defaults to 2s.\n    batch_size : int, optional\n        Batch size. Defaults to 32.\n    per_epoch : float, optional\n        Force total audio duration per epoch, in days.\n        Defaults to total duration of protocol subset.\n    snr_min, snr_max : float, optional\n        Defines Signal-to-Overlap Ratio range in dB. Defaults to [0, 10].\n    """"""\n\n    def __init__(\n        self,\n        task: Task,\n        feature_extraction: Wrappable,\n        protocol: Protocol,\n        subset: Text = ""train"",\n        resolution: Optional[Resolution] = None,\n        alignment: Optional[Alignment] = None,\n        duration: float = 2.0,\n        batch_size: int = 32,\n        per_epoch: float = None,\n        snr_min: float = 0,\n        snr_max: float = 10,\n    ):\n\n        self.snr_min = snr_min\n        self.snr_max = snr_max\n        self.raw_audio_ = RawAudio(sample_rate=feature_extraction.sample_rate)\n\n        super().__init__(\n            task,\n            feature_extraction,\n            protocol,\n            subset=subset,\n            resolution=resolution,\n            alignment=alignment,\n            duration=duration,\n            batch_size=batch_size,\n            per_epoch=per_epoch,\n            # TODO. be smart and find a way to use ""local_labels""\n            local_labels=False,\n        )\n\n    def overlap_samples(self):\n        """"""Random overlap samples\n\n        Returns\n        -------\n        samples : generator\n            Generator that yields {\'waveform\': ..., \'y\': ...} samples\n            indefinitely.\n        """"""\n\n        uris = list(self.data_)\n        durations = np.array([self.data_[uri][""duration""] for uri in uris])\n        probabilities = durations / np.sum(durations)\n\n        while True:\n\n            # choose file at random with probability\n            # proportional to its (annotated) duration\n            uri = uris[np.random.choice(len(uris), p=probabilities)]\n\n            datum = self.data_[uri]\n            current_file = datum[""current_file""]\n\n            # choose one segment at random with probability\n            # proportional to its duration\n            segment = next(random_segment(datum[""segments""], weighted=True))\n\n            # choose random subsegment\n            # duration = np.random.rand() * self.duration\n            sequence = next(random_subsegment(segment, self.duration))\n\n            # get corresponding waveform\n            X = self.raw_audio_.crop(\n                current_file, sequence, mode=""center"", fixed=self.duration\n            )\n\n            # get corresponding labels\n            y = datum[""y""].crop(sequence, mode=self.alignment, fixed=self.duration)\n\n            yield {""waveform"": normalize(X), ""y"": y}\n\n    def sliding_samples(self):\n        """"""Sliding window\n\n        Returns\n        -------\n        samples : generator\n            Generator that yields {\'waveform\': ..., \'y\': ...} samples\n            indefinitely.\n        """"""\n\n        uris = list(self.data_)\n        durations = np.array([self.data_[uri][""duration""] for uri in uris])\n        probabilities = durations / np.sum(durations)\n\n        sliding_segments = SlidingWindow(duration=self.duration, step=self.duration)\n\n        while True:\n\n            # shuffle files\n            np.random.shuffle(uris)\n\n            # loop on shuffled files\n            for uri in uris:\n\n                datum = self.data_[uri]\n\n                # make a copy of current file\n                current_file = dict(datum[""current_file""])\n\n                # read waveform for the whole file\n                waveform = self.raw_audio_(current_file)\n\n                # randomly shift \'annotated\' segments start time so that\n                # we avoid generating exactly the same subsequence twice\n                shifted_segments = [\n                    Segment(s.start + np.random.random() * self.duration, s.end)\n                    for s in get_annotated(current_file)\n                ]\n                # deal with corner case where a shifted segment would be empty\n                shifted_segments = [s for s in shifted_segments if s]\n                annotated = Timeline(segments=shifted_segments)\n\n                samples = []\n                for sequence in sliding_segments(annotated):\n\n                    X = waveform.crop(sequence, mode=""center"", fixed=self.duration)\n\n                    y = datum[""y""].crop(\n                        sequence, mode=self.alignment, fixed=self.duration\n                    )\n\n                    # FIXME -- this is ugly\n                    sample = {\n                        ""waveform"": normalize(X),\n                        ""y"": y,\n                        ""database"": current_file[""database""],\n                        ""uri"": current_file[""uri""],\n                        ""audio"": current_file[""audio""],\n                        ""duration"": current_file[""duration""],\n                    }\n\n                    samples.append(sample)\n\n                np.random.shuffle(samples)\n                for sample in samples:\n                    yield sample\n\n    def samples(self):\n        """"""Training sample generator""""""\n\n        sliding_samples = self.sliding_samples()\n        overlap_samples = self.overlap_samples()\n\n        while True:\n\n            # get fixed duration random sequence\n            original = next(sliding_samples)\n\n            if np.random.rand() < 0.5:\n                pass\n\n            else:\n                # get random overlapping sequence\n                overlap = next(overlap_samples)\n\n                # select SNR at random\n                snr = (\n                    self.snr_max - self.snr_min\n                ) * np.random.random_sample() + self.snr_min\n                alpha = np.exp(-np.log(10) * snr / 20)\n\n                original[""waveform""] += alpha * overlap[""waveform""]\n                original[""y""] += overlap[""y""]\n\n            speaker_count = np.sum(original[""y""], axis=1, keepdims=True)\n            original[""y""] = np.int64(speaker_count > 1)\n\n            # run feature extraction\n            original[""X""] = self.feature_extraction.crop(\n                original, Segment(0, self.duration), mode=""center"", fixed=self.duration\n            )\n\n            yield {""X"": original[""X""], ""y"": original[""y""]}\n\n    @property\n    def specifications(self):\n        return {\n            ""task"": self.task,\n            ""X"": {""dimension"": self.feature_extraction.dimension},\n            ""y"": {""classes"": [""non_overlap"", ""overlap""]},\n        }\n\n\nclass OverlapDetection(LabelingTask):\n    """"""Train overlap detection\n\n    Parameters\n    ----------\n    duration : float, optional\n        Duration of sub-sequences. Defaults to 3.2s.\n    batch_size : int, optional\n        Batch size. Defaults to 32.\n    per_epoch : float, optional\n        Total audio duration per epoch, in days.\n        Defaults to one day (1).\n    """"""\n\n    def get_batch_generator(\n        self,\n        feature_extraction: Wrappable,\n        protocol: Protocol,\n        subset: Text = ""train"",\n        resolution: Optional[Resolution] = None,\n        alignment: Optional[Resolution] = None,\n    ) -> OverlapDetectionGenerator:\n        """"""Get batch generator\n\n        Parameters\n        ----------\n        feature_extraction : Wrappable\n            Describes how features should be obtained.\n            See pyannote.audio.features.wrapper.Wrapper documentation for details.\n        protocol : Protocol\n        subset : {\'train\', \'development\', \'test\'}, optional\n            Protocol and subset.\n        resolution : `pyannote.core.SlidingWindow`, optional\n            Override `feature_extraction.sliding_window`. This is useful for\n            models that include the feature extraction step (e.g. SincNet) and\n            therefore output a lower sample rate than that of the input.\n        alignment : {\'center\', \'loose\', \'strict\'}, optional\n            Which mode to use when cropping labels. This is useful for models\n            that include the feature extraction step (e.g. SincNet) and\n            therefore use a different cropping mode. Defaults to \'center\'.\n        """"""\n        return OverlapDetectionGenerator(\n            self.task,\n            feature_extraction,\n            protocol,\n            subset=subset,\n            resolution=resolution,\n            alignment=alignment,\n            duration=self.duration,\n            per_epoch=self.per_epoch,\n            batch_size=self.batch_size,\n        )\n'"
pyannote/audio/labeling/tasks/resegmentation.py,7,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2018-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n""""""Resegmentation""""""\n\nfrom typing import Optional\nfrom typing import Type\nfrom typing import Iterable\nfrom typing import Dict\nfrom typing import Text\nimport scipy.signal\n\nimport torch\nimport tempfile\nimport numpy as np\nfrom .base import LabelingTask\nfrom .base import LabelingTaskGenerator\nfrom pyannote.audio.train.task import Task, TaskType, TaskOutput\nfrom pyannote.core import Timeline\nfrom pyannote.core import Annotation\nfrom pyannote.core import SlidingWindow\nfrom pyannote.core import SlidingWindowFeature\nfrom pyannote.database.protocol import SpeakerDiarizationProtocol\nfrom pyannote.database import get_annotated\nfrom pyannote.core.utils.numpy import one_hot_decoding\nfrom pyannote.core.utils.numpy import one_hot_encoding\nfrom pyannote.audio.train.schedulers import ConstantScheduler\nfrom torch.optim import SGD\nfrom pathlib import Path\nfrom pyannote.audio.utils.signal import Binarize\n\nfrom pyannote.audio.features import FeatureExtraction\nfrom pyannote.database.protocol.protocol import ProtocolFile\nfrom pyannote.audio.train.model import Model\nfrom pyannote.audio.train.model import Resolution\nfrom pyannote.audio.train.model import Alignment\nfrom pyannote.audio.train.task import Task\n\n\nclass ResegmentationGenerator(LabelingTaskGenerator):\n    """"""Batch generator for resegmentation self-training\n\n    Parameters\n    ----------\n    tasl : Task\n    current_file : `ProtocolFile`\n    resolution : `pyannote.core.SlidingWindow`, optional\n        Override `feature_extraction.sliding_window`. This is useful for\n        models that include the feature extraction step (e.g. SincNet) and\n        therefore output a lower sample rate than that of the input.\n    alignment : {\'center\', \'loose\', \'strict\'}, optional\n        Which mode to use when cropping labels. This is useful for models that\n        include the feature extraction step (e.g. SincNet) and therefore use a\n        different cropping mode. Defaults to \'center\'.\n    duration : float, optional\n        Duration of audio chunks. Defaults to 4s.\n    batch_size : int, optional\n        Batch size. Defaults to 32.\n    allow_overlap : bool, optional\n        Allow overlapping speakers. Defaults to False.\n        Does not really work for now...\n    mask : str, optional\n        When provided, current_file[mask] is used by the loss function to weigh\n        samples.\n    """"""\n\n    def __init__(\n        self,\n        task: Task,\n        current_file: ProtocolFile,\n        resolution: Optional[Resolution] = None,\n        alignment: Optional[Alignment] = None,\n        duration: float = 2,\n        batch_size: int = 32,\n        lock_speech: bool = False,\n        allow_overlap: bool = False,\n        mask: Text = None,\n    ):\n\n        self.current_file = current_file\n        self.allow_overlap = allow_overlap\n        self.lock_speech = lock_speech\n\n        super().__init__(\n            task,\n            ""@features"",\n            self.get_dummy_protocol(current_file),\n            subset=""train"",\n            resolution=resolution,\n            alignment=alignment,\n            duration=duration,\n            batch_size=batch_size,\n            mask=mask,\n        )\n\n    @property\n    def resolution(self):\n        return self.current_file[""features""].sliding_window\n\n    def get_dummy_protocol(\n        self, current_file: ProtocolFile\n    ) -> SpeakerDiarizationProtocol:\n        """"""Get dummy protocol containing only `current_file`\n\n        Parameters\n        ----------\n        current_file : ProtocolFile\n\n        Returns\n        -------\n        protocol : SpeakerDiarizationProtocol instance\n            Dummy protocol containing only `current_file` in both train,\n            dev., and test sets.\n\n        """"""\n\n        class DummyProtocol(SpeakerDiarizationProtocol):\n            def trn_iter(self):\n                yield current_file\n\n            def dev_iter(self):\n                yield current_file\n\n            def tst_iter(self):\n                yield current_file\n\n        return DummyProtocol()\n\n    def postprocess_y(self, Y: np.ndarray) -> np.ndarray:\n        """"""Generate labels for resegmentation\n\n        Parameters\n        ----------\n        Y : (n_samples, n_speakers) numpy.ndarray\n            Discretized annotation returned by\n            `pyannote.core.utils.numpy.one_hot_encoding`.\n\n        Returns\n        -------\n        y : (n_samples, 1) or (n_samples, n_speakers) numpy.ndarray\n            When allow_overlap is True, y has shape (n_samples, n_speakers)\n                * y[t, i] = 1 means speaker i is active\n            When allow_overlap is False, y has shape (n_samples, 1)\n                * y[t] = 0 indicates non-speech,\n                * y[t] = i + 1 indicates speaker i.\n\n        See also\n        --------\n        `pyannote.core.utils.numpy.one_hot_encoding`\n        """"""\n\n        # when allowing overlap, multiple speakers can be active at once.\n        # hence, Y sticks to shape (n_samples, n_speakers)\n        if self.allow_overlap:\n            return Y\n\n        # when overlap is not allowed, reshape Y to (n_samples, 1)\n        else:\n\n            # if speech / non-speech status is locked\n            # there is no class for non-speech.\n            if self.lock_speech:\n                y = np.argmax(Y, axis=1)\n                return np.int64(y)[:, np.newaxis]\n\n            # otherwise, we add a non-speech class at index 0\n            else:\n\n                # +1 because...\n                y = np.argmax(Y, axis=1) + 1\n\n                # ... 0 is for non-speech\n                non_speech = np.sum(Y, axis=1) == 0\n                y[non_speech] = 0\n\n                return np.int64(y)[:, np.newaxis]\n\n    @property\n    def specifications(self) -> Dict:\n        """"""Task & sample specifications\n\n        Returns\n        -------\n        specs : `dict`\n            [\'task\'] (`pyannote.audio.train.Task`) : task\n            [\'X\'][\'dimension\'] (`int`) : features dimension\n            [\'y\'][\'classes\'] (`list`) : list of classes\n        """"""\n\n        specs = {\n            ""X"": {""dimension"": self.current_file[""features""].dimension},\n            ""task"": self.task,\n        }\n\n        if self.allow_overlap:\n            specs[""y""] = {""classes"": self.segment_labels_}\n\n        else:\n            if self.lock_speech:\n                # when locking speech / non-speech status,\n                # there is no non-speech class\n                specs[""y""] = {""classes"": self.segment_labels_}\n            else:\n                # when speech / non-speech status can be updated,\n                # one must add a non-speech class\n                specs[""y""] = {""classes"": [""non_speech""] + self.segment_labels_}\n\n        return specs\n\n\nclass Resegmentation(LabelingTask):\n    """"""Re-segmentation\n\n    Parameters\n    ----------\n    feature_extraction : FeatureExtraction\n        Feature extraction.\n    Architecture : Model subclass\n    architecture_params : dict\n    epochs : `int`, optional\n        (Self-)train for that many epochs. Defaults to 5.\n    ensemble : `int`, optional\n        Average output of last `ensemble` epochs. Defaults to no ensembling.\n    duration : float, optional\n        Duration of audio chunks. Defaults to 2s.\n    step : `float`, optional\n        Ratio of audio chunk duration used as step between two consecutive\n        audio chunks. Defaults to 0.1.\n    batch_size : int, optional\n        Batch size. Defaults to 32.\n    device : `torch.device`, optional\n    lock_speech: `boolean`, optional\n        Keep speech/non-speech state unchanged. Defaults to False.\n    allow_overlap : bool, optional\n        Allow overlapping speakers. Defaults to False.\n    mask : str, optional\n        When provided, current_file[mask] is used by the loss function to weigh\n        samples.\n    """"""\n\n    def __init__(\n        self,\n        feature_extraction: FeatureExtraction,\n        Architecture: Type[Model],\n        architecture_params: dict,\n        lock_speech: bool = False,\n        epochs: int = 5,\n        learning_rate: float = 0.1,\n        ensemble: int = 1,\n        duration: float = 2.0,\n        step: float = 0.1,\n        n_jobs: int = 1,\n        device: torch.device = None,\n        batch_size: int = 32,\n        allow_overlap: bool = False,\n        mask: Text = None,\n    ):\n\n        self.feature_extraction = feature_extraction\n\n        self.Architecture = Architecture\n        self.architecture_params = architecture_params\n\n        self.epochs = epochs\n        self.learning_rate = learning_rate\n\n        self.ensemble = ensemble\n\n        self.n_jobs = n_jobs\n\n        self.lock_speech = lock_speech\n        self.allow_overlap = allow_overlap\n        self.mask = mask\n\n        if device is None:\n            device = ""cuda"" if torch.cuda.is_available() else ""cpu""\n        self.device_ = torch.device(device)\n\n        super().__init__(\n            duration=duration, batch_size=batch_size, per_epoch=None, step=step\n        )\n\n    @property\n    def task(self):\n        if self.allow_overlap:\n            # when allowing overlap, multiple speakers can be active at the\n            # same time (hence multi-label classification task)\n            return Task(\n                type=TaskType.MULTI_LABEL_CLASSIFICATION, output=TaskOutput.SEQUENCE\n            )\n\n        else:\n            # when overlap is not allowed, only one speaker can be active at\n            # a particular time (hence: multi-class classification)\n            return Task(\n                type=TaskType.MULTI_CLASS_CLASSIFICATION, output=TaskOutput.SEQUENCE\n            )\n\n    def get_batch_generator(\n        self, current_file: ProtocolFile\n    ) -> ResegmentationGenerator:\n        """"""Get batch generator for current file\n\n        Parameters\n        ----------\n        current_file : `dict`\n            Dictionary obtained by iterating over a subset of a\n            `pyannote.database.Protocol` instance.\n\n        Returns\n        -------\n        batch_generator : `ResegmentationGenerator`\n        """"""\n\n        resolution = self.Architecture.get_resolution(\n            self.task, **self.architecture_params\n        )\n        alignment = self.Architecture.get_alignment(\n            self.task, **self.architecture_params\n        )\n\n        return ResegmentationGenerator(\n            self.task,\n            current_file,\n            resolution=resolution,\n            alignment=alignment,\n            duration=self.duration,\n            batch_size=self.batch_size,\n            lock_speech=self.lock_speech,\n            allow_overlap=self.allow_overlap,\n            mask=self.mask,\n        )\n\n    def _decode(\n        self,\n        current_file: ProtocolFile,\n        hypothesis: Annotation,\n        scores: SlidingWindowFeature,\n        labels: Iterable,\n    ) -> Annotation:\n\n        N, K = scores.data.shape\n\n        if self.allow_overlap:\n            active_speakers = scores.data > 0.5\n\n        else:\n            if self.lock_speech:\n                active_speakers = np.argmax(scores.data, axis=1) + 1\n\n            else:\n                active_speakers = np.argmax(scores.data, axis=1)\n\n        # reconstruct annotation\n        new_hypothesis = one_hot_decoding(\n            active_speakers, scores.sliding_window, labels=labels\n        )\n\n        new_hypothesis.uri = hypothesis.uri\n\n        if self.lock_speech:\n            speech = hypothesis.get_timeline().support()\n            new_hypothesis = new_hypothesis.crop(speech)\n\n        return new_hypothesis\n\n    def __call__(\n        self,\n        current_file: ProtocolFile,\n        hypothesis: Annotation,\n        debugging: bool = False,\n    ) -> Annotation:\n        """"""Apply resegmentation using self-supervised sequence labeling\n\n        Parameters\n        ----------\n        current_file : ProtocolFile\n            Dictionary obtained by iterating over a subset of a\n            `pyannote.database.Protocol` instance.\n        hypothesis : Annotation, optional\n            Current diarization output. Defaults to current_file[\'hypothesis\'].\n\n        Returns\n        -------\n        new_hypothesis : Annotation\n            Updated diarization output.\n        """"""\n\n        # make sure current_file is not modified\n        current_file = dict(current_file)\n\n        current_file[""annotation""] = hypothesis\n        current_file[""features""] = self.feature_extraction(current_file)\n\n        debug = {}\n\n        # when locking speech / non-speech status, we add a (or update\n        # existing) mask so that the loss is not computed on non-speech regions\n        if self.lock_speech:\n\n            encoded = one_hot_encoding(\n                hypothesis,\n                get_annotated(current_file),\n                current_file[""features""].sliding_window,\n                mode=""center"",\n            )\n            speech = 1.0 * (np.sum(encoded, axis=1, keepdims=True) > 0)\n            current_file[""speech""] = speech\n            debug[""speech""] = speech\n\n            if self.mask is None:\n                self.mask = ""speech""\n\n            else:\n                mask = current_file[self.mask]\n                current_file[self.mask] = mask * speech.align(mask)\n\n            debug[""mask""] = current_file[self.mask]\n\n        batch_generator = self.get_batch_generator(current_file)\n\n        model = self.Architecture(\n            batch_generator.specifications, **self.architecture_params\n        )\n\n        chunks = SlidingWindow(duration=self.duration, step=self.step * self.duration)\n\n        # create a temporary directory to store models and log files\n        # it is removed automatically before returning.\n        with tempfile.TemporaryDirectory() as train_dir:\n\n            epochs = self.fit_iter(\n                model,\n                batch_generator,\n                warm_start=0,\n                epochs=self.epochs,\n                get_optimizer=SGD,\n                scheduler=ConstantScheduler(),\n                learning_rate=self.learning_rate,\n                train_dir=Path(train_dir),\n                verbosity=1,\n                device=self.device,\n                callbacks=None,\n                n_jobs=self.n_jobs,\n            )\n\n            scores = []\n            for i, current_model in enumerate(epochs):\n\n                # do not compute scores that are not used in later ensembling\n                # simply jump to next training epoch (except when debugging)\n                if not debugging and i < self.epochs - self.ensemble:\n                    continue\n\n                current_model.eval()\n\n                scores.append(\n                    current_model.slide(\n                        current_file[""features""],\n                        chunks,\n                        batch_size=self.batch_size,\n                        device=self.device,\n                        return_intermediate=None,\n                        progress_hook=None,\n                    )\n                )\n                current_model.train()\n\n        debug[""scores""] = scores\n\n        # ensemble scores\n        scores = SlidingWindowFeature(\n            np.mean([s.data for s in scores[-self.ensemble :]], axis=0),\n            scores[-1].sliding_window,\n        )\n        debug[""final_scores""] = scores\n\n        labels = batch_generator.specifications[""y""][""classes""]\n        if not self.lock_speech:\n            labels = labels[1:]\n\n        debug[""labels""] = labels\n\n        decoded = self._decode(current_file, hypothesis, scores, labels)\n\n        decoded.debug = debug\n        return decoded\n\n\nclass ResegmentationWithOverlap(Resegmentation):\n    """"""Re-segmentation with overlap\n\n    Parameters\n    ----------\n    feature_extraction : FeatureExtraction\n        Feature extraction.\n    Architecture : Model subclass\n    architecture_params : dict\n    overlap_threshold : `float`, optional\n        Defaults to 0.5.\n    lock_speech: `boolean`, optional\n        Keep speech/non-speech state unchanged. Defaults to False.\n    epochs : `int`, optional\n        (Self-)train for that many epochs. Defaults to 5.\n    ensemble : `int`, optional\n        Average output of last `ensemble` epochs. Defaults to no ensembling.\n    duration : float, optional\n        Duration of audio chunks. Defaults to 2s.\n    step : `float`, optional\n        Ratio of audio chunk duration used as step between two consecutive\n        audio chunks. Defaults to 0.1.\n    batch_size : int, optional\n        Batch size. Defaults to 32.\n    device : `torch.device`, optional\n    mask : str, optional\n        When provided, current_file[mask] is used by the loss function to weigh\n        samples.\n    """"""\n\n    def __init__(\n        self,\n        feature_extraction: FeatureExtraction,\n        Architecture: Type[Model],\n        architecture_params: dict,\n        lock_speech: bool = False,\n        overlap_threshold: float = 0.5,\n        epochs: int = 5,\n        learning_rate: float = 0.1,\n        ensemble: int = 1,\n        duration: float = 2.0,\n        step: float = 0.1,\n        n_jobs: int = 1,\n        device: torch.device = None,\n        batch_size: int = 32,\n        mask: Text = None,\n    ):\n\n        super().__init__(\n            feature_extraction,\n            Architecture,\n            architecture_params,\n            lock_speech=lock_speech,\n            epochs=epochs,\n            learning_rate=learning_rate,\n            ensemble=ensemble,\n            duration=duration,\n            step=step,\n            n_jobs=n_jobs,\n            device=device,\n            batch_size=batch_size,\n            mask=mask,\n        )\n\n        self.overlap_threshold = overlap_threshold\n        self.binarizer_ = Binarize(\n            onset=self.overlap_threshold,\n            offset=self.overlap_threshold,\n            scale=""absolute"",\n            log_scale=True,\n        )\n\n    def _decode(\n        self,\n        current_file: ProtocolFile,\n        hypothesis: Annotation,\n        scores: SlidingWindowFeature,\n        labels: Iterable,\n    ) -> Annotation:\n\n        # obtain overlapped speech regions\n        overlap = self.binarizer_.apply(current_file[""overlap""], dimension=1)\n\n        frames = scores.sliding_window\n        N, K = scores.data.shape\n\n        if self.lock_speech:\n\n            # K = 1 <~~> only non-speech\n            # K = 2 <~~> just one speaker\n            if K < 3:\n                return hypothesis\n\n            # sequence of two most likely speaker indices\n            # (even when non-speech is in fact the most likely class)\n            best_speakers_indices = np.argsort(-scores.data[:, 1:], axis=1)[:, :2]\n\n            active_speakers = np.zeros((N, K - 1), dtype=np.int64)\n\n            # start by assigning most likely speaker...\n            for t, k in enumerate(best_speakers_indices[:, 0]):\n                active_speakers[t, k] = 1\n\n            # ... then add second most likely speaker in overlap regions\n            T = frames.crop(overlap, mode=""strict"")\n\n            # because overlap may use a different feature extraction step\n            # it might happen that T contains indices slightly large than\n            # the actual number of frames. the line below remove any such\n            # indices.\n            T = T[T < N]\n\n            # mark second most likely speaker as active\n            active_speakers[T, best_speakers_indices[T, 1]] = 1\n\n            # reconstruct annotation\n            new_hypothesis = one_hot_decoding(active_speakers, frames, labels=labels)\n\n            # revert non-speech regions back to original\n            speech = hypothesis.get_timeline().support()\n            new_hypothesis = new_hypothesis.crop(speech)\n\n        else:\n\n            # K = 1 <~~> only non-speech\n            if K < 2:\n                return hypothesis\n\n            # sequence of two most likely class indices\n            # sequence of two most likely class indices\n            # (including 0=non-speech)\n            best_speakers_indices = np.argsort(-scores.data, axis=1)[:, :2]\n\n            active_speakers = np.zeros((N, K - 1), dtype=np.int64)\n\n            # start by assigning the most likely speaker...\n            for t, k in enumerate(best_speakers_indices[:, 0]):\n                # k = 0 is for non-speech\n                if k > 0:\n                    active_speakers[t, k - 1] = 1\n\n            # ... then add second most likely speaker in overlap regions\n            T = frames.crop(overlap, mode=""strict"")\n\n            # because overlap may use a different feature extraction step\n            # it might happen that T contains indices slightly large than\n            # the actual number of frames. the line below remove any such\n            # indices.\n            T = T[T < N]\n\n            # remove timesteps where second most likely class is non-speech\n            T = T[best_speakers_indices[T, 1] > 0]\n\n            # mark second most likely speaker as active\n            active_speakers[T, best_speakers_indices[T, 1] - 1] = 1\n\n            # reconstruct annotation\n            new_hypothesis = one_hot_decoding(active_speakers, frames, labels=labels)\n\n        new_hypothesis.uri = hypothesis.uri\n        return new_hypothesis\n'"
pyannote/audio/labeling/tasks/speaker_change_detection.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2018-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n""""""Speaker change detection""""""\n\nfrom typing import Optional\nfrom typing import Text\n\nimport numpy as np\nfrom .base import LabelingTask\nfrom .base import LabelingTaskGenerator\nfrom pyannote.audio.train.task import Task, TaskType, TaskOutput\nimport scipy.signal\nfrom pyannote.audio.features.wrapper import Wrappable\nfrom pyannote.database.protocol.protocol import Protocol\nfrom pyannote.audio.train.model import Resolution\nfrom pyannote.audio.train.model import Alignment\nfrom pyannote.audio.train.model import RESOLUTION_FRAME\n\n\nclass SpeakerChangeDetectionGenerator(LabelingTaskGenerator):\n    """"""Batch generator for training speaker change detection\n\n    Parameters\n    ----------\n    feature_extraction : Wrappable\n        Describes how features should be obtained.\n        See pyannote.audio.features.wrapper.Wrapper documentation for details.\n    protocol : Protocol\n    subset : {\'train\', \'development\', \'test\'}, optional\n        Protocol and subset.\n    resolution : `pyannote.core.SlidingWindow`, optional\n        Override `feature_extraction.sliding_window`. This is useful for\n        models that include the feature extraction step (e.g. SincNet) and\n        therefore output a lower sample rate than that of the input.\n        Defaults to `feature_extraction.sliding_window`\n    alignment : {\'center\', \'loose\', \'strict\'}, optional\n        Which mode to use when cropping labels. This is useful for models that\n        include the feature extraction step (e.g. SincNet) and therefore use a\n        different cropping mode. Defaults to \'center\'.\n    duration : float, optional\n        Duration of audio chunks. Defaults to 2s.\n    batch_size : int, optional\n        Batch size. Defaults to 32.\n    per_epoch : float, optional\n        Force total audio duration per epoch, in days.\n        Defaults to total duration of protocol subset.\n    mask : str, optional\n        When provided, protocol files are expected to contain a key named after\n        this `mask` variable and providing a `SlidingWindowFeature` instance.\n        Generated batches will contain an additional ""mask"" key (on top of\n        existing ""X"" and ""y"" keys) computed as an excerpt of `current_file[mask]`\n        time-aligned with ""y"". Defaults to not add any ""mask"" key.\n    collar : float, optional\n        Duration of positive collar, in seconds. Default to 0.1 (i.e. frames\n        less than 100ms away from the actual change are also labeled as\n        change).\n    regression : bool, optional\n        Use triangle-shaped label sequences centered on actual changes.\n        Defaults to False (i.e. rectangle-shaped label sequences).\n    non_speech : bool, optional\n        Keep non-speech/speaker changes. Defauls to False (i.e. only\n        speaker/speaker changes are marked as such).\n    """"""\n\n    def __init__(\n        self,\n        task: Task,\n        feature_extraction: Wrappable,\n        protocol: Protocol,\n        subset: Text = ""train"",\n        resolution: Optional[Resolution] = None,\n        alignment: Optional[Alignment] = None,\n        duration: float = 2.0,\n        batch_size: int = 32,\n        per_epoch: float = None,\n        mask: Text = None,\n        collar: float = 0.1,\n        regression: bool = False,\n        non_speech: bool = False,\n    ):\n\n        self.collar = collar\n        self.regression = regression\n        self.non_speech = non_speech\n\n        # number of samples in collar\n        if resolution in [None, RESOLUTION_FRAME]:\n            resolution = feature_extraction.sliding_window\n        self.collar_ = resolution.duration_to_samples(collar)\n\n        # window\n        self.window_ = scipy.signal.triang(self.collar_)[:, np.newaxis]\n\n        super().__init__(\n            task,\n            feature_extraction,\n            protocol,\n            subset=subset,\n            resolution=resolution,\n            alignment=alignment,\n            duration=duration,\n            batch_size=batch_size,\n            per_epoch=per_epoch,\n            exhaustive=False,\n            mask=mask,\n            local_labels=True,\n        )\n\n    def postprocess_y(self, Y: np.ndarray) -> np.ndarray:\n        """"""Generate labels for speaker change detection\n\n        Parameters\n        ----------\n        Y : (n_samples, n_speakers) numpy.ndarray\n            Discretized annotation returned by `pyannote.core.utils.numpy.one_hot_encoding`.\n\n        Returns\n        -------\n        y : (n_samples, 1) numpy.ndarray\n\n        See also\n        --------\n        `pyannote.core.utils.numpy.one_hot_encoding`\n        """"""\n\n        # replace NaNs by 0s\n        Y = np.nan_to_num(Y)\n        n_samples, n_speakers = Y.shape\n\n        # True = change. False = no change\n        y = np.sum(np.abs(np.diff(Y, axis=0)), axis=1, keepdims=True)\n        y = np.vstack(([[0]], y > 0))\n\n        # mark change points neighborhood as positive\n        y = np.minimum(1, scipy.signal.convolve(y, self.window_, mode=""same""))\n\n        # HACK for some reason, y rarely equals zero\n        if not self.regression:\n            y = 1 * (y > 1e-10)\n\n        # at this point, all segment boundaries are marked as change\n        # (including non-speech/speaker changes\xc3\xa0\n\n        # remove non-speech/speaker change\n        if not self.non_speech:\n\n            # append (half collar) empty samples at the beginning/end\n            expanded_Y = np.vstack(\n                [\n                    np.zeros(((self.collar_ + 1) // 2, n_speakers), dtype=Y.dtype),\n                    Y,\n                    np.zeros(((self.collar_ + 1) // 2, n_speakers), dtype=Y.dtype),\n                ]\n            )\n\n            # stride trick. data[i] is now a sliding window of collar length\n            # centered at time step i.\n            data = np.lib.stride_tricks.as_strided(\n                expanded_Y,\n                shape=(n_samples, n_speakers, self.collar_),\n                strides=(Y.strides[0], Y.strides[1], Y.strides[0]),\n            )\n\n            # y[i] = 1 if more than one speaker are speaking in the\n            # corresponding window. 0 otherwise\n            x_speakers = 1 * (np.sum(np.sum(data, axis=2) > 0, axis=1) > 1)\n            x_speakers = x_speakers.reshape(-1, 1)\n\n            y *= x_speakers\n\n        return y\n\n    @property\n    def specifications(self):\n\n        specs = {\n            ""task"": self.task,\n            ""X"": {""dimension"": self.feature_extraction.dimension},\n        }\n\n        if self.regression:\n            specs[""y""] = {""classes"": [""change"",]}\n        else:\n            specs[""y""] = {""classes"": [""non_change"", ""change""]}\n\n        return specs\n\n\nclass SpeakerChangeDetection(LabelingTask):\n    """"""Train speaker change detection\n\n    Parameters\n    ----------\n    collar : float, optional\n        Duration of positive collar, in seconds. Default to 0.1 (i.e. frames\n        less than 100ms away from the actual change are also labeled as\n        change).\n    regression : bool, optional\n        Use triangle-shaped label sequences centered on actual changes.\n        Defaults to False (i.e. rectangle-shaped label sequences).\n    non_speech : bool, optional\n        Keep non-speech/speaker changes (and vice-versa). Defauls to False\n        (i.e. only keep speaker/speaker changes).\n    duration : float, optional\n        Duration of sub-sequences. Defaults to 3.2s.\n    batch_size : int, optional\n        Batch size. Defaults to 32.\n    per_epoch : float, optional\n        Total audio duration per epoch, in days.\n        Defaults to one day (1).\n    """"""\n\n    def __init__(self, collar=0.100, regression=False, non_speech=False, **kwargs):\n        super().__init__(**kwargs)\n        self.collar = collar\n        self.regression = regression\n        self.non_speech = non_speech\n\n    def get_batch_generator(\n        self,\n        feature_extraction,\n        protocol,\n        subset=""train"",\n        resolution=None,\n        alignment=None,\n    ):\n        """"""\n        resolution : `pyannote.core.SlidingWindow`, optional\n            Override `feature_extraction.sliding_window`. This is useful for\n            models that include the feature extraction step (e.g. SincNet) and\n            therefore output a lower sample rate than that of the input.\n        alignment : {\'center\', \'loose\', \'strict\'}, optional\n            Which mode to use when cropping labels. This is useful for models\n            that include the feature extraction step (e.g. SincNet) and\n            therefore use a different cropping mode. Defaults to \'center\'.\n        """"""\n        return SpeakerChangeDetectionGenerator(\n            self.task,\n            feature_extraction,\n            protocol,\n            resolution=resolution,\n            alignment=alignment,\n            subset=subset,\n            collar=self.collar,\n            regression=self.regression,\n            non_speech=self.non_speech,\n            duration=self.duration,\n            batch_size=self.batch_size,\n            per_epoch=self.per_epoch,\n        )\n\n    @property\n    def task(self):\n\n        if self.regression:\n            return Task(type=TaskType.REGRESSION, output=TaskOutput.SEQUENCE)\n        else:\n            return Task(\n                type=TaskType.MULTI_CLASS_CLASSIFICATION, output=TaskOutput.SEQUENCE\n            )\n'"
pyannote/audio/labeling/tasks/speech_activity_detection.py,14,"b'#!/usr/bin/env python\n# encoding: utf-8\n\n# The MIT License (MIT)\n\n# Copyright (c) 2018-2020 CNRS\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n# AUTHORS\n# Herv\xc3\xa9 BREDIN - http://herve.niderb.fr\n\n""""""Speech activity detection""""""\n\nfrom typing import Optional\nfrom typing import Text\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom .base import LabelingTask\nfrom .base import LabelingTaskGenerator\nfrom pyannote.audio.train.task import Task, TaskType, TaskOutput\nfrom ..gradient_reversal import GradientReversal\nfrom pyannote.audio.models.models import RNN\nfrom pyannote.audio.features.wrapper import Wrappable\nfrom pyannote.database.protocol.protocol import Protocol\nfrom pyannote.audio.train.model import Resolution\nfrom pyannote.audio.train.model import Alignment\n\n\nclass SpeechActivityDetectionGenerator(LabelingTaskGenerator):\n    """"""Batch generator for training speech activity detection\n\n    Parameters\n    ----------\n    task : Task\n        Task\n    feature_extraction : Wrappable\n        Describes how features should be obtained.\n        See pyannote.audio.features.wrapper.Wrapper documentation for details.\n    protocol : Protocol\n    subset : {\'train\', \'development\', \'test\'}, optional\n        Protocol and subset.\n    resolution : `pyannote.core.SlidingWindow`, optional\n        Override `feature_extraction.sliding_window`. This is useful for\n        models that include the feature extraction step (e.g. SincNet) and\n        therefore output a lower sample rate than that of the input.\n        Defaults to `feature_extraction.sliding_window`\n    alignment : {\'center\', \'loose\', \'strict\'}, optional\n        Which mode to use when cropping labels. This is useful for models that\n        include the feature extraction step (e.g. SincNet) and therefore use a\n        different cropping mode. Defaults to \'center\'.\n    duration : float, optional\n        Duration of audio chunks. Defaults to 2s.\n    batch_size : int, optional\n        Batch size. Defaults to 32.\n    per_epoch : float, optional\n        Force total audio duration per epoch, in days.\n        Defaults to total duration of protocol subset.\n    mask : str, optional\n        When provided, protocol files are expected to contain a key named after\n        this `mask` variable and providing a `SlidingWindowFeature` instance.\n        Generated batches will contain an additional ""mask"" key (on top of\n        existing ""X"" and ""y"" keys) computed as an excerpt of `current_file[mask]`\n        time-aligned with ""y"". Defaults to not add any ""mask"" key.\n\n    """"""\n\n    def __init__(\n        self,\n        task: Task,\n        feature_extraction: Wrappable,\n        protocol: Protocol,\n        subset: Text = ""train"",\n        resolution: Optional[Resolution] = None,\n        alignment: Optional[Alignment] = None,\n        duration: float = 2.0,\n        batch_size: int = 32,\n        per_epoch: float = None,\n        mask: Text = None,\n    ):\n\n        super().__init__(\n            task,\n            feature_extraction,\n            protocol,\n            subset=subset,\n            resolution=resolution,\n            alignment=alignment,\n            duration=duration,\n            batch_size=batch_size,\n            per_epoch=per_epoch,\n            exhaustive=False,\n            mask=mask,\n            local_labels=True,\n        )\n\n    def postprocess_y(self, Y: np.ndarray) -> np.ndarray:\n        """"""Generate labels for speech activity detection\n\n        Parameters\n        ----------\n        Y : (n_samples, n_speakers) numpy.ndarray\n            Discretized annotation returned by\n            `pyannote.core.utils.numpy.one_hot_encoding`.\n\n        Returns\n        -------\n        y : (n_samples, 1) numpy.ndarray\n\n        See also\n        --------\n        `pyannote.core.utils.numpy.one_hot_encoding`\n        """"""\n\n        # number of speakers for each frame\n        speaker_count = np.sum(Y, axis=1, keepdims=True)\n\n        # mark speech regions as such\n        return np.int64(speaker_count > 0)\n\n    @property\n    def specifications(self):\n        specs = {\n            ""task"": self.task,\n            ""X"": {""dimension"": self.feature_extraction.dimension},\n            ""y"": {""classes"": [""non_speech"", ""speech""]},\n        }\n\n        for key, classes in self.file_labels_.items():\n\n            # TODO. add an option to handle this list\n            # TODO. especially useful for domain-adversarial stuff\n            if key in [""duration"", ""audio"", ""uri""]:\n                continue\n            specs[key] = {""classes"": classes}\n\n        return specs\n\n\nclass SpeechActivityDetection(LabelingTask):\n    """"""Train speech activity (and overlap) detection\n\n    Parameters\n    ----------\n    duration : float, optional\n        Duration of sub-sequences. Defaults to 3.2s.\n    batch_size : int, optional\n        Batch size. Defaults to 32.\n    per_epoch : float, optional\n        Total audio duration per epoch, in days.\n        Defaults to one day (1).\n    """"""\n\n    def get_batch_generator(\n        self,\n        feature_extraction,\n        protocol,\n        subset=""train"",\n        resolution=None,\n        alignment=None,\n    ):\n        """"""\n        resolution : `pyannote.core.SlidingWindow`, optional\n            Override `feature_extraction.sliding_window`. This is useful for\n            models that include the feature extraction step (e.g. SincNet) and\n            therefore output a lower sample rate than that of the input.\n        alignment : {\'center\', \'loose\', \'strict\'}, optional\n            Which mode to use when cropping labels. This is useful for models\n            that include the feature extraction step (e.g. SincNet) and\n            therefore use a different cropping mode. Defaults to \'center\'.\n        """"""\n        return SpeechActivityDetectionGenerator(\n            self.task,\n            feature_extraction,\n            protocol,\n            subset=subset,\n            resolution=resolution,\n            alignment=alignment,\n            duration=self.duration,\n            per_epoch=self.per_epoch,\n            batch_size=self.batch_size,\n        )\n\n\nclass DomainAwareSpeechActivityDetection(SpeechActivityDetection):\n    """"""Domain-aware speech activity detection\n\n    Trains speech activity detection and domain classification jointly.\n\n    Parameters\n    ----------\n    domain : `str`, optional\n        Batch key to use as domain. Defaults to \'domain\'.\n        Could be \'database\' or \'uri\' for instance.\n    attachment : `int`, optional\n        Intermediate level where to attach the domain classifier.\n        Defaults to -1. Passed to `return_intermediate` in models supporting it.\n    rnn: `dict`, optional\n        Parameters of the RNN used in the domain classifier.\n        See `pyannote.audio.models.models.RNN` for details.\n    domain_loss : `str`, optional\n        Loss function to use. Defaults to \'NLLLoss\'.\n    """"""\n\n    DOMAIN_PT = ""{train_dir}/weights/{epoch:04d}.domain.pt""\n\n    def __init__(\n        self, domain=""domain"", attachment=-1, rnn=None, domain_loss=""NLLLoss"", **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.domain = domain\n        self.attachment = attachment\n\n        if rnn is None:\n            rnn = dict()\n        self.rnn = rnn\n\n        self.domain_loss = domain_loss\n        if self.domain_loss == ""NLLLoss"":\n            # Default value\n            self.domain_loss_ = nn.NLLLoss()\n            self.activation_ = nn.LogSoftmax(dim=1)\n\n        elif self.domain_loss == ""MSELoss"":\n            self.domain_loss_ = nn.MSELoss()\n            self.activation_ = nn.Sigmoid()\n\n        else:\n            msg = f""{domain_loss} has not been implemented yet.""\n            raise NotImplementedError(msg)\n\n    def more_parameters(self):\n        """"""Initialize trainable trainer parameters\n\n        Yields\n        ------\n        parameter : nn.Parameter\n            Trainable trainer parameters\n        """"""\n\n        domain_classifier_rnn = RNN(\n            n_features=self.model.intermediate_dimension(self.attachment), **self.rnn\n        )\n\n        n_classes = len(self.specifications[self.domain][""classes""])\n        domain_classifier_linear = nn.Linear(\n            domain_classifier_rnn.dimension, n_classes, bias=True\n        ).to(self.device)\n\n        self.domain_classifier_ = nn.Sequential(\n            domain_classifier_rnn, domain_classifier_linear\n        ).to(self.device)\n\n        # TODO: check if we really need to do this .to(self.device) twice\n\n        return self.domain_classifier_.parameters()\n\n    def load_more(self, model_pt=None) -> bool:\n        """"""Load classifier from disk""""""\n\n        if model_pt is None:\n            domain_pt = self.DOMAIN_PT.format(\n                train_dir=self.train_dir_, epoch=self.epoch_\n            )\n        else:\n            domain_pt = model_pt.with_suffix("".domain.pt"")\n\n        domain_classifier_state = torch.load(\n            domain_pt, map_location=lambda storage, loc: storage\n        )\n        self.domain_classifier_.load_state_dict(domain_classifier_state)\n\n        # FIXME add support for different domains\n        return True\n\n    def save_more(self):\n        """"""Save domain classifier to disk""""""\n\n        domain_pt = self.DOMAIN_PT.format(train_dir=self.train_dir_, epoch=self.epoch_)\n        torch.save(self.domain_classifier_.state_dict(), domain_pt)\n\n    def batch_loss(self, batch):\n        """"""Compute loss for current `batch`\n\n        Parameters\n        ----------\n        batch : `dict`\n            [\'X\'] (`numpy.ndarray`)\n            [\'y\'] (`numpy.ndarray`)\n\n        Returns\n        -------\n        batch_loss : `dict`\n            [\'loss\'] (`torch.Tensor`) : Loss\n        """"""\n\n        # forward pass\n        X = torch.tensor(batch[""X""], dtype=torch.float32, device=self.device_)\n        fX, intermediate = self.model_(X, return_intermediate=self.attachment)\n\n        # speech activity detection\n        fX = fX.view((-1, self.n_classes_))\n        target = (\n            torch.tensor(batch[""y""], dtype=torch.int64, device=self.device_)\n            .contiguous()\n            .view((-1,))\n        )\n\n        weight = self.weight\n        if weight is not None:\n            weight = weight.to(device=self.device_)\n        loss = self.loss_func_(fX, target, weight=weight)\n\n        # domain classification\n        domain_target = torch.tensor(\n            batch[self.domain], dtype=torch.int64, device=self.device_\n        )\n\n        domain_scores = self.activation_(self.domain_classifier_(intermediate))\n\n        domain_loss = self.domain_loss_(domain_scores, domain_target)\n\n        return {\n            ""loss"": loss + domain_loss,\n            ""loss_domain"": domain_loss,\n            ""loss_task"": loss,\n        }\n\n\nclass DomainAdversarialSpeechActivityDetection(DomainAwareSpeechActivityDetection):\n    """"""Domain Adversarial speech activity detection\n\n    Parameters\n    ----------\n    domain : `str`, optional\n        Batch key to use as domain. Defaults to \'domain\'.\n        Could be \'database\' or \'uri\' for instance.\n    attachment : `int`, optional\n        Intermediate level where to attach the domain classifier.\n        Defaults to -1. Passed to `return_intermediate` in models supporting it.\n    alpha : `float`, optional\n        Coefficient multiplied with the domain loss\n    """"""\n\n    def __init__(self, domain=""domain"", attachment=-1, alpha=1.0, **kwargs):\n        super().__init__(domain=domain, attachment=attachment, **kwargs)\n        self.alpha = alpha\n        self.gradient_reversal_ = GradientReversal()\n\n    def batch_loss(self, batch):\n        """"""Compute loss for current `batch`\n\n        Parameters\n        ----------\n        batch : `dict`\n            [\'X\'] (`numpy.ndarray`)\n            [\'y\'] (`numpy.ndarray`)\n\n        Returns\n        -------\n        batch_loss : `dict`\n            [\'loss\'] (`torch.Tensor`) : Loss\n        """"""\n        # forward pass\n        X = torch.tensor(batch[""X""], dtype=torch.float32, device=self.device_)\n\n        fX, intermediate = self.model_(X, return_intermediate=self.attachment)\n\n        # speech activity detection\n        fX = fX.view((-1, self.n_classes_))\n\n        target = (\n            torch.tensor(batch[""y""], dtype=torch.int64, device=self.device_)\n            .contiguous()\n            .view((-1,))\n        )\n\n        weight = self.weight\n        if weight is not None:\n            weight = weight.to(device=self.device_)\n\n        loss = self.loss_func_(fX, target, weight=weight)\n\n        # domain classification\n        domain_target = torch.tensor(\n            batch[self.domain], dtype=torch.int64, device=self.device_\n        )\n\n        domain_scores = self.activation_(\n            self.domain_classifier_(self.gradient_reversal_(intermediate))\n        )\n\n        if self.domain_loss == ""MSELoss"":\n            # One hot encode domain_target for Mean Squared Error Loss\n            nb_domains = domain_scores.shape[1]\n            identity_mat = torch.sparse.torch.eye(nb_domains, device=self.device_)\n            domain_target = identity_mat.index_select(dim=0, index=domain_target)\n\n        domain_loss = self.domain_loss_(domain_scores, domain_target)\n\n        return {\n            ""loss"": loss + self.alpha * domain_loss,\n            ""loss_domain"": domain_loss,\n            ""loss_task"": loss,\n        }\n'"
